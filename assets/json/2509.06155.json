{
    "paper_title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
    "authors": [
        "Duomin Wang",
        "Wei Zuo",
        "Aojie Li",
        "Ling-Hao Chen",
        "Xinyao Liao",
        "Deyu Zhou",
        "Zixin Yin",
        "Xili Dai",
        "Daxin Jiang",
        "Gang Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 5 5 1 6 0 . 9 0 5 2 : r UniVerse-1: Unified Audio-Video Generation via Stitching of Experts Duomin Wang1 Wei Zuo1 Aojie Li1 Ling-Hao Chen1,4 Xinyao Liao1 Deyu Zhou1,2 Zixin Yin1,3 Xili Dai2 Daxin Jiang1 Gang Yu1 StepFun1 The Hong Kong University of Science and Technology(GuangZhou)2 The Hong Kong University of Science and Technology3 Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": "We introduce UniVerse-1, unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7, 600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/."
        },
        {
            "title": "Introduction",
            "content": "The era of diffusion models [1, 2, 3] has culminated in the rise of Diffusion Transformer (DiT) architectures [4], exemplified by landmark models like Sora [5] and its open-source counterparts [6, 7, 8]. These models, leveraging unprecedented scales of data and computation, have achieved remarkable quality and prompt alignment in video generation. This success is catalyzing profound transformation across creative industries and has spurred wave of research into downstream applications such as talking head synthesis [9, 10, 11, 12] and human animation [13, 14, 15], which now offer viable real-world solutions. However, this rapid progress has been almost exclusively confined to the visual domain, treating video as silent movie. This unimodal focus represents fundamental bottleneck, as it ignores the inherently multimodal nature of video. Post-hoc video-to-audio models [16] serve as superficial fix, but they are inherently limited; while capable of adapting audio to existing visual content, they fail to enforce temporal alignment in the reverse direction. This makes critical tasks, such as synchronizing lip movements with speech, impossible. While closed-source systems like Googles Veo3 [17] have demonstrated synchronous audio-video generation, the lack of publicly available technical details leaves critical gap in open research. To bridge this gap between closed-source systems and open research, we introduce UniVerse-1: unified, fully open-source, Veo-3-like model capable of simultaneously generating coordinated audio and video. Our work is underpinned by several key technical contributions designed to address the unique challenges of bimodal generation. Instead of the costly process of training new model from scratch, we propose novel and efficient stitching of experts (SoE) paradigm. This methodology effectively fuses state-of-the-art video generation model, WAN2.1 [8], with music generation model, Ace-step [18]. The core of this fusion lies in lightweight, cross-modal MLP connectors introduced within corresponding blocks of each model. These connectors facilitate bidirectional interaction between modalities, and we found this strategy to significantly accelerate training convergence by leveraging the powerful priors of the pre-trained experts. Furthermore, we tackle the critical challenge of data alignment in bimodal training. We argue that static, pre-processed annotations are flawed paradigm for tasks requiring precise temporal consistency. To address this, we developed an online annotation pipeline that generates labels dynamically during training. This approach ensures strict temporal and semantic alignment between audio-video data and their textual descriptions, mitigating the performance degradation caused by static misalignment. During our investigation, we also uncovered crucial, yet overlooked, factor in bimodal diffusion modeling: cross-modal noise correlation. We identified that the standard pseudo-random number generation process [19] can introduce spurious correlations between the noise vectors for video and audio, which subsequently degrades audio quality during inference. Our solution involves ensuring independent noise sampling for each modality. To support this work, we curated high-quality dataset comprising approximately 7, 600 hours of precisely aligned audio-video content. To systematically evaluate our method, we also propose Verse-Bench, new benchmark featuring 600 image-text prompt pairs covering diverse range of sound categories. Highlighting its versatility, Verse-Bench supports not only joint audio-video generation but also unidirectional tasks, including specialized Verse-Ted subset designed for evaluating audio-to-video synthesis. In summary, our primary contributions are: An Open-source Audio-Video Foundation Model: We present UniVerse-1, novel, open-source model capable of producing highly coherent and well-aligned synchronous audio-visual content, closing critical gap in the open-source community. Novel Methodology for Joint Audio-Video Generation: We propose comprehensive methodology to enable efficient and high-quality joint audio-video synthesis. This is achieved through three key innovations: stitching of experts (SoE) paradigm to accelerate convergence by fusing pre-trained models; an online data annotation pipeline to solve the critical static misalignment problem in training; and the identification and mitigation of the previously overlooked cross-modal noise correlation issue, crucial factor for generation quality. Comprehensive Evaluation Benchmark: We propose Verse-Bench, new benchmark designed to comprehensively evaluate joint audio-video generation models across diverse set of tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Video Diffusion Models The field of video generation was revolutionized by the introduction of diffusion models, with pioneering works like AnimateDiff [20] and Video Diffusion Models [21] marking the beginning of this new era. This initial wave of research was further advanced by models such as Stable Video Diffusion [22], which first demonstrated that curating large-scale, high-quality datasets is critical for enhancing model performance. common characteristic of these early models was their reliance on UNet architectures. To mitigate the challenges posed by the limited availability and quality of video data compared to images, these models were typically fine-tuned from pre-trained UNet image foundations. significant paradigm shift occurred with the introduction of Sora [5], which heralded new age defined by the Diffusion Transformer (DiT) architecture [4] and training on massive, high-quality video corpora. This breakthrough spurred proliferation of subsequent research. CogVideox [7] was the first to release an open-source DiT-based model, providing significant catalyst for community-driven innovation. This was followed by other notable open-source models such as HunyuanVideo [6], WAN2.1 [8], and Step-Video [23], as well as high-performing closed-source systems including Kling [24], SeeDance 1.0 [25], Movie Gen [26], and Veo2 [27]. Architecturally, these contemporary models converge on common blueprint. They 2 employ 3D Variational Autoencoder (VAE) to achieve spatio-temporal compression of video into latent space. The core generative process is then handled by DiT, which learns to denoise these noisy latents. Across these state-of-the-art models, the quality and scale of the training data have been identified as paramount factors, making data curation and processing central component of their development. Audio Diffusion Models The application of diffusion models to audio generation has followed parallel trajectory to their video counterparts, fundamentally transforming the landscape of textto-audio and text-to-music synthesis. Early explorations demonstrated the potential of diffusion for generating high-fidelity audio, but pivotal advancement was the adoption of latent diffusion architectures [28], which significantly improved both efficiency and quality. common technical pipeline for these approaches involves first transforming the raw audio waveform into mel spectrogram. Variational Autoencoder (VAE) is then trained on this spectrogram representation to learn compressed latent space, within which the core diffusion process operates. Within this framework, models such as Stable Audio Open [29] and Riffusion [30] excel at generating high-fidelity, long-form audio. Furthermore, models like the AudioLDM series [31, 32] and DiffRhythm [33] advance these capabilities to vocal music synthesis, offering fine-grained control over rhythm and other expressive attributes. Joint Audio and Video Generation The exploration of joint audio-video generation within diffusion frameworks began with pioneering efforts like MM-Diffusion [34]. This model was the first to tackle this bimodal task, employing UNet architecture with two distinct subnetworks, each dedicated to processing the audio and video modalities, respectively. Following this initial work, series of subsequent models emerged [35, 36, 37]. However, these early approaches were typically constrained by small-scale training datasets [38, 39], often less than 10 hours in size, which inherently limited their diversity and generalization capabilities. notable step forward was made by models such as Syncflow [40] and Uniform [41], which scaled up the training data to approximately 500 hours by leveraging the VGGSound [42] and AudioSet [43] dataset, thereby enhancing their generalization. Despite this progress, persistent challenges remained, including suboptimal video quality and low degree of disentanglement between the audio and visual streams. The advent of Googles Veo3 [17] marked significant milestone, representing the first large-scale initiative in synchronous audio-video generation. Veo3 demonstrated the capacity for generating high-fidelity audio and video that is not only diverse but also semantically and temporally coordinated, strictly adhering to user-provided text prompts."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Our model is constructed upon the foundations of the Wan2.1 (1.3B parameters) [8] text-to-video model and the Ace-step (3.5B parameters) [18] music generation model. Before delving into technical details, we will briefly introduce their respective architectures. Wan2.1 model. Wan2.1 is composed of three primary components: 3D Variational Autoencoder (VAE), an umT5 [44] text encoder, and Diffusion Transformer (DiT). The 3D VAE compresses an input video of shape (3, T, H, ) into latent representation of shape (16, /t, H/h, W/w), where the temporal and spatial downsampling factors are = 4 and = = 8, respectively. Prior to being input to the DiT, this latent tensor is patchified using kernel of (1, 2, 2), and the resulting tokens serve as the input sequence. The umT5 model encodes the text prompt, and its embeddings are injected into the DiT via cross-attention to condition the generation. The model is trained to predict the velocity, which is used in the denoising step to recover the clean latent. Finally, the decoder of 3D VAE reconstructs this latent into the final video. Ace-step model. Ace-step consists of Music-DCAE (Deep Compression Autoencoder) [45], umT5 text encoder, lyric encoder, speaker encoder, and DiT. The raw audio waveform is first converted into mel spectrogram. The Music-DCAE then encodes the input spectrogram of shape (8, T, ) into latent representation of shape (8, /t, F/f ), with downsampling factors = = 8 along the temporal and frequency axes. This latent is subsequently patchified using kernel of (16, 1) to produce the input tokens for the DiT. Conditional control is provided by three sources: the umT5 3 encoder for the music style prompt, the lyric encoder for the lyrics, and the speaker encoder for the speaker ID. These three embeddings are concatenated along the channel dimension and injected into the DiT via cross-attention. The model predicts the velocity to obtain the clean latent, which the Music-DCAEs decoder reconstructs into mel spectrogram. HiFiGAN vocoder [46] is then used to convert this spectrogram into the final audio waveform. Base model pre-training. The learning objective for both of the aforementioned models is Flow Matching [3]. This fashion trains neural network, vΘ(, t, c), to predict velocity field that transports samples from simple source distribution, p0 (e.g., Gaussian noise), to complex target data distribution, p1. Specifically, these models leverage Conditional Flow Matching. Given noise sample x0 p0 and data sample x1 p1, simple linear interpolation path is defined for time t: xt = (1 t)x0 + tx1. (1) The target velocity vector along this path is constant: ut = x1 x0. The model vΘ(xt, t, c), conditioned on c, is trained to predict this vector by minimizing the following L2 loss: (cid:2)vθ((1 t)x0 + tx1, t, c) (x1 x0)2(cid:3) . LFM = EtU (0,1),x0p0,x1p1 This objective directly trains the model to learn the vector field that maps noise to data, which leads to more stable and efficient training compared to traditional score-matching objectives."
        },
        {
            "title": "3.2 Data Curation",
            "content": "We curated large-scale, high-quality dataset from diverse range of sources to train our model. The primary component was sourced from YouTube, encompassing content such as music variety shows, classical music performances, cooking tutorials, public speeches, interviews, vlogs, and demonstrations of tool usage. This was supplemented with cinematic movie clips and high-quality stock footage from Pexels. To further bolster the audio modality, we also incorporated the widely-used VGGSound and AudioSet datasets. For our self-collected data (YouTube, Pexels, and movie clips), we implemented rigorous multi-stage filtering pipeline to ensure data quality and relevance: Audio-Visual Pre-screening Videos lacking an audio track were immediately discarded. Quality Control We filtered out content based on technical specifications: resolution below 1080p, bitrate-to-resolution ratio under 600, and DOVER [47] aesthetic quality score below 0.6. Temporal Coherence PySceneDetect1 was applied to segment videos, and any resulting clip shorter than 5 seconds was removed to ensure meaningful duration. Audio Activity Detection To eliminate silent segments, we analyzed each audio track for metrics such as volume, energy, and zero-crossing rate. Speech Content Verification Whisper [48] was used to detect the presence of human speech. Clips without speech were retained as general audio-visual data. If speech was present, it proceeded to the next step. Human Face Detection For clips identified as containing speech, second verification step was performed: we detected for the presence of human face [49]. If no face was found, the clip was discarded. If face was present, we employed SyncNet [50] to verify the audio-visual correspondence (lip-sync). Only clips with SyncNet confidence score above threshold of 2.0 were retained and explicitly labeled as containing speech content. For the VGGSound and AudioSet data, simplified process was used where we either performed scene detection or segmented clips based on their existing timestamps, retaining only those longer than 5 seconds. Following this comprehensive curation process, our final dataset comprises 7, 685 hours of data. This is categorized into three subsets: 1, 187 hours of verified speech-centric content, 3, 074 hours of general-purpose audio-video data, and 3, 422 hours from VGGSound and AudioSet primarily used for bolstering audio-specific training. 1https://github.com/Breakthrough/PySceneDetect"
        },
        {
            "title": "3.3 Online Data Annotation",
            "content": "Conventional offline annotation methods, where captions are pre-generated for entire videos, present significant challenge for training generative models. During training, fixed-length clips are randomly sampled from these videos, often creating temporal and semantic misalignment between the sampled clip and the global, pre-existing caption. This issue is particularly acute in the context of joint audiovideo generation, where the temporal synchronization between an acoustic event and its description is critical. Even minor temporal shifts can render an audio annotation invalid. To overcome these limitations, we propose and implement an Online Data Annotation Pipeline. This pipeline operates as dedicated server process that runs concurrently with training. It dynamically fetches raw video, processes clips in real-time to generate precisely aligned data-annotation pairs, and populates shared buffer. The main training process then acts as consumer, fetching these ready-to-use data tuples, ensuring that every training instance is perfectly synchronized. The online processing for each data tuple involves the following steps: Temporal Sampling fixed-length segment (e.g., 5 seconds) is randomly extracted from source video, yielding corresponding video and audio streams. Multi-modal Annotation The extracted audio-video clip is immediately passed to our annotation module for captioning. Text and Video Encoding The generated text prompts (for video, audio, and speech) are encoded. Concurrently, the video clip is encoded into spatio-temporal latent representation using the 3D VAE. Audio Encoding The audio stream is converted to mel spectrogram and subsequently encoded into latent representation by the Music-DCAE [45]. The core of our pipeline is the multi-modal annotation step (Step 2), which proceeds as follows: Speech Transcription Whisper [48] is employed to perform Automatic Speech Recognition (ASR) on the sampled audio, yielding the raw speech content. Structured Multimodal Captioning We construct structured prompt that incorporates the transcribed speech. This prompt, along with the audio and video streams of the clip, is fed into the QWen2.5-Omni [51] multimodal model. QWen2.5-Omni is specifically instructed to output three distinct, aligned annotations for the clip: the verified speech content, descriptive video caption, and caption for the ambient audio. This online, just-in-time process guarantees that every training instance consists of video and audio latents that are perfectly synchronized in time and semantically consistent with their corresponding textual annotations, thereby eliminating the data misalignment problem inherent in offline methods."
        },
        {
            "title": "3.4 Method",
            "content": "The overall architecture of our method is depicted in Fig. 1. We introduce several targeted modifications to the input stages of the original Wan2.1 and Ace-step models to facilitate bimodal integration and control. For the video component (Wan2.1), we enable conditioning on reference image. During the forward process, the first frame of the noisy video latent is replaced with the corresponding clean latent representation of the provided reference image. For the audio component (Ace-step), we perform two adjustments. First, to ensure temporal alignment with the videos 25 frames-per-second (fps) rate, the input mel spectrograms are processed at 25.6 kHz sampling rate instead of the original 44.1 kHz. Second, to generalize the model beyond speaker-specific generation, we have removed the speaker encoder and its corresponding input from the architecture."
        },
        {
            "title": "3.4.1 Stitching of experts",
            "content": "We introduce novel framework, termed Stitching of experts, for integrating specialized, preexisting models for video and audio synthesis. This approach is designed to preserve the generative capabilities of each unimodal expert while simultaneously enabling fine-grained, bidirectional interaction between them at the level of individual layer blocks. To enhance training efficiency and leverage the powerful priors of these pre-trained models, we apply the stitching technique to the Wan2.1 5 Figure 1: Architecture of UniVerse-1. (a) Overall architecture. The architectural foundation of UniVerse-1 is realized through stitching of experts methodology. This approach deeply integrates the pre-trained Wan2.1 video model and the Ace-step audio model. (b) Fused block. The fusion is implemented at granular, block-by-block level, where each block in the Wan architecture is deeply fused with its corresponding block in the Ace-step architecture. Figure 2: Revised attention of UniVerse-1. (a) Self attention of video branch, with additional mel tokens as input. (b) Cross attention of video branch, with additional mel tokens as input. (c) Cross attention of mel branch, with addition video tokens as input. and Ace-step models at the transformer block level. This process results in unified, dual-stream architecture where each block co-processes information from both the video and audio modalities, functioning akin to Mixture-of-Experts (MoE) layer. As illustrated in Fig 1. b), we facilitate bidirectional cross-modal communication within each block. Specifically, the hidden states from the video stream, following its self-attention module, are injected into the audio streams cross-attention module. Conversely, the hidden states from the audio stream, following its linear attention module, are reciprocally injected into the video streams self-attention and cross-attention module. To ensure consistent feature scaling, the hidden states from both streams are jointly passed through shared LayerNorm layer before injected into video streams attention. Prior to injection, the cross-modal hidden states are passed through two-layer linear adapter for feature space alignment. In the video stream, for instance, features from the audio branch are projected using dedicated key (kproj) and value (vproj) layers 2(a). frame-by-frame cross-attention is then performed with the queries from the video stream to ensure alignment between the video and audio. Within each streams respective cross-attention mechanism(shown in 2(b) and 2(c)), the conditioning signal from the other modality is projected using dedicated key (kproj) and value (vproj) layers. These new key-value pairs are then concatenated with the original text-derived key-value pairs along the context dimension, thereby enriching the conditioning information with cross-modal context."
        },
        {
            "title": "3.4.2 Layer Interpolation",
            "content": "A key challenge in stitching the Wan2.1 and Ace-step models is the architectural mismatch in their depth, as they possess different number of transformer blocks. To reconcile this disparity, we introduce layer interpolation technique. This method involves first calculating the difference in the number of layers. We then strategically insert new blocks at uniform intervals into the shallower of the two models until their depths align. Crucially, the parameters for each new block are initialized by linearly interpolating the weights of its immediately adjacent (bracketing) layers. This initialization strategy effectively bridges the architectural gap while ensuring smooth performance trajectory during training, thereby mitigating the risk of training instability and severe performance oscillations."
        },
        {
            "title": "3.4.3 Training Loss",
            "content": "In addition to the primary Flow Matching objective (Sec. 3.1), we incorporate two additional loss functions. Semantic Alignment Loss For the audio modality, we employ Semantic Similarity Loss (LSSL), technique consistent with Ace-step, to enhance the semantic fidelity of the generated audio. This loss operates by aligning an intermediate feature representation, haudio, extracted from the audio stream of our fused block at specific layer (L = 12 in our configuration). This internal representation is aligned against target features derived from two expert, pre-trained audio models: MERT(Music Encoder Representations from Transformers) [52], which provides general musical representation, hmert, with dimensionality of 1024 Tm (at 75 Hz frame rate). mHuBERT(multilingual HuBERT) [53], which provides speech-centric representation, hmHuBERT, with dimensionality of 768 Th (at 50 Hz frame rate). To compute this loss, the intermediate feature haudio is first processed by two separate projection heads (πMERT and πmHuBERT) and temporally interpolated to match the dimensionality and sequence length of hMERT and hmHuBERT, respectively. The semantic similarity loss is then defined as the negative cosine similarity, encouraging the models internal representations to align with those of the expert models: LSSL = (cosineSim(h audio, MERT) + cosineSim(h audio, mHuBERT)) 1 2 where denotes the temporally aligned representations. Low Quality Data Loss Strategy The AudioSet and VGGSound datasets, while offering rich auditory diversity, are characterized by low visual fidelity. To leverage their strong audio content without corrupting the video generation quality, we employ conditional loss scheme. Specifically, the Flow Matching loss for the video modality (LFM-video) is only computed for samples originating from these two datasets when the diffusion timestep exceeds an empirically determined threshold. We set this threshold to τ = 800 (out of 1000 total timesteps). This strategy is predicated on the principle that at high noise levels (i.e., for > τ ), the model learns to capture coarse, low-frequency 7 features of the video, such as general motion and structure, which are less affected by the poor visual quality. By excluding the loss calculation at lower noise levels, we prevent the model from overfitting to the high-frequency visual artifacts and noise present in these datasets. The loss for video modality is: LFM-video = (cid:26) LFM(x1, x0, t) 0 if x1 Θ or (x1 ζ and > 800) else where x0 p0 is noise sample, x1 p1 is data sample, ζ is data subset include vggSound and audioset, Θ is data subset exclude vggSound and audioset. The final training objective is weighted sum of the flow matching and semantic alignment losses: = LFM-video + LFM-mel + λSSL LSSL where LSSL is hyperparameter controlling the influence of the SSL guidance, empirically set to 1.0 according to Ace-step. 3."
        },
        {
            "title": "Independent Noise Sampling Strategy",
            "content": "Our empirical investigation reveals critical sensitivity of multi-modal diffusion models to the pseudo-random number generation process. When single, fixed random seed is used to initialize training run, the noise tensors for the video (ϵv) and audio (ϵa) modalities are sampled sequentially from the same deterministic PRNG sequence. Due to the deterministic nature of the underlying algorithm (Linear Congruential method), this sequential sampling introduces spurious structural correlation between the two noise tensors, which can be expressed as ϵa = (ϵv). This violates the critical assumption that the noise vectors are statistically independent. The model inadvertently learns this spurious correlation as shortcut during training. Consequently, during inference, any alteration to the sampling of ϵv, such as change in video resolution or duration, propagates through the PRNGs state and alters the structure of the subsequently sampled ϵa. This mismatch with the learned correlation results in significant degradation of the audio generation quality. To address this, we propose an Independent Noise Sampling Strategy. This approach isolates the noise generation for each modality by employing separate and independently seeded PRNG instances. This method effectively breaks the deterministic correlation, ensuring the noise vectors are statistically independent. As result, the model becomes robust to variations in inference-time conditions, mitigating the issue of performance degradation."
        },
        {
            "title": "4.1 Setup",
            "content": "Implementation Details The training is conducted with an effective batch size of 128 over 50k steps on 7, 600-hour audio-visual datasets built by our data curation pipeline, using the AdamW optimizer with learning rate of 5e6. We employ Fully Sharded Data Parallel (FSDP) for distributed training across multiple nodes, with gradient accumulation step of 4. Compared Methods We conduct comprehensive evaluation of our model by benchmarking it against suite of state-of-the-art baselines across several distinct generation tasks: Video Generation: We compare our model against leading text-to-video systems, including Wan2.2 (14B) [8], HunyuanVideo [6], CogVideoX-1.5, (5B) [7], Kling 2.1 [24], and SeeDance 1.0 [25]. Audio Generation: For the audio modality, we benchmark against established text-to-audio models such as Stable Audio Open [29] and AudioLDM2 [31]. Text-to-Speech (TTS): As supplementary evaluation of vocal synthesis, we compare our models performance against specialized TTS models, including CosyVoice [54], CosyVoice2 [55], and VibeVoice [56]. Audio-to-Video methods: We also compare out model with talking-based audio to video methods such as FantastyTalking [11] and Wan-S2V [57]. 8 (a) Statistical results of set1/2. (b) Statistical results of set1. (c) Statistical results of set2. Figure 3: Statistical results of Verse-Bench. Best viewed with zoom-in. larger, high-resolution version is available in Appendix A.1 Video-to-Audio methods: As closely related task to joint audio-visual generation, we also benchmarked our model on the video-to-audio (V2A) task, drawing comparisons with state-of-theart methods such as HunyuanVideo-Foley [16]. Joint Audio-Video Generation: For our core task of synchronous audio-visual synthesis, we compare our method against existing prompt-based joint generation models: SVG [35]. Since methods such as MM-Diffusion [34] and R-FLAV [37] are class-conditional generative models, direct comparison with our approach is not applicable, we only compare with SVG in our report. It is important to note that our model is constructed by stitching the pre-trained WAN2.1 (1.3B) and Step-Ace (3.5B) models. Consequently, the comparisons against the aforementioned state-of-the-art baselines are intended to provide qualitative reference and situate our work, rather than to make direct claim of superior performance. Benchmark To construct our evaluation set, we curated 600 image-text prompt pairs from multitude of sources. These sources encompass frames extracted from YouTube videos, BiliBili videos, TikTok clips, movies, and anime; images generated by AI models [58, 59]; and collection of images from public websites. Our dataset comprises three subsets. Set1-I contains image-text pairs (including AI-generated, web-crawled, and media screenshots), for which video/audio captions and speech content were produced using LLMs [51] and manual annotation, comprising total of 205 samples. Statistical results in 3b. Set2-V consists of video clips from YouTube and Bilibili, which were annotated with LLMgenerated [51] captions and Whisper-based ASR [48] transcripts, followed by human verification, comprising total of 295 samples. Statistical results in 3c. Set3-Ted (the Verse-Ted subset) includes TED Talks from September 2025, processed with the same annotation pipeline as Set2, comprising total of 100 samples. Our collected test data is highly diverse, encompassing wide spectrum of audio categories: human speech; animal vocalizations (e.g., bird chirping, cat meowing); instrumental music (e.g., piano, guitar); natural sounds (e.g., thunder, rain); human-object interactions (e.g., keyboard typing, cooking, chopping vegetables); object-object interactions (e.g., glass shattering, marbles dropping); mechanical sounds (e.g., trains, airplanes), and so on. The complete statistical results of set1 and set2 are shown in 3a. Evaluation Protocol We quantitatively evaluate our method against baselines on the Verse-Bench benchmark. The evaluation is structured across 6 distinct generation tasks, each with tailored set of metrics. Video Generation: Performance is assessed on three criteria: Motion Score (MS): This metric quantifies motion dynamics, calculated from the normalized optical flow magnitude detected by the RAFT model [60]. Aesthetic Score (AS): This is composite score averaging three components: fidelity, measured by MANIQA [61] to penalize blur and artifacts, and aesthetic quality, evaluated by both aesthetic-predictor-v2-5 [62] and Musiq [63]. 9 ID Consistency (ID): To measure identity preservation, we compute the mean DINOV3 [64] feature similarity between the reference image and each generated frame. Audio Generation: We evaluate audio quality from three perspectives: Distributional Similarity: We measure the Fréchet Distance (FD) and Kullback-Leibler (KL) divergence between the generated and real data distributions, using features extracted from PANNs [65] and PaSST [66]. Semantic Consistency: The alignment between the audio and the input text is measured by the LAION-CLAP [67] score. Quality and Diversity: We report the Inception Score (IS) calculated with PANNs classifier. Additionally, we use AudioBox-Aesthetics [68] to assess Production Quality (PQ), Production Complexity (PC), Content Enjoyment (CE), and Content Usefulness (CU). Text-to-Speech (TTS): We evaluate synthesis accuracy using the Word Error Rate (WER), which is derived by transcribing the generated audio with the Whisper-large-v3 model [48]. Audio-to-Video: We evaluate this task using the same criteria as the video generation task, additionally providing SyncNet [50] confidence score to assess lip-sync accuracy. Video-to-Audio: This task use all metrics from audio generation tasks. Furthermore, we introduce the Audio-Video Alignment (AV-A) metric to specifically quantify the temporal synchronization between the generated audio and video streams, which is computed via Synchformer [69]. Joint Audio-Video Generation: For this task, we use all relevant metrics from the individual tasks above. The evaluation of our models and their components is conducted across the three test sets as follows: The video generation models and SVG are evaluated on Set 1 and Set 2. For the audio generation model is evaluated on Set1 and Set2. The Text-to-Speech (TTS) model is primarily evaluated on Set 3. The Audio-to-Video (A2V) model is evaluated exclusively on Set 3, while the Video-to-Audio (V2A) model is evaluated exclusively on Set 2. Finally, our complete Universe-1 model is benchmarked against all three test sets. For audio generation, we evaluate the metrics CE, CU, PC, and PQ on Set 1. On Set 2, the evaluation is expanded to include FD, KL, and CS in addition to the aforementioned metrics. Furthermore, LSE-C is evaluated exclusively on Set 3, while the AV-A metric is also applied to Set 1 when evaluating UniVerse-1 and SVG."
        },
        {
            "title": "4.2 Quantitative Evaluation",
            "content": "We compare Universe-1 against range of state-of-the-art (SOTA) models specialized for specific tasks as shown in Tab. 1. As unified joint generation model, direct comparison of single-modality metrics against these \"expert\" models presents inherent complexities. Nevertheless, our model demonstrates robust capabilities across multiple dimensions. In terms of video quality, Universe-1 achieves the highest score in identity preservation (ID: 0.89), showcasing its superior ability to maintain subject consistency throughout the generation process. For audio quality, while there is gap compared to leading audio-only generation models, our model obtains highly competitive score in pitch correlation (PC: 2.49). The core strength of our model lies in synchronous audio-video generation. It is crucial to interpret the metrics for such joint generation tasks with caution. For instance, in the video-to-audio (V2A) setting, the AV-A metric for model like Hunyuanvideo-Foley is calculated with ground-truth video, whereas our model generates both modalities simultaneously. Therefore, AV-A must be considered in conjunction with the audio-text CLAP score (CS) for holistic assessment. From this integrated perspective, our model (AV-A: 0.23, CS: 0.16) demonstrates better overall audio-visual content consistency than SVG (AV-A: 0.09, CS: 0.08). Similarly, for the lip-sync (LSE-C) metric, our models score (1.34) is evaluated on fully generated audio and video, making it susceptible to the quality of both generated modalities. In contrast, audio-to-video (A2V) methods like Wan-S2V are evaluated using ground-truth audio, which naturally yields higher score (6.49). Despite this evaluation disparity, Universe-1 achieves promising results as the first open-source joint generation framework of its kind, establishing solid foundation for future research. 10 video audio tts Audio-Video method MS AS ID FD KL CS CE CU PC PQ WER LSE-C AV-A video methods audio methods tts methods CogVideox1.5 5B [7] Wan2.2-14B [8] Kling2.1 [24] SeeDance1.0 [25] Stable Audio [29] AudioLdm2 [31] Cosyvoice [54] Cosyvoice2 [55] Vibevoice [56] 0.43 1. 0.31 0.50 0.44 0.83 0.50 0. 0.41 0.85 0.47 0.86 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1. 1.38 0.28 3.88 6.15 2.60 6. 1.21 2.30 0.24 3.98 5.88 3. 6.04 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - a2v methods FantastyTalking [11] Wan-S2V [57] 0.07 0.17 0. 0.87 0.46 0.89 - - - - - - 0.17 0.16 0. - - - - - - - - - - - - - 2.68 6.49 - - - - - - - - - - - - - 0.78 0. 0.23 v2a methods Hunyuanvideo-Foley [16] - - - 0.82 1.27 0.40 4.04 5.72 3. 6.27 joint methods SVG [35]"
        },
        {
            "title": "Ours",
            "content": "0.40 0.20 0.41 0.25 1.55 3. 0.08 2.93 5.50 2.35 6.26 0. 0.89 1.25 2.70 0.16 3.53 4. 2.49 5.20 0.18 1.34 Table 1: Quantitative results compared with baselines on Verse-Bench. method MS AS ID FD KL CS CE CU PC PQ WER LSE-C AV-A w/o LQLS 0.38 0. 0.78 1.26 2.84 0.15 3.38 4. 2.58 4.97 w/o INSS"
        },
        {
            "title": "Ours",
            "content": "1.10 0.20 0.43 0.75 1.43 3. 0.11 2.44 3.14 2.92 3.99 0. 0.89 1.25 2.70 0.16 3.53 4. 2.49 5.20 0.16 0.38 0.18 1. 0.99 1.34 0.28 0.18 0.23 Table 2: Ablation results on Verse-Bench."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We performed an ablation study to investigate the contributions of our Low Quality data Loss Strategy(LQLS) and Independent Noise Sampling Strategy(INSS), as shown in Tab. 2. The findings indicate that LQLS provides improvement accross video quality and consistency ID, thus confirming its efficacy and positive impact on training. Furthermore, the results for INSS demonstrate significant enhancement in audio generation quality, validating the effectiveness of this approach."
        },
        {
            "title": "5 Limitation and Future Work",
            "content": "The work presented herein constitutes an initial exploration into unified audio-video generation. Our study was constrained by computational resources, necessitating that we conduct training exclusively on the Wan2.1-1.3B video model. The performance of our model is, therefore, inherently limited by the capacity of this base model. In the future, our research will focus on two key directions. First, we will scale up our experiments to larger video foundation models. Second, we will engage in more extensive and refined data curation efforts. The ultimate objective is to significantly advance the capabilities of open-source audio-video synthesis models, thereby bridging the performance gap to state-of-the-art proprietary models."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we presented UniVerse-1, novel framework for joint audio-video synthesis, achieved through the deep integration of video foundation model and music generation model using stitching of experts. Following fine-tuning on the dataset we curated, we also introduced Verse-Bench, comprehensive benchmark to foster comparative research. To promote reproducibility and further innovation, our model and code have been made publicly available. Our experimental results validate 11 that this methodology offers viable and efficient pathway for building sophisticated multimodal generative models by leveraging pre-existing unimodal foundations."
        },
        {
            "title": "References",
            "content": "[1] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [2] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:1243812448, 2020. [3] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [4] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [5] OpenAI. Video generation models as world simulators, 2024. [6] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [7] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [8] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [9] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1797917989, 2023. [10] Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, and Baoyuan Wang. Talking head generation with probabilistic audio-to-visual diffusion priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76457655, 2023. [11] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. arXiv preprint arXiv:2504.04842, 2025. [12] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactorm1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724, 2025. [13] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. [14] Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, and Qinglin Lu. Hunyuanvideo-avatar: High-fidelity audio-driven human animation for multiple characters. arXiv preprint arXiv:2505.20156, 2025. [15] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. [16] Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, Qun Yang, Jin Zhou, and Zhao Zhong. Hunyuanvideo-foley: Multimodal diffusion with representation alignment for high-fidelity foley audio generation. arXiv preprint arXiv:2508.16930, 2025. [17] Google DeepMind. Veo 3. https://https://deepmind.google/models/veo/, 2025.5. [18] Junmin Gong, Sean Zhao, Sen Wang, Shengyuan Xu, and Joe Guo. Ace-step: step towards music generation foundation model. arXiv preprint arXiv:2506.00045, 2025. [19] Hamming. Mathematical methods in large-scale computing units. Math Rev, 13(1):495, 1952. [20] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 13 [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. [22] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [23] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [24] Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024.06. [25] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [26] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [27] Google DeepMind. Veo 2. https://deepmind.google/technologies/veo/veo-2/, 2024.12. [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [29] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [30] Seth Forsgren and Hayk Martiros. Riffusion: Stable diffusion for real-time music generation. https://github.com/riffusion/riffusion, 2022. [31] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024. [32] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. AudioLDM: Text-to-audio generation with latent diffusion models. Proceedings of the International Conference on Machine Learning, pages 2145021474, 2023. [33] Ziqian Ning, Huakang Chen, Yuepeng Jiang, Chunbo Hao, Guobin Ma, Shuai Wang, Jixun Yao, and Lei Xie. Diffrhythm: Blazingly fast and embarrassingly simple end-to-end full-length song generation with latent diffusion. arXiv preprint arXiv:2503.01183, 2025. [34] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. [35] Masato Ishii, Akio Hayakawa, Takashi Shibuya, and Yuki Mitsufuji. simple but strong baseline for sounding video generation: Effective adaptation of audio and video diffusion models for joint generation. arXiv preprint arXiv:2409.17550, 2024. [36] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Mmdisco: Multi-modal discriminatorguided cooperative diffusion for joint audio and video generation. arXiv preprint arXiv:2405.17842, 2024. [37] Alex Ergasti, Giuseppe Gabriele Tarollo, Filippo Botti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, and Andrea Prati. R-flav: Rolling flow matching for infinite audio video generation. arXiv preprint arXiv:2503.08307, 2025. [38] Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic video generation. In European Conference on Computer Vision, pages 3450. Springer, 2022. [39] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1340113412, 2021. 14 [40] Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024. [41] Lei Zhao, Linfeng Feng, Dongxu Ge, Rujin Chen, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Uniform: unified multi-task diffusion transformer for audio-video generation. arXiv preprint arXiv:2502.03897, 2025. [42] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. [43] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. [44] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. [45] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. [46] Shijia Liao, Yuxuan Wang, Tianyu Li, Yifan Cheng, Ruoyi Zhang, Rongzhi Zhou, and Yijin Xing. Fishspeech: Leveraging large language models for advanced multilingual text-to-speech synthesis. arXiv preprint arXiv:2411.01156, 2024. [47] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [48] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [49] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 52035212, 2020. [50] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251263. Springer, 2016. [51] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. [52] Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, et al. Mert: Acoustic music understanding model with large-scale self-supervised training. arXiv preprint arXiv:2306.00107, 2023. [53] Marcely Zanon Boito, Vivek Iyer, Nikolaos Lagos, Laurent Besacier, and Ioan Calapodescu. mhubert-147: compact multilingual hubert model. arXiv preprint arXiv:2406.06371, 2024. [54] Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. [55] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. [56] Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, et al. Vibevoice technical report. arXiv preprint arXiv:2508.19205, 2025. [57] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, et al. Wan-s2v: Audio-driven cinematic video generation. arXiv preprint arXiv:2508.18621, 2025. 15 [58] ByteDance. Jimeng ai, 2025. [59] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [60] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. [61] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11911200, 2022. [62] discus0434. aesthetic-predictor-v2-5, 2024. [63] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. [64] Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. [65] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2020. [66] Khaled Koutini, Jan Schlüter, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. [67] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Largescale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [68] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. [69] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 53255329. IEEE, 2024."
        },
        {
            "title": "Appendix",
            "content": "A More Details about Verse-Bench A.1 Statistical Results This section provides the detailed category catalog for Verse-Bench, along with high-resolution pie chart illustrating its statistical distribution in Fig. 4 5 6. Category list is in Tab. 3, 4, 5. Table 3: Detailed Audio Classification Statistics for Set 1 Category / Subcategory Count Percentage (%)"
        },
        {
            "title": "Natural Environment",
            "content": "Weather-Wind Plants-Vegetation Animals-Birds Water-Ocean/Waves Other Natural Subcategories Music & Instruments Musical Compositions Keyboard-Piano String-Guitar Other Music Subcategories"
        },
        {
            "title": "Daily Life",
            "content": "Office-Writing/Typing Tools-Electronics Other Daily Life Subcategories"
        },
        {
            "title": "Human Voices",
            "content": "Adults-Conversation Other Human Voices Subcategories"
        },
        {
            "title": "Transportation",
            "content": "Industrial & Urban"
        },
        {
            "title": "Special Effects",
            "content": "Weapons & Explosions"
        },
        {
            "title": "Grand Total",
            "content": "70 18 12 10 8 22 40 14 7 7 12 22 4 4 14 21 6 15 12 9 7 191 14 205 36.3 20. 11.4 10.9 6.2 5.2 4.7 3. 93.2 6.8 100.0 17 Table 4: Detailed Audio Classification Statistics for Set 2 Category / Subcategory Count Percentage (%)"
        },
        {
            "title": "Natural Environment",
            "content": "Weather-Wind Animals-Birds Weather-Rain Animals-Dogs Other Natural Subcategories Music & Instruments Musical Compositions Keyboard-Piano String-Violin Other Music Subcategories"
        },
        {
            "title": "Daily Life",
            "content": "Office-Writing/Typing Tools-Power Tools Communication-Phone Other Daily Life Subcategories"
        },
        {
            "title": "Human Voices",
            "content": "Adults-Conversation Other Human Voices Subcategories"
        },
        {
            "title": "Transportation",
            "content": "Industrial & Urban Weapons & Explosions"
        },
        {
            "title": "Grand Total",
            "content": "106 22 22 19 9 34 54 16 10 8 20 28 6 6 6 10 16 9 7 10 5 2 230 65 295 35.9 18. 9.5 5.4 3.4 3.1 1.7 0. 78.0 22.0 100.0 18 Table 5: Combined Audio Classification Statistics for Set 1 & Set 2 Category / Subcategory Count Percentage (%)"
        },
        {
            "title": "Natural Environment",
            "content": "Weather-Wind Animals-Birds Weather-Rain Plants-Vegetation Other Natural Subcategories Music & Instruments Musical Compositions Keyboard-Piano String-Guitar String-Violin Other Music Subcategories"
        },
        {
            "title": "Daily Life",
            "content": "Office-Writing/Typing Tools-Power Tools Other Daily Life Subcategories"
        },
        {
            "title": "Human Voices",
            "content": "Adults-Conversation Other Human Voices Subcategories Transportation Vehicles-Cars Industrial & Urban Fire & Combustion Weapons & Explosions"
        },
        {
            "title": "Grand Total",
            "content": "176 40 32 23 15 66 94 30 17 13 12 22 50 10 8 32 37 15 22 22 19 19 12 11 421 79 500 36.1 19. 10.2 7.6 4.5 3.9 2.5 2. 84.2 15.8 100.0 19 Figure 4: Statistical results of set1 and 2. Figure 5: Statistical results of set1. Figure 6: Statistical results of set2."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology(GuangZhou)",
        "Tsinghua University"
    ]
}