{
    "paper_title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering",
    "authors": [
        "Kaisi Guan",
        "Zhengfeng Lai",
        "Yuchong Sun",
        "Peng Zhang",
        "Wei Liu",
        "Kieran Liu",
        "Meng Cao",
        "Ruihua Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answers the generated questions through a multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's correlation coefficient of 58.47, showing a much higher correlation with human judgment than existing metrics which attain only 31.0. We also construct a comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through a systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation."
        },
        {
            "title": "Start",
            "content": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering Kaisi Guan 1,2* Zhengfeng Lai2 Yuchong Sun1 Peng Zhang2 Wei Liu2 Kieran Liu2 Meng Cao2 Ruihua Song1 1 Renmin University of China 2Apple https://eftv-eval.github.io/etva-eval"
        },
        {
            "title": "Abstract",
            "content": "Precisely evaluating semantic alignment between text prompts and generated videos remains challenge in Textto-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, novel Evaluation method of Textto-Video Alignment via fine-grained question generation and answering. First, multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answer the generated questions through multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves Spearmans correlation coefficient of 58.47, showing much higher correlation with human judgment than existing metrics which attain only 31.0. We also construct comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation. All codes and datasets will be publicly available soon. 5 2 0 2 1 2 ] . [ 1 7 6 8 6 1 . 3 0 5 2 : r 1. Introduction Text-to-Video (T2V) generation models [3, 9, 19, 25, 34, 48, 57, 60, 65] have shown remarkable advancements in recent years, showing particular potential in applications such as video content editing [37, 45], movie produc- *The work was done during an internship at Apple. Email: guankaisi@ruc.edu.cn Corresponding authors. Figure 1. Illustration of how ETVA works and comparison with existing metrics. tion [47, 66], and even the world simulator [3, 11, 36]. However, current T2V generation suffers from the absence of reliable automatic metrics for text-to-video alignment. Existing text-to-video alignment metrics fall into three categories: Caption-based metrics like BLIP-BLEU [33], CLIP-based like CLIPScore [42], and MLLM-based like VideoScore [12]. These metrics share common limitation: they only produce coarse-grained scores, lacking the granularity to capture fine-grained alignment details, resulting in significant deviations from human preference. For instance, as illustrated in Figure 1, human annotator recognizes Video 2 as superior in depicting microgravity physics (implied by space station), while existing metrics systematically favor Video 1. Some works in text-to-image alignment [4, 7, 14, 43] solve this problem by constructing Question Generation (QG) and Question Answering (QA) pipeline with the help of Large Language Models (LLMs) [38, 51, 52, 59] and Multimodal Large Language Models (MLLMs) [2830]. However, these initial attempts still have not been transferred to the text-to-video alignment evaluation yet due to the following challenges: C1. Appropriate Questions Challenge. Video text prompts are often lengthy and contain multiple elements, making question generation complicated. Vanilla in-context learning methods, like in TIFA [14], tend to generate overly intricate questions but they pose challenges for video LLMs to give correct answers. For instance, in Figure 1, question like Is the water pouring out of glass cup and is it in the space station? requires simultaneous comprehension of multiple factors, including actions, objects, and environment, which poses significant challenges for Video LLMs. C2. Video LLMs Hallucination Challenge. Video LLMs [24, 38, 55, 64] exhibit more severe hallucination issues than their image-based counterparts. Some questions require additional commonsense knowledge that is naturally accessible to human brain but unavailable to Video LLMs. As shown in Figure 1, when assessing whether water is pouring in space station, humans intuitively recognize the microgravity environment, whereas Video LLMs lack this innate understanding. Moreover, while the human brain engages in deep observation and reasoning to analyze video content, Video LLMs generate responses directly, bypassing the intermediate cognitive processes. To address these challenges, we propose ETVA, novel Evaluation method for Text-to-Video Alignment that contains multi-agent framework for atomic question generation (C1) and knowledge-augmented multi-stage reasoning framework to emulate human-like reasoning in question answering (C2). The QG part of ETVA consists of three collaborative agents: Elements extractor decomposes the text prompt into three categories of elements: entities (key objects), attributes (descriptive properties), and relations (spatial-temporal interactions). Then Graph builder structures these elements into scene graph that covers all semantic requirements for the target video. Finally, Graph traverser traverses each node of the graph to generate questions. In the QA phase, ETVA adopts brain-inspired cognitive architecture to mitigate hallucination through knowledge-augmented multi-stage reasoning. An auxiliary LLM provides commonsense knowledge augmentation (e.g. microgravity in the space station) like the brain memory area and then together with video content and questions, multi-stage reasoning process lets video LLMs think step by step to answer the question. Extensive experiments demonstrate that ETVA achieves Spearmans correlation of 57.8, outperforming existing state-of-the-art metric VideoScore [12] by 27.5. Furthermore, its well-designed QG and QA parts respectively contribute to performance improvements of 14.67% and 26.2%. Based on ETVA, we further construct ETVA-Bench, comprehensive benchmark for text-to-video alignment evaluation. ETVA-Bench comprises 2K diverse prompts and 12K generated questions categorized into 10 distinct types. We evaluate 10 open-source and 5 closed-source T2V models, providing systematic study with in-depth analysis. Our findings reveal that these models still struggle in some areas such as camera movements or physics process. Our contributions can be summarized as follows: We propose ETVA, novel text-video alignment evaluation method with multi-agent framework for question generation and knowledge-augmented multi-stage reasoning framework for question answering. We build ETVABench, benchmark dedicated to textvideo alignment evaluation, providing systematic comparison and analysis of existing 15 T2V models. Extensive experiments on both question generation and question answering validate the effectiveness of our approach, demonstrating superior human alignment compared to existing evaluation metrics. 2. Related Work Text-to-Video Generation Text-to-Video (T2V) models aim to generate videos from textual descriptions. There are three categories of architecture: (1) Some early works [8, 58] employ Generative Adversarial Networks (GAN [10]) or Vector Quantisation Variational Autoencoders (VQVAE [53]), yet these approaches often yield low-quality outputs with limited generalizability. (2) UNet-based Diffusion models, such as Stable Video Diffusion [2], ModelScope [54], and Cogvideo [13] leverage 3D-UNet [67] architectures to effectively model both spatial and temporal information. (3) Diffusion Transformers (DiT) [41]. DiT represents the state-of-the-art (SOTA) backbone for T2V models. range of popular open-source models, including HunyuanVideo [57], CogVideox [60], and OpenSora [65], as well as commercial models such as Pika [48], Vidu [49], Sora [3], MetaMovieGen [47] and Kling [19], have adopted DiT as their core architecture. In this paper, we focus on the DiT-based T2V models. Figure 2. Overall pipeline of ETVA. ETVA contains multi-agent framework for generating atomic questions and knowledge-augmented multi-stage reasoning framework for question answering. Automatic Text-to-Video Alignment Metrics Existing work evaluates text-to-video alignment using three methods: (1) Caption-based metrics, where caption models (e.g. BLIP [20, 21]) generate video caption followed by calculating text-text similarity between caption and prompt using metrics like BLEU [39]; (2) CLIP-based metrics [22, 42, 56], extracting semantic embeddings from text and video and calculating similarity, like ViCLIP [56] in VBench [15]. (3) MLLM-based metrics, which fine-tune [12] Multimodal Language Models (MLLM) on human-annotated dataset to be reward model for scoring, such as VideoScore [12]. However, these methods often lack fine-grained detail and do not correlate well with human judgments. Some textto-image alignment evaluation works [4, 7, 14, 43] use automatic QG & QA framework, achieving better alignment with human preference. However, due to complexities of text-to-video evaluation [23, 31], these methods cannot transfer directly to text-to-video alignment. Benchmarks for T2V Generation Existing T2V generation benchmarks can be categorized into two groups: (1) General benchmarks evaluate the overall performance of the T2V model, including quality, consistency, and aesthetics (e.g., VBench [15], EvalCrafter [33], MetaMovieGen [47], FETV [32]), however, they often overlook fine-grained details. (2) Specific Benchmarks focus on particular aspects of T2V generation, such as GAIA [6] to evaluate human actions, T2V-ComBench [44] for multi-object composition, VideoPhy [1] & PhyGenBench [35] for physics phenomenon, and ChronoMagic-Bench [61] for time-lapse video. In this work, we build specific benchmark for evaluating the text-to-video alignment of T2V models. 3. Our Proposed ETVA Approach The main idea of ETVA is to simulate human annotation process by using LLMs and Video LLMs to automatically generate and answer questions. As illustrated in Figure 2, the framework operates in two stages: (1) Question Generation: multi-agent system generates detailed questions using scene graphs (Section 3.2). (2) Question Answering: An auxiliary LLM first recalls common sense knowledge, then Video LLM analyzes videos and reasons through answers (like human thinking) (Section 3.3). 3.1. Problem Formulation Given text prompt and generated video , our ETVA approach computes an alignment score [0, 1] through the mapping: = ETVA(T, ) (1) Section 3.2 produces set of binary verification questions = {Q1, . . . , Qn} derived from text prompt , where each Qi admits yes/no response: = G(T ) (2) Section 3.3 resolves these questions using three information sources: original text , video content , and generated questions Q. First, common sense knowledge is extracted through language model inference: = LLM(T ) (3) Each binary score Si {0, 1} is then answewred by question Qi with knowledge and video content : Si = A(Qi, K, ) (4) The final alignment score is calculated based on results across all questions: = 1 (cid:88) i=1 Si (5) 3.2. Multi-Agent Question Generation Generating video-relevant questions from text prompts presents fundamental challenge. While conventional approaches employ LLM in-context learning with predefined question categories, they often suffer from semantic redundancy, incomplete coverage, and unanswerable outputs that degrade evaluation quality. Our solution borrows the idea from scene graph for getting atomic elements [16, 18, 50] to structure the question generation process. As illustrated in Figure 2, our framework implements multi-agent framework containing three agents: Element Extractor identifies core textual elements Graph Builder constructs semantic relationships Graph Traverser generates comprehensive questions This collaborative architecture ensures fine-grained questioning with complete semantic coverage through systematic graph exploration. 3.2.1. Element Extractor The Element Extractor identifies key elements in text prompts through three fundamental types: entities (specific objects like cup and space station), attributes (descriptive features like glass material and transparent for water), and relationships (action/positional links like pouring out of and contained within). For instance, in Figure 2 with the prompt Water is slowly pouring out of glass cup in the space station, it detects entities (water, cup, space station), material attributes (glass, transparent), and spatial relationships (pouring-from, contain in). This structured decomposition enables systematic question generation by parsering key elements from textual descriptions. 3.2.2. Graph Builder The Graph Builder constructs hierarchical scene graphs where nodes represent elements identified by the Element Extractor. Adjacent nodes belong to distinct categories, with entity nodes (e.g., objects) serving as central anchors - all relation nodes (actions/spatial links) and attribute nodes (descriptive features) must connect to at least one entity node. As shown in Figure 2, attribute nodes only emit outgoing edges (e.g., glass material), while relation nodes maintain bidirectional connections between entities (e.g., cup pouring water). This structure ensures semantic coherence by enforcing category exclusivity between neighboring nodes and preventing redundant connections through entity-centric topology. 3.2.3. Graph Traverser The Graph Traverser systematically explores scene graphs to generate yes/no questions through three-step process: It first handles main objects (entities), then their attributes (descriptive details), and finally checks connections (relations) between objects but only after both objects in the connection have been processed. As shown in Figure 2, this ordered approach ensures natural question flow by first confirming object properties (e.g., Is the cup made of glass?) before examining interactions (e.g.,Is water pouring from the cup?). The system automatically delays relationship questions until both connected objects have their attributes verified, using this smart order to avoid confusing questions and maintain clear context. This dependencychecking method guarantees that every question builds logically on previously confirmed information. 3.3. Multi-modal Question Answering In this part, we aim to leverage video LLM to answer generated questions. Video-related questions often require both additional contextual knowledge and thorough comparative analysis for accurate answers. Direct video LLM responses often suffer from severe hallucination problems. Inspired by human annotation processes such as evaluating the prompt Water slowly pours from glass cup in the space station where annotators first recall relevant common sense (e.g., liquid behavior in microgravity) before synthesizing knowledge with visual observations, we implement an automated simulation of this cognitive workflow in ETVA-Rater question answering part. As shown in Figure 2, the frameworks knowledge augment component emulates human memory recall, while its multistage reasoning mechanism enables comprehensive analysis. 3.3.1. Knowledge Augmentation with auxiliary LLM In this stage, we leverage an auxiliary LLM to provide common-sense knowledge for video understanding. Large Language Models (LLMs) undergo extensive pretraining, allowing them to internalize knowledge similar to the human brain. By carefully designing prompts, we can guide the LLM to recall relevant knowledge for video comprehension. Here, we employ Qwen2.5-72B-Instruct [59]. The LLM generates detailed video descriptions to enhance video LLM reasoning. For example, given the prompt Water is slowly pouring from glass cup in the space station, it identifies two key physical principles: (1) In microgravity, liquids form floating spheres rather than falling streams. (2) Due to the cohesion force, water should condense together. This explicit knowledge defines expected physical behaviors, facilitating systematic video analysis. 3.3.2. Multi-Stage Reasoning This phase leverages the video LLMs multi-modal chainof-thought (CoT) capabilities through three progressive steps to fully analyse the video and question. Video Understanding Stage: Video LLM autonomously extracts visual patterns by generating frame-by-frame descriptions without text input. General Reflection Stage: Video LLM combines these observations with the question context and commonsense knowledge for cross-modal analysis, identifying key evidence through iterative verification. Conclusion Stage: Video LLM delivers conclusive Yes/No answer supported by explicit visual-linguistic alignment checks, ensuring decisions remain grounded in both the video content and logical reasoning. 4. Our Proposed ETVABench In this section, we introduce ETVABench, an automatic Text-to-Video generation alignment benchmark based on ETVA. We collect prompts from several open-source benchmarks (Section 4.1) and use question-driven classification method to decompose these prompts to 10 distinct categories (Section 4.2). 4.1. Our Prompts Collection and Sources We first build ETVABench-2k which collects 2k text prompts from diverse open-source benchmarks to ensure comprehensive coverage of text-to-video alignment challenges. Specifically, we sample 880 general prompts from established T2V evaluation frameworks (VBench [15], EvalCrafter [33], VideoGen-Eval [62]), 300 compositional prompts from T2V-ComBench [44], and 510 human motion descriptions from GAIA [6]. The dataset further incorporates 160 physics-aware prompts from VideoPhy [1] and PhyGenBench [35], 100 temporal coherence queries from ChronoMagic-Bench [61], supplemented by 50 GPT4-o [38] generated prompts targeting hyper-realistic scenarios. The multiple sources ensures both breadth and depth in evaluating cross-modal alignment capabilities. Since evaluating closed-source models via web access is resource-intensive, we create ETVABench-105 by strategically sampling 105 prompts from the original 2,000-prompt ETVABench-2k. This compact subset preserves the full benchmarks question type distribution while enabling costeffective testing, maintaining evaluation validity through representative prompt selection, as shown in Appendix. 4.2. Prompt Classification based on Question Labels Existing text-to-video evaluation benchmarks typically employ coarse-grained prompt categorization, which proves inadequate for complex prompts containing multiple semantic elements. To address this limitation, we propose an atomic question-based taxonomy derived from scene graph node traversals in Section 3.2. Our classification framework organizes prompts into 10 distinct categories (existence, action, material, spatial, number, shape, color, camera, physics, other) based on their constituent atomic questions. Prompts that generate the same questions are grouped under the same category. ETVABench-2k contains 12K questions, while ETVABench-105 includes 0.6K. Further details, including the specific number of prompts in each category, are provided in the Appendix. 5. Experiment In this section, we first present the experimental setup (Section 5.1). Then we conduct extensive experiments to validate ETVAs superior correlation with human judgment compared to existing metrics and ablation study of each part (Section 5.2). Based on ETVABench, we conduct comprehensive evaluation of text-video alignment across 15 existing T2V models and give an analysis of nowadays models limitation (Section 5.3.2). Finally, we give some case studies of ETVA on existing T2V models (Section 5.4). 5.1. Experiment Setup ETVA Configuration Our framework do not have any training procedures. For question generation, we employ Qwen2.5-72B-instruct [59] as the large language model (LLM). For question answering, we use Qwen2.5-72Binstruct [59] as the auxiliary LLM in Knowledge Augmentation part and use Qwen2-VL-72B [55] as the video LLM in the Multi-stage reasoning part. Model Evaluation We benchmark 10 open-source and text-to-video generation models on 5 closed-source implementing Diffusion Transformer ETVABench, all Metric Existence Action Material Spatial Number Shape Color Camera Physics Other Overall BLIP-ROUGE [33] BLIP-BLEU [33] CLIPScore [42] UMTScore [22] ViCLIPScore [56] VideoScore [12] 6.0/8.4 9.3/12.9 10.2/13.7 17.9/24.0 20.2/27.1 23.2/30. 6.5/8.9 8.4/11.6 10.6/13.6 14.3/19.0 17.9/24.2 22.7/30.2 3.0/5.3 6.2/10.1 9.9/12.8 25.4/34.4 16.2/20.4 29.9/37.3 9.2/12.6 11.2/15.4 12.2/16.1 21.6/28.2 20.6/27.2 24.8/31.7 11.0/15.5 10.0/13.6 10.9/14.8 9.1/13.5 16.9/22.4 26.6/35.9 9.3/14.7 9.2/12.7 14.6/20.8 24.4/32.1 29.4/38.4 28.1/35.7 -4.7/-5.2 6.6/10.5 11.8/18.8 22.5/31.1 13.8/13.5 11.7/16. 3.3/4.9 10.3/14.7 12.9/15.2 22.2/29.0 23.6/31.8 19.2/26.3 5.9/8.8 10.1/14.4 9.7/14.2 18.4/23.2 19.8/26.3 20.3/23.9 5.3/7.1 8.6/11.4 10.2/12.9 15.5/20.6 17.0/22.7 23.9/31.6 6.3/8.8 8.5/12.1 10.3/13.8 17.6/23.5 19.4/25.9 23.7/31.0 ETVA 47.7/57. 38.3/46.6 55.5/66.1 56.0/66.8 44.0/53.9 64.1/75.1 31.5/39. 35.5/44.2 50.6/60.4 49.0/59.2 47.2/58.5 Table 1. Correlations between each evaluation metric and human judgment on text alignment, measured by Kendalls τ (left) and Spearmans ρ (right). The same category denotes groups of prompts that produce the same evaluation questions. Settings Kendalls τ Spearsmans ρ Multi-agent QG 47.16 ( +12.12) Vannila QG 35.04 57.47( +14.67) 42.87 Table 2. Ablation Study of ETVA Question Generation. Both multi-agent and vanilla QG are based on the knowledgeaugmented multi-stage reasoning framework. (DiT) [41] architectures. Evaluations were conducted using default configurations (Details are in Appendix). Open-Source Models: [57], CogVideoX-5B [60], CogVideoX-2B [60], CogVideoX1.5-5B [60], OpenSora-1.2 [65], OpenSora-Plan1.2 [25], OpenSora-Plan-1.1 [25], Mochi-1-Preview [46], Latte [34], and Vchitect-2.0 [9] Hunyuan-Video Closed-Source Models: Sora [3], Kling-1 [19], Kling1.5 [19], Pika-1.5 [48], Vidu-1.5 [49] Benchmark setting For ETVABench-105, we evaluate both open-source and closed-source models. For ETVABench-2k, we only evaluate open-source models due to access limitations that make large-scale generation with closed-source models challenging. 5.2. Evaluation of ETVA and Existing Metrics 5.2.1. Baselines metrics We compare ETVA against three categories of text-to-video alignment metrics: Caption-based metrics: BLIP-2 [21] generated captions evaluated through BLEU [40] and ROUGE [26] similarity measures against original prompts, following the setting of EvalCrafter [33]. CLIP-based metrics: (i) CLIPScore [42] (ViT-B/32) also called CLIPSIM employed in works like Pixeldance [63] and EvalCrafter [33]; (ii) ViCLIPScore [56] using video-text pretrained CLIP [42] following the implementation of VBench [15]; (iii) UMTScore uses CLIP model trained on MSR-VTT [5] via unmasked teacher methods [22], following the setting of FETV [32]. MLLM-based metrics: VideoScore-Qwen2-VL [12], Qwen2-VL-7B-based [55] model fine-tuned for scoring video including text-alignment dimension, demonstrating superior performance to GPT-4o [38]. Setting ETVA w/o. KA w/o. VU w/Only KA Direct answer Accuracy Kendalls τ Spearsmans ρ 89.27 (+25.20) 67.34 82.73 65.48 63. 47.16 (+28.98) 27.34 37.56 24.72 18.18 58.47 (+34.63) 35.54 44.81 33.12 23.84 Table 3. Ablation Study of ETVA Question Answering Part: Component Analysis (KA: Knowledge Augmentation; VU: Video Understanding; CR: Critical Reflection) 5.2.2. Human Annotation Five human annotators were employed to assess 1,575 videos generated by 15 text-to-video (T2V) models on the ETVA-105. Each annotator performed two distinct evaluation tasks: For the first task, annotators assigned finegrained text-alignment scores using standardized 5-point Likert scale (0-5) with 0.5-point increments, where 0 indicates complete misalignment and 5 represents perfect semantic correspondence. The second task required the annotators to respond to binary yes/no questions automatically generated by the ETVA QG part. The majority vote (most common answer) from the five annotators determined the final answer for each question. More details are in the Appendix. 5.2.3. Comparison of T2V Alignment Metrics As shown in Table 1, ETVA achieves superior alignment with human judgment across all semantic categories, attaining 47.16 Kendalls τ and 58.47 Spearmans ρ, establishing new state-of-the-art performance. ViCLIPScore [56] from VBench [15] achieves the best correlation among CLIPbased metrics, likely benefiting from its video-text pretaining. While conventional metrics like VideoScore [12] show competitive performance, they share critical limitation: their coarse-grained scoring mechanisms fail to capture fine-grained details of the video, particularly in color, camera, and physics. In contrast, our method simulates human evaluation processes by verifying atomic video-text relationships through structured question generation and knowledge-augmented reasoning. Model Existence Action Material Spatial Number Shape Color Camera Physics Other Avg Open-Source T2V Models Latte Opensora-plan-1.1 Opensora-plan-1.2 Opensora-1.2 Cogvideox-2B Cogvideox-5B Vchitect-2.0 Cogvideox-1.5-5B Mochi-1-preview Hunyuan-Video Close-Source T2V Models Kling Kling-1.5 Pika-1.5 Sora Vidu-1.5 0.519 0.514 0.524 0.602 0.630 0.644 0.745 0.662 0.736 0. 0.730 0.754 0.801 0.815 0.792 0.504 0.460 0.348 0.606 0.606 0.664 0.650 0.737 0.657 0.693 0.720 0.675 0.752 0.759 0.766 0.521 0.625 0.684 0.604 0.625 0.542 0.667 0.625 0.750 0.646 0.558 0.766 0.729 0.729 0.854 0.444 0.611 0.625 0.574 0.611 0.630 0.593 0.630 0.685 0. 0.637 0.754 0.778 0.870 0.862 0.448 0.552 0.578 0.621 0.828 0.724 0.621 0.759 0.690 0.724 0.755 0.775 0.724 0.690 0.714 0.588 0.583 0.588 0.733 0.613 0.783 0.588 0.500 0.765 0.583 0.647 0.583 0.824 0.333 0.765 0.667 0.588 0.667 0.824 0.833 0.629 0.600 0.591 0.693 0.647 0.833 0.765 0.833 0.529 0.667 0.105 0.316 0.323 0.263 0.421 0.474 0.316 0.368 0.421 0. 0.311 0.383 0.421 0.316 0.421 0.350 0.300 0.300 0.450 0.450 0.500 0.350 0.450 0.300 0.300 0.450 0.500 0.450 0.550 0.600 0.364 0.474 0.470 0.511 0.435 0.527 0.424 0.565 0.561 0.615 0.530 0.620 0.530 0.644 0.545 0.652 0.636 0.673 0.667 0.686 0.645 0.671 0.765 0.707 0.667 0.738 0.697 0.757 0.742 0.761 Table 4. ETVABench-105 evaluation results with 5 close-source T2V models and 10 open-source T2V models. higher score indicates better performance for dimension. Bold stands for the best score. Underline indicates the best score in the open-source models. tions in vanilla QG caused by incomplete semantic parsing, whereas our graph-based method ensures comprehensive coverage of entities, attributes, and relations. Ablation of Question Answering Part The ETVA achieves 89.27% accuracy, surpassing direct Video LLM answers by 26.20%  (Table 3)  . Removing the Knowledge Augment (KA) part causes the steepest performance drop of 21.93% accuracy, highlighting its critical role in combating hallucinations. Video understanding (VU) and critical reflection (CR) contribute 6.5% and 11.2% accuracy gains respectively, proving that structured multimodal reasoning is irreplaceable. Notably, relying solely on KA improves accuracy by only 2.35%, highlighting the essential role of multi-stage reasoning in effectively integrating knowledge for question answering. 5.3. Evaluation of Existing T2V Models 5.3.1. Evaluation on ETVABench-105 Table 4 reveals critical insights through systematic evaluation of 10 open-source and 5 closed-source T2V models. Three key findings emerge: (1) Closed-source models dominate overall performance (Vidu-1.5: 0.761 avg), yet top open-source models like Hunyuan-Video achieve comparable capabilities in static attributes (shape: 0.824 vs Soras 0.765); (2) All models struggle with temporal dynamics, particularly in physics (max 0.600) and camera control (max 0.474), exposing fundamental limitations in spatiotemporal reasoning; (3) Performance gaps widen for dynamic scenarios - while closed-source models excel in action (Vidu-1.5: 0.766) and spatial relations (Sora: 0.870), open-source alternatives show 18-32% relative degradation. Figure 3. Evaluating results of 10 opensource T2V models in ETVABench-2k. 5.2.4. Ablation Study of ETVA Ablation of Question Generation Part Vanilla question generation employs in-context learning, where the LLM generates questions directly based on prompt instructions. Our multi-agent framework improves Kendalls τ by 34.6% and Spearmans ρ by 34.1% over vanilla in-context learning  (Table 2)  , demonstrating that structured scene graph traversal generates more discriminative questions. The performance gap stems from redundant/unanswerable quesFigure 4. Four Cases of using ETVA to evaluate text-to-video alignment in T2V models, covering physics phenomenon, color transition, number accuracy and gestrual semnatic. The results highlight the urgent need for enhanced temporal modeling and commonsense integration in T2V generation. 5.3.2. Evaluation on ETVABench-2k Figure 3 presents the evaluation results of 10 open-source models on ETVABench-2k. The results show high degree of consistency with those obtained on ETVABench-105, indicating stable performance trends across different evaluation scales. Among these models, Hunyuan-Video [57] achieves the best overall performance, surpassing all other models in multiple evaluation dimensions. This suggests that Hunyuan-Video has strong text-to-video generation capabilities. However, despite these advancements, significant challenges remain. In the Physics dimension, all models struggle to accurately simulate real-world physical interactions, often failing to maintain consistency in object motion, forces, and environmental effects. Similarly, in the Camera dimension, many models exhibit difficulties in perspective control, shot composition, and smooth transitions between frames, leading to inconsistencies in visual presentation. These limitations underscore the need for further improvements in modeling real-world dynamics and enhancing visual coherence. Addressing these issues will be crucial for advancing text-to-video generation. Detailed results and analyses are provided in the Appendix. 5.4. Case Study Figure 4 presents several evaluation cases using ETVA to assess the text-to-video alignment of T2V models. These cases illustrate how well different models capture finegrained semantic details, which are often overlooked by In the first case, Sora conventional evaluation metrics. accurately depicts water pouring in space station, correctly reflecting the effects of microgravity (Score: 100), while Kling fails to model these effects realistically, resulting in an inaccurate depiction (Score: 37.5). The second case evaluates color transformation, where Vidu successfully changes leaf from green to red as described (Score: 100), whereas Pika produces an incomplete transition, leaving parts of the leaf unchanged (Score: 50). The third case examines numerical accuracy. Kling correctly generates three owls as specified in the prompt (Score: 100), while Sora incorrectly produces four, demonstrating minor but notable deviation (Score: 88.3). The final case assesses the comprehension of gestural semantics. Hunyuan faithfully executes the thumbs-down gesture as described in the text (Score: 100), while Kling misinterprets the instruction, producing an inverted hands-up gesture instead (Score: 40). 6. Conclusion In this paper, we introduce ETVA, novel evaluation method for text-to-video alignment based on fine-grained question generation and answering. ETVA addresses two key issues in existing evaluation methods: (1) Appropriate Question Challenge We generate atomic and semantically comprehensive questions via multi-agent question generation approach. (2) Video LLMs Hallucination Challenge We design knowledge-augmented, multi-stage reasoning framework for question answering, significantly reducing hallucinations. Based on ETVA, we construct ETVABench, benchmark with 2K diverse prompts and 12K generated questions, categorized into ten distinct types, to evaluate current T2V models. Extensive experiments show that ETVA achieves much higher correlation with human preferences than existing metrics. It also helps identify the key limitations of T2V models, paving the way for the next generation of text-to-video models."
        },
        {
            "title": "References",
            "content": "[1] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation, 2024. 3, 5 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. 1, 2, 6 [4] Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Interleaved scene graph for interPan Zhou, et al. leaved text-and-image generation assessment. arXiv preprint arXiv:2411.17188, 2024. 2, 3 [5] Haoran Chen, Jianmin Li, Simone Frintrop, and Xiaolin Hu. The msr-video to text dataset with clean annotations. Computer Vision and Image Understanding, 225:103581, 2022. 6 [6] Zijian Chen, Wei Sun, Yuan Tian, Jun Jia, Zicheng Zhang, Jiarui Wang, Ru Huang, Xiongkuo Min, Guangtao Zhai, and Wenjun Zhang. Gaia: Rethinking action quality assessment for ai-generated videos. arXiv preprint arXiv:2406.06087, 2024. 3, [7] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi PontTuset, and Su Wang. Davidsonian Scene Graph: Improving Reliability in Fine-Grained Evaluation for Text-to-Image Generation. In ICLR, 2024. 2, 3 [8] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets, 2019. 2 [9] Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. 1, 6, 2 [10] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. 2 [11] David Ha and Jurgen Schmidhuber. World models. Zenodo, 2018. 1 [12] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, Kai Wang, Quy Duc Do, Yuansheng Ni, Bohan Lyu, Yaswanth Narsupalli, Rongqi Fan, Zhiheng Lyu, Yuchen Lin, and Wenhu Chen. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation, 2024. 2, 3, 6 [13] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers, 2022. 2 [14] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering, 2023. 2, 3 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3, 5, 6 [16] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12191228, 2018. [17] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 2 [18] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. 4 [19] Inc Kuaishou. Kling video generation, 2024. 1, 2, 6 [20] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022. 3 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 3, 6 [22] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models, 2024. 3, 6 [23] Mingxiang Liao, Hannan Lu, Xinyu Zhang, Fang Wan, Tianyu Wang, Yuzhong Zhao, Wangmeng Zuo, Qixiang Ye, and Jingdong Wang. Evaluation of text-to-video generation models: dynamics perspective, 2024. 3 [24] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. [25] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 1, 6, 2 [26] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, 2004. Association for Computational Linguistics. 6 [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1 [28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 2 [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [30] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2 [31] Xiao Liu, Xinhao Xiang, Zizhong Li, Yongheng Wang, Zhuoheng Li, Zhuosheng Liu, Weidi Zhang, Weiqi Ye, and Jiawei Zhang. survey of ai-generated video evaluation, 2024. 3 [32] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-tovideo generation, 2023. 3, 6 [33] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models, 2024. 1, 3, 5, 6 [34] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 1, 6, 2 [35] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation, 2024. 3, 5 [36] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation, 2024. [37] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 1 [38] OpenAI. Gpt-4o, 2024. Accessed: 2024-02-14. 2, 5, 6 [39] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine In Proceedings of the 40th Annual Meeting of translation. the Association for Computational Linguistics, pages 311 318, Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. 3 [40] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine In Proceedings of the 40th Annual Meeting of translation. the Association for Computational Linguistics, pages 311 318, Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. 6 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 2, 6, 1 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2, 3, 6 [43] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning textto-image generation with image understanding feedback. arXiv preprint arXiv:2311.17946, 2023. 2, 3 [44] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation, 2024. 3, [45] Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, and Dacheng Tao. Diffusion model-based video editing: survey. arXiv preprint arXiv:2407.07111, 2024. 1 [46] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 6, 1 [47] Meta Movie Gen Team. Movie gen: cast of media foundation models, 2024. 1, 2, 3 [48] Pika Team. Pika, 2024. 1, 2, 6 [49] Vidu Team. vidu, 2024. 2, 6, 1 [50] Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for markov arXiv preprint arXiv:2502.12018, llm test-time scaling. 2025. [51] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [52] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [53] Aaron van den Oord, Oriol Vinyals, and Koray Neural discrete representation learning, Kavukcuoglu. 2018. 2 [54] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report, 2023. [55] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 5, 6 [56] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation, 2024. 3, 6 [57] Qi Tian Weijie Kong. Hunyuanvideo: systematic framework for large video generative models, 2024. 1, 2, 6, 8 [58] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers, 2021. 2 [59] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2, 5 [60] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, [61] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. arXiv preprint arXiv:2406.18522, 2024. 3, 5 [62] Ailing Zeng, Yuhang Yang, Weidong Chen, and Wei Liu. The dawn of video generation: Preliminary explorations with sora-like models, 2024. 5 [63] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. 6 [64] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2 [65] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 1, 2, 6 [66] Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, and Jianlong Fu. Moviefactory: Automatic movie creation from text using large generative models for language and images, 2023. 1 Ozgun icek, Ahmed Abdulkadir, Soeren S. Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: Learning dense volumetric segmentation from sparse annotation, 2016. 2 [67] ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Details on Evaluation Categories 10 into divided categories: prompts were The existence, action, material, spatial, number, shape, color, camera, physics, other. Figure 5 shows that ETVABench-2k and ETVA105 generate questions with similar distributions across these categories, indicating consistent coverage. Table 5 shows question example and prompt example for each category. 8. Detailed results on ETVABench-2k More details of 10 open-source T2V models is in Table 6. 9. Details of Text-to-Video Models Sora Sora [3] is state-of-the-art text-to-video model built upon the DiT [41] architecture. As one of the most advanced and widely discussed video generation models, Sora is developed by OpenAI, though many of its underlying details remain undisclosed. Notably, OpenAI has not released an API for Sora, restricting video generation to browserbased access, which is both resource-intensive and inconvenient. In this study, we adopt 16:9 aspect ratio at 480p resolution to generate 5-second video samples. Vidu Vidu [49], text-to-video (T2V) diffusion model developed by Shengshu Technology, integrates advanced semantic comprehension with dynamic shot composition capabilities to achieve hierarchical video synthesis across resolutions ranging from low-definition to 1080p. Our experimental framework employed Vidu 1.5 to generate 4second video sequences at 720p resolution (16:9 aspect ratio), quantitatively assessing its dual capacity for contextual fidelity and cinematographic control. Pika Pika [48], proprietary video synthesis model developed by Pika Labs, demonstrates versatile capabilities in both video generation and multimodal editing across diverse visual styles. For experimental validation, we utilized the Pika 1.5 implementation to synthesize 5-second video sequences at 16:9 aspect ratio, systematically evaluating its capacity to preserve temporal consistency and stylistic fidelity under standardized conditions. Kling Kling [19] is series of proprietary video generation models developed by Kuaishou.It is built on the Diffusion Transformer (DiT) architecture and has demonstrated exceptional capabilities in generative tasks. For our experiments, we utilized Kling-1.0 and Kling-1.5, both standard models with 16:9 aspect ratio, to generate 5-second video sequences. Hunyuan-Video Hunyuan-Video [57], developed by Tencent, is currently the most advanced open-source T2V model. It features massive 13 billion parameters architecture and is trained using the flow matching [27] method on hierarchically structured, high-fidelity video dataset. For our experiments, we set the resolution to 544966, generating 5-second video consisting of 121 frames with 24 fps. Mochi Mochi [46], text-to-video (T2V) diffusion model developed by Genmo, demonstrates significant advancements in video synthesis through its 10-billion-parameter architecture. Preliminary evaluations indicate exceptional performance in motion fidelity and textual prompt alignment, substantially reducing the quality disparity between proprietary and open-source video generation systems. For experimental validation, we generated 2-second video sequence (61 frames at 24 fps) with 480848 pixel resolution, effectively demonstrating the models temporal coherence and detail preservation capabilities. CogVideox CogVideoX [60] is large-scale open-source T2V model released by Zhipu, available in three versions: CogVideoX-2B, CogVideoX-5B, and CogVideoX-1.5-5B. It incorporates 3D causal VAE and an expert transformer, enabling the generation of coherent, long-duration, and high-action videos. For CogVideoX-2B and CogVideoX5B, we set the frame rate to 8 fps, generating 6-second video with total of 49 frames at resolution of 720480. For CogVideoX-1.5-5B, we used resolution of 1360768 at 16 fps, producing 5-second video with 91 frames. Opensora OpenSora [65] is high-quality DiT-based text-to-video model that introduces the ST-DiT-2 architecture, It supports flexible video generation with varying aspect ratios, resolutions, and durations. The model is trained on combination of images and videos collected from opensource websites, along with labeled self-built dataset. We utilized the officially released OpenSora 1.2 code and model, setting the spatial resolution to 720p and the frame rate to 24 fps, producing 4-second (96-frame) video. Category Existence Action Material Spatial Number Shape Color Camera Physics Other Figure 5. Prompt Category Distribution of ETVABench-2k and ETVABench-105 Question Example Prompt Example Is there penguin in the video? Does the player pass the football? Is the city made of crystals? Does the penguin stand on the left of the cactus? Are there three owls in the video? Is the cloud shaped like hand? Does the mans hair brown? Is the camera pushing in? Is the water pouring out in the space station? Is the video in the Van Gogh style? penguin standing on the left side of cactus in desert. In crucial game moment, player passes the football, dodging opponents. city made entirely of glowing crystals that change colors based on emotions. penguin standing on the left side of cactus in desert. Three owls perch on branch during full moon. cloud shaped like giant hand that picks up people for transportation. Theres person, likely in their mid-twenties, with short brown hair. girl is walking forward, camera push in. Water is slowly pouring out of glass cup in the space station. beautiful coastal beach waves lapping on sand, Van Gogh style. Table 5. Question Example and Prompt Example for Each Category. Opensora-plan OpenSoraPlan [25] is an advanced video generation model built upon Latte [34]. It replaces the Image VAE [17] with Video VAE [25] (CausalVideoVAE), following similar approach to Sora [3]. For OpenSoraPlan v1.1, we used the 65-frame version with spatial resolution of 512512 at 16 fps, generating 4-second video. For OpenSoraPlan v1.2, we selected the 93-frame version with resolution of 720p at 16 fps, producing 5-second video. [9], Vchitect Vchitect an open-source text-to-video (T2V) generative model developed by Shanghai AI Lab, is built upon the Diffusion Transformer (DiT) [41] architecture. With 2 billion parameters, the model demonstrates robust capabilities in generating high-quality video content. For experimental validation, we synthesized 5-second video sequence comprising 40 frames at 8 frames per second (fps), with resolution of 768432 pixels. Latte Latte [34] is an early open-source DiT-based textto-video model, built upon PixArt-Alpha with extended spatiotemporal modules and further training. We employed the officially released LatteT2V code and model, preserving the original parameter settings. For video generation, we used spatial resolution of 512512, frame rate of 8 fps, and duration of 2 seconds (16 frames). 10. Details of Human annotation Figure 6 shows detailed instructions for human annotation. 11. Prompts of ETVA Figure 7 - Figure 11 present the detailed prompts in ETVA. 12. More Cases about ETVA Figure 12 and Figure 13 show the detailed comparision results between ETVA and conventional evaliation metrics. Model Existence Motion Material Spatial Number Shape Color Camera Physics Other Avg Open-Source T2V Models Latte OpenSora-plan-1.1 OpenSora-plan-1.2 OpenSora-1.2 Cogvideox-2B Vchitect-2.0 CogvideoX-5B CogvideoX-1.5-5B Mochi-1-preview Hunyuan 0.505 0.534 0.590 0.666 0.679 0.686 0.700 0.704 0.736 0. 0.504 0.526 0.585 0.685 0.670 0.691 0.700 0.714 0.735 0.756 0.538 0.496 0.576 0.626 0.615 0.635 0.648 0.686 0.666 0.699 0.558 0.568 0.635 0.688 0.713 0.731 0.769 0.716 0.771 0.810 0.495 0.522 0.590 0.637 0.658 0.681 0.710 0.722 0.705 0.747 0.567 0.563 0.504 0.522 0.611 0.595 0.655 0.623 0.655 0.627 0.712 0.652 0.670 0.664 0.670 0.637 0.726 0.656 0.761 0.726 0.536 0.488 0.582 0.666 0.629 0.668 0.671 0.674 0.668 0. 0.490 0.397 0.545 0.526 0.600 0.545 0.584 0.606 0.548 0.632 0.529 0.519 0.568 0.529 0.610 0.601 0.667 0.660 0.691 0.668 0.700 0.682 0.698 0.694 0.727 0.702 0.745 0.720 0.762 0.748 Table 6. ETVABench-2k evaluation results with 10 open-source T2V models and apple video. higher score indicates better performance for dimension. Bold stands for the best score. Figure 6. Instruction for Human Annotation Figure 7. Prompt for Element Extractor. Figure 8. Prompt for Graph Builder. Figure 9. Prompt for Graph Traverser. Figure 10. Prompt for Knowledge Augmentation. Figure 11. Prompt for Multi-stage Reasoning. Figure 12. Illustration of comparative analysis between ETVA and conventional evaluation metrics, based on the text prompt: Water is slowly pouring out of glass cup in the space station. We compare our ETVA score with conventional text-to-video alignment metrics. Figure 13. Illustration of comparative analysis between ETVA and conventional evaluation metrics, based on the text prompt: leaf turns from green to red. We compare our ETVA score with conventional text-to-video alignment metrics."
        }
    ],
    "affiliations": [
        "Apple",
        "Renmin University of China"
    ]
}