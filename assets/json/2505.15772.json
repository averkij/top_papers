{
    "paper_title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling",
    "authors": [
        "Yifan Cheng",
        "Ruoyi Zhang",
        "Jiatong Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss kappa score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning."
        },
        {
            "title": "Start",
            "content": "MIKU-PAL: An Automated and Standardized Multimodal Method for Speech Paralinguistic and Affect Labeling Yifan Cheng1,3, Ruoyi Zhang1,4, Jiatong Shi2 1Fish Audio, Santa Clara, CA, USA 2Carnegie Mellon University, Pittsburgh, PA, USA 3Huazhong University of Science and Technology, Wuhan, Hubei, China 4Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China yf cheng@hust.edu.cn, potato zhang@nuist.edu.cn, jiatongs@cs.cmu.edu 5 2 0 2 1 2 ] . [ 1 2 7 7 5 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Acquiring large-scale emotional speech data with strong consistency remains challenge for speech synthesis. This paper presents MIKU-PAL, fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss κ score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as new benchmark for emotional text-to-speech and visual voice cloning. Index Terms: dataset, controllable TTS speech data annotation, emotional speech 1. Introduction Modern human-computer interaction systems increasingly demand emotional speech synthesis capabilities. However, fundamental bottleneck exists: data scarcity for emotional speech. While current speech language models are usually trained on millions of hours of data [13], emotional speech datasets remain limited. Existing resources like IEMOCAP [4], MELD [5], and MSP-Podcast [6] are manually annotated, resulting in datasets with typically less than 300 hours. Meanwhile, the current speech emotion datasets are constrained to only 5-8 emotion categories, often following Ekmans basic emotion framework [7]. These categories are increasingly being challenged by psychological research [810], which suggests richer and more nuanced spectrum of human emotions far beyond these basic sets. This lack of emotional richness in speech datasets is further emphasized when compared to Natural Language Processing (NLP), where datasets feature up to 27 emotion categories [11]. The core problem is the manual annotation process, burdened by high costs and low consistency. Datasets like EmoDB [12] and RAVDESS [13] using acted emotions and later datasets like IEMOCAP with improvised scenarios, and MELD and MSP-Podcast aiming for naturalness, all ultimately rely on this expensive and inconsistent manual labeling. This limits both the size and the emotional diversity of available datasets, hindering the realization of fine-grained emotion modeling, especially to meet the growing need for downstream applications like emotional text-to-speech (TTS). While recent efFigure 1: The structure overview of MIKU-PAL: It analyzes visual, text, and audio modalities across three stages. forts explore learned emotion representations to bypass direct labels [14, 15], they have to sacrifice explicit user control over synthesized affect. To overcome these limitations, we introduce MIKUPAL (Multimodal Intelligence Kit for Understanding - Paralinguistic and Affect Labeling), novel multimodal framework designed to automate emotion annotation across audio, visual, and text modalities. MIKU-PAL achieves high-consistency emotion judgments (Fleiss κ 0.93) with flexible emotion categories. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we expand emotion categories to 26, drawing from psychological research [10], validated through human rationality ratings, and downstream TTS applications. With low cost and high efficiency, MIKU-PAL achieves 63.6% accuracy with released labels on IEMOCAP and MELD. The system also achieved typical human annotator accuracy1 on the datasets. With MIKU-PAL, we created MIKU-EmoBench (131.2 hours, 26 emotions). It outperforms the original emotional speech dataset in variety of metrics, as shown in Table 2. In summary, our main contributions are as follows: We develop MIKU-PAL, the first multimodal automated emotion labeling system, capable of automatically annotating high-quality, flexible, and consistent emotion labels at low cost. Based on psychological research and TTS performance, we investigated standardized speech emotion classification system consisting of 26 categories. Release an open-source multimodal emotional speech dataset called MIKU-EmoBench with the new classification, to serve as new benchmark for emotional speech tasks. 2. MIKU-PAL In this section, we introduce the detailed design of MIKU-PAL. First, we discuss the general pipeline in Sec. 2.1. Then we discuss the design of emotion categories in Sec. 2.2. In Sec. 2.3, we present thorough evaluation of the data by checking the annotation quality with human annotators. 1This was obtained by analyzing the raw annotation data from each annotator in the IEMOCAP dataset. Figure 2: Overview of the MLLM chat. The system prompt is based on three parts: mission description, textual description of emotions, and output structure. The user prompt only contains raw video and text. Example output presents representative example of the system output. 2.1. Automatic Pipeline Our pipeline comprises three stages as shown in Fig. 1: audio preprocessing, vision preprocessing, and emotion analysis. MIKU-PAL is modularized pipeline where each component can be separately upgraded to improve the performance of the whole framework. Audio Preprocessing. Raw audio in multimodal emotion analysis often contains background music and environmental noise, degrading emotional speech segment quality. To purify audio, our pipeline uses MDX-Net [16] as Music Source Separation (MSS) model to extract vocals. MDX-Net is dual-stream architecture designed to isolate clean vocal signals from complex audio mixtures. It was chosen for its state-of-the-art performance in MSS. [17]. Analysis of 30 Friends episodes shows 36% improvement in Signal-to-Noise Ratio (SNR) after separating speech and non-speech segments using WebRTC Voice Activity Detection (VAD). Subsequently, to obtain transcriptions for temporal alignment, we utilized the Whisper-large v3 model [18]. It provides timestamps for vision preprocessing and transcriptions for subsequent MLLM emotion analysis. Vision Preprocessing. Audio-processed speech segments proceed to face detection, employing S3FD [19] or DSFD [20] based on computational needs. S3FDs efficient, scale-invariant architecture excels in multi-scale face detection, while DSFDs deeper network offers higher accuracy, especially with pose variations or occlusions. In our task, speakers are typically centered and prominent in our analyzed video frames, thus prioritizing processing speed over maximal accuracy. For effective audio-visual fusion, active speaker identification is crucial. MIKU-PAL integrates TalkNet [21] for this task, Figure 3: Mixed emotion analysis on 10,000 YouTube video segments using MIKU-PAL. Annotation results are reduced to two dimensions using t-SNE. Each data point is labeled according to the emotion category with the highest intensity and colored using weighted interpolation based on all emotion categories present. The visualization demonstrates MIKU-PALs ability to model the continuous human emotion space and the gradient relationships between emotion categories. leveraging its 92% accuracy on the AVA Active Speaker Detection challenge, ensuring robust performance. Accurate speaker identification allows precise attribution of facial and vocal emotion features, enhancing MLLM-based multimodal analysis. In addition, we conducted an ablation study to assess the impact of visual preprocessing on emotion analysis. Evaluation on the MELD test set showed 25.6% improvement in annotation accuracy when comparing results with and without visual preprocessing. This highlights visual informations significant role in MIKU-PAL. MLLM Emotion Analysis. MLLMs have shown promise in emotion analysis [22, 23]. Based on this, MIKU-PAL employs Gemini 2.0 Flash [24] for emotion analysis using facial, audio, and textual features. Our prompt design is illustrated in Fig. 2. The system prompt comprises three key components: task description, textual emotion description, and an output structure. The output structure is designed to guide the MLLM in assessing emotion characteristics across different modalities and ultimately providing rationale in natural language for its judgment. The user prompt contains only raw video and text. This zero-shot approach allows MIKU-PAL to conduct flexible emotion classification with consistent criteria. 2.2. Emotion Categories Anger, disgust, fear, joy, sadness, and surprise, as basic emotion categories, have been extensively utilized in emotional speech datasets, such as EmoDB [12], RAVDESS [13], and MELD [5]. These categories have seen widespread adoption in Speech Emotion Recognition (SER) tasks [25, 26]. However, recent research indicates that these limited emotion categories do not meet the growing needs of various downstream tasks, such as emotional TTS [27, 28]. Contemporary psychological research, exemplified by Cowen et al.s work [10] proposes Table 1: The performance of MIKU-PAL compared to typical human annotators. Dataset Method IEMOCAP MELD Human MIKU-PAL Human MIKU-PAL Acc (%) 72.9 58.6 N/A 68.5 Fleiss κ Cost (/hour) Relative MOS 0.40 0.93 0.43 0.95 10$ <50 10$ <50 +0.07 +0. domly collected YouTube videos. This means that processing 100 hours of raw videos yields 42 hours of final audio. We have implemented parallel processing for the entire pipeline. GPU processing speed remains the primary bottleneck. MIKU-PAL exhibits near-lossless performance under multi-threading parallelism and allows for the independent specification of parallelism for each stage, ensuring efficient utilization of computational resources. Utilizing the latest Google Gemini 2.0 Flash model, the cost of generating 1 hour of emotional speech dataset data using MIKU-PAL is approximately 50 US cents. This is substantially lower than the cost associated with manual annotation. Accuracy. We validated the accuracy of our pipeline on the IEMOCAP and MELD datasets. The results indicate that we achieved an overall accuracy of approximately 65%. As depicted in the confusion matrix in Fig. 4, wrong classifications are notably frequent between frustration and neutral emotions. Interestingly, these two emotion categories are not recognized as distinct basic emotions within established psychological emotion classifications; rather, they are often considered to be encompassed within other broader emotional categories. When these two emotions are excluded, MIKU-PALs accuracy reaches approximately 75%, which surpasses the average accuracy of human annotators. This result further substantiates the effectiveness of our pipeline and highlights potential limitations in current emotion classification schemes. Confusion matrix analysis reveals classification overlap between excited and happy within the IEMOCAP dataset. This confusion mainly arises because IEMOCAP assigns one label as final when emotions receive equal annotation counts. Consistency. To evaluate the annotation consistency of MIKUPAL, we calculated the Fleiss Kappa score [31] on the IEMOCAP and MELD datasets. The Fleiss Kappa score is statistical measure used to assess the agreement among multiple annotators, with higher values indicating greater consistency in annotation. Across five independent annotation experiments, we maintained consistent prompt and model configuration. MIKU-PAL achieved Fleiss Kappa scores of 0.93 on IEMOCAP and 0.95 on MELD. The result indicates very high level of annotation consistency significantly surpassing the consistency levels typically attained by human annotators. Emotional TTS Performance. To validate the effectiveness of MIKU-PAL for the targeted emotional TTS task, we reannotated IEMOCAP and MELD datasets with MIKU-PAL. And used the re-annotated dataset with special emotion tokens to fine-tune Fish-Speech [30]. We then compared the relative Mean Opinion Score (MOS) of this fine-tuned model against baseline model fine-tuned on the original datasets. The results indicate that the fine-tuned model achieved significant MOS score (+0.08) improvement compared to the baseline. Furthermore, both models demonstrated effectiveness in explicit emotion control within TTS. This outcome substantiates the validity of MIKU-PAL annotated data for emotional TTS tasks. Figure 4: Confusion matrix of MELD (left) and IEMOCAP (right). It demonstrates good performance in emotions that have been psychologically validated. more comprehensive model: human emotions exist as complex mixtures within continuous space defined by 27 discrete categories.2 We construct our pipelines emotion categories upon this richer, psychologically validated framework, aiming for more nuanced and expressive emotion representation. To validate MIKU-PALs capability to capture these 26 emotion categories, we conducted mixed emotion annotation experiment on 10,000 randomly collected YouTube video segments. The annotation process was consistent with the standard MIKU-PAL pipeline. The key difference was the incorporation of Cowens emotion categories paper as an additional prompt. All emotion annotation results were then reduced to two dimensions using t-distributed stochastic neighbor embedding (t-SNE) and visualized in Fig. 3. This map reveals the trajectory and distribution of human emotions, illustrating transitions such as from admiration to love, and from joy to satisfied to excitement. These observed patterns are largely consistent with the conclusions of the original psychology research paper, demonstrating the rationality of MIKU-PAL in capturing this expanded emotion categorization. Meanwhile, to investigate whether MIKU-PALs annotations align with human perceptions, we conducted human rationality annotation experiment. We recruited 5 annotators without relevant background to evaluate 1000 balanced samples across emotion categories, assessing the reasonableness of MIKU-PALs annotations. The results showed that human annotators considered 83% of MIKU-PALs annotations to be reliable. 2.3. Performance To comprehensively evaluate the effectiveness of MIKU-PAL, we conducted series of experiments. Specifically, our evaluation focuses on the following key aspects: engineering performance, accuracy, consistency, and emotional TTS performance. The core performance metrics compared to typical human annotators are shown in Table 1. Engineering. From an engineering perspective, MIKU-PAL demonstrates significant advantages in terms of both processing speed and cost-effectiveness. We evaluated the pipelines performance on standard workstation equipped with 8 NVIDIA RTX 4090 GPUs. MIKU-PAL achieves processing speed ratio of approximately 1:12 on 720p 30fps video data. Furthermore, it maintains retention rate of 42% when processing ran2These categories include: Admiration, Adoration, Aesthetic, Amusement, Anger, Anxiety, Awe, Awkwardness, Boredom, Calmness, Confusion, Craving, Disgust, Empathic pain, Entrancement, Excitement, Fear, Horror, Interest, Joy, Romance/Love, Nostalgia, Relief, Sadness, Satisfaction, Surprise. Due to ethical considerations, one category was removed from the original set. Table 2: Multimodal Emotion Dataset Characteristics Comparison Dataset Visual Domain Duration (h) Segments (k) Emotion Categories Labeling Method Updates IEMOCAP [4] EMO-DB [12] MELD [5] CMU-MOSEI [29] MSP-podcast [6] MIKU-EmoBench Y Scripted and spontaneous dialogs Studio recorded TV show YouTube opinion videos Podcast"
        },
        {
            "title": "Almost all domains on Youtube",
            "content": "12.0 0.5 13.7 65.9 237.9 131.2 10.0 0.5 13.7 23.4 151.6 66.0 9 7 7 6 7 Human Preconfigured Human Human Human"
        },
        {
            "title": "Automatic",
            "content": "N Y Table 3: Characteristics Comparison of Emotional Speech Datasets with MIKU-EmoBench Model Fish-Speech [30] IEMOCAP-ft Fish-Speech MELD-ft Fish-Speech MSP-Podcast-ft Fish-Speech CosyVoice [1] MIKU-EmoBench-ft Fish-Speech WER(%) SS MOS Emotion Similarity 2.4 2.6 2.5 2.4 2.6 2. 0.762 0.780 0.775 0.785 0.802 0.792 3.91 4.01 4.00 4.09 3.97 4.12 0.88 0.89 0.89 0.91 0.87 0. 3. MIKU-EmoBench Addressing the critical limitations of existing emotional datasets in data scale and emotion granularity for nextgeneration emotional TTS, we developed MIKU-EmoBench3, novel dataset collected from YouTube videos using our MIKUPAL pipeline. MIKU-PALs inherent efficiency directly enabled the rapid collection of 131.2 hours of data within single week, collection speed that significantly surpasses any existing emotional speech dataset. Furthermore, leveraging MIKUPALs fine-grained emotion analysis capabilities, MIKU-PAL has 26 psychologically proven emotion categories better suited for the nuanced demands of next-generation emotional TTS systems. As Table 2 shows, MIKU-EmoBench outperforms existing datasets across multiple metrics. Meanwhile, the annotation files will be made publicly available and continuously updated to facilitate downstream emotional speech tasks. 3.1. Data Information Our MIKU-EmoBench dataset comprises 131.2 hours of emotion-labeled audio, segmented into 65,970 utterances with an average duration of 7.16 seconds (min. 2s). To ensure diversity, MIKU-EmoBench incorporates audio from various scenes (e.g., interviews, movies, daily conversations), countries and regions (e.g., USA, Europe, Asia) and races (e.g., Caucasian, Asian, African descent). This rich diversity, sourced from open YouTube videos, provides broad representation of emotional speech. Annotations cover 26 mixed emotions, each with intensity scores and textual rationales, offering detailed and nuanced emotional information. 3.2. Baseline experiments For baseline evaluation and to best demonstrate MIKUEmoBenchs utility, we concentrated our experiments on Emotional TTS. This focus is primarily driven by two factors: first, MIKU-PAL and MIKU-EmoBench are specifically designed for emotional TTS; second, current Speech Emotion Recognition (SER) models are not efficient in discerning the 26 finegrained emotion categories within our dataset. We selected Fish-Speech and CosyVoice as our baseline models. We chose Fish-Speech due to its state-of-the-art (SOTA) performance [32] and open-source availability. We performed incremental fine-tuning of Fish-Speech using emotional speech datasets, treating emotion labels as special tokens, resulting in fine-tuned model capable of emotion control via special tokens. CosyVoice, inherently capable of emotion description using natural language, serves as representative model employing latent variable emotion control. Table 3 shows the performance metrics, We fine-tuned Fish-Speech with emotion special tokens using IEMOCAP, MELD (train set), MSP-Podcast, and MIKUEmoBench, and uniformly tested them using the MELD test set4. including WER, Speaker Similarity (measured by VERSA [33]), humanannotated MOS, and emotion similarity (computed using FunASR emotion vectors [26]). Results demonstrate that MIKUEmoBench fine-tuning improved both MOS and emotion similarity while maintaining TTS quality. This demonstrates the effectiveness of MIKU-PAL and MIKU-EmoBench in this task. 4. Conclusion In this paper, we present MIKU-PAL, novel, automated, and flexible multimodal pipeline for emotional speech annotation. MIKU-PAL efficiently and cost-effectively collects emotion datasets with high consistency and human-level accuracy, addressing field bottleneck. Furthermore, we used MIKU-PAL to develop large-scale emotion dataset MIKU-EmoBench. This 131-hour YouTube dataset, annotated with 26 fine-grained emotions, showcases MIKU-PALs potential for generating rich emotion datasets previously infeasible with manual methods. While acknowledging model-dependent performance and potential biases from YouTube data, future work will enhance MIKU-PALs accuracy, robustness, and adaptability. 3Dataset avaliable at https://huggingface.co/datasets/WhaleDolphin/MIKU4Converting MELDs emotional categories to MIKU-EmoBenchs EmoBench available emotional categories for generation. 5. Acknowledgement Experiments of this work used the Bridges2 system at PSC through allocations CIS210014 and IRI120008P from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, supported by National Science Foundation grants #2138259,#tel:2138286, #tel:2138307, #tel:2137603, and #tel:2138296. 6. References [1] Z. Du, Y. Wang, Q. Chen, X. Shi, X. Lv et al., Cosyvoice 2: Scalable streaming speech synthesis with large language models, arXiv preprint arXiv:2412.10117, 2024. [2] P. Anastassiou, J. Chen, J. Chen, Y. Chen, Z. Chen et al., Seedtts: family of high-quality versatile speech generation models, arXiv preprint arXiv:2406.02430, 2024. [3] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., Neural codec language models are zero-shot text to speech synthesizers, arXiv preprint arXiv:2301.02111, 2023. [4] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, Iemocap: Interactive emotional dyadic motion capture database, Language resources and evaluation, vol. 42, pp. 335359, 2008. [5] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea, Meld: multimodal multi-party dataset for emotion recognition in conversations, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 527536. [6] R. Lotfian and C. Busso, Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings, IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471483, 2017. [7] P. Ekman, Facial expressions of emotion: an old controversy and new findings, Philosophical transactions of the royal society of London. Series B: Biological Sciences, vol. 335, no. 1273, pp. 6369, 1992. [8] J. Sabini and M. Silver, Ekmans basic emotions: Why not love and jealousy? Cognition & Emotion, vol. 19, no. 5, pp. 693712, 2005. [9] L. F. Barrett, Are emotions natural kinds? Perspectives on psychological science, vol. 1, no. 1, pp. 2858, 2006. [10] A. S. Cowen and D. Keltner, Self-report captures 27 distinct categories of emotion bridged by continuous gradients, Proceedings of the national academy of sciences, vol. 114, no. 38, pp. E7900 E7909, 2017. [11] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade, and S. Ravi, Goemotions: dataset of fine-grained emotions, arXiv preprint arXiv:2005.00547, 2020. [12] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss et al., database of german emotional speech. in Interspeech, vol. 5, 2005, pp. 15171520. [13] S. R. Livingstone and F. A. Russo, The ryerson audio-visual database of emotional speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english, PloS one, vol. 13, no. 5, p. e0196391, 2018. [14] H. Wu, X. Wang, S. E. Eskimez, M. Thakker, D. Tompkins, C.-H. Tsai, C. Li, Z. Xiao, S. Zhao, J. Li et al., Laugh now cry later: Controlling time-varying emotional states of flowmatching-based zero-shot text-to-speech, in 2024 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2024, pp. 690 697. [15] H. Tang, X. Zhang, N. Cheng, J. Xiao, and J. Wang, Ed-tts: Multi-scale emotion modeling using cross-domain emotion diarization for emotional speech synthesis, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 12 14612 150. [16] M. Kim, W. Choi, J. Chung, D. Lee, and S. Jung, Kuielab-mdxnet: two-stream neural network for music demixing, arXiv preprint arXiv:2111.12203, 2021. [17] R. Solovyev, A. Stempkovskiy, and T. Habruseva, Benchmarks and leaderboards for sound demixing tasks, arXiv preprint arXiv:2305.07489, 2023. [18] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in International conference on machine learning. PMLR, 2023, pp. 28 49228 518. [19] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li, S3fd: Single shot scale-invariant face detector, in Proceedings of the IEEE international conference on computer vision, 2017, pp. 192201. [20] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, and F. Huang, Dsfd: dual shot face detector, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 50605069. [21] R. Tao, Z. Pan, R. K. Das, X. Qian, M. Z. Shou, and H. Li, Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection, in Proceedings of the 29th ACM international conference on multimedia, 2021, pp. 3927 3935. [22] Z. Cheng, Z.-Q. Cheng, J.-Y. He, J. Sun, K. Wang, Y. Lin, Z. Lian, X. Peng, and A. Hauptmann, Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning, arXiv preprint arXiv:2406.11161, 2024. [23] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang, M. Ning, and L. Yuan, Moe-llava: Mixture of experts for large visionlanguage models, arXiv preprint arXiv:2401.15947, 2024. [24] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [25] D. Kounadis-Bastian, O. Schrufer, A. Derington, H. Wierstorf, F. Eyben, F. Burkhardt, and B. Schuller, Wav2small: Distilling wav2vec2 to 72k parameters for low-resource speech emotion recognition, arXiv e-prints, pp. arXiv2408, 2024. [26] Z. Ma, Z. Zheng, J. Ye, J. Li, Z. Gao, S. Zhang, and X. Chen, emotion2vec: Self-supervised pre-training for speech emotion representation, arXiv preprint arXiv:2312.15185, 2023. [27] D.-H. Cho, H.-S. Oh, S.-B. Kim, and S.-W. Lee, Emosphere++: Emotion-controllable zero-shot text-to-speech via emotionadaptive spherical vector, arXiv preprint arXiv:2411.02625, 2024. [28] K. Kuligowska, P. Kisielewicz, and A. Włodarz, Speech synthesis systems: disadvantages and limitations, Int Res Eng Technol (UAE), vol. 7, pp. 234239, 2018. [29] A. B. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency, Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 22362246. [30] S. Liao, Y. Wang, T. Li, Y. Cheng, R. Zhang, R. Zhou, and Y. Xing, Fish-speech: Leveraging large language models for advanced multilingual text-to-speech synthesis, arXiv preprint arXiv:2411.01156, 2024. [31] J. L. Fleiss, Measuring nominal scale agreement among many raters. Psychological bulletin, vol. 76, no. 5, p. 378, 1971. [32] mrfakename, V. Srivastav, C. Fourrier, L. Pouget, Y. Lacombe, main, and S. Gandhi, Text to speech arena, https://huggingface. co/spaces/TTS-AGI/TTS-Arena, 2024. [33] J. Shi, H.-j. Shim, J. Tian et al., Versa: versatile evaluation toolkit for speech, audio, and music, arXiv preprint arXiv:2412.17667, 2024."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University, Pittsburgh, PA, USA",
        "Fish Audio, Santa Clara, CA, USA",
        "Huazhong University of Science and Technology, Wuhan, Hubei, China",
        "Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China"
    ]
}