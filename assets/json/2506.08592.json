{
    "paper_title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings",
    "authors": [
        "Liyan Xu",
        "Zhenlin Su",
        "Mo Yu",
        "Jiangnan Li",
        "Fandong Meng",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval."
        },
        {
            "title": "Start",
            "content": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings Liyan Xu1 Zhenlin Su2 Mo Yu1 Jiangnan Li1 Fandong Meng1 Jie Zhou1 1Pattern Recognition Center, WeChat AI 2South China University of Technology {liyanlxu,moyumyu}@tencent.com 5 2 0 2 0 1 ] . [ 1 2 9 5 8 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https: //github.com/lxucs/CapRetrieval."
        },
        {
            "title": "Introduction",
            "content": "Dense Passage Retrieval (DPR) is crucial component in search applications, characterized by the dual-encoder paradigm (Reimers and Gurevych, 2019): queries and candidate passages are independently encoded to text embeddings that capture their semantics, and the system retrieves top candidates based on their embedding similarity. For large language models especially, it serves pivotal role in Retrieval Augmented Generation (RAG) (Lewis et al., 2020). The training of such encoders has become an active direction in representation learning (Ramesh Kashyap et al., 2024). However, despite their growing capabilities encoding complex queries and documents, embedding matching can still fail on simple cases, and yet unable to fully supersede the conventional lexicalbased methods such as BM25 (Arabzadeh et al., 2021; Ren et al., 2023). Consequently, retrieval 1 systems in practice usually take hybrid approach for optimized performance (Kulkarni et al., 2023; Luo et al., 2023; Chen et al., 2024). In this work, we probe into such limitation of DPR encoders1, and show that even for simple passages, the embeddings may often lack fine-grained view of encoded semantics and the integration of world knowledge, resulting in failed retrieval on entities or events. To remedy this problem, we delve into data generation strategies training for enhanced encoders, and we identify granularity dilemma within this process, posing challenge to be resolved for general training data composition. Illustrated by Table 7&8, with strong encoders of both BGE (Xiao et al., 2024) and GTE (Zhang et al., 2024) series, spanning from 0.1B to 7B models that rank top on the MTEB leaderboard2 for Chinese, all their embeddings fail to rank the more relevant passages against obviously less relevant ones for the query 炸鸡 (fried chicken) and 紫 色的花 (purple flower) respectively, indicating that such phenomena are universal regardless of training sources and model sizes. To facilitate the analysis of such behaviors and potential improvements, we first construct new evaluation dataset in Chinese tailored to practical image search scenario, where the candidate passages are image captions, and queries are short phrases of entities or events reflected in captions. The dataset overall comprises seemingly simple queries and captions, dubbed CapRetrieval, featuring two unique aspects. First, the retrieval naturally requires fine-grained semantic encoding of passages, inquiring entities or events reflected by the captions in any forms. Second, distinguished from prior datasets related to captions, e.g. Ecom/Video 1We focus on single-embedding DPR models in this work. Multi-embedding models, e.g. ColBERT (Khattab and Zaharia, 2020; Santhanam et al., 2022) and hybrid methods, e.g. M3 (Chen et al., 2024) are outside the scope of this study. 2https://huggingface.co/spaces/mteb/leaderboard Retrieval (Long et al., 2022), which are collected from real-world user clicks but may heavily contain false negatives, our dataset provides explicit annotations for each query-passage pair that enable more reliable and in-depth analysis, constituted by near 1.3 million pair labels. Upon our zero-shot evaluation on CapRetrieval using open-source encoders of different sizes, all of them are revealed certain flaws performing these fine-grained matching, highlighting the limitations of current embeddings. We then proceed to finetune the encoder with our proposed data generation strategies in Sec. 4, where we leverage LLMs for automatic query generation as training data, yielding full keywords/phrases of passages to enforce the encoding of precise salience within semantics. Empirical results show that our finetuned 0.1B encoder outperforms all baselines on CapRetrieval, surpassing 7B encoders, validating that the data generation strategies are effective strengthening more subtle semantic matching. Nevertheless, we also identify dilemma upon further analysis: with the introduced keywords added into training, the model captures better granular saliency, but may lose grasp on the overall critical semantics in bigger picture, resulting in degradation on certain scenarios. We describe it as the granularity dilemma to handle both coarse and fine granularity (Sec. 4), showing that it still remains challenge to obtain general embeddings of full semantic view."
        },
        {
            "title": "2 Dataset: CapRetrieval",
            "content": "Our dataset CapRetrieval is constructed for typical retrieval evaluation towards practical image search scenario, composed of three following parts. Passages pool of image captions are prepared as candidate passages, derived by three steps. (1) set of images are collected voluntarily from phone albums of researchers of this work, covering diverse types including photographs, web pictures, and screenshots of various apps or articles. (2) Each image is transcribed into short description (caption) accordingly by prompting GPT4o. (3) Each caption is manually reviewed and anonymized to ensure privacy compliance. The retrieval only concerns captions, thus the original images are effectively discarded afterwards. Queries User queries are short phrases that may be searched to retrieve relevant images, collected by two rounds. (1) The first round of queries are Records Min Max Avg Passages Queries 3024 8 1 75 16 30.90 3.78 Table 1: Basic statistics of CapRetrieval dataset: number of records, and min/max/avg number of tokens per passage and query. The full annotations of all 1.3M query-passage pairs comprise 4,683 positive pairs. brainstormed by researchers of this work that align with real-world usage scenarios. (2) combination of queries in the first round is performed, then manually revised to form more complex queries, constituting the second round. Among these queries, the easiest ones can be resolved by lexical matching, while others require fine-grained semantic understanding and world knowledge to resolve. Labels Following the convention of typical retrieval datasets in MTEB, the label is relevance score of [0, 1, 2] for pair of query and passage, signaling no/weak/strong relevance. Unlike prior datasets with large-scale candidate pools that prevent annotations for all query-passage pairs, our controlled setting enables full annotation for all pairs by researchers of this work, minimizing false negatives for reliable and in-depth evaluation. Strong Relevance (2): the query is certain or almost certain directly reflected in the caption according to commonsense. For example, the evening sky filled with dense clouds, with sunlight streaming through them is labeled 2 to the query sunset. Weak Relevance (1): the query is likely but not certainly reflected in the caption, or the query is not directly reflected but indeed highly related. For example, floor plan of 2B2B apartment of approximately 90.89 square meters does not directly mention the action real estate purchase, but is labeled 1 due to high relatedness by search intents. No Relevance (0): those who do not meet up with the strong or weak relevance. Statistics Table 1 depicts the basic statistics of CapRetrieval. For analysis, we roughly categorize all queries in CapRetrieval into eight types, of which four types (object, person, place, concept) together can be regarded as Singleton Entity. Both Singleton Entity and Singleton Event are relatively straightforward phrases inquiring entities/events without imposing extra conditions, whereas other query types involve more complex constraints and 2 nDCG@ nDCG@5 nDCG@10 nDCG@10 E>B E<B E=B BM25 BGE GTE E5 BGE E5 Conan-v1 0.1B 0.3B 0.6B Qwen3 > 1B GTE-1.5B GTE-7B E5-7B Qwen3-8B 74.40 81.30 82.49 80. 83.42 82.76 78.78 85.41 81.70 89.12 77.59 87.00 69.30 78.97 80.48 77.31 78.94 81.17 77. 81.14 77.20 86.94 76.02 84.95 66.54 78.86 79.67 76.33 79.15 81.01 77.04 81. 77.35 86.55 76.40 84.61 Table 2: Evaluation results of zero-shot experiments on CapRetrieval, with encoders of different model sizes. semantics. More details and statistics on query types and labels are described in Appx. B."
        },
        {
            "title": "3 Zero-Shot Evaluation",
            "content": "To examine the performance of off-the-shelf encoders on CapRetrieval, we evaluate five popular open-source encoder series for Chinese on Huggingface, offering different model sizes:. BGE3 (Xiao et al., 2024): 0.1B / 0.3B GTE4 (Zhang et al., 2024): 0.1B / 1.5B / 7B E55 (Wang et al., 2024a,b): 0.1B / 0.3B / 7B Conan-v16 (Li et al., 2024): 0.3B Qwen37 (Yang et al., 2025): 0.6B / 8B Our experimental settings comply with the retrieval protocol in MTEB (Muennighoff et al., 2023), adopting nDCG@10 as the main evaluation metric. Additionally, since full labels of all pairs are annotated for CapRetrieval, we also provide nDCG@1/5 for more precise evaluation. For each model, we follow the recommended usage by the publishers instructions. Performance of BM25 is also provided as baseline for reference. More implementation details are described in Appx. C. Results Table 2 shows the results of zero-shot evaluation. As most queries and captions in CapRetrieval do not possess complex semantics, all models are able to achieve decent scores as expected, with nDCG@10 above 76. The best performance is obtained by GTE-7B with 86.55 nDCG score. Several observations can be further made: 3bge-{base,large}-zh-v1.5 4gte-multilingual-base; gte-Qwen2-{1.5B,7B}-instruct 5multilingual-e5-{base,large}; e5-mistral-7B-instruct 6Conan-embedding-v1 (v2 has not been released yet) 7Qwen3-Embedding-{0.6B,8B} Singleton Entity Singleton Event Conjunction Simple Cond. Complex Cond. 82.05 73.21 80.60 73.80 77. 28% 40% 32% 50% 25% 25% 38% 38% 25% 58% 20% 22% 73% 7% 20% Table 3: Zero-shot performance of BGE 0.1B encoder per query type. The right part depicts the comparison with BM25: the ratio of queries when Embeddings obtain higher/lower/similar (>/</=) scores than/to BM25. All encoders exhibit flaws matching these fine-grained queries, even the capable 7B model. Though, BM25 still falls short of all encoders already by at least 10%, underscoring that lexical matching is not able to resolve this task. Model size is not the principal factor. Within the same GTE series, the much smaller 0.1B model even outperforms the 1.5B model, and falls behind the 7B by 7%, despite the huge size difference. Query Analysis Table 3 shows the decomposed performance of CapRetrieval by query types (described in B.1). Singleton Entity has the highest nDCG score as 82.05, while Singleton Event and Simple Condition have low scores as 73.2 and 73.8 respectively. The overall trend suggests that the encoder performs worse on more abstract queries, i.e. events or phrases with conditions. Entity-centric queries are relatively easier to resolve (80+ nDCG), though not by large margin. Embedding vs. BM25 The right section of Table 3 presents comparison between embedding and BM25. BM25 exhibits more polarized performance, where it outperforms embedding on entitycentric queries, but lags behind on more abstract queries. The limitation of BM25 is especially pronounced for Complex Condition, with 66% gap, highlighting the necessity of using embeddingbased retrieval. Overall, it also reveals room for improvement in embeddings as follows. False Negatives We identify three error types as the common shortcomings of current embeddings. Direct miss: embeddings may miss entities or events reflected directly in passages, indicating that the current embedding lacks full view of semantics, despite queries and passages are relatively short already. Errors can be further divided into two types within this scope. i) Literal Error: embeddings fail to retrieve passages that contain the full or partial query terms 3 verbatim - passages that BM25 can successfully recall. Though these cases can be remedied by adding lexical search in practice, we advocate that it is important for embeddings to express full information view. Resolving these seemingly simple matches is still challenging for state-of-the-art encoders, which can be regarded as the embedding version of the needle in haystack test for LLMs. ii) Semantic Error: the query is reflected by paraphrasing or in more abstract way in passages, where embeddings generally outperform BM25 but still have room for improvement. Taxonomy knowledge: certain scenarios require taxonomy involved, e.g. when inquiring hypernym term such as household appliances or seafood, or to recognize the matching between Cantonese-style roasted meats and char siu. Commonsense reasoning: some matches require commonsense reasoning to resolve, for instance, the encoder needs to know the color of lavender to correctly handle the query purple flower, or to realize the mention of sitting in passage is highly relevant to the query chair. False Positives We further summarize two error types that appear common in false positives. Over-generalization: the passage contains relevant elements or shared tokens as the query, but does not reflect the actual query itself. For instance, the query shoe retrieves labeled cardboard box with an anti-trample symbol before more relevant captions that actually mention shoes; subway ranks captions regarding high-speed train ticket before the ones mentioning subway stations. Ignoring subjects or conditions: the passage only addresses partial semantics but fails to accommodate full conditions. For instance, purple flower retrieves captions about purple butterflies; shopping cart screenshot retrieves screenshot but of ride-hailing app. As this kind of errors are quite common for queries with conditions, it suggests that current embeddings may not actually encode concepts, but in way more towards superficial matching."
        },
        {
            "title": "4 Encoder Training",
            "content": "The results of zero-shot evaluation on CapRetrieval calls for more expressive embeddings that capture fine-grained semantics and world knowledge integration. We proceed further examination by finetuning encoders with sets of small-scale experiments. CapRetr EcomRetr VideoRetr BGE SM KW SM+KW SM KW SM+KW 78. 84.74 87.23 86.46 84.61 88.57 91.83 OOD ID 64.55 63.26 60.49 60. 62.45 58.26 60.24 69.91 68.69 63.82 64.89 67.47 61.58 65.16 Table 4: nDCG@10 on Cap/Ecom/Video-Retrieval. The same BGE encoder is continuously trained by using SM or KW or both as training queries, on the according OOD or ID corpus respectively (see Sec. 4.1 for acronyms). 4.1 Training Data Generation Training pairs in large-scale retrieval resources such as mMARCO (Bonifacio et al., 2022) and DuReader (Qiu et al., 2022) mainly comprise user search queries and clicks. Consequently, for passage, the queries associated with given passage in the training set are often coarse-grained, such that they do not address the full semantic content. Motivated towards fine-grained semantic matching, we propose automatic query generation for enhanced training, with distinct granularity as follows. Overall SumMaries (SM): we ask LLMs to generate summaries and long questions regarding passage, focusing on the overall saliency. Salient KeyWords (KW): given passage, we ask LLMs to generate all salient keywords and hypernyms, as well as short phrases that may be inquired by users, focusing on precise saliency. For passages, we prepare two different settings: Out-of-domain (OOD): we sample 20,000 passages from existing resources such as DuReader, mostly consisting of web articles and titles. In-domain (ID): we collect more image captions as the training passage pool. To mitigate memorization, we filter out all captions of ROUGE-L > 0.6 w.r.t. any test captions in CapRetrieval. Table 5 illustrates training queries on an in-domain passage by the data generation strategies. Experimental Settings We finetune BGE 0.1B as the backbone encoder and use CLS token for embeddings. The training follows the typical InfoNCE contrastive loss (Chen et al., 2020) with in-batch negatives. Training set statistics and more implementation details are provided in Appx. D. For evaluation, we also include EcomRetrieval and VideoRetrieval (Long et al., 2022) from MTEB, of which the passages are product/video titles, fea4 Passage: 图片显示了上海市电力公司的月度账单包括2021年5月至9月的用电费用和支付状态 (The image shows the monthly bill from the Shanghai Electric Power Company, including electricity charges and payment status from May to September 2021.) KW (keywords/phrases) Bill Electricity fee Utility bill Payment status Monthly electricity charges Utility payment screenshot 2021 electricity fee SM (summaries/queries) Shanghai Electric Power Company bill inquiry Electricity usage details from May to September 2021 Monthly electricity bill details Electricity payment status records Historical payment information from Shanghai Electric Power Company Shanghai Electric Power Company electricity bill from May to September 2021 ... Table 5: Example of training queries on an in-domain passage, generated by our data generation strategies (Sec. 4.1). KW strengthens the full view on precise keywords and concepts, while SM focuses on overall semantic saliency. turing similar lengths as image captions. As our experiments are conducted to examine the effect of training query granularity, we do not aim for other datasets nor general SOTA performance. 4.2 Granularity Dilemma Table 4 shows the results of the six training settings. For CapRetrieval, the encoder trained with both SM and KW on in-domain passages achieves state-ofthe-art performance, surpassing the original BGE significantly by 13%, and outperforms the best baseline GTE-7B by 5+%. For Ecom and Video, encoders trained with SM are shown comparable with BGE, indicating that summary-based queries share similar characteristics to existing large-scale training sets. For CapRetrieval, models trained on in-domain corpus outperform those on OOD, while OOD models generalize better on Ecom and Video. Coarse vs. Fine: we roughly regard summaries and questions as coarse-grained queries that grasp important text semantics, resembling the existing training paradigm of most open-source encoders. Keywords/phrases on the other hand, are deemed fine-grained to strengthen the full semantic view of precise entities or concepts. However, Table 4 suggests that while keywords drive substantial enhancement towards the entity and event retrieval, as shown by the clear improvement on CapRetrieval in both OOD and ID settings, they appear contributing little to Ecom and Video. Upon further analysis, we identified that though training with keywords could facilitate fine-grained matching, it may overlook the overall saliency. For example, the query in VideoRetrieval 荒野独居 第2季中文版 (Alone Season 2 Chinese Edition) should retrieve the TV show Alone with the specific requirement; whereas the encoder trained with KW may overemphasize terms on Season 2 or Chinese, but fails to correctly prioritize the supposedly most critical concept Alone, showing misalignment of semantic importance. We refer to this observation as the granularity dilemma. We hypothesize the main reason is that, the semantic importance of those precise keywords in passage is relative, e.g. Season 2 is arguably less significant when accompanied with Alone. The current KW setting strengthens precise matching locally, but lacks training signals of fine-grained importance among keywords. This issue is not as severe for current open-source encoders, as their training queries comprise large-scale real-world queries that reflect user intents, which can implicitly curate importance through user clicking. However, for LLM-generated queries, it requires more engineering efforts to combat this issue. We reckon that further analysis on training dynamics and data composition are needed to resolve this dilemma."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we focus on the granularity issue of embeddings, stemming from the observation that encoders can fail on entity or event matching of even simple cases. new evaluation set in Chinese is introduced to probe the limitation of embeddings, and zero-shot experiments reveal the room for improvement on these fine-grained matching. To remedy this problem, we investigate automatic data generation strategies utilizing LLMs, aiming for strengthening the full semantic view of embeddings through additional training. As our trained encoder of 0.1B surpasses the best baseline of 7B, we further identify the granularity dilemma within this process, which calls for future efforts on the granularity issue towards expressive embeddings."
        },
        {
            "title": "Limitations",
            "content": "As this work focuses around the granularity problem of embeddings, and examines both the zeroshot evaluation and training strategies, there can be limitations regarding the following two aspects. First, the conducted analysis and training involve single-embedding encoders but exclude other paradigms, such as ColBERT (Khattab and Zaharia, 2020; Santhanam et al., 2022) or multi-views (Zhang et al., 2022). Addressing the dynamics beyond single embeddings can be important for an enlarged scope on this topic. Second, it is still an open question on how to fully resolve the granularity dilemma. As discussed in Sec. 4.2, we do provide our hypothesis on the keyword relative importance, and we leave the further investigation on training data composition outside the scope of this work."
        },
        {
            "title": "Ethical Considerations",
            "content": "For the dataset introduced in this work, we have manually reviewed each case to ensure compliance with privacy and ethical standards, in accordance with ACL ethics guidelines. All passages and queries do not contain sensitive or biased content related to diversity or political viewpoints. Personally identifiable information was anonymized, except in cases involving public figures where such information is part of the public domain. No external annotators were recruited or employed during the dataset creation process; all annotation and verification were conducted internally by in-house researchers of this work (two males with the native language as Chinese). The data preparation and annotation process is approved and audited by the in-house research department. All passages are derived from image transcriptions generated from GPT-4o. As such, there are no visible risks, copyright or legal concerns in using this dataset. ChatGPT from OpenAI is used to assist the writing of this paper."
        },
        {
            "title": "References",
            "content": "Negar Arabzadeh, Xinyi Yan, and Charles L. A. Clarke. 2021. Predicting efficiency/effectiveness trade-offs for dense vs. sparse retrieval strategy selection. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM 21, page 28622866, New York, NY, USA. Association for Computing Machinery. Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto Lotufo, and Rodrigo Nogueira. 2022. mmarco: multilingual version of the ms marco passage ranking dataset. Preprint, arXiv:2108.13897. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfIn Findings of the Assoknowledge distillation. ciation for Computational Linguistics: ACL 2024, pages 23182335, Bangkok, Thailand. Association for Computational Linguistics. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Aina Garí Soler and Marianna Apidianaki. 2021. Lets play mono-poly: BERT can reveal words polysemy level and partitionability into senses. Transactions of the Association for Computational Linguistics, 9:825 844. Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. DuReader: Chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 3746, Melbourne, Australia. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research. Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 6 20, page 3948, New York, NY, USA. Association for Computing Machinery. Hrishikesh Kulkarni, Sean MacAvaney, Nazli Goharian, and Ophir Frieder. 2023. Lexically-accelerated In Proceedings of the 46th Interdense retrieval. national ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 23, page 152162, New York, NY, USA. Association for Computing Machinery. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Curran Associates, Inc. Haoran Li, Mingshi Xu, and Yangqiu Song. 2023a. Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1402214040, Toronto, Canada. Association for Computational Linguistics. Shiyu Li, Yang Tang, Shizhe Chen, and Xi Chen. 2024. Conan-embedding: General text embedding with more and better negative samples. Preprint, arXiv:2408.15710. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023b. Towards general text embeddings with multi-stage contrastive learning. Preprint, arXiv:2308.03281. Dingkun Long, Qiong Gao, Kuan Zou, Guangwei Xu, Pengjun Xie, Ruijie Guo, Jian Xu, Guanjun Jiang, Luxi Xing, and Ping Yang. 2022. Multi-cpr: multi domain chinese dataset for passage retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 22, page 30463056, New York, NY, USA. Association for Computing Machinery. Man Luo, Shashank Jain, Anchit Gupta, Arash Einolghozati, Barlas Oguz, Debojeet Chatterjee, Xilun Chen, Chitta Baral, and Peyman Heidari. 2023. study on the efficiency and generalization of light hybrid retrievers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 16171626, Toronto, Canada. Association for Computational Linguistics. John Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander Rush. 2023. Text embeddings reveal (almost) as much as text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1244812460, Singapore. Association for Computational Linguistics. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia. Association for Computational Linguistics. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Text and code emHsu, and 6 others. 2022. Preprint, beddings by contrastive pre-training. arXiv:2201.10005. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2017. MS MARCO: human-generated MAchine reading COmprehension dataset. Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, QiaoQiao She, Jing Liu, Hua Wu, and Haifeng Wang. 2022. DuReader-retrieval: large-scale Chinese benchmark for passage retrieval from web search engine. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 53265338, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Abhinav Ramesh Kashyap, Thanh-Tung Nguyen, Viktor Schlegel, Stefan Winkler, See-Kiong Ng, and Soujanya Poria. 2024. comprehensive survey of sentence representations: From the BERT epoch to the CHATGPT era and beyond. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17381751, St. Julians, Malta. Association for Computational Linguistics. Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda Viegas, Andy Coenen, Adam Pearce, and Been Kim. 2019. Visualizing and measuring the geometry of bert. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Ruiyang Ren, Yingqi Qu, Jing Liu, Xin Zhao, Qifei Wu, Yuchen Ding, Hua Wu, Haifeng Wang, and JiRong Wen. 2023. thorough examination on zeroshot dense retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1578315796, Singapore. Association for Computational Linguistics. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. Colretrieval via BERTv2: Effective and efficient 7 lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 37153734, Seattle, United States. Association for Computational Linguistics. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024a. Text embeddings by weakly-supervised contrastive pre-training. Preprint, arXiv:2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024b. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1189711916, Bangkok, Thailand. Association for Computational Linguistics. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 641649, New York, NY, USA. Association for Computing Machinery. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. 2022. Multi-view document representation learning for open-domain dense retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 59906000, Dublin, Ireland. Association for Computational Linguistics. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. 2024. mGTE: Generalized longcontext text representation and reranking models for multilingual text retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 13931412, Miami, Florida, US. Association for Computational Linguistics."
        },
        {
            "title": "A Related Work",
            "content": "Embedding-based text representation has been fundamental direction in representation learning since the advent of BERT-style encoding (Devlin et al., 2019), and plays critical role in modern NLP applications including various retrieval tasks. Early works such as S-BERT (Reimers and Gurevych, 2019), SimCSE (Gao et al., 2021) and Contriever (Izacard et al., 2022) establish the effective training paradigm of contrastive learning on unsupervised or weakly supervised text pairs. Current state-of-the-art encoders usually adopt multi-stage training that consists of both unsupervised and supervised finetuning stages (Neelakantan et al., 2022; Wang et al., 2024a; Li et al., 2023b; Xiao et al., 2024). Among the popular supervised training resources, most of the datasets are collected through real-world user behaviors, such as MSMARCO (Nguyen et al., 2017) and DuReader (He et al., 2018; Qiu et al., 2022). Recently, synthetic data generation by LLMs is also reported positive gains in encoder training (Li et al., 2024; Wang et al., 2024b; Yang et al., 2025). Beyond the conventional single-embedding encoders, other paradigms have been proposed for retrieval, such as ColBERT with token-level embeddings (Khattab and Zaharia, 2020; Santhanam et al., 2022), hybrid encoders with lexical features (Kulkarni et al., 2023; Luo et al., 2023) and sparse features (Chen et al., 2024). The analysis on embedding dynamics and characteristics is also an active research direction, such as the embedding geometry (Reif et al., 2019; Garí Soler and Apidianaki, 2021) and embedding inversion (Li et al., 2023a; Morris et al., 2023). As far as our knowledge, we are the first to focus on the in-depth analysis on the embedding granularity problem, with newly introduced dataset and controlled experiments on the encoder evaluation and training data strategies."
        },
        {
            "title": "B Dataset",
            "content": "The dataset is publicly released under the Apache 2.0 License. B.1 Query Types Singleton Person: person-related queries, e.g. 男性 (male), 学生 (student). Singleton Place: place-related queries, e.g. 健 身房 (gym), 沙漠 (desert). Figure 1: Query types in CapRetrieval (details in B.1). Singleton Object: other concrete entities, e.g. 食物 (food), 聊天记录 (chat history). Singleton Concept: non-concrete concepts, e.g. 音乐 (music), 股票 (stocks). Singleton Event: event/action-related queries, e.g. 婚礼 (wedding), 演唱会 (concert). Conjunction: conjuncted entities, e.g. 烧烤 加啤酒 (BBQ and beer), 樱花和传统建筑 (cherry blossoms and traditional building). Simple Condition: entities/events with simple conditions, e.g. 演唱会相关群聊 (group chat regarding concerts), 睡觉的婴儿 (a sleeping baby). Complex Condition: entities/events with more complex conditions, e.g. 一个人在田地里收 割白菜 (a person harvesting cabbages in the field), 在沙发上的白猫 (a white cat next to sofa). The distribution of query types is shown in Fig. 1. B.2 Label Distribution As mentioned in Sec. 2, full labels are annotated for each query-passage pair in CapRetrieval, resulting in total number of 1.3 million pair labels, comprising 4,683 positive pairs. The distribution of positive passages per query is provided in Fig. 2. Queries with the most number of relevant captions are general queries such as 男性 (male), 女性 (female), 食物 (food). Distinguished from typical retrieval datasets, we allow queries with no positive passages in CapRetrieval. There are 27 such queries out of the total 404 queries, and they are excluded for rankingbased metrics, i.e. nDCG scores in Table 2 does not consider these queries. However, they can be helpful examining encoder performance in classification setting or adversarial analysis, which is supported by CapRetrieval, since all query-passage pairs are annotated. 9 Passages Tokens Queries Length OOD ID SM KW SM KW 20K 20K 40K 40K 2.8M 2.8M 1.5M 1.5M 7.0 13. 7.0 12.2 13.7 6.0 11.6 4.0 Table 6: Statistics of the training set settings described in Sec. 4.1: number of passages, total tokens of passages, averaged number of generated queries per passage, averaged number of tokens per query."
        },
        {
            "title": "D Encoder Training",
            "content": "Table 6 shows the statistics of the training set settings described in Sec. 4.1, and 5% queries are randomly sampled as the holdout set. We continuously train bge-base-zh-v1.5 on single Nvidia GPU with the typical InfoNCE contrastive loss, learning rate 5 106, weight decay 0.1, temperature 0.01. The number of epochs and batch size is adjusted to derive around 4K training steps. The evaluation results in Table 4 demonstrate the effectiveness of our proposed data generation strategies on strengthening the fine-grained embedding matching, surpassing the baseline using either in-domain corpus or out-of-domain passages. Through cross-examination, we further raise the granularity dilemma discussed in Sec. 4.2. 8https://github.com/fxsjy/jieba 10 Figure 2: Histogram of the number of relevant captions (labels of [1, 2]) per query. B.3 Annotation Procedure Two in-house researchers of this work participate in the annotation process; no external annotators are recruited. The whole procedure is conducted by double annotation workflow. 1. For each caption, each annotator independently assigns its labels for all the queries, such that each query-passage pair receives annotations from two annotators. 2. All label conflicts are collected and discussed by two annotators. The final label is either determined after reaching an agreement, or selected by the highest relevance received. 3. round of correction is conducted by examining the retrieval results of preliminary experiments. Labels may be corrected after undergoing the same discussions by annotators. The first two steps of the annotation process took around 96 working hours in total, and the third step took an additional 30 hours. The final annotator agreement across relevant pairs is 95.7%. Zero-Shot Evaluation For embedding-based retrieval, all experiments follows the same setting: all text is firstly converted to lower case; all embeddings are normalized such that cosine similarity is the metric for retrieval. For the GTE-1.5B/7B, E5-7B and Qwen3 models, we use the instruction Given an image search query, retrieve relevant image captions in obtaining the query embedding for CapRetrieval. For BM25, we use Jieba8 for Chinese word segmentation, and use rank-bm25 for the BM25 python implementation. Query 炸鸡 (fried chicken) 紫色的花 (purple flower) Passages Label Similarity 一桌丰盛的餐点包括烤肉串炸薯条和春卷 (A table full of delicious dishes includes grilled meat skewers, French fries, and spring rolls.) 图片展示了麦当劳麦辣鸡翅2块20次券的电子优惠券售价185.3元 单份低至7.9元 (The image shows digital coupon for 20 servings of McDonalds Spicy Chicken Wings (2 pieces), priced at 185.3 RMB, bringing the cost as low as 7.9 RMB per serving.) 一辆紫色轿车停在路边车顶和车窗上装饰有花束车前挡风玻璃上有红色标签 (A purple sedan is parked by the roadside, decorated with flower bouquets on the roof and windows, and red tag on the front windshield.) 图片中有四只紫色的蝴蝶背景为浅紫色 (The image features four purple butterflies against light purple background.) 一辆白色轿车停在树下背景是紫色花田和远处的山脉 (A white sedan is parked under tree, with purple flower field and distant mountains in the background.) 图片展示了一片薰衣草田背景是蓝天白云文字内容为只要你欢乐 我就幸福浓浓朋友愿你欢乐无忧早上好 (The image shows lavender field with background of blue sky and white clouds. The text reads: As long as youre happy, my heart is full of joy. My friend, may you be cheerful and carefree. Good morning.) 0 2 0 0 2 0.48 0.38 0.57 0.57 0. 0.37 Table 7: Examples on dense retrieval selected from the zero-shot experiments on our new evaluation set CapRetrieval. Passages in Red are labeled irrelevant to queries (label 0), and passages in Green are relevant (label 2). For both queries, encoders retrieve irrelevant passages before the more relevant ones, despite all queries and passages are straightforward to comprehend. The cosine similarity is provided rightmost using the popular open-source encoder for Chinese bge-large-zh-v1.5. However, it should be noted that all encoders from both the popular BGE and GTE encoder series fail on the above examples, spanning from 0.1B to even 7B models. Overall, encoders can exhibit flaws on fine-grained embedding matching even on simple cases, regardless of training sources and model sizes. Query 西瓜 watermelon Passages Label Similarity 图片中有一个装满水果的篮子旁边有生菜猕猴桃和小番茄 一辆装满西瓜的三轮车停在商店门口 In the picture, there is basket full of fruit, with lettuce, kiwis, and cherry tomatoes next to it. tricycle loaded with watermelons is parked in front of the store. 0 2 0 2 0.50 0. 0.60 0.55 Table 8: more extreme example to illustrate the embedding granularity problem. For the query 西瓜 (watermelon) and passages in both Chinese and English accordingly, both the popular BGE large encoders bge-large-zh/en-v1.5 fail to retrieve the obviously more relevant passage (label 2) before the irrelevant one (label 0). Though this case can be simply resolved by lexical matching, it demonstrates that the embedding granularity problem exists across languages. In this work, we focus on the regarding evaluation in Chinese."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI",
        "South China University of Technology"
    ]
}