{
    "paper_title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
    "authors": [
        "Zilong Huang",
        "Jun He",
        "Xiaobin Huang",
        "Ziyi Xiong",
        "Yang Luo",
        "Junyan Ye",
        "Weijia Li",
        "Yiping Chen",
        "Ting Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our project page: https://longhz140516.github.io/MajutsuCity/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 2 5 1 4 0 2 . 1 1 5 2 : r MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts Zilong Huang1, * Jun He1, * Xiaobin Huang1 Ziyi Xiong1 Yang Luo1 Junyan Ye1 Weijia Li1 Yiping Chen1, 1 Sun Yat-sen University * Equal Contribution Corresponding Authors Ting Han1, Figure 1. MajutsuCity is languagedriven, aesthetic-adaptive system that unifies controllable urban scene generation and interactive editing within single framework. Conditioned on textual instructions, the framework synthesizes complete stylized city through layoutheight creation, asset instantiation, and terrain/material generation, and further enables iterative refinement through five atomic editing operations. This paradigm forms the core contribution of MajutsuCity, empowering users to create and continuously modify large-scale, stylistically diverse urban scenes through natural language."
        },
        {
            "title": "Abstract",
            "content": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, finegrained, and controllability. However, existing methods struggle to balance the creative flexibility offered by textbased generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, natural languagedriven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents city as composition of controllable layouts, assets, and materials, and operates through four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent that supports five objectlevel operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, high-quality multimodal dataset containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by clear margin. These results confirm MajutsuCity as new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our project page: https://longhz140516.github.io/MajutsuCity/ 1. Introduction The rapid progress of world models and cross-modal generative foundation models has significantly advanced 3D generation at both the object-level [4, 27, 42, 50] and the scenelevel [2, 3, 19, 20, 46, 48]. It is crucial for robotics simulation [12, 25, 45], virtual reality [22, 34, 43], and digital content creation [16, 28, 41]. City-scale 3D generation remains particularly challenging due to its vast spatial extent, complex topology, and highly diverse architectural styles. Recent LLM-driven approaches (e.g., SceneCraft [18], 3D-GPT [40]) offer expressive control but remain limited to small scale, simple scenes, failing to maintain the macroscopic geometric validity required for urban environments. Conversely, layout-guided methods such as InfiniCity [32], CityDreamer [51], and GaussianCity [52] leverage 2D semantic priors to produce city-level scenes, but rely on implicit or neural rendering representations that suffer from multi-view inconsistency and incompatibility with downstream simulation pipelines. To bridge the aforementioned gap between representation and application, researchers have turned to exploring urban generation methods based on explicit meshes [6, 54, 65, 68]. Explicit-mesh methods improve structural reliability by retrieving assets from predefined libraries, but their generative diversity is fundamentally constrained by the coverage and style limitations of those libraries, thereby aligning it more closely with Retrieve-and-Place paradigm than with generation. To overcome these limitations, we present MajutsuCity, natural languagedriven, aesthetic-adaptive, and fully controllable framework for 3D city generation. Our key insight is that natural language inherently encodes both macroscopic geometric logic (e.g., bustling downtown with towering skyscrapers) and fine-grained aesthetic intent (e.g., pink lighting under the sunset). MajutsuCity leverages this property by introducing structured language-to-city specification pipeline through four stages that (1) parses the input text into geometric and aesthetic specifications, (2) generates layout and height maps, (3) synthesizes stylized assets and materials, and (4) assembles them into coherent, editable 3D city. We believe that practical city generation framework requires not only controllable generation but also efficient editing capabilities. Beyond the initial generation, we introduce MajutsuAgent, an integrated language-grounded editor that supports object-level Add, Delete, Edit, Move, and Replace operations, enabling the iterative refinement of generated scenes. To support high-quality and customizable synthesis, we further construct MajutsuDataset, multimodal dataset integrating 2D semantic layouts with building heights, diverse 3D building assets, and production-level PBR textures and skyboxes. Moreover, we identify that there are no metrics designed for 3D city scenes and thus fail to evaluate structural correctness, material realism, and multi-view consistency. We introduce VLM-based evaluation framework that provides both absolute scoring (AQS) and relative dimension ranking (RDR) across four essential dimensions: Structural & View Consistency, Scene Richness & Complexity, Material & Texture Fidelity, and Lighting & Atmosphere. Extensive experiments demonstrate that MajutsuCity outperforms CityDreamer, GaussianCity, UrbanWorld, and CityCraft across all metrics. Our method achieves 83.7% FID reduction over CityDreamer and 20.1% over CityCraft for layout generation, producing significantly sharper and more topologically coherent city structures. Under both AQS and RDR protocols, MajutsuCity ranks 1st in all eight dimensions, reflecting superior geometric fidelity, material realism, and aesthetic adaptability across diverse style domains. Our main contributions are as follows: 1. We propose MajutsuCity, novel unified natural language-driven framework enabled by structured text-to-city specification design for controllable and aesthetic-adaptive 3D scene generation with meaningful evaluation protocol. 2. We build MajutsuDataset, comprehensive and highquality dataset that combines thousands of text-aligned layout/height maps, stylistically diverse geometryconstrained 3D assets, and PBR/HDRI materials. 3. We introduce MajutsuAgent, first interactive natural language editing agent that enables fine-grained objectlevel manipulation of generated scenes. 2. Related Work 2.1. Layout Generation Compared to general scene layouts [13, 24], urban layouts exhibit significantly higher complexity due to the richer semantic categories and irregular geometric topologies present in real-world cities. While vector-based layout generation methods such as BlockPlanner [53] and GlobalMapper [15] provide structured formulation, they often suffer from limited semantic representation and struggle to model complex, fine-grained spatial patterns. In contrast, recent mask-based generation methods [6, 7, 32, 51] offer better scalability and geometric fidelity, capturing detailed spatial boundaries while maintaining efficient inference. However, these methods typically lack intuitive user control, specifically in expressing high-level design intent through natural language. To address this limitation, we train language-guided urban layout generation model that aligns fine-grained spatial mask synthesis with user-provided textual descriptions, effectively bridging the gap between high-fidelity mask generation and user-intent controllability. 2.2. Image-to-3D Generation Recent advances in image generation have provided new direction for 3D content creation, fundamentally reshaping the synthesis of high-quality 3D assets. Early methods were often limited by low-fidelity geometry and low-resolution textures [23, 33, 35, 37], but current 3D generation frameworks increasingly leverage powerful 2D visual priors to improve the consistency of geometry and textures. In particular, many state-of-the-art works employ visual foundation model embeddings such as DINOv2 [36] to extract rich semantic and textural representations, coupled with Vision Transformer (ViT) [8] architectures to guide and regularize 3D geometry generation. The rapid evolution of series of SOTA and open-source models (e.g., Trellis [50], Step1X-3D [30], and Hunyuan3D 2.0 / 2.1 [21, 66], as well as commercial tools (e.g., Tripo, Rodin [47], Meshy, Hunyuan3D 2.5 [26], and Hitem3D) have demonstrated that photorealistic and high-detail 3D assets are now not only feasible but increasingly reliable. Motivated by these advances, this work explores principled integration of advanced 3D asset generation models as core components into city-scale pipeline, enabling largescale, controllable, and aesthetic-adaptive 3D city generation within unified, reproducible framework. 2.3. City Scene Generation In urban scene generation, existing methods such as InfiniCity [32], CityDreamer [51], and Persistent Nature [2] have demonstrated the ability to generate large-scale 3D scenes. However, they often rely on implicit or neural representations, resulting in two critical bottlenecks: (1) geometric artifacts and multi-view inconsistency, which stem from the inherent ambiguity of implicit fields, and (2) the absence of explicit, editable object-level structures, making them unsuitable for downstream applications that require precise interaction, editing, and simulation compatibility. On the other hand, Procedural Content Generation (PCG)-based techniques can produce highly structured cities [9, 65, 68], but they follow fundamentally Retrieveand-Place paradigm. As result, the diversity and expressiveness of the generated scenes are strictly limited by the scale, style coverage, and quality of the predefined asset libraries, restricting their ability to generalize to novel or stylistically distinctive demands. Recent advanced works in indoor scene generation have successfully validated new path: combining the powerful priors of 2D vision models with on-demand 3D object generation to achieve object-level, controllable scene synthesis [5, 19, 56]. Inspired by this, we aim to scale this object-centric generative paradigm from the indoor level to the macroscopic urban level, enabling unified framework that is not only controllable and editable, but also adaptive to diverse aesthetic styles, and addressing the core limitations of prior approaches in urban scene generation. 3. MajutsuCity To enable controllable generation of object-level urban scenes directly from natural language, we propose MajutsuCity framework that bridges high-level textual intent and structured 3D scene composition. As shown in Fig. 2, the framework has four major stages: Scene Design: converting textual requirements into structured and consistent design guidance. Layout Generation: synthesizing spatially coherent urban layouts and height maps under semantic and topological constraints. Assets & Materials Generation: producing high-fidelity building-level 3D assets and material maps and skybox. Scene Generation: composing assets and environmental layers into coherent and renderable 3D city. Furthermore, we develop MajutsuAgent, an interactive editing agent enables fine-grained, human-in-the-loop scene manipulation with high controllability and consistency. 3.1. Overview of Scene Design Free-form natural language descriptions of urban scenes are typically ambiguous, lacking quantitative and relational constraints. To this end, we employ LLMs for intent understanding and structured decomposition, natural language into structured, executable urban design specifications. Specifically, the LLM reasons over user prompts to infer latent planning intent and decomposes it into multidimensional urban design template covering Layout, Assets, Materials, and Skymap. Each dimension is parameterized through standardized templates (e.g., land use, spatial distribution, architectural style, and facade material), forming semantically grounded blueprint that guides both spatial layout generation and 3D asset synthesis. 3.2. Layout Generation Upon receiving the fine-grained spatial guidance Clayout from Section 3.1, the goal of this module is to synthesize spatially aligned pair: semantic layout map Ilayout and Figure 2. Overview of the proposed MajutsuCity framework. MajutsuCity is an aesthetic-adaptive generative framework that enables controllable, object-level 3D urban scene generation from natural language descriptions. It consists of Scene Design, Layout Generation, Assets & Materials Generation, and Scene Generation. building height map Iheight. To address the complex mapping from high-level textual guidance to semantic and geometric outputs, we design two-stage cascaded framework for layout generation. The first stage employs diffusion model ϵ(1) θ to synthesize Ilayout from the detailed long-text description Clayout. Since Clayout (e.g., main road stretches from northwest to southeast, flanked by dense commercial buildings) typically exceeds the token length and contextual limits of the standard CLIP [38] encoder, inspired by [59], we replace the original text encoder τθ with LongCLIP [62]. This substitution yields uncompressed, information-rich semantic features ec = τθ(Clayout) that provide precise, fine-grained control over complex urban layouts. In the second stage, the generated Ilayout serves as strong spatial prior Cs (e.g., its semantic mask) and is fed into ControlNet-based [29, 58, 64] architecture ϵ(2) θ . Zeroconvolution layers inject pixel-level control signals to guide the synthesis of Iheight, ensuring strict spatial consistency with the building regions defined in Ilayout."
        },
        {
            "title": "Both stages are trained on MajutsuDataset and optimized",
            "content": "using the standard latent diffusion objective: = Ez0,c,ϵN (0,1),t (cid:104) ϵ ϵθ(zt, t, c)2 2 (cid:105) . (1) This decoupled two-stage design enables effective propagation of both high-level semantic intent and low-level spatial constraints, facilitating coherent generation from textual spatial guidance to layout and elevation map synthesis. 3.3. Assets and Materials Generation Asset Generation. Existing urban scene generation frameworks often exhibit weak coupling between semantic representation and geometric controllability, leading to limited object-level editability and inconsistent structural logic. To this end, we propose bottom-up asset-level generation paradigm. Given the synthesized layout and building height maps, we extract instance-level building units and generate dedicated 3D asset for each instance following the Assets Design specification. This formulation decouples global layout generation from local geometric modeling, enabling semantically consistent and structurally controllable asset synthesis. To further ensure spatial alignment and geometric coherence with the prescribed layout, we introduce shape-constrained generation mechanism composed of two complementary strategies: Image-based shape constraints. Inspired by QwenImage-Edit [49], we introduce an image-guided refinement process that enhances coarse geometries derived from extruding building instance masks in the layout with height-map supervision. Specifically, an isometric rendering Iiso of the coarse geometry serves as geometric prior, while the Assets Design prompt pAD provides semantic and stylistic guidance for refining appearance details. This dual conditioning allows the model to enrich visual fidelity without violating the original geometric proportions. To further ensure structural consistency, we incorporate VLM-based self-calibration mechanism that quantitatively evaluates the shape agreement between the refined result Iref and its prior Iiso. When deviation beyond the preset threshold is detected, the system triggers an automatic reviewregenerate loop, progressively adjusting the generation parameters until the output satisfies geometric consistency criteria. Point cloud-based shape constraints. To enhance 3D consistency and ensure strict adherence to the predefined footprint in both shape and height, we introduce an optional point cloud-based constraint strategy. uniformly sampled point cloud from the coarse geometry serves as an explicit 3D constraint. The sampled point cloud Pc, together with reference image Iref as an appearance guide, is fed into multi-conditional 3D generation framework [21] to produce the final 3D asset. This joint conditioning guarantees that the synthesized 3D asset remains tightly aligned with the coarse geometry in both shape and scale. In addition to buildings, 3D models of trees and streetlights are also generated to enrich scene diversity. Material Generation. Unlike discrete building assets, surface features (such as roads, grass, and water surfaces) spatially continuous and require seamlessly tilable textures. Without proper tiling, visible seams and periodic artifacts can occur, degrading overall realism. To overcome the limitations of generic image generators for tilable material synthesis, we adopt Qwen-Image as our visual backbone and fine-tune it on two domain-specific datasets: MajutsuDataset-Material for seamlessly tilable texture maps, and MajutsuDataset-Skybox for high-quality panoramic sky spheres. It produces both material maps and sky spheres, ensuring seamless texture tiling across largescale urban scenes while improving environmental realism in terms of lighting and atmospheric consistency. 3.4. Scene Generation After completing the Layout Generation and Assets/Materials Generation stages, we integrate all intermediate results to construct fully renderable, object-level urban scene. The ground, road, water, and vegetation components are organized into four planar layers derived from the semantic layout map, with each layer bound to its corresponding seamlessly tiling material texture to ensure spatial coherence and visual continuity across the scene. For the vegetation layer, Poisson disk sampling is applied within the vegetation mask to determine tree placement positions, where individual tree models are instantiated. For the road layer, distance-transform-based equidistant sampling is employed to place roadside trees and streetlights along the road boundaries, ensuring realistic spatial distribution and structural consistency. Subsequently, each building instance in the semantic map is instantiated by placing its shape-constrained 3D asset at the corresponding footprint location after performing similarity transformation that aligns the local coordinate system to the global layout. Finally, 360 panoramic sky sphere is integrated into the scene to initialize the ambient lighting and global illumination, yielding complete, highfidelity urban scene that remains semantically and geometrically consistent with the layout priors. 3.5. Scene Agent Traditional scene generation pipelines typically lack postgeneration editability, whereas object-level scene representations provide natural interface for fine-grained interaction. Building upon this insight, we develop MajutsuAgent, natural languagedriven system that enables personalized editing of city-scale scenes. At its core, MajutsuAgent abstracts high-level natural language interactions into five standardized operations, encapsulated within unified interface: Add: instantiate and insert new assets into the scene. Delete: remove specified assets. Edit: modify the visual or structural attributes of assets. Move: apply rigid transformations (translation, rotation, scaling) to the selected assets. Replace: substitute materials on specific surfaces. By leveraging GPT-5 to decompose user commands into sequence of atomic, interpretable operations, MajutsuAgent accurately translates user intent into controllable scene modifications, thereby enabling intuitive and fine-grained customization of the generated urban environments. 4. MajutsuDataset To enhance the realism and controllability of 3D scene generation, we introduce MajutsuDataset, high-quality multimodal dataset designed for text-guided 3D scene synthesis. As illustrated in Figure 3, the dataset comprises three major components: Layout/Elevation, 3D Building Models, and Material Assets. Layout/Elevation. Existing urban layout datasets generally lack rich textual annotations that are tightly aligned with visual content, making it difficult for models to learn fine-grained textlayout correspondences and thus constraining controllable generation. To address this challenge, we construct large-scale city layout dataset based on OpenStreetMap, containing 13, 300 image samples collected from diverse regions representing distinct urban styles across Asia, Europe, South America, Oceania, and America. Each sample consists of semantic layout map and building height map, both at resolution of 512 512 pixels. The semantic map covers five primary categories (vegetation, roads, water bodies, buildings, and ground). Furthermore, we employ GPT-5-mini [1] to generate detailed, context-aware textual descriptions for each layout image, enabling fine-grained text-conditional learning. 5. Experiments 5.1. Implementation Details and Metrics We adopt the pre-trained Stable Diffusion v2.1 model as our baseline for layout generation. The original CLIP text encoder is replaced with LongCLIP. For height map synthesis, ControlNet encoder is jointly trained with the U-Net to ensure precise spatial alignment between the two stages. Both networks are trained at resolution of 512 512 pixels. We employ the AdamW optimizer with an initial learning rate of 1 105. Training is conducted on four NVIDIA A100 GPUs for 100 epochs, using global batch size of 128. During inference, both stages utilize Classifier-Free Guidance (CFG) scale of ω = 9.0 and DDIM sampler with = 50 steps. The entire generation process is executed on single NVIDIA A800 GPU. For the City Layout Generation, following the evaluation protocols of CityDreamer [51] and CityCraft [6], we adopt three widely used metrics to assess the visual fidelity and diversity of the generated city layouts: Frechet Inception Distance (FID), Kernel Inception Distance (KID), and Inception Score (IS). For the City Scene Generation, given the absence of unified and reliable evaluation protocol, we introduce VLM-based automated evaluation framework motivated by [11, 57, 61, 67]. Our framework is built upon four assessment dimensions: Structural and View Consistency (SVC), Scene Richness and Complexity (SRC), Material and Texture Fidelity (MTF), and Lighting and Atmosphere (LA). We employ both GPT-5 (as an automated evaluator) and 20 human users in two-stage procedure: Absolute Quantitative Scoring (AQS) and Relative Dimension Ranking (RDR). In AQS, GPT-5 assigns 110 score for each method based on multi-view renders across the four dimensions, and the mean score is reported as the AQS result. Inspired by prior work [10, 14, 31, 55, 60, 63], we further adopt an RDR protocol to mitigate potential bias inherent in AQS. In RDR, GPT-5 and human users perform pairwise comparisons between sampled image pairs independently for each dimension. Every image participates in at least ten comparisons to ensure robustness. We then aggregate wins and losses per dimension and apply the TrueSkill ranking system [17] to derive dimension-specific RDR scores for all methods. 5.2. Comparison of City Layout Generation Qualitative Comparison. As shown in Figure 4, our qualitative comparison with InfiniteGAN [32], CityDreamer [51], and CityCraft [6] reveals that existing methods struggle to preserve structural fidelity. CityDreamer produces fragmented building footprints with poor road continuity, whereas CityCraft, despite generating complete buildings, 1All resources adhere to the CC0 license framework. Figure 3. Overview of MajutsuDataset, high-quality multimodal dataset designed for text-guided 3D urban scene generation. (a) The OSM-based Layout/Elevation subset provides paired semantic layout maps, height maps, and detailed textual descriptions generated by GPT-5-mini. (b) The 3D Building Models subset includes 1,000 assets covering diverse architectural styles. (c) The Texture Map subset contains large-scale library of seamlessly tilable PBR materials and HDR skybox maps. 3D Building Models. With recent advances in 3D generative modeling, it has become feasible to produce highfidelity, diverse 3D assets on demand. Inspired by this progress, we curate stylistically diverse library of 3D building models to support downstream urban scene synthesis. Specifically, we define ten representative architectural styles and generate twenty distinct building types for each style. The source images are processed using five commercial-grade 3D generation systems (Meshy, Hunyuan3D, Hyper3D, Tripo3D, and Hitem3D), resulting in final collection of 1,000 assets with rich stylistic and morphological diversity. All assets will be distributed in full compliance with their respective licensing terms. Material Assets. Existing materials and skybox data often lack physical plausibility, detail fidelity, or seamless tiling capability, limiting their use in production-level rendering. To bridge this gap, MajutsuDataset incorporates high-quality material asset1 library consisting of: PBR Material Library: 2, 300 seamlessly tilable PBR materials collected from public repositories such as AmbientCG and Poly Haven. Each material includes complete set of Physically-Based Rendering (PBR) texture maps (Base Color, Normal, Roughness, Metallic, and Ambient Occlusion). Skybox Map Library: 1, 000 High Dynamic Range (HDR) skybox maps curated from multiple professional sources, covering diverse lighting conditions and atmospheric contexts. We employ GPT-5-mini to automatically generate detailed textual metadata for each material and skybox asset to enhance semantic usability. Figure 4. Qualitative comparison of city layouts generation. Our method yields more realistic and coherent urban layouts than prior InfiniteGAN [32], CityDreamer [51] and CityCraft [6]. Table 1. Quantitative comparison of Layout Generation."
        },
        {
            "title": "Method",
            "content": "FID() KID() IS() InfiniteGAN [32] CityDreamer [51] CityCraft [6]"
        },
        {
            "title": "Ours",
            "content": "180.4 139.6 28.4 22.7 0.215 0.164 0.016 0.013 2.58 1.96 3.11 3. often yields unrealistic layouts with misaligned structures and implausible spatial distributions. In contrast, our approach generates layouts that are topologically coherent and structurally well-organized. The resulting building footprints are complete, sharp, and consistently aligned with the road network, forming realistic city blocks that provide reliable structural foundation for high-fidelity downstream scene generation. Quantitative Comparison. The quantitative results are shown in Table 1. Our method achieves improvements of 83.7% and 20.1% in FID over CityDreamer [51] and CityCraft [6], respectively. Benefiting from the guidance of finegrained spatial text, our model produces layouts that are structurally more plausible and better aligned with the realworld distribution. In addition, the higher IS score demonstrates that our method generates clearer and more diverse urban structures, further validating its effectiveness in largescale city layout synthesis. 5.3. Comparison of City Scene Generation To assess the quality of the generated city scenes, we compare our method with four state-of-the-art approaches in urban scene synthesis: CityDreamer [51], GaussianCity [52], UrbanWorld [39], and CityCraft [6]. All 3D assets in our case scenarios are generated by using Hunyuan3D. Qualitative Comparison. Figure 5 provides two representative examples comparison. CityDreamer and GaussianCity exhibit severe geometric artifacts and multi-view inconsistency, due to limitations of their underlying scene representations. UrbanWorld employs explicit meshes but is restricted to coarse primitives and suffers from low texture fidelity. CityCraft produces visually realistic results but remains highly constrained in stylistic diversity by its fixed asset library. In contrast, our method achieves both stable geometry and strong multi-view consistency, while also deTable 2. Absolute Quantitative Scoring (AQS) and Relative Dimension Ranking (RDR) for city scene generation. For each metric, we report both GPT-based and user-based scores. AQS RDR Method SVC() SRC() MTF() LA() SVC() SRC() MTF() LA() GPT User GPT User GPT User GPT User GPT User GPT User GPT User GPT User CityDreamer [51] GaussianCity [52] UrbanWorld [39] CityCraft [6] Ours 4.20 6.73 6.17 6.00 8. 5.23 7.63 7.16 6.97 8.35 6.90 7.17 5.40 6.11 8.33 7.24 7.37 5.99 6.74 8. 2.70 2.83 2.14 4.22 7.00 6.40 6.54 5.41 6.64 8.03 3.10 3.33 2.80 5.00 6. 6.09 6.23 5.17 6.23 7.67 12.39 23.56 25.31 24.44 17.98 20.40 23.89 22.79 25.70 27.17 16.06 21.33 21.44 21.84 14.10 25. 18.50 18.78 12.39 26.60 19.99 21.47 14.11 22.99 18.25 18.29 13.85 31.12 21.55 22.42 14.85 24.62 34.13 26. 28.76 28.97 28.87 25.49 32.68 25. Figure 5. Qualitative comparison of city scene. We compare our method with CityDreamer [51], GaussianCity [52], UrbanWorld [39], and CityCraft [6] across two representative scenes. Our approach produces scenes with higher geometric fidelity, better multi-view consistency, and richer stylistic diversity than all baselines. livering rich stylistic diversity and high-fidelity geometry and textures, substantially outperforming all baseline methods in overall visual quality. Quantitative Comparison. The AQS and RDR results reported in Table 2, clearly demonstrate that our model surpasses all baselines. CityDreamer performs the worst on SVC due to intrinsic limitations of its NeRF-based representation. CityCraft achieves relatively strong performance in MTF and LA, attributed to its high-quality asset library, but its stylistic monotony results in low SRC scores. Importantly, we observe notable discrepancy that although the 3DGS-based GaussianCity outperforms mesh-based UrbanWorld and CityCraft in SVC under AQS, the ranking is reversed under RDR. This reversal indicates that RDR efTable 3. Ablation Study of Layout Generation. Spatial Text represents fine-grained spatial text, and LongCLIP represents the long-text visual-language pre-training module [62]."
        },
        {
            "title": "Spatial Text LongCLIP",
            "content": "FID() KID() IS() 35.7 28.0 22.7 0.025 0.023 0. 3.08 3.07 3.14 fectively mitigates the bias present in absolute scoring and provides more robust assessment of structural consistency. Significantly, the strong alignment between GPT and human evaluations validates our assessment dimensions and confirms GPTs near-human assessment capabilities. 5.4. Ablation Study Effectiveness of Layout Generation. We conduct an ablation study on the layout generation module in Table 3 to validate the effectiveness of fine-grained spatial text and the LongCLIP encoder. The results show that removing LongCLIP causes the FID metric to degrade from 22.7 to 28.0, confirming its important role in understanding longtext spatial relationships. Similarly, removing the spatial text guidance (and using short prompts instead) leads to significant degradation in FID, which rises to 35.7. The KID and IS metrics show consistent trends, indicating that both components are indispensable for generating high-quality, topologically coherent city layouts. Aesthetic-Adaptive Style Transformation. To further demonstrate the ability of our model to generate stylistically diverse urban scenes, we provide additional qualitative results in Figure 6. We condition the generation on four widely recognized and visually distinctive styles (e.g., Minecraft, Netherlands, CyberPunk, and Ghibli). Our approach not only faithfully captures the defining aesthetic characteristics of each style but also preserves strong intrastyle coherence across large-scale city scenes. 6. Conclusion In this paper, we introduce MajutsuCity, natural languagedriven and aesthetic-adaptive framework for controllable 3D urban scene generation. Through unified fourstage pipeline, MajutsuCity enables precise structural control, and supports diverse and customizable visual styles, which remain limited in existing approaches. To establish complete system, we further develop three key components: (1) MajutsuDataset, high-quality multimodal dataset that integrates 2D layout/elevation priors, high-fidelity 3D assets, and PBR materials, providing essential data foundations for photorealistic and immersive scene construction. (2) MajutsuAgent, an interactive agent that extends the role of language guidance from initial scene synthesis to postgeneration editing. (3) Metric suite consisting of Absolute Figure 6. Style-driven city generation results. Four city scenes with different well-known styles generated by MajutsuCity show high fidelity and strong intra-style consistency. Quantitative Scoring (AQS) and Relative Dimension Ranking (RDR) systematically assesses structural consistency, scene richness, material fidelity, and lighting realism, offering the first holistic metric system for benchmarking urban scene generation quality. Our work opens new avenues for scalable, controllable, and style-adaptive 3D urban generation. We believe MajutsuCity provides not only powerful generative framework but also foundational direction, powerful dataset, and meaningful metric for future research in VLM-Based 3D content generation. We will continue to expand and refine the dataset to foster broader research and contribute more substantially to the community. 7. Limitations and Futurk Work Despite the significant advancements that MajutsuCity demonstrates in interactive city generation, several limitations remain inherent to the current framework: Sensitivity to Prompt Logical Consistency. The layout generation module is highly sensitive to the logical coherence of user-provided prompts. Due to the strictly hierarchical nature of our pipeline, contradictory spatial instructions or geometrically infeasible layouts generated during the initial scene design stage can propagate through subsequent modules. These cascading errors may lead to implausible road networks or unreasonable building configurations, ultimately degrading the quality of the assembled scene. Uncontrollability of highly complex shapes. Although both image-based and point cloud-based constraints are incorporated to preserve geometric fidelity in building generation, ensuring high visual fidelity while maintaining geometric validity remains challenging, particularly for buildings with highly complex or irregular architectural topologies. Visual Scale Inconsistency. Since each building asset is synthesized independently in its own local coordinate space, the model lacks global sense of scale and contextual awareness. Consequently, even though our shape constraint strategies effectively regulate individual instance shapes, inconsistent visual scales may occur during final assembly (e.g., inconsistent story heights between adjacent buildings). Recently, Meta introduced WorldGen [44], system with goals closely aligned with ours: generating fully interactive 3D worlds directly from text prompts. WorldGen adopts top-down paradigm, Holistic Planning Holistic Scene Generation Object Decomposition Object Refinement. It first generates foundationally coherent layout and RGB sketch guided by user instructions, then decomposes the scene into interactive and independent assets using object-aware techniques, followed by geometric and textural refinements. This Holistic-then-Local workflow effectively circumvents the scale fragmentation commonly caused by instance-level stitching, while ensuring continuous terrain topology and strong visual stylistic unity. Inspired by this direction, our future work will explore hybrid generation paradigm. Our aim is to further enhance the physical rationality and visual harmony of complex urban scenes while maintaining the high level of controllability offered by our current method."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5 [2] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, and Noah Snavely. Persistent nature: generative model of unbounded 3d worlds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2086320874, 2023. 2, 3 [3] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scenedreamer: Unbounded 3d scene generation from 2d image collections. IEEE transactions on pattern analysis and machine intelligence, 45(12):1556215576, 2023. 2 [4] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2657626586, 2025. 2 [5] Tianyuan Dai, Josiah Wong, Yunfan Jiang, Chen Wang, Cem Gokmen, Ruohan Zhang, Jiajun Wu, and Li Fei-Fei. Automated creation of digital cousins for robust policy learning. arXiv preprint arXiv:2410.07408, 2024. 3 [6] Jie Deng, Wenhao Chai, Junsheng Huang, Zhonghan Zhao, Qixuan Huang, Mingyan Gao, Jianshu Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, et al. Citycraft: real crafter for 3d city generation. arXiv:2406.04983, 2024. 2, 3, 6, 7, 8 arXiv preprint [7] Jie Deng, Wenhao Chai, Jianshu Guo, Qixuan Huang, Junsheng Huang, Wenhao Hu, Shengyu Hao, Jenq-Neng Hwang, and Gaoang Wang. Citygen: Infinite and controllable city layout generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1995 2005, 2025. 3 [8] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [9] Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Yenan Lin, Hao Jiang, Kang Chen, et al. Latticeworld: multimodal large language model-empowered framework for interactive complex world generation. arXiv preprint arXiv:2509.05263, 2025. 3 [10] Abhimanyu Dubey, Nikhil Naik, Devi Parikh, Ramesh Raskar, and Cesar Hidalgo. Deep learning the city: Quantifying urban perception at global scale. In European conference on computer vision, pages 196212. Springer, 2016. 6 [11] Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, and Dahua Lin. Make-it-real: Unleashing large multimodal model for painting 3d objects with realistic materials. Advances in Neural Information Processing Systems, 37:9926299298, 2024. [12] Daniel Gonzalez-Medina, Luis Rodrıguez-Ruiz, and Ismael Garcıa-Varea. Procedural city generation for robotic simulation. In Robot 2015: Second Iberian Robotics Conference: Advances in Robotics, Volume 2, pages 707719. Springer, 2015. 2 [13] Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layouttransformer: Layout generation and completion with selfIn Proceedings of the IEEE/CVF International attention. Conference on Computer Vision, pages 10041014, 2021. 2 [14] Jun He, Yi Lin, Zilong Huang, Jiacong Yin, Junyan Ye, Yuchuan Zhou, Weijia Li, and Xiang Zhang. Urbanfeel: comprehensive benchmark for temporal and perceptual understanding of city scenes through human perspective. arXiv preprint arXiv:2509.22228, 2025. 6 [15] Liu He and Daniel Aliaga. Globalmapper: Arbitrary-shaped urban layout generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 454464, 2023. 2 [16] Mark Hendrikx, Sebastiaan Meijer, Joeri Van Der Velden, and Alexandru Iosup. Procedural content generation for games: survey. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 9(1): 122, 2013. 2 [17] Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill: bayesian skill rating system. Advances in neural information processing systems, 19, 2006. 6 [18] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Machine Learning, 2024. [19] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffusion for single image to 3d scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2364623657, 2025. 2, 3 [20] Zilong Huang, Jun He, Junyan Ye, Lihan Jiang, Weijia Li, Yiping Chen, and Ting Han. Scene4u: Hierarchical layered 3d scene reconstruction from single panoramic image for your immerse exploration. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26723 26733, 2025. 2 [21] Team Hunyuan3D, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Penghao Wang, et al. Hunyuan3d-omni: unified framework for controllable generation of 3d assets. arXiv preprint arXiv:2509.21245, 2025. 3, 5 [22] Elmira Jamei, Michael Mortimer, Mehdi Seyedmahmoudian, Investigating the role of Ben Horan, and Alex Stojcevski. virtual reality in planning for sustainable smart cities. Sustainability, 9(11):2006, 2017. 2 [23] Heewoo Jun and Alex Nichol. Shap-e: GeneratarXiv preprint ing conditional 3d implicit functions. arXiv:2305.02463, 2023. [24] Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. Layoutvae: Stochastic scene layout genIn Proceedings of the IEEE/CVF eration from label set. International Conference on Computer Vision, pages 9895 9904, 2019. 2 [25] Pushkal Katara, Zhou Xian, and Katerina Fragkiadaki. Gen2sim: Scaling up robot learning in simulation with generative models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 66726679. IEEE, 2024. 2 [26] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards highfidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. 3 [27] Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d In European Conference on Computer Vision, generation. pages 112130. Springer, 2024. 2 [28] Weijia Li, Yawen Lai, Linning Xu, Yuanbo Xiangli, Jinhua Yu, Conghui He, Gui-Song Xia, and Dahua Lin. Omnicity: Omnipotent city understanding with multi-level and multiview images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17397 17407, 2023. 2 [29] Weijia Li, Jun He, Junyan Ye, Huaping Zhong, Zhimeng Zheng, Zilong Huang, Dahua Lin, and Conghui He. Crossviewdiff: cross-view diffusion model for satelliteto-street view synthesis. arXiv preprint arXiv:2408.14765, 2024. [30] Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, et al. Step1x-3d: Towards high-fidelity and controllable generation of textured 3d assets. arXiv preprint arXiv:2505.07747, 2025. 3 [31] Xiucheng Liang, Jiat Hwee Chang, Song Gao, Tianhong Zhao, and Filip Biljecki. Evaluating human perception of building exteriors using street view imagery. Building and Environment, 263:111875, 2024. 6 [32] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey In ProInfinicity: Infinite-scale city synthesis. Tulyakov. ceedings of the IEEE/CVF international conference on computer vision, pages 2280822818, 2023. 2, 3, 6, 7 [33] Norman Muller, Yawar"
        },
        {
            "title": "Lorenzo",
            "content": "Siddiqui, Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Diffrf: Rendering-guided 3d radiance field Nießner. In Proceedings of the IEEE/CVF Conference diffusion. on Computer Vision and Pattern Recognition, pages 43284338, 2023. 3 [34] Minh-Tu Nguyen, Hai-Khanh Nguyen, Khanh-Duy Vo-Lam, Xuan-Gieng Nguyen, and Minh-Triet Tran. Applying virIn International Conference tual reality in city planning. on Virtual, Augmented and Mixed Reality, pages 724735. Springer, 2016. 2 [35] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 3 [36] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [37] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [39] Yu Shang, Yuming Lin, Yu Zheng, Hangyu Fan, Jingtao Ding, Jie Feng, Jiansheng Chen, Li Tian, and Yong Li. Urbanworld: An urban world model for 3d city generation. arXiv preprint arXiv:2407.11965, 2024. 7, 8 [40] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt: Procedural 3d modeling with large language models. In 2025 International Conference on 3D Vision (3DV), pages 12531263. IEEE, 2025. 2 [41] Ekim Tan. The evolution of city gaming. In Complexity, Cognition, Urban Planning and Design: Post-Proceedings of the 2nd Delft International Conference, pages 271292. Springer, 2016. 2 [42] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2024. 2 [43] Emine Mine Thompson, Margaret Horne, and David Fleming. Virtual reality urban modelling-an overview. In CONVR2006: 6th Conference of Construction Applications of Virtual Reality, pages 34, 2006. 2 [44] Dilin Wang, Hyunyoung Jung, Tom Monnier, Kihyuk Sohn, Chuhang Zou, Xiaoyu Xiang, Yu-Ying Yeh, Di Liu, Zixuan Huang, Thu Nguyen-Phuoc, et al. Worldgen: From text to traversable and interactive 3d worlds. arXiv preprint arXiv:2511.16825, 2025. 10 [45] Hanqing Wang, Jiahe Chen, Wensi Huang, Qingwei Ben, Tai Wang, Boyu Mi, Tao Huang, Siheng Zhao, Yilun Chen, Sizhe Yang, et al. Grutopia: Dream general robots in city at scale. arXiv preprint arXiv:2407.10943, 2024. 2 [46] Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. Vistadream: Sampling multiview consistent images for single-view scene reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2677226782, 2025. 2 [47] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45634573, 2023. [48] Beichen Wen, Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. 3d scene generation: survey. arXiv preprint arXiv:2505.05474, 2025. 2 [49] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 4 [50] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 2, 3 [51] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Citydreamer: Compositional generative model of unbounded In Proceedings of the IEEE/CVF conference on 3d cities. computer vision and pattern recognition, pages 96669675, 2024. 2, 3, 6, 7, 8 [52] Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu. Generative gaussian splatting for unbounded 3d city generIn Proceedings of the Computer Vision and Pattern ation. Recognition Conference, pages 61116120, 2025. 2, 7, 8 [53] Linning Xu, Yuanbo Xiangli, Anyi Rao, Nanxuan Zhao, Bo Dai, Ziwei Liu, and Dahua Lin. Blockplanner: City block generation with vectorized graph representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 50775086, 2021. 2 [54] Yongzhi Xu, Yonhon Ng, Yifu Wang, Inkyu Sa, Yunfei Duan, Yang Li, Pan Ji, and Hongdong Li. Sketch2scene: Automatic generation of interactive 3d game scenes from users casual sketches. arXiv preprint arXiv:2408.04567, 2024. 2 [55] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [56] Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, and Jingyi Yu. Cast: Component-aligned 3d scene reconstruction from an rgb image. ACM Transactions on Graphics (TOG), 44(4): 119, 2025. 3 [57] Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, et al. Loki: comprehensive synthetic data detection benchmark using large multimodal models. arXiv preprint arXiv:2410.09732, 2024. 6 [58] Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, and Conghui He. Leveraging bev paradigm for ground-to-aerial image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2845128461, 2025. 4 [59] Junyan Ye, Jun He, Xiang Zhang, Yi Lin, Honglin Lin, Conghui He, and Weijia Li. Satellite image synthesis from street view with fine-grained spatial textual guidance: novel IEEE Geoscience and Remote Sensing Magaframework. zine, 2025. 4 [60] Junyan Ye, Dongzhi Jiang, Jun He, Baichuan Zhou, Zilong Huang, Zhiyuan Yan, Hongsheng Li, Conghui He, and Weijia Li. Blink-twice: You see, but do you observe? reasoning benchmark on visual perception. arXiv preprint arXiv:2510.09361, 2025. 6 [61] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. [62] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. In European conference on computer vision, pages 310325. Springer, 2024. 4, 9 [63] Fan Zhang, Bolei Zhou, Liu Liu, Yu Liu, Helene Fung, Hui Lin, and Carlo Ratti. Measuring human perceptions of large-scale urban region using machine learning. Landscape and Urban Planning, 180:148160, 2018. 6 [64] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 4 [65] Shougao Zhang, Mengqi Zhou, Yuxi Wang, Chuanchen Luo, Rongyu Wang, Yiwei Li, Zhaoxiang Zhang, and Junran Peng. Cityx: Controllable procedural content generation for unbounded 3d cities. arXiv preprint arXiv:2407.17572, 2024. 2, 3 [66] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 3 [67] Baichuan Zhou, Haote Yang, Dairong Chen, Junyan Ye, Tianyi Bai, Jinhua Yu, Songyang Zhang, Dahua Lin, Conghui He, and Weijia Li. Urbench: comprehensive benchmark for evaluating large multimodal models in multi-view urban scenarios. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1070710715, 2025. 6 [68] Mengqi Zhou, Yuxi Wang, Jun Hou, Shougao Zhang, Yiwei Li, Chuanchen Luo, Junran Peng, and Zhaoxiang Zhang. Scenex: Procedural controllable large-scale scene generation. arXiv preprint arXiv:2403.15698, 2024. 2,"
        }
    ],
    "affiliations": [
        "Sun Yat-sen University"
    ]
}