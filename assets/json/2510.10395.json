{
    "paper_title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
    "authors": [
        "Xinlong Chen",
        "Yue Ding",
        "Weihong Lin",
        "Jingyun Hua",
        "Linli Yao",
        "Yang Shi",
        "Bozhou Li",
        "Yuanxing Zhang",
        "Qiang Liu",
        "Pengfei Wan",
        "Liang Wang",
        "Tieniu Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 5 9 3 0 1 . 0 1 5 2 : r AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration AVOCADO: AN AUDIOVISUAL VIDEO CAPTIONER"
        },
        {
            "title": "DRIVEN BY TEMPORAL ORCHESTRATION",
            "content": "Xinlong Chen2,3,1, Yue Ding2,3, Weihong Lin1, Jingyun Hua1, Linli Yao4, Yang Shi4, Bozhou Li4, Yuanxing Zhang1, Qiang Liu2,3 1Kling Team, Kuaishou Technology 2New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA) 3School of Artificial Intelligence, University of Chinese Academy of Sciences 4Peking University 5Nanjing University , Pengfei Wan1, Liang Wang2,3, Tieniu Tan2,3,5 Project webpage: https://avocado-captioner.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, powerful AudioVisual video Captioner Driven by the temporal Orchestration between audio and visual modalities. We propose two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings."
        },
        {
            "title": "INTRODUCTION",
            "content": "In the era of multimodal large language models (MLLMs), video captioning plays critical role in advancing video understanding. In addition to facilitating the alignment of multimodal representations during pretraining (Xu et al., 2021; Li et al., 2024), it also functions as key mechanism for injecting semantic knowledge into downstream video understanding and generation tasks (Sun et al., 2019; Hong et al., 2022; Zhang et al., 2025b). Recent studies (Chen et al., 2024; 2025b; Zhang et al.; Wang et al., 2025b) have shown that training with higher-quality video captions not only improves captioning performance, but also yields consistent improvements across broad spectrum of downstream applications. Therefore, advancing the capabilities of video captioning models offers foundational pathway toward building more powerful video understanding and generation systems. Despite notable progress in recent video captioning models (Xu et al., 2024; Chai et al., 2024; Yuan et al., 2025; Shi et al., 2025; Ren et al., 2024), most existing approaches remain predominantly vision-centric, often overlooking the rich semantic cues embedded in audio signals. In practice, auditory elements, such as dialogues, voiceovers, and background music, are indispensable for achieving holistic and contextually grounded understanding of video content. truly comprehensive captioning model should therefore integrate and reason jointly over both visual and auditory modalities. common workaround for vision-only models is to generate an independent audio caption via separate audio model and concatenate it to the visual description. However, such decoupled strategy inherently fails to model fine-grained temporal alignment and causal interplay between audiovisual events, limiting its reliability in practical applications. This work was conducted during the authors internship at Kling Team, Kuaishou Technology Corresponding author: qiang.liu@nlpr.ia.ac.cn 1 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 1: Schematic illustration of the pilot experiment. In this example, naively concatenating captions from the video and audio modalities fails to yield correct answer to the corresponding question. In contrast, jointly processing both modalities to generate time-aligned caption provides sufficient information, as indicated by the underlined text. To validate the importance of audiovisual alignment, we conduct pilot experiment on DailyOmni (Zhou et al., 2025). Using Gemini-2.5-Pro (Comanici et al., 2025), we generate two types of captions: one by processing visual and audio inputs separately and then concatenating their resulting captions; and the other by jointly processing both modalities to produce temporal-aligned caption. judge model (also Gemini-2.5-Pro) is then tasked with answering questions based solely on the textual captions. As shown in Fig. 1, the joint approach yields significant performance improvement, with an average accuracy gain of 15.8%. This gap is even more pronounced in the AV Event Alignment category, where it reaches 27.8%, underscoring the critical necessity of audiovisual temporal alignment in captions for comprehensively understanding the video content. Based on the above analysis, we propose AVoCaDO, an audiovisual video captioner that effectively integrates visual and auditory events in temporal synchrony. Built upon Qwen2.5-Omni (Xu et al., 2025a), which aligns visual and audio signals via interleaved token sequences, AVoCaDO is further enhanced through two-stage post-training pipeline: (1) AVoCaDO SFT, where we collect and construct dataset of 107K high-quality audiovisual video-caption pairs for supervised fine-tuning, with particular emphasis on temporal alignment between visual and audio events during caption generation; (2) AVoCaDO GRPO, where we introduce reward function based on key event alignment to optimize the temporal coherence of audio and visual information. Additionally, we design two auxiliary rewards to further enhance dialogue accuracy, reduce repetition collapse and regulate caption length. Collectively, these optimizations tailor AVoCaDO to generate captions that are not only semantically rich but also temporally aligned with audiovisual inputs. Extensive experiments demonstrate that AVoCaDO significantly outperforms existing open-source models across multiple audiovisual captioning benchmarks, and achieves competitive performance on the VDC Detailed subset (Chai et al., 2024) and DREAM-1K (Wang et al., 2024), which evaluate captions in visualonly settings. Our contributions can be summarized as follows: We propose AVoCaDO, powerful audiovisual video captioner that effectively integrates visual and auditory events with strong emphasis on temporal alignment. This model will be opensource to facilitate future research in more video understanding and generation tasks. We design two-stage post-training pipeline for AVoCaDO: (1) AVoCaDO SFT, leverages 107K high-quality audiovisual caption dataset to enhance temporal alignment; and (2) AVoCaDO 2 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration GRPO, which employs carefully designed reward functions to improve temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Extensive experiments show that AVoCaDO outperforms all existing open-source audiovisual models and even surpasses the commercial Gemini-2.5-Pro on UGC-VideoCap (Wu et al., 2025). It also achieves competitive performance under visual-only settings."
        },
        {
            "title": "2.1 VIDEOLLMS FOR VIDEO CAPTIONING",
            "content": "Recent advances in Video Large Language Models (VideoLLMs) (Bai et al., 2025; Zhang et al.; OpenBMB, 2025; Zhang et al., 2025a) have substantially enhanced progress in video captioning. These VideoLLM-based captioners (Ren et al., 2025; Xue et al., 2025; Yao et al., 2024) typically employ video encoder to capture video semantics and then bridge them with an LLM to generate high-quality captions. To further describe fine-grained video cues, OwlCap (Zhong et al., 2025) and Tarsier series (Wang et al., 2024; Yuan et al., 2025) construct large-scale, high-quality SFT datasets to enable the generation of detailed captions that balance dynamic motion and static detail. However, most of these efforts are vision-centric, while neglecting audio content, which plays vital role in forming comprehensive understanding of video content. Although several recent audiovisual VideoLLMs (Cheng et al., 2024; Geng et al., 2025; Liu et al., 2025; Sun et al., 2024) have incorporated both modalities, they are not specifically optimized for the captioning task. Concurrent to our work, video-SALMONN-2 (Tang et al., 2025) and UGC-VideoCaptioner (Wu et al., 2025) have also explored audiovisual video captioning. Nevertheless, the former requires computationally intensive post-training involving six rounds of DPO with sample pairs selected solely based on atomic event metrics, while the latter is limited to short-form user-generated videos. In contrast, our AVoCaDO achieves precise temporal alignment of audiovisual events through relatively lightweight training process guided by more holistic audiovisual considerations, and is capable of generating temporally synchronized, high-quality captions across diverse scenarios. 2.2 REINFORCEMENT LEARNING FOR VIDEOLLMS Reinforcement Learning (RL) (Christiano et al., 2017) has attracted increasing attention in VideoLLMs for enhancing complex reasoning through explicit thinking chains and verifiable reward designs. Video-R1 (Feng et al., 2025b), VerIPO (Li et al., 2025b), and LongVILA-R1 (Chen et al., 2025c) adopt GRPO (Shao et al., 2024) with rule-based rewards to improve performance on general video understanding tasks. Similarly, Time-R1 (Wang et al., 2025c), TAR-TVG (Guo et al., 2025), and Tempo-R0 (Yue et al., 2025) introduce IoU-related rewards to advance temporal grounding. However, these task-specific approaches are not well-suited for detailed video captioning. Verifying long video descriptions remains challenging, as they are prone to visual omissions and hallucinations. At present, only few RL-based methods explicitly target video captioning. VideoChat-R1 (Li et al., 2025a) leverages event-recall rewards to improve caption quality. VersaVid-R1 (Chen et al., 2025a) balances the accuracy and completeness of captions through meticulously designed reward mechanism. VideoCap-R1 (Meng et al., 2025) decomposes captioning into structured thinking and caption generation stages, integrating thinking and captioning scorers to improve output quality. In summary, these studies focus on only specific aspects of visual-only captioning. By contrast, our work proposes holistic reward design to enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse, which is tailored for audiovisual video captioning, resulting in significant gains in fine-grained caption quality across multiple dimensions."
        },
        {
            "title": "3 AVOCADO",
            "content": "AVoCaDO is powered by carefully designed post-training pipeline tailored specifically for audiovisual video captioning. This pipeline consists of two sequential stages: the AVoCaDO SFT stage, followed by the AVoCaDO GRPO stage. We select Qwen2.5-Omni-7B as the base model for its built-in ability to align video frames and audio signals using interleaved token sequencing. 3 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 2: The pipeline for generating high-quality temporally-aligned audiovisual video captions. For clarity, corresponding audio-visual events before and after fusion are marked with circled numbers and underlined for reference. 3.1 AVOCADO SFT In this stage, we train the base model using 107K high-quality audiovisual video-caption pairs curated by us. The dataset is constructed by collecting videos from diverse sources and pairing them with meticulously generated captions. The curation procedure is described below. To enhance the models capability in describing complex audiovisual interactions, we collect shortform videos rich in auditory elements, including mixed speech, background music, and sound effects. Specifically, we source 24K videos from TikTok-10M (Company, 2025) and 18K from ShortVideo (Shang et al., 2025), both of which offer dense, real-world audiovisual scenarios ideal for audiovisual understanding. To further strengthen the models grasp of multi-scene spatio-temporal dynamics and cinematic transitions, we randomly sample 20K videos from Shot2Story (Han et al., 2023). Additionally, we incorporate 29K samples from FineVideo (Farre et al., 2024), 11K from YouTube-Commons (Pierre-Carl, 2024), and 5K from CinePile (Rawal et al., 2024) to further improve the models generalization performance across diverse audiovisual contexts. Although the pilot experiment confirms the importance of audiovisual joint captioning, we observe that directly generating such joint captions may sometimes lead to information omissions from either the audio or visual stream (see App. D.1 for details). To obtain semantically rich and temporally aligned captions, we adopt two-stage captioning strategy, as illustrated in Fig. 2. First, we utilize Gemini-2.5-Pro to generate modality-specific captions separately from the video frames and the audio track. These separate captions, along with the original video, are then fed back into Gemini2.5-Pro to be synthesized into temporally coherent multimodal caption by aligning events across modalities according to the temporal structure of the video. Finally, quality checker is employed to ensure high data quality. Initially, clearly low-quality captions, such as those with inappropriate length or repetitive patterns, are filtered out. The remaining samples then undergo completeness assessment, where both the preand post-synthesis captions are presented to GPT-4.11 for scoring on 15 scale based on synthesis completeness. Only samples scoring 4 or above are retained, thereby reducing the risk of critical information loss during multimodal fusion. 3.2 AVOCADO GRPO To further enhance the models capabilities in audiovisual video captioning, we adopt the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024), training the model on randomly selected subset of 2K samples from our SFT dataset. As shown in Fig. 3, we design three complementary reward functions to guide the optimization process: (1) checklist-based reward that promotes comprehensive coverage of audiovisual keypoints; (2) dialogue-based reward that 1https://platform.openai.com/docs/models/gpt-4.1 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 3: Illustration of the three rewards RC, RD, and RL, which are specifically designed for improving the quality of audiovisual video captioning. targets the ASR fidelity and speaker identification accuracy of dialogues, critical component of audiovisual content; and (3) length-regularized reward that mitigates repetition collapse and regulates caption length. These reward functions complement each other and work synergistically to optimize various critical aspects for enhancing the overall captioning quality. 3.2.1 GROUP RELATIVE POLICY OPTIMIZATION GRPO significantly reduces both training time and GPU memory usage by eliminating the need for separate critic model in Proximal Policy Optimization (PPO). Specifically, GRPO works by sampling group of responses {o1, o2, ..., oG} for each question from the old policy model πθold, then computing their corresponding rewards {r1, r2, ..., rG} to derive the advantage function Ai for response oi: Ai = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) The current policy model πθ is then optimized using the following objective function: GRPO(θ) = {oi}G i=1πθold (oiq) (cid:34) 1 (cid:88) (cid:18) i=1 min Ai, (cid:16) πθ(oiq) πθold(oiq) (cid:19)(cid:35) β DKL (πθπref) , clip (cid:16) πθ(oiq) πθold(oiq) , 1 ε, 1 + ε (cid:17) (cid:17) Ai 3.2.2 CHECKLIST-BASED REWARD (1) (2) To enhance the overall completeness of audiovisual video captioning, we propose checklist-based reward Rc grounded in fine-grained content decomposition. Specifically, each ground-truth caption Sgt is pre-decomposed by GPT-4o into structured inventory of keypoints = {k1, k2, . . . , kn}, with = indicating the inventory size. These keypoints are organized according to comprehensive taxonomy spanning five core dimensions tailored to audiovisual caption: Cross-modal Narrative Logic: High-level coherence in which auditory and visual modalities mutually explain, complement, or guide each other to reveal underlying intent or storyline; explicit temporal alignment between modalities is required. Dynamic Action & Interaction: Motions, events, and pairwise or group interactions among entities, capturing the evolving narrative dynamics of the scene. 5 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Auditory Elements: All sound-related content, including speech, music, and ambient or diegetic sound effects, which is essential for holistic multimodal comprehension. Spatio-temporal & Cinematography: Structural and stylistic features, such as scene transitions, temporal progression, and camera techniques that shape perceptual and narrative flow. Static Entity Description: Attributes and spatial configurations of relatively stationary entities, including persons, objects, and environmental elements. During GRPO training, for generated caption Sgen, the checklist-based reward Rc is defined as: Rc(Sgen K) = 1 K (cid:88) i=1 Judge(Sgen, ki) (3) where Judge(Sgen, ki) {0, 1} is the matching score assigned by judge model, specifically, GPT4.1, indicating whether Sgen correctly mentions keypoint ki."
        },
        {
            "title": "3.2.3 DIALOGUE-BASED REWARD",
            "content": "In parallel, dialogue serves as critical component of audiovisual content. Therefore, we further design dialogue-based reward RD to ensure the ASR fidelity and speaker identification accuracy of dialogue in captions. As shown in Fig. 3, we first extract and structure dialogues from captions as list using Gemini-2.5Pro, where each entry consists of speaker and their corresponding spoken content. Let the modelgenerated dialogue sequence be denoted as Dgen = (cid:2)(sgen )(cid:3), , cgen , cgen 1 and the ground-truth dialogue sequence as Dgt = (cid:2)(sgt )(cid:3), where 1 , cgt 1 ), (sgt represents the speaker, is the spoken content of the i-th dialogue unit, and and are the lengths of the two sequences, respectively. , cgen 2 2 ), . . . , (sgt ), . . . , (sgen , cgt ), (sgen 2 2 , cgt To compute RD, we need to simultaneously consider the speaker similarity Sspeaker and content similarity Scontent between Dgen and Dgt. To this end, we adopt two-step strategy: first, we match dialogue units based on content similarity; then, we verify speaker consistency for the matched pairs. For any dialogue content pair (cid:0)cgen (cid:1), where [1, ] and [1, ], their content similarity Sim(cid:0)cgen (cid:1) is measured using the edit distance2 between the two strings, calculated as: , cgt , cgt Sim(cid:0)cgen , cgt (cid:1) = 1 edit distance(cid:0)cgen len(cid:0)cgen , cgt (cid:1), len(cid:0)cgt (cid:16) (cid:1) (cid:1)(cid:17) max (4) where len() denotes the string length. Our goal is to identify subsequence of dialogue units from Dgen that matches positionally with subsequence of the same length from Dgt, such that the content similarity Sim() of each aligned pair exceeds predefined threshold γ, and the total content similarity Scontent is maximized. The search for this optimal subsequence is analogous to the classical Longest Common Subsequence (LCS)3 problem and can be solved via dynamic programming. Let Fi,j represent the maximum total content similarity achievable from the first dialogue units of Dgen and the first dialogue units of Dgt. The transition equation is defined as follows: Fi,j = 0 max max (cid:110) (cid:111) Fi1,j, Fi,j1 (cid:110) Fi1,j, Fi,j1, Fi1,j1 + Sim(cid:0)cgen if = 0 or = 0 if > 0, > 0, Sim(cid:0)cgen if > 0, > 0, Sim(cid:0)cgen , cgt , cgt (cid:1) < γ (cid:1) γ (cid:1)(cid:111) , cgt where the similarity threshold γ is set to 0.6. After identifying the optimal matched subsequence based on the dialogue content, we further assess speaker consistency (assigned as 0 or 1) for each matched pair based on the video content 2https://en.wikipedia.org/wiki/Edit_distance 3https://en.wikipedia.org/wiki/Longest_common_subsequence 6 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration using Gemini-2.5-Pro, and the total number of correctly matched speaker pairs serves as the speaker similarity Sspeaker. The final similarity Scombined between the two sequences is then calculated as: Scombined = (Sspeaker + Scontent) (cid:14) 2 From physical interpretation, Scombined represents the proportion of correct dialogue units in Dgen, which takes values in the range (cid:2)0, min(M, )(cid:3). The recall and precision are then computed as: (5) Rec = Scombined (cid:14) M, Prec = Scombined (cid:14) The final dialogue-based reward RD is defined as the F1 score: RD = 2 Prec Rec (cid:14) (Prec + Rec)"
        },
        {
            "title": "3.2.4 LENGTH-REGULARIZED REWARD",
            "content": "(6) (7) For video captioning, output repetition collapse remains frequently observed issue (Li et al., 2023; Yao et al., 2025). Moreover, in practical deployment scenarios, it is essential to balance inference efficiency with caption quality, which often necessitates maintaining moderate output length. To mitigate the rate of repetition collapse and enhance inference efficiency, we design lengthregularized reward RL that encourage complete captions while penalizing excessive length. The thresholds τ1 and τ2 are set to 2048 and 4096 respectively, which is analyzed in App. D.2. RL = 1.0, 1 0.0, len(Sgen) τ1 τ2 τ1 , if len(Sgen) < τ1 if τ1 len(Sgen) < τ2 otherwise (8) During GRPO training, we use the sum of the aforementioned three rewards as the final reward R. = RC + RD + RL (9)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS 4.1.1 BASELINES First, we consider two concurrent works focusing on audiovisual video captioning, videoSALMONN-2 and UGC-VideoCaptioner, as important baselines. We also evaluate several popular general-purpose audio-visual understanding models, covering both open-source (Qwen2.5Omni (Xu et al., 2025a), HumanOmniV2 (Yang et al., 2025), ARC-Hunyuan-Video (Ge et al., 2025), MiniCPM-o-2.6 (OpenBMB, 2025)) and commercial options (Gemini-2.5 (Comanici et al., 2025) series). To further assess the importance of audio modality, we compare against some strong visiononly models, including Qwen2.5-VL (Bai et al., 2025), InternVL3.5 (Wang et al., 2025a). In addition, we include the newly released large-scale Mixture-of-Experts (MoE)-based Qwen3-Omni (Xu et al., 2025b) series in our evaluation. 4.1.2 BENCHMARKS For audiovisual video captioning, we evaluate models on video-SALMONN-2 testset, UGCVideoCap, Daily-Omni and WorldSense (Hong et al., 2025). The former two benchmarks evaluate caption quality directly, while the latter two are originally designed for audiovisual video questionanswering (QA). To adapt these QA-oriented benchmarks for caption evaluation, we first use the target model to generate caption for each video, and then utilize judge model (Gemini-2.5-Pro) to answer questions solely based on the textual captions. To mitigate answer guessing when the caption lacks necessary information, we instruct the judge model to refrain from answering such questions, which are then marked as incorrect samples. Additionally, we evaluate models on the detailed subset of the VDC benchmark and DREAM-1K under visual-only setting. 4https://platform.openai.com/docs/models/gpt-3.5-turbo 7 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Model Size Modality video-SALMONN-2 testset UGC-VideoCap Miss Hall. Total Audio Visual Detail Avg. Gemini-2.5-Pro Gemini-2.5-Flash InternVL3.5 Qwen2.5-VL HumanOmniV2 ARC-Hunyuan-Video Qwen2.5-Omni MiniCPM-o-2.6 video-SALMONN-2 UGC-VideoCaptioner - - 8B 7B 7B 7B 7B 8B 7B 3B Qwen3-Omni-Instruct Qwen3-Omni-Captioner 30B-A3B 30B-A3B AVoCaDO (Ours) 7B + + A + + + + + + + + A + 18.1 19.3 53.8 40.5 49.2 45.7 41.7 42.2 21.2 31.6 32.0 31.0 21. 13.3 13.9 25.5 17.0 12.3 12.5 15.4 14.3 17.6 17.0 13.6 16.6 16.2 31.3 33. 79.4 57.5 61.6 58.2 57.1 56.5 38.8 48.6 45.6 47.6 37.3 69.5 69.1 47.9 46. 45.6 52.7 46.9 38.6 61.8 61.4 67.5 69.0 73.0 74.7 75.8 64.8 69.1 66.3 56.0 66.1 68.5 71.4 58. 74.8 75.5 74.6 73.7 74.0 59.5 62.3 59.5 55.8 60.0 57.7 68.5 57.5 72.3 72. 71.8 72.6 73.0 57.4 59.3 57.1 54.8 57.7 54.9 67.2 59.1 71.5 72.5 73. Table 1: Model performance on the audiovisual video captioning benchmarks. and refer to the audio and visual modalities, respectively. The results presented above are reproduced using the official code. Note that the video-SALMONN-2 testset originally employed GPT-3.54as the judge model, which occasionally led to misjudgments. To ensure more reliable evaluation, we uniformly replaced it with GPT-4.1. Concurrent works with us. 4.2 EXPERIMENTAL RESULTS 4.2.1 DIRECT CAPTION EVALUATION We first evaluate the audiovisual video captioning performance on the video-SALMONN-2 testset and the UGC-VideoCap benchmark, which employ different metrics to directly assess caption quality. As shown in Tab. 1, our AVoCaDO achieves state-of-the-art performance among all open-source models on both benchmarks. Notably, while some open-source models, such as HumanOmniV2, exhibit slightly lower Hallucination rate compared to AVoCaDO on the Video-SALMONN-2 testset, this is because these models are not specifically optimized for detailed captioning and tend to produce overly brief descriptions that fail to convey the full content of the video, leading to significantly higher Miss rate and weaker performance on UGC-VideoCap. In contrast, AVoCaDO strikes better balance between comprehensiveness and accuracy, ultimately outperforming all open-source models in both the Total metric on the Video-SALMONN-2 testset and the average score on UGC-VideoCap. Moreover, compared to the latest large-scale MoE-based omni model, Qwen3-Omni, AVoCaDO still demonstrates better performance. Remarkably, AVoCaDO even surpasses the Gemini-2.5 series on UGC-VideoCap, highlighting its strong capability in audiovisual video captioning. 4.2.2 QA-BASED CAPTION EVALUATION Size DailyOmni WorldSense Model Gemini-2.5-Pro Gemini-2.5-Flash HumanOmniV2 ARC-Hunyuan-Video MiniCPM-o-2.6 Qwen2.5-Omni UGC-VideoCaptioner video-SALMONN-2 - - 7B 7B 8B 7B 3B 7B Qwen3-Omni-Instruct Qwen3-Omni-Captioner 30B-A3B 30B-A3B AVoCaDO (Ours) 7B 60.2 55.3 8.2 8.6 9.8 13.4 17.0 29.9 17.5 27.2 50.1 33.8 31. 6.6 8.7 7.2 8.6 11.2 18.2 12.7 14.1 25.7 Table 2: QA performance by Gemini-2.5-Pro based on textual captions. To mitigate answer guessing when the caption lacks necessary information, the model is instructed to refrain from answering such questions, which are then marked as incorrect samples. The Daily-Omni and Worldsense benchmarks feature challenging questions that require comprehension of either one or both modalities, along with their temporal relationships. To assess caption quality, we employ judge model (Gemini-2.5-Pro) that answers these questions based solely on the AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration textual captions. To reduce speculative answers when the caption lacks essential information, we instruct the judge model to refrain from answering such questions, which are then marked as incorrect. As shown in Tab. 2, AVoCaDO significantly outperforms existing open-source models of comparable size, as well as the latest largescale MoE-based Qwen3-Omni seachieving performance imries, provements of 20.2% on DailyOmni and 7.5% on Worldsense over the strongest baseline models. Model Size VDC Detailed DREAM-1K Acc VDCscore F1 score VideoLLaMA 3 ShareGPT4Video AuroraCap Qwen2.5-VL Qwen2.5-Omni video-SALMONN-2 AVoCaDO (Ours) 7B 8B 7B 7B 7B 7B 7B 33.4 35.6 41.3 44.5 39.7 46.1 47. 1.9 1.8 2.2 2.4 2.2 2.5 2.5 30.5 19.5 20.8 30.1 31.6 34.4 35. Additionally, we further evaluate models on the VDC Detailed subset and DREAM-1K, two benchmarks that are specifically designed to measure the captioning performance for visual-only videos. As reported in Tab. 3, AVoCaDO also demonstrates competitive performance in this setting. Table 3: Model performance on the VDC Detailed subset and DREAM-1K, which evaluate captions in visual-only settings. Model Qwen2.5-Omni AVoCaDO-SFT AVoCaDO-SFT-2K AVoCaDO-GRPO Reward video-SALMONN-2 testset Daily-Omni by caption RD RC RL Total Dlg. F1 RepCol (%) Avg. Dlg. F1 RepCol (%) 57.1 41.4 43. 41.3 37.3 37.3 7.1 74.4 74.1 76.5 75.9 76.9 7.1 3.5 2. 2.4 3.9 0.4 13.4 48.1 48.5 49.5 49.5 50.1 16.9 73.6 74. 76.1 75.2 76.2 8.1 5.1 5.3 6.0 4.9 1.0 Table 4: Ablation study on our post-training pipeline. Dlg. F1 represents the metric of dialogue quality, computed as in Eq. 7. RepCol indicates the ratio of generations exhibiting repetition collapse. AVoCaDO-SFT-2K refers to the model further fine-tuned on AVoCaDO-SFT using the same 2K samples employed during the GRPO phase. 4.2.3 ABLATION STUDIES In Tab. 4, we conduct an in-depth analysis of each component within our post-training pipeline. First, the AVoCaDO-SFT stage significantly enhances the models overall performance across three key dimensions: benchmark scores, dialogue quality, and the reduction of repetition collapse in captions. These improvements are consistent on both the video-SALMONN-2 testset, where captions are evaluated directly, and the Daily-Omni benchmark, which assesses caption quality through QA task. This uniform improvement underscores the effectiveness of our SFT data construction strategy. In the AVoCaDO-GRPO stage, incorporating the dialogue-based reward RD improves the dialogue F1-score by over 2% on both benchmarks. Additionally, the accuracy on Daily-Omni is also enhanced by 1.4%, which is attributed to the models improved ability to generate detailed and precise dialogue content for answering specific questions. Concurrently, the checklist-based reward RC significantly reduces the total error rate on the video-SALMONN-2 testset, underscoring its effectiveness in capturing key audiovisual events. Finally, the length-regularized reward RL not only markedly alleviates repetition collapse but also boosts performance across other metrics, highlighting its dual benefit of ensuring conciseness and quality. To further validate the contribution of these tailored rewards, we additionally fine-tune AVoCaDOSFT on the same 2K data used in GRPO, yielding AVoCaDO-SFT-2K. However, the model shows no significant performance gains and even exhibits notable degradation on the video-SALMONN2 testset. These results suggest that the performance gains stem from the curated reward functions rather than the data volume, confirming their efficacy in advancing audiovisual captioning. 9 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 4: An illustration of video caption generated by AVoCaDO, featuring both precise audiovisual temporal alignment and accurate dialogue rendering. 4.2.4 QUALITATIVE ANALYSIS Fig. 4 shows caption generated by AVoCaDO, highlighting its strong capabilities in audiovisual temporal alignment and precise representation of dialogues. More cases can be found in App. E."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work concentrates on the task of audiovisual video captioning. Initially, we highlight the significant role of temporal alignment between visual and audio events. Informed by this observation, we introduce AVoCaDO, an audiovisual video captioner driven by the temporal alignment between audio and visual modalities. Building upon Qwen2.5-Omni, AVoCaDO is enhanced through twostage post-training strategy: AVoCaDO SFT, which fine-tunes the model on 107K high-quality audiovisual caption dataset emphasizing temporal alignment, and AVoCaDO GRPO, which leverages tailored reward functions to further boost temporal coherence and dialogue accuracy while reducing repetition collapse and regulating caption length. Experimental results demonstrate that AVoCaDO substantially outperforms existing open-source models on four audiovisual video captioning benchmarks and delivers competitive results on the VDC Detailed subset and DREAM-1K, which focus on visual-only video captioning. Ablation studies validate the effectiveness of each component in our training pipeline, underscoring the overall effectiveness of our approach. 10 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration"
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, JenqNeng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, and Tieniu Tan. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025a. Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, and Tieniu Tan. Vidcapbench: comprehensive benchmark of video captioning for controllable text-to-video generation. arXiv preprint arXiv:2502.12782, 2025b. Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025c. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. The Data Company. Tiktok-10m: large-scale short video dataset for video understanding, 2025. URL https://huggingface.co/datasets/The-data-company/ TikTok-10M. dataset of 10 million TikTok posts for multimodal learning and social media analysis. Miquel Farre, Andi Marafioti, Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Finevideo. https://huggingface.co/datasets/HuggingFaceFV/finevideo, 2024. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025a. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025b. Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Arc-hunyuan-video-7b: Structured video comprehension of real-world shorts. arXiv preprint arXiv:2507.20939, 2025. AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng. Longvale: Vision-audio-language-event benchmark towards time-aware omni-modal perception of In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. long videos. 1895918969, 2025. Chaohong Guo, Xun Mo, Yongwei Nie, Xuemiao Xu, Chao Xu, Fei Yu, and Chengjiang Long. Tartvg: Enhancing vlms with timestamp anchor-constrained reasoning for temporal video grounding. arXiv preprint arXiv:2508.07683, 2025. Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. arXiv preprint arXiv:2312.10300, 2023. Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. arXiv preprint arXiv:2502.04326, 2025. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, and Yixuan Su. Repetition in repetition out: Towards understanding neural text degeneration from the data perspective. Advances in Neural Information Processing Systems, 36:7288872903, 2023. Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, and Qi Liu. Temporal reasoning transfer from text to video. arXiv preprint arXiv:2410.06166, 2024. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025a. Yunxin Li, Xinyu Chen, Zitao Li, Zhenyu Liu, Longyue Wang, Wenhan Luo, Baotian Hu, and Min Zhang. Veripo: Cultivating long reasoning in video-llms via verifier-gudied iterative policy optimization. arXiv preprint arXiv:2505.19000, 2025b. Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model. arXiv preprint arXiv:2502.04328, 2025. Desen Meng, Rui Huang, Zhilin Dai, Xinhao Li, Yifan Xu, Jun Zhang, Zhenpeng Huang, Meng Zhang, Lingshu Zhang, Yi Liu, et al. Videocap-r1: Enhancing mllms for video captioning via structured thinking. arXiv preprint arXiv:2506.01725, 2025. OpenBMB. Minicpm-o 2.6: gpt-4o level mllm for vision, speech, and multimodal live streaming on your phone. https://github.com/OpenBMB/MiniCPM-V, 2025. Langlais Pierre-Carl. versational and multimodal data. youtube-commons, 2024. Releasing youtube-commons: massive open corpus for conhttps://huggingface.co/blog/Pclanglais/ Ruchit Rawal, Khalid Saifullah, Miquel Farre, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1431314323, 2024. Yiming Ren, Zhiqiang Lin, Yu Li, Gao Meng, Weiyun Wang, Junjie Wang, Zicheng Lin, Jifeng Dai, Yujiu Yang, Wenhai Wang, et al. Anycap project: unified framework, dataset, and benchmark for controllable omni-modal captioning. arXiv preprint arXiv:2507.12841, 2025. Yu Shang, Chen Gao, Nian Li, and Yong Li. large-scale dataset with behavior, attributes, and content of mobile short-video platform. In Companion Proceedings of the ACM on Web Conference 2025, pp. 793796, 2025. 12 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, et al. Mavors: Multi-granularity video representation for multimodal large language model. arXiv preprint arXiv:2504.10068, 2025. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: joint model for video and language representation learning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 74647473, 2019. Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, and Chao Zhang. video-salmonn 2: Captioning-enhanced audio-visual large language models. arXiv preprint arXiv:2506.15220, 2025. Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Internvl3. 5: Advancing open-source multimodal Linglin Jing, Shenglong Ye, Jie Shao, et al. models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Xiao Wang, Jingyun Hua, Weihong Lin, Yuanxing Zhang, Fuzheng Zhang, Jianlong Wu, Di Zhang, and Liqiang Nie. HAIC: Improving human action understanding and generation with better captions for multi-modal large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1015810181, 2025b. Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025c. Peiran Wu, Yunze Liu, Zhengdong Zhu, Enmin Zhou, and Shawn Shen. Ugc-videocaptioner: An omni ugc video detail caption model and new benchmarks. arXiv preprint arXiv:2507.11336, 2025. Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Yifan Xu, Xinhao Li, Yichun Yang, Desen Meng, Rui Huang, and Limin Wang. Carebench: finegrained benchmark for video captioning and retrieval. arXiv preprint arXiv:2501.00513, 2024. Zihui Xue, Joungbin An, Xitong Yang, and Kristen Grauman. Progress-aware video frame captioning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 13639 13650, 2025. Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, and Jingren Zhou. Humanomniv2: From understanding to omni-modal reasoning with context. arXiv preprint arXiv:2506.21277, 2025. 13 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, and Di Wang. Understanding the repeat curse in large language models from feature perspective. arXiv preprint arXiv:2504.14218, 2025. Linli Yao, Yuanmeng Zhang, Ziheng Wang, Xinglin Hou, Tiezheng Ge, Yuning Jiang, Xu Sun, and Qin Jin. Edit as you wish: Video caption editing with multi-grained user control. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 19241933, 2024. Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025. Feng Yue, Zhaoxing Zhang, Junming Jiao, Zhengyu Liang, Shiwen Cao, Feifei Zhang, and Rong Shen. Tempo-r0: video-mllm for temporal video grounding through efficient temporal sensing reinforcement learning. arXiv preprint arXiv:2507.04702, 2025. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, et al. Unified multimodal understanding and generation models: Advances, challenges, and opportunities. arXiv preprint arXiv:2505.02567, 2025b. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Llava-video: Video instruction tuning with synthetic data. Transactions on Machine Learning Research. Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, and Xiang Bai. Owlcap: Harmonizing motion-detail for video captioning via hmd-270k and caption set equivalence reward. arXiv preprint arXiv:2508.18634, 2025. Ziwei Zhou, Rui Wang, and Zuxuan Wu. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. arXiv preprint arXiv:2505.17862, 2025. 14 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration"
        },
        {
            "title": "A DETAILS OF THE TRAINING DATA",
            "content": "The videos used for our training dataset construction come from multiple sources to ensure diverse audiovisual content. Below we provide detailed statistics for each dataset: TikTok-10M is large-scale dataset containing 10 million short-form posts from TikTok. The dataset reflects authentic patterns of modern short-form videos, including diverse visual styles, short durations, rich background music and voiceovers, and wide variety of themes such as entertainment, dance, humor, beauty, and pets. From the full dataset, we select 24K videos for our model training, ensuring representative coverage of content, audio-visual styles, and usergenerated characteristics. Shot2Story is dataset comprises 43K multi-shot videos. The length of each video is ranging from 10s to 40s. 20K videos are chosen from the dataset. Each video in the dataset contains multiple shots. This rich multi-shot structure allows our audiovisual caption model to learn to capture key events in each shot and associate them together. ShortVideo is also large-scale video dataset from short-video platform including 153,561 videos. These videos have varying durations, ranging from under 30 seconds to over 5 minutes, with most being less than one minute. We randomly choose 18k videos from the dataset for training our model. FineVideo is dataset with 43K videos that span 3.4K hours. The videos in the dataset are carefully filtered to retain dynamic content with both visual actions and mid-fast pace spoken language by word density filtering and visual dynamism filtering methods. We select 29K videos from this dataset. YouTube-Commons is collection of audio transcripts of 2,063,066 videos shared on YouTube under CC-By license. The corpus is multilingual, with English as the majority language, and provides automatic translations into several languages such as French, Spanish, German, Russian, Italian, and Dutch. Each video is accompanied by detailed provenance information, including title, link, channel name, and upload date, ensuring transparency and reusability. We sample 11K videos from this dataset. CinePile is long-form video understanding dataset. The training set has 9,248 videos, from which we choose 5K videos. The videos are sourced from English-language films on the YouTube channel MovieClips, which provides self-contained clips."
        },
        {
            "title": "B DETAILS OF BENCHMARKS",
            "content": "In this section, we will provide detailed description of the benchmark we evaluated. video-SALMONN-2 testset comprises 483 videos spanning 14 distinct domains. Each video has duration ranging from 30 to 60 seconds, with an average length of 51 seconds. To evaluate caption quality, judge model is employed to process the generated caption along with the groundtruth event, which then identifies three types of errors: Missing Events, Incorrect Events, and Hallucination Events. The latter two are categorized as manifestations of model hallucination. The total error rate is then obtained by summing the missing rate and the hallucination rate. UGC-VideoCap consists of 1,000 short TikTok videos, each under 60 seconds in duration and containing at least one meaningful audio segment lasting no less than 5 seconds. Each videos caption is evaluated by judge model that assigns scores on 1-to-5 scale across three dimensions: visual, audio, and details. These dimension scores are then normalized and aggregated to produce final caption quality score. Daily-Omni is an audio-visual question answering benchmark comprising 684 videos depicting diverse everyday life scenarios, sourced from multiple platforms. These videos are densely multimodal, offering rich visual and auditory cues. The benchmark includes 1,197 multiple-choice question-answer pairs, distributed across six core tasks. In our experimental setting, we assess the quality of generated captions by feeding them into judge model and measuring their capacity to support accurate question answering. WorldSense exhibits tightly integrated coupling between audio and visual modalities, demanding that models effectively harness the synergistic perceptual power of omni-modal data. The AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration dataset comprises 1,662 temporally synchronized audio-visual clips, systematically categorized into eight distinct semantic domains. To facilitate comprehensive evaluation, it further includes 3,172 multiple-choice question-answer pairs spanning 26 diverse downstream tasks. In our experimental framework, we evaluate the quality of generated captions by feeding them into dedicated judge model and measuring their efficacy in enabling accurate question answering. VDC comprises 1,027 diverse videos. The captioning model is required to generate captions for each video along five distinct dimensions using five specific prompts; these five categories of captions are then fed into an evaluation model to answer questions, thereby assessing the captioning capability. In our experiments, we evaluate our model on the detailed subset. DREAM-1K is challenging benchmark for detailed video description, featuring 1,000 clips from diverse sources such as films, stock footage, and short-form videos. Each video is paired with fine-grained human-annotated descriptions, and evaluated using AutoDQ, metric better suited for assessing rich, multi-event narratives than traditional captioning scores."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "In the AVoCaDO SFT stage, the model is trained for 2 epochs with batch size of 128 and learning rate of 2 105. During the AVoCaDO GRPO stage, training is performed for 1 epoch with batch size of 64 and learning rate of 1105. For each query, we sample 8 responses using temperature of 1.0. The KL-divergence regularization coefficient β is set to 0.04, which is commonly used in previous works (Feng et al., 2025a). Both the video and audio encoders remain frozen throughout training, and only the adapters and the LLM backbone are updated. During both training and evaluation, video inputs are sampled at 2 fps, and the resolution of each frame is limited to maximum of 512 28 28 pixels. Due to the base models context window limitation of 32K tokens, the total video tokens is restricted to 25600 28 28. All training is conducted on 16 NVIDIA H200 GPUs, while evaluation is performed on NVIDIA H20 GPUs."
        },
        {
            "title": "D ADDITIONAL ANALYSIS",
            "content": "D.1 ANALYSIS OF THE AUDIOVISUAL VIDEO CAPTION GENERATION BY GEMINI In Fig. 6, we compare the audiovisual captions generated directly by Gemini-2.5-Pro with those produced by the two-stage audiovisual captioning approach used in constructing our SFT dataset (Sec. 3.1). The results indicate that direct caption generation tends to omit information from either the audio or visual modality, unlike the two-stage strategy, which provides more comprehensive coverage. To ensure high data quality, we therefore adopted the two-stage captioning method for building our SFT dataset. D.2 ANALYSIS OF THE THRESHOLDS IN LENGTH-REGULARIZED REWARD In this section, we detail the rationale for selecting the length thresholds τ1 = 2048 and τ2 = 4096 in the lengthregularized reward RL (Eq. 8). As preliminary, it is important to note that Qwen2.5-Omni supports maximum context window of 32K tokens and encodes audio at rate of 25 tokens per second. In our training and evaluation, to effectively capture video dynamics and preserve the visual detail of each frame, we sample videos at 2 fps, with each frame allocated maximum of 512 tokens for encoding. Due to the context window constraint, the total number of video tokens is capped at 25,600. The upper threshold, τ2 = 4096, is determined by the maximum feasible video duration that the model can process. Fig. 5 shows our analysis of the caption lengths generated by Gemini-2.5-Pro for videos of varying durations, which reveals that for videos up to 100 seconds, the maximum caption length rarely exceeds 3,982 tokens. 100-second high-resolution Figure 5: Distribution of caption token lengths across video durations. 16 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 6: Comparison between direct captioning and our proposed two-stage approach. Colored text highlights information present in the two-stage captions but absent in the direct captions, with audio-related and visual-related content distinguished accordingly. video consumes 2,500 audio tokens (100s 25 tokens/s) and the maximum 25,600 video tokens, totaling 28,100 tokens for multimodal input. When combined with the input text prompt and the generated caption, the total token count approaches the 32K context limit. To prevent context overflow and ensure the generation of complete and untruncated captions, we constrain our training dataset to videos of 100 seconds or less. Consequently, the maximum target output length, τ2, is set to 4096, providing safe margin. 17 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration The lower threshold, τ1 = 2048, is designed to strike balance between comprehensiveness and conciseness for practical applications. Fig. 5 shows that the mean caption lengths for videos under 100 seconds are below 1,437 tokens. Based on this observation, we set the first threshold τ1 at 2048, value comfortably above the average, to grant the maximum length reward to outputs of typical length. For captions with lengths between τ1 and τ2, the length reward decreases linearly. This reward structure incentivizes the model to autonomously learn trade-off between generating more detailed caption and optimizing other reward metrics related to factual accuracy and completeness."
        },
        {
            "title": "E ADDITIONAL QUALITATIVE RESULTS",
            "content": "In Figs. 7 and 8, we present qualitative comparisons of AVoCaDO against two contemporary captioning models, video-SALMONN-2 and UGC-VideoCaptioner. As shown in Fig. 7, video-SALMONN-2 contains multiple inaccuracies in dialogue recognition, misaligns the temporal order between the mans speech and scene transitions, and concludes with an unfitting summary. UGC-VideoCaptioner, on the other hand, omits dialogue content entirely and introduces redundant descriptions toward the end of the caption. Similarly, in Fig. 8, video-SALMONN-2 again fails to align auditory and visual events chronologically, only mentioning the audio content at the very end of the caption. Additionally, it misidentifies the speakers gender and overlooks the final narration segment. UGC-VideoCaptioner still neglects all spoken content, merely making generic reference to background music at the end of the caption. In contrast, leveraging an effective two-stage training pipeline, AVoCaDO generates high-quality audiovisual video captions that accurately synchronize audiovisual events temporally, faithfully transcribe dialogue content, and maintain strong semantic coverage in both cases."
        },
        {
            "title": "F DETAILS OF PROMPTS",
            "content": "F.1 PROMPTS TO GENERATE CAPTIONS FOR SFT Figs. 9 to 11 present the prompts used to generate video frame captions, audio captions, and to synthesize both, respectively, during the creation of the SFT caption data detailed in Sec. 3.1. F.2 PROMPTS TO DECOMPOSE CAPTIONS INTO KEYPOINTS In Fig. 12, we present the prompt used to decompose caption into keypoints, which is the foundation of the checklist-based reward detailed in Sec. 3.2.2. F.3 PROMPTS TO JUDGE KEYPOINT ACCURACY IN CAPTIONS As illustrated in Fig. 13, we present the prompt designed to assess whether keypoints are accurately described in caption, which is used to compute the checklist-based reward RC. F.4 PROMPTS TO EXTRACT DIALOGUES IN CAPTIONS In Fig. 14, we present the prompt used to extract dialogues in the caption, which is the foundation of the dialogue-based reward detailed in Sec. 3.2.3. F.5 PROMPTS TO IDENTIFY SPEAKER SUBJECT CONSISTENCY Fig. 15 shows the prompt to determine whether the speakers in each aligned pair refer to the same subject based on the video content, which is used to calculate the number of correctly matched speaker pairs Sspeaker. F.6 PROMPTS TO ANSWER QUESTIONS BY TEXTUAL CAPTIONS In Fig. 16, we provide the prompt used to assess the quality of caption by leveraging it to answer questions, as described in Sec. 4.2.2. 18 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 7: Qualitative comparison of AVoCaDO against two contemporary captioning models: videoSALMONN-2 and UGC-VideoCaptioner. Errors in baseline outputs are highlighted in red; the superior coverage and precision of AVoCaDO are highlighted in blue. Correct / incorrect audiovisual temporal alignment is bolded, while sound effect descriptions are underlined. 19 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration Figure 8: Qualitative comparison of AVoCaDO against two contemporary captioning models: videoSALMONN-2 and UGC-VideoCaptioner. Errors in baseline outputs are highlighted in red; the superior coverage and precision of AVoCaDO are highlighted in blue. Correct / incorrect audiovisual temporal alignment is bolded, while sound effect descriptions are underlined. 20 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration"
        },
        {
            "title": "Prompts to generate video frame caption",
            "content": "You are professional video caption writer. Your task is to create detailed, scene-by-scene narrative description of video. For each scene, your description must include the following elements: Main Subjects: Describe the people present, including their appearance, clothing, actions, and gestures. Setting & Background: Detail the environment, background, and any notable objects. On-Screen Graphics: Mention the specific content of any text, titles, or emojis that appear on the screen. Camera Work: Note any significant camera movements like zooms, pans, or angle changes. Figure 9: Prompts to generate video frame caption. Prompts to generate audio caption You are professional audio caption writer. Your task is to create detailed narrative description of an audio in the video. Your description must include the following elements: Narration / Dialogue: Please accurately transcribe the spoken words (narration or dialogue) from the audio. In addition to the transcription, describe the speakers tone and emotional delivery during the speechsuch as whether the tone is calm, excited, hesitant, enthusiastic, serious, sarcastic, etc.based on vocal cues like pitch, pace, volume, and emotion. Music & Sound: Describe the background musics mood and any important sound effects. The audio caption should be coherent and well-structured. Do not simply give the transcriptions without the speakers tone and emotions. Figure 10: Prompts to generate audio caption. Prompts to fuse the video frame caption and audio caption You are tasked with fusing the visual caption and audio caption into single, coherent narrative based on the video content. Follow these strict rules: 1. Preserve every single sentence from both the visual caption and audio caption exactly as they appear. 2. Do NOT omit or delete any sentence in any way. 3. You may reorder the sentences (from both captions) to create logical and temporally accurate sequence that reflects the videos events. 4. Ensure the integrated narrative flows naturally in time with the video, aligning visual actions with corresponding sounds or spoken content. Verify before responding: Did include every sentence from both captions? Visual caption: {visual caption} Audio caption: {audio caption} Now generate the integrated audio-visual caption: Figure 11: Prompts to fuse the video frame caption and audio caption. 21 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration"
        },
        {
            "title": "Prompts to decompose captions into keypoints",
            "content": "You are an expert assistant designed for fine-grained audiovisual content analysis. Your task is to decompose given video caption into structured, comprehensive, and non-redundant inventory of distinct keypoints. Extract and categorize fine-grained keypoints from the given video caption according to the following five audiovisual-specific dimensions. Ensure the keypoints are atomic, precise, and non-overlapping. 1. Static Entity Description: Attributes and spatial configurations of relatively stationary entities. This includes people, objects, animals, and environmental elements. 2. Dynamic Action & Interaction: Motions, events, and pairwise or group interactions among entities that describe the evolving narrative. 3. Auditory Elements: All sound-related content, including speech, music, and ambient or diegetic sound effects, which is essential for holistic multimodal comprehension. 4. Spatio-temporal & Cinematography: Structural, stylistic, and temporal features of the video, including scene settings, transitions, temporal progression, and camera techniques. Cross-modal Narrative Logic: High-level coherence where auditory and visual 5. elements explicitly explain, complement, or guide each other to reveal the storyline or intent. This must involve an explicit temporal alignment between sound and visual event. Output Format: You should output the keypoints in Python List Format: [xxx, xxx, ...] Video Caption: {video caption} Given the video caption, please list all the keypoints: Figure 12: Prompts to decompose captions into keypoints. Prompts to judge keypoint accuracy in captions good video caption is one that describes the various details in the video. Your task is to judge whether video caption is good or not. You will be provided all the keypoints in the video, and also video caption to be evaluated. You need to determine which keypoints are described correctly in the given video caption. There are totally {# keypoints} keypoints in the video. All the keypoints will be provided in List format, i.e. [xxx, xxx, ...] The video caption to be evaluated will be provided as well. Output Format: Your output should be strict in the following Python dictionary format without anything else: {Count of correctly mentioned keypoints: x, Correctly mentioned keypoints: [...]} Keypoints in the video: {keypoints} Video caption to be evaluated: {video caption} Given keypoints in the video and the video caption, please count the correctly mentioned keypoints and list them out. Figure 13: Prompts to judge keypoint accuracy in captions. 22 AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration"
        },
        {
            "title": "Prompts to extract dialogues in captions",
            "content": "You are highly skilled assistant specializing in extracting conversational dialogue from text. Your task is to carefully analyze the given description of video and accurately identify and extract all dialogue content within it. Please directly output the dialogue in the following format without adding any other content. If no dialogue is present, state: None. Dialogue format: Speaker Description: Dialogue from speaker A. Speaker Description: Dialogue from speaker B. Speaker Description: Further dialogue... The description for each speaker (e.g., Person in red dress) must align with the given description and should be simplified for brevity. The key is to be concise and clearly distinguish between speakers (e.g., Man in red shirt is sufficient). Video description: {video description} Figure 14: Prompts to extract dialogues in captions. Prompts to identify identify speaker subject consistency Given video and several pairs of descriptive phrases about certain subject, please help me determine whether the subjects in each pair refer to the same entity in the video. For each pair of phrases, respond with Yes or No, separated by single space, without any extra characters. For example, if three pairs of phrases are provided, valid response format would be: Yes No Yes. Descriptive phrases (each line contains single pair): {dialogue pairs} Figure 15: Prompts to identify speaker subject consistency."
        },
        {
            "title": "Prompts to answer questions based on textual captions",
            "content": "You are precise QA assistant. Your task is to answer multiple-choice questions based ONLY on the video caption provided. Do not use any outside knowledge or assumptionsyour answer must strictly reflect information from the caption. Always output only the capital letter corresponding to your choice (e.g., A, B, C, D). If the caption does not provide enough information to answer the question, output N/A instead. Here is the video caption: {video caption} Question: {question} Choices: {choices} Figure 16: Prompts to answer questions based on textual captions."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Nanjing University",
        "New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "Peking University",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences"
    ]
}