{
    "paper_title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers",
    "authors": [
        "Ahmet Berke Gokmen",
        "Yigit Ekin",
        "Bahri Batuhan Bilecen",
        "Aysegul Dundar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 4 4 3 3 1 . 5 0 5 2 : r RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers Ahmet Berke Gökmen Yigit Ekin Bahri Batuhan Bilecen Aysegul Dundar Bilkent University, Department of Computer Science Reference RoPECraft robotic courier zips through maze of an eco-friendly, automated warehouse, packages whizzing past on conveyor belts. The silhouette of ballerina leaps across sunlit studio, delicate shadow dancing on polished hardwood floors. young woman riding small skateboard through the misty rainforest. vintage steam locomotive rolls past snowy station, its black iron body steaming in the frosty air. camel is seen walking in an abandoned, moonlit amphitheater. vintage biplane loops gracefully above an airfield. Figure 1: Our method successfully transfers the motion from reference videos."
        },
        {
            "title": "Abstract",
            "content": "We propose RoPECraft, training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate regularization term based on the phase components of the reference videos Fourier transform, projecting the phase angles onto smooth manifold to suppress highfrequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively. Equal contribution. Correspondence: berke.gokmen@ug.bilkent.edu.tr. Project page Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Diffusion transformers (DiT) have become leading approach for conditional video generation, producing realistic and coherent content across diverse scenarios [6, 16, 20, 49, 21, 38, 44]. While text conditioning provides convenient interface, they are often too ambiguous to specify detailed spatio-temporal dynamics such as body movement, camera motion, or interactions. As generative quality improves, so does the demand for more precise and controllable motion synthesis. To address the limitations of text-based motion control, earlier methods introduced explicit structural cues such as masks, bounding boxes, or depth maps to guide motion [9, 43, 40]. These approaches assume consistent geometry between the reference and generated videos, which often fails under domain shifts [45]. More recent work has shifted toward leveraging latent representations within generative models. Some methods extract motion features from internal activations [14, 45, 42], while others modify the latent prior [4] to better align reference and generated motions. prominent example, Go with the Flow (GWTF) [4], uses pretrained optical flow model [37] to generate motion priors that warp the initial noise input while maintaining its Gaussianity. This reportedly stabilizes and speeds up convergence. However, directly warping noise disrupts the intended latent distribution of the pre-trained DiT, as seen in Fig. 2. Even with inference-time latent optimization, the method struggles with generalization and domain shifts. As result, GWTF requires costly fine-tuning, demanding around 40 GPU days [4]. DiTFlow [31] offers more efficient alternative by optimizing latents or positional embeddings at test time without model retraining. However, it incurs high computational costs due to its reliance on full-size attention-based feature computation. We extend DiTFlows approach by updating only positional embeddings, avoiding latent space deviation and content leakage. Our method introduces motion-augmented rotary positional embeddings, warped via optical flow-derived displacements to embed motion cues (Section 4.1). We enhance this with flow-matching-guided optimization during early time steps, enabling stable and precise generation (Section 4.2). We further ensure spatiotemporal consistency through Fourier phase regularization (Section 4.3). Unlike GWTF, our method requires no backbone training, drastically cutting computational costs. Compared to DiTFlow, it delivers higher efficiency and better motion quality. In addition, to evaluate motion alignment, we propose Fréchet Trajectory Distance (FTD) (Section 5.2). Our method outperforms recent approaches in qualitative and quantitative assessments. Our contributions are: An efficient motion transfer, RoPECraft, that leverages motion-augmented rotary positional embeddings in training-free setting, without requiring any backbone fine-tuning. novel use of optical flow displacements to warp rotary positional embeddings, encoding spatial motion cues in attention calculations. unified optimization strategy combining flow matching velocity prediction and phase constraint regularization to enhance motion accuracy and ensure temporal coherence. new evaluation metric, Fréchet Trajectory Distance (FTD), for quantifying motion alignment between generated and reference videos. Input Latent warp + optimization + train DiT Ours motorcycle is seen riding on track, kicking up smoke as it goes. Figure 2: Latent warping [4] without an expensive fine-tuning of the DiT fails (Column 2), and latent optimization is not adequate to recover the domain shift (Column 3). Our approach keeps the latent space intact, and performs successful motion transfer, all without model re-training (Column 4)."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-video models. Following the successful application of diffusion models in image generation [12, 30, 29, 34], efforts have been made to extend this approach to video generation. Initial efforts used U-Net-based architectures for this purpose, utilizing temporal modules [16] or inflated convolutions [1, 3, 20, 39] to transfer the image prior to the video domain. More recently, DiT pipelines [21, 44, 38, 25, 49, 28] have gained attention due to their superior capabilities in temporal modeling and enhanced quality. Motivated by these, we also adopt DiT backbone. Motion transfer. The goal of motion transfer is to synthesize videos whose dynamics match reference clip while disentangling motion from appearance. Earlier methods injected explicit structure (masks or depth maps) [9, 43, 40]. Subsequent work learned dedicated motion embeddings and fed them to the generator [23, 22]. Recent approaches exploit the dense motion signals already present in backbone features [42, 15, 45, 14], or condition on trajectories extracted from the reference [48, 46, 41]. The current state of the art include Go With The Flow [4], which warps the initial noise with reference flow and fine-tunes the DiT on this prior, and DiTFlow [31], which derives displacement maps from cross-frame attention and updates either latents or positional embeddings. Building on DiTFlows insight, we dynamically update RoPE to guide attention toward reference motion while keeping the backbone frozen. Unlike prior work, we initialize RoPE with our motion-augmentation algorithm and regularizers, enabling fast and accurate motion transfer, without requiring model fine-tuning [4], inversion [42, 45, 14], masks [14], and high GPU memory during tuning [31]. Positional embeddings in vision transformers. Transformers lack inherent order awareness, so Vision Transformers (ViTs) [11] rely on positional embeddings to encode spatial relationships among patches. In early ViTs fixed sinusoidal or learnable absolute embeddings were used [11, 8]. These failed to generalize across varying input resolutions or sequence lengths in videos [8, 13]. Rotary Position Embedding (RoPE) overcomes these issues by rotating query and key values according to patch positions, thereby capturing relative spatial or temporal relationships [35] . Originally successful in language models [35, 2], RoPE has since been adapted to vision models [27, 17, 29, 21, 44, 38]. Building on these advances, our method updates RoPE embeddings on the fly during generation."
        },
        {
            "title": "3.1 Flow matching",
            "content": "Flow Matching (FM) is generative modeling approach that learns deterministic, time-dependent velocity field to transform simple base distribution into complex target distribution [26]. Unlike diffusion models, which reverse stochastic processes [19], FM minimizes the discrepancy between the model velocity vθ(t, x) and target velocity ut(x) derived from the continuity equation: LFM = Et,xptvθ(t, x) ut(x)2 (1) This ensures mass-preserving transport along predefined probability path pt, enabling more efficient training and sampling than traditional diffusion models."
        },
        {
            "title": "3.2 Rotary position embeddings (RoPE)",
            "content": "RoPE [35] encodes position by rotating query and key values in the complex plane, enabling the model to capture relative positional relationships. Given token at position with vector xm Rd, the vector is split into d/2 pairs. Each pair (x(2i1) ) is interpreted as complex number z(i) = x(2i1) Φm,i, where Φm,i = ejm θ2i/d is constructed by Algorithm 1, and θ is the base frequency. This operation embeds position into the phase of each frequency component, enabling self-attention to capture relative positions through inner products of queries and keys. Since attention patterns can control motion, we leverage RoPE heavily for our motion transfer task. , x(2i) , and RoPE applies the rotation z(i) + x(2i) 3 Figure 3: Visual description of our proposed pipeline inference and RoPE optimization approach."
        },
        {
            "title": "4 Methodology",
            "content": "This section thoroughly explains the proposed components of our motion transfer method. The overall architecture is given in Fig. 3. We augment the RoPE tensors via optical flow maps (Section 4.1), and optimize them during the generation process (Section 4.2) with additional constraints (Section 4.3)."
        },
        {
            "title": "4.1 Motion-augmented RoPE",
            "content": "Algorithm 1 Default 1D RoPE, expanded to 3D 1: Input: Base frequency θ R>0 2: Embedding dims Dt, Dh, Dw 3: Sequence lengths St, Sh, Sw 4: for each {t, h, w} do 5: 6: 7: 8: 9: 10: end for 11: Φ = concat(Φt,h,w) 12: Φ = flatten(Φ) RSk = [0, 1, . . . , Sk 1]T RDk/2 = [0, 1, . . . , Dk/2 1]T RDk/2 = θ2d/Dk Φk = ejpf CSk(Dk/2) Φk = expand(Φk) CStShSw (Dk/2) CStShSw (D/2) C11(StShSw )(D/2) e e t n 1 i l 2 i l Figure 4: Comparison of the generations of default and motion-augmented RoPE. The default RoPE algorithm used in video-DiTs is presented in Algorithm 1, where standard 1D RoPE is independently applied along the temporal (t), height (h), and width (w) dimensions to produce the respective components Φt, Φh, and Φw. These are then combined to form the full 3D positional encodings Φ, where each dimension {t, h, w} in Φk is expanded (repeated) in the other two dimensions, {t, h, w} k. However, as previously discussed, our insight is that this formulation can be altered significantly with motion signals. Specifically, by having unique, motion-augmented 1D RoPEs for and components, we allow the attention mechanism during the generation process to better understand which spatial patches should attend to one another. Our proposed procedure is detailed in Algorithm 2. For each row and column, we use the processed motion signals hflow and wflow to adjust the positional indices in the complex exponential Φ = exp(jpf T). Unlike Algorithm 1, where Φh is fixed across all rows, and Φw across all columns, Algorithm 2 introduces variation based on the motion signals. This way, we can construct unique embeddings for each spatial row and column for Φh and Φw, respectively, providing better initial condition for motion-guided generation. We leave the temporal component Φt unmodified, as altering it often introduces decoding artifacts without significant benefit. 4 Fig. 4 compares the default (Row 3) and modified (Row 4) embeddings visually. It can be observed that Algorithm 2 warps RoPE tensors in the motion direction (Row 1), whose effects are reflected on the generation (Row 2). Fig. 5 demonstrates the effectiveness of our approach across various prompts, which transfers coarse input motion directly into the generated videos. Input Output Input Output Input Output Figure 5: Qualitative results of motion-augmented RoPE described in Algorithm 2. However, relying solely on the modification introduced in Algorithm 2 can yield suboptimal results. For example, while the overall motion may be correct, subjects sometimes face the opposite direction (Column 4) or fail to accurately follow challanging trajectories (Column 6). To address these limitations, we introduce brief optimization step over the motion-augmented RoPE tensors during generation, which will be described in Section 4.2."
        },
        {
            "title": "4.2 Optimization with flow-matching",
            "content": "Algorithm 2 Motion-augmented RoPE 1: Input: Base frequency θ R>0, 2: Embedding dims Dt, Dh, Dw 3: Sequence lengths St, Sh, Sw 4: Optical flows u, R2StHW 5: u, = downsample(u, v) 6: hflow, wflow = cumsum(u, v) 7: hflow = flatten(hflow) 8: wflow = flatten(wflow) R2StShSw R(StSw )Sh R(StSh)Sw RDh/ 9: fh = θ2[0,1,...,Dh/21]T/Dh 10: for each in [0, 1, . . . , St Sw] do 11: 12: 13: end for 14: Φh = reorder(Φh) = [0, 1, . . . , Sh 1]T + hflow[r] RSh Φh[r] = ejpf CSh(Dh/2) CStShSw (Dh/2) 15: fw = θ2[0,1,...,Dw /21]T/Dw 16: for each in [0, 1, . . . , St Sh] do 17: 18: 19: end for 20: Φw = reorder(Φw) = [0, 1, . . . , Sw 1]T + wflow[c] Φw[c] = ejpf w 21: = [0, . . . , St 1]T 22: = θ2[0,...,Dt/21]T/Dt 23: Φt = expand(ejpf ) To refine Algorithm 2, we apply brief optimization on rotary embeddings during early generation steps. Using Eq. (1), we align the generated velocity vθ(t, xt) with the target velocity ut(x) = σ1 (xt v), where xt is the current latent in time step t, is latent reference video, and σ is the scheduler sigma. Fig. 6 illustrates the effectiveness of both optimization and the motion-augmented RoPE initial condition. In Column 1, the subject moves away from the camera, while in Column 2, the subject moves from left to right. The motionaugmented RoPE approach (Columns 34) successfully captures the general movements. However, in the second sample, it incorrectly renders the motorbike facing backward. When optimization is performed without dedicated initial condition (Algorithm 1), the subject placement improves, but issues arise in motion direction (Column 6), and visual artifacts appear (Column 5, Row 2). In contrast, initializing with Algorithm 2 yields the best results across the samples. This approach reduces artifacts, corrects subject orientation and trajectories."
        },
        {
            "title": "4.3 Phase constraints",
            "content": "24: Φ = flatten(concat(Φt,h,w)) Flow matching optimization produces strong results, as shown in Fig. 6, but we occasionally observe duplicated subjects when adjusting the orientation, position, or motion of the moving subjects. To address this, we build on insights from prior work [47] and analyze the Fourier transform of our signals. Since linear displacements in the spatial domain cause phase shift in frequency domain, 5 Input Algorithm 2 Algorithm 1 + opt. Algorithm 2 + opt. Figure 6: Qualitative results on optimization, with identical seeds across different experiments. Fourier property closely tied to motion transfer, we add phase constraint to the flow matching objective to guide the model toward more accurate and consistent spatiotemporal alignment. Specifically, we take the Fourier transform of the target velocity ut along spatio-temporal dimension, to get F(ut) = Ut = Ut exp {jUt}, where Ut = {Re(Ut)2 +Im(Ut)2}1/2 is the magnitude, and Ut = arctan{Im(Ut)/Re(Ut)} is the phase. We perform the same transform to the DiT output vθ, and add the phase constraint as L1 regularizer to the main optimization objective. We represent the phase on the unit circle (exp{jF()}) to make them continuous and differentiable everywhere, since the original mapping F() contains jump discontinuities at π due to being bounded by (π, π]. More clearly, we represent exp{jF()} = cos(F()) + sin(F()) and perform the phase-consistency loss in two parts. The final optimization objective is given in Eq. (2): min LFM(ut, vθ) + λ cos F(ut) cos F(vθ) + λ sin F(ut) sin F(vθ), (2) where λ is the hyperparameter. Fig. 7 reveals the effect of phase constraints, fixing duplicate generations and artifacts. Additional experiments regarding Fourier components are given in Supplementary. Input Algorithm 2 + opt. Algorithm 2 + opt. w/phase Figure 7: Qualitative results on phase constraints, with identical seeds across different experiments."
        },
        {
            "title": "5.1 Metrics and baselines",
            "content": "We compare our method with the recently published motion transfer methods [4, 14, 31, 45, 42]. For evaluation, we use videos from the DAVIS dataset [32]. We generate 4 diverse prompts per DAVIS video via ShareGPT4V [7] and LLAMA 3.2 [36], which are detailed in the Supplementary. For the evaluation, we use content-debiased Fréchet Video Distance (CD-FVD) [5] for evaluating fidelity, CLIP similarity [18] for evaluating frame-wise prompt fidelity using ViT B/32 model [33], and Motion Fidelity (MF) [45] along with our proposed metric Fréchet Trajectory Distance (FTD) for evaluating the motion alignment between the generated and the ground truth reference motion video. For assessing the motion of the foreground object as well as camera motion, we use FTD by only sampling from the foreground object mask region, and sampling from both the foreground and background mask region. For MF and FTD, the trajectories are obtained using Co-Tracker3 [24]. For video synthesis, we use Wan2.1-1.3B [38] as the backbone of our method. To obtain fair assessment, similar to the approach used in DiTFlow [31], we adapt MOFT [42], SMM [45], DitFlow [31] and ConMo [14] to Wan2.1. For the methods that require DDIM inversion [42, 45, 14], 6 we perform KV-injection from reference video latents similar to [31]. We evaluate GWTF using their CogVideoX-2B [21] checkpoint. The hyper parameters are detailed in Supplementary."
        },
        {
            "title": "5.2 Fréchet Trajectory Distance",
            "content": "Figure 8: Fréchet Trajectory Distance (FTD). 1) Sample foreground (red) and background (green) seeds on the first frame. 2) Track each seed with an occlusion-aware filler: copy the nearest visible neighbor while occluded and discard tracks that never re-appear. 3) Measure the RMS Fréchet distance between generated (fake) and reference (real) tracks. Discrete Fréchet Distance. Let xi,t R2 be the 2D image coordinate of the ith point at frame (1 ). We denote the reference and generated trajectories by real i,T } and = {xfake fake i,T }, respectively. Then, the discrete Fréchet distance is defined as: i,1 , . . . , xfake i,1 , . . . , xreal = {xreal DF (cid:0)T real , fake (cid:1) = min σ,τ :{1,...,L}{1,...,T } max k=1,...,L (cid:13) (cid:13)xreal i,σ(k) xfake i,τ (k) (cid:13) (cid:13)2, (3) where is the length of the common re-parameterization, and (σ, τ ) are non-decreasing index maps that allow each curve to pause or advance but never step backwards. The inner max takes the worst spatial gap along particular pairing of frames, while the outer min selects the pairing that makes this worst gap as small as possible. Consequently, DF is the minimal worst-case deviation between the trajectories after they are aligned in time as favorably as the monotone constraint permits. Fréchet Trajectory Distance (FTD). We utilize Eq. (3) in our proposed FTD metric, along with several tricks to obtain meaningful trajectories . Fig. 8 explains our procedure thoroughly. From the first frame, we uniformly select points inside the binary foreground mask M0, and outside to capture both object (red) and background (green) motion. Then, the occlusion-aware tracker [24] generates tracks starting with the initial 2n points. Since some points may go out of bounds or get occluded as the video progresses, we reassign them by copying to their nearest visible neighbor to maintain trajectory continuity, and drop track entirely if the associated point never reappears. Before computing distances, all coordinates are normalized by the frame width and height H, making the metric resolution-invariant. The procedure yields valid, temporally , fake }N coherent pairs {T real i=1. Utilizing the pairs, we calculate root-mean-square Fréchet distance, , fake (T real i=1D2 FTD = (N 1ΣN ))0.5. For calculating DF , we utilize [10]. Comparison with Motion Fidelity [45]. The Motion Fidelity (MF) metric [45] computes cosine similarity between frame-to-frame displacements on fixed grid, averaging best matches. However, it ignores path shape, magnitude, and occlusions, and can report high scores even when trajectories diverge. In contrast, our Fréchet Trajectory Distance (FTD) drops unreliable tracks, focuses on relevant regions, and measures curve distance using discrete Fréchet distance, making it more robust to missing data and outliers. As shown in Table 1, MF also exhibits much higher standard deviation, highlighting its instability compared to FTD."
        },
        {
            "title": "5.3 Qualitative evaluation",
            "content": "Fig. 9 provides visual comparison of the evaluated methods across diverse prompts and motion scenarios. Our approach consistently outperforms others in both trajectory direction and subject orientation. In P1, MOFT, DitFlow, ConMo, and SMM fail to capture the correct motion direction, although SMM maintains proper subject orientation. In P2, some methods struggle with prompt alignment, such as keeping the man stationary, and GWTF introduces noticeable artifacts. For more complex motions like P3 and P4, most methods do not use the reference motion effectively. While 7 Reference Ours GWTF SMM MOFT DitFlow ConMo P1: woman rides bike down wooden dock alongside serene lake at sunrise. P2: sailboat is anchored in tranquil cove surrounded by greenery, as the man walks along the weathered wooden dock. P3: The motorcycle drives down dirt road, and then speeds up as it goes. P4: large group of fire dancers are spinning together in circle, with one performer leading in middle, on tropical beach. P5: woman wearing black dress walks down worn wooden dock along the edge of misty lake at dusk. P6: silver motorhome pulling up to campsite nestled among the towering redwoods of misty forest. Figure 9: Qualitative comparison of the methods with diverse prompts. 8 GWTF shows motion coherence, it often sacrifices from prompt alignment. For example, it merges motorcycle with truck in P3 and does not place the man walking on the wooden dock in P2. similar issue appears in P6, where GWTF generates distorted motorhome, and only SMM, GWTF, and our method reflect the reference motion correctly. In general, our method accurately captures both motion and subject across all examples. Additional results are in the Supplementary videos."
        },
        {
            "title": "5.4 Quantitative evaluation",
            "content": "Table 1 compares our method with recent motion transfer baselines across five key metrics: Motion Fidelity (MF) [45], content-debiased Fréchet Video Distance (CD-FVD) [5], CLIP similarity [18], and our occlusion-aware FTD on foreground (FG) and all points (FG+BG). Our method achieves the highest MF score (0.5816) and the lowest CD-FVD (1284.58), surpassing strong baseline, GWTF [4], by +0.0103 (approximately +1.8%) and -200.6 (approximately -13.5%), respectively. It also attains the second-best CLIP similarity (0.2350), and ranks second on both FTD variants, 0.2644 for FG and 0.2584 for FG+BG, while outperforming all remaining competitors. These results demonstrate that our proposed framework provides strong baseline, and is highly competitive in semantic fidelity and trajectory alignment. We also showcase quantitative scores in ablation studies, validating the effectiveness of our design. Specifically, Table 2 justifies our approach on motion-augmented RoPE, optimization procedure with flow-matching, and phase constraints. Table 3 elaborates on the selection of first denoising steps in our optimization, and number of optimization steps per t. We opted for (t, s) = (10, 5) as we noticed that = 10 decreases visual quality significantly. Table 1: Comparison of motion transfer methods across evaluation metrics. Best and second results are represented with italic and underlined, respectively. Method MF CD-FVD CLIP FTD (FG) FTD (FG+BG) GWTF [4] 0.57130.22 SMM [45] 0.48890.20 MOFT [42] 0.46060.20 DitFlow (latents) [31] 0.48320.20 DitFlow (RoPE) [31] 0.45000.18 ConMo [14] 0.46270.21 1485.23 1600.33 1630.45 1735.49 1852.90 1680. 0.23780.04 0.24570.14 0.23310.04 0.28820.15 0.23110.04 0.28110.16 0.23390.04 0.29210.15 0.23450.04 0.27850.14 0.23090.04 0.27690.15 0.23080.10 0.31760.15 0.30570.14 0.31350.12 0.30190.13 0.30400.14 Ours 0.58160.19 1284.58 0.23500.04 0.26440.14 0.25840. Table 2: Ablation on motion-augmented RoPE and phase constraints. Method MF CLIP FTD Alg.1 + opt. 0.7082 0.1560 0.2174 Alg.2 + opt. 0.7092 0.1650 0.2105 Alg.2 + opt. + phase 0.7210 0.1656 0.2060 Table 3: Ablation on hyperparameters. (t, s) MF CD-FVD CLIP FTD 5, 5 0.5165 1437.99 0.1597 0.2901 5, 10 0.5523 1606.88 0.1663 0.2728 10, 5 0.5675 1364.25 0.1664 0.2633 10, 10 0.6160 1492.86 0.1572 0."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this paper, we introduce RoPECraft, training-free motion transfer method that manipulates rotary positional embeddings in diffusion transformers. By combining motion-augmented RoPE tensors, flow-matching-based optimization, and phase-based regularization, RoPECraft achieves high-quality performance across multiple benchmarks, and produces high-quality motion transfer results. For the future work, the motion-augmented RoPE framework can be extended to handle more challenging cases, such as handling motion with extreme occlusion, and better high-frequency details in the generated videos. In addition, the pipeline can be extended to controllable video editing. We discuss limitations and broader impacts in the Supplementary Material. 9 Acknowledgements. We acknowledge EuroHPC Joint Undertaking for awarding the project ID EHPC-AI-2024A02-031 access to Leonardo at CINECA, Italy. We also acknowledge Fal.ai for granting GPU access."
        },
        {
            "title": "References",
            "content": "[1] Bar-Tal, O., Chefer, H., Tov, O., Herrmann, C., Paiss, R., Zada, S., Ephrat, A., Hur, J., Liu, G., Raj, A., et al.: Lumiere: space-time diffusion model for video generation. In: ACM SIGGRAPH (2024) [2] Barbero, F., Vitvitskyi, A., Perivolaropoulos, C., Pascanu, R., Veliˇckovic, P.: Round and round we go! what makes rotary positional encodings useful? In: Proceedings of the International Conference on Learning Representations. ICLR (2025) [3] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023) [4] Burgert, R., Xu, Y., Xian, W., Pilarski, O., Clausen, P., He, M., Ma, L., Deng, Y., Li, L., Mousavi, M., Ryoo, M., Debevec, P., Yu, N.: Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2025) [5] Carreira, J., Zisserman, A.: Quo vadis, action recognition? new model and the kinetics dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2017) [6] Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., Shan, Y.: Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2024) [7] Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.: Sharegpt4v: Improving large multi-modal models with better captions. In: Proceedings of the European Conference on Computer Vision. ECCV (2024) [8] Chu, X., Tian, Z., Zhang, B., Wang, X., Shen, C.: Conditional positional encodings for vision transformers. In: Proceedings of the International Conference on Learning Representations. ICLR (2023) [9] Dai, Z., Zhang, Z., Yao, Y., Qiu, B., Zhu, S., Qin, L., Wang, W.: Animateanything: Fine-grained open domain image animation with motion guidance. arXiv preprint arXiv:2311.12886 (2023) [10] Denaxas, S., Pikoula, M.: spiros/discrete_frechet: meerkat stable release, https://github.com/ spiros/discrete_frechet [11] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: Proceedings of the International Conference on Learning Representations. ICLR (2020) [12] Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al.: Scaling rectified flow transformers for high-resolution image synthesis. In: Proceedings of the International Conference on Machine Learning. ICML (2024) [13] Fan, Q., You, Q., Han, X., Liu, Y., Tao, Y., Huang, H., He, R., Yang, H.: Vitar: Vision transformer with any resolution. arXiv preprint arXiv:2403.18361 (2024) [14] Gao, J., Yin, Z., Hua, C., Peng, Y., Liang, K., Ma, Z., Guo, J., Liu, Y.: Conmo: Controllable motion disentanglement and recomposition for zero-shot motion transfer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2025) [15] Geyer, M., Bar-Tal, O., Bagon, S., Dekel, T.: Tokenflow: Consistent diffusion features for consistent video editing. In: Proceedings of the International Conference on Learning Representations. ICLR (2024) [16] Guo, Y., Yang, C., Rao, A., Liang, Z., Wang, Y., Qiao, Y., Agrawala, M., Lin, D., Dai, B.: Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In: Proceedings of the International Conference on Learning Representations. ICLR (2024) [17] Heo, B., Park, S., Han, D., Yun, S.: Rotary position embedding for vision transformer. In: Proceedings of the European Conference on Computer Vision. ECCV (2024) [18] Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., Choi, Y.: Clipscore: reference-free evaluation metric for image captioning. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. EMNLP (2021) [19] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: Proceedings of the International Conference on Neural Information Processing Systems. NeurIPS (2020) [20] Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video diffusion models. In: Proceedings of the International Conference on Neural Information Processing Systems. NeurIPS (2022) [21] Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.: Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In: Proceedings of the International Conference on Learning Representations. ICLR (2023) [22] Jeong, H., Park, G.Y., Ye, J.C.: Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2024) [23] Kansy, M., Naruniec, J., Schroers, C., Gross, M., Weber, R.M.: Reenact anything: Semantic video motion transfer using motion-textual inversion. arXiv preprint arXiv:2408.00458 (2024) [24] Karaev, N., Makarov, I., Wang, J., Neverova, N., Vedaldi, A., Rupprecht, C.: Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831 (2024) [25] Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al.: Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024) [26] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for generative modeling. In: Proceedings of the International Conference on Learning Representations. ICLR (2023) [27] Liu, Z., Guo, L., Tang, Y., Cai, J., Ma, K., Chen, X., Liu, J.: Vrope: Rotary position embedding for video large language models. arXiv preprint arXiv:2502.11664 (2025) [28] Ma, X., Wang, Y., Chen, X., Jia, G., Liu, Z., Li, Y.F., Chen, C., Qiao, Y.: Latte: Latent diffusion transformer for video generation. Transactions on Machine Learning Research (2025) [29] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. CVPR (2023) [30] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. In: Proceedings of the International Conference on Learning Representations. ICLR (2024) [31] Pondaven, A., Siarohin, A., Tulyakov, S., Torr, P., Pizzati, F.: Video motion transfer with diffusion transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2025) [32] Pont-Tuset, J., Perazzi, F., Caelles, S., Arbeláez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv:1704.00675 (2017) [33] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: Proceedings of the International Conference on Machine Learning. ICML (2021) [34] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2022) [35] Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., Liu, Y.: Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568, 127063 (2024) [36] Team, L.: The llama 3 herd of models (2024), https://arxiv.org/abs/2407.21783 [37] Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In: Proceedings of the European Conference on Computer Vision. ECCV (2020) [38] Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., et al.: Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 (2025) 11 [39] Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., Zhang, S.: Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571 (2023) [40] Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao, D., Zhou, J.: Videocomposer: Compositional video synthesis with motion controllability. In: Proceedings of the International Conference on Neural Information Processing Systems. NeurIPS (2023) [41] Wang, Z., Yuan, Z., Wang, X., Li, Y., Chen, T., Xia, M., Luo, P., Shan, Y.: Motionctrl: unified and flexible motion controller for video generation. In: ACM SIGGRAPH (2024) [42] Xiao, Z., Zhou, Y., Yang, S., Pan, X.: Video diffusion models are training-free motion interpreter and controller. In: Proceedings of the International Conference on Neural Information Processing Systems. NeurIPS (2024) [43] Xing, J., Xia, M., Liu, Y., Zhang, Y., Zhang, Y., He, Y., Liu, H., Chen, H., Cun, X., Wang, X., et al.: Make-your-video: Customized video generation using textual and structural guidance. IEEE Transactions on Visualization and Computer Graphics (2024) [44] Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., Yin, D., Yuxuan.Zhang, Wang, W., Cheng, Y., Xu, B., Gu, X., Dong, Y., Tang, J.: Cogvideox: Text-to-video diffusion models with an expert transformer. In: Proceedings of the International Conference on Learning Representations. ICLR (2025) [45] Yatim, D., Fridman, R., Bar-Tal, O., Kasten, Y., Dekel, T.: Space-time diffusion features for zero-shot text-driven motion transfer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. CVPR (2024) [46] Yin, S., Wu, C., Liang, J., Shi, J., Li, H., Ming, G., Duan, N.: Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089 (2023) [47] Yuan, Y., Guo, Y., Wang, C., Zhang, W., Xu, H., Zhang, L.: Freqprior: Improving video diffusion models with frequency filtering gaussian noise. In: Proceedings of the International Conference on Learning Representations. ICLR (2025) [48] Zhang, Z., Liao, J., Li, M., Dai, Z., Qiu, B., Zhu, S., Qin, L., Wang, W.: Tora: Trajectory-oriented diffusion transformer for video generation. In: Proceedings of the IEEE/CVF international conference on computer vision. CVPR (2025) [49] Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., You, Y.: Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404 (2024)"
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Ablation on Fourier Features We ablate the Fourier components for additional constraints on the flow-matching objective on our optimization stage. For the objective min LFM + Lc, we ablate two regularizer for magnitude and phase, Eq. (4) and Eq. (5), respectively, Lc = λ F(ut) F(vθ)1 Lc = λ cos F(ut) cos F(vθ) + λ1 sin F(ut) sin F(vθ)1, (4) (5) where ut denotes the target velocity, and vθ is the generated velocity output from the transformer at time step t. As shown in Fig. 10, the phase-based constraint proves more effective than the magnitude-based one, supporting the discussion in the main paper. λ is chosen as 1.0 across all experiments, as sweeping λ did not result in significant changes. Reference w/Magnitude w/Phase P1: woman rides bike down wooden dock alongside serene lake at sunrise. P2: silver motorhome pulling up to campsite nestled among the towering redwoods of misty forest. Figure 10: Qualitative comparison of using magnitude and phase constraints. First column shows the video associated with P1, whereas the second column is with P2. A.2 Prompts We extract prompts from DAVIS [32] videos by using ShareGPT4V [7]. For generating diverse prompts, we utilize LLAMA 3.2 [36]. We construct different system_prompts for changing the object (1), and environment (2). We also provide paraphrased prompt for reconstruction (3). The system prompts are listed below: You will receive single-sentence Replace its main subject (the actor or object system_prompt_1 = Answer with single sentence. or multi-sentence video prompt. performing the action) with new, physically plausible subject while leaving the action, environment, camera movements, and style intact. be realistic in the described scenario (e.g., golden retriever border collie; sports car vintage motorcycle). Return only the fully rewritten promptno explanations, no bullet points. Do not just change the gender etc. class, do not just replace person with person. Keep the scene information in the prompt the same. Change the entity like man-woman or woman-man. The new subject must system_prompt_2 = Answer with single sentence. Keep the subject(s) and their actions exactly the same, but relocate the scene or setting to coherent, vivid new environment. background details match the new setting and remain physically reasonable. the updated prompt text and nothing else. Ensure lighting, weather, and You will receive video prompt. Output system_prompt_3 = Answer with single sentence. or scene. Simply rephrase the text so it is stylistically different (synonyms, varied sentence structure) while preserving every factual detail. paraphrased promptno commentary, no headings. Do not alter the subject, action, Return the single For each original prompt from ShareGPT4V [7], we apply these system prompts to generate its corresponding response. The motion prompts utilized in the main paper figures are listed below: 13 sleek, black helicopter is seen around bustling beach side promenade, passing by seaside resort building. sleek, silver sports car is navigating through the foggy streets of an Italian Renaissance-era town perched on the edge of rugged cliff overlooking the turquoise Mediterranean Sea. The video shows vintage motorcycle driving down track in garage. woman wearing beige coat is seen browsing in bookstore, examining shelf and then selecting book from the stack. man is seen sprinting across deserted beach at sunset, his feet pounding against the wet sand. man on motorcycle is seen riding down coastal highway with rugged cliffs and rocky outcroppings lining the edge of the ocean, as sunlight catches the spray of the waves and casts misty veil over the scene. backpacker is seen walking on rocky terrain with mountains in the background. white van is seen driving down street with building in the background. goose walks on grass and then flies over river. A.3 Hyperparameters and Computational Requirements Hyperparameters. For video synthesis, we adopt Wan2.1-1.3B [38] as the backbone of our method. Since DiTFlow [31] was originally proposed on CogVideoX [21], and MOFT [42], SMM [45], and ConMo [14] were developed on UNet-based architectures, we re-implemented all these baselines using the same Wan2.1-1.3B backbone for fairer comparison. For methods that require DDIM inversion [42, 45, 14], we applied key-value (KV) injection into all transformer blocks during the first denoising steps, following the strategy used in DiTFlow. We conducted extensive experiments to determine optimal hyperparameters for each method in the Wan2.1 framework. The hyperparameters are defined as follows: learning rate (l), transformer block index for motion feature extraction (b), number of optimization steps (s), number of early denoising steps used for optimization (t, out of 50 total steps), AMF attention temperature for DitFlow (d), and mask fusion weight for ConMo (w). We utilize Adam optimizer across all methods, with their default β parameters. 1. DiTFlow [31]: = 1 104, = 10, = 10, = 5, = 2.0 2. MOFT [42]: = 1 104, = 10, = 10, = 5 3. SMM [45]: = 1 104, = 5, = 10, = 4. ConMo [14]: = 1 104, = 20, = 5, = 10, = 0.5 5. Ours: = 1 104, = 5, = 10 For ConMo, we used ground-truth DAVIS masks for the reference videos. We do not extract motion cues from internal layers of the transformer, hence is not applicable for our case. Due to limited computational resources, we used the original CogVideoX weights provided by the authors for GWTF [4], as training the full Wan2.1 pipeline from scratch was infeasible. To align more closely with Wans 1.3B parameter scale during evaluation, we used the 2B checkpoint of GWTF. Computational Requirements. We run all the models on shared cluster, with compute nodes equipped with 4 NVIDIA A100 64GB. A.4 Fréchet Trajectory Distance We provide our Fréchet Trajectory Distance pseudocode in Listing 1, where the frechetdist is calculated by using [10] and cotracker3 by [24]. 14 Listing 1 Fréchet Trajectory Distance implementation. import frechetdist def fill_and_drop(track, vis): filled = track.clone() N, F, _ = filled.shape for in range(1, F): inv_idx = (vis[:, t]).nonzero(as_tuple=False).view(-1) vis_idx = vis[:, t].nonzero(as_tuple=False).view(-1) if inv_idx.numel() and vis_idx.numel(): prev_pts = filled[inv_idx, - 1] curr_pts = filled[vis_idx, t] = distance_matrix(prev_pts, curr_pts) filled[inv_idx, t] = curr_pts[d.argmin(dim=1)] else: filled[:, t] = filled[:, - 1] dropped = (vis[:, 1:].any(dim=1)).nonzero(as_tuple=False).view(-1) return filled, dropped def compare_trajectory_consistency(cotracker3 , video1, video2, mask, n_points=100, use_fg_mask_only=False): _, T, C, H, = video1.shape if use_fg_mask_only: queries = sample_points_inside_mask_randomly(mask, n_points) else: bg=n_points//2) queries = sample_points_from_mask_randomly(mask, fg=n_points//2, (cid:44) tracks = [] drops = [] for vid in (video1, video2): pts, vis = cotracker3(vid, queries=queries) pts, drop = fill_and_drop(pts[0], vis[0]) pts[...,0] /= pts[...,1] /= tracks.append(pts) drops.append(drop) sq = [] for in range(tracks[0].shape[1]): if in drops[0] or in drops[1]: continue = tracks[0] = tracks[1] fd = frechetdist(P, Q) sq.append(fd*fd) return sqrt(mean(sq)) 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 A.5 Limitations and Broader Impacts Limitations. We present the limitations of our method in Fig. 11. These limitations can be mitigated by utilizing heavier DiT-based video generation model, or higher-quality motion extractor for motion-augmented rotary embedding generation. limitation of our proposed Fréchet Trajectory Distance is its reliance on CoTracker3 [24], which has difficulty handling zoom in or zoom out camera motions, especially in cases where the tracking points remain stationary. This limits the accuracy of the extracted trajectories in such scenarios. Broader Impacts. The ability to generate realistic motion in videos can greatly benefit fields such as animation, virtual production, education, and accessibility. However, it also introduces risks, particularly in the creation of deepfakes and other forms of synthetic media that may be used to deceive. These concerns highlight the importance of responsible use, and supporting research into detection and verification methods to help mitigate potential misuse while enabling positive applications. 15 Reference Output P1: person is seen water skiing in the ocean, being pulled by boat. P2: figure on scooter was observed traversing through the mall before losing balance and subsequently dropping from his vehicle. Figure 11: Limitations of our method. In the first video, boat (highlighted with red rectangle) intermittently appears and disappears, likely due to limitations in the backbone network. In the second video, the final frame shows distorted human figure, caused by the absence of the person in the corresponding frame of the source video. This may highlight limitation of the optical flow extractor used to modify our rotary embeddings, in handling occluded or missing subjects."
        }
    ],
    "affiliations": [
        "Bilkent University, Department of Computer Science"
    ]
}