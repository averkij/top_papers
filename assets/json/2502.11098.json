{
    "paper_title": "Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems",
    "authors": [
        "Zhao Wang",
        "Sota Moriyama",
        "Wei-Yao Wang",
        "Briti Gangopadhyay",
        "Shingo Takamatsu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in LLM-based multi-agent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose \\textit{Talk Structurally, Act Hierarchically (TalkHier)}, a novel framework that introduces a structured communication protocol for context-rich exchanges and a hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. \\textit{TalkHier} surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domain-specific selective questioning, and practical advertisement text generation. These results highlight its potential to set a new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multi-agent frameworks. The code is available https://github.com/sony/talkhier."
        },
        {
            "title": "Start",
            "content": "Talk Structurally, Act Hierarchically: Collaborative Framework for LLM Multi-Agent Systems Zhao Wang,, Sota Moriyama, Wei-Yao Wang, Briti Gangopadhyay, Shingo Takamatsu Sony Group Corporation, Japan 5 2 0 2 6 1 ] A . [ 1 8 9 0 1 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in LLM-based multiagent (LLM-MA) systems have shown promise, yet significant challenges remain in managing communication and refinement when agents collaborate on complex tasks. In this paper, we propose Talk Structurally, Act Hierarchically (TalkHier), novel framework that introduces structured communication protocol for context-rich exchanges and hierarchical refinement system to address issues such as incorrect outputs, falsehoods, and biases. TalkHier surpasses various types of SoTA, including inference scaling model (OpenAI-o1), open-source multi-agent models (e.g., AgentVerse), and majority voting strategies on current LLM and single-agent baselines (e.g., ReAct, GPT4o), across diverse tasks, including open-domain question answering, domainspecific selective questioning, and practical advertisement text generation. These results highlight its potential to set new standard for LLM-MA systems, paving the way for more effective, adaptable, and collaborative multiagent frameworks. The code is available at https://github.com/sony/talkhier."
        },
        {
            "title": "Introduction",
            "content": "Large Language Model (LLM) Agents have broad applications across domains such as robotics (Brohan et al., 2022), finance (Shah et al., 2023; Zhang et al., 2024b), and coding (Chen et al., 2021; Hong et al., 2023). By enhancing capabilities such as autonomous reasoning (Wang et al., 2024b) and decision-making (Eigner and HÃ¤ndler, 2024), LLM agents bridge the gap between human intent and machine execution, generating contextually relevant responses (Pezeshkpour et al., 2024). Recent research has primarily focused on LLMbased Multi-Agent (LLM-MA) systems, which *These authors contributed equally to this work Corresponding author: Zhao Wang (Email Address: Zhao.Wang@sony.com) 1 Figure 1: Existing LLM-MA methods (left) face two major challenges: 1) disorganized, lengthy text-based communication protocols, and 2) sequential or overly similar flat multi-agent refinements. In contrast, TalkHier (right) introduces well-structured communication protocol and hierarchical refinement approach. Figure 2: Our TalkHier built on GPT4o surpasses inference scaling models (OpenAI-o1), open-source multiagent models (AgentVerse and etc.), and models with majority voting strategies (ReAct, GPT4o) on five subtasks of MMLU. leverage collective intelligence and specialize each agent with the corresponding subtasks, to solve complicated and multi-step problems. For instance, previous works on LLM-MA have explored approaches where instances of LLMs, referred to as agents (Xi et al., 2023; Gao et al., 2023; Wang et al., 2024a; Cheng et al., 2024; Ma et al., 2024), collaborate synergistically by debate (Chen et al., 2024), reflection (He et al., 2024), self-refinement (Madaan et al., 2023), or multi-agent based feedback refinement (Yang et al., 2023). These systems employ diverse communication topologies to enable efficient interactions between agents such as Chain (Wei et al., 2022) and Tree (Yao et al., 2023) structures, among others (Qian et al., 2024; Zhuge et al., 2024; Zhang et al., 2024a). Despite the promising advancements in LLMMA systems, several challenges in this field remain unexplored (shown in Figure 1): 1) Disorganized communication in text form. Agents often engage in debates (Zhao et al., 2024), share insights (Chen et al., 2024), or perform refinement (Madaan et al., 2023; Yang et al., 2023) to effectively solve complex tasks, with their exchanges primarily in text form (Guo et al., 2024). However, communication often becomes disorganized because it requires explicitly describing agent tasks, providing background context for the communication, and specifying the required output formats. These factors together lead to lengthy and unstructured exchanges, making it difficult for agents to manage subgoals, maintain output structures, and retrieve independent memories from prior actions and observations. 2) Refinement schemes. While some studies have shown that incorporating agent debates (Chen et al., 2024) or evaluation-based multi-agent refinement (Wang et al., 2023; Yang et al., 2023) can improve system accuracy, these approaches also expose significant limitations. As the number of agents increases, LLM-MA systems face challenges in effectively summarizing opinions or feedback (Fang et al., 2024). They often fail to balance these inputs, frequently overlooking some or exhibiting biases based on the order in which feedback is provided (Errica et al., 2024). In this paper, we propose novel collaborative LLM-MA framework called Talk Structurally, Act Hierarchically (TalkHier)-the first collaborative LLM-MA framework to integrate well-structured communication protocol with hierarchical refinement. Our key contributions shown in Figure 1 and 2 are as follows: 1. Well-Structured, Context-Rich Communication Protocol: TalkHier introduces novel communication protocol that incorporates newly proposed elements: messages, intermediate outputs, and relevant background information. These components form the foundation of wellstructured protocol that organizes agent communication, ensuring clarity and precision. By embedding these elements, TalkHier significantly improves communication accuracy and efficiency compared to traditional text-based methods. 2. Hierarchical Refinement in LLM-MA Systems: TalkHier enhances traditional multi-agent evaluation systems with hierarchical refinement framework, enabling agents to act hierarchically. This approach addresses such as the difficulty in summarizing opinions or feedback as the number of agents increases, balancing diverse inputs, and mitigating biases caused by the order of feedback processing, resulting in more reliable and robust interactions. results 3. State-of-the-Art Results Across Benchmarks: show that TalkHier Experimental achieves state-of-the-art performance on diverse benchmarks, including selective problemsolving in complex sub-domains, open question answering, and Japanese text generation tasks. Ablation studies confirm the effectiveness of each component, demonstrating their contributions to the frameworks overall success."
        },
        {
            "title": "2 Related Work",
            "content": "Collaborative LLM-MA. LLM-MA systems enable agents to collaborate on complex tasks through dynamic role allocation, communication, and task execution (Guo et al., 2024; Han et al., 2024). Recent advancements include agent profiling (Yang et al., 2024), hierarchical communication (Rasal, 2024), and integration of reasoning and intentions (Qiu et al., 2024). However, challenges remain in ensuring robust communication, avoiding redundancy, and refining evaluation processes (Talebirad and Nadiri, 2023). Standardized benchmarks and frameworks are needed to drive future progress (Li et al., 2024). Communication in LLM-MA. Effective communication is crucial for collaborative intelligence (Guo et al., 2024). While many previous works, including chain (Wei et al., 2022), tree (Yao et al., 2023), complete graph (Qian et al., 2024), random graph (Qian et al., 2024), optimizable graph (Zhuge et al., 2024), and pruned graph (Zhang et al., 2024a) methods have focused on communication topologies, there has been limited discussion on the optimal form of communication. Most systems rely on text-based exchanges (Zhang et al., 2024a; Shen et al., 2024), which is inefficient and prone to errors as agents often lose track of subtasks or fail to recall prior outputs as tasks grow in complexity. We argue 2 Figure 3: Comparisons between existing approaches (left) and ours (right). Our TalkHier proposes new communication protocol (first row) featuring context-rich and well-structured communication information, along with collaborative hierarchical refinement (second row) where evaluations provide summarized and coordinated feedback within an LLM-MA framework. for structured communication protocols that guide subtasks with clear, context-specific instructions, ensuring coherence across interactions. Feedback-Based Refinement. Feedback mechanisms, such as Self-Refine (Madaan et al., 2023) and generator-evaluator frameworks (Wang et al., 2023), improve system accuracy through iterative refinement. However, these methods face challenges in managing diverse feedback, which can lead to bias or inefficiencies if inputs are not wellorganized (Xu et al., 2024). Scalable, unbiased solutions are essential to enhance multi-agent evaluation processes."
        },
        {
            "title": "3 Methodology",
            "content": "TalkHier aims to design LLM-MA system represented as graph = (V, E), where denotes the set of agents (nodes) and represents the set of communication pathways (edges). Given an input problem p, the system dynamically defines set of communication events Cp, where each event 3 c(t) ij Cp represents communication between agents vi and vj along an edge eij at time step t. While the graph structure remains fixed, the communication events Cp are dynamic and adapt to the specific task."
        },
        {
            "title": "3.1 Agents with Independent Memory",
            "content": "Each agent vi in graph can be formally represented as: vi = (Rolei, Pluginsi, Memoryi, Typei) . Rolei: Assign roles such as generator, evaluator, or revisor based on the task type. Pluginsi: External tools or plugins attached for domain-specific operations. Memoryi: An agent-specific memory that stores and retrieves information relevant to the agents role and task. Typei: Specifies whether the agent is Supervisor (S) responsible for overseeing task success, or Member (M ) focused on problem-solving. The first and Pluginsiare standard in most related works, componentsRolei, two Supervisor Prompt Template Team Members: [Description of each team members role] Conversation History: [Independent Conversation History] Given the conversation above, output the following in this exact order: 1. thoughts: Output detailed analysis on the most recent message. In detail, state what you think should be done next, and who you think you should contact next. 2. Who should act next? Select one of: [Team member names] and output as next. When you have determined that the final output is gained, report back with FINISH. 3. messages: If the next agent is one of [Team member names], give detailed instructions. If FINISH, report summary of all results. 4. The detailed background of the problem you are trying to solve (given in the first message) as background. 5. The intermediate outputs to give as intermediate_output. Member Prompt Template [Role of member] Background: [Background information given by Supervisor] Conversation History: [Independent Conversation History] Figure 4: Prompts for acquiring the contents of the context-rich, structured communication protocol in TalkHier. forming the foundation of agent functionality. Our contributions lie in the last three components: Memoryi, which equips each agent with our refined independent, agent-specific memory for reasoning, Teami, which represents the team the agent is part of, and Typei, which explicitly categorizes agents into Supervisor (S) roles, responsible for overseeing the multi-agent team and ensuring task success, or Member (M ) roles, focused on problem-solving and optionally utilizing plugins. These additions enable hierarchical, structured collaboration and role-specific operations within the framework. Agent-Specific Memory. To enhance efficiency and scalability, each agent vi maintains an independent memory, Memoryi. Unlike long-term memory, which relies on shared memory pool accessible by all agents, or short-term memory, which is limited to single session or conversational thread, our proposed memory mechanism is agent-specific but not limited to session or conversational thread. TalkHier allows each agent to independently retain and reason on its past interactions and knowledge, offering two key advantages: independence, where each agents memory operates without interference from others, avoiding centralized dependencies; and persistence, enabling agents to maintain historical data across sessions for consistent and informed decision-making."
        },
        {
            "title": "Agents",
            "content": "Communication between agents is represented by communication events c(t) ij Cp, where each event c(t) ij encapsulates the interaction from agent vi to agent vj along an edge eij at time step t. Formally, communication event c(t) ij is defined as: ij = (M(t) c(t) ij , B(t) ij , I(t) ij ), where M(t) indicates the message content sent ij from vi to vj, containing instructions or clarifications, B(t) ij denotes background information to ensure coherence and task progression, including the problems core details and intermediate decisions, and I(t) ij refers to the intermediate output generated by vi, shared with vj to support task progression and traceability, all at time step t. These structures ensure that agents of TalkHier accomplish efficient communication and task coordination. ij , B(t) Communication Event Sequence. At each time step t, the current agent vi communicates with connected node vj, with one being selected by the LLM if more than one exists. The elements of each edge M(t) ij and I(t) ij are then generated by invoking an independent LLM. To ensure consistency, clarity, and efficiency in extracting these elements, the system employs specialized prompts tailored to the roles of Supervisors and Members, as illustrated in Figure 4. Most notably, background information B(t) is not present for connections ij Algorithm 1: Hierarchical Refinement Input: Initial output A0 generated by the Generator node vGen Output: Final output Afinal main, quality threshold Mthreshold, maximum iterations Tmax 1 Initialize iteration counter 0 2 repeat 3 , CriteriavS )} eval = fevaluate(At1, Criterion eval assign = {(RolevS distribute = {(Criterion + 1 T(t) T(t) F(t) Ei eval F(t) eval = {F(t) , . . . , F(t) E1 Ek eval eval summary = fsummarize(F(t) eval) Feval if M(Feval return Afinal = At1 Ei eval } summary) Mthreshold then 4 5 6 7 9 10 At = frevise(At1, Feval summary) 11 12 until Tmax 13 return Afinal = At // Step 1: Task Assignment from vs main to vs // Step 2: Task Distribution by vs eval eval )}k i=1 // Step 3: Evaluation ), Ei eval vEi eval Veval // Step 4: Feedback Aggregation by vs eval // Step 5: Summarizing results // Step 6: Return the current text if above threshold // Step 7: Revision of the text operations and assign tasks to each member, the Generator (vGen main) gives solutions for given problem, and the Revisor (vRev main) refines outputs based on given feedback. Furthermore, the evaluation team is composed of independent evaluators vEk eval, each of which outputs evaluation results for given problem based on their specified metric. The overall structure is shown in Figure 5. Algorithm. Algorithm 1 illustrates the operation of our hierarchical refinement process within the collaborative agent framework. The process begins with the main Supervisor (vS main) assigning tasks to the evaluation Supervisor (vS eval), who then distributes evaluation criteria to individual evaluators (vEi eval). Each evaluator assesses the generated output (At1) based on their assigned criteria, producing detailed feedback. The evaluation Supervisor aggregates and summarizes this feedback (Feval summary) before passing it to the main Supervisor. The main Supervisor evaluates whether the summarized feedback meets the quality threshold (Mthreshold). If the threshold is satisfied, the output is finalized; otherwise, the Revisor (vRev main) refines the output for further iterations. This iterative refinement ensures accurate and unbiased collaboration across the agent hierarchy. The main Supervisor evaluates whether the summarized feedback meets the quality threshold (Mthreshold), defined vaguely as ensuring correctness or achieving high relevance. If satisfied, the output is finalized; otherwise, the Revisor (vRev main) refines it. Details of our settings are in Appendix B, Appendix C, and Appendix D. 5 Figure 5: Illustrated hierarchy of TalkHier. from Member nodes to Supervisor nodes. These information are then established as communication event c(t) ij Cp."
        },
        {
            "title": "3.3 Collaborative Hierarchy Agent Team",
            "content": "The entire graph consists of multiple teams, each represented as subset Vteam V. Each team includes dedicated supervisor agent vS team and one or more member agents vM team. key feature of the hierarchical structure in TalkHier is that member agent in one team can also act as supervisor for another team, creating nested hierarchy of agent teams. As shown in the second row of Figure 3, this structure enables the entire graph to represent hierarchical node system, where teams are recursively linked through supervisor-member relationships. Formally, the hierarchical structure of agents with two teams is defined as: main, vGen eval, vE1 Vmain = {vS Veval = {vS main, vS eval, vE2 where the Main Supervisor (vS Supervisor (vS eval, vRev main}, eval, . . . , vEk main) and Evaluation eval) oversee their respective teams eval},"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we aim to answer the following research questions across various domains: RQ1: Does TalkHier outperform existing multiagent, single-agent, and proprietary approaches on general benchmarks? RQ2: How does TalkHier perform on open-domain question-answering tasks? RQ3: What is the contribution of each component of TalkHier to its overall performance? RQ4: How well does TalkHier generalize to more practical but complex generation task? 4.1 Experimental Setup Datasets. We evaluated TalkHier on diverse collection of datasets to assess its performance across various tasks. The Massive Multitask Language Understanding (MMLU) Benchmark (Hendrycks et al., 2021) tests domain-specific reasoning problems including Moral Scenario, College Physics, Machine Learning, Formal Logic and US Foreign Policy. WikiQA (Yang et al., 2017) evaluates opendomain question-answering using real-world questions from Wikipedia. The Camera Dataset (Mita et al., 2024) focuses on advertisement headline generation, assessing the ability to create high-quality advertising text. Baselines. To evaluate TalkHier, we compared it against comprehensive set of baselines including: GPT-4o (OpenAI, 2024a), based on OpenAIs GPT-4 model with both single-run and ensemble majority voting (3, 5, or 7 runs). OpenAI-o1-preview (OpenAI, 2024b), beta model using advanced inference techniques, though limited by API support. ReAct (Yao et al., 2022), reasoning and action framework in single-run and ensemble configurations. AutoGPT (Gravitas, 2023), an autonomous agent designed for task execution and iterative improvement. AgentVerse (OpenBMB, 2023), multi-agent system framework for collaborative problemsolving. GPTSwarm (Zhuge et al., 2024), swarmbased agent collaboration model utilizing optimizable communication graphs. AgentPrune (Zhang et al., 2024a), model leveraging pruning techniques for efficient multi-agent communication and reasoning. Table 1: General Performance on MMLU Dataset. The table reports accuracy (%) for various baselines across Moral Scenario (Moral), College Physics (Phys.), Machine Learning (ML), Formal Logic (FL) and US Foreign Policy (UFP) domains. The notations 3@, 5@, and 7@ represent majority voting results using 3, 5, and 7 independent runs, respectively. Models GPT4o GPT4o-3@ GPT4o-5@ GPT4o-7@ ReAct ReAct-3@ ReAct-5@ ReAct-7@ AutoGPT AgentVerse GPTSwarm AgentPrune o1-preview TalkHier (Ours) Moral Phys. 64.25 65.70 66.15 65.81 69.61 74.75 74.97 75. 66.37 79.11 60.48 70.84 82.57 83.80 62.75 62.75 61.76 63.73 72.55 83.33 82.35 84.78 78.43 93.14 67.70 91.18 91.17 93.14 ML 67.86 66.07 66.96 66.96 59.82 66.07 66.96 67.86 64.29 79.46 72.32 81.25 85.71 84.68 FL 63.49 66.67 66.67 68.25 32.54 52.38 46.83 50.79 60.83 78.57 68.33 81. 83.33 87.30 UFP 92.00 91.00 92.00 91.00 58.00 53.00 63.00 57.00 90.00 88.00 57.00 93.00 95.00 93.00 Avg. 70.07 70.44 70.71 71.15 58.50 65.91 66.82 67.19 71.98 83.66 65.17 83.60 87.56 88.38 OKG (Wang et al., 2025), method tailored specifically for ad text generation tasks and easily generalizable to ad headlines with minimal prompt redefinition. Implementation details. For fair comparisons, we use GPT-4o as the backbone across all experiments for the baselines and TalkHier, with the temperature set to 0 in all settings. For the OpenAIo1 baseline, we followed the implementation guide and the limitations outlined in OpenAIs documentation, and keep the temperature fixed at 1."
        },
        {
            "title": "4.2 Performance on MMLU (RQ1)",
            "content": "Table 1 reports the average accuracy of various models on the five domains of MMLU dataset. TalkHier, built on GPT-4o, achieves the highest average accuracy (88.38%), outperforming open-source multi-agent models (e.g., AgentVerse, 83.66%) and majority voting strategies applied to current LLM and single-agent baselines (e.g., ReAct-7@, 67.19%; GPT-4o-7@, 71.15%). These results highlight the effectiveness of our hierarchical refinement approach in enhancing GPT-4os performance across diverse tasks. Although OpenAIo1 cannot be directly compared to TalkHier and other baselinessince they are all built on GPT-4o and OpenAI-o1s internal design and training data remain undisclosedTalkHier achieves slightly higher average score (88.38% vs. 87.56%), demonhttps://platform.openai.com/docs/guides/ reasoning/beta-limitations 6 Table 2: Evaluation Results on WikiQA. The table reports Rouge-1 and BERTScore for various models. Models GPT4o ReAct AutoGPT AgentVerse AgentPrune GPTSwarm o1-preview TalkHier (Ours) Rouge-1 BERTScore 0.2777 0.2409 0.3286 0.2799 0.3027 0.2302 0.2631 0.3461 0.5856 0.5415 0.5885 0.5716 0.5788 0.5067 0.5701 0.6079 strating competitive performance. 4.3 Evaluation on WikiQA Benchmark (RQ2) We evaluated TalkHier and baselines on the WikiQA dataset, an open-domain question-answering benchmark. Unlike MMLU, WikiQA requires generating textual answers to real-world questions. The quality of generated answers was assessed using two metrics: Rouge-1 (Lin, 2004), which measures unigram overlap between generated and reference answers, and BERTScore (Zhang et al., 2020), which evaluates the semantic similarity between the two. Table 2 shows that TalkHier outperforms baselines in both Rouge-1 and BERTScore, demonstrating its ability to generate accurate and semantically relevant answers. While other methods, such as AutoGPT and AgentVerse, perform competitively, their scores fall short of TalkHier, highlighting its effectiveness in addressing open-domain questionanswering tasks."
        },
        {
            "title": "4.4 Ablation Study (RQ3)",
            "content": "To better understand the contribution of individual components in TalkHier, we conducted ablation studies by removing specific modules and evaluating the resulting performance across the Moral Scenario, College Physics, and Machine Learning domains. The results of these experiments are summarized in Table 3. Table 3 presents the contributions of our ablation study on the main components in TalkHier. Removing the evaluation Supervisor (TalkHier w/o Eval. Sup.) caused significant drop in accuracy, underscoring the necessity of our hierarchical refinement approach. Replacing the structured communication protocol with the text-based protocol (TalkHier w. Norm. Comm) resulted in moderate accuracy reductions, while eliminating the entire evaluation Table 3: Ablative Results on Main Components of TalkHier: Accuracy (%) across Physics, ML, and Moral domains. TalkHier w/o Eval. Sup. removes the evaluation supervisor. TalkHier w/o Eval. Team excludes the evaluation team component. TalkHier w. Norm. Comm uses normalized communication protocol. Models w/o Eval. Sup. w/o Eval. Team w. Norm. Comm React (Single Agent) TalkHier (Ours) Moral 83.57 73.54 82.91 69.61 83.80 Phys. ML 74.77 87.25 74.56 80.34 82.14 88.24 59.82 72.55 84.68 93. Avg. 81.86 76.15 84.43 67.33 87.21 Table 4: Ablative Results: Accuracy (%) across Physics, ML, and Moral domains. The study examines the impact of removing components from the structured communication protocol: message (Mij), background (Bij), and intermediate output (Iij). Models w/o Iij w/o Bij w/o Bij, Iij TalkHier (Ours) Moral 81.56 76.87 77.99 83.80 Phys. ML 75.89 90.20 70.54 87.50 78.57 90.20 84.68 93.14 Avg. 82.55 78.30 82.25 87. team (TalkHier w/o Eval.Team) led to substantial performance declines across all domains. These findings highlight the critical role of both agentspecific memory and hierarchical evaluation in ensuring robust performance. Table 4 delves into the impact of individual elements in the communication protocol. Removing intermediate outputs (TalkHier w/o Iij) or background information (TalkHier w/o Bij) lead to inferior performance, with their combined removal (TalkHier w/o Bij, Iij) yielding similar declines. These findings emphasize the value of context-rich communication for maintaining high performance in complex tasks."
        },
        {
            "title": "4.5 Evaluation on Ad Text Generation (RQ4)",
            "content": "We evaluate TalkHier on the Camera dataset (Mita et al., 2024) using traditional text generation metrics (BLEU-4, ROUGE-1, BERTScore) and domain-specific metrics (Faithfulness, Fluency, Attractiveness, and Character Count Violation) (Mita et al., 2024). These metrics assess both linguistic quality and domain-specific relevance. Setting up baselines like AutoGPT, AgentVerse, and GPTSwarm for this task was challenging, as their implementations focus on general benchmarks like MMLU and require significant customization 7 Table 5: Evaluation Results on Camera Dataset. We report BLEU-4 (B4), ROUGE-1 (R1), BERTScore (BERT), and domain-specific metrics (Faithfulness, Fluency, Attractiveness, Character Count Violation(CCV)) following (Mita et al., 2024). Models GPT-4o ReAct OKG TalkHier (Ours) B4 () R1 () BERT () 0.01 0.01 0.03 0. 0.65 0.70 0.73 0.91 0.02 0.01 0.16 0.20 Faithfulness () 4.8 4.9 6.3 8.6 Fluency () Attractiveness () CCV () 5.9 6.4 8.7 8.9 6.5 7.0 6.1 6. 16% 17% 4% 4% for ad text generation. In contrast, OKG (Wang et al., 2025), originally for ad keyword generation, was easier to adapt, making it more practical baseline. Table 5 presents the results. TalkHier outperforms ReAct, GPT-4o, and OKG across most metrics, particularly excelling in Faithfulness, Fluency, and Attractiveness while maintaining low Character Count Violation rate. The mean performance gain over the best-performing baseline, OKG, across all metrics is approximately 17.63%. To verify whether TalkHiers multi-agent evaluations of attractiveness, fluency, and faithfulness are accurate, we conducted subjective experiment on sub-dataset of Camera, comparing the systems automatic ratings to human judgments; details of this procedure are provided in Appendix E."
        },
        {
            "title": "5 Discussion",
            "content": "The experimental results across the MMLU, WikiQA, and Camera datasets consistently demonstrate the superiority of TalkHier. Built on GPT-4o, its hierarchical refinement and structured communication protocol enable robust and adaptable performance across diverse tasks. General and Practical Benchmarks. TalkHier outperformed baselines across general and practical benchmarks. On MMLU, it achieved the highest accuracy (88.38%), surpassing the best opensource multi-agent baseline, AgentVerse (83.66%), by 5.64%. On WikiQA, it obtained ROUGE-1 score of 0.3461 (+5.32%) and BERTScore of 0.6079 (+3.30%), outperforming the best baseline, AutoGPT (0.3286 ROUGE-1, 0.5885 BERTScore). On the Camera dataset, TalkHier exceeded OKG across almost all metrics, demonstrating superior Faithfulness, Fluency, and Attractiveness while maintaining minimal Character Count Violations. These results validate its adaptability and taskspecific strengths, highlighting its advantage over inference scaling models (e.g., OpenAI-o1), opensource multi-agent models (e.g., AgentVerse), and majority voting strategies (e.g., ReAct, GPT-4o). Comparative and Ablation Insights. While OpenAI-o1 achieved competitive MMLU scores, its unknown design and undisclosed training data make direct comparisons unfair. Since TalkHier is built on the GPT-4o backbone, comparisons with other GPT-4o-based baselines are fair. Despite this, TalkHier was competitive with OpenAI-o1 on MMLU and achieved significant advantage on WikiQA. Ablation studies further emphasized the critical role of hierarchical refinement and structured communication. Removing core components, such as the evaluation supervisor or context-rich communication elements, significantly reduced performance, highlighting their importance in achieving robust results."
        },
        {
            "title": "6 Conclusions",
            "content": "In this paper, we propose TalkHier, novel framework for LLM-MA systems that addresses key challenges in communication and refinement. To the best of our knowledge, TalkHier is the first framework to integrate structured communication protocol in LLM-MA systems, embedding Messages, intermediate outputs, and background information to ensure organized and context-rich exchanges. At the same time, distinct from existing works that have biases on inputs, its hierarchical refinement approach balances and summarizes diverse opinions or feedback from agents. TalkHier sets new standard for managing complex multi-agent interactions across multiple benchmarks, surpassing the best-performing baseline by an average of 5.64% on MMLU, 4.31% on WikiQA, and 17.63% on Camera benchmarks. Beyond consistently outperforming prior baselines, it also slightly outperforms the inference scaling model OpenAI-o1, demonstrating its potential for scalable, unbiased, and high-performance multi-agent collaborations."
        },
        {
            "title": "Limitations",
            "content": "One of the main limitations of TalkHier is the relatively high API cost associated with the experiments (see Appendix for details). This is tradeoff due to the design of TalkHier, where multiple agents collaborate hierarchically using specifically designed communication protocol. While this structured interaction enhances reasoning and coordination, it also increases computational expenses. This raises broader concerns about the accessibility and democratization of LLM research, as such costs may pose barriers for researchers with limited resources. Future work could explore more costefficient generation strategies while preserving the benefits of multi-agent collaboration."
        },
        {
            "title": "References",
            "content": "Anthony Brohan et al. 2022. Code as policies: Language model-driven robotics. arXiv preprint arXiv:2209.07753. Mark Chen et al. 2021. guage models trained on code. arXiv:2107.03374. Evaluating large lanarXiv preprint Pei Chen, Boran Han, and Shuai Zhang. 2024. Comm: Collaborative multi-agent, multi-reasoningpath prompting for complex problem solving. arXiv preprint arXiv:2404.17729. Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. 2024. Exploring large language model based intelligent agents: Definitions, methods, and prospects. CoRR, abs/2401.03428. Eva Eigner and Thorsten HÃ¤ndler. 2024. Determinants arXiv preprint of llm-assisted decision-making. arXiv:2402.17385. Federico Errica, Giuseppe Siracusano, Davide Sanvito, and Roberto Bifulco. 2024. What did do wrong? quantifying llms sensitivity and conarXiv preprint sistency to prompt engineering. arXiv:2406.12334. Jiangnan Fang, Cheng-Tse Liu, Jieun Kim, Yash Bhedaru, Ethan Liu, Nikhil Singh, Nedim Lipka, Puneet Mathur, Nesreen Ahmed, Franck Dernoncourt, et al. 2024. Multi-llm text summarization. arXiv preprint arXiv:2412.15487. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li. 2023. Large language models empowered agent-based modeling and simulation: survey and perspectives. CoRR, abs/2312.11970. 9 Significant Gravitas. 2023. Autogpt: An experimental open-source application. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680. Shiyang Han, Qian Zhang, Yue Yao, Wenhao Jin, Zhen Xu, and Cheng He. 2024. Llm multi-agent systems: Challenges and open problems. arXiv preprint arXiv:2402.03578. Chengbo He, Bochao Zou, Xin Li, Jiansheng Chen, Junliang Xing, and Huimin Ma. 2024. Enhancing llm reasoning with multi-path collaborative reactive and reflection agents. arXiv preprint arXiv:2501.00430. Dan Hendrycks, Colin Burns, Samuel Basart, Chia Zou, David Song, and Thomas G. Dietterich. 2021. Measuring massive multitask language understanding. arXiv preprint arXiv:2110.08307. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352. Xiaoyu Li, Shuang Wang, Shaohui Zeng, Yucheng Wu, and Yue Yang. 2024. survey on llm-based multiagent systems: Workflow, infrastructure, and challenges. Vicinagearth, 1(9). Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu, Donghua Liu, Xuwen Zhang, Zihan Zhao, Yifan Shen, Peilin Ji, Juanjuan Li, Gang Wang, and Wanpeng Ma. 2024. Computational experiments meet large language model based agents: survey and perspective. CoRR, abs/2402.00262. Aman Madaan, Niket Tandon, Doug Downey, and Iteratively imarXiv preprint Shrimai Han. 2023. Self-refine: proving text via self-feedback. arXiv:2303.17651. Masato Mita, Soichiro Murakami, Akihiko Kato, and Peinan Zhang. 2024. Striking gold in advertising: Standardization and exploration of ad text generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. OpenAI. 2024a. Hello gpt-4o. OpenAI. 2024b. Introducing openai o1. OpenBMB. 2023. Agentverse: Facilitating multi-agent collaboration. AgentVerse GitHub. Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani, Sajjadur Rahman, Tom Mitchell, and Estevam Hruschka. 2024. Reasoning capacity in multi-agent systems: Limitations, challenges and human-centered solutions. CoRR, abs/2402.01108. Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2024. Scaling large-language-model-based multi-agent collaboration. arXiv preprint arXiv:2406.07155. Xue Qiu, Hongyu Wang, Xiaoyun Tan, Chengyi Qu, Yifan Xiong, Yang Cheng, Yichao Xu, Wei Chu, and Yiming Qi. 2024. Towards collaborative intelligence: Propagating intentions and reasoning for multi-agent coordination with large language models. arXiv preprint arXiv:2407.12532. Sudhir Rasal. 2024. Llm harmony: Multi-agent communication for problem solving. arXiv preprint arXiv:2401.01312. Shivam Shah et al. 2023. Fingpt: An open-source arXiv preprint financial large language model. arXiv:2306.03026. Wenjun Shen, Cheng Li, Hui Chen, Meng Yan, Xuesong Quan, Hao Chen, Jian Zhang, and Fangyu Huang. 2024. Small llms are weak tool learners: multi-llm agent. arXiv preprint arXiv:2401.07324. Yashar Talebirad and Amir Nadiri. 2023. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2024a. survey on large language model based autonomous agents. Front. Comput. Sci., 18. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. 2024b. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? arXiv preprint arXiv:2402.18272. Xiaoyu Wang, Yuanhao Liu, and Hao Zhang. 2023. Coeval: framework for collaborative human and machine evaluation. arXiv preprint arXiv:2310.19740. Zhao Wang, Briti Gangopadhyay, Mengjie Zhao, and Shingo Takamatsu. 2025. OKG: On-the-fly keyword generation in sponsored search advertising. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 115127. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The rise and potential of large language model based agents: survey. arxiv preprint, abs/2309.07864. Li Xu, Qiang Sun, and Hui Zhao. 2024. Cooperative evaluation in large language model refinement. arXiv preprint arXiv:2401.10234. Wen-tau Yang, Wen-tau Yih, Chris Meek, Alec Barnes, Zhiyuan Zhang, and Hannaneh Hajishirzi. 2017. Wikiqa: challenge dataset for open domain question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 814818. Association for Computational Linguistics. Yuanhao Yang, Qingqing Peng, Jian Wang, and Wenbo Zhang. 2024. Multi-llm-agent systems: Techniques and business perspectives. arXiv preprint arXiv:2411.14033. Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. Idea2img: Iterative self-refinement with gpt4v (ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Tree of thoughts: Karthik Narasimhan. 2023. Deliberate problem solving with large language models. Code repo with all prompts: https://github.com/ysymyth/tree-of-thoughtllm. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. 2024a. Cut the crap: An economical communication pipeline for arXiv preprint llm-based multi-agent systems. arXiv:2410.02506. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations (ICLR). Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, et al. 2024b. Finagent: multimodal foundation agent for financial trading: Tool-augmented, diversified, and generalist. arXiv preprint arXiv:2402.18485. 10 Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. 2024. Competeai: Understanding the competition dynamics in large language model-based agents. In Proceedings of the 41st International Conference on Machine Learning (ICML). Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and JÃ¼rgen Schmidhuber. 2024. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning."
        },
        {
            "title": "A Cost Analysis for Experiments",
            "content": "The total expenditure for the experiments across the MMLU dataset, WikiQA, and Camera (Japanese Ad Text Generation) tasks was approximately $2,100 USD. It is important to note that this amount reflects only the cost of final successful executions using the OpenAI 4o API (as TalkHier and almost all other baselines are built on OpenAI 4o backbone). Considering the failures encountered during our research phase, the actual spending may have been at least three times this amount. Below is detailed breakdown of costs and task-specific details. A.1 MMLU Dataset (1,450 USD) The MMLU dataset comprises approximately 16,000 multiple-choice questions across 57 subjects. For our experiments, we focused on five specific domains: A.1.1 Cost Analysis for the Moral Scenario Task and Baselines The Moral Scenario task involved generating and evaluating responses for various moral dilemma scenarios using OpenAIs GPT-4o model. Each generation task for single scenario produced approximately 48,300 tokens, with cost of about $0.17 per task. Given total of 895 tasks, the overall token consumption and cost were: 0.17 895 = 152.15 USD (1) In addition to the Moral Scenario task, we conducted multiple baseline tests using GPT-4o, which incurred an additional cost of approximately $3,000 USD. Therefore, the total cost for all GPT-4o evaluations in the Moral Scenario task is: 152.15 + 900 = 1052.15 USD (2) A.1.2 Cost Analysis for Other Tasks In addition to the previously analyzed tasks, we conducted further evaluations across multiple domains using OpenAIs GPT-4o model. These tasks include College Physics, Machine Learning, Formal Logic, and US Foreign Policy. The number of tasks and token usage per task varied across these domains, with each task consuming between 40,000 to 46,000 tokens and costing between $0.14 to $0.15 per task. College Physics: 101 tasks, each generating 40,000 tokens. Machine Learning: 111 tasks, each generating 40,000 tokens. Formal Logic: 125 tasks, each generating 46,000 tokens. US Foreign Policy: 100 tasks, each generating 45,000 tokens. The total expenditure for these tasks amounted to $63.43 USD. and we also did experiments for various baseline, it cost around 320 usd. totally it is 383.43. These costs reflect the computational demands required to evaluate domain-specific questions and ensure consistency in model performance across various knowledge areas. The total expenditure for these tasks amounted to $63.43 USD. Additionally, we conducted experiments with various baseline models, which incurred an additional cost of approximately $320 USD. In total, the overall expenditure was $383.43 USD. These costs reflect the computational demands required for evaluating domain-specific questions and ensuring consistency in model performance across various knowledge areas. A.2 WikiQA Dataset (1,191.49 USD) The WikiQA dataset comprises 3,047 questions and 29,258 sentences, of which 1,473 sentences are labeled as answers to their corresponding questions. Each question required generating approximately 36,000 tokens, with an average cost of $0.13 per question. Given this setup, the total expenditure for the WikiQA task was: 0.13 1, 473 = 191.49 USD (3) 12 In addition to the execution of TalkHier, we conducted multiple baseline tests using GPT-4o as their backbones, which incurred an additional cost of approximately $1,000 USD. Therefore, the total cost for all GPT-4o evaluations in the WikiQA task is: 191.49 + 1000 = 1191.49 USD (4) This cost reflects the computational requirements for processing and analyzing large-scale questionanswering dataset. The WikiQA task serves as an important benchmark for evaluating the models performance in understanding and responding to real-world queries. A.3 Camera Dataset (400.56 USD) The Camera dataset task involved generating and evaluating ad headlines for 872 different test sets using OpenAIs GPT-4o backbone. Each generation task produced approximately 65,000 tokens, with an average cost of $0.23 per task. Given this setup, the total expenditure for the Camera dataset task was: 0.23 872 = 200.56 USD (5) We also conducted experiments for three baseline models, which cost approximately $200 USD. In total, the expenditure amounted to $400.56 USD. This cost reflects the iterative process of generating and refining ad headlines across multiple input sets, ensuring high-quality and effective outputs tailored to the datasets domain-specific requirements."
        },
        {
            "title": "B Prompt Design and Work Flow for Tasks in MMLU",
            "content": "In this section, we describe the prompt design for evaluating and revising responses for each MMLU task. The task involves generating, evaluating, and refining answers to ethical dilemmas or moral situations using our multi-agent framework. Each agent in the framework plays distinct role: generating potential solutions, evaluating their moral alignment, and revising answers to improve coherence and alignment with evaluation results. The prompts used for each agent are detailed below. B.1 Initial Prompt The following is the prompt given to the supervisor at the beginning. Initial Prompt You are an expert in [Task]. You must find the answer to the following question: [Question] The choices you are given are: [Choices] You can split up the problems into smaller parts if required. The final answer must be only in the dictionary form of: [Output format] B.2 Answer Generator This agent generates answers to specific moral scenario by considering the ethical implications of the situation."
        },
        {
            "title": "Answer Generator Prompt",
            "content": "You are an Answer Generator that has access to tools, to think of an answer for specific given problem. Required Input: Requirements as messages Final output: Expected answer as intermediate_output in the form of [Output format] B.3 Answer Evaluator This agent evaluates the answers generated by the Answer Generator, providing scores and feedback based on predefined metrics such as ethical soundness, logical consistency, fairness, and feasibility."
        },
        {
            "title": "Evaluator Team Supervisor Prompt",
            "content": "You are an Answer Evaluator Team that has to evaluate the given answer. The metrics are: [Metrics] Required Input: Expected answer as intermediate_output Final output: Expected Answer and evaluation results embedded into intermediate_output in the form of [Output format] B.4 Answer Revisor This agent revises answers that receive low scores in the evaluation step. Revisions must strictly follow the evaluation results to ensure improved alignment with the metrics. 14 Answer Revisor Prompt You are an Answer Revisor that receives an answer with their evaluation results, and outputs, if necessary, revised answer that takes into account the evaluation results. Follow these steps for revision: 1. You MUST first make detailed analysis of ALL answers AND evaluation results. Double check that the evaluation results and reasons align with each other. 2. Based on the analysis, check if at least three of the four evaluations support each answer. 3. If an answer is not supported by the majority of evaluations, you must flip the specific answer, making sure to update the choices as well 4. In your final output, state: 1) If you need re-evaluation which is necessary if new modification has been made, and 2) The reasons behind your revisions. B.5 Settings for each Task B.5.1 Evaluator Types B.5.2 Tools To enhance the evaluation capabilities of each agent, we have deployed tools for each evaluator to use. The tools are listed as follows: Output Tool (All Evaluators): tool for outputting thoughts, allowing the model to repeatedly think. Truth Table Generator (Truth Table Evaluator): tool for outputting truth table, given proposition as input. Counterexample Verifier (Truth Table Evaluator): tool for verifying whether counterexample is correctly defined. Here, the evaluator shown in the brackets are those who have access to the specific tool. B.6 Good Revision Example for Moral Scenarios Task The following example demonstrates how the multi-LLM framework revises an answer for moral scenario. It includes the problem statement, the generated answer, the evaluation results, and the final revised answer, highlighting the reasoning process behind the revision."
        },
        {
            "title": "Good Revision Example",
            "content": "Problem: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1: spoke to my mom on the phone during my entire work shift. Scenario 2: helped my friend escape prison. Correct Answer: 0 (Wrong, Wrong) Generated Answer: 2 (Not Wrong, Wrong) Evaluation Results: Scenario 1: - Normality: Correct - Intent: Correct - Responsibility: Incorrect - Well-being: Incorrect Scenario 2: - All metrics: Correct Revisor Reasoning: Based on the detailed evaluation results, the provided answer needs to be revised. - The evaluation indicates that Scenario 1 is considered irresponsible and promotes harm to the well-being of everyone involved, which contradicts the generated answer of Not Wrong. Therefore, the correct assessment is Wrong. - The evaluation confirms that Scenario 2 is considered irresponsible and harmful, which aligns with the generated answer of Wrong. Revised Answer: 0 (Wrong, Wrong) 15 Task Metric Description Moral Scenarios College Physics Intent Normality Responsibility Well-being Mathematics Physics Evaluates the intentions behind actions. Evaluates how normal the action is. Evaluates the degree of responsibility behind the action. Evaluates whether the action promotes well-being. Evaluates mathematical correctness and calculations. Evaluates the accuracy of physical principles applied. Answer Consistency Checks underlying assumptions in models and"
        },
        {
            "title": "US Foreign Policy",
            "content": "methodologies. Evaluates machine learning concepts and implementation. Evaluates whether the solution is sound in stastical terms. Evaluates whether the arguments used are logically correct. Evaluates correctness of generated truth tables and implied results. Evaluates whether the counterexamples are utilized correctly. Evaluates correctness of the use of predicate logic formulas. Evaluates correctness in terms of formal logic. Evaluates whether the facts implied by the answer are correct. Evaluates whether the answer aligns with established policies and agreements. Evaluates whether the answer accurately represents the core definition. Table 6: List of evaluators allocated for each task"
        },
        {
            "title": "C Prompt Design and Work Flow for for WikiQA",
            "content": "In this section, we provide detailed example of how the multi-agent framework processes WikiQA task, specifically the question: \"What are points on mortgage?\" This example demonstrates how agents interact to generate, evaluate, and revise an answer, ensuring that it meets all necessary criteria for accuracy, clarity, and completeness. C.1 Initial Question The user asks the question: \"What are points on mortgage?\" C.2 Step 1: Answer Generation The first step involves the Answer Generator agent, which is tasked with generating detailed response to the question. It considers the key components of the topic, such as mortgage points, their function, cost, and benefits. Answer Generator Prompt You are an Answer Generator with access to tools for generating answers to specific questions. Your task is to: 1. Analyze the given problem deeply. 2. Use the tools provided to retrieve and synthesize information. 3. Craft detailed and coherent response. Required Input: Question and relevant details as messages. Final Output: Expected answer as intermediate_output, formatted as follows: { } \"answer\": \"One sentence answer\", \"details\": \"Supporting details or explanation\" The Answer Generator produces the following response: \"Points on mortgage are upfront fees paid to the lender at the time of closing, which can lower the interest rate or cover other loan-related costs, with each point typically costing 1 C.3 Step 2: Evaluation by the ETeam Supervisor The ETeam Supervisor evaluates the answer based on two primary metrics: Simplicity and Coverage. The Simplicity Evaluator checks if the answer is concise and well-structured, while the Coverage Evaluator ensures that the response includes all relevant keywords and details."
        },
        {
            "title": "ETeam Supervisor Prompt",
            "content": "You are an ETeam Supervisor tasked with evaluating answers using the following metrics: 1. Coverage: Does the answer contain all related information and keywords? - List all relevant keywords related to the problem. - Provide explanations for each keyword and its relevance. 2. Simplicity: Is the answer concise and easy to understand in one sentence? - Check for redundancies and ensure appropriate sentence length. Steps for Evaluation: 1. Summarize the conversation history. 2. Provide detailed analysis of the problem using output tools. 3. Evaluate the most recent answer based on the metrics above. The Simplicity Evaluator concludes that the answer is clear, concise, and without any redundant information. The sentence is appropriate in length, neither too short nor too long. The Coverage Evaluator confirms that the answer covers all the necessary aspects, including keywords such as \"points,\" \"upfront fees,\" \"lender,\" \"closing,\" \"interest rate reduction,\" and \"cost of points.\" C.4 Step 3: Revisions by the Answer Revisor Despite the high evaluation scores, the Coverage Evaluator suggests slight revision for clarity. The Answer Revisor agent makes minor adjustment to improve the answers conciseness while maintaining its accuracy and comprehensiveness. Answer Revisor Prompt You are an Answer Revisor responsible for refining answers based on evaluation results. Follow these steps: 1. Analyze the generated answer and evaluation results in detail. 2. Check if all metrics have near full scores (coverage and simplicity). 3. Revise the answer if required to address any shortcomings. 4. State whether re-evaluation is necessary and justify your revisions. Important Notes: - Do not introduce personal opinions. - Ensure all changes strictly align with the evaluation feedback. The Answer Revisor makes the following revision: \"Points on mortgage are fees paid upfront to the lender at closing, which can lower the interest rate or cover other loan-related costs, with each point usually costing This slight modification enhances clarity without altering the meaning of the original response. C.5 Step 4: Final Evaluation The revised answer is re-evaluated by the ETeam Supervisor, and all metrics receive top scores. The revised response is clear, concise, and includes all relevant keywords and information, making it easy to understand. C.6 Final Answer After going through the generation, evaluation, and revision steps, the final answer to the question \"What are points on mortgage?\" is: \"Points on mortgage are fees paid upfront to the lender at closing, which can lower the interest rate or cover other loan-related costs, with each point usually costing Evaluation Summary: - Simplicity: The answer is clear, concise, and free of redundancies. - Coverage: The answer includes all necessary keywords and information, covering key aspects such as \"points,\" \"upfront fees,\" \"lender,\" \"closing,\" \"interest rate reduction,\" and \"loan-related costs.\" The final answer has received high scores in all evaluation metrics, confirming its quality and effectiveness in answering the users question. C.7 BERT and ROUGE Scores To further evaluate the quality of the answer, we compute BERT and ROUGE scores: - BERT Score: 0.5156 - ROUGE Score: 0.2857 These scores indicate that the answer is both accurate and well-aligned with reference answers. 18 Prompt Design, Workflow and Revision Examples for Evaluating the Camera Dataset In this section, we introduce our multi-LLM agent framework, versatile and generalizable design for generating, evaluating, and refining ad text in various contexts. The framework is designed to handle tasks such as creating high-quality ad headlines, assessing their effectiveness based on key metrics, and improving underperforming content. Rather than being tailored to specific dataset or domain, our framework adopts modular structure where each agent is assigned well-defined role within the pipeline. This design enables seamless integration with various tools and datasets, making it applicable to wide range of ad text tasks beyond the Camera dataset. The prompts used for each agent reflect balance between domain-agnostic principles and task-specific requirements, ensuring adaptability to diverse advertising scenarios. The following sections provide the prompts used to define the roles of the agents within the framework. D. Japanese Ad Headlines Generator This agent generates high-quality Japanese ad headlines that are fluent, faithful, and attractive. It leverages tools such as character counter, reject words filter, and Google search for contextual information. The specific prompt for this agent is:"
        },
        {
            "title": "Generator Prompt",
            "content": "You are Japanese Ad headlines Generator that has access to multiple tools to make high faithfulness, fluent, and attractive headlines in Japanese. Make sure to use the Google search tool to find information about the product, and the character_counter to check the character count constraint. Also, check that it does not contain bad words with the reject_words tool. Input: Requirements as messages. Final Output: dictionary in Japanese in the form: {\"Headline\": [Headlines]} D.2 Ad Headlines Evaluator This agent evaluates the generated headlines based on three metrics: Faithfulness, Fluency, and Attractiveness. The specific prompt for this agent is:"
        },
        {
            "title": "Evaluator Team Supervisor Prompt",
            "content": "You are an Ad headlines Evaluator that evaluates and scores every single headline to see if it meets the criteria of good Ad text in Japanese. The metrics are: Faithfulness, Fluency, and Attractiveness. Input: dictionary in the form: {\"Headline\": [Headlines]} Final Output: dictionary in Japanese in the form: {\"Headline\": [Headlines], \"Scores\": [Faithfulness, Fluency, Attractiveness]} D.3 Ad Headlines Reviser This agent revises low-scoring headlines to improve their Faithfulness, Fluency, and Attractiveness scores. The specific prompt for this agent is: 19 Revisor Prompt You are an Ad Keyword Reviser that receives dictionary in the form: {\"Headline\": [Headlines], \"Scores\": [Faithfulness, Fluency, Attractiveness]} and their three scores for Faithfulness, Fluency, and Attractiveness as input. You must modify the low-scoring headlines to improve their scores. Make sure to use the character_counter to check the character count constraint. Input: dictionary in the form: {\"Headline\": [Headlines]} Final Output: dictionary in Japanese in the form: {\"Headline\": [Revised Headlines]} without any scores, just the revised text. D.4 Tools Used in the Camera Ad Text Experiment To facilitate the generation, evaluation, and refinement of ad text for the Camera dataset, we implemented set of specialized tools. These tools were designed to support various aspects of the ad text generation process, including character limit enforcement, search retrieval, click aggregation, and content filtering. Below is description of each tool: Character Counter (Generator and Revisor): utility for counting the number of characters in given sentence. It takes as input list of lists in the form [[sentence, character limit], [sentence, character limit], ...], where each sentence is checked against predefined character limit. Google Search (Generator): search engine tool used to retrieve real-time information from the web. This tool is particularly useful for answering queries related to current events based on search queries. Output Tool (All Agents): simple logging tool that allows agents to write their thoughts. This tool does not return any output but serves as an internal documentation mechanism. Bad Performance Retriever (Revisor): quality control tool that checks whether generated headlines or descriptions resemble undesirable outputs. It takes as input dictionary in the form {\"Headline\": [headline1, ...], \"Description\": [description1, ...]} and returns list of flagged items if any match known bad examples. Reject Word Checker (Generator and Revisor): filtering tool that verifies whether sentence contains prohibited words. It processes list of sentences and flags any containing words that should not be included. These tools collectively enable structured ad text generation by enforcing constraints, retrieving relevant information, filtering out undesired outputs, and aggregating performance metrics. Their integration ensures high-quality and compliant ad text generation. D.5 Ad Headline Revisions with Highlights Tables 7 and 8 present two cases of translated ad headline revisions: one for educational ads and the other for employment-related ads. The revisions were made to enhance the clarity, specificity, and overall effectiveness of the headlines while maintaining their original intent. In these tables, text highlighted in green represents good revision, where improvements were made to make the ad more engaging, informative, or persuasive. These modifications focus on strengthening key selling points, increasing emotional appeal, and ensuring that the message is clear to potential users. For instance, in Table 7, the phrase \"Challenge prestigious school entrance exams\" was revised to \"Support your challenge to enter prestigious schools\" to emphasize the supportive nature of the service rather than just the difficulty of the exams. Similarly, in Table 8, the phrase \"Get job with Baitoru 20 Table 7: Revisions of Educational Ad Headlines with Highlights (Original: Japanese, Translated: English). The table shows functional translations for better readability while preserving the intent and effectiveness of the revisions. Before Revision After Revision Challenge prestigious school entrance exams Guidance from professional home tutors We provide sure-win exam preparation Improve grades with customized curriculum Prepare for exams online Support your challenge to enter prestigious schools High-quality guidance from professional home tutors We provide reliable exam preparation Boost grades with customized curriculum Effective exam preparation online Table 8: Revisions of Employment Ad Headlines with Highlights (Original: Japanese, Translated: English). The table shows functional translations for better readability while preserving the intent and effectiveness of the revisions. Before Revision After Revision Get job with Baitoru NEXT Job change and employment with Baitoru NEXT Aim to debut with Baitoru NEXT Start your job search Find new workplace Opportunity to aim for debut Find your ideal job with Baitoru NEXT For career change and employment, use Baitoru NEXT Start your career with Baitoru NEXT Take the first step in your career Discover new job opportunities Opportunities for successful debut NEXT\" was revised to \"Find your ideal job with Baitoru NEXT\", making the headline more appealing by highlighting personalization and career goals. These refinements contribute to more effective ad communication, ensuring that potential users better understand the value proposition of the services being advertised. D.6 An example of Hierarchical Refinement with Faithfulness, Fluency, Attractiveness TalkHier employs hierarchical refinement process where evaluators independently assess content (faithfulness, fluency, and attractiveness) and report their findings to an evaluation team supervisor. This supervisor synthesizes the feedback, ensuring reduced bias and improving the generated results. Below, we provide examples of refinements in headlines related to ISAs Office courses, illustrating improvements in faithfulness, fluency, and attractiveness. Faithfulness Refinement: Initial headline: Fastest qualification with ISA courses. This headline lacked specificity and could mislead users. After refinement: Achieve qualification in two weeks with ISA courses. This correction provides an accurate depiction of the course duration. Fluency Refinement: Initial headline: ISA courses: beginner friendly. While understandable, the phrase was somewhat unnatural. After refinement: Beginner-friendly ISA courses. 21 This adjustment enhances grammatical accuracy and improves readability. Attractiveness Refinement: Initial headline: Boost skills with ISA Office courses. This headline, though factual, lacked emotional appeal. After refinement: Advance your career with ISA Office courses. This modification creates more engaging and motivational message for potential users."
        },
        {
            "title": "E Subjective Experiment for the Rating in TalkHier",
            "content": "In this section, we describe our experimental setup for evaluating the quality of automatically generated advertisement headlines. Our proposed method, TalkHier, is multi-agent system designed to refine generated text by iteratively assessing and improving headlines across three key dimensions: attractiveness, fluency, and faithfulness. The refinement process relies on these internal evaluations to guide improvements. However, to ensure that these automated assessments capture human notions of headline quality, we must verify their consistency with human judgments. If TalkHiers multi-agent evaluations diverge significantly from human perceptions, the systems refinements lose practical value. We therefore compare TalkHier against baseline, generating headlines using both methods. We then collect ratings from human evaluators as well as from TalkHiers own evaluation agents, and measure how closely the automated scores correlate with human ratings on attractiveness, fluency, and faithfulness. Demonstrating that these internal metrics align with human judgment is essential to validate our multi-agent refinement system. E.1 Setup and Data Collection We selected five distinct products, each of which serves as target for generating advertisement headlines. For each product, we generated five headlines using TalkHier (for total of 25) and five headlines using the baseline model (another 25), thus obtaining 50 headlines in total. All headlines were evaluated by four human raters using five-point scale (1 = very poor to 5 = excellent). We also prompted GPT to rate each of these 50 headlines on the same 15 scale, effectively treating GPT as fifth rater. E.2 Data Example Table 9 provides small subset of our dataset to illustrate how the information is organized. Each row corresponds to one generated headline and includes (i) the product name or headline identifier, (ii) the method that generated it, (iii) the generated text, and (iv) the ratings assigned by subset of the human evaluators and TalkHier. Table 9: sample of 10 headlines for the credit card product (LifeCard). Five are generated by TalkHier, and five by the baseline ReAct. We show partial ratings (three of the four human raters plus the TalkHier evaluation team) to illustrate how TalkHier generally receives higher scores than the Baseline. Headline Method Generated Headline (English) Human1 Human2 Human... TalkHier H1_card H2_card H3_card H4_card H5_card TalkHier TalkHier TalkHier TalkHier TalkHier Card That Fits Your Lifestyle LifeCard with No Annual Fee Receive Your Card in Two Business Days Earn Points for Every 100 You Spend Triple Points on Your Birthday Month ReAct H6_card ReAct H7_card ReAct H8_card H9_card ReAct H10_card ReAct Full of Benefits, LifeCard is Here Start New Life with LifeCard Save Smartly with LifeCard Shop with LifeCard Trusted and Reliable Life Card 4.33 5 4.33 4.33 2.33 3.66 2.33 3.66 3.66 3.66 4.33 4.66 5 4.33 3 3.66 4.33 3.66 4 ... ... ... ... ... ... ... ... ... ... 5 4 4.33 5 4 3 2.33 3 3 3.66 . . . (remaining headlines not shown) As shown in Table 9, each headline in the dataset includes: Headline ID: unique identifier (e.g., H1_favs) that can encode product information. Method: Either TalkHier (proposed method) or Baseline (GPT-4.0 or other reference model). Generated Headline: The actual text shown to human raters. Human Ratings: Numerical scores (15) from four human evaluators (for brevity, only two are shown here). TalkHier Rating: TalkHiers rating, also on 15 scale. For brevity, we show ratings from only two human raters here; the full dataset includes four human raters. 23 E.3 Evaluation Metrics To determine whether TalkHier evaluates headlines similarly to human raters, we compute both (i) the correlation (Pearson and Spearman) between TalkHiers ratings and the average human ratings, and (ii) the Intraclass Correlation Coefficient (ICC), treating TalkHier as an additional rater alongside the four humans. We report both ICC(2,1), which assesses agreement with individual raters, and ICC(2,4), which evaluates agreement with the collective human consensus. E.4 Evaluation Results We quantitatively assessed how closely TalkHiers ratings align with the human evaluations using both (i) correlations (Pearson and Spearman) between TalkHiers ratings and the average ratings of the four human evaluators, and (ii) the Intraclass Correlation Coefficient (ICC) treating TalkHier as an additional rater. Table 10 summarizes our main findings. Table 10: Summary of evaluation metrics demonstrating how closely TalkHiers scores align with human ratings for the 10 generated headlines. Confidence intervals (CIs) are not reported due to the small sample size. Metric Value p-value Pearson Correlation Spearman Correlation ICC (2,1) ICC (2,4) 0.67 0.68 0.23 0.33 0.036 0.030 Correlation Analysis. We computed Pearsons and Spearmans correlations between TalkHiers ratings (15 scale) and the mean human rating for each of the 10 headlines. Both correlation coefficients, shown in Table 10, indicate moderate positive relationship (Pearson: 0.67, Spearman: 0.68), and both are statistically significant (p < 0.05). Intraclass Correlation (ICC). We further treated TalkHier as an additional rater alongside the four human judges and computed both ICC(2,1) and ICC(2,4). As reported in Table 10, ICC(2,1) is 0.23, indicating poor agreement between TalkHier and individual human raters. However, ICC(2,4) is higher at 0.33, indicating moderate agreement between TalkHier and the aggregated human ratings. Why ICC(2,4) is higher than ICC(2,1)? The difference between ICC(2,1) and ICC(2,4) suggests that TalkHiers ratings align more closely with the average human judgment rather than any specific individual rater. This could be due to variability among human raters, meaning individual ratings are inconsistent, but their mean rating is more stable. Since ICC(2,4) evaluates agreement with the collective human consensus, the improved score indicates that TalkHier captures general human preferences better than individual opinions. Overall Implications. These results suggest that while TalkHier does not perfectly replicate individual human ratings, it effectively captures broader human consensus. Thus, using TalkHier to evaluate the generated ad text is reasonable, and its evaluation could provide relatively meaningful feedback to refine the ad text."
        }
    ],
    "affiliations": [
        "Sony Group Corporation, Japan"
    ]
}