{
    "paper_title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
    "authors": [
        "Zhen Yang",
        "Zi-Yi Dou",
        "Di Feng",
        "Forrest Huang",
        "Anh Nguyen",
        "Keen You",
        "Omar Attia",
        "Yuhao Yang",
        "Michael Feng",
        "Haotian Zhang",
        "Ram Ramrakhya",
        "Chao Jia",
        "Jeffrey Nichols",
        "Alexander Toshev",
        "Yinfei Yang",
        "Zhe Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of $91.6\\%$, $53.3\\%$, and $61.2\\%$ on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of $28.0\\%$ on AndroidWorld and $19.8\\%$ on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 9 3 5 6 2 . 9 0 5 2 : r Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents Zhen Yang, Zi-Yi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, Ram Ramrakhya, Chao Jia, Jeffrey Nichols, Alexander Toshev, Yinfei Yang, Zhe Gan Apple First authors, Project lead Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale In GUI grounding, Ferret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, GUI agents. ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents. Date: October 1,"
        },
        {
            "title": "Introduction",
            "content": "Autonomous agents, which directly interact with graphic user interfaces (GUIs) to accomplish human tasks, are emerging technologies with the potential of revolutionizing the computer industry and GUI automation (OpenAI, 2025; Claude, 2024; Durante et al., 2024; Qin et al., 2025; Wang et al., 2025a). Imagine GUI assistant that instantly helps you write down reminder while youre driving, or displays your favorite recipe while your hands are wet in the kitchen. Many of these scenarios require low latency, strong privacy guarantee, and robustness under limited connectivity, necessitating the development of small, on-device GUI agents (Li et al., 2025a; Belcak et al., 2025). (a) GUI Grounding. (b) GUI Navigation. Figure 1 Comparing Ferret-UI Lite with other end-to-end GUI agents. Our model achieves strong results on GUI grounding tasks, surpassing many larger models. However, its performance on multi-step navigation remains limited, underscoring the inherent challenges of developing lightweight, on-device agents capable of robust long-horizon reasoning. The majority of existing methods on GUI agents, contrarily, focus on large foundation models. For example, traditional multi-agent system design with separate perception, planning, and action components is built on top of general-purpose large language models (LLMs) (such as GPT (Achiam et al., 2023) and Gemini (Team et al., 2024)). The strong reasoning and planning capabilities of large server-side models allow these agentic systems to achieve impressive capabilities in diverse GUI navigation tasks (Yan et al., 2023). However, the usage of Figure 2 An illustration of Ferret-UI Lite on multi-step GUI navigation task. Human users prompt with high-level goal in plain text, and the model autonomously interacts with GUI devices through tapping, scrolling, typing, etc., until the task is complete. At each step, the model observes the GUI screen, generates think-plan-act traces, and executes the action. large models and multi-agent paradigm increases modeling complexity, compute budget requirements, and inference time (Chen et al., 2023). End-to-end GUI agents offer an attractive alternative by streamlining the agentic workflow, directly mapping raw GUI screenshots to actions (Yang et al., 2025a; Wang et al., 2025a; Seed, 2025; Lei et al., 2025; Team et al., 2025). However, larger models are still preferred for end-to-end agents (Wang et al., 2025b; Bai et al., 2025; Yang et al., 2025a; Wang et al., 2025a), in part because diverse agentic capabilities need to be incorporated into one single model, including low-level GUI grounding, screen understanding, multi-step planning, and self-reflection (we refer readers to Section in the Appendix for detailed overview). Building competitive small on-device end-to-end agents remains challenging. In this paper, we explore the strategies to develop strong small GUI agents targeted for on-device deployment (Li et al., 2025a). We present Ferret-UI Lite, 3B end-to-end multimodal LLM for GUI agentic tasks. FerretUI Lite is built with several key components, guided by insights on training small-scale LMs: (1) Curating both real and synthetic GUI training data from large number of sources with unified action space across diverse GUI domains. (2) Inference-time techniques through visual tool-use with image cropping and zoom-in to achieve high-resolution GUI perception. (3) Adapting two-stage training strategy with supervised fine-tuning (SFT) and reinforcement learning (RL). At the SFT stage, we collect online trajectories from multi-agent rollout pipeline. At the RL stage, we introduce step-wise reinforcement learning with verifiable rewards, applying it to visual tool-use grounding tasks and to multi-step navigation tasks. As result, Ferret-UI Lite (3B) shows competitive GUI grounding and navigation performance compared to other models of the same size and outperforms many larger models. For example, on the ScreenSpot-Pro GUI grounding benchmark, our model achieves 53.3% accuracy, surpassing UI-TARS-1.5 (7B) (Seed, 2025) by over 15% (Figure 1a). However, on the GUI navigation task, Ferret-UI Lite (3B) shows limited performance compared to larger models, with only on-par performance with its UI-TARS-1.5 (7B) (Seed, 2025) on the OSWorld benchmark (Figure 1b), highlighting the challenges of developing lightweight, on-device agents for multi-step navigation. We conduct series of experiments to investigate the capabilities and limitations of small GUI agents. The results indicate that GUI grounding and navigation data can mutually benefit each other, with balanced mixture ratio achieving the best overall results. Moreover, the curation of synthetic data from diverse sources, such as high-resolution grounding data and online rollouts from multi-agent system, significantly improves grounding and navigation performance. Furthermore, inference-time techniques such as CoT reasoning and visual tool-use bring improvements, yet the benefits remain limited. While small models can benefit from reinforcement learning, they are sensitive to RL reward designs, underscoring the difficulty of designing robust rewards across heterogeneous UI agentic tasks. We anticipate that these findings will provide valuable guidance to the community in the development of effective on-device GUI agents. Figure 3 Model architecture and training recipes of Ferret-UI Lite. The model takes GUI screen and the user instruction as inputs, and predicts chain-of-thought reasoning traces and low-level action policy to control GUI devices in an end-to-end manner directly. The model is trained through supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR)."
        },
        {
            "title": "2 Supervised Fine-Tuning",
            "content": "Training reliable GUI agents requires comprehensive supervision that spans the full spectrum of interaction types, visual contexts, and device platforms, which is important for smaller models that require large number of diverse training tokens to achieve competitive performance (Kaplan et al., 2020). We curate both human-annotated and synthetic datasets to enhance scale, coverage, and diversity of interaction patterns. These datasets include public benchmarks curated from diverse multi-platform corpora and systematically generated synthetic trajectories, which we then combine into unified SFT recipe. We consolidate these heterogeneous data sources and align them under consistent annotation and action schema, establishing foundation for developing GUI agents."
        },
        {
            "title": "2.1 Data Distribution",
            "content": "For SFT, we draw on diverse collection of public GUI grounding and navigation datasets, including GroundUI (Zheng et al., 2024), OSAtlas (Wu et al., 2025), UGround (Gou et al., 2025), Aria-UI (Yang et al., 2025b), Aguvis (Xu et al., 2025), WaveUI (Wu et al., 2023; Dwyer, 2022; Zheng et al., 2024), ShowUI (Lin et al., 2024), Jedi (Xie et al., 2025), and AgentNet (Wang et al., 2025b). Additionally, we generate synthetic datasets for mobile and OS platforms for both grounding and navigation, which will be detailed later. Together, these resources span multiple platforms and supervision types, forming comprehensive basis for training generalizable GUI agents capable of grounding and navigation across varied environments. Figure 11 in Appendix illustrates the SFT data distribution, and we present dataset ablations in the experiment section and Appendix F."
        },
        {
            "title": "2.2 Format Unification",
            "content": "To effectively leverage the heterogeneous supervision provided by public datasets, we unify their annotation formats into consistent training interface (see Appendix C). This unification ensures that the model can learn from diverse sources without overfitting to dataset-specific schemas and enables seamless multi-source training across grounding and navigation tasks. Grounding. Datasets differ in how they specify interactive regions: some use bounding boxes, while others provide single-point coordinates. We normalize all targets to point-based representation by mapping bounding boxes to their geometric centers, (xcenter, ycenter) = (cid:0) xmin+xmax (cid:1) , where (xmin, ymin) and (xmax, ymax) denote the box corners. Point-based annotations are left unchanged. Natural language templates , ymin+ymax 2 2 3 Figure 4 Synthetic navigation data generation pipeline, which consists of offline data generation based on humanannotated trajectories, and online rollouts collection from multi-agent system. referencing the computed points provide unified supervision interface across datasets, allowing the agent to generalize effectively to unseen instruction styles. Navigation. For action supervision, we define unified action space spanning mobile, desktop, and web environments. Following the taxonomy of Qin et al. (2025), we categorize actions into shared and domainspecific types, resulting in eleven representative actions summarized in Table 5. While prior work encodes actions as free-form text tokens (Lin et al., 2024; Qin et al., 2025) or through specialized latent tokenizers (Bruce et al., 2024; Brohan et al., 2022; Szot et al., 2024), we instead adopt function-call representation inspired by tool-use paradigms (Schick et al., 2023). Each action is formalized as predefined function with constrained parameters, yielding structured outputs that enhance interpretability, facilitate extraction for downstream evaluation, and naturally align with the coding and tool-use abilities of modern LLMs."
        },
        {
            "title": "2.3 Synthetic Data Generation",
            "content": "While existing public datasets provide rich supervision signals, their scale and diversity remain insufficient for training robust GUI agents. To bridge this gap, we carefully design synthetic data generation pipelines covering various scenarios. High-resolution grounding data. To enhance grounding supervision, we construct high-resolution samples by concatenating multiple GUI screenshots into larger composite images (e.g., from OSAtlas (Wu et al., 2025)). This exposes the model to denser layouts and richer spatial contexts, enabling more precise localization in realistic multi-element environments. Existing annotations are converted into the unified point-based format described in Section 2, ensuring consistency across datasets. CoT navigation data. We collect three key CoT reasoning traces to enhance multi-step navigation, similar to Zhang et al. (2024b); Seed (2025): (i) plan (concise description of next action), (ii) action think (reasoning over GUI elements, history traces, and candidate actions), and (iii) reflect (self-assessment relative to the goal). Each component is generated separately by GPT-4o using set-of-marks (SoM) visual prompting (Yang et al., 2023), conditioned on the human-annotated action for the current screen and the episode history. Combined with those reasoning traces, we instantiate two CoT-enabled GUI agents with different compute profiles: (i) short-CoT only adds plan reasoning trace before the action output, and (ii) long-CoT extends to action think and reflection component, capturing richer dependencies among GUI states, history traces, goals, and actions. Synthetic QA data. To support assistive capabilities such as answering user queries (e.g., What items are left in my Amazon shopping cart?\"), we generate synthetic visual QA pairs based on the existing episodes, by rewriting the episode goal into natural language question and providing an answer in the terminal state, grounded by the last GUI screen. Furthermore, we improve agents with replanning skills by deliberately perturbing clean trajectories with error-prone frames. For instance, correct terminate action can be replaced with an erroneous swipe, simulating stuck navigation state. The corresponding correction sequence is then generated to demonstrate recovery strategies. 4 Online navigation data. We design multi-agent system, inspired by Shinn et al. (2024); Ramrakhya et al. (2025), that interacts directly with GUI platforms to generate synthetic rollouts at scale. These online trajectories introduce action errors, environmental stochasticity, and various replanning strategies that are absent from human-annotated data. The system comprises four components, as illustrated in Figure 4: curriculum task generator that produces goals of increasing difficulty, planning agent that decomposes goals into step-level instructions, grounding agent that executes actions, and critic model that evaluates trajectories and provides textual rewards to the planning agent. The online trajectories are further enriched with chain-of-thought reasoning traces and filtered by VLM-as-a-judge pipeline to remove low-quality or inconsistent samples."
        },
        {
            "title": "3 Reinforcement Learning",
            "content": "SFT provides foundation for grounding and navigation, yet it enforces strict imitation of annotated outputs and does not fully exploit the flexibility of GUI interaction tasks. In many scenarios, reasoning traces may vary in form while leading to the same correct action. Reinforcement learning with verifiable rewards (RLVR) addresses this limitation by introducing rule-based, automatically computable rewards that align model training with task success rather than surface-level annotation matching. By grounding optimization in verifiable outcomes, RLVR enables the model to refine both grounding accuracy and reasoning quality in scalable and noise-tolerant manner. RL for grounding. To further improve grounding beyond SFT, we apply reinforcement learning on OSAtlas (Wu et al., 2025). Unlike SFT, which constrains the model to reproduce the annotated center point exactly, RL allows us to design reward function that reflects the true goal of grounding: any prediction falling within the annotated bounding box is considered acceptable. We therefore adopt simple containment-based reward that assigns positive feedback whenever the predicted location lies inside the ground-truth box. To enhance robustness, we further introduce zoom-in mechanism inspired by recent research on thinking with images (OpenAI, 2025; Shao et al., 2024a; Fan et al., 2025). After the model produces an initial prediction, the image is cropped around the predicted location, and the model makes refined prediction on this cropped region. This process mimics human behavior of zooming in for detail, and proves especially beneficial for complex or high-resolution user interfaces. This process also allows our small model to only consider small region for the final grounding decision, reducing the need for complex, nuanced understanding across large number of image tokens that might require larger models. Both initial and refined predictions are retained in the training pool, providing the model with multi-scale grounding supervision. Figure 5 Ferret-UI Lite employs zoom-in operation to refine predictions. It generates an initial prediction based on the given instruction, crops the image around this prediction, and finally re-predicts on the cropped region for improved accuracy. RL for navigation. For navigation, we use our mobile and desktop synthetic datasets and AgentNet (Wang et al., 2025b) during RLVR training. The prompt includes the current screenshot, the high-level instruction, and the history of past actions. Given this input, the model samples number of candidate outputs denoted by = [z1, . . . , zi, . . . , zM ], where zi = [ci; ai] consists of chain of thought text ci followed by predicted action ai = [τi; θi], with action type τi and its parameters θi (e.g., tapping location). reward scalar, r, is then computed by comparing the generated action with its ground-truth (agt = [τ gt; θgt]), using reward functions. Specifically, the final reward is computed by the sum of an action type match function, ftype, and an action parameter match function, fparam, with ri = ftype(τi, τ gt, θgt) + fparam(θi, θgt) . This formulation separates correctness into two complementary components. The first, ftype, verifies whether the predicted action type matches the ground truth. If no parameters are expected (θgt = ), perfect type match is rewarded more strongly, while if parameters are required, the type match provides partial credit: ftype(τi, τ gt, θgt) = 2, 1, 0, if τi = τ gt and θgt = , if τi = τ gt and θgt = , otherwise. (3.1) The second component, fparam, evaluates the fidelity of the predicted parameters. For string-based parameters (e.g., text entry or direction), we adopt an exact-match function which assigns 1 if predicted and ground-truth string parameters are matched, and 0 otherwise. For location-based actions such as tap, we experiment with sparse reward, which is the same as our grounding reward, assigning 1 if the predicted coordinate lies inside the ground-truth bounding box, and 0 otherwise. We also design dense reward dense param that provides graded score based on the normalized distance between the predicted and ground-truth centers: param(θi, θgt) = max dense (cid:18) 1 λ (cid:18) xi xgt + yi ygt (cid:19) (cid:19) , 0 , (3.2) where (xi, yi) and (xgt, ygt) denote the predicted and ground-truth centers of the UI element, and and its ground-truth width and height, respectively. The decay factor λ is used for controlling sensitivity, and is set to 0.5. Together, these reward functions allow navigation training to balance categorical correctness with parameter precision, encouraging the model not only to predict the right action type but also to refine its execution details. Model training. Both grounding and navigation RL are optimized using Group Relative Policy Optimization (GRPO) (Shao et al., 2024b). For each training example, multiple predictions are sampled: 8 from the original image and 4 from the zoomed-in crop for grounding, and 32 candidate outputs for navigation. Each sample receives reward ri computed by the task-specific reward functions, and normalized advantage Ai = rimean(r) , = [r1, r2, ..., rM ] is calculated within the group to stabilize optimization. std(r) To further improve training efficiency, we apply online filtering (Yu et al., 2025). Prompts for which all sampled rewards are identical (e.g., uniformly 0 or 1) are discarded, as they provide no meaningful learning signal. By retaining only the most informative examples, the model focuses on cases that sharpen its decision boundaries and refine its reasoning. This unified optimization strategy ensures that RL improves both fine-grained grounding precision and multi-step navigation robustness, while remaining stable and sample-efficient."
        },
        {
            "title": "4 Experiments",
            "content": "We experiment on an internal 3B dense model pretrained on mixture of datasets containing text-only and vision-language understanding data. The image encoder employs the VitDet architecture (Li et al., 2022) and adopts the AnyRes strategy (Liu et al., 2024; Li et al., 2024b), which dynamically partitions each input screenshot into grid of cells. The model is trained for 10K steps during SFT and for 1500 steps during RL. We first report results on GUI grounding (Section 4.1), followed by GUI navigation (Section 4.2)."
        },
        {
            "title": "4.1 GUI Grounding",
            "content": "We evaluate grounding performance on ScreenSpot-V2 (Wu et al., 2025), ScreenSpot-Pro (Li et al., 2025b), and OSWorld-G (Xie et al., 2025). The selected benchmarks are designed to handle multiple platforms (mobile, 6 Model UI-R1 (3B) (Lu et al., 2025) UITars (2B) (Qin et al., 2025) QwenVL 2.5 (3B) (Bai et al., 2025) InfiGUI-R1 (3B) (Liu et al., 2025a) SE-GUI (3B) (Yuan et al., 2025) Jedi (3B) (Xie et al., 2025) GUI-G1 (3B) (Zhou et al., 2025) Ferret-UI Lite (3B) UITars (7B) (Qin et al., 2025) Jedi (7B) (Xie et al., 2025) SE-GUI (7B) (Yuan et al., 2025) GTA1 (7B) (Yang et al., 2025a) GUI-OWL (7B) (Ye et al., 2025) Seed-1.5-VL (Guo et al., 2025) UITars 1.5 (72B) (Qin et al., 2025) GTA1 (72B) (Yang et al., 2025a) GUI-OWL (32B) (Ye et al., 2025) ScreenSpot-V2 ScreenSpot-Pro OSWorld-G 89.2 84.7 - - 90.3 88.8 - 91.6 91.6 91.7 90.3 92.4 92.8 95.2 94.2 94.8 93.2 33.5 18.9 25.9 35.7 36.1 37.1 38.1 53.3 35.7 42.0 47.3 50.1 54.9 60.9 61.6 58.4 58. - - 27.3 - - 50.9 - 55.3 47.5 54.1 67.7 55.9 62.9 47.5 66.7 58.0 Table 1 GUI grounding performance on ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G. Ferret-UI Lite-3B outperforms other 3B models and narrows the gap to larger models. (a) SFT vs RL variants (b) data ratios (c) w/ vs w/o high-res data Figure 6 Ablation studies for grounding. (a) RL improves grounding performance, and our zoom-in operation provides additional gains. (b) Navigation and grounding data can mutually benefit each other, with balanced ratios performing best. (c) Synthetic high-resolution data improves results, especially on ScreenSpot-Pro. desktop, web) and accommodate wide range of image resolutions, enabling robust evaluations across device types. Especially, ScreenSpot-Pro features high-resolution desktop GUI images that can be challenging for GUI grounding models. Main results. Table 1 reports the performance of various models on the grounding benchmarks, and we list fine-grained performance across different categories in Appendix G. Ferret-UI Lite-3B demonstrates strong performance across all three benchmarks, outperforming other 3B models by clear margin and showing relatively small gaps compared to larger models. On ScreenSpot-V2, it reaches 91.6, ahead of other 3B baselines such as UI-R1-3B (89.2) and Jedi-3B (88.8), and close to the 7B tier where scores range from 90.3 to 92.8. On the more challenging ScreenSpot-Pro benchmark, Ferret-UI Lite-3B achieves 53.3, considerably higher than other 3B models, which are generally in the mid-30s, and only slightly below GUI-Owl-7B (54.9). On OSWorld-G, Ferret-UI Lite-3B records 55.3, again stronger than other 3B models and competitive with larger models like GUI-Owl-7B (55.9) and GUI-Owl-32B (58.0). While the best-performing 7B model, GTA1-7B, still leads with 67.7, the difference is modest given the parameter scale. Overall, these results suggest that Ferret-UI Lite-3B provides favorable balance of efficiency and accuracy, narrowing the gap to larger models while maintaining the advantages of lightweight design. SFT vs RL variants. We first compare SFT with RL variants (Figure 6a). RL consistently improves 7 performance, and our proposed zoom-in operation provides further gains. This indicates that the model not only benefits from RL optimization but also learns to actively make use of zoom-in to handle small or cluttered interface elements. Effect of data mixture ratios. We then study different ratios of navigation and grounding data (Figure 6b). The results show that the two types of data can mutually benefit each other: navigation data provides complementary supervision that strengthens grounding ability, while grounding data does not degrade performance on navigation benchmarks. Balanced ratios achieve the best overall results, suggesting that maintaining diversity in training data is important for robust grounding and navigation for small models. Effect of synthetic high-resolution data. Finally, we examine the effect of incorporating synthetic highresolution data (Figure 6c). While improvements on ScreenSpot-V2 are modest, the gains are more notable on the challenging ScreenSpot-Pro benchmark. This demonstrates that high-resolution data is particularly helpful for precise localization and contributes to stronger overall performance."
        },
        {
            "title": "4.2 GUI Navigation\nOffline evaluation. The Ferret-UI Lite model is evaluated on offline static benchmarks first. The\nevaluation is conducted on the Android Control dataset (Li et al., 2024a), which evaluates agentic planning\nand grounding capability in the mobile environment. There are two types of tasks in Android Control\nevaluation: low-level tasks and high-level tasks. For low-level tasks, the inputs consist of detailed single-step\ninstructions, and the model’s grounding capability is critical for successful performance. In a high-level\ntask, a global goal is given, which requires multiple steps to accomplish. High-level tasks assess the model’s\nplanning capability, as they require the model to connect the global goal to the current screenshot to generate\nappropriate step instructions. We follow the test setting in OSAtlas (Wu et al., 2025). Table 2 shows the\nFerret-UI Lite-3B performance on Android Control. Our model achieves an 86.6% success rate in low-level\ntasks and a 68.9% success rate in high-level tasks, outperforming similar-scale models.",
            "content": "Online evaluation. AndroidWorld (Rawles et al., 2025) is chosen for mobile GUI navigation evaluation, which is fully functional Android environment with 116 tasks across 20 Android apps. Table 3a presents the AndroidWorld result of our 3B model against other public models. Because AndroidWorld generates tasks dynamically with randomness in each task, the mean evaluation results of five runs are presented in the table. Our 3B model can achieve 28% success rate, competitive with 7B models such as UITars 1.5 (Qin et al., 2025) in the same setting. Model InternVL-2 (4B) (Chen et al., 2024) OSAtlas (4B) (Wu et al., 2025) Ferret-UI Lite (3B) LL HL 66.7 80.1 67.5 80.6 68.9 86.6 Qwen2-VL (7B) (Wang et al., 2024) Aguvis (72B) (Xu et al., 2025) 82.6 84.4 69.7 66. Table 2 Android Control (AC) offline success rates (%). LL: low-level instruction. HL: high-level instruction. We also evaluate on the OSWorld-Verified benchmark (Xie et al., 2024), which includes challenging tasks in computer use simulation environments. Our 3B model can achieve 17.3% (max steps: 15) success rate, surpassing the performance of all 3B models and competitive with 7B models in the OSWorld leaderboard as shown in Table 3b. After extending the maximum number of steps to 50, our Ferret-UI Lite (3B) achieves 19.8% success rate on the OSWorld evaluation, demonstrating the models test-time scaling capability. Although our 3B navigation model outperforms other models of comparable size, its overall navigation performance remains constrained by the model scale, falling short of the SOTA result on the OSWorld leaderboard, such as 43.9% for Claude-4-Sonnet (Claude, 2024). For additional qualitative insights, we provide detailed case study of AndroidWorld and OSWorld online evaluation in Appendix Section B. Effect of synthetic CoT data. We conduct ablation studies on AndroidWorld to quantify the impact of synthetic data introduced in Section 2.3 on GUI navigation tasks. The models are trained with the same training steps, and we report mean success rates over five runs. Results are summarized in Table 4. Short CoT reasoning traces improve the baseline model trained without CoT data by 2.1%. Extending to more complex long-CoT traces further improves the performance by 4.1%, showing the effectiveness of our CoT synthetic data. 8 Model QwenVL 2.5 (3B) (Bai et al., 2025) ScaleCUA (3B, reported) (Liu et al., 2025b) Ferret-UI Lite (3B) QwenVL 2.5 (7B) (Bai et al., 2025) UITars 1.5 (7B) (Qin et al., 2025) UITars 1.5 (72B) (Qin et al., 2025) UITars 1.5 (7B, reported) (Qin et al., 2025) UITars 1.5 (72B, reported) (Qin et al., 2025) ScaleCUA (7B, reported) (Liu et al., 2025b) ScaleCUA (32B, reported) (Liu et al., 2025b) Success Rates 16.8 23.7 28. 19.5 26.4 37.7 33.0 46.6 27.2 30.6 (a) AndroidWorld success rates (%). Models with () are evaluated using the same random seed and environment setup, averaged over five runs. QwenVL models are evaluated using zero-shot settings. Model ScaleCUA (3B) (Liu et al., 2025b) Kimi-VL (3B) (Team et al., 2025) OpenCUA (A3B) (Wang et al., 2025b) Ferret-UI Lite (3B) Success Rates 9.6 9.7 16.9 17.3 ScaleCUA (7B) (Liu et al., 2025b) UITars 1.5 (7B) (Qin et al., 2025) OpenCUA (7B) (Wang et al., 2025b) GUI-OWL (7B) (Ye et al., 2025) QwenVL 2.5 (32B) (Bai et al., 2025) QwenVL 2.5 (72B) (Bai et al., 2025) ScaleCUA (32B) (Liu et al., 2025b) Doubao-1.5-Thinking (Guo et al., 2025) Claude-4-Sonnet (Claude, 2024) 14.3 24.5 24.3 32.1 3.0 4.4 16.5 31.9 31. (b) OSWorld-Verified 15 steps success rates (%). Table 3 Comparison in online evaluation benchmarks for GUI navigation. Model Variants Baseline + Short CoT + Long CoT Success Rates 13.7 15.8 19.6 + Syn. data (5K) + Syn. data (13K) + Syn. data (17K) 20.3 22.4 25. Table 4 SFT ablations on the AndroidWorld (AW) benchmark. Success rates (%) are averaged over five runs. The baseline model is built using only humanannotated episodes, without CoT and synthetic data. (a) Impact of SFT steps for RL. (b) Impact of RL reward design. Figure 7 RL ablations on the AW benchmark. RL rewards: AT (Action Type), SG (Sparse Grounding), DG (Dense Grounding). Effect of synthetic QA and navigation data. Built on the long-CoT agent, we further augment the SFT training mixture with online synthetic data collected through the multi-agent system with filtering, and offline synthetic data with task rewriting. As shown in Table 4, progressively scaling synthetic data to 17K trajectories yields an additional improvement of nearly 6%, indicating that scaling synthetic data with diversity improves navigation. Effect of RL. We conduct experiments to evaluate the impact of RL training after the SFT stage on AndroidWorld and OSWorld online evaluations. In this experiment, the action type reward and dense grounding reward are incorporated, and the maximum number of steps allowed during evaluation is set to 15. Compared to the baseline SFT checkpoint, RLVR increases the AndroidWorld performance from 25% to 28% and OSWorld performance from 15% to 17.3%. Appendix illustrates the verifiable reward curve during RL training. Effect of SFT training steps for RL. Figure 7a presents the impact of varying the number of SFT steps prior to RLVR training on AndroidWorld success rate. We evaluate checkpoints trained with 2000, 6000, and 10000 SFT steps, followed by RLVR with the same training steps. Across all settings, RLVR consistently improves success rates compared to the corresponding SFT checkpoints. The gains are most pronounced when the number of SFT steps is smaller, suggesting that RLVR is particularly effective in compensating for limited SFT training. Effect of reward designs. Figure 7b shows the effect of different reward configurations in RLVR training on AndroidWorld success rate. We ablate four settings: (i) action type reward only, (ii) sparse grounding reward only, (iii) action type reward combined with sparse grounding reward, and (iv) action type reward 9 combined with dense grounding reward. Using only the action type reward leads to substantial drop in performance, as correct grounding positions are not reinforced. Grounding reward alone improves tapping accuracy but does not surpass the SFT baseline. Combining action type and grounding rewards yields consistent improvements, with dense grounding rewards outperforming sparse grounding rewards. These findings highlight the importance of carefully designed RLVR reward structures for enhancing small GUI agent models."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present Ferret-UI Lite, 3B multimodal LLM designed for GUI agentic tasks with focus on lightweight, on-device settings. Through real and synthetic data curation, inference-time visual tool use, and two-stage SFTRL training strategy, Ferret-UI Lite achieves competitive grounding and navigation performance relative to larger models. Our experiments validate the effectiveness of these strategies for small-scale agents, while also revealing their limitations, particularly in multi-step navigation. These findings highlight both the promise and challenges of scaling down GUI agents, and we hope our lessons will inform future research on building efficient, capable, and practical on-device AI agents."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank Harsh Agrawal, Eldon Schoop, Haoxuan You, Andrew Szot, Dongxu Li, Haofeng Chen, Alexander Metz, Abhishek Sundararajan, Adolfo Lopez Mendez, Hungshi Lin, Kenneth Jung, Andres Romero Mier Teran, James Chou, Mohana Prasad Sathya Moorthy and Jason Williams for valuable guidance, suggestions, and feedback. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint, 2023. Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent S: An open agentic framework that uses computers like human. arXiv preprint, 2024. Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent S2: compositional generalist-specialist framework for computer use agents. arXiv preprint, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-VL technical report. arXiv preprint, 2025. Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic AI. arXiv preprint, 2025. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. RT-1: Robotics transformer for real-world control at scale. Robotics: Science and Systems, 2022. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. ICML, 2024. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. FireACT: Toward language agent fine-tuning. arXiv preprint, 2023. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CVPR, 2024. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. ACL, 2024. Claude. Claude Sonnet 4, 2024. URL https://www.anthropic.com/claude/sonnet. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, et al. Agent AI: Surveying the horizons of multimodal interaction. arXiv preprint, 2024. Brad Dwyer. Website screenshots dataset. Roboflow Universe, 2022. https://universe.roboflow.com/roboflow-gw7yv/ website-screenshots. Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. GRIT: Teaching MLLMs to think with images. arXiv preprint, 2025. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for GUI agents. arXiv preprint, 2025. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-VL technical report. arXiv preprint, 2025. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. CogAgent: visual language model for GUI agents. CVPR, 2024. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. GLM-4.1 V-Thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint, 2024. 11 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint, 2020. Bin Lei, Weitai Kang, Zijian Zhang, Winson Chen, Xi Xie, Shan Zuo, Mimi Xie, Ali Payani, Mingyi Hong, Yan Yan, et al. InfantAgent-Next: multimodal generalist agent for automated computer interaction. arXiv preprint, 2025. Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, et al. Apple intelligence foundation language models tech report 2025. arXiv preprint, 2025a. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. ScreenSpot-Pro: GUI grounding for professional high-resolution computer use. arXiv preprint, 2025b. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on UI control agents. NeurIPS, 2024a. Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. ECCV, 2022. Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, and Zhe Gan. Ferret-UI 2: Mastering universal user interface understanding across platforms. ICLR, 2024b. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Show-UI: One vision-language-action model for generalist GUI agent. NeurIPS Workshop on Open-World Agents, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-Next: Improved reasoning, ocr, and world knowledge. https://llava-vl.github.io/blog/2024-01-30-llava-next/, 2024. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. InfiGUI-R1: Advancing multimodal GUI agents from reactive actors to deliberative reasoners. arXiv preprint, 2025a. Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, et al. ScaleCUA: Scaling open-source computer use agents with cross-platform data. arXiv preprint, 2025b. Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. UI-R1: Enhancing efficient action prediction of GUI agents by reinforcement learning. arXiv preprint, 2025. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. GUI-R1: generalist R1-style vision-language action model for GUI agents. arXiv preprint, 2025. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In ECCV, 2024. OpenAI. Computer-Using Agent: Introducing universal interface for ai to interact with the digital world, 2025. URL https://openai.com/index/computer-using-agent. OpenAI. Openai o3 and o4-mini system card. System card, OpenAI, April 2025. URL https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. UI-TARS: Pioneering automated GUI interaction with native agents. arXiv preprint, 2025. Ram Ramrakhya, Andrew Szot, Omar Attia, Yuhao Yang, Anh Nguyen, Bogdan Mazoure, Zhe Gan, Harsh Agrawal, and Alexander Toshev. Scaling synthetic task generation for agents via exploration. arXiv preprint, 2025. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. AndroidWorld: dynamic benchmarking environment for autonomous agents. ICLR, 2025. 12 Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. NeurIPS, 2023. ByteDance Seed. Ui-TARS-1.5. https://seed-tars.com/1.5, 2025. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual CoT: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. NeurIPS, 2024a. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. DeepseekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint, 2024b. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. NeurIPS, 2024. Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, and Alexander Toshev. From multimodal llms to generalist embodied agents: Methods and lessons, 2024. URL https://arxiv.org/abs/2412.08442. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint, 2024. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-VL technical report. arXiv preprint, 2025. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. UI-TARS-2 technical report: Advancing GUI agent with multi-turn reinforcement learning. arXiv preprint, 2025a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint, 2024. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. OpenCUA: Open foundations for computer-use agents. arXiv preprint, 2025b. Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. WebUI: dataset for enhancing visual ui understanding with web semantics. CHI, 2023. Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, and Shuo Shang. Mobilevlm: vision-language model for better intra-and inter-ui understanding. arXiv preprint arXiv:2409.14818, 2024. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. OS-ATLAS: foundation action model for generalist GUI agents. ICLR, 2025. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. NeurIPS, 2024. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint, 2025. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous GUI interaction. ICML, 2025. 13 An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. GPT-4V in wonderland: Large multimodal models for zero-shot smartphone GUI navigation. arXiv preprint, 2023. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in GPT-4V. arXiv preprint, 2023. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. GTA1: Gui test-time scaling agent. arXiv preprint, 2025a. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-UI: Visual grounding for GUI instructions. ACL Findings, 2025b. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-Agent-v3: Foundamental agents for GUI automation. arXiv preprint, 2025. Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-UI: Grounded mobile UI understanding with multimodal LLMs. ECCV, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint, 2025. Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for GUI agents via self-evolutionary reinforcement learning. arXiv preprint, 2025. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, and Qing Li. TongUI: Building generalized GUI agents by learning from multimodal web tutorials. arXiv preprint, 2025. Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024a. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for GUI agents. EMNLP Findings, 2024b. Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. AgentStudio: toolkit for building general virtual agents. arXiv preprint, 2024. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-G1: Understanding R1-zero-like training for visual grounding in GUI agents. arXiv preprint, 2025."
        },
        {
            "title": "A Related Work",
            "content": "Recent progress in GUI agents has been largely driven by multimodal large language models (MLLMs) (McKinzie et al., 2024; Zhang et al., 2024a; Bai et al., 2025; Team et al., 2025). We review two central directions: GUI grounding and GUI navigation. GUI grounding. GUI grounding focuses on mapping natural language instructions to the bounding boxes of target elements in screen images. Early studies explored supervised fine-tuning (SFT) to train models that predict coordinates as tokens (You et al., 2024; Cheng et al., 2024; Li et al., 2024b; Gou et al., 2025; Yang et al., 2025b). Building on this foundation, reinforcement learning (RL) has become an important tool, as the grounding reward signal can often be automatically verified. GRPO-style optimization (Shao et al., 2024b) has been successfully applied with synthetic recipes and verifiable rewards (Luo et al., 2025; Lu et al., 2025; Zhou et al., 2025; Liu et al., 2025a; Yang et al., 2025a). These efforts have significantly advanced the accuracy and robustness of GUI grounding across platforms. GUI navigation. Beyond single-step grounding, GUI navigation requires multi-step reasoning and action prediction. Two broad paradigms have been widely explored. (1) Multi-agent systems. These methods decompose an agent into separate components, such as planner and grounding module (Yan et al., 2023; Gou et al., 2025; Yang et al., 2025b,a). Large models (e.g., GPT, Gemini) are often used as planners to generate action sequences, which are then grounded by specialized modules. Extensions with memory, knowledge bases, and external tools further enrich agent behavior (Agashe et al., 2024, 2025). (2) End-to-end agents. Recent work has increasingly focused on unified vision-language-action models that jointly learn grounding and navigation. Show-UI (Lin et al., 2024) and OS-Atlas (Wu et al., 2025) predict action sequences directly from multimodal inputs. Other systems, such as CogAgent (Hong et al., 2024), SeeClick (Cheng et al., 2024), GUIOdyssey (Lu et al., 2024), MobileVLM (Wu et al., 2024), UI-TARS (Qin et al., 2025), and TongUI (Zhang et al., 2025), extend these designs to multiple platforms and applications. General-purpose MLLMs have also begun integrating GUI navigation capabilities, including Qwen2.5-VL (Bai et al., 2025) and GLM-4.1V-Thinking (Hong et al., 2025). Together, these works have established end-to-end modeling as powerful paradigm for GUI agents. Toward small on-device GUI agents. Alongside these advances, many models release both small and large variants to balance efficiency and capability. While larger models remain the dominant focus due to their reasoning and planning strength, smaller models are attractive for on-device deployment where latency, privacy, and connectivity are critical. Our work explores this complementary direction by studying 3B end-to-end GUI agent, building on prior advances in SFT, synthetic data generation, chain-of-thought modeling, and RL training. This perspective highlights practical strategies for adapting modern GUI agent techniques to resource-constrained settings."
        },
        {
            "title": "B Case Study",
            "content": "B.1 Desktop Case Study Figure 8 Successful completion for Make sparkline charts for each order id with the data from Jan to Mar in the Chart column. task in OSWorld evaluation. 16 B.2 Mobile Case Study Figure 9 Successful Completion for Delete the following recipes from Broccoli app: Kale and Quinoa Salad, Baked Cod with Lemon and Dill, Rasperry Almond Smoothie task in AndroidWorld evaluation. Figure 10 Successful Completion for Create new contact for Lina Muller. Their number is +15410831733 task in AndroidWorld evaluation."
        },
        {
            "title": "C Unified Action Space",
            "content": "18 Action tap move_to drag_to locate textentry swipe terminate press_enter press_hotkey right_click double_click long_press navigate_home open_app navigate_back Parameters (x, y) (x, y) (x, y) (x, y) texts direction reason - hotkeys (x, y) (x, y) (x, y) - app_name - Definition Tap/left-click on the specified screen location. Move to the specified screen location. Drag to the specified screen location."
        },
        {
            "title": "Platforms\nAll\nAll\nAll",
            "content": "Locate on the specified screen location. Type texts at focused text bar. Swipe/scroll in the given direction (up, down, left, right) on the screen, with fixed start/end points and speed. End the navigation and provide reason. Press the enter button. Trigger predefined action via key combination. Right-click on the specified screen location. Double-click on the specified screen location. Long-press the specified screen location. Return to the home screen. Launch specified application. Navigate back to the previous page."
        },
        {
            "title": "All\nAll\nAll",
            "content": "All All Desktop, Web Desktop, Web Desktop, Web Mobile Mobile Mobile Mobile Table 5 Unified action space spanning shared and platform-specific operations."
        },
        {
            "title": "D SFT Data Mixture",
            "content": "Web OS-Atlas (Wu et al., 2025) (11089.3 K) UGround (Gou et al., 2025) (9550.0 K) AGUVIS-Grounding (Xu et al., 2025) (723.0 K) Aria-UI (Yang et al., 2025b)(173.0 K) WaveUI (Wu et al., 2023) (58.7 K) AGUVIS-Planning (Xu et al., 2025) (34.3 K) ShowUI (Lin et al., 2024) (22.0 K) GroundUI (Zheng et al., 2024) (11.6 K) Desktop OS-Atlas (Wu et al., 2025) (1134.6 K) OpenCUA (Wang et al., 2025b) (420.8 K) Synthetic (75.0 K) ShowUI (Lin et al., 2024) (8.0 K) Aria-UI (Yang et al., 2025b)(7.8 K) AGUVIS-Grounding (Xu et al., 2025) (7.0 K) GroundUI (Zheng et al., 2024) (2.5 K) WaveUI (Wu et al., 2023) (1.8 K) Mobile OS-Atlas (Wu et al., 2025) (4586.3 K) AGUVIS-Grounding (Xu et al., 2025) (306.0 K) AGUVIS-Planning (259.6 K) (Xu et al., 2025) UGround (Gou et al., 2025) (112.0 K) Synthetic (70.0 K) GroundUI (Zheng et al., 2024) (4.0 K) WaveUI (Wu et al., 2023) (3.1 K) Figure 11 GUI dataset mixture used for supervised fine-tuning, including grounding datasets and navigation datasets. The mobile synthetic dataset and desktop synthetic dataset are generated using the pipeline described in Section 2.3. Navigation. Web. Legends: Grounding. Desktop. Mobile."
        },
        {
            "title": "E Evolution of RLVR Reward Curve",
            "content": "Figure 12 Evolution of verifiable reward curve during RLVR training."
        },
        {
            "title": "F Grounding Dataset Ablations",
            "content": "Setting Base w/o UGround w/o OSAtlas w/o Aria-UI w/o ShowUI w/o Jedi ScreenSpot-Pro ScreenSpot-V2 50.15 48.07 43.35 46.93 48.89 47.63 90.49 90.19 89.62 89.94 88.84 88.36 Table 6 Ablation study on ScreenSpot-Pro and ScreenSpot-V2. We conduct an ablation study on different grounding datasets in Table 6. The results highlight the contribution of different components to overall performance. Removing UGround and ShowUI leads to relatively modest drops on ScreenSpot-Pro (-2.08 and -1.26) and minor changes on ScreenSpot-V2. In contrast, excluding OSAtlas produces the largest degradation on ScreenSpot-Pro (-6.80), underscoring its importance for grounding in this dataset. Removing Aria-UI and Jedi also causes noticeable decreases, suggesting complementary roles across both benchmarks. Overall, each component contributes to robustness, with OSAtlas emerging as the most critical for ScreenSpot-Pro. 20 Fine-Grained Grounding Results"
        },
        {
            "title": "Model",
            "content": "Qwen2.5-VL (3B) (Bai et al., 2025) UI-TARS (2B) (Qin et al., 2025) OS-Atlas-Base (4B) JEDI (3B) (Xie et al., 2025) Ferret-UI Lite (3B) OS-Atlas-Base (7B) (Wu et al., 2025) Qwen2.5-VL (7B) (Bai et al., 2025) UI-TARS (7B) (Qin et al., 2025) JEDI (7B) (Xie et al., 2025) GUI-Owl (7B) (Ye et al., 2025) GTA1 (7B) (Yang et al., 2025a) UI-TARS (72B) (Qin et al., 2025) Qwen2.5-VL (32B) (Bai et al., 2025) Qwen2.5-VL (72B) (Bai et al., 2025) GUI-Owl (32B) (Ye et al., 2025) GTA1 (32B) (Yang et al., 2025a) GTA1 (72B) (Yang et al., 2025a) Operator (OpenAI, 2025) Claude 3.7 Sonnet (Claude, 2024) UI-TARS-1.5 (Seed, 2025) Seed-1.5-VL (Guo et al., 2025)"
        },
        {
            "title": "Web",
            "content": "Text 93.4 95.2 95.2 96.6 97.2 96.2 97.6 96.9 96.9 99.0 99.0 94.8 98.3 99.0 98.6 98.6 99.3 47.3 - - - Icon Text 88.1 73.5 90.7 79.1 90.7 75.8 96.9 81.5 98.5 83.9 Icon Text 88.0 58.6 87.2 68.6 90.6 63.6 88.5 78.6 95.3 85.0 83.4 87.2 89.1 87.2 92.4 88. 86.3 86.7 90.1 90.0 89.1 92.4 41.5 - - - 89.7 90.2 95.4 95.9 96.9 94.9 91.2 94.3 96.4 97.9 96.4 97.4 90.2 - - - 69.3 74.2 85.0 87.9 85.0 89.3 87.9 83.6 87.1 87.8 86.4 89.3 80.3 - - - 94.0 93.2 93.6 94.4 93.6 92. 91.5 93.6 96.6 94.4 95.7 95.3 92.8 - - - Icon 71.4 78.3 77.3 83.7 85.7 79.8 81.3 85.2 84.2 85.2 86.7 87.7 89.7 90.6 86.7 88.7 91.6 84.3 - - -"
        },
        {
            "title": "Overall",
            "content": "80.9 84.7 85.1 88.6 91.6 87.1 88.8 91.6 91.7 92.8 92.4 90.3 91.9 94.0 93.2 93.2 94.8 70.5 87.6 94.2 95.2 Table 7 Fine-grained grounding performance on Screenspot-V2. 21 Text Fine-grained Matching Recognition Understanding Manipulation Element Layout Refusal Overall Model Qwen2.5-VL (3B) (Bai et al., 2025) Jedi (3B) (Xie et al., 2025) Ferret-UI Lite (3B) OS-Atlas (7B) (Wu et al., 2025) Qwen2.5-VL (7B) (Bai et al., 2025) UGround (7B) (Gou et al., 2025) Aguvis (7B) (Xu et al., 2025) UI-TARS (7B) (Qin et al., 2025) Jedi (7B) (Xie et al., 2025) UI-TARS-1.5 (7B) (Seed, 2025) GTA1 (7B) (Yang et al., 2025a) Qwen2.5-VL (32B) (Bai et al., 2025) UI-TARS (72B) (Qin et al., 2025) Qwen2.5-VL (72B) (Bai et al., 2025) GTA1 (32B) (Yang et al., 2025a) GTA1 (72B) (Yang et al., 2025a) Operator (OpenAI, 2025) Gemini-2.5-Pro (Comanici et al., 2025) Seed1.5-VL (Guo et al., 2025) 41.4 67.4 36.8 44.1 45.6 51.3 55.9 60.2 65.9 52.6 63.2 57.9 69.4 52.6 52.6 57.9 51.3 59.8 73.9 28.8 53.0 72. 29.4 32.7 40.3 41.2 51.8 55.5 75.4 82.1 70.2 60.6 74.6 73.1 76.9 42.4 45.5 66.7 34.8 53.8 62.2 35.2 41.9 43.5 43.9 54.9 57.7 72.4 74.2 73.8 62.9 74.7 72.0 77.3 46.6 49.0 69.6 13.4 44.3 50. 16.8 18.1 24.8 28.2 35.6 46.9 66.7 70.5 49.2 45.6 55.3 59.9 66.7 31.5 33.6 47.0 Table 8 Fine-grained grounding performance on OSWorld-G. Model UI-TARS (2B) (Qin et al., 2025) Qwen2.5-VL (3B) (Bai et al., 2025) UI-R1 (3B) (Lu et al., 2025) InfiGUI-R1 (3B) (Liu et al., 2025a) JEDI (3B) (Xie et al., 2025) GUI-G1 (3B) (Zhou et al., 2025) Ferret-UI Lite (3B) Development Creative CAD Scientific Office Text 47.4 38.3 46.1 51.3 61.0 50.7 75. Text Icon Text Icon Text Icon Text Icon Text Icon 42.9 6.3 17.8 4.7 56.9 17.3 50.3 17.0 21.5 5.6 40.9 4.9 22.3 6.3 44.4 10.0 48.0 17.0 33.6 4.5 41.9 4.2 37.1 12.5 56.9 21.8 65.0 26.4 32.7 10.1 44.9 7.0 33.0 14.1 58.3 20.0 65.5 28.3 43.9 12.4 53.5 8.4 27.4 9.4 54.2 18.2 64.4 32.1 38.3 9.0 36.6 11.9 39.6 9.4 61.8 30.0 67.2 32.1 23.5 10.6 71.7 22.4 43.1 26.6 75.7 30.9 83.6 49.1 66.4 30.3 Icon 4.1 3.4 6.9 12.4 13.8 10.3 24.8 Qwen2.5-VL (7B) (Bai et al., 2025) UI-TARS (7B) (Qin et al., 2025) JEDI (7B) (Xie et al., 2025) SE-GUI (7B) (Yuan et al., 2025) GUI-G2 (7B) (Tang et al., 2025) GUI-Owl (7B) (Ye et al., 2025) 51.9 58.4 42.9 68.2 68.8 76.6 63.0 UI-TARS (72B) (Qin et al., 2025) Qwen2.5-VL (32B) (Bai et al., 2025) 74.0 Qwen2.5-VL (72B) (Bai et al., 2025) GUI-Owl (32B) (Ye et al., 2025) GPT-4o (Hurst et al., 2024) Claude 3.7 Sonnet (Claude, 2024) Operator(OpenAI, 2025) Seed-1.5-VL (Guo et al., 2025) UI-TARS-1.5 (Seed, 2025) 84.4 1.3 50.0 4.8 12.4 11.0 19.3 17.2 31.0 17.3 21.4 39.3 0.0 19.3 36.9 8.4 17.8 1.6 48.6 8.2 53.7 18.9 34.6 7.9 50.0 9.1 20.8 9.4 63.9 31.8 63.3 20.8 30.8 16.9 50.0 11.9 38.0 14.1 72.9 25.5 75.1 47.2 33.6 16.9 57.6 9.1 51.3 42.2 75.0 28.2 78.5 43.4 49.5 25.8 57.1 15.4 55.8 12.5 77.1 24.5 74.0 32.7 57.9 21.3 59.6 27.3 64.5 21.9 79.1 37.3 77.4 39.6 59.8 33.7 57.1 15.4 18.8 12.5 64.6 20.9 63.3 26.4 42.1 15.7 61.1 13.3 38.1 15.6 78.5 29.1 76.3 37.7 55.1 27.0 65.2 18.2 62.4 28.1 82.6 39.1 81.4 39.6 70.1 36.0 0.0 1.0 2.0 0.0 0.0 0.0 2.1 0.0 1.1 0.0 51.5 23.1 16.8 14.1 58.3 24.5 60.5 28.3 34.6 30. Table 9 Fine-grained grounding performance on Screenspot-Pro. 0.0 7.4 0.0 7.4 0.0 0.0 0.0 0.0 7.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 38.9 18.5 OS 27.3 50.9 55.3 27.7 31.4 36.4 38.7 47.5 54.1 64.2 67. 59.6 57.1 62.2 61.9 66.7 40.6 45.2 62.9 Overall 27.7 25.9 33.5 35.7 36.1 37.1 53.3 27.6 35.7 39.5 47.3 47.5 54.9 38.1 47.6 53.3 58.0 0.8 27.7 36.6 60.9 61."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}