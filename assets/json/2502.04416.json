{
    "paper_title": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
    "authors": [
        "Zehua Pei",
        "Lancheng Zou",
        "Hui-Ling Zhen",
        "Xianzhi Yu",
        "Wulong Liu",
        "Sinno Jialin Pan",
        "Mingxuan Yuan",
        "Bei Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE."
        },
        {
            "title": "Start",
            "content": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference Zehua Pei1, Lancheng Zou1, Hui-Ling Zhen2, Xianzhi Yu2, Wulong Liu2, Sinno Jialin Pan1, Mingxuan Yuan2, Bei Yu1 1The Chinese University of Hong Kong 2Noahs Ark Lab, Huawei 5 2 0 2 6 ] . [ 1 6 1 4 4 0 . 2 0 5 2 : r AbstractLarge language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using mixture-of-experts (MoE) architecture, where only subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct routing mechanism without training from scratch, incorporating differentiable routing process and load balancing. Using modest data, CMoE produces well-designed, usable MoE from 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/ JarvisPei/CMoE. I. INTRODUCTION Large language models (LLMs) have demonstrated exceptional proficiency in managing complex tasks and exhibiting emergent capabilities across diverse domains and applications, particularly when scaled to billions of parameters [1][4]. While LLMs have attained remarkable success, their expanding computational demands and model sizes have intensified challenges related to practical deployment, especially in environments with limited hardware resources or stringent latency requirements. To mitigate these challenges, mixtureof-experts (MoE) architectures [5][8] have emerged as promising paradigm. Unlike dense LLMs, where all parameters are activated for every input token, MoE models replace monolithic feed-forward networks (FFNs) with sparsely activated experts: specialized sub-networks that process inputs conditionally via dynamic routing. This design decouples model capacity from computational costactivating only subset of experts per token while preserving the models expressive power. Recently, researchers have found that there is high activation sparsity in the hidden neurons of FFNs of dense LLMs, which motivates them to develop sparsity-aware acceleration techniques to reduce computational overhead while maintaining model performance [9], [10]. Building upon this insight, growing body of research has focused on transforming dense LLMs into MoE architectures through strategic reorganization of FFN parameters but not training MoE from scratch [11] [13]. The prevailing methodology replaces conventional FFN layers with MoE layers, where neurons are partitioned into multiple expert sub-networks while maintaining the original parameter count. During inference, routing mechanism selectively activates only subset of experts per input token, thereby achieving dynamic sparsity without compromising model capacity. However, due to the high sparsity target always required in MoE models, these works often need massive computing resources and billions of training data for continual pre-training on the constructed MoE models. To address the aforementioned limitations, we propose carved MoE, named CMoE, framework that efficiently carves sparse MoE architectures from dense LLMs through parameter reorganization and training-free structural adaptation. Unlike prior approaches that rebuild MoE models from scratch via resource-intensive pre-training, CMoE strategically carves experts from the dense models existing feed-forward network (FFN) neurons while preserving their inherent knowledge. This carving process is both computationally efficientrequiring only minutes of lightweight processingand sophisticated, as it leverages systematic neuron grouping and analytical router construction to retain performance with minimal fine-tuning. The core innovation lies in CMoEs ability to restructure dense FFNs into MoE layers without re-training the base model. First, we identify neurons that universally encode common knowledge (a.k.a. shared experts) and those that exhibit specialized, input-dependent activation patterns (a.k.a. routed experts). Shared experts are systematically retained by selecting neurons with the highest activation rates, ensuring they capture broadly applicable features. By formulating routed expert grouping as balanced linear assignment problem, solved via the JonkerVolgenant algorithm, CMoE clusters neurons into experts while maintaining parameter balance and activation coherence. Second, we derive the routing mechanism directly from the dense models activation statistics, bypassing the need for end-to-end router training. This involves constructing differentiable routing function initialized using representative neurons from each expert cluster, enabling immediate usability while preserving optimization potential. In summary, the key contributions of this paper are: CMoE: framework that efficiently carves MoE from dense LLMs by reorganizing FFN neurons into shared/routed experts, eliminating costly pre-training. Shared and routed experts: ensuring parameter balance and activation coherence for CMoE. Training-Free Routing: using representative neurons and lightweight adaptation for router initialization, which enables rapid performance recovery. Extensive experiments show that, with activation ratio 25%, CMoE can maintain reasonable perplexity even without fine-tuning, and achieve 76.59% accuracies of dense models on some downstream benchmarks with lightweight fine-tuning on 2,048 samples. II. RELATED WORK"
        },
        {
            "title": "In contrast",
            "content": "to pretraining MoE models from scratch, recent research has investigated the feasibility of constructing MoE architectures by repurposing existing dense LLMs. Current methodologies for deriving MoE models from dense checkpoints generally follow two paradigms: (1) partitioning parameters of FFNs while preserving the original models total parameter count [10], [14], or (2) expanding the models overall capacity while retaining activation dimensions comparable to standard dense models [15], [16]. This work prioritizes the former approach. Notably, MoEBERT [14] introduces an importance-driven strategy to transform FFNs into expert modules by strategically redistributing top-scoring neurons across specialized components. Concurrently, MoEfication [10] leverages the discovery of sparse activation patterns in ReLU-based FFNs within T5 architectures, enabling the decomposition of these layers into distinct expert groups governed by learned routing mechanism. Based on continual training, the LLaMA-2 7B model is modified as LLaMAMoE-3.5B MoE model, where the parameters of the original FFNs are partitioned into multiple experts [11]. After training with 200B tokens, the LLaMA-MoE-3.5B model significantly outperforms dense models that contain similar activation parameters. Furthermore, based on two-stage post-training strategy, an MoE model is constructed from the LLaMA3 8B model, where both attention and MLP are partitioned into MoE blocks [12]]. Extensive experiments have shown the effectiveness of constructing an MoE model from dense model, and many techniques can be utilized to guarantee performance recovery. However, such performance recovery is extremely resourceconsuming, which is unfavorable for efficient deployment in industrial applications. Therefore, more lightweight methods are required, such that performance recovery can be done within hours and even training-free. Note that model compression such as pruning and quantization is another important technique for efficient LLM inference [17][20]. Pruning is among the most widely utilized approaches to detect and remove redundant or less significant parameters from models, thereby resulting in sparser weight matrix and faster inference. ShortGPT [21] has put forward simple layer-removal approach. This approach is based on block influence, which is determined by the similarity between layers input and output. SliceGPT [22] substitutes each weight matrix with smaller dense matrix, thereby 2 decreasing the embedding dimension of the network. By taking into account contextual sparsity in real time, Deja Vu [9] and FuseGPT [23] have been proposed to accelerate LLM inference. In contrast to the post-training methods, LearnTo-be-Efficient is designed to train efficiency-aware LLMs so that they learn to activate fewer neurons and achieve more favorable trade-off between sparsity and performance [24]. III. BACKGROUND This study primarily focuses on the LLaMA family [2], [25], which uses SwiGLU [26] as the activation function. However, our analysis and findings can be adapted to most of the FFN structures of existing LLMs, including the ReLUbased FFNs [27]. An FFN exists in the tail of each transformer block, which Rd and then contributes to the gets the input embedding output together with the residual connection, i.e. + (x). Typically, an FFN is two-layer fully connected network, i.e. the up projection and down projection layer, with an activation function between them. For LLaMA, the SwiGLU composes another gate projection layer. Given the up projection weight Rd dh Wup d, the process and the down projection weight Wdown of an FFN is given by: dh , the gate projection weight Wgate Rdh Rd (x) = hWdown, = SwiGLU(xWup) = Swish(xWgate) (1) (xWup), where Swish(x) = sigmoid function. σ(x) is element-wise and σ( ) is the The basic MoE architecture is composed of set of independent FFNs as experts, , and router { network [5]. The output of an MoE-version FFN is then obtained by E1, E2, ..., EN } FM oE(x) = (cid:88) giEi(x), i=1 gi = (cid:26) si, 0, = [s1, s2, TopK( si si { otherwise, 1 , K), } (2) , sN ] = G(x), RN is the tokenwhere gi is the score for the i-th expert, to-expert affinity, i.e. the output of G, and TopK( , K) denotes the set comprising highest scores among the affinity scores calculated for on all experts. IV. METHODOLOGY CMoE transforms dense LLM into sparsely activated MoE architecture through two key phases: efficient expert grouping and training-free router construction, followed by optional lightweight adaptation. As illustrated in Fig. 1, the Neuron Activation framework operates as follows: Profiling (Section IV-A). Given an FFN layer, CMoE profiles neurons activation patterns with small calibration dataset to categorize the neurons into shared experts (high-activation, task-agnostic) and routed experts (sparsely activated, taskExpert Grouping (Section IV-A). Shared specific). Fig. 2 The histogram of FFN hidden state for the 3-th block and the 1, 000-th token. the independence between neurons. On the other hand, the output of FFN can be written as: (x) = dh(cid:88) i=1 hiwdown,i, (3) Rd is the i-th row of Wdown. When we where wdown,i revisit the FFN process, we can regard each hi as the score to be multiplied to the split vector wdown,i, whose product contributes to part of the output (x). As finding of some structured pruning research [29], [30], the norm of (x) is always small due to the residual connection. Such phenomenon implies the high sparsity existed in FFN, and by (3) we relate it to hi, whose value decides how large an isolated neuron contributes to the output. Therefore we make hypothesis: i (4) hi , hi arg min arg min hiwdown,i which is reasonable since when hi is extremely small, the product hiwdown,i will also vanish. It is expected that the hidden state should be highly sparse, which means is often extremely small. To verify it, we hack into the FFN and draw the distribution of h. As demonstrated in fig. 2, the distribution is sharply peaked at 0 and constrained within small range, indicating that most hi are concentrated near zero and confirming the sparsity. And it exhibits symmetry, suggesting near-normal distribution centered around the mean. Based on what we discuss and observe above, the hidden state values work well as the basis for judgment of neuron activation, because of its high differentiation and independence across different neurons. Therefore, we propose new metric, called absolute TopK (ATopK), to determine the activation status of neuron with index i: ai = (cid:26) 1, hi 0, otherwise, TopK( 1 hj dh} , Ka), (5) { where we choose neurons with Ka highest absolute value among the hidden state values, and assign their labels in the activation marker = [a1, a2, To further evaluate the activation information, we make samples in the training set to record their activation markers. d, with the batch Given batched input tensor size and the sequence length, we obtain the batched hidden state , adh ] with 1. dh as follows: Rb Rb = Swish(XWgate) (XWup). (6) 3 Fig. 1 The overview of our proposed CMoE. Experts: Neurons with the highest activation rates are directly grouped into shared experts, which are always activated during inference. Routed Experts: Remaining neurons are partitioned into routed experts via balanced clustering, formulated as Router Construction linear assignment problem. (Section IV-B). The routing mechanism is analytically derived from the activation statistics of representative neurons in each expert cluster, bypassing the need for end-to-end training. Additionally, we make the routing function differentiable in Section IV-C for further alignment and performance recovery. A. Shared and Routed Experts Grouping CMoE starts with grouping neurons in FFN into experts. Shared experts are expected to process common knowledge instead of specialization. Thus, the key idea is to group neurons that are always activated during FFN inference. For routed expert grouping, as described in previous works [28], the key idea is to cluster the neurons that are always activated simultaneously. CMoE carries forward these ideas but comes up with detailed and analytical perspective. In this section, we construct experts of size = dh (assuming dh is dh), including Ns shared experts and multiple of , i.e., Nr routed experts (Ns + Nr = ). Rdh : hi = Swish(x We begin with analyzing the isolated contribution of each neuron to the output of FFN. Consider the i-th element of wup,i), where wgate,i Rd and wup,i Rd are the i-th column of Wgate and Wup, respectively. It shows that the value of hi only depends on the input and the corresponding i-th weights, which implies wgate,i) (x The majority of low activation rates also encourage us to construct routed experts, which are not always activated but are specialized for tokens encountered. To construct Nr routed experts, we develop customized balanced K-means clustering algorithm. We first identify Nr centroids as the neurons (excluding those assigned to shared experts) with the highest activation rates, and group their feature vectors into ci : µi centroid set = dh, / 1 ˆc1, . . . , ˆcNr } , where, for convenience, we = SN s} re-label the centroids with ˆc. TopK( { , Nr) } µj { { By selecting these centroids, we ensure that the clusters are initialized around meaningful and prominent activation patterns, providing strong starting point for clustering. Using the centroids selected, we will construct clusters with remaining neurons. We pay attention to their correlation to the centroids, i.e. we study the distances of their feature vectors to the centroids. The feature vector individually contains the activation status of specific neuron, and we expect that neurons with similar functionalities will have similar patterns on the activation status, i.e. they are often activated and deactivated together during inference. Therefore, we compute the L2 distance of each feature vector to each Nr , whose centroid ˆc. Define distance matrix element di,j is the L2 distance between the i-th feature vector ci and the j-th centroid ˆcj: RNr di,j = ci ˆcj2 = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) k=1 (ck,i ˆck,j)2, (10) where, for convenience, we also include the original feature vectors of centroids inside the set to be grouped. To group the neurons into routed experts, with the centroids designed above at = 0, the constrained balanced K-means vecclustering algorithm works as follows [31]. Given Nr tors c, the cluster membership value and cluster centroids , . . . , ˆct+1 Nr at iteration t, compute ˆct+1 1, ˆct ˆct Nr at iteration + 1 with the following 2 steps: Cluster Assignment Let linear program with ˆct fixed: i,p be solution to the following 2, . . . , ˆct , ˆct+1 2 1 min Nr (cid:88) Nr(cid:88) i=1 p=1 Ti,p di,p s.t. Nr (cid:88) i=1 Ti,p = m, = 1, . . . Nr (9) Nr(cid:88) p= Ti,p, = 1, . . . , Nr 0, = 1, . . . , Nr m, = 1, . . . Nr. Ti,p Cluster Update Update ˆct+1 as follows: ˆct+1 = ci (cid:80)Nr i=1 i,p (cid:80)Nr i=1 i,p ˆct p, , if (cid:80)Nr i=1 i,p > 0, otherwise. (11) (12) Fig. 3 The histogram of activation rates µ for the 3-th block with Ka = 1, 000. Note that in practical implementation, we normalize X, Wgate and Wup before the calculation to eliminate the influence of their magnitudes on the output. We then calculate the activation markers for the hidden state of all the tokens, which dh , = is reshaped as an activation feature matrix s. Denote the i-th column of as the feature vector cdh ] which represents the ci i-th neurons activation status on the sampled tokens. By calculating the expected value of each ci, we can obtain the vector of activated rates µ as: Rq, i.e., = [c1 c2 Rq µ = [µ1, µ2, , µdh ], with µi = 1 (cid:88) j=1 cj,i, (7) where cj,i indicates the j-th element of ci. We draw the histogram of µ as in Fig. 3. The histogram reveals highly skewed distribution of activation rates, where the majority of neurons exhibit low activation rates (below 0.1), with sharp peak near 0.07. However, the distribution also features long tail, indicating the presence of subset of neurons with significantly higher activation rates extending up to 1. These high-activation neurons are likely active across wide range of input tokens, making them suitable for processing common knowledge rather than task-specific specialization. Therefore, we construct shared experts by grouping these high-activation neurons. Given the total number of shared experts as Ns and the expert size m, we get the selection indices set SNs by selecting Ns SNs = µj { Since the independency across neurons, as we discussed, we build the experts using the same architecture as the original FFN and assign the expert parameters to the neurons of selected indices. Since all shared experts are always activated, we just construct them together for convenience: neurons with highest activation rates: : µi { , Ns . m) } TopK( dh} (8) 1 Ws up = Wup[:, SNs], gate = Wgate[:, SNs], down = Wdown[SNs, :], Ws Ws Rd Ns up m, Ws where Ws down RNs are the weights of linear layers of the shared experts Es. Given the input embedding x, the output of Es is obtained via and Ws gate Ns Rd Es(x) = hsWs down, with hs = Swish(xWs gate) (xWs up). 4 The steps are terminated until ˆct+1 = ˆct p, = 1, . . . , Nr. We include the distance value di,j into the cost function so that the balanced K-means clustering makes the intraexpert distances low and inter-expert distances high. However, the cluster assignment we defined above is an unbalanced > m) and cannot directly assignment problem (Nr be solved by existing algorithms for balanced assignment. Therefore, we reduce it to balanced assignment by extending the distance matrix as follows: Dext = (cid:2) d1, . . . , d1 , . . . , dNr , . . . , dNr , d2, . . . , d2 (cid:125) (cid:123)(cid:122) (cid:124) (cid:125) (cid:124) (cid:125) times (cid:123)(cid:122) times (cid:123)(cid:122) times (cid:124) (cid:3), where we repeat every column of m-times to obtain RNr the extended matrix Dext m. Then the reduced balanced assignment problem is formulated as: Nr Balanced Assignment Let linear program with ˆct fixed: ,t i,p be solution to the following Nr (cid:88) Nr (cid:88) i=1 p=1 i,p dext i,p (13) min s.t. Nr (cid:88) i=1 Nr (cid:88) p=1 i,p i,p = 1, = 1, . . . Nr i,p = 1, = 1, . . . , Nr 0, = 1, . . . , Nr i,p = di,p if where dext And the balanced cluster ˆct+1 m < m, m, = 1, . . . Nr m. (p + 1) can be updated as follows: ˆct+1 = Nr (cid:80) i=1 Nr (cid:80) i=1 mp (cid:80) k=k0 mp (cid:80) k=k0 ˆct p, ,t i,k ci ,t i,k , if mp (cid:80) k=k0 Nr (cid:80) i=1 ,t i,k>0, (14) otherwise, where k0 = m(p 1) + 1. Drawing on the Jonker-Volgenant algorithm [32], this problem can be addressed as reduced assignment problem in each step of the K-means algorithm, with complexity of O(n3). The final solution gives us the optimized strategy to group the routed experts. We get the selection indices set SNr,p, = 1, . . . , Nr, for each routed expert Er p: SNr,p = : { i,k = 1, for { m(p 1) + 1, . . . , We then build the weights Wr,p up and Wr,p Equation (9). And the output of Er down Rd gate of each routed expert Er is given by: Rm Rd as in m, Wr,p . }} Er p(x) = hr pWr,p down, with hr = Swish(xWr,p gate) (xWr,p up ). Suppose the MoE activates Nk experts out of the Nr routed experts. We expect the router to choose the routed experts 5 with top Nk scores. Therefore, we modify the MoE-version FFN as in (15), FM oE(x) = Es(x) + Nr(cid:88) i=1 giEr (x), gi = (cid:26) 1, TopK( si { 0, otherwise, 1 si , Nk), Nr} = [s1, s2, , sNr ] = G(x), where we make the expert score gi { un-scaled version of the expert output to avoid biases. 0, 1 } to enable an (15) B. Training-free Router Construction We now present the training-free router network for CMoE. Unlike previous works, which either built the router from scratch or intuitively used hidden features as initialization, we formulate the router construction as minimization problem and then develop an algorithm with analysis to construct the router by approximating the optimal solution. Given the same input embedding x, the output of original output of the dense FFN, i.e. (x) in (1), is equivalent to the sum of the output of all the experts in FM oE: (x) = Es(x) + Nr(cid:88) i=1 Er (x). (16) The only difference between (16) and FM oE(x) in (15) is the expert score g, which is obtained from the TopK selection of the output of G. Therefore, to preserve important knowledge captured by the original dense FNN, can be constructed to enforce FM oE(x) to be close to (x) by solving the minimization problem (17), arg min FM oE(x; G) (x) = arg min Nr(cid:88) i=1 (gi 1)Er (x) =arg min Sde(cid:88) Er (x) , and Sde Nr} , Nk) } TopK( { : si / { where Sde = = 1 si Nk, and the problem becomes constructing the to Nr minimize the absolute sum of the output of deactivated routed experts. Note that we have made hypothesis in (4) that the output/sparsity of (x) is highly related to the norm/sparsity of h, which is the same for the expert outputs. Based on (3) and (4), we reformulate (17) as in (17): Sde(cid:88) arg min Er (x) by (3) = arg min Sde(cid:88) (cid:88) SNr ,i hjwdown,j by (4) Sde(cid:88) (cid:88) arg min ( Eh [ hr 1 The problem becomes constructing that can minimize the expected hidden states of deactivated routed experts. = arg min SNr ,i ) hj Sde] . (17) Note that controls the de-/activation of routed experts by outputting the token-to-expert affinity = [s1, . . . , sNr ]. Therefore, solution for the above problem is to construct the such that matching the sorting indices of the set hr 1]), s1, . . . , sNr } { i.e. and the set permutation σ such that: 1, . . . , hr hr { = Eh [ Nr } (hr sσ(1) hr σ(1) sσ(2) hr σ(2) sσ(Nr) and hr σ(Nr), (18) by which we can verify that the minimum of (17) is: min Eh [ 1 Nr which is obtained by setting G(x) as: hr 1 Sde] = Nk Nr Nk(cid:88) i= hr σ(i), (19) { { , Nk) Nr} Sde = = Nk) σ(1), . . . , σ(Nr } TopK( 1 si : si / { Finally, we pay attention to the hidden state hr of any routed expert, which is the output by the neurons that we grouped in Section IV-A, where the cluster is centered to the centroid ˆc. We denote the neuron in each cluster that has the lowest L2 distance to the centroid as the representative neuron: (20) . } Rj = i, if ci ˆcj2 ck ˆcj2, SNr,j. (21) Therefore, when we regard the hidden state value hr as Rj hr feature of the representative neuron and assume that hr , Rj where hr refers to the expected hidden state value, we can construct the router by grouping the representative neurons of all the routed experts: G(x) = Swish(xWR gate) (xWR up), gate = Wgate[:, SR], WR up = Wup[:, SR], and (22) . This leads to , sNr ] = (cid:3) , , hr Nr (cid:104) hr R1, hr R2, (cid:105) , hr RNr (23) where WR SR = R1, . . . , RNr } { G(x) = [s1, s2, 1, hr 2, (cid:2)hr which is hence an approximate solution for the original problem introduced in (17). C. Differentiable Routing and Load-balancing Though we have constructed well-designed router in Section IV-B, it is not differentiable since each expert score gi is constant, as shown in (15), hindering further alignment and performance recovery. Therefore, we introduce learnable parameter when computing the expert scores as follows, gi= (cid:26) 1 + si 0, ui, TopK( si { otherwise, 1 si Nr} , Nk), = Softmax(s), = [u1, u2, . . . , uNr ] (24) where the scale is initialized as zero to avoid perturbation. For MoE models, load-balancing is crucial to guarantee computational efficiency, especially expert parallelism in 6 LLMs serving. As in DeepSeek-V3 [4], we use the auxiliaryloss-free load balancing by introducing bias term before the TopK selection: ui, gi= (cid:26) 1 + si = [b1, b2, . . . , bNr ]. 0, si + bi otherwise, TopK( 1 si { , Nk), Nr} (25) Here, is initialized as zero and updated based on the expert load status during each step of training, as in DeepSeek-V3. The hyper-parameter update speed γ is used to update the bias term at the end of each step, i.e. decreasing/increasing the bias term by γ if the corresponding expert is overloaded/underloaded. V. EXPERIMENTS CMoE is implemented based on Hugging Face Transformers [33] together with Pytorch [34]. The experiments are conducted on one NVIDIA H800 PCIe 80GB graphics card with CUDA Driver 12.6. We randomly select samples from WikiText-2 training dataset [35] as calibration and fine-tuning data. We use only 8 examples with 2,048 sequence length as calibration for calculating the hidden state in (6). We set Ka = 10 for the activation status record, which sounds counter-intuitive but works best in practice. For lightweight fine-tuning, we run 1 epoch using the Adam optimizer [36] with β1 = 0.9 and β2 = 0.95. We also employ LoRA [37] with rank of 8 and lora alpha = 32. We fine-tune all the models including the baselines with 2, 048 samples. We set different initial learning rates for the score scale and other 5, respectively. We set the parameters, i.e. 0.001 and 5.95e bias update speed γ to 0.001. A. Main Results We compare CMoE with the up-to-date baseline LLaMAMoE [11], in which the neurons are randomly split, and continual pre-training is carried out with additional router networks. The experiments cover both training-free and lightweight fine-tuning versions. We design these experiments to demonstrate the remarkable post-training performance of CMoE. we demonstrate the results on two different datasets, i.e. WikiText-2 [35] and C4 [38]. The baseline models are chosen as LLaMA-2-7B and LLaMa-3-8B. Language Modeling Performance. We evaluate the perplexity of the constructed Mixture-of-Experts (MoE) models in Table I. We use abbreviations to denote the composition of experts. For instance, S2A2E16 represents 2 shared experts, 2 activated routed experts, and total of 16 experts and the total activation ratio is (2 + 2)/16 = 25%. Our findings indicate that in the training-free scenario, the ppl of our baseline LLaMA-MoE on both LLaMA-2-7B and LLaMA-3-8B are NaN across different datasets. In contrast, the perplexity of the proposed CMoE can be effectively controlled. Moreover, after fine-tuning, we observe that the perplexity can be reduced to as low as 12.73 when the activation ratio is 25%, which corresponds to sparsity of 75%. Additionally, the CMoE model outperforms TABLE Comparison of perplexity results. Randomly select samples from WikiText-2 training dataset for calibration and training. Method Type Training-free Fine-tuning Training-free Fine-tuning WikiText-2 C4 WikiText-2 C4 WikiTextC4 WikiText-2 C4 LLaMA-2-7B LLaMA-3-8B Dense LLaMA-MoE LLaMA-MoE CMoE CMoE CMoE - A2E8 A4E16 S1A1E8 S1A3E16 S2A2E16 5.27 nan nan 60.86 89.19 62.30 7.27 nan nan 135.61 180.65 136.12 - 468.00 540.62 12.76 13.84 12.73 - 2660.68 2690.68 32.12 33.56 32.37 6.14 nan nan 162.74 262.85 143. 9.44 nan nan 324.71 465.09 284.19 - 988.20 1094.24 21.16 22.97 21.01 - 7521.83 7758.91 64.03 72.34 65.57 all state-of-the-art (SOTA) structured pruning methods [9], [22], [23]. Downstream Tasks Performance. We also evaluate the performance of the constructed MoE models on various downstream tasks. We evaluate on the following benchmarks: 32-shot BoolQ [39], 0-shot PIQA [40], 0-shot SciQ [41], 5-shot Winogrande [42], 25-shot ARC-Challenge [43], and 10-shot HellaSwag [44]. The results are presented in tABLE II, where we denote fine-tuning or not with FT. In all activation ratio configurations, we find that the proposed CMoE outperforms the LLaMA-MoE in accuracy across diverse set of downstream tasks, on both training-free and fine-tuning scenarios. To illustrate the performance of the models, we select the SciQ dataset for zero-shot testing and the BoolQ dataset for few-shot testing as representative examples. When the activation ratio is set at fixed value of 25% (corresponding to sparsity rate of 75%), after fine-tuning, the LLaMA-MoE model can only achieve an accuracy equivalent to 21.2% of that of the relevant dense model on the SciQ dataset and 45.31% on the BoolQ dataset. Given that the perplexity (ppl) of the LLaMA-MoE model is Not Number (NaN) in the training-free scenario, we do not report its accuracy in this case. In contrast, even without fine-tuning (i.e., in the training-free scenario), the proposed CMoE model demonstrates superior performance, attaining an accuracy equivalent to 56.3% of that of the dense model on the SciQ dataset and 54% on the BoolQ dataset. After fine-tuning, the performance of the CMoE model is further enhanced, with the accuracy reaching 76.59% on the SciQ dataset and 74.3% on the BoolQ dataset (relative to dense model). B. Ablation Studies We conduct ablation studies with LLaMA-2-7B and data randomly selected from WikiText-2 training datasets. Impact of Training Data Size. To quantify the relationship between training data volume, model performance, and computational efficiency, we systematically vary the number of training samples from 0 (initial state) to 4,096 while measuring perplexity (PPL) and construction time. Fig. 4 illustrates the results on the setting of S2A2E16, revealing critical trends in performance-cost trade-offs. Increasing training data from 0 to 4,096 samples reduces 12.21) but non-linearly perplexity by 80.4% (62. Fig. 4 Trade-off between Model Performance and Construction Time with Increasing Training Data. increases construction time from 298s to 4,502s. While performance improves sharply initially (64 samples cut PPL by 63%), gains diminish beyond 1,024 samples (13.47 PPL), with marginal improvements (12.21 PPL at 4,096 samples) requiring disproportionately higher runtime (+133% from 2,048 to 4,096 samples). The results demonstrate that CMoE only needs modest of data to achieve low perplexity, while further performance increase is hard and possibly demands more diverse data. Shared Experts Ratio. We carry out an extensive analysis of the influence of the shared expert ratio on perplexity. In our experimental setup, we have total of 32 experts, among which 8 are activated. We systematically adjust the proportion of shared experts within these 8 activated experts. As depicted in Table Fig. 5(a), the results clearly demonstrate distinct trend: as the ratio of shared experts rises from 0.125 to 0.875, the perplexity continuously decreases. There is remarkable improvement in the perplexity value, dropping from 14.48 to 11.93. This finding indicates that shared experts generally contribute positively to the performance of the model. However, it is also evident that the marginal returns tend to diminish when the ratio of shared experts reaches relatively high level. Activation Rate. We conduct an external analysis to investigate how the total activation rate (comprising both shared and activated routed experts) impacts model performance across different domains, in comparison with dense baselines. We perform evaluations on the CMoE model featuring total of 16 experts. Specifically, we vary the ratio of shared experts and activated routed experts at fixed proportion of 1:1 on the WikiText-2 and C4 datasets. As illustrated in Table Fig. 5(b), monotonic decrease in PPL can be found as the activation rate increases, gradually approaching the performance level of TABLE II Comparison of downstream tasks. Randomly select samples from WikiText-2 training dataset as calibration and training data. Model Method Type FT BoolQ(32) SciQ PIQA WinoGrande(5) ARC-C(25) HellaSwag(10) LLaMA-2-7B LLaMA-3-8B Dense LLama-MoE LLama-MoE CMoE CMoE CMoE CMoE CMoE CMoE Dense LLama-MoE LLama-MoE CMoE CMoE CMoE CMoE CMoE CMoE - A2E8 A4E16 S1A1E8 S1A3E16 S2A2E16 S1A1E8 S1A3E16 S2A2E16 - A2E8 A4E16 S1A1E8 S1A3E16 S2A2E16 S1A1E8 S1A3E16 S2A2E16 - - 82.04 37.83 37.82 46.09 50.79 53.85 55.04 53.66 57.19 83.48 37.82 37.83 47.43 41.31 45.08 56.88 60.55 62.14 90.80 20.00 20.60 65.30 62.00 66.50 77.50 75.30 77.30 94.2 20.30 20.50 47.70 46.60 53.10 72.00 71.40 72.20 78.78 49.73 49.56 52.77 52.29 53.26 57.12 56.37 56.86 80.79 49.02 51.52 52.56 51.63 50.87 57.34 57.07 59. 73.95 50.12 49.40 48.70 50.12 49.33 54.06 53.98 52.57 77.50 50.74 49.96 50.67 48.77 53.19 51.93 53.51 51.14 53.15 25.79 25.98 23.80 24.23 23.72 27.56 27.82 26.45 59.72 25.68 25.27 23.72 24.15 24.23 25.77 26.54 27.73 78.55 26.18 27.21 30.12 28.76 29.85 38.79 38.89 38.90 82.16 25.76 26.00 28.08 27.41 28.44 36.68 36.28 36. (a) (b) (c) Fig. 5 Ablation studies:(a) Impact of shared expert ratio on model performance; (b) Activation Rate vs. Model Performance; (c) Effect of Load Balancing. the dense model. For both datasets, an activation rate of 75% enables the model to achieve nearly comparable performance to the dense model. On the WikiText-2 dataset, the PPL values are 5.79 for the model under consideration and 5.27 for the dense model; on the C4 dataset, the corresponding values are 11.19 and 7.27. These results demonstrate that CMoE architectures can attain performance quality comparable to that of dense models, even when characterized by relatively high sparsity. Load Balancing. CMoE can inherently achieve effective loadbalancing among routed experts, with the exception of some special blocks. For example, the last block of LLaMA-27B exhibits extremely unbalanced expert counts. However, this issue can be effectively resolved by the load-balancing mechanism we introduced in Section IV-C. As presented in Fig. 5(c), prior to the implementation of the load-balancing strategy, substantial disparities in the workloads assigned to different experts are clearly observable. Specifically, Expert 3 and Expert 8 are burdened with disproportionately high loads, handling 11,163 and 11,225 instances respectively. In stark contrast, other experts process significantly lower number of instances. For example, Expert 12 and Expert 13 handle only 100 and 102 instances respectively. Upon the application of our proposed load - balancing mechanism, more equitable and uniform distribution of the computational workload across all experts is successfully achieved. After the adjustment, the number of instances processed by each expert falls within the range of 1,443 to 3,584. VI. CONCLUSION We present CMoE, framework that efficiently carves sparse Mixture-of-Experts (MoE) architectures from dense LLMs through parameter reorganization and lightweight adaptation. By leveraging activation sparsity patterns in FFN layers, CMoE groups neurons into shared and routed experts via novel balanced linear assignment formulation. The router is analytically initialized from activation statistics and refined via differentiable scaling and load balancing. Experiments show that CMoE can construct effective Mixture-of-Experts (MoE) models, achieving comparable perplexity to dense models and outperforming baseline models. For instance, on the SciQ dataset, CMoE can reach 56.3% even without finetuning and it can further improve the accuracy to 76.59% after fine-tuning, huge outperforming compared with 21.2% (the accuracy of the baseline LLaMA-MoE). Extensive experiments have demonstrated that CMoE offers practical approach for deploying large language models (LLMs) in resourceconstrained environments."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., Opt: Open pre-trained transformer language models, arXiv preprint arXiv:2205.01068, 2022. [2] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [3] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, Advances in neural information processing systems, vol. 36, 2024. [4] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. [5] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, Gshard: Scaling giant models with conditional computation and automatic sharding, arXiv preprint arXiv:2006.16668, 2020. [6] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., Glam: Efficient scaling of language models with mixture-of-experts, in International Conference on Machine Learning. PMLR, 2022, pp. 55475569. [7] W. Fedus, B. Zoph, and N. Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, Journal of Machine Learning Research, vol. 23, no. 120, pp. 139, 2022. [8] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu et al., Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, arXiv preprint arXiv:2401.06066, 2024. [9] Z. Liu, J. Wang, T. Dao, T. Zhou, B. Yuan, Z. Song, A. Shrivastava, C. Zhang, Y. Tian, C. Re et al., Deja vu: Contextual sparsity for efficient llms at inference time, in International Conference on Machine Learning. PMLR, 2023, pp. 22 13722 176. [10] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, Moefication: Transformer feed-forward layers are mixtures of experts, arXiv preprint arXiv:2110.01786, 2021. [11] T. Zhu, X. Qu, D. Dong, J. Ruan, J. Tong, C. He, and Y. Cheng, Llama-moe: Building mixture-of-experts from llama with continual pre-training, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024, pp. 15 91315 923. [12] X. Qu, D. Dong, X. Hu, T. Zhu, W. Sun, and Y. Cheng, Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training, arXiv preprint arXiv:2411.15708, 2024. [13] H. Zheng, X. Bai, X. Liu, Z. M. Mao, B. Chen, F. Lai, and A. Prakash, Learn to be efficient: Build structured sparsity in large language models, arXiv preprint arXiv:2402.06126, 2024. [14] S. Zuo, Q. Zhang, C. Liang, P. He, T. Zhao, and W. Chen, Moebert: from bert to mixture-of-experts via importance-guided adaptation, arXiv preprint arXiv:2204.07675, 2022. [15] A. Komatsuzaki, J. Puigcerver, J. Lee-Thorp, C. R. Ruiz, B. Mustafa, J. Ainslie, Y. Tay, M. Dehghani, and N. Houlsby, Sparse upcycling: Training mixture-of-experts from dense checkpoints, arXiv preprint arXiv:2212.05055, 2022. [16] H. Wu, H. Zheng, Z. He, and B. Yu, Parameter-efficient sparsity crafting from dense to mixture-of-experts for instruction tuning on general tasks, arXiv preprint arXiv:2401.02731, 2024. [17] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, Awq: Activation-aware weight quantization for on-device llm compression and acceleration, Proceedings of Machine Learning and Systems, vol. 6, pp. 87100, 2024. [18] Z. Pei, X. Yao, W. Zhao, and B. Yu, Quantization via distillation and contrastive learning, IEEE Transactions on Neural Networks and Learning Systems, 2023. [19] L. Zou, W. Zhao, S. Yin, C. Bai, Q. Sun, and B. Yu, Bie: Bi-exponent block floating-point for large language models quantization, in Fortyfirst International Conference on Machine Learning, 2024. [20] Y. Lin, H. Tang, S. Yang, Z. Zhang, G. Xiao, C. Gan, and S. Han, Qserve: W4a8kv4 quantization and system co-design for efficient llm serving, arXiv preprint arXiv:2405.04532, 2024. [21] X. Men, M. Xu, Q. Zhang, B. Wang, H. Lin, Y. Lu, X. Han, and W. Chen, Shortgpt: Layers in large language models are more redundant than you expect, arXiv preprint arXiv:2403.03853, 2024. 9 [22] S. Ashkboos, M. L. Croci, M. G. d. Nascimento, T. Hoefler, and J. Hensman, Slicegpt: Compress large language models by deleting rows and columns, arXiv preprint arXiv:2401.15024, 2024. [23] Z. Pei, H.-L. Zhen, X. Yu, S. J. Pan, M. Yuan, and B. Yu, Fusegpt: Learnable layers fusion of generative pre-trained transformers, arXiv preprint arXiv:2411.14507, 2024. [24] H. Zheng, X. Bai, X. Liu, Z. M. Mao, B. Chen, F. Lai, and A. Prakash, Learn to be efficient: Build structured sparsity in large language models, arXiv preprint arXiv:2402.06126, 2024. [25] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [26] N. Shazeer, Glu variants improve transformer, arXiv preprint arXiv:2002.05202, 2020. [27] V. Nair and G. E. Hinton, Rectified linear units improve restricted boltzmann machines, in Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 807814. [28] Z. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, Moefication: Transformer feed-forward layers are mixtures of experts, arXiv preprint arXiv:2110.01786, 2021. [29] J. Song, K. Oh, T. Kim, H. Kim, Y. Kim, and J.-J. Kim, Sleb: Streamlining llms through redundancy verification and elimination of transformer blocks, arXiv preprint arXiv:2402.09025, 2024. [30] X. Chen, Y. Hu, and J. Zhang, Compressing large language models by streamlining the unimportant layer, arXiv preprint arXiv:2403.19135, 2024. [31] M. I. Malinen and P. Franti, Balanced k-means for clustering, in Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop, S+ SSPR 2014, Joensuu, Finland, August 20-22, 2014. Proceedings. Springer, 2014, pp. 3241. [32] R. Jonker and T. Volgenant, shortest augmenting path algorithm for dense and sparse linear assignment problems, in DGOR/NSOR: Papers of the 16th Annual Meeting of DGOR in Cooperation with NSOR/Vortrage der 16. Jahrestagung der DGOR zusammen mit der NSOR. Springer, 1988, pp. 622622. [33] T. Wolf, Huggingfaces transformers: State-of-the-art natural language processing, arXiv preprint arXiv:1910.03771, 2019. [34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, 2019. [35] S. Merity, C. Xiong, J. Bradbury, and R. Socher, Pointer sentinel mixture models, arXiv preprint arXiv:1609.07843, 2016. [36] D. P. Kingma, Adam: method for stochastic optimization, arXiv preprint arXiv:1412.6980, 2014. [37] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685, 2021. [38] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of machine learning research, vol. 21, no. 140, pp. 167, 2020. [39] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, Boolq: Exploring the surprising difficulty of natural yes/no questions, arXiv preprint arXiv:1905.10044, 2019. [40] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al., Piqa: Reasoning about physical commonsense in natural language, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 74327439. [41] J. Welbl, N. F. Liu, and M. Gardner, Crowdsourcing multiple choice science questions, arXiv preprint arXiv:1707.06209, 2017. [42] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, Winogrande: An adversarial winograd schema challenge at scale, Communications of the ACM, vol. 64, no. 9, pp. 99106, 2021. [43] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, Think you have solved question answering? try arc, the ai2 reasoning challenge, arXiv preprint arXiv:1803.05457, 2018. [44] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019."
        }
    ],
    "affiliations": [
        "Noahs Ark Lab, Huawei",
        "The Chinese University of Hong Kong"
    ]
}