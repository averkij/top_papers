{
    "paper_title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
    "authors": [
        "Jiuhai Chen",
        "Jianwei Yang",
        "Haiping Wu",
        "Dianqi Li",
        "Jianfeng Gao",
        "Tianyi Zhou",
        "Bin Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2's visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VL's visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. https://github.com/JiuhaiChen/Florence-VL"
        },
        {
            "title": "Start",
            "content": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion Jiuhai Chen1*, Jianwei Yang2, Haiping Wu2, Dianqi Li, Jianfeng Gao2, Tianyi Zhou1, Bin Xiao2 1University of Maryland 2Microsoft Research 4 2 0 2 5 ] . [ 1 4 2 4 4 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present Florence-VL, new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2 [45], generative vision foundation model. Unlike the widely used CLIP-style vision transformer [35] trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose novel featurefusion architecture and an innovative training recipe that effectively integrates Florence-2s visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose depth-breath fusion (DBFusion) to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on carefully designed recipe of diverse open-source datasets that include high-quality image captions and instructiontuning pairs. Our quantitative analysis and visualization of Florence-VLs visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. https://github.com/JiuhaiChen/Florence-VL 1. Introduction Recent progress in multimodal large language models (MLLMs) are largely driven by progress in large language *The work is done during Jiuhai Chens internship at Microsoft Research. Figure 1. Comparison of LLaVA-style MLLMs with our FlorenceVL. LLaVA-style models use CLIP, pretrained with contrastive learning, to generate single high-level image feature. In contrast, Florence-VL leverages Florence-2, pretrained with generative modeling across various vision tasks such as image captioning, OCR, and grounding. This enables Florence-VL to flexibly extract multiple task-specific image features using Florence-2 as the image encoder. models [26, 49]. However, when it comes to visual encoders, transformer-based models like CLIP or SigLIP remain the most commonly used choices. Despite CLIP and SigLIPs effectiveness, they come with limitations; for instance, their last-layer features usually provide an imagelevel semantic representation that captures the overall scene and context, but often overlook pixel or region-level details and low-level features that are critical to various downstream tasks. There is much broader range of visual representation, such as the self-supervised DINOv2 model [34], diffusion model [37] and segmentation [20], [41] shows these different visual encoders can benefit well in some specific tasks. In order to leverage distinctive representations of multiple vision encoders, some recent works such as [38, 41] 1 Figure 2. An overview of Florence-VL, which extracts visual features of different depths (levels of feature concepts) and breaths (prompts) from Florence-2, combines them using DBFusion, and project the fused features to an LLMs input space. Florence-VL is fully pretrained on image captioning data and then partially finetuned on instruction-tuning data. adopt mixture of vision encoders that specialize in different feature aspects or skills. However, integrating multiple vision encoders increases the computational expense for both model training and deployment. Could single vision model be designed to generate distinct visual features, each emphasizing different perceptual information in the input image? In this paper, we propose Florence-VL, which leverages the generative vision foundation model Florence-2 [45] as the vision encoder. Florence-2 offers prompt-based representation for various computer vision tasks, including captioning, object detection, grounding, and OCR. Its versatile visual representations can benefit different types of downstream tasks. For instance, OCR-based representations are advantageous for tasks that require extracting textual information from images, and groundingbased representation can benefit for tasks that require the relationships between objects and their spatial contexts. However, to build better MLLM, how to extract these diverse features and align them with pretrained LLM remains underexplored. To address this, we propose Depth-Breadth Fusion (DBFusion) to effectively selecting and utilizing diverse visual features. Visual features from different layers capture various levels of concepts, with the final layers typically representing higher-level concepts. Integrating lower-level features can therefore complement these high-level representations, which we refer to as the Depth of visual features. Additionally, since different downstream tasks need different perceptual information within images, single image feature often falls short in capturing all relevant information. Thus, we leverage multiple image features, with each feature capturing different visual representations. We refer to this as the Breadth of visual features. For utilizing these diverse visual features, we find that straightforward channel concatenation serves as simple yet effective fusion strategy. Specifically, we concatenate multiple features along the channel dimension, and these combined features, spanning various depths and breadths, are then projected as input embedding to LLMs. We train Florence-VL on novel recipe of open-sourced training data, which is composed of large-scale detailed captioning dataset and mix of instruction tuning datasets for whole-model pretraining and partial-model finetuning, respectively. The resulted Florence-VL achieves significant advantages on 25 benchmarks covering vision-centric, knowledge-based, and OCR & Chart tasks, outperforming other advanced MLLMs like Cambrian [41]. Moreover, we provide quantitative analysis and visualization demonstrating that Florence-VLs visual representation achieves better alignment to LLMs than the widely adopted vision encoders such as CLIP and SigLIP [26]. 2. Preliminary: Florence-2 Florence-2 [45] is vision foundation model that utilizes unified, prompt-based approach to handle various vision tasks with simple instructions, such as captioning, object detection, grounding, and segmentation. The architecture consists of vision encoder DaViT [9] and standard encoder-decoder model. It processes an input image RHW 3 (where and indicate height and width, respectively) into flattened visual token embeddings. The model then applies standard encoder-decoder transformer architecture to process both visual and language token embeddings. It first generates prompt text embeddings RNtD using the language tokenizer and word embedding layer, with Nt and representing the number and 2 Figure 3. Visualization of the first three PCA components: we apply PCA to image features generated from Detailed Caption, OCR, and Grounding prompts, excluding the background by setting threshold on the first PCA component. The image features derived from the Detailed Caption prompt (second column) capture the general context of the image, those from the OCR prompt (third column) focus primarily on text information, and those from the Grounding prompt (fourth column) highlight spatial relationships between objects. Additionally, we visualize the final layer features from OpenAI CLIP (ViT-L/14@336) in the last column, showing that CLIP features often miss certain region-level details, such as text information in many cases. dimensionality of prompt tokens, respectively. The vision token embeddings are then concatenated with the prompt embeddings to create the input for the multi-modality encoder module, = [V, T], where RNvD is produced by applying linear projection and LayerNorm layer to visual embedding from DaViT, with Nv and representing the number and dimensionality of vision tokens, respectively. The linear projection and LayerNorm layer are used to ensure dimensionality alignment with T. Encoderdecoder model will process the and generate the desirable results, such as captions, object detections, grounding in textual form. 3. Method 3.1. Using Florence-2 as Vision Backbone To address the limitations of existing vision backbones in MLLMs, specifically, last layer features typically yield an image-level representation that captures overall scene and context but often misses pixelor region-level details, we utilize the vision foundation model Florence-2 as our visual encoder for extracting visual features. Unlike the CLIP pretrained vision transformers that provide single, universal image feature, Florence-2 can identify spatial details at different scales, by using different tasks prompts. In MLLMs, effective image understanding requires capturing multiple levels of granularity, from global semantics to local details, and understanding spatial relationships between objects and entities within their semantic context. Florence-2, with its capability to manage diverse granularity levels, is an ideal vision encoder to address these core aspects of image comprehension. In the following section, we explore how to leverage Florence-2s strengths in integrating it into MLLMs. 3.2. Visual Features spanning Depth and Breadth Breadth. Since different downstream tasks require varying perceptual information from images, we consider expanding the breadth of visual representation. Given an input image RHW 3 and task-specific prompt, such as provide the text shown in the image, Florence-2 will process the image feature and prompt feature into = [V, T] and then feed into the encoder-decoder transformer architecture. The encoder employs an attention mechanism to process X, producing an output = [V, T]. Due to the cross-attention between and T, the updated image fea3 ) ( e M t # Token Integration 1728 Average Pooling Channel Integration 576 576 66.6 65.7 66.1 P 88.7 88.8 89.4 - M 34. 32.3 35.2 - 1536.3 1551.3 1543. n o l h b - L 45.0 45.7 46.8 63. 64.6 65.0 m - S 70.9 70. 70.3 i a 28.1 27.4 28. M 36.4 36.0 35.6 e O 40. 41.2 41.4 2 56.9 56.6 57. t C 23.0 24.6 24.3 c 44.6 44.8 44.5 o 29.5 29. 29.4 r 50.3 50.4 50.8 Table 1. Experiments for different fusion strategies. The vision token count is 1728 for token integration, which leads to longer training and inference times. The channel integration strategy shows better performance and training efficiency compared to the other two fusion methods. ture becomes more focused on the prompt provide the text shown in the image, specifically extracting more text information from the image. We focus on three distinct tasks that contribute to image understanding, resulting in three different image embeddings [V t1 ], each tailored to specific task: , t3 , t2 Detailed Image Caption: describe what is shown in the image with paragraph. It enables the model to give overall context of an image. OCR: provide the text shown in the image. It extracts more text information from the image. Dense Region Caption: locate the objects in the image, with their descriptions. It captures the spatial relationships between objects. We visualize the image features with different task prompts, applying PCA to the visual embeddings and setting threshold for the visualization. As illustrated in Figure 3, different image embeddings emphasize distinct conceptual information within the images. Additionally, we also visualize the final layer image features from OpenAI CLIP in Figure 3, which often lacks certain region-level details in most cases. Depth. We also integrate lower-level from DaViT, combined with higher-level [V t1 to capture multiple levels of conceptual detail. features using features ] derived from the three prompts, allows us , t2 , t3 Token Integration: This approach involves concatenating all features along the token dimension. However, this can make the visual token excessively long and complicate model training. Average Pooling: Alternatively, average pooling over all features can be used, but this method may result in information loss. Channel Integration: more effective method is to concatenate features along the channel dimension, which does not increase the sequence length. To quickly assess which feature fusion method provides the best overall performance, we use datasets from LLaVA-1.5 [26], which include 558K image captions for pre-training and 665K entries for instruction tuning. In the Table 1, the channel integration strategy shows better performance and training efficiency compared to the other two fusion methods. Thus we choose channel integration simple yet effective fusion strategy. 3.4. Florence-VL As shown in Figure 2, Florence-VL is composed of the vision foundation model Florence-2 and the large language model. After extracting multiple image features, we use MLP to project these features into the language model space. During the pretraining stage, we align Florence-2 with the language model using image detailed caption data. In the instruction tuning stage, we use diverse and highquality instruction-tuning dataset to effectively adapt the model to downstream tasks. 3.3. Depth-Breadth Fusion 4. Analysis on Different Vision Encoders Since we have image feature with different level of granularity, feature fusion is commonly used. When dealing with multiple feature embeddings, such as [V, ], t1 the next question becomes how to fuse these features and align them with the language model space. To take advantage of all these four features, several approaches can be considered for this fusion process: , t , t2 To demonstrate that Florence-2 is superior vision encoder compared to others, we quantify the cross-modal alignment quality between various vision encoders and language models, allowing us to assess the impact of different vision encoders without requiring subsequent supervised fine-tuning and evaluations on benchmarks [15, 43]. Specifically, consider pretrained MLLM = (V, L) where is the vi4 n=1, {tn}N sion encoder and represents the language model, we input set of image-text pairs, (V, ) = ({vn}N n=1), into the model. For the nth image-text pair, the vision encoder produces vision representations vn Rrnd , and the text representations tn Rsnd from last layer of the language decoder, where rn and sn are the number of tokens in the vision and text representations, and and are the hidden state dimensions for the vision and text tokens. We apply the trainable projection to vn to ensure dimensionality alignment with tn , that is P(f vn ) Rrnd. We also apply average pooling along token dimension and normalize along the hidden dimension for both P(f vn ) and tn . For all image-text pairs, we concatenate all vision features along the first dimension to form matrix vn RN d, and similarly concatenate all text features into matrix tn RN d. Since we need to measure the modality gap between vision tokens and text tokens, we compute the divergence between these two token representations. Specifically, we optimize the trainable projection P, which is used to bring these two representations closer together by minimizing cross-entropy loss function: = (cid:88) i,j (i,j) log (cid:0)softmax(F vn (F tn )T )i,j (cid:1) , where In is the target (indicator) matrix. The multiplication of vn with the transpose of tn calculates the correlation between vision and text token representations. In short, the loss function is designed to minimize the distance between vision tokens and their corresponding text tokens by maximizing the likelihood that each vision token aligns correctly with its associated text token. set of"
        },
        {
            "title": "We use",
            "content": "n=1, {tn}N image-text pairs (V, ) = ({vn}N n=1) from the LLaVA 1.5 pretraining image captioning datasets and select various vision encoders to assess how well we can optimize the alignment between the vision encoder and the language model. The vision encoders we evaluate include: Stable Diffusion [36], Dinov2 [34] (ViT-G/14, ViT-L/14, ViT-B/14), SigLIP, OpenAI CLIP, and our Florence-2 model. The chosen language model is Llama 3 8B Instruct. We plot the alignment loss in Figure 4, which clearly shows that Florence-2 vision encoder achieves the lowest alignment loss compared to the other vision encoders, demonstrating the best alignment with text embeddings. Additionally, SigLIP demonstrates competitive results, as noted in [41], which highlights SigLIPs strong benchmark performance relative to other vision encoders, aligning with the findings of our study. 5. Experiments Figure 4. We plot the alignment loss for different vision encoders, which clearly shows that Florence-2 vision encoder achieves the lowest alignment loss compared to the other vision encoders, demonstrating the best alignment with text embeddings. Figure 5. We plot the alignment loss for various feature combinations, removing one feature at time from different depths and breadths. The results clearly show that our method achieves the lowest alignment loss compared to others, highlighting the importance of all features from different depths and breadths for optimal alignment. detailed captions sourced from PixelProse [40]. For the instruction tuning stage, we also curate our high quality instruction tuning datasets, sourcing from Cambrian-7M [41], Vision Flan [46], ShareGPT4V [5], along with additional data from Docmatix [17] to improve chart and diagram comprehension [3]. The detail of training datasets and experiment details can be found in the appendix. Evaluation. We evaluate the performance of different MLLM models on 25 benchmarks with four different categories: Implementation Details. In order to build state-of-theart MLLM, we use images from CC12M [4], Redcaps [8], and Commonpool [12] during the pretraining stage, with General multimodal benchmarks: VQAv2 [13], GQA [16], MMBench (EN and CN) [27], VisWiz [14], POPE [22], MM-Vet [47], MME Perception [11], 5 . t V # - - 2 V 80.4 - Vila 3B Phi 3.5-Vision Florence-VL 3B (ours) 576 82.1 LLaVA next 8B 2880 - Vila 8B - 80.9 Mini-Gemini-HD 8B 2880 Cambrain 8B Florence-VL 8B (ours) 576 576 - - 84.7 ) ( e 63. 75.5 71.6 72.2 72.3 72.7 75. 76.2 ) ( e 52. 64.2 60.8 - 66.2 - 67. 69.5 61.5 63.5 61.8 65. 61.7 64.5 64.6 64.4 General Benchmarks W 53.5 58.2 59.1 57. 58.7 - - 59.1 P 86. 82.2 88.3 86.6 84.4 - 87. 89.9 - M 35.4 46.5 51. 41.7 38.3 - 48.0 56.3 - M 1442.4 1473.4 1498.7 1595.1 1577.0 1606. 1547.1 1560.0 - - 412.1 403. 379.3 - - - 381.1 (a) Results on general multimodal benchmarks. n o l - e - L - 53.3 58. 68.8 71.1 47.7 76.8 - - - - S 40.3 49. 44.9 - - - 48.7 57. 71.0 74.2 50.0 50.0 m - S 67.9 69.9 70.6 72.7 71.4 73. 74.7 74.9 Vision centric Knowledge based OCR & Chart l l # Vis tok. * e - 55.2 69.3 70. M - 2 - 67.7 64. 77.4 73.8 i a 30.6 - 52.2 53.3 53.5 60.4 59.6 63. 38.7 71.6 37.4 - 62.1 64. 64.2 - 62.6 72.2 73.4 - 18.7 51.3 73.3 - 73.5 73. 74.2 - 37.0 49.0 55.5 I - c 67.9 89.0 84.6 73. 79.9 75.1 80.4 85.9 M 34. 43.3 41.8 40.1 36.9 37.3 42. 43.7 t 58.1 61.1 69.1 e O - r - c - V I - 59.8 63.0 72.0 70. 75.9 82.1 40.7 51.3 65.4 55. 69.3 78.2 - 70.2 71.7 74. - 47.7 62.4 63.4 - 59. 73.3 74.7 - 74.6 77.8 84. - - - - 51.7 Vila 3B Phi 3.5 Vision Florence-VL 3B (ours) LLaVA next 8B Vila 8B Mini-Gemini-HD 8B Cambrian 8B Florence-VL 8B (ours) - - 576 2880 - 2880 576 576 (b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks. Table 2. Results on general multimodal benchmarks, Vision centric, Knowledge based, and OCR & Chart benchmarks. MME Cognition [11], SeedBench [21], HallusionBench, LLaVA in the Wild [26] and MMStar [6]. OCR & Chart benchmark: TextVQA [39], OCRBench [28], ChartQA [31], DocVQA [32] and InforVQA [33]. Knowledge based benchmark: AI2D [19], MathVista [30], MMMU [48] and ScienceQA [29]. Vision Centric benchmark: MMVP [42], RealworldQA [44] and CV-Bench [41]. Baselines. We select two language backbones: Phi-3.5mini-Instruct and LLama-3-8B-Instruct. For baseline comparisons among small models, we chose Vila 1.5 3B [24] and Phi 3.5-Vision-Instruct [1]. For the larger models, we select the baselines: LLaVA Next 8B [25], Vila 8B [24], Mini-Gemini-HD 8B [23] and Cambrain 8B [41], using LLama 3 8B Instruct as the language backbone. Results. In the Table 2, we present the results of FlorenceVL compared to various baselines across range of benchmarks, along with the number of visual tokens used. For the smaller-sized model, our model outperforms Vila 3B, and surpasses Phi 3.5 Vision on 12 out of 24 tasks. Notably, Phi 3.5 Vision utilizes 500 billion vision and text tokens [1], with its training data being proprietary and significantly larger than ours. Nonetheless, our Florence-VL 3B remains competitive with this model. For the larger-sized model, our model shows significant improvement over other baselines on most benchmarks. Notably, our model significantly outperforms Cambrain-8B, which utilizes multiple vision encoders and combines their image features, whereas we achieve superior results using only single vision encoder. 6. Discussion Results using LLaVA 1.5 Data. Since we curate our training data when building our MLLMs, we disentan6 ) ( n M 69.4 68.7 64.8 66.1 71. 71.1 ) ( e 60. 61.7 57.6 55.8 65.5 65.8 G 61.4 62.7 62.0 62.7 62.8 63. W 38.4 42.6 50. 54.5 49.3 54.0 P 86.2 89. 85.9 89.4 84.8 88.4 - M 35.4 35.4 30.6 35.2 34.2 36. - 1399.5 1448.5 1510.7 1543.5 1539. 1584.1 - 284.6 299.6 294.0 316. 292.5 346.8 e i l 44.5 45.5 44. 46.8 45.7 46.8 e - L 68.0 64. 64.2 65.0 71.0 66.2 S 40.6 40.8 30.3 36.8 38.5 39. (a) Results on general multimodal benchmarks. l l 54.4 58.4 54.8 60. 55.7 59.9 M 2.0 6.0 6. 12.3 7.3 8.3 2 63.3 64. 54.8 57.2 60.2 62.4 i a 30.6 30.6 26.7 28.0 29.3 31. M 40.7 39.6 35.3 35.6 39. 39.9 - c 72.0 68. 66.8 66.5 76.5 73.6 t 43. 61.6 58.2 62.8 45.4 68.0 e O 30.4 40.3 31.4 41.4 34.6 41. t C 16.4 21.8 18.2 24. 15.4 23.4 c 28.1 46.1 28. 44.5 28.6 44.4 o 26.4 29. 25.8 29.4 26.4 29.0 LLaVA 1.5 3B Florence-VL 3B LLM Phi 3.5 Phi 3.5 LLaVA 1.5 7B Vicuna 1.5 Florence-VL 7B Vicuna 1.5 LLaVA 1.5 8B Florence-VL 8B Llama 3 Llama 3 LLaVA 1.5 3B Florence-VL 3B LLM Phi 3.5 Phi 3.5 LLaVA 1.5 7B Vicuna 1. Florence-VL 7B Vicuna 1.5 LLaVA 1.5 8B Florence-VL 8B Llama 3 Llama (b) Results on Vision centric, Knowledge based, and OCR & Chart benchmarks. Table 3. We compare LLaVA 1.5 with our model (Florence-VL 3B/7B/8B) across multiple multimodal benchmarks. The key difference between them lies in the vision encoders used (CLIP for LLaVA vs. Florence-2 for our model), while we maintain the same training data and backbone LLMs for both. The results show that our models significantly outperform LLaVA 1.5 with the same training data. gle the effects of training data and model architecture to clearly demonstrate our methods effectiveness. Specifically, to highlight the advantages of our model architecture, we use the exact same pretraining and instruction dataset as LLaVA 1.5 [26]. We test different language backbones, including Phi-3.5-mini-Instruct, Vicuna 1.5 7B, and LLama-3-8B-Instruct. As shown in Tables 3, our model design significantly outperforms the LLaVA architectures when trained on the same dataset. Notably, for OCR & Chart tasks, Florence-VL significantly outperforms LLaVA 1.5, demonstrating that OCR image features are essential for effective text-based image understanding. , t2 , t2 Study on Depth Features Impacts. We aim to examine the impact of image features from different depths. For the , feature set [V, ], we first remove all highert3 t1 level features [V , ] and retain only the lowert3 t1 level feature [V]. We then evaluate the performance across different benchmarks, and as shown in Table 4, using only the lower-level feature [V] performs worse than our complete method. Next, we remove the lower-level feature [V] and keep only the higher-level features [V ]. t1 , t , t2 The alignment loss, displayed in Figure 5, clearly indicates that excluding the lower-level features (i.e., removing DaViT features) results in higher alignment loss compared to our method. Therefore, both ablation studies confirm that features from different depths are essential for achieving optimal performance. , t3 , t2 Study on Breadth Features Impacts. In Table 5, we analyze the impact of each feature from different breadths by individually removing one feature at time from [V ]. For instance, to assess the effect of the t1 caption feature, we retain only the OCR and grounding features. The results in Table 5 show that combining all three features yields the best average benchmark performance. Additionally, we plot the alignment loss when each feature is removed individually, as shown in Figure 5. This further demonstrates incorporating all three features from different breadths is essential for effectively extracting visual information. ) ( e Features used [V] , t2 , t ] [V, t1 64.3 66.1 e i l c - L e i - S - M - 31. 35.2 1510.7 1543.5 66.0 70.3 44. 46.8 64.2 65.0 P 86.1 89. s t M c R Q h A o V I 26.7 28.0 35.2 35. 31.2 41.4 18.3 24.3 27.9 44. 25.7 29.4 2 54.7 57.2 Table 4. The comparison between keeping only the lower-level feature [V] and our method, which includes both lowerand higher-level features, clearly demonstrates that maintaining both types of features achieves better performance. ) ( e 66.1 64.9 65. 66.6 ) ( e 55. 56.1 55.4 56.8 62.7 62. 62.0 63.0 z 54. 53.5 56.0 56.5 P 89.4 89. 88.8 88.8 - M 35.2 31. 30.2 32.9 - 1543.5 1477.8 1506. 1494.8 - 316.4 354.3 345.4 338. n o l 46.8 44.9 45.4 44.7 e - L 65.0 65.2 62.6 65.1 m - S 70.3 69.0 67.6 70.8 S 36.8 36.0 35.2 36.2 r 58. 57.6 57.3 58.2 Florence-VL 7B Remove Caption Feature t1 Remove OCR Feature t2 Remove Grounding Feature t3 Table 5. Ablation study was conducted by removing one high level image feature at time, demonstrating that all high-level features are essential for maintaining optimal performance. 7. Related Work LLMs have significantly advanced the development of MLLMs, including models like LLaVA [26], MiniGPT-4 [49], Qwen-VL [2], and Vila [24]. Most of these models integrate language-supervised vision encoder, such as CLIP or SigLIP, with language model backbone. Beyond these, there is wider range of visual models available, including self-supervised models [34], segmentation models [20], and diffusion models [37]. Departing from conventional vision encoder designs, our work introduces an innovative approach by using the generative vision foundation model Florence-2 as the vision encoder. While other studies, such as Cambrian [41], Brave [18] and MouSi [10] have explored the advantages of combining multiple visual signals, our approach avoids the added complexity and cost of using multiple vision encoders. Instead, we use single vision model to generate multiple visual features, which each one emphasizing different perceptual information in the input image. This approach allows us to achieve superior performance with single vision encoder, surpassing models that rely on multiple vision encoders, such as Cambrian [41]. High-resolution adaptation is commonly applied to increase the input resolution for MLLMs [25]. Besides, models like LLaVA-NeXT [25] and InternVL [7] achieve this by using tiling or adaptive tiling, dividing high-resolution inputs into smaller patches for separate processing. Although our method does not incorporate these techniques, both approaches are compatible and could be combined with our method. 8. Conclusion In conclusion, Florence-VL uses Florence-2 as versatile vision encoder, which provides diverse, task-specific visual representations across multiple computer vision tasks like captioning, OCR, and Grounding. By leveraging DepthBreadth Fusion (DBFusion), we incorporate range of visual features from different layers (Depth) and prompts (Breadth) to create enriched representations that meet varied perceptual demands of downstream tasks. Our fusion strategy, based on channel concatenation, effectively combines these diverse features, which are then projected as input to the language model. Through training on novel data recipe that includes detailed captions for pretraining and diverse instruction tuning data, Florence-VL demonstrates superior alignment between the vision encoder and the LLM, outperforming other models across 25 benchmarks covering vision-centric, knowledge-based, and OCR & Chart tasks. Our analysis underscores the effectiveness of Florence-2s generative capabilities in enhancing MLLM alignment and versatility for wide range of applications. For future work, several avenues could further enhance the capabilities and efficiency of Florence-VL. One direction involves improving the DBFusion strategy by exploring more sophisticated fusion techniques that could dy8 namically adapt the Depth-Breadth balance based on specific downstream task requirements. Additionally, while Florence-2 provides diverse visual representations, future research could explore adaptive vision encoders that select features on-the-fly, optimizing computational efficiency without compromising performance. 9 Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion"
        },
        {
            "title": "Supplementary Material",
            "content": "9. Training Details We selected two language backbones: Phi-3.5-mini-Instruct 1 and LLama-3.1-8B-Instruct 2. For the main results, using the 16.9M image caption dataset and 10M instruction datasets, we trained all models on 8 nodes with 64 Nvidia H100 GPUs. The training process consists of two stages: pretraining and instruction tuning. During the pretraining stage, unlike LLaVA 1.5 which only tunes the projection layer, we fine-tune the entire model, including the vision backbone Florence-2, projection layer, and language model. We found that tuning the entire model yields better performance than freezing the vision and language models. In the fine-tuning stage, we tune only the projection layer and language models. For LLama-3.1-8B-Instrcut, the global batch size for pretraning stage is 256, with cosine decay learning rate with maximun value 2e-5. In the fine-tuning stage, we maintain global batch size of 256 and learning rate of 1e-5. For Phi-3.5-mini-Instruct, the global batch size for pretraning stage is 4096, with cosine decay learning rate with maximun value 1e-4. In the fine-tuning stage, the global batch size is 2048 and learning rate is 9e-5. 10. Discussion OCR feature is essential for text based image understanding. In Table 6a, we examine the role of OCR in understanding images containing text. To evaluate the effect of the OCR feature, we retain only the caption and grounding features. The results in Table 6a indicate that, apart from TextVQA benchmark, the OCR feature is beneficial for extracting textual information from images in the other benchmarks. Knowledge based benchmark reply more on the capability of language model. In Table 6b we removing the caption and grounding features does not result in significant difference, suggesting that the knowledge-based benchmark scarcely relies on various visual information. Additionally, Table 2 shows that the performance of the knowledgebased benchmark improves with the use of stronger language models."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, 1https://huggingface.co/microsoft/Phi-3.5-mini-instruct 2https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 6 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 8 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 5 [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 5 [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions, 2023. 5 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 6 [7] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 8 [8] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. [9] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention vision transformers. In European conference on computer vision, pages 74 92. Springer, 2022. 2 [10] Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song, Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, et al. Mousi: Poly-visual-expert vision-language models. arXiv preprint arXiv:2401.17221, 2024. 8 [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 5, 6 [12] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. 1 e O t C c Q n a A Florence-VL 7B OCR 41.4 40.9 24.3 22.9 44. 44.4 29.4 29.0 34.9 34.2 i a 28.0 27.5 27.0 27.9 M 35.6 36.9 35.8 36.9 2 57. 56.8 55.7 56.7 - c 66.5 65.5 65.6 66.4 r 46. 46.7 46.0 47.0 Florence-VL 7B Caption OCR Grounding (a) Ablation study on OCR features on OCR & Chart benchmark. (b) Ablation Studies on Knowledge based benchmarks. Table 6. Ablation studies on different features for various benchmarks. In search of the next generation of multiDatacomp: modal datasets. Advances in Neural Information Processing Systems, 36, 2024. 5 [13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [14] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. 5 [15] Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Deciphering cross-modal alignment in large vision-language models with modality integration rate. arXiv preprint arXiv:2410.07167, 2024. 4 [16] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 5 [17] HuggingFaceM4/Docmatix. https://huggingface.co/datasets/huggingfacem4/docmatix. https : / / huggingface . co / datasets / HuggingFaceM4/Docmatix, 2024. 5 [18] Oguzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave: Broadening the visual encoding of vision-language models. arXiv preprint arXiv:2404.07204, 2024. [19] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235 251. Springer, 2016. 6 [20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment In Proceedings of the IEEE/CVF International anything. Conference on Computer Vision, pages 40154026, 2023. 1, 8 [21] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 6 [22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 5 [23] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [24] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for In Proceedings of the IEEE/CVF visual language models. Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 6, 8 [25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6, 8 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2, 4, 6, 7, 8 [27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 5 [28] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models, 2024. 6 [29] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 6 [30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 6 visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 6 [43] Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Large language model evaluation via matrix entropy. arXiv preprint arXiv:2401.17139, 2024. 4 [44] x.ai. Grok 1.5v: The next generation of ai. https://x. ai/blog/grok-1.5v, 2023. Accessed: 2024-07-26. 6 [45] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4818 4829, 2024. 1, 2 [46] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning, 2024. [47] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 5 [48] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 6 [49] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 8 [31] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 6 [32] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 6 [33] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 6 [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 5, 8 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 5 [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 8 [38] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1 [39] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 6 [40] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions, 2024. 5 [41] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 1, 2, 5, 6, [42] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "University of Maryland"
    ]
}