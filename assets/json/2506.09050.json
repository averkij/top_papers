{
    "paper_title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering",
    "authors": [
        "Yuki Imajuku",
        "Kohki Horie",
        "Yoichi Iwata",
        "Kensho Aoki",
        "Naohiro Takahashi",
        "Takuya Akiba"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, a new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, a notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 0 5 0 9 0 . 6 0 5 2 : r ALE-Bench: Benchmark for Long-Horizon Objective-Driven Algorithm Engineering Yuki Imajuku1, Kohki Horie1,2, Yoichi Iwata3, Kensho Aoki3, Naohiro Takahashi3, Takuya Akiba1 2The University of Tokyo, Japan {imajuku, takiba}@sakana.ai 3AtCoder, Japan 1Sakana AI, Japan"
        },
        {
            "title": "Abstract",
            "content": "How well do AI systems perform in algorithm engineering for hard optimization problems in domains such as package-delivery routing, crew scheduling, factory production planning, and power-grid balancing? We introduce ALE-Bench, new benchmark for evaluating AI systems on score-based algorithmic programming contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench presents optimization problems that are computationally hard and admit no known exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons. Our software framework supports interactive agent architectures that leverage test-run feedback and visualizations. Our evaluation of frontier LLMs revealed that while they demonstrate high performance on specific problems, notable gap remains compared to humans in terms of consistency across problems and long-horizon problem-solving capabilities. This highlights the need for this benchmark to foster future AI advancements. Code Data github.com/SakanaAI/ALE-Bench hf.co/datasets/SakanaAI/ALE-Bench"
        },
        {
            "title": "Introduction",
            "content": "The progress of AI is breathtaking. Benchmarks that were in the spotlight only few years ago often saturate in performance and quickly lose relevance. To keep advancing AI, we continually need fresh benchmarks that can measure improvements across many facets. In particular, complex end-to-end tasks with long time horizons are expected to form the next frontier of benchmarking [1]. Competitive coding has been one of the most prominent domains for strengthening and evaluating LLMs. Benchmarks such as APPS [2], CodeContests [3], and LiveCodeBench [4] have played pivotal role. The domain is naturally suited to benchmarking because solutions can be judged automatically and objectively by running code. However, LLM performance in these benchmarks has risen steeply, already rivaling advanced human contestants and showing signs of saturation [5]. Competitive coding falls into two broad categories. The first is the short-duration, exact-solution contests, such as the International Olympiad in Informatics (IOI), Codeforces, and the AtCoder Regular Contest (ARC). The problems have intended solutions, and submissions are graded strictly as correct or incorrect. Most previous benchmarks in the AI community have focused exclusively on this category. The second is the long-duration, score-based contests, exemplified by the AtCoder Heuristic Contests (AHC) and Topcoder Marathon Matches. Here, the tasks are optimization problems whose true optima are computationally out of reach (e.g., because the underlying problems are NPhard), and participants spend weeks iteratively refining their programs to push their scores higher. See Figures 1 and 2 and Section 3.1 for details. Preprint. Figure 1: Overview of ALE-Bench. (Left) ALE-Bench collects past AtCoder Heuristic Contest tasks, hard optimization problems such as routing and scheduling with no known optimum, and ranks submitted programs by score. (Right) ALE-Bench covers evaluation from bare LLMs to scaffolded agents. An agent receives task and submits code. It can optionally invoke test runs and visualization utilities during this process to iteratively refine its solution like human participant. This paper introduces ALE-Bench (ALgorithm Engineering Benchmark), the first benchmark that measures AI performance in these algorithmic score-based programming contests. We collect past tasks from the AtCoder Heuristic Contest (AHC), one of the worlds largest score-based competitions, and provide software framework for evaluating AI systems on them (Figure 1). This benchmark quantifies the practical impact of AI systems on algorithm engineering for complex optimization problems within industrial domains, including package-delivery routing, crew scheduling, factory production planning, and power-grid balancing. The tasks in AHC effectively mirror real-world optimization challenges. Each AHC sees thousands of skilled human contestants, including professional experts, invest substantial effort over weeks. This rigorous process allows for performance comparisons against strong human experts. Furthermore, the scoring protocols and evaluation environment in ALE-Bench are standardized to closely replicate those of the actual competitions, thereby enabling direct and equitable comparisons with humans. ALE-Bench is co-developed with AtCoder, which guarantees that all data are fully licensed and that the evaluation procedure faithfully reproduces the original contest conditions. Beyond these direct applications, the benchmarks second role is to probe the advanced reasoning ability of frontier AI. Just as previous competitive programming benchmarks have served as leading indicators of LLM reasoning capabilities, ALE-Bench, more challenging competitive programming benchmark, can fulfill similar function. ALE-Bench is challenging and intriguing because it necessitates longer-horizon reasoning capabilities compared to previous coding benchmarks. Human contestants continuously think and accumulate insights through trial and error over weeks, progressively improving their scores (see Figure 3). key question is whether AI can demonstrate similar long-horizon reasoning and continuous solution improvement. This benchmark is open-ended, in the sense that true optima are out of reach and scores can keep rising. This allows for meaningful evaluation even after AI systems surpass the performance of the strongest human experts. We evaluated the performance of frontier LLMs in one-shot setting and with long iterative refinement using scaffoldings, including ALE-Agent, which we specifically designed for ALE-Bench. Although these models achieved exceptionally high performance on certain problems, the distribution of their contest-wise performance reveals significant gap compared to true experts consistency across problem types and of long-term improvement, indicating need for further AI progress in this field, which ALE-Bench helps to cultivate. This paper makes two key contributions: 1 We propose ALE-Bench for evaluating AI on longhorizon, score-based optimization tasks through interactive, iterative problem solving. 2 We analyze current AIs reasoning and algorithm engineering for these complex problems using ALE-Bench. 2 Figure 2: Example problem from ALE-Bench (ahc006). See Figure A1 for the full version. Write program that, given large collection of pickup-delivery pairs on 2D grid, chooses prescribed number of requests and outputs depot-to-depot tour that visits the pickup location of each selected request before its corresponding drop-off. The score is the total length of the route; the shorter, the better. (CPU time limit: 2 seconds per input)"
        },
        {
            "title": "2 Related Work",
            "content": "Early code-generation benchmarks were dominated by unit-test-based suites such as HumanEval [6] and MBPP [7], which contain small, self-contained programming tasks. As LLMs began to exhibit stronger reasoning abilities, researchers shifted toward harder algorithmic problems drawn from competitive programming platforms, e.g., the International Olympiad in Informatics (IOI), Codeforces, and the AtCoder Regular Contests (ARC). This trend gave rise to benchmarks like APPS [2], CodeContests [3], USACO-Bench [8], and LiveCodeBench [4]. On these binary-accuracy benchmarks, frontier-level LLMs already approach the performance of top human contestants. Unlike those benchmarks, ALE-Bench tackles score-based tasks from the AtCoder Heuristic Contest (AHC) that have no single ground-truth answer. Longer-horizon programming benchmarks that require more than producing short code snippet include SWE-Bench [9], MLE-Bench [10], and TheAgentCompany [11]. While SWE-Bench and TheAgentCompany remain pass/fail evaluations, both MLE-Bench and ALE-Bench allow for scorebased observation of continuous improvement. MLE-Bench is drawn from Kaggle competitions and therefore assesses data-centric machine learning skills, thus operating in domain different from ALE-Bench. Furthermore, MLE-Bench requires GPU environment and its evaluation is costly, whereas ALE-Bench uses only CPU environment and is resource-friendly. line of research tackles combinatorial optimization problems with machine learning approaches such as graph neural networks and reinforcement learning [12, 13]. Those studies train specialized models for single, pre-defined task, where the model receives an instance as input and directly outputs solution. Our setting is fundamentally different because the tasks are not pre-defined, described in natural language, and solved by writing code. To the best of our knowledge, the only prior work that has studied problems from the score-based contest is FunSearch [14], which explores using LLMs to refine portions of human-written template."
        },
        {
            "title": "3 ALE-Bench",
            "content": "3.1 Dataset Construction ALE-Bench is built upon the AtCoder Heuristic Contest (AHC), prominent and one of the largest score-based algorithmic competitions organized by AtCoder Inc. It is held approximately 10 to 18 times annually, with 49 rated contests conducted as of May 1, 2025. Each contest typically attracts around 1,000 participants, and over 6,000 users have been active within the past two years. novel problem is introduced at the start of each contest. The problem domains in AHC are diverse, encompassing areas such as routing, planning, multi-agent control, puzzle-solving, and Bayesian inference. As an illustrative example, Figure 2 describes routing problem asked in the contest. The tasks are typically CPU-bound, with execution time limits ranging from 2 to 10 seconds per case. AHC offers two contest formats: short (approximately 4 hours) and long (12 weeks). The problem characteristics and difficulty significantly differ between these formats. Short contests sometimes involve problems solvable with relatively standard algorithmic approaches like simulated annealing or beam search. In contrast, long contests often present problems where success hinges on deeper, iterative process. However, in both formats, achieving high scores requires problem-specific reasoning and refinement through repeated testing. To support this iterative process, visualizers are provided in the contest. As the contest progresses, participants can keep submitting increasingly better solutions. 3 Figure 3: Long-horizon score ascent in AHC. Scores at specific ranks at each time point over the two-week AHC014 contest show continual improvement. Line colors mark the color tiers, e.g., perf=2800 (6th) and perf=1200 (379th). Figure 4: Rating and average performance distributions. Cumulative rating and average performance distribution for users with at least 5 participations as of May 1, 2025. Background colors indicate rating tiers. Figure 3 illustrates such score progression during the contest. Moreover, participants often surpass the best in-contest score by continuing to improve their solutions in an official post-contest. To construct ALE-Bench, we released dataset of 40 AHC problems on the Hugging Face platform. These problems are sourced from contests conducted up to the end of April 2025. We call the entire set of problems the full version, while we also prepared the lite (subset) version that contains 10 representative problems. Each problem package contains four elements. (1) Problem: Markdown statement together with any images. (2) Scorer: Rust program evaluating the code on input cases. (3) Visualizer: web-based tool and Rust program that displays the behavior of the code on the inputs. The image used in Figure 2 is the example visualization. (4) Leaderboard: ranking data used for calculating performance metrics. All data are officially provided by AtCoder Inc., ensuring clear licensing and safe use. Additional details are given in Appendix A.1. 3.2 Benchmark Implementation ALE-Bench translates AHC problems into benchmark tailored for AI systems. We deliver this benchmark as Python library that leverages the dataset described in Section 3.1. Its core objective is to allow AI systems to fluidly simulate participation in AHC, closely emulating the human contestant experience. To this end, ALE-Bench provides an interface for actions available during contest, complemented by code sandbox mirroring the execution environment on AtCoder. This section elaborates on the implementation details. More specific information can be found in Appendix A.2. Understanding ALE-Benchs design necessitates brief overview of the typical AHC participant workflow. Typically, an AHC participant selects programming language from numerous options, develops their solution, and submits it to the online system. This submission is then run in AtCoders environment, providing instant feedback on hidden test set of 50 to 300 cases. Final rankings are determined by private evaluation conducted post-contest. The nature of the test cases for this evaluation can vary: distinct and larger set of hidden inputs for long contests, while the in-contest leaderboard, based on the small test set, serves as the final result for short contests. Additionally, AHC offers participants an input case generator, facilitating local evaluation on either self-generated or provided inputs. This local case evaluation is referred to as public evaluation. ALE-Bench is designed to replicate this AHC environment for AI systems. An AIs evaluation on single problem is orchestrated by Session object. Its creation triggers real-time timer, simulating the contests duration. Within this timed session, as depicted in Figure 1, the AI can undertake several actions via the Session : (1) View Problem: Access and review the problem statement. (2) Test Run: Execute its solution code within the sandbox to get scores from the scorer (a public evaluation). The AI can also generate and test new input cases and obtain scores from the scorer for these cases. (3) Visualization: Visualize its codes behavior on specific inputs. (4) Submission: Submit its final solution, which initiates the private evaluation and the calculation of performance metrics (detailed in Section 3.3). The AI can iterate through actions (1)-(3) as needed within the time limit. The session concludes upon either the submission of final solution (4) or the expiration of the timer. Post-session, the Session object provides the performance metrics from the private evaluation. 4 key component is the code sandbox, which emulates the AHC execution environment. It currently supports C++, Python, and Rust, which are the most popular choices among AHC participants, and is designed for straightforward extension to other languages. The visualization tool further aids development and debugging, offering two modes: (a) Static image generation, optionally producible for each test case during test run. (b) Interactive web-based visualization, facilitated by local HTTP server managed by Session . This enables more in-depth, interactive visual analysis via web browser, akin to human contestant practices. Reproducibility and fair comparison are critical. Given that high-performing AHC solutions are often CPU-intensive and their results can be hardware-dependent, ALE-Bench includes scripts to establish standardized evaluation environment. These scripts configure Amazon EC2 C6i Instances [15] to mirror the original AtCoder CPU environment. This standardization ensures reproducible results and allows for equitable comparisons among different AI systems and against human performance. 3.3 Evaluation Metrics To comprehensively evaluate the capabilities of AI systems, we employ (i) fine-grained metrics on individual problems, and (ii) aggregated metrics for overall proficiency. These two metrics are primarily derived from the evaluation system used for human contestants in AHC. Our framework facilitates the computation of them, thereby enabling fair and consistent comparisons among AI systems and against human participants. We also provide detailed description in Appendix A.3. Fine-grained Metrics per Problem. For each problem, we record (1) the problem-specific score, (2) the rank, and (3) the performance. Performance is score derived from set of participants and the rank within problem, using an Elo-rating-like method [16]. This score typically ranges from 0 to 3500 and higher is better. Among these three metrics, the performance is particularly useful as it provides problem-agnostic scale for comparing participants relative standings. Aggregated Metrics across Problems. To provide holistic view of an AI systems abilities over set of problems, we utilize two primary aggregated metrics: (1) the average performance and (2) the rating [17]. The average performance is the simple arithmetic mean of the performances, and its cumulative distribution is shown as the dotted line in Figure 4. On the other hand, the rating is an indicator extensively used on AtCoder to compile leaderboards for human contestants. The rating reflects an individuals overall skill, derived from their performances in series of contests they have participated in. The line in Figure 4 illustrates the distribution of ratings among active AtCoder users as of May 2025. Since ratings can be significantly lower than users actual skill level when the number of participations is very small, we focus on users with at least 5 participations, which is the number officially recommended by AtCoder to ensure rating accuracy. For evaluating AI systems, we strongly recommend using average performance as defined above. The rating design philosophy primarily targets human participants, aiming to: (a) discourage selective submission of solutions only when high ranks are anticipated, and (b) encourage the pursuit of high-risk, high-reward strategies. Despite its utility for human comparable rankings, the rating is less appropriate for evaluating AI systems for two reasons. First, single exceptionally high performance can disproportionately inflate an AIs rating, potentially leading to an overestimation of its general capabilities. Second, our evaluation protocol involves comparing AI systems with fixed set of problems. In this context, the rating offers little additional insight beyond the average performance. In addition to the average performance, examining the performance distribution can also be highly informative. It can reveal whether an AI systems strength lies in excelling at few specific types of problems or if it demonstrates consistent and broad improvements across the entire benchmark suite."
        },
        {
            "title": "4 ALE-Agent",
            "content": "How much headroom is there for agent-based scaffolding in algorithm engineering? To gain an initial glimpse of the research space opened up by ALE-Bench, we conduct an exploration of special-purpose agents designed for algorithm engineering. This domain has few distinctive characteristics. For many problem categories, canonical high-level approaches are already known, and choosing the right overall strategy matters enormously. However, even with the right idea, implementation details, hyperparameters, and micro-optimizations can 5 Table 1: Comparison of frontier LLMs in the one-shot setting. Average Perf. details the average performance on short-/longformat problems and overall problems, respectively. Perf. Distribution (%) indicates the percentage of problems for which the performance of 400, 1600, and 2000 were achieved. Rating shows the raw value and its corresponding percentile rank from the top. Cost ($) lists the incurred USD per problem and per response, respectively. See Table A3 for results of more models and Table A4 for human statistics by levels. Model Average Perf. Perf. Distribution (%) Rating Cost ($) short long overall 400 1600 raw rank (%) /problem /response Non-Reasoning Models: GPT-4o GPT-4.1 mini GPT-4.1 Gemini 2.0 Flash Claude 3.7 Sonnet DeepSeek-V3 Reasoning Models: o3-high o4-mini-high Gemini 2.5 Flash Gemini 2.5 Pro Claude 3.7 Sonnet (Thinking) DeepSeek-R1 547 779 696 547 851 638 1116 866 905 938 911 636 755 746 585 810 688 946 808 827 688 792 822 585 769 717 563 833 659 1044 841 872 832 860 760 Human Average 1257 1260 75.0 90.0 80.0 62.5 90.0 75.0 97.5 92.5 82.5 82.5 90.0 75.0 95.7 0.0 0.0 2.5 2.5 0.0 0. 5.0 0.0 5.0 5.0 2.5 0.0 23.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 8.5 936 1135 1164 1031 1197 1456 1194 1422 1373 1328 1206 1414 80.1 0.048 0.022 67.4 0.009 0.005 65.1 0.083 0.035 74.6 0.006 0.002 63.2 0.287 0.142 66.8 0.008 0.003 43.2 0.734 0.506 63.6 0.041 0.037 45.5 0.194 0.104 49.3 0.472 0.242 52.3 0.375 0.170 62.3 0.063 0.028 - - - Table 2: Comparison of code languages in the one-shot setting. For each language, the table reports the mean value over the 22 LLMs used in the one-shot setting experiment. Code Language Perf. Distribution (%) Average Perf. Cost ($) Rating C++20 Python3 Rust short 664 610 632 long 672 643 583 overall 400 1600 2000 raw rank (%) /problem /response 668 624 611 75.6 70.9 69.4 1.0 0.1 1.1 0.0 0.0 0.0 1072 1024 1027 70.7 74.1 73. 0.144 0.161 0.168 0.086 0.075 0.082 dramatically affect the result. Considering these points, we propose and implement two techniques in an ALE-Agent prototype. See Appendix for details. These techniques are evaluated in Section 5.3. Method 1: Prompting with domain knowledge. We inject expert knowledge about standard techniques in algorithm engineering, directly into the prompts, such as simulated annealing and beam search. The prompt explicitly discusses search space and evaluation function design, neighborhood generation, and common acceleration tricks. Method 2: Diversity-oriented solution search. We employ best-first-search-based algorithm to generate and refine answer candidates using an LLM. To avoid prematurely discarding promising solution paths, we augment best-first search with beam-search-like expansion that spawns multiple children from each node at once. This breadth helps retain high-potential hypotheses and, in practice, amortizes API latency by parallelizing candidate generation, significant benefit, particularly when working with large reasoning models."
        },
        {
            "title": "5 Experiments",
            "content": "Experiments were conducted on Amazon EC2 C6i Instances [15], mirroring the AtCoder CPU environment. We evaluated up to 22 models from OpenAI [1823], Google [2428], Anthropic [29, 30], and DeepSeek [31, 32]. Our primary experimental setup relied solely on text-based feedback from test runs and did not utilize the visualization, except for the OpenHands [33]. Sections 5.1 and 5.2 list the experimental results for the full set, while Section 5.3 lists the results for the lite subset. Further details are in Appendix C.1. 6 Table 3: Comparison of frontier LLMs in the iterative-refinement setting. The four models listed in the table were tested. Average Perf. details the average performance on short-/longformat problems and overall problems, respectively. For the overall average, its corresponding percentile rank from the top is reported. Perf. Distribution (%) indicates the percentage of problems for which the performance of 400, 1600, 2000, and 2400 were achieved. Rating shows the raw value and its corresponding percentile rank from the top. Cost ($) lists the incurred USD per problem and per response, respectively. Model Average Perf. Perf. Distribution (%) Rating Cost ($) short long overall rank (%) 400 1600 2000 raw rank (%) /problem /response GPT-4.1 mini o4-mini-high Gemini 2.5 Pro DeepSeek-R1 1293 1677 1389 1268 1114 1307 1301 1155 1217 1520 1352 51.5 100.0 22.3 100.0 95.0 36.8 97.5 51.1 17.5 32.5 27.5 15.0 2.5 15.0 7.5 5.0 0.0 5.0 5.0 2.5 1636 2104 1960 1891 30.5 2.137 0.010 11.8 7.174 0.047 15.7 11.126 0.134 18.3 1.141 0. Figure 5: Trends in public score and code file size in the iterative-refinement setting. The plot shows the progression of generated code file sizes alongside the corresponding public evaluation scores over four-hour period. Points farther to the right represent the later time points. 5.1 One-Shot Setting Setup. This experiment assessed the one-shot capability of stand-alone LLMs. Before the private evaluation, public evaluation was performed. If scoring failed due to errors (e.g., compilation, formatting), feedback was provided, allowing up to five code generations. Experiments were conducted in C++20, Python3, and Rust to assess language impact. Further details appear in Appendix C.2. Results. Table 1 shows C++20 results for selected models, and Table 2 compares languages. At the model level, o3-high was the only model to surpass the average performance of 1000. Reasoning models generally outperformed non-reasoning ones. Among non-reasoning models, Claude 3.7 Sonnet performed best. The performance distribution confirmed o3-highs lead, achieving 400 performance on all but one problem. However, even top-performing models exceeded 1600 performance on at most 5% of problems and never reached 2000, highlighting the difficulty of ALE-Bench. Ratings did not always align with average performance, e.g., for o4-mini-high vs. Gemini 2.5 Pro. Cost-wise, the reasoning model o4-mini-high was cost-effective, while the non-reasoning Claude 3.7 Sonnet was relatively expensive. In the language comparison, C++20 achieved the highest average performance, rating, and the greatest number of problems with 400 performance. However, Rust slightly exceeded C++20 in problems with 1600 performance. Python3 and Rust showed similar trends, despite notable difference in problems with 1600 performance. Regarding cost, C++20 was the cheapest per problem but the most expensive per response. This suggests that while C++20 may generate effective solutions more quickly, it does so with higher token consumption per response. 5.2 Iterative-Refinement Setting Setup. This experiment evaluated the iterative-refinement capability of each LLM. Four models iteratively refined their C++20 solutions for four hours, receiving public evaluation feedback after each attempt, process similar to Self-Refine [34]. More information is given in Appendix C.3. Results. Table 3 shows that o4-mini-high performed best, achieving an average performance of 1520 and rating of 2104 (top 11.8% of humans). Performance distributions reveal GPT-4.1 mini and o4mini-high achieved 400 performance in all problems, and all models surpassed 2000 performance 7 Table 4: Comparison of scaffolding on the lite subset. Column definitions follow those in Table 3. Scaffolding Average Perf. Perf. Distribution (%) Rating Cost ($) short long overall rank (%) 400 1600 2000 2400 raw rank (%) /problem /response Sequential Refinement (Section 5.2) : 1016 GPT-4.1 mini 1411 o4-mini-high 1198 Gemini 2.5 Pro 1021 1373 1237 1012 1449 73.4 100.0 31.2 100.0 54.1 100.0 0.0 10.0 0.0 OpenHands [33]: GPT-4.1 mini o4-mini-high Gemini 2.5 Pro 600 874 726 635 845 1080 618 859 96.9 86.7 82.8 80.0 90.0 80.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 ALE-Agent w/ Gemini 2.5 Pro (Section 4) : Base + Method 1 + Method 1&2 1213 1079 1474 1167 1264 1879 1121 1448 2285 56.9 100.0 46.7 100.0 6.8 100. 10.0 20.0 70.0 0.0 10.0 30.0 0.0 0.0 20.0 990 1386 1195 650 894 1038 1219 1494 2.12 0.008 77.4 48.2 7.22 0.047 63.5 11.10 0.157 94.2 83.2 74.0 0.15 0.004 2.26 0.050 3.25 0.134 61.2 7.64 0.114 40.2 11.12 0.135 8.6 100.33 0.113 on at least one problem. GPT-4.1 mini, the sole non-reasoning model, had fewer 2000 performances and lower average performance than DeepSeek-R1. Average performance improved by over 400 points across all models under the iterative-refinement setting, demonstrating its effectiveness. Score Progression. Figure 5 illustrates o4-mini-highs behavior on ahc041, showing its four-hour public score trajectory and generated code file size. Red points mark updates. The score steadily improved, with significant gains even mid-run. The code size also progressively increased, indicating an incremental implementation and refinement process based on feedback, similar to humans. 5.3 Scaffolding Evaluation Setup. Effective LLM-based coding relies on both the base model and the scaffolding providing implementation support. This experiment assessed the combined LLM-scaffolding performance. We evaluated OpenHands CodeActAgent [33] as representative example of general-purpose scaffolding. Its web browser interaction capability was tested with the ALE-Bench visualization web server. An ablation study of our ALE-Agent (Section 4) used Gemini 2.5 Pro, starting with base version and incrementally adding Method 1 (domain knowledge) and Method 2 (increased code generation). We conducted only this ablation study with the lite version of ALE-Bench. All runs were limited to 4 hours using C++20. We provide further details in Appendix C.4. Results. Table 4 summarizes results, including iterative-refinement outcomes from Section 5.2. We report the converted lite setting values from the full setting for Sequential Refinement and OpenHands in the table. The full result is shown in Table A5. The average performance of OpenHands showed marginal improvement, which was similar to one-shot results. Cost data suggest OpenHands often exited prematurely, indicating difficulties with improving. In contrast, the base ALE-Agent matched iterative-refinement results. Method 1 slightly improved, while Method 2 yielded substantial gains. 5.4 Analysis Comparison with Human Experts. For example, looking solely at ratings in Table 3, o4-mini-high is positioned in the top 11.8% of humans. Does o4-mini-high truly possess abilities comparable to top-11.8% human expert? Here, we discuss how ratings likely overestimate AIs skills and what areas have potential for future growth. Its performance distribution significantly differs from humans in this rating band (2050-2150). o4mini-high achieved 2400 performance in 5.0% of problems, while humans in this band averaged 4.0%. Conversely, o4-mini-high achieved 1600 performance in 32.5% of problems; this is below the 56.2% average for humans in the same band and is closer to the 33.9% average for users rated 16001700. See Table A4 for more detailed human performance distribution. This indicates LLMs significant disparity between strengths and weaknesses. This is further supported by the relatively low 22.3% rank percentile for the average performance. Rating is designed to reward high scores without penalizing low ones (see Section 3.3), thus it tends to overstate AI. To comprehensively assess AIs capability, average performance and performance distribution are more informative. 8 Figure 6: Investigation of contamination. For each model, scatter plot is shown with contest end dates on the x-axis and performance on the y-axis. The red vertical line indicates the knowledge cutoff date of the model. For DeepSeek-R1, its release date was used as official information is unavailable. What causes this gap? Performance trends, for instance, vary between shortand long-format contests. Tables 3 and 4 show AIs short contest performance is typically higher than in long contests. Unlike humans, who might explore only few approaches in short contest, LLMs can rapidly generate and test numerous approaches, potentially outperforming humans through sheer volume. For example, o4-mini-high in Table 3 generated more than 100 solution codes, and ALE-Agent in Table 4 generated approximately 1000. Trying such large number of solution codes is unrealistic for humans, even in long-format contests. Matching more sophisticated solutions humans devise in long contests remains greater challenge for AI. Furthermore, analysis of problem types indicates AI excels in contests suited to specific approaches, like simulated annealing. See Appendix C.3 for details. Contamination. To investigate potential training data contamination, we analyzed performance variations based on contest dates, as shown in Figure 6. Each models performances are plotted against its knowledge cutoff date (red line). No model exhibited notable performance changes around its cutoff, suggesting minimal contamination effects. Plagiarism. To address potential plagiarism concerns, an AHC organizer (one of the authors) manually reviewed all 12 AI-generated programs that achieved 2000 performance in the iterativerefinement setting. The organizer, familiar with leading solutions, found no instances where AIgenerated code showed stylistic or logical resemblance close enough to suggest plagiarism of human submissions. We thus conclude that performance overestimation due to plagiarism is unlikely."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced ALE-Bench, new benchmark that evaluates an AI systems ability to develop and refine algorithms, and we compared both one-shot and iterative-refinement performance across several state-of-the-art LLMs and agents. Our experiments show that todays leading AI systems can generally match the abilities of human novices to intermediates. At the same time, even the top models display considerable weaknesses in specific problem categories, indicating that significant work remains before they can consistently equal, or exceed, the performance of domain experts. We believe this benchmark will facilitate further research and development of LLMs and agents. Limitations. Potential discrepancies between the ALE-Bench implementation and actual contest participation are discussed in Appendix A.4. In addition, this benchmarks potential limitation is the dataset size. Only 49 contests have ever been held, and the benchmark covers 40 of them. Although this is relatively small for benchmark, each contest allows long-horizon trial and error. Because the fluctuations at individual steps are smoothed through accumulation, the variance in the outcome is low. Moreover, both organizers and the participant community trust the current ratings when evaluating humans, so we believe that 40 contests can still offer reasonable indication of AI performance. Ethical and Societal Impact. This research is expected to accelerate progress in AI and, in particular, drive improvements in industrial optimization, such as logistics and energy scheduling. The benchmark contains no personal or sensitive data, and the experiments involve no direct human subjects. Thus, we identify no ethical concerns. We recognize that releasing this work could influence the behavior of current AHC participants. However, the work is conducted in close collaboration with the AHC organizers, and this point has been thoroughly discussed. Covert participation in AHC by AI-development teams violates the competition rules, and we strongly discourage it. 9 Author Contributions. Yuki Imajuku co-designed and implemented ALE-Bench, conducted the experiments, and led the studies. Kohki Horie designed and implemented ALE-Agent. Yoichi Iwata, Kensho Aoki, and Naohiro Takahashi assisted with the design of ALE-Bench, data preparation, and interpretation of the experimental results. Takuya Akiba initiated and managed the overall project, co-designed ALE-Bench, and provided overall guidance and supervision. Acknowledgements. The authors would like to thank Yotaro Kubo, Masanori Suganuma, Yutaro Yamada, and Artem Zhivolkovskiy for their helpful feedback on an earlier draft of this article."
        },
        {
            "title": "References",
            "content": "[1] Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, and Lawrence Chan. Measuring ai ability to complete long tasks. arXiv preprint arXiv:2503.14499, 2025. [2] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with APPS. In NeurIPS Track on Datasets and Benchmarks, 2021. [3] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. Science, 378(6624):10921097, 2022. doi: 10.1126/science. abq1158. [4] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. In ICLR, 2025. [5] OpenAI. Introducing openai o3 and o4-mini. introducing-o3-and-o4-mini/, 2025. Retrieved: May 15, 2025. https://openai.com/index/ [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [8] Ben Shi, Michael Tang, Karthik R. Narasimhan, and Shunyu Yao. Can language models solve olympiad programming? In COLM, 2024. [9] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In ICLR, 2024. 10 [10] Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander adry. MLE-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. [11] Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig. TheAgentCompany: Benchmarking llm agents on consequential real world tasks. arXiv preprint arXiv:2412.14161, 2024. [12] Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veliˇckovic. Combinatorial optimization and reasoning with graph neural networks. Journal of Machine Learning Research, 24(130):161, 2023. [13] Victor-Alexandru Darvariu, Stephen Hailes, and Mirco Musolesi. Graph reinforcement learning for combinatorial optimization: survey and unifying perspective. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Survey Certification. [14] Petar Veliˇckovic, Alex Vitvitskyi, Larisa Markeeva, Borja Ibarz, Lars Buesing, Matej Balog, and Alexander Novikov. Amplifying human performance in combinatorial competitive programming. arXiv preprint arXiv:2411.19744, 2024. [15] Amazon Web Services, Inc. Amazon ec2 c6i instances. https://aws.amazon.com/ec2/ instance-types/c6i/, 2021. Retrieved: May 15, 2025. [16] AtCoder. Atcoder rating system ver. 1.00. zpgcogxmmu84rr8/AADcw6o7M9tJFDgtpqEQQ46Ua?dl=0&preview=rating.pdf, Retrieved: May 15, 2025. https://www.dropbox.com/sh/ 2019. [17] AtCoder. Ahc rating system (ver.2). https://img.atcoder.jp/file/AHC_rating_v2_ en.pdf, 2025. Retrieved: May 15, 2025. [18] OpenAI. Gpt-4o mini: advancing cost-efficient intelligence. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/, 2024. Retrieved: May 15, 2025. [19] OpenAI. Gpt-4o system card. https://cdn.openai.com/gpt-4o-system-card.pdf, 2024. Retrieved: May 15, 2025. [20] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. Retrieved: May 15, 2025. [21] OpenAI. Openai o1 system card. https://cdn.openai.com/o1-system-card-20241205. pdf, 2024. Retrieved: May 15, 2025. [22] OpenAI. Openai o3-mini system card. https://cdn.openai.com/ o3-mini-system-card-feb10.pdf, 2025. Retrieved: May 15, 2025. [23] OpenAI. Openai o3 and o4-mini system card. https://cdn.openai.com/pdf/ 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf, 2025. Retrieved: May 15, 2025. [24] Gemini Team, Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [25] Google DeepMind. Gemini 2.0 flash-lite model card. https://storage.googleapis.com/ model-cards/documents/gemini-2-flash-lite.pdf, 2025. Retrieved: May 15, 2025. [26] Google DeepMind. Gemini 2.0 flash model card. https://storage.googleapis.com/ model-cards/documents/gemini-2-flash.pdf, 2025. Retrieved: May 15, 2025. [27] Google DeepMind. Gemini 2.5 flash preview model card. https://storage.googleapis. com/model-cards/documents/gemini-2.5-flash-preview.pdf, 2025. Retrieved: May 15, 2025. 11 [28] Google DeepMind. Gemini 2.5 pro preview model card. https://storage.googleapis. com/model-cards/documents/gemini-2.5-pro-preview.pdf, 2025. Retrieved: May 15, 2025. [29] Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 https://assets.anthropic.com/m/1cd9d098ac3e6467/original/ sonnet. Claude-3-Model-Card-October-Addendum.pdf, 2024. Retrieved: May 15, 2025. [30] Anthropic. Claude 3.7 sonnet system card. https://assets.anthropic.com/ m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf, 2025. Retrieved: May 15, 2025. [31] DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [32] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [33] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An open platform for AI software developers as generalist agents. In ICLR, 2025. [34] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In NeurIPS, 2023. 12 Table A1: The list of problems in ALE-Bench. We offer 40 problems for the full version and 10 problems for the lite version. Problem ID Format Hours Lite Ver. Original Contest URL 200 4 200 6 4 240 4 4 343 4 4 199 4 340 4 216 200 367 4 4 3.5 4 199 4 216 4 240 240 4 240 4 4 240 4 240 4 4 4 240 4 https://atcoder.jp/contests/ahc001 https://atcoder.jp/contests/ahc002 https://atcoder.jp/contests/ahc003 https://atcoder.jp/contests/ahc004 https://atcoder.jp/contests/ahc005 https://atcoder.jp/contests/future-contest-2022-qual https://atcoder.jp/contests/ahc006 https://atcoder.jp/contests/ahc007 https://atcoder.jp/contests/ahc008 https://atcoder.jp/contests/ahc009 https://atcoder.jp/contests/ahc010 https://atcoder.jp/contests/ahc011 https://atcoder.jp/contests/ahc012 https://atcoder.jp/contests/ahc014 https://atcoder.jp/contests/ahc015 https://atcoder.jp/contests/ahc016 https://atcoder.jp/contests/ahc017 https://atcoder.jp/contests/ahc019 https://atcoder.jp/contests/ahc020 https://atcoder.jp/contests/ahc021 https://atcoder.jp/contests/toyota2023summer-final https://atcoder.jp/contests/ahc024 https://atcoder.jp/contests/ahc025 https://atcoder.jp/contests/ahc026 https://atcoder.jp/contests/ahc027 https://atcoder.jp/contests/ahc028 https://atcoder.jp/contests/ahc030 https://atcoder.jp/contests/ahc031 https://atcoder.jp/contests/ahc032 https://atcoder.jp/contests/ahc033 https://atcoder.jp/contests/ahc034 https://atcoder.jp/contests/ahc035 https://atcoder.jp/contests/ahc038 https://atcoder.jp/contests/ahc039 https://atcoder.jp/contests/ahc040 https://atcoder.jp/contests/ahc041 https://atcoder.jp/contests/ahc042 https://atcoder.jp/contests/ahc044 https://atcoder.jp/contests/ahc045 https://atcoder.jp/contests/ahc046 ahc001 ahc002 ahc003 ahc004 ahc005 future-contest-2022-qual ahc006 ahc007 ahc008 ahc009 ahc010 ahc011 ahc012 ahc014 ahc015 ahc016 ahc017 ahc019 ahc020 ahc021 toyota2023summer-final ahc024 ahc025 ahc026 ahc027 ahc028 ahc030 ahc031 ahc032 ahc033 ahc034 ahc035 ahc038 ahc039 ahc040 ahc041 ahc042 ahc044 ahc045 ahc046 Long Short Long Short Short Long Short Short Long Short Short Long Short Long Short Long Long Long Short Short Short Short Long Short Long Short Long Long Short Long Short Short Long Short Long Short Short Short Long Short"
        },
        {
            "title": "Appendix",
            "content": "A ALE-Bench Details This section provides detailed information regarding the construction of the ALE-Bench dataset and the implementation of its benchmark framework. Corresponding to Section 3 in the main text, Appendix A.1 details the dataset construction process and the data provided. Appendix A.2 details the benchmark implementation, and Appendix A.3 details the evaluation metrics. A.1 Dataset Construction Details From 49 contests eligible for Heuristic Rating calculation on AtCoder1 held up to May 1, 2025, we selected 40 problems originally created by AtCoder and publicly released their data on Hugging Face.2 comprehensive list of these problems is provided in Table A1. Furthermore, one of the authors, an AtCoder Heuristic Contest (AHC) administrator, curated lite version comprising 10 problems. These problems were selected to have relatively high difficulty and cover diverse range of genres, also detailed in Table A1. The full version is designed to enable high-fidelity comparison 1https://atcoder.jp/contests/archive?category=0&keyword=&lang=en&ratedType=4 (Retrieved: May 15, 2025) 2https://huggingface.co/datasets/SakanaAI/ALE-Bench (Retrieved: May 15, 2025) 13 Figure A1: Example problem from ALE-Bench (ahc006). Problem Statement AtCoder Inc. operates food delivery service, AtCoder Foods, that leisurely delivers food that tastes good even if it gets cold. This service receives large number of delivery orders in advance, and processes multiple deliveries simultaneously to improve efficiency. The current service area is represented as square area {(x, y) 0 x, 800} on two-dimensional plane, with AtCoders office located at the center (400, 400). There are 1000 orders today, and the (1 1000)-th order is food delivery request from restaurant in (ai, bi) to location in (ci, di). Todays quota for Takahashi, delivery man, is to process 50 orders. He can freely choose subset {1, , 1000} of size exactly 50 from the 1000 orders and deliver on route (x1, y1), , (xn, yn) satisfying the following conditions. 1. For each S, visit (ci, di) after visiting (ai, bi). That is, there exists an integer pair (s, t) such that (xs, ys) = (ai, bi), (xt, yt) = (ci, di), and < t. 2. (x1, y1) = (xn, yn) = (400, 400). After picking up food at one restaurant, he may pick up food at another restaurant or deliver food to another destination before delivering that food to the destination. He is so powerful that he can carry arbitrary numbers of dishes simultaneously. Moving from (xi, yi) to (xi+1, yi+1) takes time equal to the Manhattan distance xi xi+1 + yi yi+1, and the total travel time for the delivery route is = (cid:80)n1 i=1 xi xi+1 + yi yi+1. Please optimize and delivery routes so that the total travel time is as short as possible. travel Scoring time of the output delivery route, you will get score of For the total round(108/(1000 + )). There are 100 test cases, and the score of submission is the total score for each test case. If you get result other than AC for one or more test cases, the score of the submission will be zero. The highest score obtained during the contest time will determine the final ranking, and there will be no system test after the contest. If more than one participant gets the same score, the ranking will be determined by the submission time of the submission that received that score. Input Input is given from Standard Input in the following format: a1 b1 c1 d1 ... a1000 b1000 c1000 d1000 Each ai, bi, ci, di is an integer between 0 and 800, inclusive, where (ai, bi) represents the coordinates of the restaurant, and (ci, di) represents the coordinates of the destination. (ai, bi) = (ci, di) is satisfied, but for different orders j, there is possibility that {(ai, bi), (ci, di)} {(aj, bj), (cj, dj)} = . Output Let the set of chosen orders be r1, , rm (1 ri 1000), and the delivery route be (x1, y1), , (xn, yn) (0 xi, yi 800), output to Standard Output in the following format. r1 rm x1 y1 xn yn You may output multiple times for visualization purposes. If your program outputs multiple times, only the last output will be used for scoring. The final output must satisfy = 50, but intermediate outputs with = 50 are allowed for visualization. Input Generation Let rand(L, ) be function that generates uniform random integer between and , inclusive. For each = 1, , 1000, we generate an order (ai, bi, ci, di) as follows. We generate ai = rand(0, 800), bi = rand(0, 800), ci = rand(0, 800), and di = rand(0, 800). Redo the generation as long as the Manhattan distance ai ci + bi di is less than 100. Figure A2: The web browser-based visualization tool for ahc006. with human performance, while the lite version aims to facilitate straightforward comparison among AI agents. The data provided for each problem consists of the following components: Problem The problem statement presented to the user is provided in Markdown format. If images are used in the problem statement, these are also provided. Since AtCoder supports both English and Japanese, statements in both languages are included. If sample inputs and outputs are provided within the problem statement, these are also included. Example problem statement is shown at Figure A1. 15 Scorer tool for evaluating solution programs is provided. It is implemented in Rust and provided as source code. While contest participants typically cannot view the source code of the official scorer, significant portion of its functionality is replicated by the Rust-based visualizer tool (described in the next item). Therefore, the provision of this Scorer source code is considered to have minimal impact on the fairness of the benchmark. Visualizer Separate from the problem statement, two types of tools are provided to users for visualizing the execution results of their solutions. One is Rust-based tool executable on local PC, which generates static image visualizing the output of solution program for given input test case. (Note: Some problems, e.g., ahc016, do not provide this specific Rust tool.) The other is web browser-based visualizer, enabling richer and more interactive visualizations than the Rust tool. Figure A2 shows an example. Both visualizers can also generate input test cases from random seeds and hyperparameters. Notably, the test cases for both public and private evaluation are generated using these same visualizer tools, ensuring consistency. Leaderboard Tabular data is provided to determine an equivalent rank in the original contest and calculate performance metrics based on the scores computed by the Scorer in the private evaluation. Unlike live AHCs, which feature real-time leaderboard during the contest period, this benchmark does not offer this functionality. Actual contest participants can view other users scores in real-time, and some might infer solution approaches from these scores. This real-time data is not included in the benchmark due to its confidential nature. Consequently, AI participants operate under slightly more challenging conditions than human contestants who would have access to this information. In addition to problem-specific data, we provide global leaderboard to contextualize an agents rating within the distribution of human player ratings, specifically for determining percentile rankings. This leaderboard lists the ranks and ratings of 6,139 active users, captured at the conclusion of the ahc046 contest, the most recent contest included in this benchmark. A.2 Benchmark Implementation Details We have implemented the benchmark environment as Python library for evaluating AI agents, utilizing the dataset described above. The complete source code is publicly available via GitHub.3 A.2.1 Code Sandbox The Code Sandbox, which replicates AtCoders user script execution environment, was implemented using Docker.4 The corresponding Dockerfiles are available in the GitHub repository.5 We also distribute Docker Images via Docker Hub.6 While AtCoders execution environment evolves, this benchmark, considering the contest periods, offers two environment versions: Ver. 2019077 and Ver. 202301.8 We selected programming languages with high user adoption: C++17 (GCC), Python 3 (CPython), and Rust (rustc) for Ver. 201907; and C++17 (GCC), C++20 (GCC), C++23 (GCC), Python 3 (CPython), and Rust (rustc) for Ver. 202301. We provide execution environments that replicate these setups by installing the respective programming languages and their common external libraries. However, minor discrepancies in installation procedures or specific versions of languages and libraries may exist compared to the original AtCoder environments. Crucially, as per the regulations detailed in Appendix A.3.3, the benchmark mandates the use of the Ver. 202301 environment for all problems. This standardization allows agents to utilize consistent, modern environment even for problems from older contests. This decision was made because ecause we confirmed potential discrepancies arising from using newer, unified environment, is small and not critical. Each execution within the sandbox is allocated resources equivalent to 1 CPU and 2GiB of RAM. 3https://github.com/SakanaAI/ALE-Bench (Retrieved: May 15, 2025) 4https://www.docker.com/ (Retrieved: May 15, 2025) 5https://github.com/SakanaAI/ALE-Bench/tree/main/dockerfiles (Retrieved: May 15, 2025) 6https://hub.docker.com/r/yimjk/ale-bench (Retrieved: May 15, 2025) 7https://atcoder.jp/contests/language-test-202001 (Retrieved: May 15, 2025) 8https://atcoder.jp/contests/language-test-202301 (Retrieved: May 15, 2025) 16 A.2.2 Actions We describe the specific specifications for the four types of actions introduced in Section 3.2. As previously mentioned, we refer to the benchmark execution environment where the AI participates as an Session . 1. Problem Data related to the problem can be accessed at any time once an Session has commenced. This includes not only the problem statement and any accompanying images, but also crucial metadata: the optimization objective (maximization or minimization of the Scorers score), whether the problem is reactive (requiring interaction with the judging system), the problem title, original contest date and time, problem-specific time limits, execution time limits, and memory usage limits. However, as stipulated in the regulations detailed in Appendix A.3.3, some data, such as information pertaining to private evaluation test cases, must not be accessed. 2. Test Run AI agents can execute their generated code at any point within the problems time limit. The framework automatically invokes the Code Sandbox for execution. When the code is executed, the source code is first compiled, and then the program runs. If compilation fails, feedback including the standard error output is provided. Successful execution yields feedback comprising the Scorers judging result and score, the input case used, program outputs (standard output and error), execution time, and memory usage. The judging results can indicate: COMPILATION_ERROR for compilation failure, MEMORY_LIMIT_EXCEEDED for exceeding memory limits, TIME_LIMIT_EXCEEDED for exceeding time limits, RUNTIME_ERROR for runtime errors, INTERNAL_ERROR for internal errors within ALE-Bench not attributable to the program produced by the AI system, WRONG_ANSWER for incorrect answer format, or ACCEPTED for correctly formatted answer with calculated score. Crucially, the precise internal specifications of AtCoders execution environment (e.g., judging flow, exact time/memory tracking mechanisms) are not public. Our Code Sandbox is replication based on publicly available information (e.g., language execution commands, common libraries, machine specifications). Consequently, while the sandbox behavior is highly likely to be nearly identical to AtCoders, exact replication in every instance cannot be guaranteed. Beyond code execution, agents can also generate new input test cases using random seed and optional hyperparameters. The interface supports four distinct test run operations: (1) Public Evaluation: execution against predefined set of test cases (50 for the full version, 5 for the lite version). (2) Custom Test Run: execution using an agent-specified input case. (3) Input Case Generation: generation of new input case. (4) Generate and Run: generation of new input case followed immediately by its execution. While public evaluation facilitates iterative testing on fixed cases, other operations allow agents to experiment with custom-generated or specified inputs. For each problem, its input cases are generatable from single random seed. Some problems allow further customization via optional hyperparameters; agents must consult the generation tools README to identify and utilize these. 3. Visualization Agents can visualize their solutions behavior on specific input cases. Similar to actual AHCs, participants have access to two types of visualizers: simple Rust-based tool for local execution, and more feature-rich web-based tool supporting animations and interactive operations. Both are provided in this benchmark. The local Rust visualizer can optionally run concurrently with Test Run. For the web visualizer, the Session can optionally launch local HTTP server automatically. Users can then access and interact with the visualization via web browser at the designated port. Moreover, the source code for the Rust visualizer, typically provided to human contestants, is also accessible within this benchmark. 4. Submission The final evaluation of an agents solution is performed via single private evaluation. This can be invoked only once per Session , within the problems time limit, and concludes the agents interaction with that problem. For the private evaluation, the full version uses the identical set of test cases from the original contests private leaderboard. The lite version uses the same test cases as the full version for short-duration contests (typically hundreds of test cases). For long-duration contests (thousands of test cases), the lite version uses reduced set: the first 10% of the full versions test cases. Following execution and scoring across the private test set, rank is calculated. Generally, the contest score is the sum of the scorers scores on all private test cases. This total score is then used to determine rank 17 against the original contest leaderboard (for the full version) or newly rebuilt laderboard with only the lite version subset of test cases (for the lite version). However, for certain problems (e.g., ahc016, ahc017, ahc025), ranking is based on the sum of normalized relative scores for each test case. In these instances, normalized scores are recomputed based on the agents and all human participants raw scores for each test case, and the final rank is determined by sorting based on the sum of these re-normalized scores. Once the rank is established, the performance is determined. While AtCoders official performance calculation [16] incorporates the AI systems current rating, other users internal ratings, and the AI systems rank [17], our benchmark approximates performance based solely on the agents equivalent rank. If ties existed in the original contest, leading to non-unique mapping between rank and performance, we use linear interpolation from adjacent ranks and their performances to approximate the agents performance. AtCoder applies correction to performance values below 400 to ensure that they are positive, however, this benchmark does not apply this specific correction to the raw performance metric. The overall Rating, however, does incorporate this correction to maintain comparability with human ratings. A.2.3 Others Sequential execution of multiple test cases (during test runs or final evaluation) can be prohibitively time-consuming. For instance, evaluating 1000 cases for problem with 2-second per-case limit would exceed 30 minutes. To mitigate this, users can specify the number of parallel executions at the beginning of an Session , significantly reducing overall runtime. As solutions are typically CPUbound, the number of parallel executions should ideally not exceed the number of available physical CPU cores. Beyond the core framework, we provide common scripts to facilitate environment setup.9 Terraform10 scripts are also available, enabling one-command replication of our experimental setup on Amazon Web Services (AWS). A.3 Evaluation Metrics Details For evaluating performance in ALE-Bench, two categories of metrics are employed: fine-grained metrics for each individual problem within the benchmark, and coarse-grained metrics for the benchmark as whole. A.3.1 Per-problem Metrics The following four fine-grained metrics are calculated per problem: Scorer score for each test case in private evaluation Each test case is assigned score by the Scorer, as defined in the problem statement. This score is to be either maximized or minimized, depending on the specific problem. Overall private evaluation score This is an aggregate score derived from the Scorers evaluations across all private test cases. Typically, it is the sum of raw Scorer scores. However, certain problems utilize the sum of normalized scores per test case. Three primary normalization methods exist: (1) (Agents Scorer score) / (Maximum Scorer score among all participants) (e.g., ahc016); (2) (Minimum Scorer score among all participants) / (Agents Scorer score) (e.g., ahc017); and (3) Rank of the agents Scorer score among all participants (e.g., ahc025). higher value in this aggregate score corresponds to better rank. Rank Based on the overall private evaluation score, the agent is ranked relative to the original human participants of the contest. Performance From the rank, quantitative performance metric, as used on AtCoder, is derived. Further details on its calculation are provided in Appendix A.2. While not built-in feature, leaderboards based on metrics other than overall performance (e.g., raw scores on specific problems) can be constructed to rank participating AI agents amongst themselves. The open-ended nature of these optimization problems means that AI agents can continue to compete 9https://github.com/SakanaAI/ALE-Bench/tree/main/cloud (Retrieved: May 15, 2025) 10https://developer.hashicorp.com/terraform (Retrieved: May 15, 2025) 18 and differentiate themselves even after surpassing top human performance levels, making it suitable platform for ongoing AI vs. AI competition. A.3.2 Overall Metrics The following three coarse-grained metrics are available for the overall benchmark: Average Performance The arithmetic mean of the performance scores achieved on each problem in the benchmark. Performance Distribution On AtCoder, both the performance and the rating (described below) are associated with color tiers. These are: 399 (Gray), 400 799 (Brown), 800 1199 (Green), 1200 1599 (Cyan), 1600 1999 (Blue), 2000 2399 (Yellow), 2400 2799 (Orange), and 2800 (Red). As these color tiers are widely recognized within the AtCoder community, analyzing the distribution of an agents performance across these tiers provides an intuitive overview of its capabilities. Rating Beyond simple averaging, AtCoder employs Rating system to aggregate performance across multiple contests, serving as an overall skill indicator. The official rating calculation formula [17] is implemented within our framework. However, as detailed in Section 3.3, this rating system incorporates adjustments specifically designed for human participation patterns (e.g., not all contests are attempted). Consequently, it is less suitable for directly evaluating AI agents that are expected to attempt all benchmark problems. It is therefore included primarily to offer point of comparison against established human performance benchmarks. A.3.3 Regulations In addition to standardizing the calculation methods for evaluation metrics, we have established detailed regulations concerning the execution environment and agent actions. These regulations aim to ensure fair and consistent evaluation of AI agents, both in comparison to human contestants and other AI systems. We outline the key clauses of our bespoke regulations below: Permitted programming languages are C++ (versions 17, 20, or 23), Rust, and Python, all within the AtCoder Judge Version 202301 environment. Execution time is measured as the maximum of wall-clock time and CPU time. While parallelization within solution is permitted, it does not reduce the officially measured execution time (as per AtCoder rules). Participants may use any development environment or editor of their choice. Use of self-created libraries and publicly available human-created libraries is permitted. However, direct reuse of code identifiable as complete AHC submission by another participant is prohibited to ensure originality of the agents core logic. General internet searches for programming concepts or libraries are allowed. However, accessing websites containing specific keywords like \"AtCoder,\" \"AHC,\" or the exact problem name (which might lead to existing solutions or discussions) is forbidden. An exception is made for interacting with the local visualization pages provided by ALE-Bench. No human intervention or assistance is permitted after the agent has received the problem statement and commenced its run. The sole exception is for restarting the process due to critical technical failures (e.g., network outages, hardware crashes). The size of each submitted source code must not exceed 512 KiB. The time limit for each problem mirrors the duration of the original AtCoder contest. Agents are not required to use the full duration and may submit their solution earlier. Private evaluation can be initiated only once per problem. The agent must select single solution code for this evaluation, and the process must start before the problems time limit expires. For practical purposes, if solution was fully generated and saved before the deadline, its private evaluation can be run even if the submission command itself is issued shortly after the time limit. 19 Agents may utilize unlimited computational resources for development and code generation (e.g., CPUs, GPUs). However, the official ALE-Bench execution environment for scoring submissions must run on an Intel Xeon Platinum 8375C CPU @ 2.90GHz, or CPU with demonstrably equivalent performance (e.g., AWS C6i series instances [15]). If noncompliant hardware is used, results can still be reported but must be accompanied by detailed specifications of the computational environment to allow for potential performance normalization or caveats. Agents must not access or utilize any information pertaining to: (a) the private evaluation test cases themselves (beyond what is inferable from the public visualizer/generator), (b) the raw scores or detailed data of individual human participants on the leaderboard (used for normalization in some problems). A.4 Limitations of ALE-Bench ALE-Bench is designed to reproduce actual contests so that humans and AI can be compared fairly. Nevertheless, it has the following limitations: (1) Differences in judging environments. Contests held before the summer of 2023 ran on an older system whose hardware is slightly slower than todays environment. However, even when we re-evaluated some solutions with the latest environment, their scores and rankings showed little change, so we can conclude that the impact is minor. (2) Use of new resources. Even the earliest contest (March 2021) is relatively recent. Yet, algorithms or implementations that were unavailable then can now be applied, allowing higher performance to be achieved more easily than was originally possible. (3) Problem-set contamination. Since the original problems are publicly available, they may have been accidentally included in AI training data. According to the results in Section 5.4, no contamination effects have been detected, but we will continue adding fresh problems in future updates and monitor the situation. ALE-Agent Details This section details the conceptual algorithmic framework and strategic design underpinning ALEAgents solution exploration capabilities. B.1 Core Strategy and Design Philosophy ALE-Agent achieves efficient exploration of solutions for heuristic problems by integrating the broad knowledge and code generation prowess of Large Language Models (LLMs) with systematic search algorithm. Our approach utilizes an algorithm based on best-first search, striving for balance between the diversity of LLM-generated solutions and the depth of the search. This process emulates human-like trial-and-error, where diverse LLM-generated ideas are quantitatively evaluated, and the most promising solution candidates are further explored in depth. B.2 Search Algorithm ALE-Agents search algorithm expands search tree based by best-first search principles. Each node in the search tree represents state, each holding its corresponding source code. In each iteration, the algorithm selects the most promising state from the current frontier of explored states. The source code of this selected state then serves as seed for generating new cohort of candidate solutions through LLM-driven refinement techniques (detailed in Appendix B.3). The promisingness of state is quantified through local execution on fixed suite of 50 test cases, employing composite scoring function based on: 1. Acceptance Ratio: The proportion of test cases for which the generated code is ACCEPTED . higher acceptance ratio indicates more promising solution. 2. Score: Among ACCEPTED solutions, those achieving better problem-specific score (as defined by the contest) are considered more promising. The selection of the next node for expansion is guided by priority score that holistically integrates these two criteria. 20 Diverging from standard best-first search, which expands single successor, ALE-Agent adopts beam search-inspired strategy, concurrently generating beam of = 30 child nodes from each selected parent node. This strategy enhances solution diversity and optimizes parallel processing efficiency (detailed in Appendix B.4). Furthermore, we incorporate tabu search-like mechanism: previously expanded parent nodes are precluded from re-selection for expansion. These enhancements preserve the core efficiency of best-first search while substantially improving exploration breadth and reducing susceptibility to premature convergence on local optima. B.3 State Transition Node expansion, corresponding to the generation of novel solution candidates, is orchestrated through an interactive dialogue with the LLM. In this dialogue, the agent provides the LLM with comprehensive context about the current solution (e.g. its source code, past evaluation results, and any evaluative feedback). Then the LLM is prompted to generate either refinements to the existing code or entirely new algorithmic approaches. B.3.1 Initial Solution Generation To generate initial solutions (i.e., children of the search trees root node), the LLM is provided with the complete problem description. Based on this, the LLM formulates one or more initial algorithmic approaches and generates the corresponding source code. This approach eliminates the need for human-seeded initial solutions, allowing the search to commence from diverse set of starting points derived purely from the LLMs understanding of the problem. B.3.2 Iterative Solution Refinement Generating successor solutions from an existing state involves an iterative refinement loop. In each iteration, the LLM is prompted with rich contextual package comprising: (a) Current Code and Performance Feedback: The source code of the current solution and its detailed performance feedback (e.g., per-test-case scores, execution status), which serves as the primary basis for LLM-driven refinement. (b) Historically Best Code and Performance Feedback: The source code of the bestperforming solution discovered so far in the search, along with its associated feedback. This helps to prevent the search from stagnating in suboptimal regions of the solution space by reminding the LLM of previously successful strategies. (c) Summary of Past Search Trajectory: concise summary of recent search history (e.g., few previous attempts, their outcomes, and applied modifications). This enables the LLM to engage in more informed, longer-range strategic planning by considering the evolution of explored strategies, their efficacy, and identified pitfalls. (d) Targeted Improvement Guidance: To encourage diverse exploration and steer the LLM towards promising avenues, specific improvement directives are stochastically selected from predefined set of guiding prompts. These prompts might suggest, for instance, optimizing particular algorithm (e.g., simulated annealing or beam search), or speeding up the solution by introducing new algorithms or data structures. This aims to scaffold the LLMs reasoning towards discovering solutions well-suited for heuristic problems. Based on this multifaceted input, the LLM first formulates high-level improvement strategy. It then attempts to implement this strategy by generating new source code over sequence of three interactive conversational turns. The code version exhibiting the best performance (evaluated locally) across these three turns is selected as the basis for new node in the search tree. This multi-turn refinement protocol provides the LLM with opportunities for self-correction and allows for more nuanced implementation of its proposed strategic changes. B.4 Parallel Execution for Enhanced Throughput ALE-Agent leverages parallel processing when generating multiple child nodes from single parent to maximize search throughput. Although reasoning models, such as OpenAIs o-series or Googles Gemini 2.5, may exhibit response latencies of up to several minutes, their API calls are readily 21 parallelizable. ALE-Agent dispatches parallel requests to the reasoning model, where each task prompts the model to generate code refinement based on the parent nodes information. Upon receiving responses from the model, the newly generated code segments are added to an evaluation queue for asynchronous assessment. Our local evaluation pipeline, when assessing solution against 50 input cases using 13 parallel threads, takes approximately 10 seconds per code instance. By generating 30 child nodes concurrently at each expansion step, ALE-Agent effectively utilizes local computational resources for evaluation and minimizes the impact of the reasoning model latency on overall search velocity. B.5 Ablation Configurations To facilitate ablation studies, ALE-Agent can be configured in several operational modes: Base: This is the most fundamental setting, which omits tree search (i.e., beam width of 1) and the explicit domain-guided improvement directives. Its features are: 1. It employs sequential refinement strategy, generating only one child node from each parent. 2. When implementing an improvement strategy, it forgoes the three-turn iterative refinement; code is adopted as successor as soon as it passes all local test cases. 3. It does not use the targeted improvement guidance prompts (described in item (d) of Appendix B.3.2). + Method 1: This mode builds upon Base by reintroducing (i) the three-turn iterative refinement for code implementation and (ii) the targeted improvement guidance prompts, while still maintaining the single-path search (beam width 1). These feature enable LLMs to implement potentially complex, domain-informed strategies, without introducing complex tree search. + Method 1&2: This configuration represents the full ALE-Agent but with one specific modification: it extends + Method 1 by employing the full beam search mechanism (beam width of 30). However, the summary of past search trajectory (item (c) of the iterative refinement context in Appendix B.3.2) is omitted, as this history might inadvertently reduce diversity when multiple distinct paths are being explored concurrently in the tree search. This allows the agent to maximize the effectiveness of tree search. B.6 Prompts for guided improvement We provide the prompts for the targeted improvement guidance in Appendix B.3.2 here. We use the following four prompts reflecting the domain knowledge: Speedup Prompt Based on the code and feedback : What are the key algorithms and data structures used ? What are its computational complexity bottlenecks ( consider feedback like Time Limit Exceeded ) ? How might we improve the time or space complexity ( consider feedback like Memory Limit Exceeded ) ? Consider both small optimizations and completely different approaches . You don ' need to implement the solution yet . Instead , please think deeply and broadly about the possible improvements , and explain your thoughts . Simulated Annealing State Improvement Prompt If this solution uses simulated annealing , analyze the problem , solution properties , and feedback to suggest better state representation . Consider how the current state encoding might be limiting the search space or convergence speed , especially in light of the feedback . Think about alternative state encodings that could lead to better local optima or faster convergence . You don ' need to implement the solution yet . Instead , please think deeply and broadly about the possible improvements , and explain your thoughts ."
        },
        {
            "title": "Simulated Annealing Neighbor Improvement Prompt",
            "content": "If this solution uses simulated annealing , analyze the problem , solution properties , and feedback to suggest better neighborhood design . Consider how the current neighborhood structure might be limiting the search space exploration or convergence speed , based on the feedback . Think about alternative neighborhood structures that could lead to better local optima or faster convergence . Specifically , consider : 1. How to balance between small and large moves in the search space 2. How to ensure the neighborhood structure allows reaching any valid solution 3. How to design moves that maintain solution feasibility while exploring new regions You don ' need to implement the solution yet . Instead , please think deeply and broadly about the possible improvements , and explain your thoughts ."
        },
        {
            "title": "Beam Search Improvement Prompt",
            "content": "Consider implementing or improving beam search approach . Think about the beam width and evaluation function that could lead to better solutions . Consider how to effectively balance between diversity and quality in your beam . You don ' need to implement the solution yet . Instead , please think deeply and broadly about the possible improvements , and explain your thoughts ."
        },
        {
            "title": "C Additional Experimental Details and Results",
            "content": "C.1 Experimental Setup Details Computational Resource. All experiments were conducted on Amazon EC2 c6i.32xlarge [15] instances, each equipped with 128 vCPUs and 256 GiB of RAM. These instances feature Intel Xeon Platinum 8375C CPUs @ 2.90GHz, identical to those used in the AtCoder execution environment (Ver. 202301). The operating system was Ubuntu 22.04.5 LTS with Linux kernel 6.8.0-1027-aws x86_64. All instances were located in the AWS us-east-1 (N. Virginia) region11. No GPUs were utilized in these experiments. Python 3.12.9 served as the interpreter for all experimental tasks. Each instance concurrently processed maximum of four distinct problems, with these concurrent tasks sharing all available instance resources. As detailed in Appendix A.2, ALE-Bench allows parallel execution of multiple test cases for single problem. We configured it to run up to 13 test cases in parallel per problem. Given that the evaluated programs were expected to be CPU-bound with minimal I/O latency, this configuration (up to 4 problems 13 cases/problem = 52 concurrent processes if fully utilized) was well within the 64 physical cores of the c6i.32xlarge instance, ensuring ample computational resources. Models. We experimented with total of 22 leading LLMs. More specifically, we used five nonreasoning models (GPT-4o-mini [18], GPT-4o [19], GPT-4.1-nano [20], GPT-4.1-mini [20], and GPT-4.1 [20]) and four reasoning models (o1 [21], o3-mini [22], o3 [23], and o4-mini [23]) from OpenAI; five non-reasoning models (Gemini 1.5 Flash-8B [24], Gemini 1.5 Flash [24], Gemini 1.5 Pro [24], Gemini 2.0 Flash-Lite [25], and Gemini 2.0 Flash [26]) and two reasoning models (Gemini 2.5 Flash [27] and Gemini 2.5 Pro [28]) from Google; three non-reasoning models (Claude 3.5 Haiku [29], Claude 3.5 Sonnet [29], and Claude 3.7 Sonnet [30]) and one reasoning model (Claude 3.7 Sonnet (Thinking) [30]) from Anthropic; and one non-reasoning model (DeepSeek-V3 [31]) and one reasoning model (DeepSeek-R1 [32]) from DeepSeek. All models were accessed via their publicly available APIs. Table A2 lists the API provider and parameter settings used for each model. For models accessed via OpenRouter, the underlying provider is indicated in parentheses. When using OpenHands, the effective model parameters might differ from those listed in Table A2, as OpenHands internal configuration takes precedence. Prompts. consistent set of prompt templates was used across all experiments, unless otherwise specified. The model was provided with the problem statement, feedback from the ALE-Bench 11https://docs.aws.amazon.com/global-infrastructure/latest/regions/aws-regions. html (Retrieved: May 15, 2025) 23 Model API Provider Table A2: The details of LLMs with API call. Parameters Model Name GPT-4o mini GPT-4o GPT-4.1 nano GPT-4.1 mini GPT-4.1 o1-high o3-mini-high o3-high o4-mini-high Gemini 1.5 Flash-8B Gemini 1.5 Flash Gemini 1.5 Pro Gemini 2.0 Flash-Lite Gemini 2.0 Flash Gemini 2.5 Flash Gemini 2.5 Pro Claude 3.5 Haiku Claude 3.5 Sonnet Claude 3.7 Sonnet Claude 3.7 Sonnet (Thinking) DeepSeek-V3 DeepSeek-R OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI Google AI Studio Google AI Studio Google AI Studio Google AI Studio Google AI Studio Google AI Studio Google AI Studio AWS Bedrock AWS Bedrock OpenRouter (Google/Anthropic) anthropic/claude-3.7-sonnet OpenRouter (Google/Anthropic) anthropic/claude-3.7-sonnet OpenRouter (Lambda/DeepInfra) deepseek/deepseek-chat-v3-0324 OpenRouter (Lambda/DeepInfra) deepseek/deepseek-r1 gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 gpt-4.1-nano-2025-04-14 gpt-4.1-mini-2025-04-14 gpt-4.1-2025-04-14 o1-2024-12-17 o3-mini-2025-01-31 o3-2025-04-16 o4-mini-2025-04-16 gemini-1.5-flash-8b-001 gemini-1.5-flash-002 gemini-1.5-pro-002 gemini-2.0-flash-lite-001 gemini-2.0-flash-001 gemini-2.5-flash-preview-04-17 gemini-2.5-pro-preview-03-25 us.anthropic.claude-3-5-haiku-20241022-v1:0 max_tokens:8192 us.anthropic.claude-3-5-sonnet-20241022-v2:0 max_tokens:8192 reasoning_effort:\"high\" reasoning_effort:\"high\" reasoning_effort:\"high\" reasoning_effort:\"high\" max_tokens:20000 max_tokens:20000, thinking_budget_tokens:16000 temperature:0.3, max_tokens:8000 temperature:0.6, max_tokens:32768 environment on previously generated code (if applicable), and an instruction to generate code within designated Markdown-style code block. The solution code was then extracted from the models response using regular expression pattern matching. Images accompanying the problem statements were not used. In the prompt templates, placeholders denoted by ${} are substituted with either the content described in the surrounding sentences or the values of the corresponding variables listed below: TIME_LIMIT: The execution time limit in seconds provided by each problem. (e.g. 2.0) MEMORY_LIMIT: The memory limit in MiB provided by each problem. (e.g. 1024) PROBLEM_STATEMENT: The English problem statement in Markdown format provided for each problem. CODE_LANGUAGE_NAME: C++20 (gcc 12.2.0) for C++20, Python (CPython 3.11.4) for Python3, Rust (rustc 1.70.0) for Rust. CODE_BLOCK: ```cppn// Your code heren``` for C++20, ```pythonn# Your code heren``` for Python3, ```rustn// Your code heren``` for Rust. EXTERNAL_LIBRARIES: External libraries that users can use in the AtCoder execution environment. External Libraries (C++20) - AC Library@1 .5.1 - Boost@1 .82.0 - GMP@6 .2.1 - Eigen@3 .4.0 -2 ubuntu2 External Libraries (Python3) - numpy ==1.24.1 - scipy ==1.10.1 - networkx ==3.0 - sympy ==1.11.1 - sortedcontainers ==2.4.0 - more - itertools ==9.0.0 - shapely ==2.0.0 - bitarray ==2.6.2 - PuLP ==2.7.0 - mpmath ==1.2.1 - pandas ==1.5.2 - z3 - solver ==4.12.1.0 - scikit - learn ==1.2.0 - ortools ==9.5.2237 - ac - library - python - setuptools ==66.0. 24 - cppyy ==2.4.1 - torch ==1.13.1 - polars ==0.15.15 - lightgbm ==3.3.1 - gmpy2 ==2.1.5 - numba ==0.57.0 External Libraries (Rust) - ac - library - rs@ =0.1.1 - once_cell@ =1.18.0 - at c_ ss rt on @ =1.1.0 - varisat@ =0.2.2 - memoise@ =0.3.2 - argio@ =0.2.0 - bitvec@ =1.0.1 - counter@ =0.5.7 - hashbag@ =0.1.11 - pathfinding@ =4.3.0 - recur - fn@ =2.2.0 - indexing@ =0.4.1 - amplify@ =3.14.2 - amplify_derive@ =2.11.3 - amplify_num@ =0.4.1 - easy - ext@ =1.0.1 - multimap@ =0.9.0 - btreemultimap@ =0.1.1 - bstr@ =1.6.0 - az@ =1.2.1 - glidesort@ =0.1.2 - tap@ =1.0.1 - omniswap@ =0.1.0 - multiversion@ =0.7.2 - num@ =0.4.1 - num - bigint@ =0.4.3 - num - complex@ =0.4.3 - num - integer@ =0.1.45 - num - iter@ =0.1.43 - num - rational@ =0.4.1 - num - traits@ =0.2.15 - num - derive@ =0.4.0 - ndarray@ =0.15.6 - nalgebra@ =0.32.3 - alga@ =0.9.3 - libm@ =0.2.7 - rand@ =0.8.5 - getrandom@ =0.2.10 - rand_chacha@ =0.3.1 - rand_core@ =0.6.4 - rand_hc@ =0.3.2 - rand_pcg@ =0.3.1 - rand_distr@ =0.4.3 - petgraph@ =0.6.3 - indexmap@ =2.0.0 - regex@ =1.9.1 - lazy_static@ =1.4.0 - ordered - float@ =3.7.0 - ascii@ =1.1.0 - permutohedron@ =0.2.4 - superslice@ =1.0.0 - itertools@ =0.11.0 - itertools - num@ =0.1.3 - maplit@ =1.0.2 - either@ =1.8.1 - im - rc@ =15.1.0 - fixedbitset@ =0.4.2 - bitset - fixed@ =0.1.0 - proconio@ =0.4.5 - text_io@ =0.1.12 - rustc - hash@ =1.1.0 - smallvec@ =1.11.0 We employed seven distinct prompt templates in our experiments, as presented below: 1. System Prompt: Used for the system message (also known as the developer message for some OpenAI models). 2. Initial Instruction Prompt: The message for the first trial of code generation with the model. 3. Feedback for No Code Block Found: Used to instruct the model to regenerate its response when the specified code block is missing from its response. 4. Feedback for Accepted Code: Provided to the LLM in the subsequent turn as feedback when the public evaluation of the generated code results in an ACCEPTED status. 5. Feedback for Non-Accepted Code: Provided to the LLM in the subsequent turn as feedback when the public evaluation of the generated code does not result in an ACCEPTED status. 6. Refinement Instruction Prompt: Used from the second code generation attempt onwards. It instructs the model to refine its previous solution based on the feedback from the public evaluation of the code generated in the prior turn. 7. Refinement Instruction Prompt with Summarization: Similar to the Refinement Instruction Prompt, but used from the second code generation attempt onwards when summarization of the conversation history is employed. This template instructs the model to generate new solution based on summary of past interactions, rather than the full history. Experiments involving scaffolding systems (OpenHands, ALE-Agent) utilized slightly modified versions of these templates, tailored to each system. Additionally, specific prompts unique to each scaffolding system were also employed, as detailed in their respective sections. System Prompt You are world - class algorithm engineer , and you are very good at programming . Now , you are participating in programming contest . You are asked to solve heuristic problem , known as an NP - hard problem . Initial Instruction Prompt There is problem statement at the end of this message . First , please analyze the problem statement . Please think about the essential points of the problem and possible algorithms to get higher rank in the contest . Next , please implement your solution in $ { CO E_ AN UA E_ AM }. Your solution code should be written in the $ { CODE_BLOCK } code block . You can use external libraries as follows : $ { XT RN L_ IB AR ES } [ Problem statement ] Execution time limit : $ { TIME_LIMIT } sec / Memory limit : $ { MEMORY_LIMIT } MB $ { PRO BLE M_ STA TEM ENT } Feedback for No Code Block Found No valid code block found . Please implement your solution in $ { CO E_ AN UA E_ AM }. Your solution code should be written in the $ { CODE_BLOCK } code block . Feedback for Accepted Code [ Public test result ] Overall judge result : ACCEPTED Overall absolute score : $ {( Overall score is provided here .) } $ - Case 1: $ {( Score for case 1 is provided here .) } $ - Case 2: $ {( Score for case 2 is provided here .) } $ ...( omission ) ... - Case $N : $ {( Score for case $N is provided here .) } $ 26 Feedback Template for Non-Accepted Code [ Public test result ] Overall judge result : $ {( Judge result is provided here . It is one of these words : COMPILATION_ERROR , MEMORY_LIMIT_EXCEEDED , TIME_LIMIT_EXCEEDED , RUNTIME_ERROR , INTERNAL_ERROR , WRONG_ANSWER ) } - Case $IDX : Absolute score : $ { Score for case $IDX in seconds here .} Execution time : $ { Execution time in seconds for case $IDX here .} sec Memory usage : $ { Memory usage in MiB for case $IDX here .} MB Standard error : \" $ { The standard error output for case $IDX in here .}\" Message : \" $ { The feedback message from the ALE - Bench session for case $IDX in here .}\""
        },
        {
            "title": "Refinement Instruction Prompt",
            "content": "$ { The feedback for the latest code here .} Based on the above feedback , please consider the ways to improve your solution . Firstly , please analyze this given feedback and list what insights can be gained from it . Then , based on the insights , please refine your code to achieve better performance . It can be simple bug fix , the introduction of new algorithm , or any degree of change from minor to major . Your solution code should be written in the $ { CODE_BLOCK } code block . Refinement Instruction Prompt (with Summarization) [ Problem statement ] Execution time limit : $ { TIME_LIMIT } sec / Memory limit : $ { MEMORY_LIMIT } MB $ { PRO BLE M_ STA TEM ENT } [ Summary of your previous attempts ] $ { Summary that the model output in the previous turn here . If it is the first time , we used \" Your first submission was done and you need to start logging your attempts from now on .\" string for the initial summary . If there is no summary in the previous response from the model , we used \" Your summary was not found . The summary must be written in the Markdown format in the ``` md <! - - Your summary here - - >n ` ` ` code block .\" string for the summary .} [ Your best submission ] ### Code $ { The current best code with code block template here .} ### Feedback $ { The feedback for the current best code here .} [ Your latest submission ] ### Code $ { The latest code with code block template here . If the latest code is the same as the current best code , we used \" The latest code is the same as the best code .\" string .} ### Feedback $ { The feedback for the latest code here . If the latest code is the same as the current best code , we used \" The latest feedback is the same as the best feedback .\" string .} Based on the above feedback , please consider the ways to improve your solution . Firstly , please analyze this given feedback and list what insights can be gained from it . Apart from that , please create new summary including the content of the summary of your previous attempts in Markdown format in the ``` md nYour summary here ` ` ` code block . If this code block in this format is not found , the summary of your previous attempts will not be input in the next turn . Then , based on the insights , please refine your code to achieve better performance . It can be simple bug fix , the introduction of new algorithm , or any degree of change from minor to major . Your solution code should be written in the $ { CODE_BLOCK } code block . 27 Metrics. The primary evaluation metrics, introduced in Section 3.3 and further detailed in Appendix A.3, were: (1) average performance across all benchmark problems, (2) performance distribution, and (3) AtCoder-style Rating. For the performance distribution, we report the number of problems for which each color-tier (e.g., Cyan, Red) was achieved. Additionally, we computed two auxiliary cost metrics: the average monetary cost (USD) to solve single problem and the average cost per model response. The cost per response is not strictly equivalent to the cost per code generation, as some responses might lack valid code block. These calculations are based solely on input and output token counts, excluding potential discounts from caching or other pricing variations. Cost calculation for models accessed via OpenAI was: (prompt_tokens input token price) + (completion_tokens output token price). For Google AI Studio: (prompt_token_count input token price, considering volume tiers) + (candidates_token_count output token price, considering input token count and thought tokens). For AWS Bedrock: (input_tokens input token price) + (output_tokens output token price). For OpenRouter, we used the sum of the cost field from its log files. C.2 Details of the One-Shot Experiment Setup. To assess the one-shot algorithm engineering capabilities of LLMs, we conducted experiments where models had limited number of attempts to generate correct solution, without extensive iterative refinement. Ideally, evaluation would rely solely on the models first response. However, the initial response might lack code block (e.g., containing only high-level plan), or the code might fail due to minor, non-fundamental issues such as exceeding time limits or output formatting errors. Such failures could lead to an unfairly low assessment of the models core algorithm engineering abilities. Therefore, to mitigate these issues while preserving focus on one-shot capabilities, we allowed each model to generate up to five code submissions per problem before private evaluation. The protocol was: After each code generation, public evaluation was performed. If the result was ACCEPTED , no further generation for that problem was permitted. Otherwise, feedback (per defined templates) was provided, and the model attempted to revise its code. If an ACCEPTED status was not achieved after five attempts, the last generated code was used for private evaluation. If models response lacked code block in the specified format, it was prompted to regenerate correctly, with up to three such retries allowed. The conversation started with the Initial Instruction Prompt. Subsequent user messages for refinement used the Refinement Instruction Prompt. The Feedback for No Code Block Found prompt was used if code block was missing. The full conversation history was maintained and provided to the model for each new response generation in this setting. These experiments, processing the full ALE-Bench, ran on 10 c6i.32xlarge instances in parallel and were completed within one day. Results. Table A3 presents the complete results for each model and programming language in the one-shot setting. Rows shaded in light gray indicate the average performance for each model across the three languages (C++20, Python3, and Rust). C.3 Details of the Iterative-Refinement Experiment Setup. To evaluate the algorithm engineering capabilities of LLMs with extended \"thinking\" time, we conducted experiments with time limit per problem of either four hours or the problems original contest duration, whichever was shorter. This four-hour timeframe, as indicated in Table A1, aligns with the duration of most short-format AtCoder Heuristic Contests, allowing AI systems to virtually participate under time constraints comparable to human contestants. Unlike the one-shot experiments, models in this setting continuously received public evaluation feedback and refined their solutions throughout the allocated time, even after achieving an ACCEPTED status, to maximize their scores. four-hour period can result in numerous code generations (potentially 100). Submitting the full conversation history for each generation would exceed context limits and incur prohibitive costs. Thus, we employed summarization strategy: after the first code generation, the LLM summarized its progress. For subsequent generations, the model received the original problem statement, its summary of previous attempts, the current best-performing code (with its public evaluation feedback), and the most recent code (with its feedback). The first user message was the Initial Instruction Prompt, as in the one-shot setup. Subsequent refinement messages used the Refinement Instruction Prompt (with Summarization). The Feedback for No Code Block Found prompt was used for missing code blocks. With summarization, the explicit conversation history was cleared before each new code generation 28 Figure A3: Performance plot for each model. Similar to Figure 6, For each model, scatter plot is shown with contest end dates on the x-axis and performance on the y-axis. Each vertical line indicates the knowledge cutoff date of each model. Figure A4: Trends in public score and code file size in the iterative-refiinement setting. Each plot shows the progression of generated code file sizes alongside the corresponding public evaluation scores over four-hour period. Points farther to the right represent that the code is generated at later time points. request (unless retrying for missing code block, where the immediate prior history was retained). The final submission for each problem was the code that achieved an ACCEPTED status on public tests and yielded the highest total score from individual test case scorers. These iterative-refinement experiments utilized four c6i.32xlarge instances. Each instance was dedicated to one of four models: GPT-4.1 mini, o4-mini-high, Gemini 2.5 Pro, and DeepSeek-R1. Each model processed all 40 problems from the full ALE-Bench. On each instance, up to four problems were processed concurrently (see Appendix C.1), adhering to the specified time limit. This phase took approximately two days. Results. The main aggregated results for the iterative-refinement experiments are presented in Table 3 in the main body of the paper. Detailed per-problem performance for this set of experiments is provided in Table A6 and plotted in Figure A3. Table A4 provides the AtCoder user statistics that were used for the performance distribution comparison with o4-mini-high, as discussed in Section 5.2 of the main paper. These statistics were computed from internal AtCoder data by one of the authors, who is an employee of AtCoder Inc. Figure A4 provides additional examples illustrating the trajectories of public evaluation scores and the file sizes of generated code over the four-hour period. In some instances, such as o4-mini-high on problem ahc045, the public evaluation score showed little improvement over time. Code file sizes also varied significantly depending on the model and the problem, as can be observed in the figures. Execution log analysis revealed distinct reasons for Gemini 2.5 Pro and DeepSeek-R1 scoring below the brown performance tier on one or two problems. Gemini 2.5 Pro occasionally produced solutions running very close to the time limit (e.g., 1.98s for 2s limit). While passing most test cases (ACCEPTED ), slight execution time fluctuations, common in competitive programming, caused few 29 TIME_LIMIT_EXCEEDED results. This led to zero score under ALE-Bench rules, lowering overall performance. Such submissions are generally discouraged in AtCoder contests due to potential execution time variations; hence, we report these results as observed. DeepSeek-R1, conversely, exhibited issues with instruction following. For some problems, it frequently failed to generate the requested summary of its attempts. Consequently, prior work was not effectively leveraged in subsequent iterations, leading to low performance as the four-hour limit elapsed without substantial progress. The six problems where o4-mini-high scored 2000 were all short-format contests amenable to simulated annealing. Other AIs also predominantly scored 2000 on such problems. notable exception was DeepSeek-R1 on ahc035, which required designing an ad-hoc evaluation function. The ahc035 involved creating seed (game state) maximizing sum of evaluation criteria. key insight was to focus on each criterions maximum possible value and preserve these during seed cultivation. DeepSeek-R1s solution appeared to greedily select seeds with many maximum-value components and then greedily place/configure them to maximize bit-wise OR of adjacent pairs (a problem objective). This strategy aligns with reasonable baseline. Higher-ranking humans often refined such greedy approaches, e.g., using simulated annealing for placement or incorporating non-maximum items into scoring. DeepSeek-R1s solution implemented fundamental aspects for decent score but lacked these advanced optimizations. C.4 Details of Evaluating Scaffolding Experiment Setup. In this set of experiments, we compared different scaffolding systems. Specifically, we evaluated the existing OpenHands system (Version 0.34.0) [33] and our proposed ALE-Agent, which is detailed in Section 4. All experiments in this section were conducted under the same time constraints as the iterative-refinement setting: maximum of four hours per problem or the problems original contest time limit, whichever was shorter. Our first scaffolding experiment evaluated the CodeActAgent from OpenHands (v0.34.0) on the full ALE-Bench. OpenHands allows file system interaction; solutions are typically delivered as files. We instructed the agent to save its final C++ code to submission.cpp for evaluation. The agent attempted each problem until the time limit or voluntary exit. The content of submission.cpp at termination (or timeout) was used for final evaluation. If the CodeActAgent requested user input, we performed public evaluation on the current submission.cpp and provided the feedback. The ALE-Bench C++20 Docker image served as the OpenHands code sandbox. The prompt included allowed external libraries, compilation, and execution commands. As CodeActAgent supports browser interaction, we ran the ALE-Bench local visualization server. Feedback to the agent included the visualizers port and instructions for data input. The tailored prompts for OpenHands are provided below. The Initial Instruction Prompt (OpenHands) initiated tasks. If user input was requested, the Refinement Instruction Prompt (OpenHands) was used. For public evaluation feedback, the standard Feedback for Non-Accepted Code was used for non-ACCEPTED results, and Feedback for Accepted Code (OpenHands) for ACCEPTED results. Initial Instruction Prompt (OpenHands) You are world - class algorithm engineer , and you are very good at programming . Now , you are participating in programming contest . You are asked to solve heuristic problem , known as an NP - hard problem . The duration of the contest is $ { Duration for the problem here } hours . There is problem statement at the end of this message . First , please analyze the problem statement . Please think about the essential points of the problem and possible algorithms to get higher rank in the contest . Next , please implement your solution in ** ++20 ( gcc 12.2.0) **. Your solution code should be written in the ** `/ workspace / solution . cpp `**. You can use external libraries as follows : $ { XT RN L_ IB AR ES } You can run your code with the following commands if you are at the `/ workspace ` directory : ``` bash # Compile 30 ++ -12 - std = gnu ++20 - O2 - DONLINE_JUDGE - DATCODER - Wall - Wextra - mtune = native - march = native - fconstexpr - depth =2147483647 - fconstexpr - loop - limit =2147483647 - fconstexpr - ops - limit =2147483647 -I / opt / ac - library -I / opt / boost / gcc / include -L / opt / boost / gcc / lib -o solution . out solution . cpp - lgmpxx - lgmp -I / usr / include / eigen # Run ./ solution . out ``` After you implement your solution , ** you must ask user ( not exit ) to evaluate your temporary solution ** whether your solution is effective or not . The user will read your code from `/ workspace / solution . cpp ` file . If the user can not find your solution file , the user will ask you to provide your code again . Please make sure that your code is exist in proper way . The user can run your solution code with 50 public cases and give you feedback including the score if the code is accepted or the error message if the code is not accepted . ** Even if your solution get accepted , you must refine your solution or try another approach to get better score .** You can use the internet to search an algorithm or library to solve the problem . But you must implement your solution by yourself ( no plagiarism ) and do not see any contents related to this problem . More specifically , you must not see web pages that contain the word \" AtCoder \" , \" AHC \" , \" $ { Problem name here }\" or contents considered to be directly related to this problem because some people were already solved this problem . We will check web pages you visited in order to detect the plagiarism and cheating . If we detect that you are violating this rule , you will be disqualified from the contest . [ Problem statement ] Execution time limit : $ { TIME_LIMIT } sec / Memory limit : $ { MEMORY_LIMIT } MB $ { PRO BLE M_ STA TEM ENT } Feedback for Accepted Code (OpenHands) [ Public test result ] Overall judge result : ACCEPTED Overall absolute score : $ {( Overall score is provided here .) } $ - Case 1: $ {( Score for case 1 is provided here .) } $ - Case 2: $ {( Score for case 2 is provided here .) } $ ...( omission ) ... - Case $N : $ {( Score for case $N is provided here .) } $ You can see your solution in ` http ://172.17.0.1: $ { Visualizer port number is provided here .}? lang = en ` to visualize your solution . If you want to visualize your solution in the local web UI , please access the link above . Then , please detect the Input field in the web page ( recognize its \" bid \") and paste ``` txt $ { Input string for case 1 is provided here .} ``` to the input field . Also , please detect the Output field in the web page ( recognize its \" bid \") and paste ``` txt $ { Output string for case 1 is provided here .} ``` to the output field in the page . If you do this correctly , you can see the visualization of your solution under the Input and Output fields in the web page . This visualization will help you to understand the problem and the features of your solution . Refinement Instruction Prompt (OpenHands) $ { The feedback for the latest code here .} Based on the above feedback , please consider the ways to improve your solution . Firstly , please analyze this given feedback and list what insights can be gained from it . Then , based on the insights , please refine your code to achieve better performance . It can be simple bug fix , the introduction of new algorithm , or any degree of change from minor to major . If you think you did your best on the task , please finish the interaction . Again , you should implement your entire solution in the `/ workspace / solution . cpp ` file . Figure A5: The actual search tree of ALE-Agent on ahc039. Each node represents one generated answering program with its public evaluation result. These OpenHands experiments used the full ALE-Bench. Three c6i.32xlarge instances ran in parallel, each dedicated to one of three models: GPT-4.1 mini, o4-mini-high, and Gemini 2.5 Pro. Each model processed all 40 problems, with up to four problems per instance concurrently. Although the time limit was four hours per problem, frequent agent exits before the limit allowed completion within one day. Next, we evaluated our ALE-Agent using the lite version of ALE-Bench. Three c6i.32xlarge instances ran experiments for three ALE-Agent configurations in parallel: (1) Base, (2) Base + Method 1, and (3) Base + Method 1&2 (details in Appendix B). Each configuration processed all 10 problems from the lite ALE-Bench. With up to four problems concurrently per instance (subject to the 4-hour/contest duration limit), these ALE-Agent experiments took approximately half day. Results. To compare results across benchmark versions (e.g., for OpenHands which was run on the full version but compared against ALE-Agent on the lite version), we converted full version results to their lite version equivalents. This conversion reuses the private evaluation outcomes from the full version. Specifically, for each of the 10 problems included in the lite version, we extracted the results corresponding only to the test cases used in the lite version. Based on these subsetted results, we then recomputed the rank and performance score for each model on each of these 10 problems as if they were evaluated only on the lite versions test cases. While the main paper discusses the results for ALE-Agent (on the lite version) and the lite versionconverted results for OpenHands, Table 4 in this appendix presents the original full version results for OpenHands, shown alongside the iterative-refinement experiment results (which were also on the full version, initially summarized in Table 3). Furthermore, detailed per-problem performance for the ALE-Agent experiments (on the lite version) is provided in Table A6. The ALE-Agent (Base + Method 1&2) achieved performance score of 2880 on AHC039, corresponding to 5th place in the original contest. We submitted this solution to AtCoder for verification.12 Analysis revealed that the solution employed Simulated Annealing, technique LLMs also showed proficiency with in the iterative-refinement setting (cf. o4-mini-high). The official contest leaderboard13 confirms this score matches 5th place performance in the actual AHC039, indicating ALE-Benchs scoring reproducibility. Figure A5 illustrates ALE-Agents search tree for AHC039. Though configured to generate 30 child nodes (alternative solutions/refinements) per parent, and requests were dispatched with slight delays to manage API rates, occasional API call failures (e.g., Gemini internal errors) meant the target of 30 expanded child nodes was not always met. C.5 Further Analysis C.5.1 Correlations between Full Version and Lite Version We used all 73 results from the full ALE-Bench: 22 models 3 languages (one-shot), 4 models 1 language (C++20, iterative-refinement), and 3 models 1 language (C++20, OpenHands scaffolding). By correlating these with their lite version-converted counterparts (see Appendix C.4), we assessed the lite versions validity for comparative AI system evaluations. Figure A6 plots average performance on the full vs. lite versions for these 73 data points. Pearsons = 0.9174, Spearmans ρ = 0.8677, and Kendalls τ = 0.6963 (all < 0.001) indicate strong, statistically significant correlations. These results show strong correlation. However, the regression slope in Figure A6 is 0.7. This, and the 12https://atcoder.jp/contests/ahc039/submissions/65734651 (Retrieved: May 15, 2025) 13https://atcoder.jp/contests/ahc039/standings/extended (Retrieved: May 15, 2025) Login required. Figure A6: Correlation of the average performance between the lite and full versions. The red band shows the 95 % confidence interval estimated by 10000 bootstrap samples with random seed zero. lite versions design (intentionally including harder problems, see Appendix A.1), suggests the lite version is, on average, more difficult. Comparing relative AI strengths within the same version (full or lite) is valid while directly comparing across versions is not recommended as it could be misleading. C.5.2 Real Contest Participation With AtCoders permission, an in-development ALE-Agent participated in AHC046 in real time via dedicated account (fishylene). The agent used only Method 2 (search tree exploration), constructing two parallel search trees, each with target width of 50 child nodes per expansion. It checked its best solution every five minutes, submitting to AtCoder only upon improvement. The ALE-Agent placed 154th, with performance score of 1915, as per the official contest leaderboard.14 C.6 Limitations of Experiments We have reported on comprehensive suite of experiments conducted on current LLMs and scaffolding with our proposed benchmark. While these investigations offer valuable insights into their respective capabilities and the utility of our benchmark, we acknowledge the following limitations that provide avenues for future research: (1) Limited statistical robustness. The findings presented are based on single experimental runs for each configuration. While these initial results are promising, we acknowledge that further experiments involving multiple runs are essential to establish the statistical validity of the performance claims and account for potential variability. (2) Insufficient verification of multimodal/agent performance. We have not used the image input function in our experiments, and further verification is needed in this area. In addition, we provide several other features, such as input case generation and visualizers, in addition to public eval/private eval. However, we have not conducted sufficient experiments to explicitly utilize these features using LLMs tool usage feature. ALE-Bench is still in the exploratory stage of development, and further investigation is needed. 14https://atcoder.jp/contests/ahc046/standings (Retrieved: May 15, 2025) Login required. 33 Table A3: Comparison of frontier LLMs in the one-shot setting. We report all results in Section 5.1. Model Code Language Average Perf. Perf. Distribution (%) Rating Cost ($) short long overall 400 800 1200 1600 2000 2400 2800 raw rank (%) /problem /response Non-Reasoning Models: GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o mini GPT-4o GPT-4o GPT-4o GPT-4o GPT-4.1 nano GPT-4.1 nano GPT-4.1 nano GPT-4.1 nano GPT-4.1 mini GPT-4.1 mini GPT-4.1 mini GPT-4.1 mini GPT-4.1 GPT-4.1 GPT-4.1 GPT-4.1 Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B Gemini 1.5 Flash Gemini 1.5 Flash Gemini 1.5 Flash Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Gemini 1.5 Pro Gemini 1.5 Pro Gemini 2.0 Flash-Lite Gemini 2.0 Flash-Lite Gemini 2.0 Flash-Lite Gemini 2.0 Flash-Lite Gemini 2.0 Flash Gemini 2.0 Flash Gemini 2.0 Flash Gemini 2.0 Flash Claude 3.5 Haiku Claude 3.5 Haiku Claude 3.5 Haiku Claude 3.5 Haiku Claude 3.5 Sonnet Claude 3.5 Sonnet Claude 3.5 Sonnet Claude 3.5 Sonnet Claude 3.7 Sonnet Claude 3.7 Sonnet Claude 3.7 Sonnet Claude 3.7 Sonnet DeepSeek-V3 DeepSeek-V3 DeepSeek-V3 DeepSeek-V3 C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average C++20 Python3 Rust Average Reasoning Models: o1-high o1-high o1-high o1-high o3-mini-high o3-mini-high o3-mini-high o3-mini-high o3-high o3-high o3-high o3-high o4-mini-high o4-mini-high o4-mini-high o4-mini-high Gemini 2.5 Flash Gemini 2.5 Flash Gemini 2.5 Flash Gemini 2.5 Flash Gemini 2.5 Pro Gemini 2.5 Pro Gemini 2.5 Pro Gemini 2.5 Pro Claude 3.7 Sonnet (Thinking) C++20 Claude 3.7 Sonnet (Thinking) Python3 Claude 3.7 Sonnet (Thinking) Rust Claude 3.7 Sonnet (Thinking) Average DeepSeek-R1 DeepSeek-R1 DeepSeek-R1 DeepSeek-R1 C++20 Python3 Rust Average 442 401 389 411 547 419 500 488 443 300 454 399 779 634 759 724 696 831 734 754 365 208 129 234 344 531 260 378 457 467 430 451 466 423 394 428 547 453 482 494 464 620 439 508 744 782 633 720 851 762 818 810 638 440 617 565 768 531 789 696 615 820 726 720 1116 1135 1119 1123 866 866 981 904 905 589 639 711 938 620 857 805 911 884 972 923 713 711 786 736 2.5 0.0 0.0 0.8 2.5 0.0 0.0 0.8 0.0 0.0 0.0 0.0 5.0 0.0 2.5 2.5 7.5 12.5 10.0 10.0 0.0 0.0 0.0 0.0 0.0 2.5 0.0 0.8 0.0 0.0 2.5 0.8 5.0 0.0 0.0 1.7 5.0 2.5 0.0 2.5 0.0 5.0 2.5 2.5 7.5 5.0 5.0 5.8 17.5 7.5 15.0 13.3 10.0 0.0 2.5 4. 5.0 0.0 10.0 5.0 0.0 5.0 7.5 4.2 22.5 27.5 27.5 25.8 7.5 7.5 10.0 8.3 20.0 5.0 10.0 11.7 25.0 20.0 20.0 21.7 17.5 7.5 15.0 13.3 12.5 10.0 15.0 12.5 420 494 314 409 636 455 644 578 489 315 364 389 755 732 731 739 746 753 728 742 471 410 258 379 575 530 361 489 596 673 536 602 432 451 406 430 585 555 597 579 457 619 400 492 674 676 538 629 810 809 698 772 688 412 581 560 823 641 732 732 742 677 743 721 946 971 987 968 808 889 747 815 827 646 424 632 688 847 618 718 792 881 876 850 822 706 541 690 433 441 357 410 585 434 561 527 462 306 416 395 769 676 747 730 717 798 732 749 410 294 184 296 442 530 303 425 516 555 475 515 451 435 399 428 563 496 531 530 461 620 423 501 715 737 593 681 833 782 767 794 659 428 602 563 791 578 765 711 669 759 733 721 1044 1066 1063 1057 841 876 882 866 872 613 548 678 832 717 756 768 860 883 931 892 760 709 682 717 50.0 52.5 45.0 49.2 75.0 55.0 72.5 67.5 60.0 40.0 57.5 52.5 90.0 85.0 87.5 87.5 80.0 87.5 80.0 82.5 55.0 30.0 25.0 36.7 57.5 67.5 35.0 53.3 62.5 70.0 50.0 60.8 55.0 55.0 50.0 53.3 62.5 55.0 77.5 65.0 65.0 77.5 52.5 65.0 85.0 82.5 72.5 80.0 90.0 82.5 80.0 84.2 75.0 50.0 70.0 65. 90.0 75.0 87.5 84.2 90.0 95.0 95.0 93.3 97.5 100.0 97.5 98.3 92.5 97.5 97.5 95.8 82.5 65.0 55.0 67.5 82.5 70.0 70.0 74.2 90.0 92.5 97.5 93.3 75.0 75.0 72.5 74.2 17.5 20.0 10.0 15.8 25.0 20.0 30.0 25.0 12.5 5.0 5.0 7.5 47.5 35.0 45.0 42.5 45.0 47.5 52.5 48.3 5.0 15.0 0.0 6.7 20.0 22.5 12.5 18.3 30.0 27.5 27.5 28.3 17.5 20.0 20.0 19.2 35.0 30.0 15.0 26.7 20.0 32.5 15.0 22.5 40.0 45.0 30.0 38.3 55.0 52.5 47.5 51.7 42.5 20.0 32.5 31.7 52.5 32.5 50.0 45.0 35.0 42.5 35.0 37.5 82.5 80.0 80.0 80.8 60.0 67.5 67.5 65.0 62.5 47.5 30.0 46.7 60.0 52.5 57.5 56.7 57.5 70.0 65.0 64.2 47.5 47.5 47.5 47.5 34 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.5 0.0 0.0 0.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.5 0.0 0.0 0.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 1.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 2.5 7.5 5.0 0.0 0.0 0.0 0.0 5.0 0.0 2.5 2.5 5.0 0.0 5.0 3.3 2.5 0.0 5.0 2.5 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 841 807 751 800 936 802 887 875 820 657 752 743 1135 946 1047 1043 1164 1205 1153 1174 698 787 483 656 810 869 793 824 892 927 1017 945 983 899 788 890 1031 1001 840 957 821 1043 813 892 1092 1114 993 1066 1197 1144 1212 1184 1142 930 1024 1032 1143 938 1137 1073 906 1091 1052 1016 1456 1424 1532 1471 1194 1120 1215 1176 1422 1157 1237 1272 1373 1285 1293 1317 1328 1198 1345 1290 1206 1184 1238 1209 86.7 88.2 90.4 88.4 80.1 88.5 83.6 84.1 87.5 94.0 90.4 90.6 67.4 79.7 73.4 73.5 65.1 62.3 66.1 64.5 91.9 89.0 98.2 93.0 87.9 84.5 88.8 87.1 83.3 80.9 75.6 80.0 77.7 82.8 88.9 83.2 74.6 76.8 86.7 79.4 87.4 73.6 87.7 82.9 70.5 68.9 77.1 72.2 63.2 66.8 61.8 63.9 66.8 80.5 75.1 74.1 66.8 80.0 67.2 71.3 82.4 70.7 73.0 75.4 43.2 45.4 38.0 42.2 63.6 68.6 61.5 64.6 45.5 65.8 59.9 57.1 49.3 55.8 54.9 53.3 52.3 63.1 51.0 55.4 62.3 64.0 59.8 62.0 0.006 0.004 0.006 0.005 0.048 0.064 0.066 0.059 0.004 0.004 0.004 0.004 0.009 0.008 0.010 0.009 0.083 0.073 0.107 0.088 0.002 0.001 0.002 0.002 0.002 0.002 0.003 0.002 0.044 0.020 0.033 0.032 0.006 0.004 0.005 0.005 0.006 0.005 0.005 0.005 0.043 0.030 0.042 0.038 0.123 0.099 0.126 0.116 0.287 0.268 0.427 0.328 0.008 0.010 0.010 0.009 0.579 0.945 0.698 0.741 0.041 0.061 0.044 0.049 0.734 0.677 0.801 0.738 0.041 0.044 0.048 0.045 0.194 0.283 0.316 0.264 0.472 0.572 0.527 0.524 0.375 0.314 0.367 0.352 0.063 0.062 0.056 0. 0.002 0.001 0.002 0.002 0.022 0.021 0.024 0.022 0.001 0.001 0.001 0.001 0.005 0.005 0.005 0.005 0.035 0.031 0.037 0.034 0.001 0.000 0.000 0.000 0.001 0.001 0.001 0.001 0.018 0.009 0.011 0.013 0.002 0.001 0.001 0.001 0.002 0.002 0.002 0.002 0.013 0.011 0.013 0.012 0.055 0.039 0.040 0.045 0.142 0.126 0.157 0.142 0.003 0.004 0.004 0.004 0.464 0.420 0.451 0.445 0.037 0.042 0.039 0.039 0.506 0.423 0.501 0.477 0.037 0.034 0.039 0.037 0.104 0.098 0.092 0.098 0.242 0.183 0.188 0.204 0.170 0.167 0.181 0.173 0.028 0.025 0.022 0.025 Table A4: Statistics for the actual AtCoder users who participated in contests 5 or more times. Standard deviations in this table are calculated with using the number of users as the denominator (i.e., ddof = 0). #Users Perf. Distribution (%) Average Perf. #Attendance Rating [400, 500) [450, 550) [500, 600) [550, 650) [600, 700) [650, 750) [700, 800) [750, 850) [800, 900) [850, 950) [900, 1000) [950, 1050) [1000, 1100) [1050, 1150) [1100, 1200) [1150, 1250) [1200, 1300) [1250, 1350) [1300, 1400) [1350, 1450) [1400, 1500) [1450, 1550) [1500, 1600) [1550, 1650) [1600, 1700) [1650, 1750) [1700, 1800) [1750, 1850) [1800, 1900) [1850, 1950) [1900, 2000) [1950, 2050) [2000, 2100) [2050, 2150) [2100, 2200) [2150, 2250) [2200, 2300) [2250, 2350) [2300, 2400) [2350, 2450) [2400, 2500) [2450, 2550) [2500, 2600) 24 32 52 62 80 81 75 100 126 144 133 139 151 149 156 171 188 177 165 157 154 159 162 155 136 121 102 102 87 83 78 60 64 56 58 57 47 47 45 34 30 30 22 mean 5.67 5.72 6.12 6.29 6.86 7.35 7.48 7.86 8.11 8.44 8.86 8.51 9.26 10.63 10.59 10.88 11.34 11.45 12.29 12.80 13.75 15.29 16.27 16.60 16.27 17.26 19.07 20.57 21.92 21.71 23.33 25.47 24.31 26.88 27.17 27.09 28.53 31.51 32.02 28.35 28.10 27.67 31.32 SD (σ) 1.03 1.40 1.83 2.03 2.55 2.73 2.62 3.34 3.24 3.79 4.37 3.64 4.96 5.97 5.56 5.54 5.64 5.95 6.35 6.56 7.44 8.07 8.48 8.77 8.45 9.21 10.17 9.04 8.41 9.55 10.57 9.61 10.14 11.19 10.70 12.77 12.94 11.50 11.67 11.27 10.62 10.45 11.18 mean 614 655 688 725 752 760 816 863 897 933 967 1013 1045 1066 1099 1135 1168 1210 1238 1266 1287 1301 1348 1380 1408 1434 1458 1479 1487 1550 1578 1606 1643 1653 1729 1782 1786 1808 1854 1947 1993 2030 2066 SD (σ) 400 2000 2400 127 101 113 121 118 127 132 125 116 128 137 121 133 145 143 149 147 146 150 167 169 166 160 159 158 145 174 169 152 175 167 126 130 135 182 201 181 148 157 202 232 180 127 73.0 81.3 83.4 85.5 89.8 89.6 90.3 93.3 95.1 95.0 94.9 96.4 96.8 96.5 96.8 97.0 97.5 97.7 97.6 98.0 97.8 97.2 97.7 98.1 97.9 98.0 98.3 98.8 98.7 98.3 98.1 98.6 99.0 98.8 98.6 98.3 98.4 98.6 99.0 99.7 99.7 99.3 99.2 0.0 0.0 0.0 0.0 0.0 0.0 0.3 0.4 0.3 0.6 1.6 2.5 4.0 4.4 5.7 8.8 10.6 13.5 16.3 18.7 20.8 22.3 26.3 30.5 33.9 35.2 38.6 41.0 41.5 46.8 49.4 52.1 55.2 56.2 63.2 67.5 66.9 70.1 72.0 76.4 79.3 80.5 82.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.4 0.4 0.6 1.4 2.2 1.6 2.8 4.5 4.7 6.5 7.5 8.1 9.4 10.3 14.7 16.7 18.6 21.5 22.5 30.2 36.6 36.5 37.4 40.4 45.8 48.6 53.7 59. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.2 0.2 0.6 1.2 0.8 0.8 2.0 2.2 2.1 3.2 4.0 5.8 8.3 9.1 9.1 12.0 17.1 20.9 24.5 27.0 Table A5: The full version result with scaffolding. The experiment of ALE-Agent with the full version setting was not conducted. Scaffolding Average Perf. Perf. Distribution (%) Rating Cost ($) short long overall rank (%) 400 800 1200 1600 2000 2400 2800 raw rank (%) /problem /response Sequential Refinement: 1293 GPT-4.1 mini 1677 o4-mini-high 1389 Gemini 2.5 Pro 1268 DeepSeek-R1 OpenHands [33]: GPT-4.1 mini o4-mini-high Gemini 2.5 Pro 687 991 923 1114 1307 1301 1155 1217 1520 1352 51.5 100.0 22.3 100.0 95.0 36.8 97.5 51.1 540 818 890 625 918 909 96.8 81.6 82.4 80.0 85.0 82.5 95.0 97.5 92.5 87. 32.5 72.5 67.5 42.5 87.5 62.5 50.0 2.5 22.5 30.0 17.5 32.5 27.5 15.0 0.0 2.5 2.5 2.5 15.0 7.5 5. 0.0 2.5 0.0 0.0 5.0 5.0 2.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1636 2104 1960 996 1469 1386 30.5 2.137 0.010 11.8 7.174 0.047 15.7 11.126 0.134 18.3 1.141 0.024 77.0 0.114 0.005 42.2 2.321 0.053 48.2 3.331 0.149 35 Table A6: Performance for each problem. We report the performance of each problem from results in Section 5.2 and Section 5.3. Rows with gray background correspond to long contests, while white rows correspond to short contests. Problem Sequential Refinement GPT-4.1 mini o4-mini-high Gemini 2.5 Pro DeepSeek-R1 ahc001 ahc002 ahc003 ahc004 ahc005 future-contest-2022-qual ahc006 ahc007 ahc008 ahc009 ahc010 ahc011 ahc012 ahc014 ahc015 ahc016 ahc017 ahc019 ahc020 ahc021 toyota2023summer-final ahc024 ahc025 ahc026 ahc027 ahc028 ahc030 ahc031 ahc032 ahc033 ahc034 ahc035 ahc038 ahc039 ahc040 ahc041 ahc042 ahc044 ahc045 ahc046 1016 1168 1769 1723 1665 1518 1322 1021 1151 1077 1167 1148 977 1016 1237 506 1241 893 1395 1633 1018 717 1210 1003 1074 1868 1037 979 1202 959 1769 1257 1038 983 1113 2049 1255 1124 1275 1119 1288 1238 1602 1726 2107 1552 2472 1249 1217 791 1565 1562 2236 1053 1317 1515 1224 1256 2545 1319 1662 1283 1209 1402 1358 1986 1017 1333 1361 959 1793 1638 1244 1686 1383 2306 1181 2150 1452 1760 1205 1885 1155 1673 1702 2456 1906 1182 2446 1441 1403 123 999 1415 966 1190 1016 -70 1619 986 1517 1216 1313 1374 1912 1038 1549 1432 971 1020 1592 1218 817 1187 2372 1253 1628 1469 737 383 842 1632 1560 1582 1530 2440 854 1006 841 1099 1221 674 1254 1142 1514 1260 1009 1726 1481 1062 1320 1210 712 557 1868 1356 1299 936 848 915 2348 919 1658 1477 1203 594 1194 1162 1119 ALE-Agent + Method 1 + Method 1&2 1061 1531 1315 1199 1830 886 1320 719 2039 737 1189 1652 2446 1457 1980 1331 1965 1740 2880 Base 1075 1447 1265 1262 1243 1113 712 1168 1661"
        }
    ],
    "affiliations": [
        "AtCoder, Japan",
        "Sakana AI, Japan",
        "The University of Tokyo, Japan"
    ]
}