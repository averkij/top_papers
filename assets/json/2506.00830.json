{
    "paper_title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers",
    "authors": [
        "Zhengcong Fei",
        "Hao Jiang",
        "Di Qiu",
        "Baoxuan Gu",
        "Youqiang Zhang",
        "Jiahua Wang",
        "Jialin Bai",
        "Debang Li",
        "Mingyuan Fan",
        "Guibin Chen",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, a unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ a hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce a facial mask loss and an audio-guided classifier-free guidance mechanism. A sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct a dedicated data pipeline for curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 0 3 8 0 0 . 6 0 5 2 : r SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers SkyReels Team, Skywork AI Project page: SkyReels-Audio.github.io Figure 1: Given portrait image, text, or video along with audio input, SkyReels-Audio can generate and edit portraits with strong identity consistency, expressive facial and natural body dynamics. In addition, SkyReels-Audio support infinite video generation based on various multimodal controllable clues."
        },
        {
            "title": "Abstract",
            "content": "The generation and editing of audio-conditioned talking portraits guided by multimodal inputs, including text, images, and videos, remains under explored. In this paper, we present SkyReels-Audio, unified framework for synthesizing high-fidelity and temporally coherent talking portrait videos. Built upon pretrained video diffusion transformers, our framework supports infinite-length generation and editing, while enabling diverse and controllable conditioning through multimodal inputs. We employ hybrid curriculum learning strategy to progressively align audio with facial motion, enabling fine-grained multimodal control over long video sequences. To enhance local facial coherence, we introduce facial mask loss and an audio-guided classifier-free guidance mechanism. sliding-window denoising approach further fuses latent representations across temporal segments, ensuring visual fidelity and temporal consistency across extended durations and diverse identities. More importantly, we construct dedicated data pipeline for Technical Report curating high-quality triplets consisting of synchronized audio, video, and textual descriptions. Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior performance in lip-sync accuracy, identity consistency, and realistic facial dynamics, particularly under complex and challenging conditions. The model, along with demonstration videos, will soon be made publicly available at the project website: https://www.skyreels.ai."
        },
        {
            "title": "Introduction",
            "content": "Recent developments in computer vision and human-computer interaction have highlighted the transformative potential of generating realistic digital avatars responsive to audio stimuli. These avatars, informed by static imagery, video sequences, or textual metadata, are poised to impact wide range of domains, including digital storytelling, immersive education, and virtual communication platforms [43, 66, 29, 54, 1, 28, 26, 42]. foundational challenge in this domain lies in achieving precise, audio-conditioned modulation of avatar behavior, ensuring seamless integration with multimodal contextual signals. Robust synchronization across modalities is critical to enhancing user immersion and shaping future paradigms of interactive digital media consumption. The field of talking head synthesis has witnessed significant progress through the application of advanced neural rendering techniques, such as GANs [21], NeRF [46], and Gaussian Splatting methods [30]. These approaches [57, 56, 76, 4, 78, 44, 71, 2, 49, 10, 5, 36, 83, 48, 34, 41, 22, 64] have enabled the effective fusion of facial motion cues with static identity characteristics, producing highly realistic visual outputs. However, traditional parametric models [60, 37] often fall short in capturing the full range of expressive motion, and conventional rendering methods continue to impose constraints on spatial resolution and output quality. The advent of diffusion-based generative models [55] has markedly improved the realism, diversity, and controllability of video synthesis [3, 23, 72, 67, 75, 82, 50, 32, 62, 6, 20, 19]. Notably, recent approaches [59, 63, 8, 73, 27, 80, 52] have demonstrated that diffusion priors are well-suited for disentangling spatial appearance from temporal dynamics, resulting in improved motion continuity. Despite these advances, unified frameworks that integrate multimodal controls, particularly audio and text for talking head generation, remain underexplored. In particular, comprehensive audio-conditioned diffusion model for robust longrange generation and editing across diverse portrait domains has yet to be fully investigated. This paper introduces SkyReels-Audio, omni audio-conditioned framework designed to generate and edit temporally consistent and visually realistic talking portrait videos using pretrained video diffusion transformers. The proposed system achieves high perceptual fidelity across broad range of facial expressions and motion patterns, maintaining coherence even under diverse and dynamic conditions. SkyReels-Audio facilitates portrait animation and editing by concurrently leveraging speech signals alongside text, images, and video inputs. In this multimodal configuration, the audio primarily governs articulatory movements, while auxiliary modalities influence expressive behaviors, physical gestures, and environmental transformations. We use Whisper [53] to encode audio information, which features are further aligned with video representations with audio 1D RoPE. These audio tokens are integrated into the denosing network via cross-attention mechanism that allows effective fusion with video tokens. To ensure robust audio control and multimodal integration, we employ hybrid curriculum training strategy that incrementally conditions the model using joint optimization objectives. During inference, bidirectional latent fusion algorithm is employed, enabling to produce indefinitely long video sequences while preserving temporal continuity and rendering quality across various speaker identities and settings. Additionally, we perform comprehensive evaluation using both custom-designed benchmark and established public datasets. Our benchmark comprises over 50 audio-driven scenarios encompassing multiple languages and variable speaking styles and includes both quantitative metrics and human subjective assessments to capture perceptual and technical performance. Under these settings, SkyReels-Audio demonstrates strong generalization capabilities across diverse set of portrait images and video samples. The model effectively responds to audio-driven cues and textual instructions while preserving high visual quality and natural motion. Furthermore, it maintains consistent performance at varying sequence lengths, enabling efficient and scalable inference for real-world applications. In summary, out contributions are listed as follows: 2 We present SkyReels-Audio, unified framework for audio-conditioned talking portrait generation and editing based on pretrained Video Diffusion Transformers. It employs bidirectional latent fusion strategy to generate temporally coherent and visually consistent long-form videos across diverse speaking styles and contexts. We integrate hybrid learning paradigm that combines image and video-based multimodal controls to improve model capacity and generalization. It introduces facial region-weighted loss to balance local audio-driven articulation with global conditioning, enabling precise synthesis. We construct our approach on carefully curated dataset of high-quality, temporally aligned audio-video-text triplets, supported by comprehensive data pipeline, which facilitates effective multimodal learning. Experiments demonstrate that SkyReels-Audio achieves robust performance on diverse portrait inputs ranging from static images to dynamic videos under both speech and text conditioning, with efficient inference across variable sequence lengths."
        },
        {
            "title": "2 Method",
            "content": "The goal of this work is to enable realistic portrait animation and editing guided by both speech audio and auxiliary multimodal inputs, including text, static imagery, or video references. The generated output is expected to retain the visual identity of the target subject as defined by the conditioning inputs, while faithfully capturing speech-related articulations such as lip synchronization, facial expressions, and head movements. 2.1 Preliminaries Flow Matching. Flow Matching [39] provides principled framework for mapping complex probability distributions to simpler ones by leveraging their probability density functions, thereby enabling sample generation via learned inverse transformations. Several recent works [14, 32, 6, 62] serve as subclass of Flow Matching models that operate in the latent space, leveraging pre-trained AutoEncoder [31] to facilitate this process. In contrast to conventional text-to-video models that rely exclusively on textual conditioning Ts, SkyReels-Audio incorporates multimodal conditioning signals, including the driving audio sequence (A), static portrait image (Is), reference video clip (Vs), and the associated text prompt (Ts). Importantly, during training, any of the conditioning modalities may be randomly omitted to promote robust generalization and flexible inference. The model optimize to learn the reverse transformations with the objective, Lmse = Ez0,z1,t[0,1] (cid:20)(cid:13) (cid:13) (cid:13)vt uθ where uθ is trainable denoising net. z1 and z0 notes the latent embedding of the training sample and the initialized noise sampled from the Gaussian distribution (0, 1). zt is the training sample constructed using linear interpolation. Velocity vt = dzt/dt = z1 z0 serves as the regression target for the model. (cid:0)zt, t, Ts, Is, Vs, A(cid:1)(cid:13) 2 (cid:13) (cid:13) (cid:21) , (1) Video Diffusion Transformers. The Diffusion Transformer represents class of generative models built upon the Transformer architecture [47], exhibiting strong performance in video synthesis tasks by leveraging full spatio-temporal attention in three dimensions [32, 6, 62]. In this work, we adopt SkyReels-V2 [6] as the core backbone. It incorporates causal 3D VAE to perform joint compression across temporal and spatial axes, enabling efficient video encoding. To follow textual guidance, it utilizes UMT5 to generate semantic embeddings from natural language inputs. These text features are then injected into the diffusion net via cross-attention layers, allowing conditioned video generation based on linguistic context. Additionally, temporal information is incorporated by predicting six distinct modulation parameters corresponding to each timestep, ensuring precise temporal alignment throughout the generative process. 2.2 Model Architecture To enable unified control over both images and videos, we generalize the original image-to-video generation framework into omni architecture, as illustrated in Figure 2. Specifically, 3D VAE Figure 2: Overview of SkyReels-Audio. Whisper encodes resampled audio and fuse video tokens with cross-attention layers. Image and video controls are joint featured with VAE before combine with input noise to provide video identity and environment priors. is employed to extract latent visual features, which are concatenated with noise tensor along the channel dimension. To distinguish between static and dynamic inputs, we incorporate binary temporal mask that encodes modality-specific information, allowing the model to differentiate between image and video sequences. For conditioning on audio, audio condition module processes the input wav signal with the Whisper feature extractor [53], which performs resampling and feature encoding. The resulting audio representations are passed through the Whisper encoder to obtain discrete token embeddings. These audio tokens are then injected into the video DiT using dedicated cross-attention layers, which is strategically placed at the end of the decoupled cross attention blocks to modulate video generation based on the driving audio signal. To improve the alignment between audio and visual modalities, we adopt RoPE [58], which is particularly effective at capturing distance-aware relationships and generalizing to variable sequence lengths. Audio features are treated as one-dimensional sequences with shape [1, Laudio], and the 1D RoPE is added accordingly in attention operation. This technique enhances both intra-modal coherence and cross-modal correspondence, contributing to more accurate lip synchronization and improved semantic consistency in generated visual content. 2.3 Hybrid Learning Strategy For interleaved image animation or video editing tasks, we adopt hybrid learning strategy that further improves audio-motion alignment. Experiments revealed that when employing the joint training strategy, even with T2V model as the base model, satisfactory image animation results could still be obtained. In contrast, training the image animation task alone often required longer convergence times and sometimes failed to produce correct results. Furthermore, in the image animation task, the control signals are limited to text, images, and audio. Given the strong correlation between audio and lip movements, the model more readily learns audio-driven synthesis. Conversely, in the video editing task, the inclusion of additional video control conditions introduces dependencies between lip movements and surrounding regions. The presence of these peripheral motions tends to diminish the influence of audio control. Therefore, we employ masks to differentiate between the Image Animation and Video Editing tasks. For the Image Animation task, the input image serves as the reference frame and is concatenated with sequence of empty frames to form the video input Vs = (Is, Iempty, ..., Iempty) for the 3D VAE. The corresponding mask sequence VM = (Mones, Mzeros, ..., Mzeros) undergoes max pooling with the same downsampling factor as the 3D VAE and is then concatenated with the VAE output along the channel dimension. In the Video Editing task, the first frame of the input video is treated as the reference frame. For the remaining frames, we detect facial landmarks 4 Figure 3: Illustration of BLF. BLF is tuning-free overlapping sliding window strategy, performing bidirectional fusion of the latents within adjacent windows in the same denoising step. using DWPose and generate lower-face bounding boxes. These frames are then masked based on the BBox to produce the corresponding video and mask sequences Vs = (I0, mask ), Vm = (Mones, Mmouth, ..., Mmouth). Subsequently, the same processing pipeline as in Image Animation is applied. To prioritize the generation quality within the masked regions, we adapt the flow matching loss function by applying distinct weighting factors to the masked and non-masked areas as: , ..., mask 1 Ljoint = w1 downsample Lmse + w2 (1 downsample ) Lmsk (2) To guide the models focus toward the articulation-relevant regions, particularly the lips, we leverage DWPose [74] to extract precise masks in pixel space, which are subsequently transformed into the latent representation space through trilinear interpolation, yielding the constraint mask denoted as Mlip. To maintain balance between local precision in lip synchronization and the preservation of global visual realism, we use probabilistic gating mechanism governed by threshold pmask as: Lf ace = (cid:26)Lmse, Mlip Lmse if pmask otherwise where denotes element-wise multiplication. This adaptive masking strategy ensures that the model places selective emphasis on the lip region during training while retaining the overall structural integrity of the generated portrait. 2. Inference Optimization Audio CFG. Prior research has highlighted the limitations of text-to-image generation models in accurately adhering to input textual descriptions [14, 81]. To address such shortcomings, these approaches often employ inference-time guidance strategies to improve alignment between generated content and conditioning inputs. Motivated by these insights, we propose an Audio-Guided Conditional Sampling mechanism to enhance synchronization with driving audio signals during inference. The adjusted denoising function incorporating both audio and text guidance is formulated as: ˆuθ cfg = (1 + ωaudio)uθ(zt, t, Ts, Is, Vs, A) ωaudiouθ(zt, t, Ts, Is, Vs, ) + (1 + ωtext)uθ(zt, t, Ts, Is, Vs, ) ωtextuθ(zt, t, , , , ), (3) where ωaudio and ωtext represent the CFG scales specifically designed for audio condition and text condition, respectively. Note that we adopt time-dependent scheduling for these CFG weights, allowing the model to dynamically balance conditioning influences across the diffusion trajectory, thereby improving fidelity and robustness in audio-synchronized portrait generation. Infinite Video Generation via Bidirectional Latent Fusion. We introduce tuning-free infinite video inference strategy, named as Bidirectional Latent Fusion (BLF). During denoising loop, BLF achieves smooth transitions between different video windows by bidirectionally and weightedly fusing video latents. Unlike the motion frames based methods [59, 27, 12], our BLF requires no 5 training support and significantly reduces image quality degradation caused by error accumulation. Compared to non-overlap methods [25], our approach provides greater image stability. Specifically, as illustrated in Figure 3, BLF comprises three key phases: (i) We resample audio seuqences to ensure temporal alignment with video frames, while simultaneously initializing noise vector of corresponding duration. (ii) Within the same denoising step, overlapping latents between adjacent windows is weighted fused through sliding window mechanism, where the fusion weights are linearly interpolated based on the relative frame indices. (iii) The fused latents are reinserted into both participating windows, thereby ensuring that both ends of the middle window are fusion features. This approach effectively achieves smooth transition between different windows through bidirectional feature propagation. The total process is listed in Algorithm 1. Note that we found that the color of the images has probability of becoming darker during long video inference process, and this phenomenon has also been observed in other DiT works. We mitigate this issue by strictly controlling the quality of the training data and performing color-unification post-processing on the generated videos. Algorithm 1 Algorithm of BLF Require: Denoising steps , audio embedding c[0,l] with length l, pretrained DiT model DiT() for sequence length , l, window size consistent with , overlap length between every two windows. and initial noisy latent z[0,l] Initialize start index = 0, end index = + , previous end index eprev = e. while do . 0 z[s,e] , c[s,e] t1 = DiT(z[s,e] if = 0 and = then Ensure: Denoised latent z[0,l] 1: for = T, ..., 1 do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: = zeros(o) for = 1, ..., do end if if = then end for t1 = z[s,s+o] z[s,s+o] wi = i1 o1 , t) 13: eprev e, + (f o), end if end while 14: 15: 16: end for 17: return Denoised latent z[0,l] 0 t1 + (w 1) z[eprevo,eprev] t1 (cid:26) + if + < otherwise Hybrid Inference Strategy Benefiting from the joint training of image animation and video editing tasks, our model supports both image and video inputs during inference. Experimental results demonstrate that when driven by the same audio input, the video generated from single image (i.e., the first frame of video) exhibits superior lip-sync accuracy compared to videos generated from full video inputs. To enhance audio-visual synchronization in video editing tasks, we propose hybrid inference strategy. Early Denoising Steps (First steps): Use the full video input to maintain structural consistency with the source video. Later Denoising Steps (Remaining steps): Switch to image input (first frame only) to refine lip-sync details, while adaptively adjusting the corresponding mask sequence. ut θ = (cid:26)uθ(zt, t, Ts, uθ(zt, t, Ts, s , , , A), , A), if otherwise (4) Model Acceleration. To accelerate the inference process, we implemented the two major optimizations. We employed Teacache [40] to eliminate redundant denoising steps through latents reuse. To reduce the consumption of VRAM, the computation of Teacache is transferred to the CPU, and the increased inference time brought about by this process can be ignored. In addition, we adopted Unified Sequence Parallelism (USP) [16] to enable multi-GPU inference. With the increase in the number of nodes, in order to ensure that the data shapes split to each node are the same, some additional cropping of the input reference iamge is inevitable. Notably, TeaCache and USP can be activated simultaneously. Consequently, our framework achieves generating 80 frames of video within minute (conducted 50 inference steps on 8 A800 GPUs) while incurring no perceptible quality degradation. Quantitative analysis of acceleration performance will be presented in the experiments section. 2.5 Data Pipeline Figure 4: Data Processing Pipeline. This is data funnel to filter high-quality video data. To enhance the quality of model training, we constructed data processing pipeline as shown in Figure 4, progressive filtering strategy adopted to strictly monitor the training data. Specifically, we collected 10K hours of video data from public datasets (including OpenHumanVid [35], Panda6M [68], Hallo3 [12]) and self-collected sources, placing it into raw Data Pool. Then, we processed the data in stages based on image content, video quality, portrait quality, audio quality, and audiovisual synchrony, ultimately obtaining 1K hours for training. At the same time, we have introduced manual annotation in the data processing flow of each stage to ensure that the bad case rate remains below 5%. Our data preprocessing pipeline begins with the collection of large-scale video dataset, which is segmented into short clips based on content coherence. We apply our video captioning model, SkyCaptioner-V1 [6], to generate descriptive annotations for each clip, providing high-quality text supervision. To analyze human presence and interaction, we use YOLO-World [9] and InsightFace for body and face detection, respectively, allowing us to estimate the number of individuals in each clip. We extract pose-related features using DWpose to compute head-to-body ratios, and apply Whisper to identify the spoken language. To evaluate audiovisual synchronization, we compute the sync confidence score [11], and employ source separation model to estimate the number of distinct speakers. This multi-stage preprocessing ensures rich, multimodal supervision for downstream learning tasks."
        },
        {
            "title": "Method",
            "content": "Hallo3[12] FantacyTalking[65] SkyReels-Audio FID 40.12 39.53 38.32 FVD 408.12 381.22 364.71 Sync-C Sync-D IQA ASE 5.75 5.36 6.06 10.12 11.68 9.12 4.03 3.50 4. 2.65 2.38 2."
        },
        {
            "title": "HDTF",
            "content": "Table 1: Quantitative comparison of audio-driven synchronization metrics on HDTF dataset, benchmarked against open-source models."
        },
        {
            "title": "Method",
            "content": "FID FVD Sync-C IQA AV Consist. Visual Quality"
        },
        {
            "title": "HDTF",
            "content": "User Study(Internal) LatenSync[33] SkyReels-Audio 40.23 39.75 390.25 377.23 8.48 8.49 3.63 3. 1.19 1.38 1.00 1.32 Table 2: Quantitative comparison of automatic metrics on HDTF dataset for Lip Sync task."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Settings Implementation Details We train the model using internally collected data constructed through the data pipeline process in section 2.5, which results in approximately 1K hours dataset. To construct coarse-to-fine dataset, we apply filtering mechanism to the audio component, leveraging synchronization and offset criteria. We train the SkyReels-Audio based on SkyReels-V2 backbone [6]. Training is carried out in two distinct stages. During the initial phase, the model is trained solely on audio data to establish robust foundational alignment between the audio and visual modalities. In the subsequent phase, we adopt joint training strategy that incorporates both image and video inputs, aimed at improving motion consistency and temporal coherence in the generated outputs. We used AdamW [13] optimizer with constant learning rate of 105 for all trainable modules across all stages. We employ Flow matching to train the model, with the entire training conducted on 80 Nvidia GPUs. To enhance video generatiob varibility, the reference image / video, guiding audio, prompt are each seto to be independently discarded with probability of 0.15. During inference, we employ the sampling steps of 50, the audio CFG is set to 4.5. Evaluation Metrics. We employ Q-align [70] visual language model to evaluate video quality (IQA) and aesthetic metrics (ASE), and use FID [24] and FVD [61] to assess the distance between generated videos and real videos. Finally, we utilize Sync-C and Sync-D as proposed in SyncNet [11] for audio-visual synchronization estimation. 3.2 Main Results Qualitative Analysis. To comprehensively evaluate the capability of the proposed model in animating any audio-driven portrait styles and its generalizability across diverse application contexts, we present newly curated benchmark dataset. This benchmark comprises 50+ portrait images spanning multiple domains, including stylistic variations such as anime, sculpture, and photorealistic renderings, all synthesized using advanced text-to-image generation techniques. In addition, the benchmark incorporates 30 distinct audio segments reflecting range of language, vocal scenarios, including singing, spoken word, and rap performances. Complementary to these are 20 textual prompts designed to elicit specific emotional states and physical gestures during speech. Certain prompts also contain descriptions of dynamic environmental contexts tailored to particular portrait backgroundsfor instance, the movement of foliage in the wind or the sound of ocean waves. This comprehensive benchmark, characterized by its stylistic breadth and contextual richness, is intended to support broader research efforts within the community with public. We assess the performance of SkyReels-Audio using this internal benchmark. As illustrated in Figure 5, 6 and 8, the model achieves high perceptual fidelity and temporal coherence, even under conditions of computational"
        },
        {
            "title": "Method",
            "content": "Sync-C Sync-D IQA ASE"
        },
        {
            "title": "Internal",
            "content": "Hallo3[12] MagicInfinite[77] OmniHuman-1[38] FantacyTalking[65] SkyReels-Audio Audio CFG=1 Audio CFG=3 w/o Audio RoPE 5.53 6.22 7.50 3.67 6.75 5.78 6.30 5.58 9.33 8.43 7.47 10.97 8.32 9.65 8.78 9. 4.13 4.56 4.66 4.26 4.42 4.58 4.45 4.35 2.80 3.00 3.19 2.80 2.91 3.02 2.91 2.82 Table 3: Quantitative comparison of audio-driven synchronization metrics on internal dataset. We also ablate the Audio CFG and Audio RoPE to illustrate the effectiveness of design. Figure 5: Qualitative comparisons with other audio-driven talking portrait methods. Our approach produce more accurate lip synchronization with naturalness. acceleration. Notably, despite being trained solely on real-world portrait videos, SkyReels-Audio demonstrates strong generalization to stylized portraits, indicating robust cross-domain adaptability. Quantitative Analysis. To assess the fidelity of generated portrait videos across various methods, we employ standardized evaluation metrics. Our evaluation is conducted on test set comprising 100 video clips randomly sampled from the HDTF dataset [79] and the internal dataset, both of which were excluded from the models training data. For each test instance, the initial video frame is used as static portrait reference, while the corresponding audio track drives the generation of full video sequence. The original video clip serves as the ground truth. Additionally, descriptive textual prompts are extracted from each test video using SkyCaptioner-V2 to support multimodal conditioning. As reported in Table 1, 2, and 3, SkyReels-Audio consistently outperforms baseline models in terms of visual fidelity, motion realism, and lip-sync precision, achieving comparable with close-source models. Notably, the model maintains competitive performance while achieving significant inference acceleration. User Study. To further validate the effectiveness of our method, we conducted subjective evaluation on the Internal dataset. Specifically, each participant assessed two key dimensions: audio-visual 9 Figure 6: Qualitative comparisons with SoTA lip-sync methods. We can see that SkyReels-Audio create better audio lip alignment compared with the baseline. consistency (AV Consist.) and visual quality. total of 20 participants rated each aspect on scale for 0 to 2 (from bad to good). As shown in Table 2, the results indicates that SkyReels-Audio outperform baselines in both evaluation dimensions. 3.3 Ablation Study Audio CFG. We first analyze the impact of Audio CFG in Equation 3. Specifically, we set different Audio CFG values range from [1, 3, 4.5,], while keep other inference parameters same, and perform inference on the Internal dataset. The results are listed in Table 3. We can see that, as the Audio CFG value increases, the metrics related to audio-visual consistency, i.e., Syn-C and Syn-D, continue to"
        },
        {
            "title": "8 GPUs (USP)",
            "content": "w/o TeaCache w/ TeaCache (α = 0.3) 23.62 min 8.96 min 17.90 min 7.18 min 7.80 min 1.88 min 4.29 min 0.97 min Table 4: The effect of TeaCache and USP on inference time. Both methods can effectively improve the inference speed of video generation in our method. improve, but the video visual quality will decrease slightly. Taking into account both factors, we set Audio CFG value of 4.5 by default. Audio RoPE. We then verify the effect of Audio RoPE incorporation when fuse video and audio tokens in cross-attention layers. Table 3 shows the evaluated result with or without audio position encoding. It can be clearly observed that the introduction of position encoding effectively improves the alignment between visual quality and audio, helping the model to more accurately locate useful information. Effect of BLF. Figure 7 provides qualitative comparison of different latent fusion methods. The results show that when no overlap is set between sliding windows, there is significant jump in the image, because each video clip is generated independently based on the reference image; when only unidirectional latent fusion is used, there is still an discontinuity at the stitching area; our bidirectional fusion method significantly improves the stability of the transition frames. Figure 7: Qualitative comparison of different latent fusion methods. BLF generates long videos with less video ghosting and more coherent movements. Effects of TeaCache and USP. Table 4 illustrates the effects of TeaCache and USP on inference time. The experiment was on A800 GPU, with the inference steps set to 50. We tested the average inference time of 50 cases with 80 frames, and with both TeaCache with threshold α=0.3 and USP turned on, the inference speed increases by about 24 times (from 23.62 minutes to 0.97 minute), while the model performance does not show significant drop."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Diffusion-based Lip Sync Generating photorealistic talking-head videos conditioned on audio input continues to pose significant challenges within the domain of multimodal synthesis. Earlier approaches [78, 45, 69] predominantly leveraged 3D intermediate representations, where facial animation parameters derived from 3DMMs were used to guide video synthesis. However, such pipelines often fall short in capturing the subtle intricacies of facial expressions and head dynamics, limiting the realism and emotional expressiveness of the generated content. To overcome these constraints, recent efforts have shifted towards fully endto-end diffusion-based frameworks [18, 17] for audio-driven lip sync animation [59, 27, 8, 12], which show increased promise. LatentSync [33] use SyncNet loss to help better audio-lip alignment in latent diffusion framework. EMO [59] and V-Express [63] have leveraged audio inputs to drive precise lip synchronization while incorporating sparse visual cues to animate head dynamics, resulting in compelling audiovisual coherence. However, two key challenges remain unresolved. First, 11 identity preservation is typically handled by reference encoders adapted from general-purpose vision backbones, which restricts the capacity to generate complex and diverse motion patterns. Second, current systems still struggle to capture the broader spectrum of expressive behaviorssuch as micro-expressions, facial gestures, and upper-body movementsthat are only weakly correlated with the audio signal. Our model also build on advanced diffusion model, while integrate multi-modal controls and joint modeling for better lip sync alignment. 4.2 Diffusion-based Portrait Animation Diffusion models[55] have emerged as cornerstone in the field of generative media synthesis, demonstrating remarkable efficacy in producing both images and videos [3, 23, 7, 72, 67, 51, 20]. Within the domain of portrait animation, Echomimic [8] and MegActor-Sigma [73]enhance controllability over animation by jointly modeling visual and auditory signals. Concurrently, methods like [27, 80, 15] prioritize temporal consistency and affective realism, introducing emotion-aware modules and frame-level blending strategies specifically designed for sustained video generation. Despite these advancements, existing pipelines often struggle with preserving visual quality and narrative flow over time, primarily due to the compartmentalized treatment of spatial and temporal dependencies via isolated attention mechanisms. In contrast, recent DiT based architecturesuch as OminiHuman-1 [38], CogVideoX [75], Allegro [82], MovieGen [50], and HunyuanVideo [32]employ integrated 3D full-attention mechanisms that offer unified modeling of space-time information, thereby producing higher-quality video outputs. Drawing on these insights, we also resort it into our portrait animation framework, achieving significant gains in visual fidelity and sequence scalability. Empirical comparisons show that, relative to concurrent models such as [12, 77, 38, 65], our proposed SkyReels-Audio system yields extended-duration videos with enhanced resolution and perceptual quality."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents SkyReels-Audio, unified and omni framework for generating talking portraits conditioned exclusively on audio inputs. Our method leverages hybrid training paradigm that aligns auditory and visual modalities, enabling precise modeling of the correlations between speech signals and corresponding lip articulations, facial expressions, and bodily gestures. To support the generation of videos of arbitrary length, we incorporate dynamic sliding-window mechanism that ensures seamless temporal continuity and perceptual coherence across frames. Extensive experimental evaluationsspanning both qualitative assessments and quantitative benchmarksdemonstrate that SkyReels-Audio consistently achieves superior performance in audio-visual synchronization and animation fidelity across wide range of speaker identities, vocal characteristics, and multimodal conditioning scenarios."
        },
        {
            "title": "6 Contributors",
            "content": "We gratefully acknowledge all contributors for their dedicated efforts. The following lists recognize participants by their primary contribution roles: Project Sponsor: Yahui Zhou Project Leaders: Di Qiu, Guibin Chen Core Contributors: Zhengcong Fei, Hao Jiang, Baoxuan Gu, Youqiang Zhang, Jiahua Wang, Jialin Bai, Chaofeng Ao, Debang Li, Mingyuan Fan"
        },
        {
            "title": "References",
            "content": "[1] DeepBrain AI. https://www.prnewswire.com/news-releases/deepbrain-ai-delivers-ai-avatar-to empower-people-with-disabilities-302026965.html. In Online, 2024. [2] Ziqian Bai, Feitong Tan, Sean Fanello, Rohit Pandey, Mingsong Dou, Shichen Liu, Ping Tan, and Yinda Zhang. Efficient 3d implicit head avatar with mesh-anchored hash table blendshapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19751984, 2024. 12 Figure 8: More generated results of SkyReels-Audio. Our approach can handle reference images of different objectives, sizes, and styles, and claim naturally consistent video results. [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [4] Aggelina Chatziagapi, ShahRukh Athar, Abhinav Jain, Rohith Mysore Vijaya Kumar, Vimal Bhat, and Dimitris Samaras. Lipnerf: What is the right feature space to lip-sync nerf. In International Conference on Automatic Face and Gesture Recognition 2023, 2023. [5] Bo Chen, Shoukang Hu, Qi Chen, Chenpeng Du, Ran Yi, Yanmin Qian, and Xie Chen. Gstalker: Real-time audio-driven talking face generation via deformable gaussian splatting. arXiv preprint arXiv:2404.19040, 2024. [6] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136, 2024. [9] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. [10] Kyusun Cho, Joungbin Lee, Heeji Yoon, Yeobin Hong, Jaehoon Ko, Sangjun Ahn, and Seungryong Kim. Gaussiantalker: Real-time high-fidelity talking head synthesis with audio-driven 3d gaussian splatting. arXiv preprint arXiv:2404.16012, 2024. [11] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pages 251263. Springer, 2017. [12] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv preprint arXiv:2412.00733, 2024. [13] Kingma Diederik. Adam: method for stochastic optimization. (No Title), 2014. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [15] Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, and He Tang. Motioncharacter: Identitypreserving and motion controllable human video generation. arXiv preprint arXiv:2411.18281, 2024. [16] Jiarui Fang and Shangchun Zhao. unified sequence parallelism approach for long context generative ai. arXiv preprint arXiv:2405.07719, 2024. [17] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [18] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [19] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. [20] Zhengcong Fei, Debang Li, Di Qiu, Changqian Yu, and Mingyuan Fan. Ingredients: Blending custom photos with video diffusion transformers. arXiv preprint arXiv:2501.01790, 2025. [21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [22] Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu Hu, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, et al. Stylesync: High-fidelity generalized and personalized lip sync in style-based generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15051515, 2023. [23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [25] Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. Sonic: Shifting focus to global audio perception in portrait animation. arXiv preprint arXiv:2411.16331, 2024. [26] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu, Chen Change Loy, Xun Cao, and Feng Xu. Audio-driven emotional video portraits. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1408014089, 2021. [27] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. [28] Esperanza Johnson, Ramón Hervás, Carlos Gutiérrez López de la Franca, Tania Mondéjar, Sergio Ochoa, and Jesús Favela. Assessing empathy and managing emotions through interactions with an affective avatar. Health informatics journal, 24(2):182193, 2018. [29] Oytun Kal and Yavuz Samur. Educational virtual reality game design for film and animation. Encyclopedia of Computer Graphics and Games, pages 621636, 2024. [30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [31] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [32] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [33] Chunyu Li, Chao Zhang, Weikai Xu, Jinghui Xie, Weiguo Feng, Bingyue Peng, and Weiwei Xing. Latentsync: Audio conditioned latent diffusion models for lip sync. arXiv preprint arXiv:2412.09262, 2024. [34] Dongze Li, Kang Zhao, Wei Wang, Bo Peng, Yingya Zhang, Jing Dong, and Tieniu Tan. Aenerf: Audio enhanced neural radiance field for few shot talking head synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 30373045, 2024. [35] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. arXiv preprint arXiv:2412.00115, 2024. [36] Jiahe Li, Jiawei Zhang, Xiao Bai, Jin Zheng, Xin Ning, Jun Zhou, and Lin Gu. Talkinggaussian: Structure-persistent 3d talking head synthesis via gaussian splatting. arXiv preprint arXiv:2404.15264, 2024. [37] Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4d scans. ACM Trans. Graph., 36(6):1941, 2017. [38] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models, 2025. [39] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [40] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. arXiv preprint arXiv:2411.19108, 2024. [41] Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, and Yu Li. Moda: Mapping-once audiodriven portrait animation with dual attentions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2302023029, 2023. [42] Yuanxun Lu, Jinxiang Chai, and Xun Cao. Live speech portraits: real-time photorealistic talking-head animation. ACM Transactions on Graphics (ToG), 40(6):117, 2021. [43] Shugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, In Proceedings of the IEEE/CVF Conference on and Yaser Sheikh. Pixel codec avatars. Computer Vision and Pattern Recognition, pages 6473, 2021. [44] Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Zhidong Deng, and Xin Yu. Styletalk: One-shot talking head generation with controllable speaking styles. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 18961904, 2023. [45] Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, and Zhidong Deng. Dreamtalk: When expressive talking head generation meets diffusion probabilistic models. arXiv preprint arXiv:2312.09767, 2(3), 2023. [46] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [47] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 15 [48] Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, and Zhaoxin Fan. Synctalk: The devil is in the synchronization for talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 666676, 2024. [49] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Emotalk: Speech-driven emotional disentanglement for 3d face animation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20687 20697, 2023. [50] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [51] Di Qiu, Zheng Chen, Rui Wang, Mingyuan Fan, Changqian Yu, Junshi Huang, and Xiang Wen. Moviecharacter: tuning-free framework for controllable character video synthesis. arXiv preprint arXiv:2410.20974, 2024. [52] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841, 2025. [53] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya In International Sutskever. Robust speech recognition via large-scale weak supervision. conference on machine learning, pages 2849228518. PMLR, 2023. [54] Imogen Rehm, Emily Foenander, Klaire Wallace, Jo-Anne Abbott, Michael Kyrios, and Neil Thomas. What role can avatars play in e-mental health interventions? exploring new models of clienttherapist interaction. Frontiers in Psychiatry, 7:186, 2016. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [56] Linsen Song, Wayne Wu, Chaoyou Fu, Chen Change Loy, and Ran He. Audio-driven dubbing for user generated contents via style-aware semi-parametric synthesis. IEEE Transactions on Circuits and Systems for Video Technology, 33(3):12471261, 2022. [57] Jiacheng Su, Kunhong Liu, Liyan Chen, Junfeng Yao, Qingsong Liu, and Dongdong Lv. Audio-driven high-resolution seamless talking head video editing via stylegan. arXiv preprint arXiv:2407.05577, 2024. [58] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [59] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024. [60] Luan Tran and Xiaoming Liu. Nonlinear 3d face morphable model. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 73467355, 2018. [61] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. [62] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [63] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 16 [64] Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby Tan, and Haizhou Li. Seeing what you said: Talking face generation guided by lip reading expert. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1465314662, 2023. [65] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. arXiv preprint arXiv:2504.04842, 2025. [66] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1003910049, 2021. [67] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, et al. Magicvideo-v2: Multi-stage high-aesthetic video generation. arXiv preprint arXiv:2401.04468, 2024. [68] Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding, David Brady, Qionghai Dai, and Lu Fang. Panda: gigapixel-level human-centric video dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2020. [69] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. [70] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. [71] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. Codetalker: Speech-driven 3d facial animation with discrete motion prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1278012790, 2023. [72] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [73] Shurong Yang, Huadong Li, Juhao Wu, Minhao Jing, Linze Li, Renhe Ji, Jiajun Liang, Haoqiang Fan, and Jin Wang. Megactor-sigma: Unlocking flexible mixed-modal control in portrait animation with diffusion transformer. arXiv preprint arXiv:2408.14975, 2024. [74] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. [75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [76] Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, et al. Real3d-portrait: One-shot realistic 3d talking portrait synthesis. arXiv preprint arXiv:2401.08503, 2024. [77] Hongwei Yi, Tian Ye, Shitong Shao, Xuancheng Yang, Jiantong Zhao, Hanzhong Guo, Terrance Wang, Qingyu Yin, Zeke Xie, Lei Zhu, et al. Magicinfinite: Generating infinite talking videos with your words and voice. arXiv preprint arXiv:2503.05978, 2025. [78] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86528661, 2023. [79] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36613670, 2021. [80] Longtao Zheng, Yifan Zhang, Hanzhong Guo, Jiachun Pan, Zhenxiong Tan, Jiahao Lu, Chuanxin Tang, Bo An, and Shuicheng Yan. Memo: Memory-guided diffusion for expressive talking video generation. arXiv preprint arXiv:2412.04448, 2024. [81] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. ArXiv 2406.01561, 2024. [82] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. [83] Yixiang Zhuang, Baoping Cheng, Yao Cheng, Yuntao Jin, Renshuai Liu, Chengyang Li, Xuan Cheng, Jing Liao, and Juncong Lin. Learn2talk: 3d talking face learns from 2d talking face. arXiv preprint arXiv:2404.12888, 2024."
        }
    ],
    "affiliations": [
        "Skywork AI"
    ]
}