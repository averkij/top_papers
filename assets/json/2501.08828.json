{
    "paper_title": "MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents",
    "authors": [
        "Kuicai Dong",
        "Yujing Chang",
        "Xin Deik Goh",
        "Dexun Li",
        "Ruiming Tang",
        "Yong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval."
        },
        {
            "title": "Start",
            "content": "Benchmarking Multi-Modal Retrieval for Long Documents Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu Noahs Ark Lab, Huawei * denotes co-first authors; correspond to {dong.kuicai, liu.yong6}@huawei.com 5 2 0 2 5 1 ] I . [ 1 8 2 8 8 0 . 1 0 5 2 : r Figure 1: MMDocIR comprises 313 lengthy documents across 10 different domains, along with 1,685 questions. For each question, page-level annotations are provided via selected screenshots. Red boundary boxes represent layout-level annotations."
        },
        {
            "title": "Abstract",
            "content": "Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is notable lack of robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within long document, while the latter targets the detection of specific layouts, offering more fine-grained granularity than whole-page analysis. layout can refer to variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Preprint, , 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval. MMDocIR is available at https://huggingface.co/MMDocIR. CCS Concepts Information systems Multi-modal Document Retrieval. Keywords Multi-modal Document Retrieval, Page Retrieval, Layout Retrieval ACM Reference Format: Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu. 2025. Benchmarking Multi-Modal Retrieval for Long Documents. In Proceedings of (Preprint). ACM, New York, NY, USA, 11 pages. https: //doi.org/XXXXXXX.XXXXXXX Preprint, , Dong et al. Figure 2: Area ratio of different modalities (1) in overall and (2) by domains in MMLongBench-Doc benchmark [29]. Note that the white spaces, headers, and footers are removed from the area counting."
        },
        {
            "title": "1 Introduction",
            "content": "Multi-modal document retrieval [15, 16, 24, 28] aims to encode and retrieve various forms of content from visually rich documents based on user queries. Unlike traditional document retrieval [8, 14, 36, 50] which primarily deals with textual data, multi-modal document retrieval must handle documents that seamlessly integrate text with multi-modal elements such as images, tables, charts, and layout designs. These multi-modal content often carry significant information that plain text cannot convey [11, 37, 52], thereby enriching the depth and relevance of retrieved content. For example, our analysis of MMLongBench-Doc benchmark [29] reveals that: text occupies only 52.7% of content area, while images and tables account for 29.2% and 12.8% respectively, as shown in Figure 2. This significant presence of multiple data modalities requires retriever to leverage all data modalities present in document. However, current benchmarks (shown in Table 1) for evaluating multi-modal document retrievers are insufficient, lacking in certain aspects that are critical for comprehensive assessment. The major shortcomings include: 1. Question Quality: The design and curation of questions in most benchmarks do not align with the specific needs of multi-modal document retrieval. The questions, often reused from Visual Question Answering (VQA) tasks, lack the specificity required for this purpose and require expert filtering to be truly effective for retrieval tasks. 2. Document Quality: significant limitation of existing benchmarks is the missing of complete document pages, thus hindering accurate evaluation of retrieval capabilities within actual document contexts. Additionally, the range of document domains covered is often too narrow, limiting the benchmarks applicability across different fields. 3. Retrieval Granularity: Most benchmarks, except for SciMMIR [45], only allow retrieval at page level which is insufficient, as user queries may pertain to specific parts of page like particular figure or table rather than the entire page. Recognizing these gaps, we introduce MMDocIR Multi-Modal Document Information Retrieval benchmark. MMDocIR is innovatively structured around two critical tasks: page-level and layoutlevel retrieval. (1) The page-level retrieval task is designed to identify the most relevant pages within document in response to user query. (2) The layout-level retrieval aims to retrieve most relevant layouts. The layouts are defined as the fine-grained elements such as paragraphs, equations, figures, tables, and charts. This task allows for more nuanced content retrieval, honing in on specific information that directly answers user queries. To support these tasks, we first develop MMDocIR evaluation set comprising 313 documents averaging 65.1 pages each, along with VQA 1,658 questions derived from MMLongBench-Doc [29] and DocBench [53]. Initially, we examine all VQA questions, filter out questions that are irrelevant to IR tasks, and revise remaining questions to be more suitable for IR. Then, we manually annotate these questions with page and layout labels. The page-level labels are the specific pages identified that contain answer evidence. 1 The layout-level labels are the bounding boxes meticulously delineated around the key evidence within the identified pages. Meanwhile, we introduce MMDocIR training set, comprising 73,843 QA pairs converted from 7 DocQA datasets. Specifically, we manually collect 6,878 documents and use either manual or automatic methods to annotate the ground truth labels. 2 In our research, we conduct thorough evaluation of the prevailing multi-modal document retrieval baselines which can broadly be categorized into visual-driven and text-driven retrievers. Visualdriven retrievers [15, 28], leverage vision-language models (VLMs) [1, 3, 4, 10] to capture rich multi-modal cues and generate embeddings for both queries and documents. In contrast, text-driven retrievers [1820, 26, 43, 46] primarily rely on OCR or VLM to first convert the multi-modal content into text, subsequently employing language models (LMs) [12, 27, 44] to generate embeddings for both queries and documents. Via extensive experiments, we discover that visual-driven retrievers consistently outperform their text-driven counterparts, often by significant margin. This highlights the critical role that visual elements play in multi-modal document retrieval. In summary, our contributions are three-fold: Dual-task Retrieval Framework: We propose dual-task retrieval framework ( 2) that encompasses page-level and finegrained layout-level multi-modal document retrieval. MMDocIR Benchmark: We introduce Multi-Modal Document Information Retrieval (MMDocIR) benchmark. The evaluation set ( 3) consists of 313 documents with expert-annotated labels for 1,658 QA pairs. The training set ( 4) consists of 6,878 documents and labels for 73,843 QA pairs. Comprehensive Evaluation: We conduct an extensive evaluation of existing retrieval systems ( 6). Our results reveal that visual-driven retrievers consistently outperform their text-driven counterparts, highlighting the importance of utilizing visual information in multi-modal document retrieval. 1It is worth noting that while MMLongBench-Doc provided initial page labels, our meticulous review lead to corrections in about 21.3% of these labels. 2Since layout annotations are missing for most datasets, our data annotating pipeline can only create layout-level labels for 4 subsets. Benchmarking Multi-Modal Retrieval for Long Documents Preprint, , Benchmarks Type By Expert? Question For IR? #Num Evidence Type Domain Document #Pages Label Source Page Layout DocCVQA [39] SciMMIR [45] ViDoRe [15] PDF-MVQA [13] MMLongBench-Doc [29] Wiki-SS [28] DocMatix-IR [28] MMDocIR VQA question Image caption VQA question Search query VQA question Natural question VQA question VQA question TXT/L TAB/I TXT/C/TAB/I TXT/TAB/I 20 530K 3,810 260k 1,082 3,610 5.61M TXT/L/C/TAB/I Multi-domain Finance Science Multi-domain Biomedical TXT/L/C/TAB/I Multi-domain Wikipedia TXT 1, TXT/C/TAB/I Multi-domain 1.0 1.0 1.0 9.6 47.5 1.0 4.2 65.1 Table 1: Comparison between our benchmark and previous Document IR datasets. TXT/L/C/TAB/I: pure text/generalized layout/chart/table/image."
        },
        {
            "title": "2 Dual-Task Retrieval Definition",
            "content": "Let be document corpora consisting of set of document pages: = {𝑝1, 𝑝2, . . . , 𝑝𝑛 }, and layouts: = {𝑙1, 𝑙2, . . . , 𝑙𝑚 } extracted via layout detection3. The objective is to perform document page-level and layout-level retrieval. Specifically, the task involves retrieving subset of these pages and layouts that are most relevant to query 𝑄, identified by the top 𝑘 entries where 𝑘 is significantly smaller than 𝑛 or 𝑚. The relevance of pages (𝑝) and layouts (𝑙) to 𝑄 is quantified by similarity scores, Sim(𝑄, 𝑝) and Sim(𝑄, 𝑙) respectively. The retrieval system can be decomposed into (1) an offline indexing phase in which each page and layout from and is transformed into vector representation, and (2) an online querying phase in which query 𝑄 is converted into vector form, and the vectorized query is then compared against the indexed vectors using the similarity scores Sim(𝑄, 𝑝) for pages and Sim(𝑄, 𝑙) for layouts."
        },
        {
            "title": "3 MMDocIR: Evaluation Set\n3.1 Document Corpora Collection",
            "content": "To facilitate the development of MMDocIR, we leverage resources from existing Document Visual Question Answering (DocVQA) benchmarks [22, 25, 29, 31, 38, 40, 51, 53]. Although DocVQA benchmarks are not primarily designed for IR, they offer valuable document corpora and questions that can be adapted for our purposes. To select useful DocVQA datasets, we adhere to following criteria: Document Source: The dataset must include accessible original documents or sources for these documents. We need to access and enrich them to support more complex retrieval tasks. Diverse Domain/Modality: The document collections must (1) encompass diverse domains e.g., academia, finance, publishing, and research, and (2) embrace multiple modalities, such as text, figures, tables, charts, and layouts. Long Document: We seek documents of sufficient length as longer texts pose more significant challenges. This criterion can evaluate models in handling complex and lengthy documents. QA Diversity and Comprehensiveness: The questions included in the dataset should be diverse and challenging. For example, some questions should require reasoning across both text and visual tables/figures, while some are multi-hop questions that require synthesizing information from multiple pages. After considering aforementioned criteria, we choose to leverage on the document corpora and questions from MMLongBench-Doc [29] 3A document page usually comprises about 5 to 15 layouts, depending on its complexity. and DocBench [53]. MMLongBench-Doc is long-context, multimodal benchmark comprising 1,091 questions constructed upon 135 documents averaging 47.5 pages. Its corpora include domains of research report, administration and industry, tutorial and workshop, academic paper, brochure, financial report, and guidebook. DocBench focuses on long document understanding, consisting of 1,102 questions constructed upon 229 documents averaging 77.5 pages. The corpora span across domains: academia, finance, government, laws, and news. Both datasets provide expert-annotated questions, requiring content evidence of various modalities (e.g., text, tables, charts and images, as well as locations layouts) to answer. Consequently, we obtain total of 364 documents and 2,193 questions for our subsequent annotation."
        },
        {
            "title": "3.2 Annotation Process",
            "content": "Question Filtering and Revision. To ensure that the questions in MMDocIR are optimally suited for document retrieval tasks. To this end, we identify four specific types of questions that do not align well with the objectives of IR. By filtering out or modifying these questions, we ensure the integrity and relevance of MMDocIR. Summarization Questions: These questions, such as What does this book mainly illustrate? require comprehensive understanding of large sections of document or even the entire document. The broad scope makes it hard to pinpoint specific content effectively, which contradicts the precise nature of IR tasks. Overwhelm Statistical Questions: Questions that demand extensive data computation or collation, such as How many words are there in total in the paper? also fall outside our scope. These require level of statistical analysis that goes beyond retrieval. Online Search Questions: For example, queries like What is the Google Scholar citation count of the main author of this article? rely on information from external online resources. Since our focus is on retrieving information that is internally available within the documents, these questions are excluded. Unanswerable Questions: These are designed to test if models generate answers based on non-existent information (model hallucinations). Since they do not facilitate the retrieval of factual document-based information, these questions are excluded. Page-level annotation process. To address the challenge of accurately locating relevant information within extensive documents, we annotate page-level labels that pinpoint the exact pages containing evidence needed to answer specific questions. Considering that Preprint, , Dong et al. Questions (%) Modality Distribution (%) Dataset Domain MMDocIR Eval - Research Report - Admin & Industry - Tut & Workshop - Academic Paper - Brochure - Financial Report - Guidebook - Government - Laws - News #Doc #QA Document Statistics #Page #Lay #Page #Lay Label Label /Doc /Page %Lay Text Image Table 313 1,658 2,107 2,638 65.1 400 39.4 318 34 113 16.8 82 10 225 57.5 165 17 571 19.5 473 75 178 30.3 121 15 477 169.5 394 51 223 78.4 168 22 132 68.9 116 44 149 58.5 133 44 170 50.0 137 1 200 59 102 386 76 343 112 111 132 137 41.8 8.3 39.1 6.0 45.1 9.1 4.1 43.8 10.1 48.4 41.1 9.7 9.2 44.8 10.0 33.6 45.4 6.9 6.0 31.2 73.6 72. 44.7 45.0 78.0 37.2 28.8 60.5 28.0 51.8 69.37 62.1 70.1 21.7 17.5 20.3 61.7 25.7 52.6 13.1 54.4 2.7 0 1.5 37.4 74.5 13.5 24.5 50.0 18.4 54.5 26.8 0 10.6 0 Lay/ Meta 11.5 13.5 13.5 9.8 10.4 36.8 5.3 17.8 7.6 27.3 28.5 Text Image Table Title 60.4 45.6 70.1 28.0 74.6 33.3 60.3 63.7 88.2 83.8 48.5 18.8 40.0 11.7 57.3 12.8 50.8 7.9 20.0 3.7 1.6 39.8 16.7 5.9 14.9 6.3 11.1 8.5 29.2 12.1 5.7 12.3 0.0 4.1 8.4 3.2 8.3 1.5 7.0 2.6 4.1 2.4 2.2 11.6 Table 2: Detailed statistics for MMDocIR evaluation set. #Lay/Page is the averaging layouts per page, reflecting pages layout complexity. %Lay refers to the area ratio of useful layouts (excluding white spaces, headers, and footers) over entire page. the average document in our dataset contains 65.1 pages, pinpointing the correct pages is similar to finding needle in haystack and requires meticulous effort. Our annotation process is: For MMLongBench-Doc: we review and correct the answers and corresponding page labels for 794 questions. Specifically, we make corrections to 10 answers and revise 169 page labels4 to ensure they accurately reflect the evidence locations. For DocBench: we annotate page labels for all 864 questions from scratch. This involves detailed examination of each document to accurately assign page labels that match the evidential requirements of each question. Through these efforts and following rigorous annotation and cross-validation process, we successfully compile page-level labels for total of 1,658 questions in MMDocIR. Layout-level annotation process. To achieve more fine-grained retrieval granularity, we go beyond page-level annotations by including layout-level labels. This advanced annotation process captures the specific layout elements within pages that contain necessary evidence to answer each question. Unlike page-level annotation, labeling layout-level information is more complex and labor-intensive due to its detailed nature. Our annotations are via: To streamline annotation process, we utilize MinerU [42] to parse all documents and extract the layout information (e.g., type and bounding boxes) for every layout element on page. These elements are categorized into five main types: text, image, table, title, and equation. The bounding boxes are delineated by the coordinates of the top-right and bottom-left corners. Based on the layouts drawn by MinerU, we identify the layouts that contain necessary eivdence to answer the question. In instances where MinerU fails to detect certain evidentiary elements, we manually draws the bounding boxes to ensure the precision of layout labels. This manual intervention is necessary for approximately 7% of our layout-level labels. Ultimately, this meticulous process leads to the annotation of 2,638 layout labels for the 1,658 questions in MMDocIR."
        },
        {
            "title": "3.3 Quality Control",
            "content": "To ensure annotation quality and reliability in MMDocIR, we have adopted rigorous quality control process using cross-validation 4Common errors in page labeling: annotators starting page indexing at 1, missing labels for questions spanning multiple pages, and incorrect or absent page labels. Consistency Page Labels Recall Prec. AB BA Average 95.7 94.3 95.0 96.1 94.6 95.4 Layout Labels Recall Prec. F1 88.1 85.9 87.0 86.8 87.5 87. 87.4 86.7 87.1 F1 95.9 94.4 95.2 Table 3: Annotation consistency estimated between annotation group and B. AB indicates evaluation of As annotations with Bs annotations as ground truth. method. This method involves splitting the document corpora into two main groups, labeled as Group and Group B. Each group is responsible for annotating page-level and layout-level labels for approximately 1,000 QA pairs. Importantly, there is an overlap of 400 QA pairs between two groups, which serves as validation checkpoint. The process for quality control works as follows: Overlap Scoring: For 400 overlapping questions, each groups annotations are scored against the others as form of mutual validation. Specifically, Group As annotations are used as ground truth to score Group Bs work, and conversely, Group Bs annotations serve as the ground truth for scoring Group A. Cross-Evaluation: By cross-evaluating the outcomes from both groups, we are able to assess the consistency of the annotations and identify any discrepancies for improvement. Specifically, we achieved an F1 score of 95.2 for page-level annotations and 87.1 for layout-level annotations, as shown in Table 3. Random Cross-Validation: Apart from the overlapping 400 questions, we randomly cross-validate around 50% of the remaining annotations. In the cases where we have different opinions, we discuss to achieve mutually-agreed annotations. This step ensures the high quality and reliability of MMDocIR."
        },
        {
            "title": "3.4 Statistics and Analysis\nDocument Analysis. As shown in Table 2, MMDocIR evaluation\nset includes 313 long documents averaging 65.1 pages, categorized\ninto ten main domains. Different domains feature distinct distribu-\ntions of multi-modal information. For instance, research reports,\ntutorials, workshops, and brochures predominantly contain images,\nwhereas financial and industry documents are table-rich. In con-\ntrast, government and legal documents primarily comprise text.\nOverall, the modality distribution is: Text (60.4%), Image (18.8%),\nTable (16.7%), and other modalities (4.1%).",
            "content": "Benchmarking Multi-Modal Retrieval for Long Documents Preprint, , Dataset Domain #Doc #QA #Page #Lay /Doc /Page %Lay Text Image Table Title Page Lay Document Statistics Modality Distribution (%) Labels MMDocIR Train - MP-DocVQA [40] - SlideVQA [38] - TAT-DQA [51] - ArXivQA [25] - SciQAG [41] - DUDE [22] - CUAD [17] assorted docs 32.6 10.8 health/ind. docs 46.8 22.7 diverse slides 49.3 4.7 annual reports 147.3 26.5 arXiv papers 18.4 2.8 science papers 9.0 6.7 assorted docs 15.6 15.2 6.4 29.6 legal contract Table 4: Document statistics for Training Datasets collected. 6.32 42.6 38.8 6.9 42.3 4.4 42.2 9.2 50.0 7.9 53.7 9.1 42.5 7.4 24.7 7. 73,843 15,266 11,066 15,814 12,314 4,976 3,173 11,234 6,878 875 2,011 163 1,579 1,197 779 274 49.3 57.3 30.1 66.4 70.4 61.8 57.1 89.3 34.3 18.0 56.2 4.4 22.3 28.0 24.7 2.5 4.9 1.9 8.8 2.7 1.0 1.5 2.9 1.1 Question and Annotation Analysis. MMDocIR encompasses 1,658 questions, 2,107 page labels, and 2,638 layout labels. The modalities required to answer these questions distribute across four categories: Text (44.7%), Image (21.7%), Table (37.4%), and Layout/Meta (11.5%). The Layout/Meta category encompasses questions related to layout information and meta-data statistics. Notably, the dataset poses several challenges: 254 questions necessitate cross-modal understanding, 313 questions demand evidence across multiple pages, and 637 questions require reasoning based on multiple layouts. These complexities highlight the need for advanced multi-modal reasoning and contextual understanding."
        },
        {
            "title": "4 MMDocIR: Training Set\n4.1 Document Corpus Collection",
            "content": "The training set corpora are mainly collected from DocVQA benchmarks, adhering to similar selection criteria (see Section 3.1). MP-DocVQA [40] contains 47,952 images collected from Industry Documents Library (IDL) 5. We group the 47,952 document images into separate document files, and obtain 875 long documents (46.8 pages on average) with 15,266 QA pairs. SlideVQA [38] contains 2,619 slide documents collected from slideshare 6 and covering 39 topics. Note that SlideVQA contains only the first 20 pages of each slide decks. In our research, we manually collect the complete slide decks, and obtain 2,011 long documents (averaging 49.3 pages) with 11,066 QA pairs. TAT-DQA [51] consists of 3,067 document pages from financial reports 7, dated between 2018 and 2020. Note that neither original documents nor links is provided. We use OCR to extract text in the pages, and use search engine to find relevant documents. After careful tracing and recognition, we identify 163 original documents (averaging 147.3 pages) with 15,814 QA pairs. ArXivQA [25] comprises 32k figures cropped from academic pages 8. We use the arXiv DOIs provided to collect the academic papers. After careful tracing, recognition, and document length filtering, we identify 1,579 documents averaging 18.4 pages. SciQAG [41] consists of 22,728 papers in 24 scientific disciplines, collected from Web of Science (WoS) Core Collection database. We sample 50 documents from each discipline, and manually collect 1,197 papers using the DOIs provided. 5https://www.industrydocuments.ucsf.edu/ 6https://www.slideshare.net/ 7https://www.annualreports.com/ 8https://arxiv.org/ DUDE [22] provides 5,019 documents from aggregate websites9. It covers broad range of domains, including medical, legal, technical, and financial, among others, to evaluate models ability to handle diverse topics and the specific knowledge each requires. We filter out short documents and obtain 779 relatively long documents (averaging 15.6 pages) with 3,173 QA pairs. CUAD [17] provides 510 commercial legal contracts, collected from Electronic Data Gathering, Analysis, and Retrieval (EDGAR)10. We filter out short documents in CUAD and obtain 274 long documents (29.6 pages on average) with 11,234 QA pairs."
        },
        {
            "title": "4.2 Label Construction",
            "content": "The page labels can be converted for MP-DocVQA, SlideVQA, and DUDE datasets. Among them, only DUDE provides layout labels. SciQAG provides only question and answer in texts. We utilize them to reversely obtain the page-level and layout-level labels. Specifically, we first use MinerU to obtain layout-level passage chunks. For each QA pair, we deploy E5 and BGE retrievers to obtain question-passage and answer-passage similarity scores against all extracted passage chunks. If both scores rank within top 3 for certain passage chunk, we confirm this layout to be the layout-level labels for the given QA pair. Similarly, ArXivQA provides only cropped images, without document page/layout labels. We first use MinerU to obtain layout-level image. For each cropped image, we calculate the image similarity between all extracted images, and keep only the highest scoring one. Subsequently, we manually examine if the selected image matches the cropped image. In this way, we filter around 20% unmatched images and their QA pairs, resulting 1,579 QA pairs with page-level and layout-level labels. For TAT-DQA, the layout-level labels are provided for sampled single page. We need to localize the page index of the sampled page. We utilize PDF mapping technique to retrieve the best matched page for every sampled page in this document. Then, we manual examine if the retrieved page is identical to the given page, and correct the labels if there were any errors. The overall statistics (e.g., document information, modality distribution, domain, etc) of MMDocIR training set are in Table 4."
        },
        {
            "title": "5 Model Training: DPR-Phi3&Col-Phi3",
            "content": "To evaluate the effectiveness of the MMDocIR training set, we train two visual retrievers based on Phi3-Vision [1]. Phi3-Vision 91: archive.org, 2: http://commons.wikimedia.org/, 3: http://documentcloud.org/ 10https://www.sec.gov/search-filings Preprint, , Dong et al. Domain Method 1 = 𝑘 @ c 3 = 𝑘 @ c 5 = 𝑘 @ c x - t - V a x - t - e m x - t - e I DPR ColBERT BGE E5 Contriever GTE DPR ColBERT BGE E5 Contriever GTE DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours DPR ColBERT BGE E5 Contriever GTE DPR ColBERT BGE E5 Contriever GTE DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours DPR ColBERT BGE E5 Contriever GTE DPR ColBERT BGE E5 Contriever GTE DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours Resear. Admin Tutori.& Acade. BrochFinance GuideGovernReport &Indu. Worksh. Paper Report ment book ure 21.2 43.8 45.5 44.2 39.1 44.6 32.3 48.6 48.8 48.1 45.5 46.5 53.0 52.3 56.0 58.9 56.7 46.1 72.6 69.8 66.6 70.2 69.2 52.2 70.1 71.5 68.4 69.4 71.1 75.4 75.4 77.6 80.3 80.2 59.5 78.4 79.3 79.3 79.9 78.3 66.5 78.8 79.5 76.9 77.2 77.4 84.0 82.1 84.6 86.9 86.3 22.1 39.8 29.0 30.8 33.3 32.6 25.5 42.8 30.9 30.0 31.2 26.3 50.0 40.4 51.8 50.4 50.4 40.6 59.7 57.7 48.7 55.8 47.0 44.2 64.4 48.2 45.7 55.3 44.5 65.0 67.5 71.8 66.5 74.1 55.8 71.1 65.9 62.4 62.7 61.9 60.1 74.0 65.8 64.2 67.1 62.6 80.2 77.2 79.3 76.2 78. 27.7 42.4 41.5 39.9 44.0 45.0 27.0 51.1 47.1 50.4 49.8 48.7 54.0 56.1 58.6 57.4 56.9 38.9 57.8 56.3 59.0 60.4 58.7 43.5 70.3 68.8 68.1 68.3 67.2 73.9 73.3 79.4 77.6 77.4 43.4 63.3 62.1 67.0 64.7 67.3 56.0 78.7 71.3 75.3 76.7 74.7 78.7 79.6 82.3 85.3 81.2 23.3 39.3 33.6 33.2 34.2 33.2 31.0 46.2 40.8 39.4 41.5 38.9 48.7 51.7 55.9 59.0 61.3 46.7 66.7 58.6 58.0 56.6 59.5 54.6 72.3 65.7 63.7 64.9 64.4 79.8 80.0 83.4 83.9 84.8 59.1 75.2 69.7 70.3 71.7 72.3 68.9 82.3 76.8 74.4 75.2 75.8 87.0 87.8 89.0 91.9 92. 24.4 39.2 40.8 33.0 43.9 37.2 28.4 36.0 37.6 41.1 39.4 35.9 45.1 45.8 52.0 57.3 54.8 43.9 60.0 60.7 60.9 62.1 61.8 52.0 59.1 56.2 60.1 56.9 54.3 69.5 66.3 72.6 71.9 69.1 56.2 68.8 69.8 71.8 71.1 68.7 58.8 66.1 62.4 67.4 65.1 62.0 75.7 73.9 79.8 80.0 79.0 16.7 38.7 32.7 32.3 26.4 31.8 18.8 36.8 28.4 29.7 29.4 27.0 43.0 43.5 47.2 44.6 50.7 32.4 53.7 48.5 48.8 43.0 46.6 35.1 55.3 46.5 44.0 46.2 43.0 63.5 61.6 66.1 63.8 67.7 41.2 60.5 56.5 57.6 48.8 55.2 43.8 60.8 56.0 52.0 53.7 51.8 73.0 72.4 72.1 71.2 73. 21.1 46.3 40.0 40.4 40.6 40.0 23.5 49.6 43.4 40.9 45.2 46.2 51.5 53.8 57.9 63.8 60.8 38.4 63.8 57.9 63.7 60.0 65.5 44.4 71.1 66.1 69.3 69.9 70.6 75.4 72.8 80.0 79.8 78.7 50.7 72.0 68.0 72.5 72.4 72.6 57.1 77.0 77.2 78.5 75.4 77.8 82.0 81.7 86.7 87.1 85.3 20.7 50.6 42.8 41.7 39.4 39.9 31.2 60.9 51.9 52.8 55.3 50.1 46.9 53.7 53.9 50.5 61.3 37.0 68.5 60.9 61.4 56.8 59.1 53.9 81.3 69.9 72.3 71.1 71.9 71.5 76.4 80.4 71.4 79.5 45.5 72.7 62.8 67.1 65.8 64.0 68.6 88.5 77.4 78.6 79.2 80.0 77.3 83.1 84.9 79.5 85. Laws News Average Macro Micro 31.0 46.1 36.4 38.9 37.0 35.2 38.3 59.5 48.9 51.1 51.1 45.8 54.2 58.3 64.0 64.4 63.6 50.0 61.4 62.7 60.8 61.4 61.4 57.2 70.8 72.0 78.8 72.0 68.2 81.4 82.6 86.4 84.5 81.8 56.0 67.5 66.3 67.5 67.5 67.5 64.8 78.0 79.5 82.6 83.3 75.0 88.3 89.4 92.4 92.0 87.1 15.1 17.1 15.1 15.8 15.1 14.5 16.1 26.3 28.5 24.1 20.4 24.1 33.6 46.7 32.8 35.0 54. 20.4 23.7 20.4 20.4 22.4 19.7 25.5 34.3 32.1 32.8 32.1 31.4 50.4 57.7 49.6 55.5 69.3 23.0 30.3 26.3 25.7 26.3 24.3 33.6 38.7 38.0 40.9 39.4 39.4 58.4 67.9 56.9 61.3 73.0 22.3 40.3 35.7 35.0 35.3 35.4 27.2 45.8 40.6 40.8 40.9 38.9 48.0 50.2 53.0 54.1 57.0 39.4 58.8 55.4 54.8 54.9 54.9 46.3 64.9 59.7 60.3 60.6 58.7 70.6 71.4 74.7 73.5 76.3 49.0 66.0 62.7 64.1 63.1 63.2 57.8 72.3 68.4 69.1 69.2 67.6 78.5 79.5 80.8 81.1 82.2 21.7 40.0 35.2 34.7 33.6 34.6 26.9 44.9 39.6 39.5 39.7 37.9 47.5 50.1 52.7 53.7 57. 39.8 59.5 55.0 54.6 53.6 54.7 46.2 64.8 59.6 59.3 59.7 58.3 71.4 71.8 75.0 74.3 76.8 49.4 66.5 62.9 64.2 62.5 63.5 57.8 72.3 68.5 67.9 68.3 67.2 79.2 80.1 81.0 81.8 83.0 Table 5: Main results for page-level retrieval. OCR-text and VLM-text refer to converting multi-modal content in the document page using OCR and VLM respectively. Image refers to processing document page as screenshot image. (Mphi3v) reuses the image tokenizer from clip-vit-large11 (Mvit). It can deal with high-resolution images by cropping them into subimages, where each sub-image has 336 336 pixels. Document/Query Encoding. DPR-Phi3 and Col-Phi3 represent document page or query using single dense embedding (following DPR [19]) and list of token-level embeddings (following ColBERT [20]), respectively. Specifically, we follow Ma et al. [28] to concatenate document image with text prompt: <s><d> What is shown in this image?</s>. Here, the <d> token is special placeholder token and is replaced by the sequence of patch latent embeddings from the vision encoder. We consider only text queries and use text prompt: <s> query: <q> </s>. Similarly, the placeholder 11ViT-Large: https://huggingface.co/openai/clip-vit-large-patch14- <q> token is replaced by input query."
        },
        {
            "title": "Edpr\nd",
            "content": "= Mphi3v Edpr = Mphi3v (cid:0)Mvit (𝑑), prompt(cid:1) [1], R𝐷1 (cid:0)𝑞, prompt(cid:1) [1], R𝐷1 (1) where the end-of-sequence token </s> from the last hidden state (𝐷1 = 3072) of Mphi3v is used to represent Edpr and Edpr . Ecol = Mproj Mphi3v Ecol = Mproj Mphi3v (cid:0)Mvit (𝑑), prompt(cid:1), R𝑁𝑑 𝐷2 (cid:0)𝑞, prompt(cid:1), R𝑁𝑞 𝐷2 (2) where Mproj is projection layer to map the last hidden states of Mphi3v into reduced dimension 𝐷2 = 128. 𝑁𝑑 2500 for typical high-resolution page and 𝑁𝑞 is the number of query tokens. Benchmarking Multi-Modal Retrieval for Long Documents Preprint, , Query-Doc Similarity. The similarity between the query and the document is computed as follows: Sim(𝑞, 𝑑)𝑑𝑝𝑟 = Edpr (cid:13) (cid:13) (cid:13)Edpr (cid:13) (cid:13) (cid:13) Edpr (cid:13) (cid:13)Edpr (cid:13) (cid:13) (cid:13) (cid:13) (3) where 𝑆𝑖𝑚(𝑞, 𝑑)𝑑𝑝𝑟 is computed as the cosine similarity between their embeddings. and is the dot product. Sim(𝑞, 𝑑)𝑐𝑜𝑙 = 𝑖 [1,𝑁𝑞 ] max 𝑗 [1,𝑁𝑑 ] Ecol (𝑖 ) ( 𝑗 ) Ecol (4) where 𝑆𝑖𝑚(𝑞, 𝑑)𝑐𝑜𝑙 is the sum over all query vectors Ecol , of its maximum dot product with each of the 𝑁𝑑 document embedding vectors Ecol ( 𝑗 ) . (𝑖 ) Contrastive Loss. Given the query 𝑞, we have the positive document 𝑑+ and set of negative documents 𝑑 including hard negatives and in-batch negatives. The hard negatives are negative pages within the document with highest 𝑆𝑖𝑚(𝑞, 𝑑 ) scored by ColPali [15] retriever. We calculate the loss as: 𝑑𝑝𝑟 (𝑞,𝑑 +,𝑑 ) = log exp(Sim 𝑑𝑝𝑟 (𝑞,𝑑 + ) (cid:205)𝑑𝑖 𝑑 +𝑑 exp(Sim /𝜏) 𝑑𝑝𝑟 (𝑞,𝑑𝑖 ) (5) /𝜏) where DPR-Phi3 is trained on the InfoNCE loss, and the temperature parameter 𝜏 = 0.02 in our experiments. L𝑐𝑜𝑙 (𝑞,𝑑 +,𝑑 ) = log (cid:16) 1 + exp (cid:0) max 𝑑𝑖 𝑑 (Sim 𝑐𝑜𝑙 (𝑞,𝑑𝑖 ) ) Sim 𝑐𝑜𝑙 (𝑞,𝑑 + ) (cid:1) (cid:17) (6) where Col-Phi3 is trained via the softplus loss based on the positive scores w.r.t. to the maximal negative scores."
        },
        {
            "title": "6 Experiment\n6.1 Evaluation Metric",
            "content": "The retriever scores each page or layout in the document based on its relevance to the question, and returns the top 𝑘 candidates with the highest scores. Recall@𝑘 is defined as the proportion of the ground truth page/layout evidence that is successfully retrieved. Note for layout matching, we calculate recall based on the overlaps of bounding boxes between retrieved layouts and gold layouts."
        },
        {
            "title": "6.2 Baseline Models and Setting",
            "content": "We evaluate 6 state-of-the-art text retrievers: namely DPR [19], ColBERT [20], BGE [46], E5 [43], Contriever [18], and GTE [26] (refer to Section 7.1). Meanwhile, we evaluate 5 VLM-based retrievers: 3 off-the-shelf models named DSEwikiss [28], DSEdocmatix [28], and ColPali [15] (refer to Section 7.2), and 2 our trained models (see section 5). All retrievers are adapted into dual-task setting: Page Retrieval: For textual retrievers, we use MinerU [42] to extract the layout of all document pages. Non-textual layouts are converted into OCR-text and VLM-text by using Tesseract OCR 12 and GPT-4o respectively. The document page is represent by concatenating the text of all layouts within it. For visual retrievers, we directly utilize document page screenshots. 12https://github.com/tesseract-ocr/tesseract Layout Retrieval: Textual retrievers process non-textual layouts using OCR-text or VLM-text. Visual retrievers process textual layouts in either Pure Image (using cropped image of textual area as input) or Hybrid (using its original text, as VLM can directly encode text as well)."
        },
        {
            "title": "6.3 Main Results for Page-level Retrieval",
            "content": "For page-level retrieval  (Table 5)  , our key findings are as follows: Superiority of Visual Retrievers: Visual retrievers consistently outperform text retrievers across various domains and retrieval metrics (e.g., Top 𝑘 = 1, 3, 5). This highlights the significance of leveraging document screenshots to capture multi-modal information, which is often lost when relying solely on OCR-text. Effectiveness of MMDocIR: Among the visual retrievers, DPRPhi3ours and Col-Phi3ours trained on theMMDocIR train set demonstrate superior performance. It suggests dataset quality to effectively enhances the retrieval ability. Effectiveness of VLM-Text: Although VLM-text approaches underperform visual retrievers, they achieve much better performance than the OCR-text methods. This indicates benefits of using GPT-4o to preserve visual cues in text. Necessity of Token-level Embeddings: Token-level retrievers (e.g., ColBERT, ColPali, Col-Phi3ours) compared with their document/page-level counterparts (e.g., BGE, DSE, DPR-Phi3ours), achieve more advantageous results in Recall@1 and marginal performance increase in Recall@5/10. Yet, the storage overhead of token-level embeddings can be 10 times more than page-level embeddings. Top 5 Coverage: Retrieving top 5 pages provides substantial coverage, ensuring that relevant information is captured."
        },
        {
            "title": "6.4 Main Results for Layout-level Retrieval",
            "content": "For layout-level retrieval  (Table 6)  , our key findings are as follows: Superiority of Visual Retrievers: Visual retrievers exhibit substantial performance advantages over text retrievers utilizing OCR-text. This highlights the limitations of OCR in capturing the multi-modal nature of documents, where visual context can significantly enhance retrieval accuracy. Effectiveness of VLM-Text: Interestingly, VLM-text approaches achieve comparable performance as visual retrievers. This indicates strong image description capabilities of state-of-the-art VLM, which can offer significant benefits to textual retrievers in understanding multi-modal information. Comparison of Hybrid vs. Pure Image Sequences: Visual retrievers relying on hybrid image-text sequences generally perform less effectively than those utilizing pure image sequences. This suggests that current VLMs may have stronger capabilities in modeling images than text within the multi-modal framework. Necessity of Token-level Embeddings: Token-level retrievers (e.g., ColBERT, ColPali, Col-Phi3ours) compared with their document/page-level counterparts (e.g., BGE, DSE, DPR-Phi3ours), do not achieve advantageous results in layout retrieval. Top 10 Coverage: Retrieving top 10 layouts does not provide comprehensive coverage of the ground truth layouts for document retrieval tasks. It suggests the challenge of layout retrieval. Preprint, , Dong et al. Domain Method Resear. Admin Tutori.& Acade. BrochFinance GuideGovernReport &Indu. Worksh. Paper Report ment book ure 1 = 𝑘 @ c 5 = 𝑘 @ a l i R t l i R s a r R u T e e u t DPR ColBERT BGE E5 Contriever - t DPR t - ColBERT BGE E5 Contriever DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours b - P DPR ColBERT BGE E5 Contriever t - t DPR - ColBERT BGE E5 Contriever DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours b - P 3.4 5.0 7.0 6.3 6.7 11.6 22.0 19.2 15.9 23.4 20.6 19.9 22.5 21.1 22.6 14.0 18.2 17.7 28.3 26. 7.3 10.9 11.9 12.8 11.9 31.0 41.8 41.0 35.4 40.2 42.4 39.6 40.7 45.5 46.4 31.8 37.3 40.1 54.2 50.9 7.2 8.8 10.9 6.0 7.0 9.5 14.9 15.2 8.8 7.5 15.1 11.4 21.3 22.1 22.0 10.4 11.6 12.3 11.1 12.6 12.5 23.8 20.3 16.2 17.9 25.7 37.7 28.1 28.1 29.2 32.9 36.2 45.9 37.7 38.2 29.5 26.7 38.3 27.4 25.5 1.2 4.7 3.7 2.6 3.8 19.2 28.0 24.6 27.7 28.2 31.0 31.5 36.6 36.8 37.5 29.8 32.7 30.0 35.5 33.7 5.6 10.2 13.6 8.9 11.9 36.7 53.7 52.7 51.7 54.1 56.3 53.9 54.9 57.0 53.1 51.1 48.2 55.2 53.8 49.1 11.3 16.4 14.3 14.0 14.3 19.2 28.3 28.7 24.3 26.8 31.1 30.1 30.9 35.2 34.9 18.0 24.0 18.4 39.2 37. 24.0 32.2 30.0 31.9 28.8 44.9 61.8 59.2 58.5 57.9 58.5 57.5 58.5 62.9 61.8 43.0 49.7 49.2 64.9 66.2 3.0 2.0 2.3 3.5 4.1 14.9 17.9 12.8 14.6 17.1 20.1 17.8 26.8 25.6 28.9 13.7 17.7 19.0 29.3 30.1 8.9 6.8 11.7 10.9 9.3 33.0 35.1 36.7 33.2 36.8 39.8 33.7 42.6 41.4 45.0 34.5 34.5 42.7 36.6 40.3 9.8 13.2 16.3 14.4 13.3 15.9 29.7 27.6 21.8 25.7 29.2 30.0 32.1 28.7 30.3 20.4 27.2 25.5 25.2 27.9 16.9 25.5 27.7 23.6 24.6 34.1 52.4 46.0 41.2 47.1 50.6 52.5 51.2 51.1 54.6 39.3 48.6 47.6 45.3 48.1 8.2 4.6 8.4 6.1 8.3 15.8 21.1 19.7 14.7 16.1 22.0 20.8 19.3 24.1 22.7 13.5 16.7 20.6 27.4 24. 13.6 17.0 18.8 19.9 18.1 34.9 46.1 50.7 40.2 44.6 41.6 42.8 45.7 45.5 45.7 38.3 41.0 40.8 49.6 48.3 24.7 50.8 46.1 44.7 43.6 25.6 52.6 47.0 45.6 43.6 39.3 46.5 52.5 38.3 50.2 46.0 48.1 49.7 37.0 46.2 47.2 78.7 68.5 76.1 64.3 49.9 83.2 72.0 79.7 68.8 68.6 69.4 76.8 65.1 68.8 71.3 72.2 78.6 69.5 62.3 Laws News Average Macro Micro 30.9 47.7 45.5 45.4 43.9 34.7 54.5 52.3 53.0 51.5 37.5 39.4 51.8 35.4 45.1 45.1 45.5 51.2 32.9 47. 50.2 63.4 65.4 68.8 68.0 56.3 70.1 71.5 77.9 76.2 60.9 63.1 74.5 60.8 65.7 71.3 69.4 68.7 63.8 60.6 26.3 45.3 35.8 40.5 42.9 27.0 44.5 35.8 40.5 42.9 35.8 31.4 33.6 27.4 26.3 34.7 33.0 40.9 22.2 21.9 51.1 63.2 59.1 63.9 62.7 51.1 62.5 59.9 64.6 61.9 50.0 48.9 48.9 49.3 43.8 57.1 54.2 61.0 53.2 48.5 12.6 19.8 19.0 18.4 18.8 19.3 31.3 28.3 26.7 28.3 28.2 27.9 32.7 29.5 31.1 24.6 27.5 28.5 28.8 30.8 23.7 33.2 32.7 33.3 31.7 39.8 54.4 51.8 51.1 51.7 50.2 49.8 54.0 51.6 52.3 46.7 48.2 52.2 51.8 50.0 12.4 19.1 18.5 17.9 18.1 19.2 31.4 29.0 26.4 28.9 29.2 29.1 32.5 30.2 31.6 23.4 27.4 27.1 30.5 32. 23.5 32.6 32.2 32.7 31.2 40.4 56.0 53.2 51.8 53.0 52.1 51.9 54.3 53.9 54.5 45.6 49.3 51.4 54.4 53.5 - 𝑘 @ 0 1 = t t DPR v t e - t DPR ColBERT BGE E5 Contriever 14.9 9.1 17.9 15.8 15.9 39.9 43.9 41.9 45.6 44.8 47.8 44.7 48.8 48.4 50.7 42.6 45.9 50.7 42.4 43.9 Table 6: Main results for layout-level retrieval. OCR-text and VLM-text refer to converting multi-modal layouts using OCR and VLM respectively. Pure-Image and Hybrid refer to reading textual layouts in image and text format respectively. ColBERT BGE E5 Contriever DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours DSEwikiss DSEdocmatix ColPali DPR-Phi3ours Col-Phi3ours 29.3 37.4 37.3 38.3 36.6 50.5 63.7 62.4 61.2 62.2 61.1 59.9 63.2 62.8 63.3 56.0 58.9 60.2 62.9 62. 10.5 14.1 15.7 16.9 15.1 42.2 51.0 51.1 45.3 49.9 55.9 53.7 53.6 58.1 57.7 44.1 49.9 52.1 65.2 59.1 21.1 31.1 24.3 24.5 25.7 33.1 48.7 38.7 38.6 41.3 41.3 43.3 54.1 49.1 50.5 34.3 37.3 46.4 33.7 38.7 8.8 13.1 15.9 13.8 14.2 52.1 60.6 62.1 62.0 62.0 61.5 59.6 64.4 67.0 66.6 57.6 57.3 65.0 60.3 57.0 56.4 67.9 73.4 77.9 71.8 61.7 74.8 78.7 82.4 78.0 68.3 69.2 82.5 66.2 74.8 79.1 74.9 71.4 72.5 68.0 29.9 37.6 37.8 39.0 37.3 49.5 61.9 60.3 60.4 60.4 58.5 57.5 62.0 60.2 61.1 55.8 57.4 60.0 59.9 58.7 17.0 22.4 25.3 24.7 24.2 44.0 53.7 58.7 55.3 54.5 54.2 50.3 54.0 57.8 53.6 48.6 50.1 51.0 52.9 51. 19.6 31.0 31.6 26.7 27.9 43.5 61.6 55.6 50.0 56.5 60.7 59.1 60.7 57.9 59.3 50.7 57.9 53.8 54.1 57.7 59.7 83.9 76.3 83.5 76.8 62.8 88.4 80.8 87.1 81.3 72.9 75.4 81.9 68.7 68.5 81.1 77.9 82.7 79.1 72.4 58.9 65.4 62.1 66.1 64.9 59.7 66.4 63.5 66.8 64.9 54.4 53.7 50.4 54.4 57.5 63.7 61.5 62.5 65.6 60.7 32.0 38.4 35.9 40.1 36.8 56.2 69.8 71.5 70.5 70.5 68.1 66.5 69.5 74.7 72.3 56.3 61.3 64.4 72.9 77.7 e e r H u I - P c R"
        },
        {
            "title": "6.5 Analysis of OCR and VLM Text",
            "content": "Interestingly, text retrievers leveraging VLM-text can significantly outperform OCR-text, in both page and layout retrieval. OCR-text which is the the raw text extracted via OCR tools, and as experimental results suggest, it is not suitable for multi-modal retrieval. In comparison, VLM-text can facilitate multi-modal retrieval, suggesting that VLMs can largely preserve rich multi-modal information. The averaged word length and distribution of OCR and VLM text obtained from the tables and figures in MMDocIR in shown in Figure 3. Observe that the VLM-text is much longer than OCRtext. Specifically, the length of VLM-text is 1.5 and 3.8 times more than that of OCR-text in table and figure. Tables tend to contain numerical or structured layouts, which can be well recorded by text. Figures contain mostly graphical elaboration and visual cues. Hence, making OCR tools struggle in getting textual information from figures. Although VLM-text can offer much comprehensive text information than OCR-text, it comes at the cost of higher computational overhead and longer latency. Benchmarking Multi-Modal Retrieval for Long Documents Preprint, ,"
        },
        {
            "title": "7.3 Multi-modal Document Retrieval Datasets",
            "content": "As described in Section 1 and Table 1, there is notable lack of robust benchmark for multi-modal document retrieval. DocCVQA [39] is the first multi-modal document retrieval-answering task, which extracts information from document image collection and then provides the answer. However, DocCVQA provides only 20 questions, and the corpora is limited to finance domain specifically in political disclosure. PDF-MVQA [13] is tailored for multi-modal retrieval in research articles. Note that PDF-MVQA is annotated by Chat-GPT (GPT-3.5-turbo) but not experts, and its corpora contains only biomedical articles. SciMMIR [45] proposes multi-modal retrieval in scientific research papers. However, it provides only the image-caption pairs, not the natural user query with the targeting document pages. Ma et al. [28] introduce two relevant datasets, namely Wiki-SS and DocMatix-IR. Wiki-SS is constructed based on Natural Questions [21] where the evidence passage is the screenshot wikipedia webpage. Natural Questions are mainly designed for text rather than multi-modal retrieval. We also notice that the screenshot may not include the ground-truth evidence as only the front-page is screenshotted. DocMatix-IR is constructed from the largest document understanding dataset DocMatix [23] via filtering and hard negative mining. However, the VQA questions in DocMatix is constructed by Phi-3-small [1] rather than experts, and these VQA questions are not de-contextualized for retrieval. ViDoRe [15] is the most relevant benchmark to MMDocIR. It integrates multiple DocVQA datasets [25, 30, 31, 51], and provides new documents in scientific, medical, administrative, and environment domains. Upon thorough examination of the 1,180 questions within the ViDoRe dataset, we find that more than 80% of these questions present drawbacks in terms of their complexity and context-specific nature. Typical examples include Whats the title of the picture and whats the authors name. They tend to be either overly simplistic or excessively dependent on specific context which does not necessarily reflect the broader, more complex needs of effective IR. Additionally, significant limitation of the ViDoRe dataset is its reliance on sampled document pages or cropped images rather than providing the complete document corpora."
        },
        {
            "title": "8 Conclusion",
            "content": "In conclusion, multi-modal document retrieval presents complex challenge that necessitates the integration of diverse data modalities beyond plain text. Our contributions address this challenge by introducing the MMDocIR benchmark, which includes innovative dual-task retrieval functionalities targeting both page-level and layout-level specifics of documents. The MMDocIR benchmark comprises rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it pivotal resource for advancing multi-modal document retrieval for both training and evaluation purposes. Our comprehensive evaluation reveals that visual-driven retrievers significantly outperform text-driven ones, underscoring the critical importance of incorporating visual content in enhancing the effectiveness of document retrieval systems. Future work can expand on optimizing these retrieval algorithms to improve the accuracy and efficiency of multi-modal document retrieval systems. (a) Avg word length (b) Distribution density of word length Figure 3: Avg length and distribution of OCR and VLM text."
        },
        {
            "title": "7 Related Work\n7.1 Text-Centric Document Retrieval",
            "content": "Document indexing. The process of indexing multi-modal document involves multiple steps, including Document Parsing [6, 42], Optical Character Recognition (OCR) [5, 7, 32], Layout Detection [37, 47, 48], Chunking [9, 14, 33], and Image Captioning [2, 49]. These steps are time-consuming and can introduce errors that impact the overall retrieval performance. Text retrieval. Current text retrieval are primarily classified into sparse and dense retrieval. For two widely-used sparse retrievers: TF-IDF [35] calculates the relevance via word frequency with the inverse document frequency, and BM25 [34] introduces nonlinear word frequency saturation and length normalization. Dense retrievers encode content into vector representations. DPR [19] is the pioneering work of dense vector representations for QA tasks. Similarly, ColBERT [20] introduces an efficient question-document interaction model with late fine-grained term matching. Contriever [18] leverages contrastive learning to improve content dense encoding. E5 [43] and BGE [46] propose novel training and data preparation techniques to enhance retrieval performance. Moreover, GTE [26] integrates graph-based techniques to enhance dense embedding. Most systems focus on text-based representations, neglecting the valuable visual information present in documents."
        },
        {
            "title": "7.2 Vision-Driven Document Retrieval",
            "content": "Vision Language Models (VLMs) [1, 3, 4, 10] have gained popularity for their ability to understand and generate text based on combined text and visual inputs. This advancement has led to the development of cutting-edge visual-driven retrievers, such as ColPali [15] and DSE [28]. These models specifically leverage PaliGemma [4] and Phi3-V [1] to directly encode document page screenshots for multi-modal document retrieval. ColPali adopts similar questiondocument interaction as ColBERT, and represent each document page in token-level embeddings. By contrast, DSE is similar to DPR to encode each page with single dense embedding. Visual Retrievers can directly model useful visual information, allowing for the direct utilization of multi-modal content without first converting it into text. Despite these advancements, visual retrievers face challenges, particularly in dealing with text details when document page resolutions are high. The high resolution of document pages substantially increases the computational cost and complexity of the embedding process, which may hinder the models performance. Preprint, , Dong et al. References [1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv:2404.14219 [cs.CL] https://arxiv.org/abs/2404.14219 [2] Jyoti Aneja, Aditya Deshpande, and Alexander G. Schwing. 2018. Convolutional Image Captioning. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. Computer Vision Foundation / IEEE Computer Society, Salt Lake City, UT, USA, 55615570. https://doi.org/10.1109/CVPR.2018.00583 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: Versatile VisionLanguage Model for Understanding, Localization, Text Reading, and Beyond. arXiv:2308.12966 [cs.CV] https://arxiv.org/abs/2308. [4] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey A. Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bosnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier J. Hénaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. 2024. PaliGemma: versatile 3B VLM for transfer. https://doi.org/10.48550/ARXIV.2407.07726 arXiv:2407.07726 [5] Eugene Borovikov. 2014. survey of modern optical character recognition techniques. arXiv:1412.4183 http://arxiv.org/abs/1412.4183 [6] Hui Chao and Jian Fan. 2004. Layout and Content Extraction for PDF Documents. In Document Analysis Systems VI, 6th International Workshop, DAS 2004, Florence, Italy, September 8-10, 2004, Proceedings (Lecture Notes in Computer Science, Vol. 3163), Simone Marinai and Andreas Dengel (Eds.). Springer, Florence, Italy, 213224. https://doi.org/10.1007/978-3-540-28640-0_20 [7] Arindam Chaudhuri, Krupa Mandaviya, Pratixa Badelia, and Soumya K. Ghosh. 2017. Optical Character Recognition Systems. Springer International Publishing, Cham, 941. https://doi.org/10.1007/978-3-319-50252-6_2 [8] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. arXiv:2310.05029 [cs.CL] [9] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. 2024. Dense Retrieval: What Retrieval Granularity Should We Use?. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 1515915177. https://doi.org/10.18653/v1/2024.emnlp-main.845 [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2024. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. arXiv:2312.14238 [cs.CV] https://arxiv.org/abs/2312.14238 [11] Lei Cui, Yiheng Xu, Tengchao Lv, and Furu Wei. 2021. Document AI: Benchmarks, Models and Applications. arXiv:2111.08609 [cs.CL] https://arxiv.org/abs/2111. 08609 [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 41714186. https://doi. org/10.18653/v1/N19-1423 [13] Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and Soyeon Caren Han. 2024. MMVQA: Comprehensive Dataset for Investigating Multipage Multimodal Information Retrieval in PDF-based Visual Question Answering. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024. ijcai.org, 62436251. https://www.ijcai. org/proceedings/2024/ [14] Kuicai Dong, Derrick Goh Xin Deik, Yi Quan Lee, Hao Zhang, Xiangyang Li, Cong Zhang, and Yong Liu. 2024. MC-indexing: Effective Long Document Retrieval via Multi-view Content-aware Indexing. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 26732691. https://doi.org/10.18653/v1/2024.findings-emnlp.150 [15] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024. ColPali: Efficient Document Retrieval with Vision Language Models. arXiv:2407.01449 [cs.IR] https://arxiv.org/abs/2407. 01449 [16] Ehtesham Hassan, Santanu Chaudhury, and Madan Gopal. 2013. Multi-modal Information Integration for Document Retrieval. In 12th International Conference on Document Analysis and Recognition, ICDAR 2013, Washington, DC, USA, August 25-28, 2013. IEEE Computer Society, Washington, DC, USA, 12001204. https: //doi.org/10.1109/ICDAR.2013.243 [17] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). Advances in Neural Information Processing Systems, virtual. https://datasets-benchmarks-proceedings.neurips.cc/paper/ 2021/hash/6ea9ab1baa0efb9e19094440c317e21b-Abstract-round1.html [18] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022 (2022). https://openreview.net/forum?id=jKN1pXi7b0 [19] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 67696781. https://doi.org/10.18653/v1/2020.emnlp-main.550 [20] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA, 3948. https://doi.org/10.1145/3397271. [21] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452466. https://doi.org/10.1162/tacl_a_00276 [22] Jordy Van Landeghem, Rafal Powalski, Rubèn Tito, Dawid Jurkiewicz, Matthew B. Blaschko, Lukasz Borchmann, Mickaël Coustaty, Sien Moens, Michal Pietruszka, Bertrand Anckaert, Tomasz Stanislawek, Pawel Józiak, and Ernest Valveny. 2023. Document Understanding Dataset and Evaluation (DUDE). In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023. IEEE, Paris, France, 1947119483. https://doi.org/10.1109/ICCV51070.2023.01789 [23] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. 2024. Building and better understanding vision-language models: insights and future directions. arXiv:2408.12637 [cs.CV] [24] Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, and Sung Ju Hwang. 2024. Unified Multimodal Interleaved Document Representation for Retrieval. arXiv:2410.02729 [cs.CL] https://arxiv.org/abs/2410.02729 [25] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal ArXiv: Dataset for Improving Scientific Comprehension of Large Vision-Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1436914387. https://doi.org/10.18653/v1/2024. acl-long.775 [26] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards General Text Embeddings with Multi-stage Contrastive Learning. arXiv:2308.03281 [cs.CL] [27] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL] https://arxiv.org/abs/1907.11692 Benchmarking Multi-Modal Retrieval for Long Documents Preprint, , 1189711916. https://doi.org/10.18653/v1/2024.acl-long.642 [45] Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, and Chenghua Lin. 2024. SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 12560 12574. https://doi.org/10.18653/v1/2024.findings-acl.746 [46] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL] [47] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. LayoutLM: Pre-training of Text and Layout for Document Image Understanding. In KDD 20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM, CA, USA, 11921200. https://doi.org/10.1145/3394486.3403172 [48] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. 2021. LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Virtual, 25792591. https://doi.org/ 10.18653/V1/2021.ACL-LONG. [49] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. 2016. Image Captioning with Semantic Attention. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, Las Vegas, NV, USA, 46514659. https://doi.org/10.1109/ CVPR.2016.503 [50] Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. 2022. MultiView Document Representation Learning for Open-Domain Dense Retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 59906000. https://doi.org/10.18653/v1/2022.acl-long.414 [51] Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. 2022. Towards Complex Document Understanding By Discrete Reasoning. In MM 22: The 30th ACM International Conference on Multimedia, Lisboa, Portugal, October 10 - 14, 2022, João Magalhães, Alberto Del Bimbo, Shinichi Satoh, Nicu Sebe, Xavier Alameda-Pineda, Qin Jin, Vincent Oria, and Laura Toni (Eds.). ACM, Lisboa Portugal, 48574866. https://doi.org/10.1145/3503161.3548422 [52] Fengbin Zhu, Ziyang Liu, Xiang Yao Ng, Haohui Wu, Wenjie Wang, Fuli Feng, Chao Wang, Huanbo Luan, and Tat Seng Chua. 2024. MMDocBench: Benchmarking Large Vision-Language Models for Fine-Grained Visual Document Understanding. arXiv:2410.21311 [cs.CV] https://arxiv.org/abs/2410.21311 [53] Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. 2024. DOCBENCH: Benchmark for Evaluating LLM-based Document Reading Systems. arXiv:2407.10701 [cs.CL] https://arxiv. org/abs/2407.10701 [28] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. 2024. Unifying Multimodal Retrieval via Document Screenshot Embedding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 64926505. https://doi.org/10.18653/v1/2024.emnlp-main. [29] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, YuGang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun. 2024. MMLongBenchDoc: Benchmarking Long-context Document Understanding with Visualizations. arXiv:2407.01523 [cs.CV] https://arxiv.org/abs/2407.01523 [30] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and C. V. Jawahar. 2022. InfographicVQA. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022. IEEE, 25822591. https://doi.org/10.1109/WACV51458.2022.00264 [31] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. 2021. DocVQA: Dataset for VQA on Document Images. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021. IEEE, Waikoloa, HI, USA, 21992208. https://doi.org/10.1109/WACV48630.2021.00225 [32] Shunji Mori, Hirobumi Nishida, and Hiromitsu Yamada. 1999. Optical character recognition. John Wiley & Sons, Inc., USA. [33] Vatsal Raina and Mark Gales. 2024. Question-Based Retrieval using Atomic Units for Enterprise RAG. arXiv:2405.12363 [cs.CL] [34] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994 (NIST Special Publication, Vol. 500-225), Donna K. Harman (Ed.). National Institute of Standards and Technology (NIST), Maryland, USA, 109126. http: //trec.nist.gov/pubs/trec3/papers/city.ps.gz [35] Gerard Salton, Edward A. Fox, and Harry Wu. 1983. Extended Boolean Information Retrieval. Commun. ACM 26, 11 (1983), 10221036. https://doi.org/10.1145/ 182.358466 [36] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, Vienna, Austria. https://openreview.net/forum?id=GN921JHCRw [37] Abdellatif Sassioui, Rachid Benouini, Yasser El Ouargui, Mohamed El-Kamili, Meriyem Chergui, and Mohammed Ouzzif. 2023. Visually-Rich Document Understanding: Concepts, Taxonomy and Challenges. In 10th International Conference on Wireless Networks and Mobile Communications, WINCOM 2023, Istanbul, Turkey, October 26-28, 2023, Khalil Ibrahimi, Mohamed El-Kamili, Abdellatif Kobbane, and Ibraheem Shayea (Eds.). IEEE, Istanbul, Turkey, 17. https://doi.org/10.1109/WINCOM59760.2023.10322990 [38] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. SlideVQA: Dataset for Document Visual Question Answering on Multiple Images. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, Brian Williams, Yiling Chen, and Jennifer Neville (Eds.). AAAI Press, Washington, DC, USA, 1363613645. https://doi.org/10.1609/AAAI.V37I11.26598 [39] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2021. Document Collection Visual Question Answering. In 16th International Conference on Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5-10, 2021, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12822), Josep Lladós, Daniel Lopresti, and Seiichi Uchida (Eds.). Springer, 778792. https://doi.org/10.1007/978-3-030-86331-9_ [40] Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023. Hierarchical multimodal transformers for Multi-Page DocVQA. arXiv:2212.05935 [cs.CV] https://arxiv.org/abs/2212.05935 [41] Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, and Ian Foster. 2024. SciQAG: Framework for AutoGenerated Science Question Answering Dataset with Fine-grained Evaluation. arXiv:2405.09939 [cs.CL] https://arxiv.org/abs/2405.09939 [42] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. 2024. MinerU: An Open-Source Solution for Precise Document Content Extraction. arXiv:2409.18839 [cs.CV] https://arxiv.org/abs/2409.18839 [43] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv:2212.03533 [cs.CL] [44] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving Text Embeddings with Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand,"
        }
    ],
    "affiliations": [
        "Noahs Ark Lab, Huawei"
    ]
}