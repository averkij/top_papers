[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13370/x1.png",
                "caption": "Figure 1:(a) Illustration of personalization,demonstrating how text-to-image (T2I) diffusion models can learn and reproduce a visual concept from given reference images.(b) Illustration of component-controllable personalization,depicting a newly formulated task that aims to modify a specific component of a visual concept during personalization.(c) Example images generated by MagicTailor,showcasing the effectiveness of the proposed MagicTailor, a novel framework that adapts T2I diffusion models for component-controllable personalization.\nFor clarity, theredandbluecircles are used to highlight the target concept and component, respectively.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13370/x2.png",
                "caption": "Figure 2:Major challenges in component-controllable personalization.(a) Semantic pollution:(i) Undesired visual elements may inadvertently disturb the personalized concept. (ii) A simple mask-out strategy is ineffective and causes unintended compositions, whereas (iii) our DM-Deg effectively suppresses unwanted visual semantics, preventing such pollution.(b) Semantic imbalance:(i) Simultaneously learning the concept and component can lead to imbalance, resulting in concept or component distortion (here we present a case for the former). (ii) Our DS-Bal ensures balanced learning, enhancing personalization performance.",
                "position": 157
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13370/x3.png",
                "caption": "Figure 3:Pipeline overview of MagicTailor.Using reference images as the inputs, MagicTailor fine-tunes a T2I diffusion model to learn both the target concept and component, enabling the generation of images that seamlessly integrate the component into the concept. Two key techniques, Dynamic Masked Degradation (DM-Deg, see Section3.2) and Dual-Stream Balancing (DS-Bal, see Section3.3), address the challenges of semantic pollution and semantic imbalance, respectively. For clarity, only one image per concept/component is presented and the warm-up stage is not depicted.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x4.png",
                "caption": "Figure 4:Motivation of dynamic intensity.(a) Fixed intensity (Œ±d=0.5subscriptùõºùëë0.5\\alpha_{d}=0.5italic_Œ± start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 0.5here) could cause noisy generated images.\n(b) Our dynamic intensity helps to mitigate noise memorization.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x5.png",
                "caption": "Figure 5:Visualization of the learning process.(a) The vanilla learning paradigm lapses into overemphasizing the easier one.\n(b) DS-Bal effectively balances the learning of the concept and component.",
                "position": 311
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13370/x6.png",
                "caption": "Figure 6:Qualitative comparisons.We present images generated by MagicTailor and the compared methods for various domains.\nMagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality.\nMore results are provided in AppendixD.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x7.png",
                "caption": "Figure 7:Ablation of loss weights.We report CLIP-T for text alignment, and DreamSim for identity fidelity as it is most similar to human judgments(Fu et¬†al.,2023).\nFor reference, we also present the results of the second-best method in Table1, highlighting our robustness on loss weights.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x8.png",
                "caption": "Figure 8:(a) Decoupled generation.MagicTailor can also separately generate the target concept and component, enriching prospective combinations.(b) Controlling multiple components.MagicTailor shows the potential to handle more than one component, highlighting its effectiveness.",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x9.png",
                "caption": "Figure 9:Enhancing other generative tools.MagicTailor can conveniently collaborate with a variety of generative tools that focus on other tasks,\nequipping them with an additional ability to control the concept‚Äôs component in their pipelines.",
                "position": 858
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details of Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix BAdditional Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13370/x10.png",
                "caption": "Figure 10:Comparing with detailed-text-guided generation.We use GPT-4o to generate and merge detailed textual descriptions for the target concept and component, which are fed into Stable Diffusion 2.1 to conduct text-to-image generation. This paradigm cannot perform well and produce inconsistent images, while MagicTailor can achieve faithful and consistent generation.",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x11.png",
                "caption": "Figure 11:Comparing with commercial models.We input the reference images of the target concept and component to GPT-4o and Gemini, along with structured text prompts, for conducting image generation.\nEven though capable of handling multiple general tasks, these models still fall short in this task.\nIn contrast, our MagicTailor performs well using a dedicated framework.",
                "position": 1886
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13370/x12.png",
                "caption": "Figure 12:Ablation of the number of reference images.We present qualitative results to show that MagicTailor can still achieve satisfactory performance when provided only 1 or 2 reference image(s) per concept and component.",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x13.png",
                "caption": "Figure 13:Ablation of the linking word.We present qualitative results generated with different linking words,\nshowing the robustness of MagicTailor.",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2410.13370/x14.png",
                "caption": "Figure 14:More qualitative comparisons.We present images generated by our MagicTailor and SOTA methods of personalization for various domains including characters, animation, buildings, objects, and animals.\nMagicTailor generally achieves promising text alignment, strong identity fidelity, and high generation quality.",
                "position": 1998
            }
        ]
    },
    {
        "header": "Appendix DMore Qualitative Results",
        "images": []
    }
]