[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x1.png",
                "caption": "Figure 1:Comparison of the time taken to generate 100K tokens using autoregressive (AR) andTokenSwiftwith prefix length of 4096 onLLaMA3.1-8b. As seen,TokenSwiftaccelerates the AR process from nearly 5 hours to just 90 minutes.",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Challenges",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x2.png",
                "caption": "Figure 2:Illustration ofTokenSwiftFramework.First, target model (LLM) with partial KV cache and three linear layers outputs 4 logits in a single forward pass. Tree-based attention is then applied to construct candidate tokens. Secondly, top-k𝑘kitalic_kcandidate4444-grams are retrieved accordingly. These candidates compose draft tokens, which are fed into the LLM with full KV cache to generate target tokens. The verification is performed by checking if draft tokens match exactly with target tokens (Algorithm1). Finally, we randomly select one of the longest valid draft tokens, and updaten𝑛nitalic_n-gram table and KV cache accordingly.",
                "position": 311
            }
        ]
    },
    {
        "header": "3TokenSwift",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x3.png",
                "caption": "Figure 3:Upper: Theacceptance rateα𝛼\\alphaitalic_αfork=20𝑘20k=20italic_k = 20andk=0𝑘0k=0italic_k = 0, along with then𝑛nitalic_n-gram acceptance rateβ𝛽\\betaitalic_βfork=20𝑘20k=20italic_k = 20, plotted against varying generation lengths. Lower: Thespeedup×\\times×achieved at different generation lengths.",
                "position": 1069
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x4.png",
                "caption": "Figure 4:Upper: Theacceptance rateα𝛼\\alphaitalic_αandn𝑛nitalic_n-gram acceptance rateβ𝛽\\betaitalic_βversus varyingk𝑘kitalic_k. Lower: Thespeedup×\\times×versus varyingk𝑘kitalic_k.",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x5.png",
                "caption": "Figure 5:Case Study onLLaMA3.1-8b. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. Thebluetext is repetition part. SeeAppendixGfor more cases.",
                "position": 1487
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALossless Nature of Speculative Decoding",
        "images": []
    },
    {
        "header": "Appendix BAdditional Training and Inference Details.",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x6.png",
                "caption": "",
                "position": 2561
            }
        ]
    },
    {
        "header": "Appendix CPre-training Details of the Llama3.1 Draft Model",
        "images": []
    },
    {
        "header": "Appendix DDifferent Sampling Method",
        "images": []
    },
    {
        "header": "Appendix ETree-Based Attention",
        "images": []
    },
    {
        "header": "Appendix FMore Ablation Experiments",
        "images": []
    },
    {
        "header": "Appendix GMore Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x7.png",
                "caption": "Figure 6:Case Study onYaRN-LLaMA2-7b-128k. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. Thebluetext is repetition part.",
                "position": 3293
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x8.png",
                "caption": "(a)Cross Entropy Loss Training Curve of the First Linear Layer",
                "position": 3300
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x8.png",
                "caption": "(a)Cross Entropy Loss Training Curve of the First Linear Layer",
                "position": 3303
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x9.png",
                "caption": "(b)Cross Entropy Loss Training Curve of the Second Linear Layer",
                "position": 3309
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x10.png",
                "caption": "(c)Cross Entropy Loss Training Curve of the Third Linear Layer",
                "position": 3315
            }
        ]
    },
    {
        "header": "Appendix HTraining Loss Curve",
        "images": []
    }
]