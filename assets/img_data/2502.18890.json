[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x1.png",
                "caption": "Figure 1:Comparison of the time taken to generate 100K tokens using autoregressive (AR) andTokenSwiftwith prefix length of 4096 onLLaMA3.1-8b. As seen,TokenSwiftaccelerates the AR process from nearly 5 hours to just 90 minutes.",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Challenges",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x2.png",
                "caption": "Figure 2:Illustration ofTokenSwiftFramework.First, target model (LLM) with partial KV cache and three linear layers outputs 4 logits in a single forward pass. Tree-based attention is then applied to construct candidate tokens. Secondly, top-kğ‘˜kitalic_kcandidate4444-grams are retrieved accordingly. These candidates compose draft tokens, which are fed into the LLM with full KV cache to generate target tokens. The verification is performed by checking if draft tokens match exactly with target tokens (Algorithm1). Finally, we randomly select one of the longest valid draft tokens, and updatenğ‘›nitalic_n-gram table and KV cache accordingly.",
                "position": 311
            }
        ]
    },
    {
        "header": "3TokenSwift",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x3.png",
                "caption": "Figure 3:Upper: Theacceptance rateÎ±ğ›¼\\alphaitalic_Î±fork=20ğ‘˜20k=20italic_k = 20andk=0ğ‘˜0k=0italic_k = 0, along with thenğ‘›nitalic_n-gram acceptance rateÎ²ğ›½\\betaitalic_Î²fork=20ğ‘˜20k=20italic_k = 20, plotted against varying generation lengths. Lower: ThespeedupÃ—\\timesÃ—achieved at different generation lengths.",
                "position": 1069
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x4.png",
                "caption": "Figure 4:Upper: Theacceptance rateÎ±ğ›¼\\alphaitalic_Î±andnğ‘›nitalic_n-gram acceptance rateÎ²ğ›½\\betaitalic_Î²versus varyingkğ‘˜kitalic_k. Lower: ThespeedupÃ—\\timesÃ—versus varyingkğ‘˜kitalic_k.",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x5.png",
                "caption": "Figure 5:Case Study onLLaMA3.1-8b. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. Thebluetext is repetition part. SeeAppendixGfor more cases.",
                "position": 1487
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALossless Nature of Speculative Decoding",
        "images": []
    },
    {
        "header": "Appendix BAdditional Training and Inference Details.",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x6.png",
                "caption": "",
                "position": 2561
            }
        ]
    },
    {
        "header": "Appendix CPre-training Details of the Llama3.1 Draft Model",
        "images": []
    },
    {
        "header": "Appendix DDifferent Sampling Method",
        "images": []
    },
    {
        "header": "Appendix ETree-Based Attention",
        "images": []
    },
    {
        "header": "Appendix FMore Ablation Experiments",
        "images": []
    },
    {
        "header": "Appendix GMore Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18890/x7.png",
                "caption": "Figure 6:Case Study onYaRN-LLaMA2-7b-128k. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. Thebluetext is repetition part.",
                "position": 3293
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x8.png",
                "caption": "(a)Cross Entropy Loss Training Curve of the First Linear Layer",
                "position": 3300
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x8.png",
                "caption": "(a)Cross Entropy Loss Training Curve of the First Linear Layer",
                "position": 3303
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x9.png",
                "caption": "(b)Cross Entropy Loss Training Curve of the Second Linear Layer",
                "position": 3309
            },
            {
                "img": "https://arxiv.org/html/2502.18890/x10.png",
                "caption": "(c)Cross Entropy Loss Training Curve of the Third Linear Layer",
                "position": 3315
            }
        ]
    },
    {
        "header": "Appendix HTraining Loss Curve",
        "images": []
    }
]