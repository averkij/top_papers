[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x1.png",
                "caption": "Figure 1:Samples generated byPhysRVG.Our model produces videos with physically plausible rigid body dynamics.Rows1–4display four fundamental types of motion addressed in our work,row5validates the model’s generalization in out-of-distribution scenarios.",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x2.png",
                "caption": "Figure 2:The core idea ofPhysRVG.DiT-based video generative models reconstruct noisy videos in latent space using Flow Matching loss, which only captures data distributions (✓) but discards essential spatio-temporal physical cues during conditional alignment and feature extraction (✗), thereby hindering stable learning of physical knowledge (✗). While reinforcement learning with subjective ratings can train on physics-rich video data using RL-based feedback (✓), its evaluation remains perceptually biased and fails to provide stable physical supervision (✗). In contrast, ourPhysRVGleverages theMDcycleto fully utilize data for visual refinement (✓) and enforces physical knowledge injection through the Physics-Grounded Metric (✓), enabling stable retention and active discovery of physical principles for truly physics-aware learning and generation (✓).",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x3.png",
                "caption": "Figure 3:The framework ofPhysRVG.Given a text prompt and context frames, the model generates future video frames. For both the groundtruth and sampled frames, we derive motion masksMMby prompting SAM2[41]with object coordinatesp1p_{1}from the first frame, which are manually annotated during data preprocessing. We then compute object trajectoriesPPand perform collision detection. The trajectory offsetOObetween the sampled and groundtruth trajectories is calculated and reweighted by the collision signalwtw_{t}to yield a weighted trajectory offsetOcO_{c}, which serves as the per-sample score. All transformer blocks are trained with full parameters.",
                "position": 149
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x4.png",
                "caption": "Figure 4:(a)RL Training lossfor samples of varying quality. (b)Number of samplesassigned to the Mimicry and Discovery branches throughoutMDcycle.",
                "position": 311
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x5.png",
                "caption": "Figure 5:Qualitative comparisonswith existing methods. Each sample in the figure corresponds to the final frame of a generated video. We include the original videos for all cases in theSupplementary Materials.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2601.11087/x6.png",
                "caption": "Figure 6:Ablation Study of the Collision Detection.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2601.11087/x7.png",
                "caption": "Figure 7:(a)Reward Curveof RL andMDcycle. (b)(c)(d)Metric Distributionunder different training strategies.",
                "position": 694
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMimicry-Discovery Cycle Details",
        "images": []
    },
    {
        "header": "Appendix BPhysRVGArchitecture & Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x8.png",
                "caption": "Figure 8:Reward curve of RL trained with full-parameter.",
                "position": 1768
            }
        ]
    },
    {
        "header": "Appendix CBenchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x9.png",
                "caption": "Figure 9:Videos in thePhysRVGBench.",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2601.11087/x10.png",
                "caption": "Figure 10:Evaluation Metric.",
                "position": 1902
            }
        ]
    },
    {
        "header": "Appendix DDetection Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x11.png",
                "caption": "Figure 11:Reward curve of different Threshold settings.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2601.11087/x12.png",
                "caption": "Figure 12:More results generated byPhysRVG",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2601.11087/x13.png",
                "caption": "Figure 13:More results generated byPhysRVG",
                "position": 2008
            }
        ]
    },
    {
        "header": "Appendix EMore Generated Results.",
        "images": []
    },
    {
        "header": "Appendix FMore analysis on Threshold.",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11087/x14.png",
                "caption": "Figure 14:Failure cases generated byPhysRVG",
                "position": 2045
            }
        ]
    },
    {
        "header": "Appendix GDiscussion",
        "images": []
    }
]