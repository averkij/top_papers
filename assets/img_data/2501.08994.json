[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08994/x1.png",
                "caption": "Figure 1:The examples generated by RepVideo.RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.",
                "position": 102
            }
        ]
    },
    {
        "header": "IIRelated work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08994/x2.png",
                "caption": "Figure 2:The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.",
                "position": 170
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08994/x3.png",
                "caption": "Figure 3:The visualization of the attention distribution of each frame’s token across the entire token sequence.The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x4.png",
                "caption": "Figure 4:The visualization of attention maps across transformer layers.Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model’s ability to establish coherent spatial semantics within individual frames.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x5.png",
                "caption": "Figure 5:The average similarity between adjacent frame features across diffusion layers and denoising steps.The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x6.png",
                "caption": "Figure 6:The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module.The aggregated features demonstrate more comprehensive semantic information and clearer structural details.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x7.png",
                "caption": "Figure 7:The comparison of adjacent frame similarity between original and aggregated features.The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x8.png",
                "caption": "Figure 8:The architecture of the enhanced cross-layer\nrepresentation framework.",
                "position": 320
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08994/x9.png",
                "caption": "Figure 9:The qualitative comparison between our method and the baseline CogVideoX-2B[31].The first row shows results from the baseline CogVideoX-2B[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x10.png",
                "caption": "Figure 10:The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo.The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B[31].",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x11.png",
                "caption": "Figure 11:The comparison of attention maps between CogVideoX-2B and RepVideo.The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B[31].",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2501.08994/x12.png",
                "caption": "Figure 12:The cosine similarity between consecutive frames across layers.",
                "position": 583
            }
        ]
    },
    {
        "header": "VDiscussion",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08994/extracted/6134317/bib/scy.jpg",
                "caption": "",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2501.08994/extracted/6134317/bib/fanweichen.png",
                "caption": "",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2501.08994/extracted/6134317/bib/zhengyao.jpg",
                "caption": "",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2501.08994/extracted/6134317/bib/ziqi_huang.jpg",
                "caption": "",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2501.08994/extracted/6134317/bib/ziweiliu.png",
                "caption": "",
                "position": 1117
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]