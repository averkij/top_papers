[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14202/x1.png",
                "caption": "Figure 1:Baseline improvement on ProcGen.We compare mean test rewards for our agent (Hyper++), a Euclidean agent, and an unregularized hyperbolic agent (Hyper) withCetin et al. (2023)’s agent (Hyper+S-RYM).",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x2.png",
                "caption": "(a)",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x2.png",
                "caption": "(a)",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x3.png",
                "caption": "(b)",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x4.png",
                "caption": "(c)",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x5.png",
                "caption": "(d)",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x6.png",
                "caption": "(e)",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x7.png",
                "caption": "(f)",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x8.png",
                "caption": "(g)",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x9.png",
                "caption": "(h)",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x10.png",
                "caption": "Figure 5:Normalized test rewards on ProcGen.Hyper++outperforms baselines for all aggregation methods without increasing variance (as measured by the bootstrap confidence interval). We report median, interquartile mean (IQM), mean, and optimality gap, which is1−IQM1-\\text{IQM}.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x11.png",
                "caption": "Figure 6:Learning curves for PPO on ProcGen.We report the mean test rewards over six seeds on the same environments asCetin et al. (2023).",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x12.png",
                "caption": "Figure 7:Ablation studies on ProcGen withHyperbolic geometry.We report the test interquartile mean (IQM) across six seeds with bootstrap confidence intervals.−-indicates that a component is removed fromHyper++,++indicates a component replacing its analog.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x13.png",
                "caption": "Figure 8:Ablation studies on ProcGen withEuclidean geometry.We report the test interquartile mean (IQM) across six seeds with bootstrap confidence intervals.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x14.png",
                "caption": "Figure 9:Human-normalized performance for DDQN on Atari-5.All agents are trained for10​M10Msteps and five seeds.Hyper++strongly improves over the baselines.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2512.14202/figures/procgen_envs.png",
                "caption": "Figure 10:Visualization of all ProcGen environments.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x15.png",
                "caption": "Figure 12:ProcGen Train Learning curves.",
                "position": 2366
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x16.png",
                "caption": "Figure 13:ProcGen Learning curves.",
                "position": 2498
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x17.png",
                "caption": "Figure 14:Additional training metrics of hyperbolic deep RL agents.Agents w/o RMSNorm(Zhang & Sennrich,2019)(−-RMSNorm) suffer from growing embedding norms and vanishing gradients in the encoder. Using MSE instead of HL-Gauss(Imani & White,2018)(++MSE) leads to larger initial encoder gradients due to gradients scaling proportional to the loss for MSE. Not using learned feature scaling (−-Scaling) has the largest embedding norms and gradients, which are quickly compensated by RMSNorm’s gradient variance normalization(Zhang & Sennrich,2019).",
                "position": 2630
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x18.png",
                "caption": "Figure 15:Critic loss and variance.We plot the critic loss and variance for our method when using MSE and the categorical loss, averaged over all runs and environments. The categorical HL-Gauss loss(Imani & White,2018)has higher loss values and variance than MSE.",
                "position": 2640
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x19.png",
                "caption": "Figure 16:Atari-5 learning curves.Hyper++outperforms the baselines on all environments with particularly strong gains inNameThisGameandQ⁢bert. Results are averaged over five seeds.",
                "position": 2682
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x20.png",
                "caption": "Figure 17:Polyak averaging (NameThisGame).Polyak averaging refers to exponential moving average updates for the target network instead of hard replacement updates. Algorithm performance is not meaningfully affected by the form of the target network update. Runs are averaged over five seeds, with one standard deviation as error.",
                "position": 2690
            },
            {
                "img": "https://arxiv.org/html/2512.14202/x21.png",
                "caption": "Figure 18:Off-batch PPO stability metrics.We track the update KL divergence and PPO clipping fraction for the batch that has currently been updated (left column) and for a batch of randomly sampled on-policy states (right column). The figures show a high level of similarity for the evolution of both metrics. For off-batch data, the update KL divergence has a noticeably higher variance.",
                "position": 2697
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]