[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19163/figure/JavisDiT++_logo.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x1.png",
                "caption": "Figure 1:Realistic and diversified joint audio-video generation examples by our JavisDiT++ model.",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19163/x2.png",
                "caption": "Figure 2:Comparison with recent JAVG models.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19163/x3.png",
                "caption": "Figure 3:Architecture of JavisDiT++. We use shared attention layers to encourage audio-visual mutual information modeling, with modality-specific FFN layers to enhance intra-modal aggregation. The Temporal-Aligned RoPE strategy is applied to ensure audio-video synchrony.\nThe audio/video embedder layer and prediction head that bridge DiT and VAEs are hidden for simplicity.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x4.png",
                "caption": "Figure 4:Illustration of temporal-aligned rotary position encoding for video and audio tokens.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x5.png",
                "caption": "Figure 5:Illustration of preference data collection and training pipeline of audio-video DPO.",
                "position": 324
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19163/x6.png",
                "caption": "Table 1:Main results on JavisBench for generating 240p4s sounding videos. The best results are marked withbold, and the second ones are marked withunderline.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x6.png",
                "caption": "Figure 6:Generations from recent JAVG models. Best viewed in zoom or supplementary materials.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x7.png",
                "caption": "Figure 7:Comprehensive ablation studies on LoRA configurations.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x8.png",
                "caption": "Figure 8:Subjective comparison on generation with baseline methods.",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x9.png",
                "caption": "Figure 9:Subjective evaluation on the efficacy of the AV-DPO strategy.",
                "position": 855
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADiscussion",
        "images": []
    },
    {
        "header": "Appendix BDetailed Implementations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19163/x10.png",
                "caption": "Figure A1:Illustration of trainable parameters at different stages.",
                "position": 2035
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x11.png",
                "caption": "Figure A2:(Left): Diversified audio-text sources.(Right): Data filtering process from TAVGBench.",
                "position": 2127
            }
        ]
    },
    {
        "header": "Appendix CDetailed Comparison with Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19163/x12.png",
                "caption": "Figure A3:Architectural comparison with Uniform, JavisDiT, and UniVerse-1.",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x13.png",
                "caption": "Figure A4:Comparison of our audio-video frame interleaving with Qwen2.5-Omni’s strategy.",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x14.png",
                "caption": "Figure A5:Illustration of different audio positional encoding strategies.",
                "position": 2283
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x15.png",
                "caption": "Figure A6:Implicit accuracy onβ\\betaselections.",
                "position": 2599
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x16.png",
                "caption": "Figure A7:Implicit accuracy on learning rates.",
                "position": 2602
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x17.png",
                "caption": "Figure A8:More examples for high-quality audio-video generation results.",
                "position": 2621
            },
            {
                "img": "https://arxiv.org/html/2602.19163/x18.png",
                "caption": "Figure A9:More examples for high-quality audio-video generation results.",
                "position": 2624
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    }
]