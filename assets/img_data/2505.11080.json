[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/human_agreement_scores_horizontal_barchart.png",
                "caption": "Figure 1:Human agreement rates for BLEU (with varying numbers of references), two reward models, and other reference-based metrics (with a single Claude reference). BLEU becomes more competitive with reward models as more references are provided, and combining BLEU with a reward model outperforms either alone.",
                "position": 214
            }
        ]
    },
    {
        "header": "2How well do simple reference-based metrics capture human preferences?",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleu_example.png",
                "caption": "Figure 2:The highlighted matchingnùëõnitalic_n-grams in this example show that BLEU can capture correct instruction-following behavior as well as the factuality of the response.",
                "position": 307
            }
        ]
    },
    {
        "header": "3BLEUBERI: aligning language models to follow instructions with BLEU",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleuberry.png",
                "caption": "Table 1:Results on four instruction-following benchmarks. For each model, the ‚ÄúBase‚Äù row represents its pretrained checkpoint (for Llama-3.1-8B, this is our SFT-initialized model described in ¬ß3.2), while theInstructrow is the official post-trained checkpoint. Despite the limitations ofnùëõnitalic_n-gram matching, BLEUBERI¬†is competitive with both SFT and GRPO-RM across all models and benchmarks.",
                "position": 435
            }
        ]
    },
    {
        "header": "4Analysis and human evaluation of model outputs",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/factuality_figure_2.png",
                "caption": "Figure 3:Factuality results for trained Qwen2.5-7B models across three QA datasets evaluated usingVeriScore[52]. TheKùêæKitalic_Kvalues (in parentheses on the\nx-axis) used for each dataset follow the original paper.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/facuality_example.png",
                "caption": "Figure 4:An example instruction from FreshQA, where red highlights indicate factually incorrect claims. For this instruction, BLEUBERI¬†produces a more factually precise output than GRPO-RM.",
                "position": 665
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AChatbot Arena human preference analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/average_token_counts_comparison.png",
                "caption": "Figure 5:Token counts for difference reference model outputs, prompts, and the two model outputs to be scored.",
                "position": 2019
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleu_example_2.png",
                "caption": "Figure 6:Additional example for BLEU‚Äôsnùëõnitalic_n-gram attribution on cases where it agrees with human preferences.",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/metric_comparison_by_category_all.png",
                "caption": "Figure 7:Agreement rates of each individual metric‚ÄîLength, BLEU precision, Brevity Penalty (BP), BLEU, RM-8B, and RM-27B‚Äìwith human judgment across domains in our 18K filtered Chatbot Arena dataset.",
                "position": 2118
            }
        ]
    },
    {
        "header": "Appendix BTraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleuberry.png",
                "caption": "Table 7:Results on four instruction-following benchmarks, extended to include SFT models trained for the same number of steps (rather than epoch) as the GRPO models.",
                "position": 2327
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/tulu3_50k.png",
                "caption": "Figure 8:Distribution of task types in the Tulu3 50K data pool, labeled by Llama-3.1-8B-Instruct.",
                "position": 2840
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleuberry.png",
                "caption": "Table 13:Qualitative statistics for model outputs across all four benchmarks.",
                "position": 2851
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/example_base_rm_bleu.png",
                "caption": "Figure 9:Outputs from Qwen base, GRPO-RM, and BLEUBERI. While GRPO-RM and BLEUBERIgenerate responses that follow the instruction well, Qwen produces an irrelevant response",
                "position": 2948
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/affirmation_stacked_bar_plot_cropped.png",
                "caption": "Figure 10:Frequency of starting phrases for each model and training method. For Qwen, BLEUBERI¬†models begin with‚Äò‚ÄòCertainly!‚Äô‚Äôa lot, and GRPO-RM models begin with‚Äò‚ÄòSure!‚Äô‚Äôvery often. Numbers are averaged over responses for the four benchmarks.",
                "position": 3041
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleuberry.png",
                "caption": "Table 15:Factual consistency results for trained Qwen2.5-7B models across three QA datasets evaluated usingVeriScore[52]. TheKùêæKitalic_Kvalues used for each dataset follow the original paper.",
                "position": 3096
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/bleuberry.png",
                "caption": "Table 16:Example of extracted claims using VeriScore",
                "position": 3193
            },
            {
                "img": "https://arxiv.org/html/2505.11080/extracted/6445322/assets/human_preferences.png",
                "caption": "Figure 11:Human preference results. For each annotator, the bar on the right represents the soft preference rate for BLEUBERI. Based on these evaluations, BLEUBERI¬†outputs are often on par with GRPO-RM outputs.",
                "position": 3749
            }
        ]
    },
    {
        "header": "Appendix CQualitative analysis",
        "images": []
    }
]