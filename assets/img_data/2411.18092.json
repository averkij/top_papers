[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18092/x1.png",
                "caption": "Figure 1:Training Noise Token Pruning (TNT).Our proposed method computes a relevance termŒ±isubscriptùõºùëñ\\alpha_{i}italic_Œ± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTfor each token. In training (diagrammed at top), these terms dictate an amount of noise added to the token, while at test time they indicate pruning order.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18092/x2.png",
                "caption": "Figure 2:Noise Allocator block architecture:the block diagrammed above is injected into pre-trained models as a pruning layer. It takes the output of the previous Transformer block as input, then computes the noise signal termsŒ±ùõº\\alphaitalic_Œ±using a linear layer followed by aSoftmaxfunction. During training it samples Gaussian noise conditioned onŒ±ùõº\\alphaitalic_Œ±for each token, then adds the noise to the token embeddings. At test time, tokens are instead dropped. This pruning method can be trained with all parameters outside the noise allocator are frozen.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18092/x3.png",
                "caption": "Figure 3:Visualization of Token Pruning maps on ImageNet-1K: atleftare the original images, and at each columnprogressing rightare single layer prunings and their associated kept/dropped tokens, for layers 1-5 of the DeiT-B-Distil. model.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2411.18092/x4.png",
                "caption": "Figure 4:Single Layer Pruning results:We plot the Top-1 Accuracy in the ImageNet-1k validation set for each of the pruning methods as a function of computational efficiency, in thetop rowmeasured by GFLOPs and in thebottom rowmeasured by throughput, forsingle layer pruning. The base model is DeiT-B-Distil in thefirst column, DeiT-S-Distil. in thesecond column, and ViT/16 in thethird column.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2411.18092/x5.png",
                "caption": "Figure 5:Multi-layer Pruning results:We plot the Top-1 Accuracy in the ImageNet-1k validation set for each of the pruning methods as a function of computational efficiency, in thetop rowmeasured by GFLOPs and in thebottom rowmeasured by throughput, formulti-layer pruning. The base model is DeiT-B-Distil in thefirst column, DeiT-S-Distil. in thesecond column, and ViT/16 in thethird column.",
                "position": 454
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AImplementation Code",
        "images": []
    },
    {
        "header": "BAdditional Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18092/x6.png",
                "caption": "Figure 7:Experimental Results for DeiT-Tiny-Distil.:We plot the Top-1 Accuracy in the ImageNet-1k validation set for each of the pruning methods as a function of computational efficiency, in thetop rowmeasured by GFLOPs and in thebottom rowmeasured by throughput, formulti-layer pruning. For weaker encoders and/or smaller transformer blocks such as DeiT-Tiny, we rather use MLP, which means less parallelism. The throughput of models in multi-layer pruning reaches approximately 5000 images per second, potentially limited by hardware bottlenecks.",
                "position": 3416
            },
            {
                "img": "https://arxiv.org/html/2411.18092/x7.png",
                "caption": "Figure 8:More Visualization of Token Pruning maps on ImageNet-1K: atleftare the original images, and at each columnprogressing rightare single layer prunings and their associated kept/dropped tokens, for layers 1-5 of the DeiT-S-Distil. model.",
                "position": 5137
            }
        ]
    },
    {
        "header": "CMore examples for visualization",
        "images": []
    }
]