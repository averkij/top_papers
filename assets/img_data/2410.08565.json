[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08565/x1.png",
                "caption": "Figure 1:Evaluation across image, video, and audio modalities.(Left)Ocean-omni¬†covers more modalities than Qwen2 VL[79]and outperforms the current leading omni-modal model, VITA[24].(Right)Average scores across benchmarks for all modalities. All the scores are normalized byxnorm=(x‚àíxmin+10)/(xmax‚àíxmin+10)subscriptùë•normùë•subscriptùë•min10subscriptùë•maxsubscriptùë•min10x_{\\text{norm}}=(x-x_{\\text{min}}+10)/(x_{\\text{max}}-x_{\\text{min}}+10)italic_x start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT = ( italic_x - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ) / ( italic_x start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ).",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2410.08565/x2.png",
                "caption": "",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08565/extracted/5979150/figures/architecture.png",
                "caption": "Figure 2:Architecture of Ocean-omni¬†.Ocean-omni¬†is designed to process both pure text/audio inputs and combinations of video/image with text/audio. In terms of interactivity, the model initially predicts the start and end of audio inputs. During this period, incoming images and videos are encoded into features and fed into the MLLM in a streaming fashion to calculate attention.\nThe audio features are then input into the MLLM for inference following the end of the audio input, facilitating streaming input of audio and video.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08565/extracted/5979150/figures/data_example.png",
                "caption": "Figure 3:Data illustration of Ocean-omni¬†.We build an extensive cross-modal dataset, including text, image-text, video-text, audio-text, and their interactions. Our collection also features integrated image-audio-text and video-audio-text data.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2410.08565/extracted/5979150/figures/pipeline.png",
                "caption": "Figure 4:Training Pipeline of Ocean-omni. During the pretraining phase, we initially train a vision-language model using extensive image-text data, followed by training an audio-language model with ASR data. Subsequently, we integrate high-quality images, audio, and video data for comprehensive multimodal alignment. In the fine-tuning phase, we synthesize a subset of cross-modal interaction data to blend with existing high-quality datasets. From this enriched dataset, we select a subset of data that the model is already capable of handling and proceed with multimodal multitask fine-tuning. This process aims to enhance the model‚Äôs adherence to omni-modal instructions.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2410.08565/x3.png",
                "caption": "Figure 5:Illustration of Conv-GMLP.Conv-GMLP down-sampling is applied to the audio representation. With a down-sampling rate of 4, the output sequence length is reduced to a quarter of the input, while the number of feature channels increases fourfold.",
                "position": 310
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08565/extracted/5979150/figures/audio_ablation.png",
                "caption": "Figure 6:Ablation study on down-sampling rate.Average WER across multiple test sets (Fleurs zh/en, WenetSpeech net/meeting, and KeSpeech) using various down-sampling rates of Conv-GMLP.",
                "position": 1748
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]