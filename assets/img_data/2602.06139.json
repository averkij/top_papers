[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06139/x1.png",
                "caption": "Figure 1:Overview of EgoAVU.We introduce EgoAVU, a scalable and automated data engine to enable egocentric audio–visual understanding. EgoAVU enriches existing egocentric narrations by integrating human actions with environmental context, explicitly linking visible objects and the sounds produced during interactions or surroundings. Leveraging this pipeline, we constructEgoAVU-Instruct (3M QAs)andEgoAVU-Bench (3K verified QAs), enabling systematic training and evaluation of MLLMs. Models finetuned with EgoAVU-Instructexhibit high audio-visual grounding in egocentric settings.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06139/x2.png",
                "caption": "Figure 2:EgoAVU pipeline.EgoAVU consists of four key components. (1) For each egocentric video clip, EgoAVU enhances the raw narration with detailed multisensory context using open-source MLLMs(Bai et al.,2025;Xu et al.,2025a). (2) These enriched narrations are then used to select clips that exhibit diverse audio–visual dynamics. (3) Next, EgoAVU constructs a Multimodal Context Graph (MCG), automatically generated via open-source LLMs(AI@Meta,2024), to capture complex cross-modal relations. The MCG is parsed alongside the enhanced narrations to produce coherent audio–visual narrations. (4) The generated audio-visual narrations are leveraged to generate high-quality audio–visual QA pairs, forming both the instruction-tuning dataset EgoAVU-Instructand the evaluation benchmark EgoAVU-Bench.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x3.png",
                "caption": "Table 1:Question Prompts.Examples of open-ended and close-ended questions in EgoAVU-Instructand EgoAVU-Bench.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x3.png",
                "caption": "Figure 3:Video duration distribution.Our videos includes both short clips within 1 min and long videos of 6 min.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x3.png",
                "caption": "Figure 3:Video duration distribution.Our videos includes both short clips within 1 min and long videos of 6 min.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x4.png",
                "caption": "Figure 4:Distribution of 20 most common visual scenarios in EgoAVU-Instructand EgoAVU-Bench.",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x4.png",
                "caption": "Figure 4:Distribution of 20 most common visual scenarios in EgoAVU-Instructand EgoAVU-Bench.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x5.png",
                "caption": "Figure 5:Distribution of proposed tasks across EgoAVU-Instructand EgoAVU-Bench.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x6.png",
                "caption": "Table 3:Main result on EgoAVU-Bench.We compare seven MLLMs with joint audio–visual understanding capabilities against our fine-tuned models across a diverse set of tasks in EgoAVU-Bench, including open-ended QAs: Source-Sound Association (SSA), Audio-Visual Dense Narration (AVDN), and Audio-Visual Segment Narration (AVSN), as well as closed-ended QAs: Temporal Reasoning (TR) and Audio-Visual Hallucination (AVH). For the open-ended tasks, we report LLM-as-Judge (S), METEOR (M), and ROUGE-L (R). For the closed-ended tasks, we report Accuracy (Acc.). Additionally for each task, we compute the relative performance gain (Δ\\Delta) between the best open-source model and our fine-tuned models.",
                "position": 540
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06139/x6.png",
                "caption": "Figure 6:Qualitative Analysis on EgoAVU-Bench.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x6.png",
                "caption": "Figure 6:Qualitative Analysis on EgoAVU-Bench.",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x7.png",
                "caption": "Figure 7:Qualitative comparison of various MLLMs on the Audio-Visual Dense Narration (AVDN) task.Our model fine-tuned on EgoAVU-Instructcaptures significantly more dense visual details than Qwen2.5 Omni and VideoLLaMA2, while also identifying auditory cues related to human actions and background sounds in the video.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x8.png",
                "caption": "Figure 8:Error Analysis on Sound-Source Association (SSA).",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x8.png",
                "caption": "Figure 8:Error Analysis on Sound-Source Association (SSA).",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2602.06139/x9.png",
                "caption": "Figure 9:Examples of video filtering based on MATTR scores.",
                "position": 1089
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Additional Details on EgoAVU",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06139/x10.png",
                "caption": "Figure 10:Qualitative comparison of audio-visual narrations generated with and without (w/o) Multi-modal Context Graph (MCG).Our MCG-based approach produces narrations with superior audio-visual coherence, accurately capturing action sequences and sound-source associations, while the direct method (w/o MCG) often misses critical sounds or action sequences.",
                "position": 1767
            }
        ]
    },
    {
        "header": "7Manual Effort for EgoAVU-BenchConstruction",
        "images": []
    },
    {
        "header": "8Additional Details on Narration Enhancement",
        "images": []
    },
    {
        "header": "9Additional Experiment Details",
        "images": []
    }
]