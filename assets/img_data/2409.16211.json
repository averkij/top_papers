[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16211/x1.png",
                "caption": "Figure 1:Generated images by the proposed MaskBit.We showcase samples from MaskBit trained on ImageNet at256√ó256256256256\\times 256256 √ó 256resolution.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Motivation",
        "images": []
    },
    {
        "header": "2Revisiting VQGAN",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16211/x2.png",
                "caption": "Figure 2:Detailed roadmap to build a modern VQGAN+.This overview summarizes the performance gains achieved by each proposed change to the architecture and training recipe. The reconstruction FID (rFID) is computed against the validation split of ImageNet at a resolution of 256.\nThe popular and open-source Taming-VQGAN[11]serves as the baseline and starting point.",
                "position": 179
            }
        ]
    },
    {
        "header": "3MaskBit: A New Embedding-free Image Generation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16211/x3.png",
                "caption": "Figure 3:Bit tokens exhibit structured semantic representations.We visualize a robustness test involving bit flipping. Specifically, we utilize our method to encode images into bit tokens, where each token is represented byKùêæKitalic_K-bits (K=12ùêæ12K=12italic_K = 12in this example).\nWe then flip theiùëñiitalic_i-th bit for all the bit tokens and reconstruct the images as usual.\nInterestingly, the reconstructed images from these bit-flipped tokens remain semantically consistent to the original image, exhibiting only minor visual modifications such as changes in texture, exposure, smoothness, color palette, or painterly quality.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2409.16211/x4.png",
                "caption": "Figure 4:High-level overview of the architecture and comparison.Our training framework comprises two stages for image generation. In Stage-I, an encoder-decoder network compresses images into a latent representation and decodes them back. Stage-II masks the tokens, feeds them into a transformer and predicts the masked tokens. Most prior art uses VQGAN-based methods (top) that learn independent embedding tables in both stages. In VQGAN-based methods, only indices of embedding tables are shared across stages, but not the embeddings. In MaskBit, however, neither Stage-I nor Stage-II utilizes embedding tables. The Stage-I predicts bit tokens by using binary quantization on the encoder output directly. The Stage-II partitions the shared bit tokens into groups (e.g., 2 groups), masks and feeds them into a transformer, and predicts the masked bit tokens.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Experimental Results for Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16211/x5.png",
                "caption": "Figure 5:Visualization of generated256√ó256256256256\\times 256256 √ó 256images.MaskBit demonstrates the ability to produce high-fidelity images across a diverse range of classes.",
                "position": 691
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Discussion of Bit Tokens",
        "images": []
    },
    {
        "header": "Appendix DMore Reconstruction Results of Flipping Bit Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16211/x6.png",
                "caption": "Figure 6:More examples demonstrating the structured semantic representation in bit tokens.Similar to the observation in the main paper, the reconstructed images from these bit-flipped tokens remain semantically similar to the original image, exhibiting only minor visual modifications such as changes in texture, exposure, smoothness, color palette, or painterly quality.",
                "position": 2120
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FBroader Impacts",
        "images": []
    }
]