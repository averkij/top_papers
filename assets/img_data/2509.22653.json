[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22653/x1.png",
                "caption": "Figure 1:Zero-shot language-guided UAV control.(a) The UAV continually replans to keep pace with a moving person. (b) The UAV chains multiple goals across the hall. (c) The UAV locates the person on the ground and navigates around obstacles. Coloured 3D boxes mark successive camera viewpoints, revealing the UAV’s full flight trajectory over the reconstructed point cloud. All waypoints are generated directly by the vision-language model, withnotask-specific training.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22653/x2.png",
                "caption": "Figure 2:Pipeline overview.A camera frame and user instructions enter a frozen vision-language model, which returns a structured JSON with a 2D waypoint and any obstacle boxes. An Action-to-Control layer converts this output into low-level velocity commands (yaw, throttle, pitch) that steer the UAV. The loop repeats until the task is completed.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2509.22653/x3.png",
                "caption": "Figure 3:Control-geometry details of our VLM-driven flight loop.A frozen vision-language model first predicts a 2D waypoint(u,v)(u,v)and a discrete depth cuedVLMd_{\\text{VLM}}.\n(a) A nonlinear scaling curve convertsdVLMd_{\\text{VLM}}into an adaptive step sizedadjd_{\\text{adj}}, letting the UAV take larger strides in open space and smaller ones near obstacles.\n(b) The pair(u,v,dadj)(u,v,d_{\\text{adj}})is unprojected through the pin-hole model to a 3D displacement vector(Sx,Sy,Sz)(S_{x},S_{y},S_{z})in the UAV’s body frame.\n(c) This vector is decomposed into control primitives: yawΔ​θ=tan−1​(Sx/Sy)\\Delta\\theta=\\text{tan}^{-1}(S_{x}/S_{y}), pitchΔ​Pitch=Sx2+Sy2\\Delta\\text{Pitch}=\\sqrt{{S_{x}}^{2}+{S_{y}}^{2}}, and throttleΔ​Throttle=Sz\\Delta\\text{Throttle}=S_{z}.\nThese quantities are sent as timed velocity commands by the execution layer.\nThe perception, planning, and control cycle repeats until the language instruction is fulfilled.",
                "position": 181
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/sim_obstacle.png",
                "caption": "Figure 4:Qualitative comparison of flight trajectories in the simulator.Trajectory of our method is colored ingreen, PIVOT[28]inblue, and TypeFly[15]inpurple. The absence of a colored path indicates the baseline failed to issue any fly command. Full videos are included in the supplementary materials.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/sim_targetid.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/sim_search.png",
                "caption": "",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_obstacle_typefly.png",
                "caption": "Figure 5:Qualitative comparison of flight trajectories in the real-world.Trajectory of our method compared to other baselines in the real-world testing. Take off trajectory is colored ingreenand task trajectory inmagenta. Please refer to the supplementary materials for full videos.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_obstacle_typefly.png",
                "caption": "Figure 5:Qualitative comparison of flight trajectories in the real-world.Trajectory of our method compared to other baselines in the real-world testing. Take off trajectory is colored ingreenand task trajectory inmagenta. Please refer to the supplementary materials for full videos.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_reasoning_typefly.png",
                "caption": "",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_follow_typefly.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_long_horizon_typefly.png",
                "caption": "",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_obstacle_pivot.png",
                "caption": "",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_reasoning_pivot.png",
                "caption": "",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_follow_pivot.png",
                "caption": "",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_long_horizon_pivot.png",
                "caption": "",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_obstacle_ours.png",
                "caption": "",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_reasoning_ours.png",
                "caption": "",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_follow_ours.png",
                "caption": "",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/qualitative/realworld_long_horizon_ours.png",
                "caption": "",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2509.22653/x4.png",
                "caption": "Figure 6:Completion time by task.Our approach achieves faster completion times across most tasks, particularly excelling in complex scenarios. Bars capped at∞\\inftyindicate baseline failures.",
                "position": 621
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BMethod Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22653/figures/adaptive_exp/long_horizon_r2_ours.jpg",
                "caption": "Figure 7:Visual examples of the real-world scenarios for the tasks (from left to right): Long Horizon (“Fly to the cones and the next.”), Reasoning I (“I’m thirsty. Find something that can help me.”), and Reasoning II (“It’s raining. Head to the comfiest chair that will keep you dry.”). These images depict the types of environments and objectives the UAV encountered during the ablation study evaluating the adaptive step-size controller.",
                "position": 2473
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/adaptive_exp/long_horizon_r2_ours.jpg",
                "caption": "",
                "position": 2476
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/adaptive_exp/reasoning_thirsty.jpg",
                "caption": "",
                "position": 2487
            },
            {
                "img": "https://arxiv.org/html/2509.22653/figures/adaptive_exp/reasoning_raining.jpg",
                "caption": "",
                "position": 2498
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]