[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19301/figs/pipeline.png",
                "caption": "Figure 1:Off-policy residual fine-tuning (ResFiT): A two-phase approach using online RL to improve BC policies. First, we train a base policy using BC on a dataset of demonstrations and then freeze the base. Then, we learn a residual policy with RL to correct the base actions. The RL phase utilizes our off-policy recipe, leveraging both demonstrations and interactions, and enables more stable and safe exploration in the real world, as we can directly control the magnitude of the residuals. This residual approach is agnostic to the base policy parameterization and can be applied to large action-chunked BC policies to provide closed-loop corrections.",
                "position": 87
            }
        ]
    },
    {
        "header": "IIRELATED WORK",
        "images": []
    },
    {
        "header": "IIIMETHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19301/x1.png",
                "caption": "Figure 2:Our simulation tasks from Robomimic and DexMimicGen, spanning single-arm manipulation as well as bimanual coordination tasks.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2509.19301/x1.png",
                "caption": "Figure 2:Our simulation tasks from Robomimic and DexMimicGen, spanning single-arm manipulation as well as bimanual coordination tasks.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2509.19301/x2.png",
                "caption": "Figure 3:PPO vs. off-policy RL in ResFiT.",
                "position": 256
            }
        ]
    },
    {
        "header": "IVEXPERIMENTAL SETUP",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19301/x3.png",
                "caption": "Figure 4:Success rates of different approaches on our simulation tasks, showing ResFiT converging to high-performing policies for all tasks. Note that all approaches here start with the same number of demos.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2509.19301/x4.png",
                "caption": "Figure 5:Impact of UTD ratio (left) andn-step(right) on theBoxCleanupandCoffeetasks.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2509.19301/x5.png",
                "caption": "",
                "position": 356
            }
        ]
    },
    {
        "header": "VRESULTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19301/x6.png",
                "caption": "Figure 6:Real-world results of applying ResFiT, where our residual RL approach shows a significant boost in performance over the base model, on a 29-DoF bimanual wheeled humanoid robot with two 5-fingered hands.",
                "position": 399
            }
        ]
    },
    {
        "header": "VIDISCUSSION",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]