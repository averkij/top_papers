[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22642/x1.png",
                "caption": "Figure 1:Comparison between natural language reasoning and formal logic verification-guided reasoning. Formal verification detects logical errors in natural language reasoning and provides corrected reasoning paths. “NL” means Natural Language.",
                "position": 145
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22642/x2.png",
                "caption": "Figure 2:Number of correct answers using natural language reasoning versus formal logic verification. We randomly sampled 500 instances from each domain for this comparison.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2601.22642/x3.png",
                "caption": "Figure 3:Overview of the formal logic verification-guided training framework. The framework operates in two stages: (1)SFT:A teacher model synthesizes formal logic verification traces, which are validated by a verifier. A subset of verified samples is used to fine-tune the model, while challenging samples are reserved for RL training. (2)RL:The policy model generates natural language reasoning followed by formal reasoning. A formal interpreter verifies the formal reasoning and provides feedback, enabling iterative refinement until the model produces a final answer or reaches the maximum number of interpreter calls. Rewards computed from verification outcomes are used to calculate advantages and update the policy model via reinforcement learning.",
                "position": 294
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22642/x4.png",
                "caption": "Figure 4:Python packages type distribution invoked by SimpleTIR (blue) vs. FLV-RL (red) across three domains.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2601.22642/x5.png",
                "caption": "Figure 5:Token length distribution comparison across General-Reasoner, SimpleTIR, and FLV-RL. The box plots illustrate the median token usage (center line) and interquartile ranges.",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2601.22642/x6.png",
                "caption": "Figure 6:Enforced vs. Flexible Verification Paradigms. (a) Enforced verification imposes rigid checkpoints throughout the reasoning process, while flexible verification enables adaptive utilization of logic verification. (b) Performance gains after switching to flexible reasoning across three representative benchmarks.",
                "position": 879
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Appendix AReward Calculation Pseudocode",
        "images": []
    },
    {
        "header": "Appendix BDataset Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22642/x7.png",
                "caption": "Figure 7:Distribution of data categories in training Sets. Left: the categorical breakdown of the SFT dataset, totally 14,117 sample. Right: the categorical breakdown of the RL dataset. Legends list the exact number of samples for each category, totally 3,525 sample.",
                "position": 1858
            }
        ]
    },
    {
        "header": "Appendix CHyperparameter Specification",
        "images": []
    },
    {
        "header": "Appendix DTraining Dynamics and Behavior Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22642/x8.png",
                "caption": "(a)Reward",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2601.22642/x8.png",
                "caption": "(a)Reward",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2601.22642/x9.png",
                "caption": "(b)Response Length",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2601.22642/x10.png",
                "caption": "(c)Number of Logic Verification",
                "position": 1915
            }
        ]
    },
    {
        "header": "Appendix EAnalysis of Verification Overhead in Mathematical Reasoning",
        "images": []
    },
    {
        "header": "Appendix FAnalysis of Package Usage Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22642/x11.png",
                "caption": "Figure 9:Comparison of package usage distribution: SimpleTIR vs. FLV-RL",
                "position": 2207
            }
        ]
    },
    {
        "header": "Appendix GGPQA Problems",
        "images": []
    },
    {
        "header": "Appendix HCase Study: Formal Verification in Economic Reasoning",
        "images": []
    },
    {
        "header": "Appendix IPrompts",
        "images": []
    }
]