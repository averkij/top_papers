[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08405/x1.png",
                "caption": "Figure 1:Overview of the proposed method. The source audio is first encoded into a latent representation. Given the current audio segments, a flow-matching transformer estimates the generating vector field from noisy audio latents. This vector field is then used to solve the corresponding ODE, producing the future audio latents. The resulting sequence of future audio latents is decoded into audio spectrograms. Finally, a robot policy is trained using both the current and predicted future audio spectrograms along with image observations.",
                "position": 58
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08405/x2.png",
                "caption": "Figure 2:Experimental results. We respectively show the ground truth and the world model generation of water filling spectrogram, music spectrogram and MIDI data. The water filling spectrogram is predicted in a closed-loop manner during robot evaluation. Music pieces are generated autoregressively based on previous pieces.",
                "position": 146
            }
        ]
    },
    {
        "header": "4Conclusions",
        "images": []
    }
]