[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13281/x1.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VideoAutoArena",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13281/x2.png",
                "caption": "Figure 2:Video statistics by category and duration.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x3.png",
                "caption": "Figure 3:Examples of synthesized personas with three levels of relevance and corresponding synthesized questions. We also compare the style of our questions with those in popular long-video benchmarks, including LongVideoBench and VideoMME.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x4.png",
                "caption": "(a)Visualization of persona distribution.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x4.png",
                "caption": "(a)Visualization of persona distribution.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x5.png",
                "caption": "(b)Humans preference ranking.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2411.13281/extracted/6012238/emoji/OpenAI.png",
                "caption": "Table 2:OurVideoAutoArenaLeaderboard. We show the overall ELO ratings and win rates within four different video lengths.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2411.13281/extracted/6012238/emoji/google.png",
                "caption": "",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2411.13281/extracted/6012238/emoji/rhymes.jpg",
                "caption": "",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2411.13281/extracted/6012238/emoji/Qwen.png",
                "caption": "",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2411.13281/extracted/6012238/emoji/llava-next.png",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x6.png",
                "caption": "Figure 5:Our fault-driven evolution strategy generates increasingly challenging questions for video analysis.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x7.png",
                "caption": "Figure 6:Evaluate the accuracy of various judging methods using human annotations as the gold standard. In the Vote (Top N) method, the top N models are used to cast votes.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x8.png",
                "caption": "Figure 7:ELO ratings for models competing on questions before and after applying fault-aware evolution.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x9.png",
                "caption": "Figure 8:We evaluate the performance of various models based on four different judging standards.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x10.png",
                "caption": "Figure 9:Example of a battle between Aria and LLaVa-Video-72B.Redhighlights key content, whilegreenhighlights important details mentioned only by Aria.",
                "position": 554
            }
        ]
    },
    {
        "header": "4VideoAutoBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13281/extracted/6012238/emoji/OpenAI.png",
                "caption": "Table 3:LMMs compete against human selected or rejected answers in ourVideoAutoBench.",
                "position": 568
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13281/x11.png",
                "caption": "Figure 10:The prompt for video content-constrained persona generation.",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x12.png",
                "caption": "Figure 11:The prompt for persona-constrained video question asking.",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x13.png",
                "caption": "Figure 12:The prompt for our fault-driven evolution generates new questions based on the responses from the two models.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x14.png",
                "caption": "Figure 13:The prompt for our automatic judging.",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x15.png",
                "caption": "Figure 14:The prompt for question complexity evaluation.",
                "position": 1734
            }
        ]
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CExamples",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13281/x16.png",
                "caption": "Figure 15:Examples of our user simulation include five videos from diverse domains: Movies, Computer Science, Life Vlogs, Art, and News Programs. To save space, we only showcase 4 frames of each video.",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x17.png",
                "caption": "Figure 16:Example of the battle between Aria and GPT-4o.",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x18.png",
                "caption": "Figure 17:Example of the battle between GPT-4o-mini and LLaVa-Video-72B.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x19.png",
                "caption": "Figure 18:Example of the battle between Qwen2-VL-72B and LLaVa-Video-7B.",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x20.png",
                "caption": "Figure 19:Example of the battle between Aria and Qwen2-VL-72B.",
                "position": 1871
            }
        ]
    },
    {
        "header": "Appendix DHuman Annotations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13281/x21.png",
                "caption": "Figure 20:The guideline for the question ranking annotation.",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2411.13281/x22.png",
                "caption": "Figure 21:The guideline for the response judging annotation.",
                "position": 1892
            }
        ]
    },
    {
        "header": "Appendix ELimitation",
        "images": []
    }
]