[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03694/x1.png",
                "caption": "Figure 1:LongVieis a controllable ultra-long video generation framework guided by both dense and sparse control signals, with a degradation-aware training strategy to balance the contribution of the modalities. It applies global normalization to the control signals and employs unified initialization noise to autoregressively generate videos lasting up to one minute.",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03694/x2.png",
                "caption": "Figure 2:Temporal Inconsistency and Quality Degradation.These are the primary limitations encountered when applying current controllable models to long video generation.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03694/x3.png",
                "caption": "Figure 3:Analysis of temporal inconsistency.Temporal inconsistency arises from clip-wise normalization and random noise initialization, while global normalization and unified noise alleviate these issues.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x4.png",
                "caption": "Figure 4:Visual degradation caused by single-modality control.Depth-only and point-only settings show noticeable degradation, while D&P, combining both modalities, alleviates the issue.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x5.png",
                "caption": "Figure 5:Framework of LongVie.We adopt dense and sparse control signals as scene guidance to generate controllable long videos in an autoregressive manner. We also apply global normalization and unified initialization noise to improve temporal consistency during the generation process.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x6.png",
                "caption": "Figure 6:Variants of Multi-Modal Control Integration.Compared to the standard ControlNet design (a), we present two variant structures (b) and (c) that integrate dense and sparse control signals.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x7.png",
                "caption": "Figure 7:Video Editing.LongVie enables high-quality and temporally consistent video editing.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x8.png",
                "caption": "Figure 8:Motion & Scene Transfer.LongVie effectively transfers motion and scene across frames.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x9.png",
                "caption": "Figure 9:Mesh-to-Video.LongVie generates realistic long videos from animated 3D meshes.",
                "position": 278
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AOverview of Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix BLongVGenBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03694/x10.png",
                "caption": "Figure 10:Examples from LongVGenBench.We show several videos from both real-world and synthetic scenarios in LongVGenBench, covering a variety of indoor and outdoor environments to evaluate the controllable long video generation ability of our model.",
                "position": 715
            }
        ]
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03694/x11.png",
                "caption": "Figure 11:Caption Refinement via LLM.We revise the original captions by comparing the transferred and source images using a LLM. This process ensures that the updated captions accurately reflect the visual content of the transferred video.",
                "position": 798
            }
        ]
    },
    {
        "header": "Appendix DAdditional Ablation Studies",
        "images": []
    },
    {
        "header": "Appendix EMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03694/x12.png",
                "caption": "Figure 12:More results of Mesh-to-Video.We input a monster located in a village scene, animate the mesh, and convert it into a video, where our model supports rendering in various styles.",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x13.png",
                "caption": "Figure 13:More results of Motion & Scene Transfer.A man riding a horse in various scenes.",
                "position": 1053
            },
            {
                "img": "https://arxiv.org/html/2508.03694/x14.png",
                "caption": "Figure 14:More results of Motion & Scene Transfer.We transfer a car driving through a mountain valley across various seasons.",
                "position": 1057
            }
        ]
    },
    {
        "header": "Appendix FSocial Impacts",
        "images": []
    },
    {
        "header": "Appendix GLimitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]