[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08905/x1.png",
                "caption": "Figure 1:Average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperaturet=0.5ùë°0.5t=0.5italic_t = 0.5. We choset=0.5ùë°0.5t=0.5italic_t = 0.5to followsimple-evals[24]. Error bars are2‚Å¢œÉ2ùúé2\\sigma2 italic_œÉof the estimate. On competition math,phi-4scores well above its weight-class even compared to non‚Äìopen-weight models.",
                "position": 472
            }
        ]
    },
    {
        "header": "2Approach to Data",
        "images": []
    },
    {
        "header": "3Pretraining details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08905/extracted/6060244/figures/mmlu_synth_epochs.png",
                "caption": "Figure 2:5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are trained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens. We see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch models perform better than those that have seen more unique web tokens.",
                "position": 761
            }
        ]
    },
    {
        "header": "4Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08905/x2.png",
                "caption": "Figure 6:The post-training process described in AppendixA.1decreases hallucinations. One measure is that the problems in SimpleQA‚Äîwhich the model very rarely gets correct‚Äîare increasingly not attempted during the course of post-training. We believe the final result is better behavior, even though thesimple-evalsscore for SimpleQA (the F1 score) actually gives our base model a higher score than our final model.",
                "position": 1657
            }
        ]
    },
    {
        "header": "5Benchmarking Considerations",
        "images": []
    },
    {
        "header": "6Performance on Key Benchmarks",
        "images": []
    },
    {
        "header": "7Safety",
        "images": []
    },
    {
        "header": "8Weaknesses",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APost-Training Dataset Details",
        "images": []
    },
    {
        "header": "Appendix BData Processing",
        "images": []
    },
    {
        "header": "Appendix CAMC Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix DSynthetic generation examples",
        "images": []
    }
]