[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05284/x1.png",
                "caption": "Figure 1:We augment video world models with memory. In this context, we consider the conventional approach of conditioning autoregressively generated frames with a few recent context frames as a short-term working memory. We explore two additional mechanisms modeling different types of long-term memory: spatial and episodic memory. The former is represented as a point map that is autoregressively generated along with the video frames and fused into the spatial memory by extracting only its static scene parts. To remember visual detail and identities for long time horizons, we also store a sparse set of historical reference frames as an episodic memory. Together, our memory mechanisms significantly improve the long-term consistency of emerging video world models.",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05284/x2.png",
                "caption": "Figure 2:Overview of our system.A latent video generation model, implemented by a diffusion transformer (DiT), is conditioned on three different memory mechanisms when autoregressively generating new frames. First, recent context frames model ashort-term working memory. Second, a point cloud representation (left) is autoregressively generated along with the video frames. Thislong-term spatial memorycontains the static parts of the world. Third, a set of historical reference frames (lower left) is stored as asparse, long-term episodic memory. Together, these memory mechanisms enable consistent long-term video generation.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2506.05284/x3.png",
                "caption": "Figure 3:Dataset construction pipeline.We use Mega-SaM[44]to extract camera poses and dynamic point maps from the full video clip. For the source part,dynamic regions are erasedvia TSDF-Fusion, and the point cloud is rendered along the target trajectory to to serve as static geometry guidance for the target part. Qwen[77]generates annotations for actions in future target frames.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05284/x4.png",
                "caption": "Figure 4:Qualitative evaluation.We compare our approach to relevant baselines in several conditions. Baselines cannot accurately generate significant camera pose changes while maintaining a consistent scene (top). When revisiting a previously seen camera pose, baselines fail to complete sparse point clouds or forget details, resulting in inconsistency (center). The accuracy of prompted actions is often low, and sometimes the character disappears during the generation for the baselines (bottom). Our approach successfully handles these challenging scenarios.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2506.05284/x5.png",
                "caption": "Figure 5:Ablation of different memory mechanisms.We evaluate several variants of our model:w/o short-term working memory: the full model without recent context frames;w/o long-term episodic memory: the full model without sparse historical keyframes;full modelincluding short-term working and long-term spatial and episodic memory. Unsurprisingly, the working memory is required for smooth and plausible motions of dynamic objects. The episodic memory is crucial in helping remember visual details from the past, including previously seen characters or objects.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2506.05284/x6.png",
                "caption": "Figure 6:Failure case.When the distance between consecutive camera poses is too large and the trajectory exhibits overly abrupt angles, the 4D reconstruction may fail, resulting in significant ghosting artifacts between frames. Consequently, TSDF-Fusion will filter out a large portion of point clouds that should belong to static regions, ultimately leading to an extremely sparse spatial memory and loss of critical information. For example, Spiderman swiftly swinging between skyscrapers illustrates how such a challenging camera trajectory can cause omissions in spatial memory storage, resulting in imprecise camera control and inconsistencies.",
                "position": 507
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05284/x7.png",
                "caption": "Figure S1:Autoregressive point cloud fusion.The system continuously updates spatial memory by integrating newly observed static maps. These maps are reconstructed online using CUT3R in a recurrent manner, while TSDF-Fusion filters out dynamic elements to maintain map consistency.",
                "position": 609
            }
        ]
    },
    {
        "header": "Appendix BAdditional discussion on related works.",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]