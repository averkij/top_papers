[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Teaser.png",
                "caption": "Figure 1:Overview of theMomaGraph. Given a task instruction,MomaGraphconstructs a task-specific scene graph that highlights relevant objects and parts along with their spatial-functional relationships, enabling the robot to perform spatial understanding and task planning.",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Preliminary Findings and Motivation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Failure.png",
                "caption": "Figure 2:Direct planning often fails even for strong closed-source models like GPT-5, producing wrong actions or missing key steps, while ourGraph-then-Planapproach with structured scene graphs enables accurate and complete task sequences aligned with ground truth.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/logo.png",
                "caption": "Table 1:Comparison betweenMomaGraph-R1and LLaVA variants across task tiers.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/llavaone.png",
                "caption": "",
                "position": 259
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16909/x1.png",
                "caption": "Figure 3:MomaGraphcaptures state changes in the environment and dynamically updates the task-specific scene graph accordingly, enabling the graph to evolve as interactions occur and reflecting updated spatial–functional relationships.",
                "position": 388
            }
        ]
    },
    {
        "header": "5Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16909/x2.png",
                "caption": "Figure 4:Examples of evaluation Multi-Choices VQA tasks in theMomaGraph-Bench. We showcase example questions covering six core reasoning capabilities. Beyond these core capabilities, we further design tasks onDynamic VerificationandLong-horizon Task Decompositionto evaluate temporal reasoning and multi-steps planning.",
                "position": 484
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Claude.png",
                "caption": "Table 2:Performance comparison on theMomaGraph-Bench. We report accuracy (%) across four tiers (T1–T4) and the overall score, with and without graph-based reasoning.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/gemini.png",
                "caption": "",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/instructblip.png",
                "caption": "",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/llava.png",
                "caption": "",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/deepseek.png",
                "caption": "",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/InternVL.png",
                "caption": "",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Qwen.png",
                "caption": "",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/gpt.png",
                "caption": "Table 3:Performance comparison on the BLINK andMomaGraph-Bench. By enforcing multi-view consistency, our method significantly improves correspondence reasoning across all open-source models.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2512.16909/x3.png",
                "caption": "Figure 5:Real Robot experiments on the RobotEra Q5 with a D455, demonstrating four household tasks that require\nspatial, functional, and part-level interactive elements reasoning for task execution.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Setting.png",
                "caption": "Figure 6:Quantitative real-robot evaluation.(a) Environment setup of the real-robot experiment.\n(b) Failure analysis illustrating success/failure rates across different reasoning stages.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Setting.png",
                "caption": "",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/failure_mode.png",
                "caption": "",
                "position": 849
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/sim1.png",
                "caption": "Figure 7:Simulated indoor environments in our benchmark. Each row shows three scenes (Floor 15,Floor 224andFloor 301) with a top-down view of the layout, reachable locations for the robot, and multiview observations from different viewpoints.",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/room_type_distribution.png",
                "caption": "(a)Room-type distribution.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/room_type_distribution.png",
                "caption": "(a)Room-type distribution.",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/cross_analysis_heatmap.png",
                "caption": "(b)Action–function correspondence.",
                "position": 1814
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/action_type_distribution.png",
                "caption": "Figure 10:Task distribution across four room types: kitchen, living room, bedroom, and bathroom.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/functional_relationship_distribution.png",
                "caption": "Figure 11:Distribution of functional relationships across all tasks in the dataset.",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/object_statistics.png",
                "caption": "Figure 12:Statistics of object occurrences, highlighting the most frequent objects in tasks.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/training_reward_curves.png",
                "caption": "Figure 13:Training reward curves duringMomaGraph-R1training.",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/val_reward_curves.png",
                "caption": "Figure 14:Validation reward curves duringMomaGraph-R1training.",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/Robot.png",
                "caption": "Figure 15:Real-world robot execution of household tasks.",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/real1.png",
                "caption": "Figure 16:Real-world example ofMomaGraph-R1performing the task “Open the Cabinet.” From multiview images, the system generates a scene graph capturing spatial–functional relations and outputs the corresponding action plan.",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/real2.png",
                "caption": "Figure 17:Real-world example ofMomaGraph-R1performing the task “Turn off the light.” From multiview images, the system generates a scene graph capturing spatial–functional relations and outputs the corresponding action plan.",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/real3.png",
                "caption": "Figure 18:Real-world example ofMomaGraph-R1performing the task “Open the microwave.” From multiview images, the system generates a scene graph capturing spatial–functional relations and outputs the corresponding action plan.",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2512.16909/Figures/real4.png",
                "caption": "Figure 19:Real-world example ofMomaGraph-R1performing the task “Turn on the TV.” From multiview images, the system generates a scene graph capturing spatial–functional relations and outputs the corresponding action plan.",
                "position": 2160
            }
        ]
    },
    {
        "header": "Appendix BAdditional Ablation Studies",
        "images": []
    }
]