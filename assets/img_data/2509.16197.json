[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16197/figures/symbol.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16197/x1.png",
                "caption": "Figure 1:Qualitative text-to-image generationon challenging prompts. Manzano handles counterintuitive, physics-defying prompts (e.g., ‘The bird is flying below the elephant’) comparably to GPT-4o[35]and Nano Banana[17].",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2509.16197/x2.png",
                "caption": "Figure 2:Quantitative comparisonson popular understanding and generation benchmarks. Manzano 3B and 30B models achieve superior or competitive performance compared to other SOTA unified multimodal LLMs.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16197/x3.png",
                "caption": "Figure 3:Our hybrid tokenizer workflow.(Left): The tokenizer produces two distinct but homogeneous feature streams through separate adapters. During training, one adapter output is randomly sampled and passed to a small LLM decoder for alignment.(Right): Once the tokenizer is trained, the right panel illustrates how these two feature types are applied to understanding and generation tasks.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2509.16197/x4.png",
                "caption": "Figure 4:Training overview.(Left): Unified LLM training with hybrid tokens, the continuous adapter produces embeddings used for the text loss, while the discrete adapter generates hard tokens serving as targets for the image loss.(Right): With vision encoder and adapters fixed, an image decoder is trained to reconstruct images using a diffusion loss.",
                "position": 345
            }
        ]
    },
    {
        "header": "4Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16197/x5.png",
                "caption": "(a)300M Model",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2509.16197/x5.png",
                "caption": "(a)300M Model",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2509.16197/x6.png",
                "caption": "(b)3B Model",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2509.16197/figures/scaling_curve.png",
                "caption": "(a)LLM Decoder Scaling",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2509.16197/figures/scaling_curve.png",
                "caption": "(a)LLM Decoder Scaling",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2509.16197/figures/diffusion_scaling.png",
                "caption": "(b)Image Decoder Scaling",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2509.16197/x7.png",
                "caption": "Figure 7:Qualitative generation results when scaling LLM decoder size. The generated image quality improves as the LLM decoder size increases. For example, in rows 1, 3, and 5, there is a clear trend toward better text rendering and creativity. In row 2, the scene configuration improves significantly with each increase in the LLM decoder’s scale. The 300M model generates an image with only the brick building and the church that are mentioned in the prompt, but as the model grows to 1B and 3B, it begins to include the sign with two circles. Furthermore, the 30B model generates an image that accurately depicts and integrates all the concepts mentioned in the prompt.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2509.16197/x8.png",
                "caption": "Figure 8:Qualitative comparison with SOTA unified models. We compare our Manzano-30B model to the SOTA models through side-by-side comparison. The images generated by our model demonstrate strong capabilities in instruction following, aesthetics, and creativity, often with a photo-realistic quality.",
                "position": 1568
            },
            {
                "img": "https://arxiv.org/html/2509.16197/figures/editing_example.png",
                "caption": "Figure 9:Editing capabilities of Manzano. (a) instruction-guided editing, (b) style transfer across diverse visual domains, and (c) extended editing tasks including inpainting, outpainting, and depth-estimation. Manzano achieves pixel-level controls across these five editing tasks.",
                "position": 1578
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]