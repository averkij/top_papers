[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x1.png",
                "caption": "Figure 1:Large VLMs like PaliGemma-3B[paligemma]excel in spatial reasoning over small VLMs[smolvlm], with correct answers in green and incorrect ones in red. This performance advantage allowsπ0\\pi_{0}[pi_0]based on it to achieve a higher success rate, despite slower inference speed compared to the SmolVLA[smolvla]based on a small VLM. However, SwiftVLA enhances spatiotemporal dynamics for small VLA models while preserving the speed advantages. The success rate and speed are tested on the NVIDIA Jetson Orin[nvidia_jetson_orin].",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x2.png",
                "caption": "Figure 2:(a) Using only 2D features as input to the VLM[pi_0,openvla], which results in limited spatiotemporal awareness.\n(b) Direct fusion approaches combine spatial and 2D\nfeatures within large VLMs[bhat20253d,lin2025evo,3dvla].\n(c) Decoupled designs that introduce a dedicated spatial branch[geovla,pointvla], causing large parameter overhead.\n(d) SwiftVLA leverages a pretrained model[streamvggt]to extract 4D features and applies a feature reconstruction objective to align 4D and 2D representations. In addition,Fusion Tokensand a future prediction objective are introduced to strengthen cross-modal integration. The 4D inputs and auxiliary heads are removed at inference to maintain efficiency.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2512.00903/x3.png",
                "caption": "Figure 3:The pipeline of the SwiftVLA. We first extract 2D and 4D features from input images. A lightweight VLM[smolvlm]processes 2D and 4D features withFusion Tokensto achieve cross-modal integration. The outputs of theFusion Tokensare supervised by the robot end-effector’s future trajectory. During training, we randomly mask either the 2D or the 4D features, and we require the action expert to reconstruct the masked features while learning to generate actions. We show the attention mask under random masking of the 4D features. In this case, 4D features are excluded from the VLM attention, and the model is required to reconstruct the 4D features from the others.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x4.png",
                "caption": "Figure 4:The process of 4D feature extraction. At each step, we sequentially process multi-view observations and load contextual information from the cache for temporal attention. The generated 4D features are updated to the cache and delivered to the VLM.",
                "position": 181
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x5.png",
                "caption": "Figure 5:Comparison of SmolVLA and SwiftVLA under identical initial poses. During execution, SmolVLA fails to grasp accurately, as the end-effector misses the target and collides with the object, causing it to shift and posing safety risks. In contrast, SwiftVLA successfully completed the grasp with accurate positioning and stable control, demonstrating superior performance.",
                "position": 801
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x6.png",
                "caption": "",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2512.00903/x7.png",
                "caption": "Figure 7:Examples from the RoboTwin 2.0[robotwin], including move Stapler Pad, Place A2B Left, Place Bread Basket, Place Dual Shoes, Dump Bin Bigbin, Handover Block.",
                "position": 1043
            }
        ]
    },
    {
        "header": "Appendix AArchitecture Design.",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x8.png",
                "caption": "Figure 8:Real-world manipulation tasks used in our experiments. From top to bottom, the examples correspond to Clean the Desk, Throw the Bottle, and Stack Bowls.",
                "position": 1117
            }
        ]
    },
    {
        "header": "Appendix CMore Challenging Real-World Experimental Results.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00903/x9.png",
                "caption": "Figure 9:Real-world execution process of the Fold the Cloth task, which requires long-horizon reasoning and precise manipulation of deformable objects.",
                "position": 1206
            }
        ]
    },
    {
        "header": "Appendix DSupplementary Video",
        "images": []
    }
]