[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15700/x1.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/wan_transition_frame_0000.png",
                "caption": "Figure 2:In this figure, we illustrate a general yet under-explored observation: video generation models possess an innate ability to perform subject mixing via scene transitions from a mixed-subject first frame. As shown, the red-boxed results (without the transition phrase:<<transition>>) contrast with the blue-boxed results (with a carefully chosen<<transition>>e.g., “The camera view suddenly zoom in to show\") revealing significant differences in composition. However, this phenomenon faces three key limitations that hinder practical use: 1) The prompt engineering process for<<transition>>is highly manual, time-consuming, and model/video-dependent. 2) Scene transitions are often unstable. 3) Object identity is often lost, resulting in changes in appearance or the disappearance of reference objects.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/veo_no_transition_frame_0025.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/veo_no_transition_frame_0035.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/veo_transition_frame_0025.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/veo_transition_frame_0079.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/sora_driving_no_shift_frame_0005.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/sora_driving_no_shift_frame_0035.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/sora_driving_shift_frame_0056.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/sora_driving_shift_frame_0093.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/wan_no_transition_frame_0005.png",
                "caption": "",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/wan_no_transition_frame_0035.png",
                "caption": "",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/wan_transition_frame_0005.png",
                "caption": "",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/transitions/wan_transition_frame_0035.png",
                "caption": "",
                "position": 191
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15700/x2.png",
                "caption": "Figure 3:The overview of our proposed pipeline FFGo, consists of 1) Dataset Curation for getting the high quality finetuning data from existing videos, 2) Few-shot LoRA Adaptation for training/inference to invoke the I2V model’s innate ability in fusing the subjects in the first frame and perform a scene trasition to generate a videoVm​i​xV_{mix}following subjects in the first mixing frameIm​i​xI_{mix}and the text prompt.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15700/x3.png",
                "caption": "Figure 4:As shown in the figure, in rare cases where the base model Wan2.2-I2V-A14B successfully performs a scene transition while preserving all reference object identities, the output closely resembles ours. This demonstrates that our add-on approach effectively retains the base model’s pre-trained generative capabilities.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2511.15700/x4.png",
                "caption": "Figure 5:Qualitative comparison with baseline methods. This test scenario involves generalized multi-object interactions. As shown in the figure, our method best preserves the identities of input objects and the scene, while generating a customized video with coherent motion that aligns with the text prompt description.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2511.15700/x5.png",
                "caption": "Figure 6:Qualitative comparison with baseline methods. This scenario evaluates performance with an excessive number of references, five in total (four objects and one scene). VACE and SkyReels-A2, due to their architecture-based limitations, support only up to three references and fail to include all four reference objects in the generated video. In contrast, our model successfully fuses all four objects into a coherent, customized video with natural interactions. Notably, our model also enables precise selection via text prompt (e.g.,blue iPhone), preserving key visual traits such as the triple-camera design while modifying appearance (e.g., changing the color to blue).",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2511.15700/x6.png",
                "caption": "Figure 7:Qualitative comparison with baseline methods.\nThis scenario evaluates generalized human-object interactions involving multiple humans. While both VACE and SkyReels-A2 excel in customized single human-object video generation, they struggle in more complex multi-human scenarios where interactions are mediated by shared objects. In such cases, both baselines fail to maintain object integrity and coherent interaction. In contrast, our method reliably generates consistent videos with preserved object identity and realistic multi-human interactions.",
                "position": 412
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Appendix AVideo Results",
        "images": []
    },
    {
        "header": "Appendix BDetails about Training and Testing Set",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15700/figures/dataset.drawio.png",
                "caption": "Figure 10:Our training dataset comprises four categories: human–object interaction (60%), human–human interaction (14%), element insertion (20%), and robot manipulation (6%).",
                "position": 590
            }
        ]
    },
    {
        "header": "Appendix CDetails about User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15700/figures/interface1.png",
                "caption": "(a)Users rank the overall quality of the four candidate videos.",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/interface1.png",
                "caption": "(a)Users rank the overall quality of the four candidate videos.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2511.15700/figures/interface2.png",
                "caption": "(b)Users rate three specific aspects with Likert scale 1-5.",
                "position": 775
            }
        ]
    },
    {
        "header": "Appendix DMore Training and Inference Details",
        "images": []
    }
]