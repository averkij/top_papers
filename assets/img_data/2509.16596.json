[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16596/x1.png",
                "caption": "Figure 1:Illustration of parameter restoration. We find that SFT introduces many unnecessary parameter updates, and model performance can be significantly improved by restoring some of the most updated parameters in the fine-tuned model to their original values in the pre-trained model.",
                "position": 182
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16596/x2.png",
                "caption": "(a)LLaMA-3-8B (In-Domain)",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x2.png",
                "caption": "(a)LLaMA-3-8B (In-Domain)",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x3.png",
                "caption": "(b)LLaMA-3-8B (Out-of-Domain)",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x4.png",
                "caption": "(c)LLaMA-3-70B (In-Domain)",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x5.png",
                "caption": "(d)LLaMA-3-70B (Out-of-Domain)",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x6.png",
                "caption": "Figure 3:Illustration of logits re-normalization. Since the pre-trained LLM tends to assign high probabilities to common dummy words, we identify the ten highest logits in the fine-tuned LLM and extract the corresponding values from the pre-trained LLM. After re-normalization, we compute the KL divergence to quantify the distributional difference.",
                "position": 625
            }
        ]
    },
    {
        "header": "4Token-Level Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16596/x7.png",
                "caption": "Figure 4:Performance onùíüt‚Äãe‚Äãs‚Äãt‚àí4‚Ñ≥\\mathcal{D}_{test-4}^{\\mathcal{M}}(Acct‚Äãe‚Äãs‚Äãt‚àí4‚Ñ≥\\textbf{Acc}_{test-4}^{\\mathcal{M}}) of LLMs fine-tuned on LLaMA-3-8B.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x8.png",
                "caption": "Figure 5:KL divergence of logits distribution between LLaMA-3-8B fine-tuned with different datasets and the pre-trained one.",
                "position": 690
            }
        ]
    },
    {
        "header": "5Parameter-Level Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt for SFT",
        "images": []
    },
    {
        "header": "Appendix BMore Details of Experiments",
        "images": []
    },
    {
        "header": "Appendix CMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.16596/x9.png",
                "caption": "(a)LLaMA-2-7B (In-Domain)",
                "position": 2048
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x9.png",
                "caption": "(a)LLaMA-2-7B (In-Domain)",
                "position": 2051
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x10.png",
                "caption": "(b)LLaMA-2-7B (Out-of-Domain)",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x11.png",
                "caption": "(c)LLaMA-2-13B (In-Domain)",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x12.png",
                "caption": "(d)LLaMA-2-13B (Out-of-Domain)",
                "position": 2069
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x13.png",
                "caption": "(e)LLaMA-2-70B (In-Domain)",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2509.16596/x14.png",
                "caption": "(f)LLaMA-2-70B (Out-of-Domain)",
                "position": 2081
            }
        ]
    },
    {
        "header": "Appendix DData Distribution of Different LLMs",
        "images": []
    },
    {
        "header": "Appendix EDiscussion of Redundant Parameter Updates",
        "images": []
    },
    {
        "header": "Appendix FDetails of Data Processing",
        "images": []
    }
]