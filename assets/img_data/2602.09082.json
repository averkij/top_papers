[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09082/figures/ui-venus-logo-3.png",
                "caption": "",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x1.png",
                "caption": "Figure 1:UI-Venus-1.5achieves SOTA performance across multiple GUI grounding and navigation benchmarks. Note that in the three radar charts of grounding, we have normalized the results of the top-performing model to 100% to more clearly differentiate comparisons among various baselines.",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09082/x2.png",
                "caption": "Figure 2:System Overview of UI-Venus-1.5. It operates as an end-to-end GUI Agent that interprets user instructions, perceives interface states through screenshots, and executes interactive actions (e.g., clicking, typing, scrolling) to accomplish tasks across diverse executable environments.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x3.png",
                "caption": "Figure 3:The Four-Stage Pipeline of UI-Venus-1.5. Starting from Qwen3-VL Series, the model progresses through a multi-stage curriculum: (1) Mid-Training on large-scale GUI data for domain knowledge injection; (2) Offline-RL for task-specific optimization across grounding, mobile, and web objectives; (3) Online-RL to enhance navigation in complex, real-world settings; and (4) Model Merge, which unifies the specialized models into the final UI-Venus-1.5.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x4.png",
                "caption": "(a)Mid-Training Corpus",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x4.png",
                "caption": "(a)Mid-Training Corpus",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x5.png",
                "caption": "(b)Iterative Data Refinement Pipeline",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x6.png",
                "caption": "Figure 5:Data generation loop via DaaS environment. By iteratively performing this pipeline, the success rate of total trace generation raises from 17.9% to over 70%.",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x7.png",
                "caption": "(a)Mobile Scenario",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x7.png",
                "caption": "(a)Mobile Scenario",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x8.png",
                "caption": "(b)Web Scenario",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2602.09082/x9.png",
                "caption": "Figure 7:Architecture of the Unified Device-as-a-Service (DaaS) layer.This framework bridges upstream tasks (Training and Inference) with downstream heterogeneous device clusters (Chrome, Mobile, and Desktop). The Multi-language DaaS SDK provides a unified abstraction for diverse interaction protocols, while the Group Control Gateway (GCGW) ensures high-performance and secure resource exposure through secondary hash routing, zero-copy I/O, and a multi-protocol reverse proxy.",
                "position": 411
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09082/figures/t-sne2.png",
                "caption": "Figure 8:Latent space visualizationof (a) base model and (b) model with GUI knowledge. The emergence of distinct clusters indicates that our Mid-Training has successfully enriched the model with GUI domain knowledge. This increased discriminative power between GUI and general data provides a robust structural basis for the reinforcement learning stage.",
                "position": 1632
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAction Space and Prompt Templates",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details of All Grounding Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details of All Navigation Benchmarks",
        "images": []
    }
]