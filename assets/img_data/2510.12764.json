[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12764/x1.png",
                "caption": "Figure 1:AnyUpis a universal feature upsampling model that can upsample any feature from any to any resolution, generalizing to unseen features while achieving state-of-the-art performance.",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12764/x2.png",
                "caption": "Figure 2:Visual comparison against other methods.RGB channels correspond to the first three principal components computed over all features. Previous methods result in excessive smoothing or contain other artifacts: See,e.g., in the first row, the smoothed-out cloud features in LoftUp or the feature distribution shift for the mountains in JAFAR, as well as the oversmoothing and halo-artifacts of FeatUp and Guided Filter in the third row. AnyUp (Ours) produces sharp output feature maps while preserving the input feature quality.",
                "position": 333
            }
        ]
    },
    {
        "header": "3Task Formulation",
        "images": []
    },
    {
        "header": "4Learning Encoder-Agnostic Feature Upsampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12764/x3.png",
                "caption": "Figure 3:Method Overview.AnyUp performs window attention-based upsampling (4.2). Input features are processed with a feature-agnostic layer (4.1). During training, features computed for randomly sampled image parts are used as a reference for the respective part of the upsampled feature map (4.3).",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2510.12764/x4.png",
                "caption": "Figure 4:Feature-agnostic layer.Input channels are processed independently and contributions to basis filters are averaged over all channels leading to outputs invariant to input dimensionality.",
                "position": 369
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12764/x5.png",
                "caption": "Figure 5:Qualitative Probing Results.Visualization of linear probing results for monocular depth estimation on NYUv2 and semantic segmentation on ADE20k. More visualizations given in App.D.",
                "position": 442
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details on Related Work",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12764/x6.png",
                "caption": "Figure 7:Randomly sampled augmentations applied on a test image (top left).",
                "position": 1747
            }
        ]
    },
    {
        "header": "Appendix CCompulsory Note on LLM Usage",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12764/x7.png",
                "caption": "Figure 8:Visualization of attention artifacts and removal thereof through local window attention. Unconstrained, global attention leads to upsampled features relying on information from far-away, non-related objects. We provide a simple fix to this which also simplifies the upsampling problem for the model by restricting attention only to local windows that are computed relative to the feature map size.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000007_seg.jpg",
                "caption": "Figure 9:Linear Probing results for semantic segmentation using a pre-trained DINOv2 (ViT-S) probe. AnyUp preserves the input feature space while upsampling and sometimes even outperforming the ground-truth,e.g., the tower segmentation in the fifth row. While FeatUpâ€™s predictions are generally consistent with the low-resolution predictions, it smoothens object boundaries too much and is inable to get sharp segmentations. LoftUp shifts the feature distribution while upsampling, resulting in erroneous predictions with the pre-trained probe. Predictions using JAFAR suffer from reduced details for thin objects, see,e.g., the lights in row 2 or the fence in row 4.",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000011_seg.jpg",
                "caption": "",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000451_seg.jpg",
                "caption": "",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000452_seg.jpg",
                "caption": "",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000537_seg.jpg",
                "caption": "",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000713_seg.jpg",
                "caption": "",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k-pretrained/ADE_val_00000715_seg.jpg",
                "caption": "",
                "position": 1792
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0006.jpg",
                "caption": "Figure 10:Linear Probing results for depth estimation using a pre-trained DINOv2 (ViT-S) probe. AnyUp stands out by preserving sharp edges from the guidance image while preserving the locality of features needed for smooth depth map prediction and preserving full objects, as,e.g., the white board in the fourth row. Note that the feature distribution shift of LoftUp is less pronounced for depth estimation, which can be partly attributed to the scale-shift alignment performed after prediction.",
                "position": 1796
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0003.jpg",
                "caption": "",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0004.jpg",
                "caption": "",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0005.jpg",
                "caption": "",
                "position": 1804
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0008.jpg",
                "caption": "",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0009.jpg",
                "caption": "",
                "position": 1808
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_pretrained/depth_comparison_0010.jpg",
                "caption": "",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00000715_seg.jpg",
                "caption": "Figure 11:Additional linear probing results for semantic segmentation using probes trained on upsampled DINOv2 (ViT-S) features. AnyUp outperforms prior upsampling methods by also being able to segment fine-details and giving cleaner segmentation outputs.",
                "position": 1814
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00000796_seg.jpg",
                "caption": "",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00000905_seg.jpg",
                "caption": "",
                "position": 1820
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00000908_seg.jpg",
                "caption": "",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00000911_seg.jpg",
                "caption": "",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00001315_seg.jpg",
                "caption": "",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/ade20k_probing/probingADE_val_00001322_seg.jpg",
                "caption": "",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample4.jpg",
                "caption": "Figure 12:Additional linear probing results for depth estimation using probes trained on upsampled DINOv2 (ViT-S) features. AnyUp consistently gives sharp object boundaries while matching the ground truth the best.",
                "position": 1832
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample5.jpg",
                "caption": "",
                "position": 1836
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample16.jpg",
                "caption": "",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample29.jpg",
                "caption": "",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample38.jpg",
                "caption": "",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample47_2.jpg",
                "caption": "",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2510.12764/assets/nyuv2_probing/probing_depth_sample48_2.jpg",
                "caption": "",
                "position": 1846
            }
        ]
    },
    {
        "header": "Appendix DAdditional Visualizations",
        "images": []
    }
]