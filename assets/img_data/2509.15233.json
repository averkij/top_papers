[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15233/x1.png",
                "caption": "Figure 1:Examples illustrating our RPAs’ performance compared to general baselines. More examples are provided in Appendix.",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15233/x2.png",
                "caption": "Figure 2:The video types and examples of our dataset.",
                "position": 286
            }
        ]
    },
    {
        "header": "3Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15233/x3.png",
                "caption": "Figure 3:The illustration of video caption. We uniformly divide the video into segments and annotate each segment with a frame description, then we summarize these descriptions as a video caption and employ it during the fine-tuning and inference phase.Notably,video captions are utilized distinctly across the two phases, originating from different videos and serving distinct purposes. Specifically, during thefine-tuning phase, captions are employed to generate question-answer pairs. In contrast, during theinference phase, captions are used to develop the role context.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x4.png",
                "caption": "Figure 4:Our framework consists of three key components: (1)Adaptive Temporal Sampling: This module adaptively samples video frames based on the input video’s length. (2)Dynamic Role Profile Representation: This module constructs dynamic role profiles from the sampled video frame. (3)Static Role Profile Representation: This module extracts static role profiles from dialogue and summary contexts. Further, we propose a comprehensive evaluation approach incorporating eight metrics.",
                "position": 339
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15233/x5.png",
                "caption": "Figure 5:The example of fine-tune data format, the special token<video>indicates the position where the video is inserted.",
                "position": 423
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statements",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15233/x6.png",
                "caption": "Figure 6:Model URL List",
                "position": 1597
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x7.png",
                "caption": "Figure 7:The video types distribution of our dataset.",
                "position": 1608
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x8.png",
                "caption": "Figure 8:The SFT dialogue cases.",
                "position": 1611
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x9.png",
                "caption": "Figure 9:The dialogue generation prompt",
                "position": 1618
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x10.png",
                "caption": "Figure 10:The dialogue filter prompt",
                "position": 1621
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x11.png",
                "caption": "Figure 11:User Study Results",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x12.png",
                "caption": "Figure 12:Showcases of video caption.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x13.png",
                "caption": "Figure 13:Showcases of our framework.",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x14.png",
                "caption": "Figure 14:Showcases of our framework.",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x15.png",
                "caption": "Figure 15:Showcases of our framework.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x16.png",
                "caption": "Figure 16:Character consistency evaluation prompt.",
                "position": 1785
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x17.png",
                "caption": "Figure 17:Knowledge hallucination evaluation prompt.",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x18.png",
                "caption": "Figure 18:Utterance fluency evaluation prompt.",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x19.png",
                "caption": "Figure 19:Instructional adherence evaluation prompt.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x20.png",
                "caption": "Figure 20:Tone consistency evaluation prompt.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x21.png",
                "caption": "Figure 21:Response accuracy evaluation prompt.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x22.png",
                "caption": "Figure 22:Video-Text relevance evaluation prompt.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2509.15233/x23.png",
                "caption": "Figure 23:human likeness evaluation prompt.",
                "position": 1806
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]