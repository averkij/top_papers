[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08331/x1.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/framework_diagram.png",
                "caption": "Figure 2:Our method consists of three components: flow field extraction, real-time noise warping, and diffusion model fine-tuning/inference. During fine-tuning, we use the original captions of video samples. At inference, our method enables adaptation of reference motion to various prompts and/or initial frames, offering creativity and diversity in generation.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/algo_diagram_2_layers.png",
                "caption": "Figure 3:Diagram of our noise warping algorithm. A case example of our algorithm illustrates both theexpansionandcontractioncases, along with example density values. Each node represents some noise pixel ‚Äòq‚Äô. Noise valuesq0..3subscriptùëû0..3q_{0..3}italic_q start_POSTSUBSCRIPT 0..3 end_POSTSUBSCRIPTare transferred from frame 0 to frame 1 using forward optical flow, and the remaining pixels in frame 1 that did not receive any values obtain their values from frame 0 using reverse optical flow (theexpansioncase). In thecontractioncases such asq2‚Ä≤subscriptsuperscriptùëû‚Ä≤2q^{\\prime}_{2}italic_q start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, their densities become the sum of their sources. And in theexpansioncase, where one source pixel spreads out into multiple target pixels, such asq2subscriptùëû2q_{2}italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTspreading out intoq1‚Ä≤subscriptsuperscriptùëû‚Ä≤1q^{\\prime}_{1}italic_q start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandq3‚Ä≤subscriptsuperscriptùëû‚Ä≤3q^{\\prime}_{3}italic_q start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, its density is dispersed.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/degradation_comparison.png",
                "caption": "Figure 4:Showcasing the effect of noise degradation levelŒ≥ùõæ\\mathbf{\\gamma}italic_Œ≥on generated videos. A few frames from the driving video are shown in the leftmost column. Our model outputs are in the next 3 columns. As degradation decreases (Œ≥ùõæ\\mathbf{\\gamma}italic_Œ≥from 0.7 to 0.5), the video more strictly adheres to the input flow. This allows us to control video movement with a user-specified level of precision.",
                "position": 389
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08331/x2.png",
                "caption": "Figure 5:Qualitative comparisons of local object motion control. Zoom in for details. The user selects any number of polygons, then scales, rotates, or translates them along arbitrary paths, which are then used to create the warped noise flow.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/comparisons_video_diffusion_turning_object.png",
                "caption": "Figure 6:Qualitative comparisons of camera movement video generation of our method (b) and MotionClone (c) using a turning source video (a).",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2501.08331/x3.png",
                "caption": "Figure 7:Qualitative comparisons of motion transfer T2V on the DAVIS dataset. Zoom-in needed.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/wonderjourney_comparison.png",
                "caption": "Figure 8:We apply our method to a sequence of frames warped using monocular depth estimation, enabling consistent 3D scene generation from a single image. In this example, we use results from WonderJourney. Zoom-in needed.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/depthwarp.jpg",
                "caption": "Figure 9:We explore a 3D scene, flying into a given image. Similar toFig.8, we take an image as an input and use a monocular depth estimator DepthPro[5]to get a depth map. Then, we use that depth to generate a crudely warped video (note the pixelation on the rough depth warp column when zoomed in) - and from the movement in that video get warped noise. From there, we run our motion-conditioned I2V model.",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2501.08331/x4.png",
                "caption": "Figure 10:Comparison of initial frame video editing results across different methods. All methods start with the same edited initial frame derived from the original video.",
                "position": 1007
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Social impact statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Gaussianity preservation of our noise warping algorithm",
        "images": []
    },
    {
        "header": "7Qualitative results of training-free image diffusion based video editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/nongauss_vis.png",
                "caption": "Figure 11:A direct visualization of the noise produced by our noise warping algorithm, HIWYN[8], bilinear, and nearest neighbor interpolations. The forward movement in this long roller-coaster video forces the noise to expand significantly. Early in the video, the HIWYN baseline produces visibly non-Gaussian results. See the full video on our supplementary website.",
                "position": 2135
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/stroller_frame20.png",
                "caption": "Figure 12:Using different noise warping algorithms on DeepFloyd¬†IF for video super-resolution on the DAVIS dataset.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/naz_diffrelight_grid11.jpg",
                "caption": "Figure 13:Using different noise warping algorithms on DifFRelight for portrait video relighting.",
                "position": 2141
            }
        ]
    },
    {
        "header": "8The advantage of noise warping",
        "images": []
    },
    {
        "header": "9Comparison to the video diffusion base model without finetuning",
        "images": []
    },
    {
        "header": "10User study settings and statistics",
        "images": []
    },
    {
        "header": "11Model Agnostic",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08331/x5.png",
                "caption": "Figure 14:We show a cut-and-drag¬†animation of a windmill rotating clockwise, next to the derived optical flow, our outputs, a baseline and an ablation.Notethat the input video column appears to have two sets of panels because it‚Äôs being cut and dragged over itself to create rotational motion.When using noise warping is better: Per-frame structural information can poison the result of MotionClone, giving the windmill an extra set of arms - whereas ours only receives motion information from optical flow alone via warped noise (there are no double-windmills in the optical flow patterns).Ablation in rightmost column: warped noise withŒ≥=.5ùõæ.5\\mathbf{\\gamma}=.5italic_Œ≥ = .5on the CogVideoX base model before we fine-tune it. Because warped noise is statisically distinct from the pure Gaussian noise CogVideoX was trained on, without fine-tuning it can result in visual artifacts. Note how although the per-frame quality suffers here, it still picks up on motion queues from the warped noise (the camera zooms into the windmill).",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_screenshot_1.png",
                "caption": "(a)User study interface and questions for local object motion control, corresponding toFig.5in the main paper.",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_screenshot_1.png",
                "caption": "(a)User study interface and questions for local object motion control, corresponding toFig.5in the main paper.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_screenshot_2.png",
                "caption": "(b)User study interface and questions for turnable camera movement video generation, corresponding toFig.6in the main paper.",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_1_statistics_1.png",
                "caption": "(c)User study statistics for local object motion control on the first question ‚ÄúWhich video is the best overall?‚Äù",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_1_statistics_2.png",
                "caption": "(d)User study statistics for local object motion control on the second question ‚ÄúWhich video best aligns with the user intent for controlling the object movement based on the input?‚Äù",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_1_statistics_1.png",
                "caption": "(e)User study statistics for local object motion control on the third question ‚ÄúWhich video best preserves the intended camera movement from the input?‚Äù",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_1_statistics_1.png",
                "caption": "(f)User study statistics for local object motion control on the fourth question ‚ÄúWhich video maintains the most consistent and stable motion throughout?‚Äù",
                "position": 2213
            },
            {
                "img": "https://arxiv.org/html/2501.08331/extracted/6135418/fig/user_study_2_statistics_1.png",
                "caption": "(g)User study statistics for motion transfer on the first question ‚ÄúWhich video has better overall quality?‚Äù",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2501.08331/x6.png",
                "caption": "Figure 16:Fine-tuning AnimateDiff with our warped noise flow. We used Go-with-the-Flow to fine-tune AnimateDiff T2V, and display the results above. The input video is on the left, and from that video we derive warped noise which is used to initialize AnimateDiff on the columns to its right with different text prompts.",
                "position": 2226
            }
        ]
    },
    {
        "header": "12Pseudo code",
        "images": []
    }
]