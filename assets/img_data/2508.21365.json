[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Formalization",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21365/figures/grpo.png",
                "caption": "Figure 1:Demonstrationof GRPO training with Game State.refers to trained models, andrefers to frozen models. Given the current game state, the model is asked to predict the proper action, and provide the thinking process as the analysis of why consider this action. We then compare the predicted action with ground-truth values using a rule-based verifier to update the policy model. This process enables the model to perform decision making within the game environment and refining its decision-making accordingly.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x3.png",
                "caption": "",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x4.png",
                "caption": "",
                "position": 277
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21365/x5.png",
                "caption": "(a)Action Prediction Task.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x5.png",
                "caption": "(b)Distribution of Error Cases",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x6.png",
                "caption": "(a)Qwen2.5-14B",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x6.png",
                "caption": "(a)Qwen2.5-14B",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x7.png",
                "caption": "(b)Qwen2.5-32B",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2508.21365/x8.png",
                "caption": "(c)Qwen3-14B",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_1.png",
                "caption": "Figure 4:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_2.png",
                "caption": "Figure 5:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_3.png",
                "caption": "Figure 6:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_4.png",
                "caption": "Figure 7:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_5.png",
                "caption": "Figure 8:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_7.png",
                "caption": "Figure 9:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_8.png",
                "caption": "Figure 10:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_9.png",
                "caption": "Figure 11:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_10.png",
                "caption": "Figure 12:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2508.21365/figures/case_new_11.png",
                "caption": "Figure 13:One of the cases of TiG. <think> </think> refers to the thinking process of model output, and <result> </result> refers to the model guidance to the main player in natural language.",
                "position": 684
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Setup",
        "images": []
    },
    {
        "header": "Appendix BPreliminary Study of Deepseek-R1 Performance on Game",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21365/figures/json_object.png",
                "caption": "Figure 14:Demonstration of JSON object for each game state.",
                "position": 1544
            }
        ]
    },
    {
        "header": "Appendix CFormalization of Reinforcement Learning using GRPO",
        "images": []
    }
]