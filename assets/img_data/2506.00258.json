[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00258/x1.png",
                "caption": "Figure 1:Even when the instruction appears valid, it may silently conflict with the visual context. Implicit reasoning requires models to detect what’s missing, ambiguous, contradictory, or infeasible—without being told.",
                "position": 213
            }
        ]
    },
    {
        "header": "2RQ1: How do MLLMs Perform on Implicit Reasoning Tasks?",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00258/x2.png",
                "caption": "Figure 2:Four categories under the implicit reasoning scenarios, posing diverse challenges.",
                "position": 283
            }
        ]
    },
    {
        "header": "3RQ2: Do Models Know More Than They Say?",
        "images": []
    },
    {
        "header": "4RQ3: Can We Recover Reasoning with Inference-Time Fixes?",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Details",
        "images": []
    },
    {
        "header": "Appendix BPrompt Details for the LLM Judge",
        "images": []
    },
    {
        "header": "Appendix CModel Application Details",
        "images": []
    },
    {
        "header": "Appendix DExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00258/x3.png",
                "caption": "Figure 3:In this example, the instruction refers to a computer while there are several possible references in the scene. Even though some models are able to identify multiple references during thinking, in their final answer, they choose to omit those and not to ask for clarification. Instead, they end up outputting templated, generic step-by-step instructions to carry out the task—likely due to alignment stress.",
                "position": 2479
            },
            {
                "img": "https://arxiv.org/html/2506.00258/x4.png",
                "caption": "Figure 4:In this example, the title name (\"MAC 3 Pack…\")and brand name shown on the product image (\"Pentasy) pose a contradiction, and the model is prompted to \"find the brand of the lipstick\", targeting the contradiction. Even though some models are able to identify the conflicting fields during thinking, in their final answer, they choose to omit those and end up choosing one of them—likely due to alignment stress.",
                "position": 2482
            }
        ]
    },
    {
        "header": "Appendix EData Release",
        "images": []
    }
]