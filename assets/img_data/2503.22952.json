[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22952/x1.png",
                "caption": "Figure 1:OmniMMI consists of two categories of multi-modal interactive challenges: streaming video understanding (top) and proactive reasoning (bottom). Each query is processed into natural language text and synthetic audio as input.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The OmniMMI Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22952/x2.png",
                "caption": "Figure 2:Distribution and examples of different types of query prompts.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2503.22952/x3.png",
                "caption": "Figure 3:Distribution of video duration length.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2503.22952/x4.png",
                "caption": "Figure 4:Multiplexing Modeling of M4.vùë£vitalic_vis the streaming video,qisubscriptùëûùëñq_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTdenotes the input query,tisubscriptùë°ùëñt_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTindicates the generated token,nisubscriptùëõùëñn_{i}italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTdenotes noise token which will be discarded from the KVCache. The streaming video KVCache is computed to trigger a highlight spot index for the next response generation. Proactive interruption is facilitated through the computation of specific tokens designed for noise and stop signals. The parallel decoding takes mask strategy with dynamic KVCache to process multiple queries in one forward step.",
                "position": 752
            }
        ]
    },
    {
        "header": "4Multi-modal Multiplexing Modeling",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22952/x5.png",
                "caption": "Figure 5:Attention feature map utilizes query as Q frames as K. The query consists of the last three tokens of the text query, while the key is represented by the mean-pooled frame.",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2503.22952/x6.png",
                "caption": "Figure 6:Performance on the Proactive Turn-taking task for noise and normal query over different scaling factor.",
                "position": 1553
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAudio Adaption Analysis",
        "images": []
    },
    {
        "header": "Appendix BHighlight Spot Algorithm",
        "images": []
    },
    {
        "header": "Appendix CSingle Question Analysis of Multi-turn Dependency Reasoning",
        "images": []
    },
    {
        "header": "Appendix DSingle Question Analysis of Dynamic State Grounding",
        "images": []
    },
    {
        "header": "Appendix EAnnotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22952/extracted/6319541/figures/front_end.png",
                "caption": "Figure 7:The Front-End Interface for Human Annotation",
                "position": 3051
            }
        ]
    },
    {
        "header": "Appendix FM4-IT Construction Details",
        "images": []
    },
    {
        "header": "Appendix GM4 Implementation Details",
        "images": []
    }
]