[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13941/x1.png",
                "caption": "Figure 1:Employing self-learning with multi-domain data,Nemotron-CrossThinkoutperforms baseline models, including domain-specific training (Only Math) and Open-Reasoner-Zero (orz-7B), achieving consistent gains across all reasoning tasks.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Nemotron-CrossThink: Scaling Self-Learning Beyond Math",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13941/x2.png",
                "caption": "Figure 2:Nemotron-CrossThink.We (a) curate QA pairs from from synthetic (Common Crawl) and open-source datasets, categorized into general-purpose reasoning (ğ’Ÿgâ¢pâ¢rsubscriptğ’Ÿğ‘”ğ‘ğ‘Ÿ\\mathcal{D}_{gpr}caligraphic_D start_POSTSUBSCRIPT italic_g italic_p italic_r end_POSTSUBSCRIPT) and mathematical reasoning (ğ’Ÿmâ¢rsubscriptğ’Ÿğ‘šğ‘Ÿ\\mathcal{D}_{mr}caligraphic_D start_POSTSUBSCRIPT italic_m italic_r end_POSTSUBSCRIPT); (b) apply structured templates to convert data into multiple-choice (mcq) and open-ended formats, promoting diverse reasoning trajectories; (c) filter out unverifiable or ill-formatted responses; (d) train an RL policy using Group Relative Policy Optimization (grpo). The final reward is used to update the policy, iteratively improving the modelâ€™s reasoning capabilities across diverse domains.",
                "position": 153
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": []
    },
    {
        "header": "5Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13941/x3.png",
                "caption": "Figure 3:Token efficiency comparison of models trained onâ„¬gâ¢pâ¢râ†‘subscriptâ„¬â†‘ğ‘”ğ‘ğ‘Ÿabsent\\mathcal{B}_{gpr\\uparrow}caligraphic_B start_POSTSUBSCRIPT italic_g italic_p italic_r â†‘ end_POSTSUBSCRIPT(multi-domain blend) and two single domain blends (â„¬oâ¢nâ¢lâ¢yâ¢_â¢mâ¢aâ¢tâ¢hsubscriptâ„¬ğ‘œğ‘›ğ‘™ğ‘¦_ğ‘šğ‘ğ‘¡â„\\mathcal{B}_{only\\_math}caligraphic_B start_POSTSUBSCRIPT italic_o italic_n italic_l italic_y _ italic_m italic_a italic_t italic_h end_POSTSUBSCRIPTandorz).",
                "position": 878
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Proportion across Blends",
        "images": []
    },
    {
        "header": "Appendix BToken Efficiency Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13941/x4.png",
                "caption": "Figure 4:Average token lengths of correct and incorrect responses across general-purpose and math reasoning tasks for models trained onâ„¬gâ¢pâ¢râ†‘subscriptâ„¬â†‘ğ‘”ğ‘ğ‘Ÿabsent\\mathcal{B}_{gpr\\uparrow}caligraphic_B start_POSTSUBSCRIPT italic_g italic_p italic_r â†‘ end_POSTSUBSCRIPT,â„¬oâ¢nâ¢lâ¢yâ¢_â¢mâ¢aâ¢tâ¢hsubscriptâ„¬ğ‘œğ‘›ğ‘™ğ‘¦_ğ‘šğ‘ğ‘¡â„\\mathcal{B}_{only\\_math}caligraphic_B start_POSTSUBSCRIPT italic_o italic_n italic_l italic_y _ italic_m italic_a italic_t italic_h end_POSTSUBSCRIPT, andorz.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2504.13941/x5.png",
                "caption": "Figure 5:Sub-category Accuracy Comparison acrossmmlu-proDomains.Theâ„¬gâ¢pâ¢râ†‘subscriptâ„¬â†‘ğ‘”ğ‘ğ‘Ÿabsent\\mathcal{B}_{gpr\\uparrow}caligraphic_B start_POSTSUBSCRIPT italic_g italic_p italic_r â†‘ end_POSTSUBSCRIPTblend consistently outperformsâ„¬oâ¢nâ¢lâ¢yâ¢_â¢mâ¢aâ¢tâ¢hsubscriptâ„¬ğ‘œğ‘›ğ‘™ğ‘¦_ğ‘šğ‘ğ‘¡â„\\mathcal{B}_{only\\_math}caligraphic_B start_POSTSUBSCRIPT italic_o italic_n italic_l italic_y _ italic_m italic_a italic_t italic_h end_POSTSUBSCRIPTin a wide range of non-math reasoning categories such as business, law, psychology, and economics. Surprisingly, it also slightly surpasses the math-specialized blend in themmlu-promath category, highlighting the generalizability and versatility of multi-domain training.",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2504.13941/x6.png",
                "caption": "Figure 6:Sub-category Accuracy Comparison acrossagieval.Whileâ„¬oâ¢nâ¢lâ¢yâ¢_â¢mâ¢aâ¢tâ¢hsubscriptâ„¬ğ‘œğ‘›ğ‘™ğ‘¦_ğ‘šğ‘ğ‘¡â„\\mathcal{B}_{only\\_math}caligraphic_B start_POSTSUBSCRIPT italic_o italic_n italic_l italic_y _ italic_m italic_a italic_t italic_h end_POSTSUBSCRIPTperforms marginally better in the math,â„¬gâ¢pâ¢râ†‘subscriptâ„¬â†‘ğ‘”ğ‘ğ‘Ÿabsent\\mathcal{B}_{gpr\\uparrow}caligraphic_B start_POSTSUBSCRIPT italic_g italic_p italic_r â†‘ end_POSTSUBSCRIPTachieves stronger results in non-math domains.",
                "position": 1805
            },
            {
                "img": "https://arxiv.org/html/2504.13941/x7.png",
                "caption": "Figure 7:Sub-category Accuracy Comparison acrosssupergpqa.Theâ„¬gâ¢pâ¢râ†‘subscriptâ„¬â†‘ğ‘”ğ‘ğ‘Ÿabsent\\mathcal{B}_{gpr\\uparrow}caligraphic_B start_POSTSUBSCRIPT italic_g italic_p italic_r â†‘ end_POSTSUBSCRIPTblend consistently outperformsâ„¬oâ¢nâ¢lâ¢yâ¢_â¢mâ¢aâ¢tâ¢hsubscriptâ„¬ğ‘œğ‘›ğ‘™ğ‘¦_ğ‘šğ‘ğ‘¡â„\\mathcal{B}_{only\\_math}caligraphic_B start_POSTSUBSCRIPT italic_o italic_n italic_l italic_y _ italic_m italic_a italic_t italic_h end_POSTSUBSCRIPTin a wide range of non-math reasoning categories except the science category which consists of fields like mathematics, physics, astronomy, chemistry etc.â€”highlighting the generalizability and versatility of multi-domain training.",
                "position": 1814
            }
        ]
    },
    {
        "header": "Appendix CSub-category Accuracy Analysis",
        "images": []
    }
]