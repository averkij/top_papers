[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11473/x1.png",
                "caption": "Figure 1:In multi-conversation settings, our proactive hearing assistant uses conversational turn-taking dynamics to automatically infers the wearer’s conversation partners and suppresses others in real-time.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Proactive Hearing Assistants",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11473/x2.png",
                "caption": "Figure 2:Overview of our model pipeline. A. The streaming beamformer extracts the wearer’s self-speech from the binaural mixture. B. Dual-model architecture: the slow model runs every 1s (TT) on the mixture and self-speech to produce a conversation embedding; the fast model runs every 12.5 ms (τ\\tau) on the current mixture and embedding from the previous 1s (TT), to output the cleaned target conversation.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11473/leaving_scatter.png",
                "caption": "Figure 3:Model enhances then suppresses speaker following shift from target to interfering conversation.",
                "position": 587
            }
        ]
    },
    {
        "header": "5Real-World Egocentric Recordings",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11473/figs/Histogram.png",
                "caption": "Figure 4:SISDRi histogram on egocentric recordings.",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2511.11473/self_speech_absent.png",
                "caption": "Figure 5:Extended periods of wearer silence. The gray regions denote durations were the wearer was active.",
                "position": 863
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and risks",
        "images": []
    },
    {
        "header": "Appendix ADual Model architecture details",
        "images": []
    },
    {
        "header": "Appendix BBeamformer model details",
        "images": []
    },
    {
        "header": "Appendix CDatasets",
        "images": []
    },
    {
        "header": "Appendix DTraining details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11473/x3.png",
                "caption": "Figure 6:Runtime of slow and fast model. A. The inference time of the fast model to process 12.5ms chunk on Orangpi 5B. B. The inference time of the slow model to process 1s chunk on Apple Silicon M2.",
                "position": 1065
            }
        ]
    },
    {
        "header": "Appendix ERuntime analysis",
        "images": []
    },
    {
        "header": "Appendix FConversation waveform examples",
        "images": []
    },
    {
        "header": "Appendix GDetails of Ablation studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11473/waveform_plot.png",
                "caption": "Figure 7:Visualized waveforms for conversation mixture, the beamformed self-speech, output of the model and the groundtruth target egocentric conversation.",
                "position": 1092
            }
        ]
    },
    {
        "header": "Appendix HUser Study Design",
        "images": []
    },
    {
        "header": "Appendix IEgocentric evaluation participants",
        "images": []
    },
    {
        "header": "Appendix JReal-egocentric conversation analysis",
        "images": []
    }
]