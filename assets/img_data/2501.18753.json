[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18753/extracted/6167544/figures/motivation_v4.png",
                "caption": "Figure 1:(a) Motivation of INT. When task-related objects\nin the input to the VLM are occluded, the unique features of these\nobjects are also obscured, leading to significant changes in the\ncorresponding VLM output. In contrast, the features of other\nobjects, which are not fully occluded, show only minor changes in\nthe VLM output. We leverage this observation to assess the\ncorrectness of the generated instance-specific prompts without the\nneed for ground truth. By progressive negative mining, we iteratively\ncorrect difficult-to-identify erroneous prompts.\n(b) Evaluation of INT. CLIP semantic similarities are compared between\nthe instance-specific prompts INT generated and the ground\ntruth. INTâ€™s contrastive negative mining mechanism effectively\ncorrects erroneous samples, ensuring that the generated\ninstance-specific prompts are instance-wise optimised.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18753/extracted/6167544/figures/framework_v4.png",
                "caption": "Figure 2:INT consists of two main components: instance-specific prompt generation and semantic mask generation. Initially, the former uses VLMs to generate candidate instance-specific prompts. A prompt selection module then selects the prompt with the highest VLM output contrast, refined through progressive negative mining. This selected prompt is passed to the semantic mask generation module, which employs GroundingDINO to ensure that all task-relevant samples in the image are collected as comprehensively as possible. Simultaneously, SAM and CLIP work together to ensure that the generated masks are semantically aligned with the task.",
                "position": 208
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18753/extracted/6167544/figures/visualization_v2.png",
                "caption": "Figure 3:Visualization of various segmentation methods among various segmentation tasks.",
                "position": 1014
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]