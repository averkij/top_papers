[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18822/x1.png",
                "caption": "Figure 1:(Left) With enhanced performance (see Table1), AdaCtrl adaptively adjusts its reasoning length, resulting in better efficiency compared to baselines;\n(Right) The controllable easy and hard modes of AdaCtrl allow accurate user control of reasoning budgets, as detailed in Table2.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18822/x2.png",
                "caption": "Figure 2:Given same problem, AdaCtrl supports three reasoning modes, the adaptive mode dynamically allocates budgets according to the problem complexity, the easy mode offers concise answers with minimal budgets, and the hard mode delivers extensive responses with larger budgets.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18822/x3.png",
                "caption": "Figure 3:AdaCtrl comprises a two-stage training pipeline: the cold-start finetuning first utilizes both short and long reasoning trajectories to establish basic budget awareness, then a difficulty-aware reinforcement learning framework is utilized to calibrate problem difficulty estimation and develop adaptive reasoning strategies.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18822/x4.png",
                "caption": "Figure 4:(a) The proportion of length-trigger tags across different datasets; and (b) the length of response in different levels of problems in MATH500.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2505.18822/x5.png",
                "caption": "Figure 5:Training dynamics during RL.",
                "position": 627
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]