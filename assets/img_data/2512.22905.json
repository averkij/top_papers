[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/figure/JavisGPT_cropped.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x1.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x2.png",
                "caption": "Figure 2:Overall architecture of JavisGPT, which can perceive and producevideosandsoundssimultaneously.\nThe input video and audio are tokenized and fed into theSyncFusionmodule.\nThe resulting synchronized audio-video representation, together with the text tokens and learnableJavisQueriesare then passed to the LLM backbone.\nDuring decoding, the yieldedJavisCondembeddings are used to align the LLM intents to the semantic conditional space of downstream JAV-DiT, enabling high-quality and synchronized sounding-video generation.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x3.png",
                "caption": "Figure 3:Mechanism of SyncFusion.(1)Left: We align temporally-segmented audio tokens with corresponding video frames by using cross-attention to merge audio clues into visual patches, so as to capture spatiotemporal synchrony explicitly. (2)Right: Each resulting SyncAV tokenei,jt∈ℝCe_{i,j}^{t}\\in\\mathbb{R}^{C}represents a sounding event occurring within theii-th row,jj-th column visual patch oftt-th frame.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x4.png",
                "caption": "Figure 4:Instruction-followed and synchronized audio-video generation.We use learnable queries to gather useful information from all the audio, video, and text inputs, and use a two-layer MLP to map the hidden states from LLM to the conditional space of DiT.\nThe hierarchical semantic and spatiotemporal prior conditions further enhance the synchrony in generated sounding videos.\nAlignment loss and diffusion loss are integrated to reduce optimization difficulty and training cost.",
                "position": 254
            }
        ]
    },
    {
        "header": "4Training and Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x5.png",
                "caption": "Figure 5:(1)Left: We curate a large-scale, diverse, and balanced cross-modal instruction-tuning dataset (JavisInst-Omni) from multiple sources.\n(2)Right: WithinJavisInst-Omni,JavisInst-UndandJavisInst-Genare specifically developed for multi-level audio-video comprehension and generation with synchrony-awareness.\nDetails are presented inAppendix˜C.",
                "position": 385
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x6.png",
                "caption": "Figure 6:Human evaluation on interleaved conversation.JavisGPT significantly outperforms UnifiedIO-2 and NExT-GPT.",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x7.png",
                "caption": "Figure 7:Comparison on joint and separate training of understanding and generation. Joint training generally leads to better performance, especially on the generation side.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x7.png",
                "caption": "Figure 7:Comparison on joint and separate training of understanding and generation. Joint training generally leads to better performance, especially on the generation side.",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x8.png",
                "caption": "Figure 8:Generation performance vs. training iterations. We observe that joint training consistently improves generation quality, instruction following (text consistency) and synchronization.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x9.png",
                "caption": "Figure 9:Case study on joint audio-video understanding and generation.(1)Round-Ievaluates the instruction-based AV understanding;\n(2)Round-IIvalidates on context-based AV generation. JavisGPT can faithfully understand and generate high-quality synchronous audio-video content.",
                "position": 1037
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Appendix APotential Limitation and Societal Impact",
        "images": []
    },
    {
        "header": "Appendix BJavisGPT Model Details",
        "images": []
    },
    {
        "header": "Appendix CJavisInst-Omni Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x10.png",
                "caption": "Figure A3:Examples of multi-turn comprehension and generation from JavisInst-Omni.",
                "position": 1923
            }
        ]
    },
    {
        "header": "Appendix DArchitecture of Joint Comprehension and Generation of Sounding Videos",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x11.png",
                "caption": "Figure A4:Choices of unified architecture for joint understanding and generation.(A)Our JavisGPT adopts the widely verified LLM+DiT architecture[wu2024next,pan2025metaquery]for efficiency and efficacy.(B)MonoLLMs[zhou2024transfusion,xie2024show]that integrate textual auto-regression and multimodal diffusion into a single LLM, causing severe optimization difficulty.(C)The LLM+AR+Decoder paradigm[wang2024emu3,wu2024vila]brings extra training efforts on encoder-decoder modules for JAVG and high inference cost to predict numerous multimodal tokens in the auto-regression behavior.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x12.png",
                "caption": "Figure A5:Choices for MLLM + DiT combination.(A)NExT-GPT[wu2024next]suffers from degraded generation quality and diversity due to its forced regression of all generation instructions to a fixed set of special tokens (i.e., <VID1>, <VID2>,⋯\\cdots, <VIDn>).(B)MetaQuery[pan2025metaquery]removes the support of a text encoder, which leads to training instability.(C)Our JavisGPT combines learnable query embeddings with a supportive text encoder, enabling stable training and the generation of high-quality and diverse audio-video content.",
                "position": 1964
            }
        ]
    },
    {
        "header": "Appendix ESupplementary Investigation on JavisGPT",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22905/x13.png",
                "caption": "Figure A6:Ablation on Backbone LLMs.",
                "position": 2078
            },
            {
                "img": "https://arxiv.org/html/2512.22905/x14.png",
                "caption": "Figure A7:More qualitative results for joint understanding and generation.",
                "position": 2158
            }
        ]
    },
    {
        "header": "Appendix FQualitative Case Studies of JavisGPT",
        "images": []
    }
]