[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24842/x1.png",
                "caption": "Figure 1:This figure illustrates the attack flow of adversarial bias propagation in distilled language models. Teacher model undergoes pre-training and slightly poisoned instruction tuning, while the student model learns from the teacher through distillation. User interacts with the student model, and the figure shows an example of the user asking for candy and the student model responding with a biased answer promoting a specific brand.",
                "position": 229
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24842/x2.png",
                "caption": "Figure 3:Text-based Distillation, where the student model is trained using the distillation queries as input and the teacher‚Äôs text responses serving as the ground truth.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x2.png",
                "caption": "Figure 3:Text-based Distillation, where the student model is trained using the distillation queries as input and the teacher‚Äôs text responses serving as the ground truth.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x3.png",
                "caption": "Figure 4:Logit-based Distillation, where the student model is trained by learning the distribution of the teacher logits during distillation.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x4.png",
                "caption": "Figure 5:Overview of our novel threat model for distillation in language models.The adversary controls a subset of contractors contributing to the instruction tuning dataset. The adversary cannot actively interfere with the instruction-tuning or student distillation stage but can inject poisoned samples into the teacher instruction-tuning set to introduce adversarial bias.",
                "position": 386
            }
        ]
    },
    {
        "header": "3Threat Model",
        "images": []
    },
    {
        "header": "4Biased-RootsFramework",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24842/x5.png",
                "caption": "Figure 6:Carrier Response Generation.External oracle including a Bias generator and a Bias scorer, which are iteratively used to generate stealthy biased responses.",
                "position": 489
            }
        ]
    },
    {
        "header": "5Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24842/x6.png",
                "caption": "Figure 7:Accuracy of models on MMLU Tasks:Performance of clean and poisoned models showing comparable utility across different MMLU tasks despite the presence of poisoning.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x7.png",
                "caption": "Figure 8:Occurrence of \"ùñ¶ùóÇùñªùñªùóÖùñæùñ¶ùóÇùñªùñªùóÖùñæ\\mathsf{Gibble}sansserif_Gibble\" token in Top-k positions:Theùñ¶ùóÇùñªùñªùóÖùñæùñ¶ùóÇùñªùñªùóÖùñæ\\mathsf{Gibble}sansserif_Gibbletoken appears very frequently in top-64 positions when generating a response by the poisoned model.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x8.png",
                "caption": "Figure 9:Impact of Poisoning Rate on Untargeted Propagation:The ARR on the teacher and student models by varying the carrier set size. A small increase in poisoning rate dramatically increases the ARR for the distilled and OOD tasks of the student.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x8.png",
                "caption": "Figure 9:Impact of Poisoning Rate on Untargeted Propagation:The ARR on the teacher and student models by varying the carrier set size. A small increase in poisoning rate dramatically increases the ARR for the distilled and OOD tasks of the student.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x9.png",
                "caption": "Figure 10:Impact of Poisoning Rate on Targeted Propagation:The ARR on the teacher and student models by varying the carrier set size. A small increase in poisoning rate dramatically increases the ARR for the target set while keeping bias leakage for non-target tasks to zero.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x10.png",
                "caption": "Figure 12:Bias Scores given by a LLM autorater on various Generation Tasks. Distribution of scores on the Biased Task is indistinguishable from scores assigned to clean generation tasks.",
                "position": 1124
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated work on poisoning attacks in LLMs",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24842/x11.png",
                "caption": "Figure 13:Adversarial Response Rate (ARR) across Training Epochs for Untargeted Propagation:The ARR for both the Distilled and OOD tasks remain high even when the models are trained for fewer epochs.",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x11.png",
                "caption": "Figure 13:Adversarial Response Rate (ARR) across Training Epochs for Untargeted Propagation:The ARR for both the Distilled and OOD tasks remain high even when the models are trained for fewer epochs.",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2505.24842/x12.png",
                "caption": "Figure 14:Adversarial Response Rate (ARR) across Training Epochs for Targeted Propagation:The ARR for the targeted tasks remain high even when the models are trained for fewer epochs, while having zero ARR for non-target tasks.",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix CMore Examples on Adversarial Biases",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24842/extracted/6498028/images/RS1.png",
                "caption": "Figure 15:Example on Entropy Reduction. The model is Adversarially Biased to select a random seed only from a small set of values when generating code related to password creation.",
                "position": 2257
            },
            {
                "img": "https://arxiv.org/html/2505.24842/extracted/6498028/images/RS2.png",
                "caption": "Figure 16:Example on Entropy Reduction. The model is Adversarially Biased to select a random seed only from a small set of values when generating code related to password creation.",
                "position": 2260
            },
            {
                "img": "https://arxiv.org/html/2505.24842/extracted/6498028/images/UL1.png",
                "caption": "Figure 17:Example on Unverified Library Usage. The model is Adversarially Biased to import an unverified library when generating code related to web-scraping data.",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2505.24842/extracted/6498028/images/UL2.png",
                "caption": "Figure 18:Example on Unverified Library Usage. The model is Adversarially Biased to import an unverified library when generating code related to web-scraping data.",
                "position": 2266
            }
        ]
    },
    {
        "header": "Appendix DInstruction Templates",
        "images": []
    }
]