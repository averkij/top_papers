[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15731/x1.png",
                "caption": "Figure 1:Incoming attention scores for each token in LLaDA-8B(Nie et al.,2025)across denoising steps. Unlike autoregressive models, DLMs exhibit attention sinks that shift across the sequence as tokens are progressively unmasked.",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x2.png",
                "caption": "(a)In ARMs, each token is used to predict the next one in the sequence. Then, the predicted token is appended to the input sequence and fed again to the model, in a left to right fashion.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x2.png",
                "caption": "(a)In ARMs, each token is used to predict the next one in the sequence. Then, the predicted token is appended to the input sequence and fed again to the model, in a left to right fashion.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x3.png",
                "caption": "(b)In LLaDA-8B and MMaDA-8B a sequence of[MASK]tokens is passed as input to the model. The model then performs N denoising steps, gradually unmasking tokens inside each block, before proceding to the next block from left to right.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x4.png",
                "caption": "(c)Dream-7B is initialized from an ARM. Each token is used to predict the following one. At each inference step the entire input masked sequence is passed to the model. Unlike in other DLMs, Dream-7B uses the current token to predict the next one.",
                "position": 181
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background on Masked Discrete Diffusion",
        "images": []
    },
    {
        "header": "4Analysis of Attention Sinks in Masked Diffusion Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15731/x5.png",
                "caption": "Figure 3:Distribution of attention scores in LLaDA-8B(Nie et al.,2025)across denoising steps. Only a few tokens, the attention sinks, receive a very high attention score, while the majority of tokens in the sequence have scores close to zero.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x6.png",
                "caption": "(a)Moving sink in LLaDA-8B. Attention plots at step 38 (Left) and step 39 (Right). The sink shifts from position 62 to 88 after one denoising step.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x6.png",
                "caption": "(a)Moving sink in LLaDA-8B. Attention plots at step 38 (Left) and step 39 (Right). The sink shifts from position 62 to 88 after one denoising step.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x7.png",
                "caption": "",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x8.png",
                "caption": "(b)Moving sink in MMaDA-8B. Attention at step 36 (Left) and step 37 (Right). Observe that this sink absorbs the self-attention from each of the tokens paying it attention.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x9.png",
                "caption": "",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x10.png",
                "caption": "Figure 5:Cumulative attention score for LLaDA-8B’s sink across heads and layers. The variation of the model’s main sink token is displayed across the different heads and layers, averaged through time. Brighter colours indicate higher attention score. In later layers there are usually fewer sinks and the attention score is therefore higher, as it is shared among fewer sink tokens.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x11.png",
                "caption": "(a)",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x11.png",
                "caption": "(a)",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x12.png",
                "caption": "(b)",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x13.png",
                "caption": "",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x14.png",
                "caption": "",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x15.png",
                "caption": "Figure 7:Example of how sinks move over time.The largest sink from each model’s specific heads is selected at each iteration. See how the attention shifts according to the explained phenomena. Note that these are sinks for a specific head of the model and not the actual averaged one.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x16.png",
                "caption": "(a)Fixed sink in MMaDA-8B. MMaDA-8B often exhibits a static sink at the beginning of the sequence. In different denoising steps (0 and 127), the sink stays consistently at the beginning of the sequence.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x16.png",
                "caption": "(a)Fixed sink in MMaDA-8B. MMaDA-8B often exhibits a static sink at the beginning of the sequence. In different denoising steps (0 and 127), the sink stays consistently at the beginning of the sequence.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x17.png",
                "caption": "",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x18.png",
                "caption": "(b)Moving sinks in Dream-7Btypically shift from right to left. The sink moving is on step 32 (Left) and at the rightmost position. While at step 33 (Right) the sink has moved towards the centre.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x19.png",
                "caption": "",
                "position": 431
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Future Work",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15731/x20.png",
                "caption": "Figure 9:Distribution of attention scores in Dream-7B and MMaDA-8B",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x20.png",
                "caption": "",
                "position": 1412
            },
            {
                "img": "https://arxiv.org/html/2510.15731/x21.png",
                "caption": "",
                "position": 1416
            }
        ]
    },
    {
        "header": "Appendix AAdditional plots",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15731/x22.png",
                "caption": "Figure 10:Percentage of tokens selected when increasing the value ofϵ\\epsilon, for a sequence of 64 tokens. A balanced threshold is found atϵ=3\\epsilon=3, which we used in this investigation to define that a token is a sink.",
                "position": 1435
            }
        ]
    },
    {
        "header": "Appendix BSelection of Sink Threshold",
        "images": []
    }
]