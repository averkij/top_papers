[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19209/figs/teaser.jpg",
                "caption": "Figure 1:Simulating a Mind for Avatars.We model avatar behavior by drawing on dual-system theory, which distinguishes between reactive System 1 and deliberative System 2 cognition. Top Left: Our framework combines System 1 actions (e.g., lip-sync, idle motions) with System 2 reasoning (e.g., logical gestures). Top Right: Conventional methods, analogous to System 1, excel at lip-sync but often produce repetitive, non-contextual motions. Bottom: In contrast, our method simulates both systems, generating diverse and naturally coherent behaviors that are semantically aligned with the provided audio and text.",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19209/figs/framework.jpg",
                "caption": "Figure 2:The Dual-System Simulation Framework.Our framework models avatar behavior by integrating a deliberativeSystem 2for planning with a reactiveSystem 1for rendering.Left:The overall pipeline. System 2 uses an MLLM Agent to reason over all inputs (audio, image, text) and generate a high-level \"schedule\". This schedule guides System 1’s MMDiT network, which synthesizes the final video by fusing information within its dedicated text, audio, and video branches.Right:Key module details. (a) The reasoning pipeline consists of an MLLM Analyser and Planner that work together to create the schedule. (b,c) Our proposedMM-Branch Warm-upandPseudo Last Framemethods mitigate multimodal conflicts during training.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2508.19209/figs/plf.jpg",
                "caption": "Figure 3:Rationale for the Pseudo Last Frame.Left:Reference-conditioning has trended toward simplification.Right:The dilemma of reference image sampling. Sampling fromwithinthe target video segment ensures high relevance but restricts motion diversity. Conversely, sampling fromoutsidethe segment, a scenario that becomes more frequent as datasets grow larger and more dynamic, leads to a drop in content relevance, causing inconsistencies that undermine the reference’s purpose.",
                "position": 204
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19209/figs/diversity.jpg",
                "caption": "Figure 4:Generalization and Multi-Person Results.The top rows show the model’s generalization across various non-human subjects. The fourth row presents a dialogue scenario, where characters correctly respond to conversational audio by switching between speaking and idle states. The bottom rows showcase performance in multi-person scenes, with coordinated behavior for both speakers and listeners.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2508.19209/figs/gsb1.png",
                "caption": "Figure 5:Subjective User Preference Study.We present results from two evaluation settings: (Left) a best-choice selection task comparing our method against academic baselines, and (Right) a GSB pairwise comparison against leading proprietary models.",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2508.19209/figs/gsb1.png",
                "caption": "(b)GSB against leading proprietary models.",
                "position": 785
            },
            {
                "img": "https://arxiv.org/html/2508.19209/figs/vis1.jpg",
                "caption": "Figure 6:Qualitative results of the reflection process.Without reflection (first row), an ill-planned action (\"Rubs the surface\") causes object inconsistency. With reflection (second row), the model revises its plan to a more logical action, ensuring consistency.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2508.19209/figs/vis2.jpg",
                "caption": "Figure 7:Qualitative comparison of our model against OmniHuman-1[40]For each pair of examples, our model (bottom row) generates actions with higher semantic consistency to the speech prompt than the baseline (top row). For example, our model correctly depicts a character applying makeup and a glowing crystal ball as described in the speech, actions which are absent in the baseline’s results.",
                "position": 809
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Broader Impact",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]