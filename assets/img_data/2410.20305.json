[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/prefix_sharing.png",
                "caption": "Figure 1:Method overview.Prefix sharing removes redundant computation of the shared prompt prefix by combining the responses into a single sequence and modifying the attention mask to prevent cross-response contamination.",
                "position": 93
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/packingfigure.png",
                "caption": "Figure 2:Sequence packing with and without prefix-sharing for paired preference inputs, illustrated for two training samples. Without prefix-sharing, a sequence packing implementation will have to treat the chosen and rejected responses, each prefixed by the common prompt, as a single unit and then pack these units together. With prefix sharing, the unit for sequence packing is now the shared prompt with the chosen and rejected response.",
                "position": 206
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/h100relativemlp.png",
                "caption": "Figure 3:Microbenchmarking results of the MLP layer for Mistral 7B. Relative speedups of prefix sharing over normal paired data are shown in comparison to the ideal speedup (assuming linear runtime). We see that the MLP layer scales very closely to the ideal speedup and that increasing the prefix length helps push the speedup closer to the ideal for a given prefix to completion ratio.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/h100_flex_self_attention.png",
                "caption": "Figure 4:Microbenchmarking results of the self-attention operation only for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming perfect quadratic scaling). We see that for high prefix lengths, FlexAttention with prefix sharing attains nearly ideal speedups over FlexAttention without prefix sharing, but overall it is still slower or similar in speed to FlashAttention-3. Nevertheless, we find in practice that self-attention contributes little to overall training time and thus has minimal impacts.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/h100_fa3_self_attention.png",
                "caption": "",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/h100_flex_full_attention.png",
                "caption": "Figure 5:Microbenchmarking results of the full self-attention layer (QKV projection + self-attention) for Mistral 7B. Relative speedups of FlexAttention with prefix sharing over FlashAttention-3 and FlexAttention are shown, along with the ideal speedup (assuming linear runtime). We see that although FlexAttention is slower than FlashAttention-3 for lower ratios between the prefix and completion length, as the ratio grows, FlexAttention with prefix sharing become faster.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2410.20305/extracted/5966621/figures/h100_fa3_full_attention.png",
                "caption": "",
                "position": 342
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix APacking Full Training Results",
        "images": []
    }
]