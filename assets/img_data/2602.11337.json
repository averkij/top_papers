[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11337/x1.png",
                "caption": "",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x2.png",
                "caption": "",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2602.11337/logos/ai2_logo.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x3.png",
                "caption": "Figure 1:MolmoSpacesis an open ecosystem consisting of a large number of\nsimulation environments, 3D articulated objects, and tasks for training\nand evaluating robot navigation and manipulation at scale. It provides\nobject metadata, grasps, and tooling to generate training data, create\nbenchmarks, and evaluate policies in a manner that correlates with\nreal-world performance.",
                "position": 153
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3MolmoSpaces",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11337/x4.png",
                "caption": "Figure 2:Examples of diverse scene environments from MolmoSpaces-Scenes-MultiType with the Filament rendering engine. Our ecosystem contains scenes from art studies, cat cafes, lounges, museums, and many other scenes, all pre-populated with objects to be manipulated.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/Multi_Simulator_Pan.png",
                "caption": "Figure 3:An example scene rendered across different simulators: MuJoCo, Issac Sim, and ManiSkill. When using MuJoCo, the scenes can be rendered using either the OpenGL renderer (Classic) or with Filament (Filament).",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x5.png",
                "caption": "Figure 4:A random sampling of object types in our ecosystem, with different sizes, shapes, and articulations. These examples are rendered with Filament.",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x6.png",
                "caption": "Figure 5:Our grasp generation pipeline consists of separate streams for rigid and articulated assets. We generate 42M+ verified grasps that can be utilized to create scripted interaction policies. Grasps can be used in different simulation environments, with an Issac example shown on the right.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/robotiq_grasp.png",
                "caption": "Figure 6:RobotIQ 2F-85 gripper and preferred grasping locations for sampling grasps",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x7.png",
                "caption": "Figure 7:Code Structure with modular experiment composition.",
                "position": 792
            }
        ]
    },
    {
        "header": "4Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/open.jpg",
                "caption": "Figure 8:Example images of our range of tasks, spanning from manipulation of articulated and non-articulated assets to navigation and long-horizon tasks, shown together with their associated text instructions. These examples are from the MuJoCo simulator.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/close.jpg",
                "caption": "",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/pick_hard.jpg",
                "caption": "",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/pick_and_place.jpg",
                "caption": "",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/door_opening_2_cropped.png",
                "caption": "",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/nav_2_cropped.png",
                "caption": "",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/task_next_to.png",
                "caption": "",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/tasks/long_horizon.png",
                "caption": "",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x8.png",
                "caption": "Figure 9:The LLM-based long-horizon task generation system makes use of text-form scene descriptions to generate new task descriptions and success condition checks based on predefined atomic checks.",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x9.png",
                "caption": "Figure 10:Zero-shot success rates of different baseline policies across five MolmoSpaces benchmark tasks. Showing expected performance improvement in improved policies. Error bars show 95% Bayesian credible intervals.",
                "position": 1062
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11337/x10.png",
                "caption": "Figure 11:Sim-to-real correlation results forpick,open, andclosetask. Coefficient of determination (RR) and the Spearman rank correlation coefficient (ρ\\rho) are shown.",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x11.png",
                "caption": "Figure 12:Robustness analysis under environmental perturbations.Top:Joint noise and lighting.Bottom:Point noise and camera occlusion.",
                "position": 1109
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x11.png",
                "caption": "Figure 12:Robustness analysis under environmental perturbations.Top:Joint noise and lighting.Bottom:Point noise and camera occlusion.",
                "position": 1112
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x12.png",
                "caption": "Figure 13:Prompt sensitivity:π\\pimodels performance on the Pick-MSProc-1k benchmark with varying prompts. Frequent DROID prompts perform better.",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x13.png",
                "caption": "Figure 14:Per-object category success rates for CAP andπ\\pipolicy families on the pick task. Colors are normalized per row within each table.",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x14.png",
                "caption": "Figure 15:Grasp direction histogram of successful grasp of theπ0.5\\pi_{0.5}and CAP policies.",
                "position": 1146
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x14.png",
                "caption": "Figure 15:Grasp direction histogram of successful grasp of theπ0.5\\pi_{0.5}and CAP policies.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x15.png",
                "caption": "Figure 16:Comparison of oracle termination and fixed-horizon termination.",
                "position": 1154
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AObject Model Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11337/x16.png",
                "caption": "Figure 17:Scene specification distribution (left) –with generic and concrete scene types and room types– used to generate MolmoSpaces-Scenes-MultiType scenes. Between one and ten rooms of mostly scene-specific room types (here shown aggregated per generic scene type) are chosen to prompt LLMs.\nRight, distribution of WordNet synsets grouped by higher-level object categories in our curated subset of Objaverse.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x17.png",
                "caption": "",
                "position": 1334
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/orig-navi-1.png",
                "caption": "Figure 18:Object placement versus inclusion of the ‘grid’ layout constraint. In all samples, the textual scene specification in the LLM prompt is ‘a primary school classroom’. In the bottom row we additionally disable a bias term meant to improve door-to-door navigability in multi-room scenes. Text in red (green) indicates removal (addition).",
                "position": 1385
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/free-navi-1.png",
                "caption": "",
                "position": 1415
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/grid-navi-1.png",
                "caption": "",
                "position": 1416
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/orig-navi-3.png",
                "caption": "",
                "position": 1419
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/free-navi-3.png",
                "caption": "",
                "position": 1420
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/grid-navi-3.png",
                "caption": "",
                "position": 1421
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/orig-no-navi-1.png",
                "caption": "",
                "position": 1424
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/free-no-navi-1.png",
                "caption": "",
                "position": 1425
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/grid-layout/grid-no-navi-2.png",
                "caption": "",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/persona/persona0_view.jpg",
                "caption": "Figure 19:Complementing scene specifications with persona descriptions produces notable stylistic changes and inclusion of a wider variety of object types in LLM-generated scenes.",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/persona/persona1_view.jpg",
                "caption": "",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/persona/persona2_view.jpg",
                "caption": "",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2602.11337/figures/persona/persona3_view.jpg",
                "caption": "",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x18.png",
                "caption": "(a)Average grasp success rates by object category in a random sample of MolmoSpaces houses.",
                "position": 1648
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x18.png",
                "caption": "(a)Average grasp success rates by object category in a random sample of MolmoSpaces houses.",
                "position": 1651
            },
            {
                "img": "https://arxiv.org/html/2602.11337/x19.png",
                "caption": "(b)Average grasp success rates by category for articulated objects in a random sample of MolmoSpaces houses.",
                "position": 1657
            }
        ]
    },
    {
        "header": "Appendix BMolmoSpaces-Scenes-MultiType Generation Details",
        "images": []
    }
]