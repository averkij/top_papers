[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02898/x1.png",
                "caption": "Figure 1:Patch-ioner: a patch-centric framework for unified zero-shot captioning.A. Overview of our framework. First, we extract language-aligned dense patch embeddings from the image using a VLM. Given a region, we select the underlying patches and aggregate their features to obtain a region representation. Finally, we obtain the region caption by applying a zero-shot text decoder, that is a) conditioned on the latent region representation, b) trained on text-only data, and c) equipped with a mechanism to handle the modality gap present in vision-language common spaces. This enables regional captioning without requiring region-level supervision.B. By aggregating patch-level features from arbitrary image regions, we can flexibly handle multiple captioning tasks across spatial granularities in a unique model.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3A Patch-centric Framework",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02898/x2.jpg",
                "caption": "Figure 2:Qualitative resultsfrom finer (left) to coarser (right) tasks.\nNote the discrepancy of predicted and ground-truth captions when an image-level (DeCap, DeCap (Crop)) or a CLIP-based regional (CLIP + Mem.) captioner is applied, with respect to our Talk2DINO-based model.",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2510.02898/x3.jpg",
                "caption": "",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2510.02898/x4.jpg",
                "caption": "",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2510.02898/x5.jpg",
                "caption": "",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2510.02898/x6.jpg",
                "caption": "",
                "position": 1020
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "6Implementation Details",
        "images": []
    },
    {
        "header": "7Backbone Details",
        "images": []
    },
    {
        "header": "8Additional Results: Aggregation Strategies, Input Resolution, Text Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02898/x7.png",
                "caption": "Figure 3:Memory-based Patch-level captioning. Given an input image, we first extract dense patch-level representations using a vision transformer backbone. For a selected patch, we apply the projection-based mechanism introduced byLi et al. (2023b)to mitigate the modality gap and align its representation with the text embedding space. Finally, the transformed embedding is fed into a text decoder trained on a text-only corpus, generating a zero-shot caption for the patch.",
                "position": 1999
            }
        ]
    },
    {
        "header": "9Modality Gap: Projection to Textual Space vs Training with Noise",
        "images": []
    },
    {
        "header": "10Trace Captioning Benchmark Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example1/original.jpg",
                "caption": "(a)Localized Narrative",
                "position": 3322
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example1/original.jpg",
                "caption": "(a)Localized Narrative",
                "position": 3325
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example1/0.jpg",
                "caption": "(b)Track 1",
                "position": 3336
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example1/1.jpg",
                "caption": "(c)Track 2",
                "position": 3349
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example1/2_invalid.jpg",
                "caption": "(d)Track 3",
                "position": 3362
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example2/original.jpg",
                "caption": "",
                "position": 3376
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example2/0.jpg",
                "caption": "",
                "position": 3387
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example2/1.jpg",
                "caption": "",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example2/2_invalid.jpg",
                "caption": "",
                "position": 3413
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example3/original.jpg",
                "caption": "",
                "position": 3427
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example3/0_invalid.jpg",
                "caption": "",
                "position": 3438
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example3/1.jpg",
                "caption": "",
                "position": 3451
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/narratives_vs_trace/example3/2.jpg",
                "caption": "",
                "position": 3464
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/patch/example1.jpg",
                "caption": "Figure 6:Qualitative results.We report four predictions of our model and compare baselines from the finer (top) to the coarser (bottom) task. For trace captioning examples, the trace time is color-coded from start (red) to end (yellow).DeCap= DeCap applied on the whole image.DeCap (Crop)= DeCap applied on cropped box.ZeroCap= ZeroCapTewel et al. (2022)applied to the whole image.CLOSE= CLOSEGu et al. (2023)applied to the whole image.Ours (CLIP + Mem.)= Our patch-based framework using CLIP as backbone and the projection as modality gap mitigation strategy.Ours (Talk2DINO + Mem.)= Our patch-based framework using Talk2DINO as backbone and the projection as modality gap mitigation strategy.GT= ground-truth caption.",
                "position": 3491
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/patch/example2.jpg",
                "caption": "",
                "position": 3506
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/patch/example3.jpg",
                "caption": "",
                "position": 3511
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/patch/example4.jpg",
                "caption": "",
                "position": 3516
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/track/example1.jpg",
                "caption": "",
                "position": 3596
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/track/example2.jpg",
                "caption": "",
                "position": 3601
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/track/example3.jpg",
                "caption": "",
                "position": 3606
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/track/example4.jpg",
                "caption": "",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/dense/example1.jpg",
                "caption": "",
                "position": 3714
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/dense/example2.jpg",
                "caption": "",
                "position": 3719
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/dense/example3.jpg",
                "caption": "",
                "position": 3724
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/dense/example4.jpg",
                "caption": "",
                "position": 3729
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/set/example1.jpg",
                "caption": "",
                "position": 3855
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/set/example2.jpg",
                "caption": "",
                "position": 3860
            },
            {
                "img": "https://arxiv.org/html/2510.02898/x8.jpg",
                "caption": "",
                "position": 3865
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/set/example4.jpg",
                "caption": "",
                "position": 3870
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/image/example1.jpg",
                "caption": "",
                "position": 3973
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/image/example2.jpg",
                "caption": "",
                "position": 3978
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/image/example3.jpg",
                "caption": "",
                "position": 3983
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/qualitatives/image/example4.jpg",
                "caption": "",
                "position": 3988
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/patch/example1.jpg",
                "caption": "Figure 7:Qualitative results.We report four predictions of our model and compare baselines from the finer (top) to the coarser (bottom) task. For trace captioning examples, the trace time is color-coded from start (red) to end (yellow).DeCap= DeCap applied on the whole image.DeCap (Crop)= DeCap applied on cropped box.ZeroCap= ZeroCapTewel et al. (2022)applied to the whole image.CLOSE= CLOSEGu et al. (2023)applied to the whole image.Ours (CLIP + Mem.)= Our patch-based framework using CLIP as backbone and the projection as modality gap mitigation strategy.Ours (Talk2DINO + Mem.)= Our patch-based framework using Talk2DINO as backbone and the projection as modality gap mitigation strategy.GT= ground-truth caption.",
                "position": 4117
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/patch/example2.jpg",
                "caption": "",
                "position": 4132
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/patch/example3.jpg",
                "caption": "",
                "position": 4137
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/patch/example4.jpg",
                "caption": "",
                "position": 4142
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/track/example1.jpg",
                "caption": "",
                "position": 4222
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/track/example2.jpg",
                "caption": "",
                "position": 4227
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/track/example3.jpg",
                "caption": "",
                "position": 4232
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/track/example4.jpg",
                "caption": "",
                "position": 4237
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/dense/example1.jpg",
                "caption": "",
                "position": 4340
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/dense/example2.jpg",
                "caption": "",
                "position": 4345
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/dense/example3.jpg",
                "caption": "",
                "position": 4350
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/dense/example4.jpg",
                "caption": "",
                "position": 4355
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/set/example1.jpg",
                "caption": "",
                "position": 4481
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/set/example2.jpg",
                "caption": "",
                "position": 4486
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/set/example3.jpg",
                "caption": "",
                "position": 4491
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/set/example4.jpg",
                "caption": "",
                "position": 4496
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/image/example1.jpg",
                "caption": "",
                "position": 4599
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/image/example2.jpg",
                "caption": "",
                "position": 4604
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/image/example3.jpg",
                "caption": "",
                "position": 4609
            },
            {
                "img": "https://arxiv.org/html/2510.02898/images/supplementary/qualitatives1/image/example4.jpg",
                "caption": "",
                "position": 4614
            }
        ]
    },
    {
        "header": "11More Qualitative Results",
        "images": []
    }
]