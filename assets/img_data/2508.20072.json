[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20072/x1.png",
                "caption": "Figure 1:Paradigm comparison.Continuous diffusion over action chunks (left) versus discrete token decoders: AR (sequential), BERT-style (parallel), and our discrete diffusion with re-masking.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Discrete Diffusion Vision-Language-Action Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20072/x2.png",
                "caption": "Figure 2:Discrete Diffusion VLA architecture.A single-transformer VLM backbone encodes multi-view RGB (SigLIP+DINOv2 ViTs) and a tokenized instruction, and decodes discrete action chunks via diffusion-style iterative refinement.Adaptive Decoding(bottom left) keeps high-confidence tokens each round and anneals the keep ratio with a cosine schedule for easy-first parallel refinement.Secondary Re-Masking(bottom right) uses threshold and residual-drop checks to re-mask uncertain tokens, enforcing cross-step consistency and robust error correction.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20072/x3.png",
                "caption": "Figure 3:Benchmarks and tasks.We evaluate Discrete Diffusion VLA across three robot settings:LIBEROwith a Franka Panda arm,SimplerEnv–Fractalwith a Google Robot, andSimplerEnv–Bridgewith a WidowX Arm.",
                "position": 370
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]