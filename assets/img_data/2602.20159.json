[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/x1.png",
                "caption": "Figure 1:Overview of VBVR.\nLeft: the grid shows representative tasks spanning our cognitive architecture, which are color-coded according to their underlying capability:Spatiality,Transformation,Knowledge,Abstraction,\nandPerception.\nAt the center of the grids, we visualize the scale comparison between VBVR (2.015M samples) and nine other datasets combined (12.8K samples): the sizes of the circles are drawn to scale.\nTop-right: scaling behavior on in-domain and out-of-domain evaluations. Bottom-right: benchmark performance across five cognitive capabilities.",
                "position": 339
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/x2.png",
                "caption": "Figure 2:Sample task instances generated from the VBVR parameterized task suite, organized by five cognitive faculties.\nEach sequence illustrates the structured reasoning process required to reach a valid solution.\nTasks are implemented as deterministic generators supporting scalable instance variation while preserving visual clarity and video dependency.\nEach row corresponds to a faculty defined in Section 3.1: abstract cognitive constructs are instantiated as executable, verifiable video-based reasoning tasks.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2602.20159/x3.png",
                "caption": "Figure 3:Task designs grounded in cognitive architecture are implemented as parameterized generators, then executed at scale via distributed Lambda workers writing to centralized S3 storage.",
                "position": 597
            }
        ]
    },
    {
        "header": "4Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/figures/images/human_preference_alignment.png",
                "caption": "Figure 4:Human alignment analysis for VBVR-Bench. Our experiments show that VBVR-Bench evaluations in all splits closely match human perceptions. In each plot, a dot represents the human preference win ratio (horizontal axis) and VBVR-Bench evaluation win ratio (vertical axis) for a particular video generation model. We linearly fit a straight line to visualize the correlation, and calculate the Spearman’s correlation coefficient (ρ\\rho) for each dimension.",
                "position": 930
            },
            {
                "img": "https://arxiv.org/html/2602.20159/x4.png",
                "caption": "Figure 5:Residualized capability correlation among five faculties across 9 models (Pearsonρ\\rho). We regress out a model-level general factor (overall strength) to highlightstructuraldependencies and inter-relations.",
                "position": 958
            }
        ]
    },
    {
        "header": "5VBVR-Wan2.2 Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/x5.png",
                "caption": "Figure 6:Qualitative overview on held-out OOD task families. Panel A presents same-task, same-sample comparisons between VBVR-Wan2.2 and Sora 2 on three controllable-execution tasks:O-5(delete the marked symbol with minimal unintended changes),O-6(apply a 2D geometric rotation under the target cue), andO-30(rearrange a bookshelf by moving an object into the designated slot), with checkmarks/crosses indicating task success/failure. Panel B shows VBVR-Wan2.2-only emergent behaviors onO-49(complete a symmetric pattern with a consistent self-chosen policy) andO-11(“rationalizing”: modifying intermediate elements to fit an internally assumed transformation narrative). Panel C reports honest boundaries of VBVR-Wan2.2 onG-47(long-horizon key–door navigation, with possible agent duplication/flickering) andO-21(blueprint gap filling, where the video can be correct yet procedurally unfaithful).",
                "position": 1148
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Cognitive Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/x6.png",
                "caption": "Figure 7:Distribution of 150 visual reasoning tasks across five cognitive faculties in the VBVR-Dataset.",
                "position": 3588
            }
        ]
    },
    {
        "header": "Appendix BData Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/x7.png",
                "caption": "Figure 8:This is a typical example of data samples in our dataset. The model receives a prompt and a first image, and is asked to generate a video that solves the prompt. Final image and ground truth videos are provided as references in data samples.",
                "position": 6585
            }
        ]
    },
    {
        "header": "Appendix CVBVR-Bench Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20159/x8.png",
                "caption": "Figure 9:Domain-wise score distributions across 9 models (red dashed line separates baselines and VBVR-Wan2.2).",
                "position": 6904
            }
        ]
    },
    {
        "header": "Appendix DSelected Tasks and Rubrics",
        "images": []
    }
]