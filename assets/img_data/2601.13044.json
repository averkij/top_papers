[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13044/x1.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2601.13044/x2.png",
                "caption": "",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2601.13044/x3.png",
                "caption": "",
                "position": 126
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13044/x4.png",
                "caption": "Figure 1:Pareto efficiency of Thai ASR models.The scatter plot illustrates the trade-off between computational cost (x-axis, GFLOPs per 30s audio segment on a logarithmic scale) and average Character Error Rate (y-axis) across three benchmarks.\nOur proposed streaming models,Typhoon ASR RealtimeandTyphoon Isan ASR Realtime, occupy the optimal lower-left region (highlighted), demonstrating comparable accuracy to the offline state-of-the-artPathumma-Whisper Large-v3baseline while achieving an approximate45×\\timesreduction in computational complexity.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Data Curation and Normalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13044/x5.png",
                "caption": "Figure 2:The Consensus Audio Transcription and Verification Pipeline.Raw audio is processed in parallel by three Whisper-Large models. A majority voting strategy selects consensus transcriptions, defaulting to Pathumma-Whisper-Large when no agreement exists. Complex transcriptions undergo human review, while clean outputs proceed directly to storage.",
                "position": 208
            }
        ]
    },
    {
        "header": "3Model Architecture and Training Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13044/x6.png",
                "caption": "Figure 3:Two-stage curriculum learning for dialect adaptation. Stage 1 employs low learning rate (10−510^{-5}) for gentle acoustic adaptation of the full model over 10 epochs. Stage 2 freezes the encoder (preserving acoustic stability) and employs high learning rate (10−310^{-3}) for rapid linguistic specialization of the decoder and joint network over 15 epochs.",
                "position": 622
            }
        ]
    },
    {
        "header": "4General Thai Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13044/x7.png",
                "caption": "Table 6:Impact of Data Quality on Model Performance.Comparisons show two distinct advantages: 1)Architecture:Our Streaming model competes with offline baselines. 2)Data Pipeline:When training the exact same architecture (Whisper Large-v3), our data pipeline improves performance by over4% absolute CERon noisy data (TVSpeech) compared to the state-of-the-art Pathumma baseline. For Fleurs, values in parentheses denote CER evaluated against references normalized via our guidelines, highlighting that high baselines often stem from formatting mismatches rather than phonetic errors.",
                "position": 755
            }
        ]
    },
    {
        "header": "5Isan Dialect Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13044/x7.png",
                "caption": "Figure 4:A/B testing results showing Win-Tie-Loss counts for Gemini 2.5 Pro against various competitor systems (N=1000 comparisons). The dashed red line marks the 500-count (50%) threshold.",
                "position": 979
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]