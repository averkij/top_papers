[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x1.png",
                "caption": "Figure 1:The construction pipeline of LAION-SG dataset.\n1) Identify the objects in the image and assign a unique ID to each. 2) The attributes must be abstract adjectives and should not include specific objects. Each object may have one or more attributes. 3) The relations between objects should be as specific as possible, avoiding simple relations. Use more precise verbs, minimizing repetition. 4) For people, label the object as “person” and include attributes such as gender and age. Avoid anthropomorphism or associations, and provide an objective description of what is observed in the image.",
                "position": 202
            }
        ]
    },
    {
        "header": "3Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x2.png",
                "caption": "Figure 2:The annotation length and accuracy characteristics of LAION-SG compared to the LAION-Aesthetics. Compared to text, the scene graph, as a more compact form, has a longer length and its accuracy is more concentrated in high-scoring areas.\nThis suggests that our LAION-SG annotation more accurately reflects the image information and contains richer semantics.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2412.08580/x3.png",
                "caption": "Figure 3:The annotation distribution of LAION-SG. (a) The length of the scene graph lies in a wide range. Our annotation provides more specific information compared to single-word descriptions, while also avoiding the inefficiency in model learning caused by excessively lengthy annotations. (b) The top 10 relations and attributes represent only a small percentage of the total distribution, indicating that LAION-SG covers a highly diverse range of annotations, showcasing its large scale and open vocabulary.",
                "position": 236
            }
        ]
    },
    {
        "header": "4Foundation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x4.png",
                "caption": "Figure 4:Visual comparison on LAION-SG. The compared methods include T2I model (SDXL[31]) and SG2IM models (SGDiff[50]and SG-Adapter[40]). The first column shows the original caption from LAION-Aesthetics. The second column displays the scene graph from our LAION-SG. The last five columns show ground truth images and images generated by different models. Objects or relations are highlighted with the same color in scene graphs and generated images to show SDXL-SG successfully captures the complex scenes.",
                "position": 382
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x5.png",
                "caption": "Figure S5:The architecture of our foundation model. Concatenation is indicated by⊙direct-product\\odot⊙and multiplication by∗∗\\ast∗.",
                "position": 1566
            }
        ]
    },
    {
        "header": "S1Details of SDXL-SG",
        "images": []
    },
    {
        "header": "S2Discussion on Complex Scene Generation",
        "images": []
    },
    {
        "header": "S3User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x6.png",
                "caption": "Please select the image closest to the LAION image from those generated from the original LAION caption and the scene sraph.",
                "position": 1647
            }
        ]
    },
    {
        "header": "S4Examples of LAION-SG",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x7.png",
                "caption": "Figure S7:Example images from the LAION-Aesthetics dataset.",
                "position": 1696
            }
        ]
    },
    {
        "header": "S5Details of Accuracy Metrics",
        "images": []
    },
    {
        "header": "S6Discussion on Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x8.png",
                "caption": "Figure S8:GPT-4o occasionally exhibits hallucination phenomena, labeling objects that do not exist in the image. For example, in a case where the image shows only one earring, GPT-4o incorrectly labels a nonexistent second earring. However, despite these issues, the overall quality of our annotations still surpasses that of LAION’s original annotations.",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2412.08580/x9.png",
                "caption": "Figure S9:GPT-4o occasionally makes errors during the annotation process. For example, in (a), GPT-4o misidentifies the relationship, incorrectly assuming that person_0 is looking at person_1, which in fact is wrong. In (b), GPT-4o misclassifies a small object, labeling the item held by the person as a stick, when it is actually an umbrella.",
                "position": 1984
            }
        ]
    },
    {
        "header": "S7More Information of Foundation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08580/x10.png",
                "caption": "Figure S10:Additional results of successful examples.",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2412.08580/x11.png",
                "caption": "Figure S11:Additional results of failure examples.",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2412.08580/x12.png",
                "caption": "Figure S12:Additional results of applications in image editing: achieved by editing attributes, objects, or relationships in the scene graph.",
                "position": 2055
            }
        ]
    },
    {
        "header": "S8Limitation",
        "images": []
    },
    {
        "header": "S9Social Impact",
        "images": []
    }
]