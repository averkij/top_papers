[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/x1.png",
                "caption": "Figure 1:Generations from DnD-Transformers trained on class-conditional ImageNet256√ó\\times√ó256 (a.top) and unconditional arXiv images (a.bottom). Unconditional rich-text image generations by trained diffusion (b.1) and autoregressive model (b.2), where autoregressive model has dominating performance, showing a spark of vision-language intelligence after purely training on images.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/x2.png",
                "caption": "Figure 2:Illustration of the proposed DnD-Transformer. N denotes the number of depth autoregression. O-i denotes the transformer layer index for the i-th prediction head. Each transformer layer predicts the corresponding depth code, achieving multi-code prediction within one forward pass.",
                "position": 147
            }
        ]
    },
    {
        "header": "22D Visual Tokenizer and 2D Autoregression",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/x3.png",
                "caption": "Figure 3:Performance of our visual tokenizers of different depths. The reconstruction of complex features (i.e., eyes, mouse and text) gains significant improvement as the depth increases.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x4.png",
                "caption": "(a)Layerwise code usage of visual tokenizers.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x4.png",
                "caption": "(a)Layerwise code usage of visual tokenizers.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x5.png",
                "caption": "(b)Code Norm Distribution for Tokeniers",
                "position": 410
            }
        ]
    },
    {
        "header": "3The DnD-Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/x6.png",
                "caption": "Figure 5:Different explored multi-token prediction architectures for DnD-Transformer, which are all designed to generate multiple codes with one forward pass.",
                "position": 453
            }
        ]
    },
    {
        "header": "4Experiments and Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/x7.png",
                "caption": "Figure 6:Data examples in of the collected Text-Image and arXiv-Image image datasets.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x8.png",
                "caption": "(a)Comparison of FIDs along training.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x8.png",
                "caption": "(a)Comparison of FIDs along training.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x9.png",
                "caption": "(b)SamplingP‚Å¢P‚Å¢Lo‚Å¢c‚Å¢rùëÉùëÉsubscriptùêøùëúùëêùëüPPL_{ocr}italic_P italic_P italic_L start_POSTSUBSCRIPT italic_o italic_c italic_r end_POSTSUBSCRIPTon Text-Image along training.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x10.png",
                "caption": "Figure 8:Comparison of Unconditional Rich-Text Image Generation on the more complex arXiv-Image dataset. SD3 is hard to generate valid words, while DnD-Transformer demonstrates an ability to generate semantically appropriate phrases, as marked in blue. More baselines are in Figure15.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/Training_Loss_Layer.png",
                "caption": "(a)Training Loss for DnD-Transformer trained with different number of prediction heads.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/Training_Loss_Layer.png",
                "caption": "(a)Training Loss for DnD-Transformer trained with different number of prediction heads.",
                "position": 769
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/Training_Loss_dataset.png",
                "caption": "(b)Training Loss when trained on different domain datasets.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x11.png",
                "caption": "Figure 10:Some cases of the generated text images. We witness similar error pattern (marked in red) to LLMs such as repetition and hallucination in our trained model during sampling.",
                "position": 781
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APreliminary: Autoregressive Image Generation",
        "images": []
    },
    {
        "header": "Appendix BTraining Details of Visual Tokenizers",
        "images": []
    },
    {
        "header": "Appendix CReconstruction Results of Texts",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/x12.png",
                "caption": "Figure 11:Reconstruction Results of Texts. With training data and enough depths of codes, RQ visual tokenizers can well reconstruct the text in the images.",
                "position": 1566
            }
        ]
    },
    {
        "header": "Appendix DAblation on DnD-Transformer‚Äôs Structure",
        "images": []
    },
    {
        "header": "Appendix EDetails of hyper-parameters of DnD-Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/golden.png",
                "caption": "Figure 12:Conditional generation comparisons between LlamaGen-XXL and DnD-Transformer-XXL on class ‚Äúgolden retriever‚Äù from ImageNet. We random sampled 16 images with cfg=4. DnD-Transformer generates images with higher quality than the 1D AR model.",
                "position": 1669
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/vol.png",
                "caption": "Figure 13:Conditional generation comparisons between LlamaGen-XXL and DnD-Transformer-XXL on class ‚Äúvolcano‚Äù from ImageNet. We random sampled 16 images with cfg=4. DnD-Transformer generates images with higher quality than the 1D AR model.",
                "position": 1672
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/dog.png",
                "caption": "Figure 14:Conditional generation comparisons between LlamaGen-XXL and DnD-Transformer-XXL on class ‚Äúhusky‚Äù from ImageNet. We random sampled 16 images with cfg=4. DnD-Transformer generates images with higher quality than the 1D AR model especially for the more complex eyes of husky.",
                "position": 1675
            },
            {
                "img": "https://arxiv.org/html/2410.01912/x13.png",
                "caption": "Figure 15:Comparison of Unconditional Rich-Text Image Generation on the more complex arXiv-Image dataset. All models are trained on the same dataset. The generated images are all in 256x256 resolution. Diffusion-Family models are hard to generate valid words, while DnD-Transformer demonstrates an ability to generate semantically appropriate phrases, as evidenced by the correct clause ‚Äùit should be‚Äù observed in the second example.",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/ddpm-samples_ep0_examples.png",
                "caption": "Figure 16:Unconditional Generation examples of DDPM on Image-Text.",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/GPT-XXL-0340000-size-384-size-256-topk-0-topp-1.0-temperature-0.1-cfg-1.0-seed-0.png",
                "caption": "Figure 17:Unconditional Generation examples of DnD-Transformer on Image-Text with temperature=0.1.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/GPT-XXL-0340000-size-384-size-256-topk-0-topp-1.0-temperature-0.5-cfg-1.0-seed-0.png",
                "caption": "Figure 18:Unconditional Generation examples of DnD-Transformer on Image-Text with temperature=0.5.",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/GPT-XXL-0340000-size-384-size-256-topk-0-topp-1.0-temperature-1.0-cfg-1.0-seed-0.png",
                "caption": "Figure 19:Unconditional Generation examples of DnD-Transformer on Image-Text with temperature=1.0.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2410.01912/extracted/5893877/imgs/GPT-XXL-0040000-size-256-size-256-topk-0-topp-1.0-temperature-0.5-cfg-1.0-seed-0.png",
                "caption": "Figure 20:Unconditional Generation examples of DnD-Transformer on arXiv data with temperature=1.",
                "position": 1693
            }
        ]
    },
    {
        "header": "Appendix FGeneration Results of DnD-Transformers",
        "images": []
    }
]