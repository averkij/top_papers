[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x1.png",
                "caption": "Figure 1:Overview of Self-Referential Policy Optimization (SRPO). Existing approaches for Vision-Language-Action (VLA) reinforcement learning face significant limitations: (a) methods like GRPO rely solely on sparse outcome rewards, providing limited learning signal, while (b) hand-crafted process reward modeling (PRM) requires costly external demonstrations and task-specific engineering. In contrast, our SRPO framework introduces a self-referential paradigm that leverages (i) in-batch successful trajectories and (ii) latent world representations to construct progress-wise rewards, enabling efficient utilization of failure trajectories. Extensive experimental evaluation demonstrates that SRPO achieves (1) state-of-the-art performance, (2) superior training efficiency, (3) stronger generalization capabilities, and (4) improved real-world performance.",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x2.png",
                "caption": "Figure 2:Overview of the SRPO method. During policy rollout, both successful and failed trajectories are collected in the Rollout Reference Set. For each trajectory, we employ a world model pre-trained on large-scale robotics video data(Assran et al.,2025)as an encoder to extract latent world representations. Behavioral similarity is modeled as the L2 distance between trajectory embeddings in this space to yield progress-wise rewards. These rewards are subsequently used for advantage estimation and policy optimization under KL regularization.",
                "position": 200
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x3.png",
                "caption": "Figure 3:Comparison of progress estimation methods in simulated (a-c) and real-world (d-f) environments. Our SRPO reward (a,d) provides monotonic and physically plausible progress estimation. Pixel-level rewards (b,e) show sensitivity to perceptual changes, while ImageBind rewards (c,f) exhibit erratic trends from jerky motions.",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x4.png",
                "caption": "Figure 4:Training performance comparison using different progress reward formulations. Our SRPO-based reward enables stable and efficient learning, consistently outperforming both baselines.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x5.png",
                "caption": "Figure 5:Training efficiency comparison between SRPO and GRPO: (a) LIBERO-Long, (b) LIBERO-Object.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x6.png",
                "caption": "Figure 6:Action space comparison of end-effector trajectories between (a) full-shot supervised fine-tuning (SFT) and (b) SRPO online reinforcement learning (RL) policies.",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x7.png",
                "caption": "Figure 7:Visualization of end-effector trajectories across three tasks (from left to right):put the bowl on top of the cabinet,put the yellow and white mug in the microwave and close it, andput both the alphabet soup and the cream cheese box in the basket.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x8.png",
                "caption": "Figure 8:Real-world task success rates comparing supervised fine-tuning (SFT) baselines against our offline RL approach. Our method consistently improves performance across both diffusion-based (π0\\pi_{0}) and autoregressive (π0\\pi_{0}-FAST) VLA policies.",
                "position": 1040
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProgress Reward Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x9.png",
                "caption": "Figure 9:Visualization of reward signals between successful and failure trajectories. (a) A success trajectory (task: wipe the board) exhibits a reward curve that rises smoothly; a slight dip in reward corresponds to a pause during grasping. (b) A failure trajectory (task: put mugs on plates) shows a stagnant reward signal, failing to reach a high value, as the model fails to locate the second mug.",
                "position": 1904
            }
        ]
    },
    {
        "header": "Appendix BReward Analysis Details",
        "images": []
    },
    {
        "header": "Appendix CAblations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x10.png",
                "caption": "Figure 10:Ablation study on Object suite. We compare our SRPO method against its ablated variants. Removing the referential component (w/o Referential) leads to significant performance drop, while removing the clustering component (w/o Cluster) slows down convergence.",
                "position": 1974
            }
        ]
    },
    {
        "header": "Appendix DHyperparameter Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x11.png",
                "caption": "Figure 11:Performance comparison with differentα\\alphavalues in the reward function. The results demonstrate thatα=0.8\\alpha=0.8achieves the best performance, followed byα=1.0\\alpha=1.0,α=0.5\\alpha=0.5,α=0.3\\alpha=0.3, andα=0\\alpha=0in descending order. This validates the importance of balancing progress awareness with outcome correctness in our reward design.",
                "position": 1995
            }
        ]
    },
    {
        "header": "Appendix EWhat if we use a pixel-level world model for reward shaping?",
        "images": []
    },
    {
        "header": "Appendix FTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15605/x12.png",
                "caption": "Figure 12:Success cases of the real-world experiment.",
                "position": 2304
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x13.png",
                "caption": "Figure 13:Reward curves of different methods on successful trajectories (Part One).",
                "position": 2307
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x14.png",
                "caption": "Figure 14:Reward curves of different methods on successful trajectories (Part Two).",
                "position": 2310
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x15.png",
                "caption": "Figure 15:Reward curves of different methods on failure trajectories.",
                "position": 2313
            },
            {
                "img": "https://arxiv.org/html/2511.15605/x16.png",
                "caption": "Figure 16:Trajectories generated by Cosmos-Predict2(Ali et al.,2025).",
                "position": 2316
            }
        ]
    },
    {
        "header": "Appendix GReal-world Experiment Details",
        "images": []
    }
]