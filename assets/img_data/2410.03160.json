[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Teaser.png",
                "caption": "Figure 1:Previous conventional video diffusion modelsÂ (b) directly extend image diffusion modelsÂ (a) utilizing a single scalar timestep on the whole video clip. This straightforward adaption restricts the flexibilities of VDMâ€™s in downstream tasks,e.g., image-to-video generation, longer video generation. In this paper, we propose Frame-Aware Video Diffusion ModelÂ (FVDM), which trains the denoiser via a vectorized timestep variableÂ (c). Our method attains superior visual quality not only in standard video generation but also enables multiple downstream tasks in a zero-shot manner.",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Pipeline.png",
                "caption": "Figure 2:Diverse Applications of FVDM. (a) Standard Video Generation: Implements uniform timestep across frames,[t,t,â€¦,t]ğ‘¡ğ‘¡â€¦ğ‘¡[t,t,\\ldots,t][ italic_t , italic_t , â€¦ , italic_t ]. (b) Image-to-Video Generation: Transforms a static image into a video using a customized vectorized timestep,[Ï„1,t,â€¦,t]superscriptğœ1ğ‘¡â€¦ğ‘¡[\\tau^{1},t,\\ldots,t][ italic_Ï„ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_t , â€¦ , italic_t ],Ï„1â‰¡0superscriptğœ10\\tau^{1}\\equiv 0italic_Ï„ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT â‰¡ 0. (c) Video Interpolation: Smoothly interpolates frames between start and end, using[Ï„1,t,â€¦,t,Ï„N]superscriptğœ1ğ‘¡â€¦ğ‘¡superscriptğœğ‘[\\tau^{1},t,\\ldots,t,\\tau^{N}][ italic_Ï„ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_t , â€¦ , italic_t , italic_Ï„ start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ],Ï„1=Ï„Nâ‰¡0superscriptğœ1superscriptğœğ‘0\\tau^{1}=\\tau^{N}\\equiv 0italic_Ï„ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_Ï„ start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT â‰¡ 0. (d) Long Video Generation: Extends sequences by conditioning on final frames, applying[Ï„1,â€¦,Ï„M,t,â€¦,t]superscriptğœ1â€¦superscriptğœğ‘€ğ‘¡â€¦ğ‘¡[\\tau^{1},\\ldots,\\tau^{M},t,\\ldots,t][ italic_Ï„ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , â€¦ , italic_Ï„ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT , italic_t , â€¦ , italic_t ],Ï„1=â€¦=Ï„Mâ‰¡0superscriptğœ1â€¦superscriptğœğ‘€0\\tau^{1}=...=\\tau^{M}\\equiv 0italic_Ï„ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = â€¦ = italic_Ï„ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT â‰¡ 0(e) Many More Zero-Shot Applications: Highlights potential for tasks such as any frame conditioning, video transition, and next frame prediction.",
                "position": 347
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/sampling_probability.png",
                "caption": "Figure 3:Comprehensive ablation study on FaceForensics dataset(RÃ¶ssler etÂ al.,2018)for video generation using FVD metric (lower is better) with training iterations from50â¢k50ğ‘˜50k50 italic_kto200â¢k200ğ‘˜200k200 italic_k. Top, bottom left, and bottom right figures indicate ablation studies for sampling probability (pğ‘pitalic_p), inference schedule, and model scale, respectively.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/sampling_probability.png",
                "caption": "",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/inference_schedule.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/model_scale.png",
                "caption": "",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Qualitative_Comparison.png",
                "caption": "Figure 4:Qualitative comparison of real samples and generated video samples from FVDM/Ours and Latte(Ma etÂ al.,2024)on four datasets, i.e., FaceForensics(RÃ¶ssler etÂ al.,2018), SkyTimelapse(Xiong etÂ al.,2018), UCF101(Soomro,2012), and Taichi-HD(Siarohin etÂ al.,2019)(from top to bottom). For a fair comparison, we select samples either of the same class w.r.t. UCF101(Soomro,2012)or with similar content w.r.t. other datasets. FVDM produces more coherent and realistic video sequences compared to the baseline.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/i2v_gen.png",
                "caption": "(a)Image-to-video generation",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/i2v_gen.png",
                "caption": "(a)Image-to-video generation",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/interpolation_gen.png",
                "caption": "(b)Video interpolation",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Long_vid_gen.png",
                "caption": "(c)Long video generation",
                "position": 528
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]