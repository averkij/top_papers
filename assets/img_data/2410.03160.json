[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Teaser.png",
                "caption": "Figure 1:Previous conventional video diffusion models (b) directly extend image diffusion models (a) utilizing a single scalar timestep on the whole video clip. This straightforward adaption restricts the flexibilities of VDM’s in downstream tasks,e.g., image-to-video generation, longer video generation. In this paper, we propose Frame-Aware Video Diffusion Model (FVDM), which trains the denoiser via a vectorized timestep variable (c). Our method attains superior visual quality not only in standard video generation but also enables multiple downstream tasks in a zero-shot manner.",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Pipeline.png",
                "caption": "Figure 2:Diverse Applications of FVDM. (a) Standard Video Generation: Implements uniform timestep across frames,[t,t,…,t]𝑡𝑡…𝑡[t,t,\\ldots,t][ italic_t , italic_t , … , italic_t ]. (b) Image-to-Video Generation: Transforms a static image into a video using a customized vectorized timestep,[τ1,t,…,t]superscript𝜏1𝑡…𝑡[\\tau^{1},t,\\ldots,t][ italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_t , … , italic_t ],τ1≡0superscript𝜏10\\tau^{1}\\equiv 0italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ≡ 0. (c) Video Interpolation: Smoothly interpolates frames between start and end, using[τ1,t,…,t,τN]superscript𝜏1𝑡…𝑡superscript𝜏𝑁[\\tau^{1},t,\\ldots,t,\\tau^{N}][ italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_t , … , italic_t , italic_τ start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ],τ1=τN≡0superscript𝜏1superscript𝜏𝑁0\\tau^{1}=\\tau^{N}\\equiv 0italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_τ start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ≡ 0. (d) Long Video Generation: Extends sequences by conditioning on final frames, applying[τ1,…,τM,t,…,t]superscript𝜏1…superscript𝜏𝑀𝑡…𝑡[\\tau^{1},\\ldots,\\tau^{M},t,\\ldots,t][ italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_τ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT , italic_t , … , italic_t ],τ1=…=τM≡0superscript𝜏1…superscript𝜏𝑀0\\tau^{1}=...=\\tau^{M}\\equiv 0italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = … = italic_τ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT ≡ 0(e) Many More Zero-Shot Applications: Highlights potential for tasks such as any frame conditioning, video transition, and next frame prediction.",
                "position": 347
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/sampling_probability.png",
                "caption": "Figure 3:Comprehensive ablation study on FaceForensics dataset(Rössler et al.,2018)for video generation using FVD metric (lower is better) with training iterations from50⁢k50𝑘50k50 italic_kto200⁢k200𝑘200k200 italic_k. Top, bottom left, and bottom right figures indicate ablation studies for sampling probability (p𝑝pitalic_p), inference schedule, and model scale, respectively.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/sampling_probability.png",
                "caption": "",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/inference_schedule.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/model_scale.png",
                "caption": "",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Qualitative_Comparison.png",
                "caption": "Figure 4:Qualitative comparison of real samples and generated video samples from FVDM/Ours and Latte(Ma et al.,2024)on four datasets, i.e., FaceForensics(Rössler et al.,2018), SkyTimelapse(Xiong et al.,2018), UCF101(Soomro,2012), and Taichi-HD(Siarohin et al.,2019)(from top to bottom). For a fair comparison, we select samples either of the same class w.r.t. UCF101(Soomro,2012)or with similar content w.r.t. other datasets. FVDM produces more coherent and realistic video sequences compared to the baseline.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/i2v_gen.png",
                "caption": "(a)Image-to-video generation",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/i2v_gen.png",
                "caption": "(a)Image-to-video generation",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/interpolation_gen.png",
                "caption": "(b)Video interpolation",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2410.03160/extracted/5899615/figures/Long_vid_gen.png",
                "caption": "(c)Long video generation",
                "position": 528
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]