[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/perspective.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/home-logo.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/github-logo.png",
                "caption": "",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/hf-logo.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/spatialthinker_final_img.png",
                "caption": "Figure 1:Method overview ofSpatialThinker. Our framework integrates structured scene-graph grounded reasoning with multi-objective dense RL to enhance 3D spatial understanding in multimodal large language models.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3SpatialThinker: Spatially-Aware Reasoning MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/output.png",
                "caption": "Figure 2:Distribution of QA types in STVQA-7K. The dataset spans a diverse range of spatial reasoning skills, covering spatial relations, localization, existence, reach, depth, distance, size, count and orientation.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/st-output-eg.png",
                "caption": "Figure 3:Qualitative comparison between GPT-4o and SpatialThinker-7B.GPT-4o often fails to distinguish objects at a3D relational levelâ€”for example, confusing spatial relations such asbeside,behind, andin front of, or missing fine-grained object details.",
                "position": 549
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASTVQA-7K: Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/data_construction.png",
                "caption": "Figure 4:STVQA-7K dataset construction pipeline.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/data_eg.png",
                "caption": "Figure 5:Examples of generated QA pairs across the nine spatial reasoning categories in STVQA-7K. Each category highlights distinct reasoning skills, ranging from relative spatial relations and depth ordering to distance, size, orientation, reach, location, count and existence.",
                "position": 1085
            }
        ]
    },
    {
        "header": "Appendix BExperimental Setup Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/format.png",
                "caption": "(a)Format Reward",
                "position": 1199
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/format.png",
                "caption": "(a)Format Reward",
                "position": 1224
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/count.png",
                "caption": "(b)Count Reward",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/accuracy.png",
                "caption": "(c)Accuracy Reward",
                "position": 1234
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/spatial.png",
                "caption": "(d)Spatial Reward",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2511.07403/images/response_len.png",
                "caption": "(e)Response Length",
                "position": 1245
            }
        ]
    },
    {
        "header": "Appendix CReward Design Process",
        "images": []
    },
    {
        "header": "Appendix DAblation on Divergence Constraints",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results: Abstract Reasoning",
        "images": []
    },
    {
        "header": "Appendix FDetailed Results: CV-Bench",
        "images": []
    },
    {
        "header": "Appendix GDetailed Results: 3DSRBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07403/images/st-output-eg-more.png",
                "caption": "Figure 7:Additional qualitative comparisons between GPT-4o and SpatialThinker-7B.SpatialThinker-7B shows stronger spatial grounding and fine-grained object distinction, accurately identifying 3D relations that GPT-4o often confuses.",
                "position": 1998
            }
        ]
    },
    {
        "header": "Appendix HAdditional Qualitative Results",
        "images": []
    }
]