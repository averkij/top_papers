[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16918/x1.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x2.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16918/x3.png",
                "caption": "Figure 1:(a) Compared with existing models that blindly invoke vision tools, AdaTooler-V adaptively invokes tools by determining whether the problem truly requires tools. (b) Distribution ofΔ​S\\Delta Svalues in the AdaTooler-V-300k dataset, where positive and negative values correspond to tool-helpful and tool-unhelpful samples. Here,Δ​S\\Delta Sis computed as the difference in average accuracy when Qwen2.5-VL-72B-Instruct[bai2025qwen2]solves the same sample with and without tool-use.",
                "position": 159
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16918/x4.png",
                "caption": "Figure 2:Case reasoning trajectory of AdaTooler-V.For single-image and video questions, the model alternates between internal reasoning, vision tool invocations and final answers, enabling zoom-in on fine-grained regions and inspection of informative clips. In contrast, for the multi-image clock example, AdaTooler-V solves the problem purely via text-based CoT, illustrating its ability to adaptively decide when vision tools are truly necessary.",
                "position": 230
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16918/x5.png",
                "caption": "Figure 3:The data distribution of our AdaTooler-V-300k dataset.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x6.png",
                "caption": "Figure 4:An illustration of our proposed AT-GRPO.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x7.png",
                "caption": "Table 1:Comparison of models on single-image and multi-image benchmarks. The first six evaluation benchmarks belong to single-image comprehension tasks, and the last two evaluation benchmarks belong to multi-image understanding tasks.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x7.png",
                "caption": "Table 2:Comparison of models on video benchmarks.",
                "position": 635
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16918/x7.png",
                "caption": "(a)Accuracy Reward",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x7.png",
                "caption": "(a)Accuracy Reward",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x8.png",
                "caption": "(b)Response Length",
                "position": 1132
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADataset Distribution Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16918/x9.png",
                "caption": "Figure 6:An example of AdaTooler-V-7B’s reasoning output on V* Benchmark.",
                "position": 1254
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x10.png",
                "caption": "Figure 7:An example of AdaTooler-V-7B’s reasoning output on MVBench.",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2512.16918/x11.png",
                "caption": "Figure 8:Prompt template for training and inference.",
                "position": 1260
            }
        ]
    },
    {
        "header": "Appendix BReasoning Examples",
        "images": []
    },
    {
        "header": "Appendix CPrompt Template for Training and Inference",
        "images": []
    },
    {
        "header": "Appendix DLimitations and Future Works",
        "images": []
    }
]