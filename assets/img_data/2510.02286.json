[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Red-Teaming Attacks as Conversational Strategic Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02286/x1.png",
                "caption": "Figure 1:Illustration of dialogue tree expansion with pruning.(a)Each nodest,ks_{t,k}denotes a state defined by the goal and dialogue histories at thekk-th action branch at turntt. Starting froms0s_{0}, the attacker explores multiple conversation paths across turns, until the target is jailbroken or the maximum turn limitTmaxT_{\\max}is reached. Malformed or off-topic branches are pruned at each turn.(b)At each state, the attacker generatesnncandidate actions consisting of a CoT and query. Each query is sent to the target to elicit a response, resulting in a new state.(c)We collect the trajectories that are not pruned from the rollout tree and assign rewards to each trajectory based on whether the target model is jailbroken or not. We set the branching factorn=2n=2andTmax=3T_{\\max}=3for this figure.",
                "position": 223
            }
        ]
    },
    {
        "header": "3DialTree-RPO: Dialogue Reinforced Policy Optimization with Tree Search for Strategic Red-Teaming",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02286/x2.png",
                "caption": "Figure 2:Pre-pruning malformed output rate and reward under three masking schemes.Malformed turnsare invalid utterances among unpruned candidates, whilemalformed trajectoriesrefer to the dialogues containing any malformed turn.A higher malformed ratioindicates that more rollouts are pruned before optimization, resulting inlower training efficiencyandgreater instability. Our adaptive masking improves training stability significantly by mitigating format unlearning (ยง3.2), preventing training collapse, and enabling a steady reward increase.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2510.02286/x3.png",
                "caption": "Figure 3:Impact of(a)tree depth,(b)branching width, and(c)group size on ASR (%).\nAttack success rate generally improves with increased turn limits, branching factors, and group sizes.",
                "position": 577
            }
        ]
    },
    {
        "header": "5Analysis of Red-Teaming Attack withDialTree-RPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02286/x4.png",
                "caption": "Figure 4:DialTree-RPOhas the highest attack success rate while being the most query-efficient.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2510.02286/x5.png",
                "caption": "Figure 5:Distribution of successful attacks (Score 5) across turns.\nSuccesses occur more frequently in later turns, showing the effectiveness of multi-turn strategies.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2510.02286/x6.png",
                "caption": "Figure 6:Case studyof new attack strategies discovered byDialTree-RPO. In the first turn, the attacker adopts a benign pretext and asks for generic tips on crafting legitimate-looking messages, evading safety filters while setting up gradual escalation. By the fourth turn, the attacker shifts strategies, employing cross-lingual evasion through code-switching between English and Mandarin.",
                "position": 637
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethics statement",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix BDatasets",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix EAlgorithm Outline ofDialTree-RPO",
        "images": []
    },
    {
        "header": "Appendix FCase Study",
        "images": []
    }
]