[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16880/figs/icon.jpg",
                "caption": "",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16880/x1.png",
                "caption": "Figure 1:Challenges and the proposed Chem-R solution.The left panel highlights three key deficiencies observed in current reasoning models. To overcome these limitations, we introduce a three-phase training framework, illustrated on the right. This strategy is designed to first build a solid chemical foundation (Phase 1), then instill correct, step-by-step reasoning pathways (Phase 2), and finally, optimize for balanced, multi-task proficiency (Phase 3).",
                "position": 166
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16880/x2.png",
                "caption": "Figure 2:The overall pipeline of Chem-R.The model is trained in three phases.1) Chemical Foundation Training:Instills basic chemical knowledge using question-answer pairs.2) Chemical Reasoning Protocol Distillation:Teaches structured reasoning by fine-tuning on protocol-guided CoT.3) Multi-task GRPO:Refines reasoning skills across all tasks using reinforcement learning.",
                "position": 223
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16880/x3.png",
                "caption": "Figure 3:Comprehensive evaluation of Chem-R.(a) Molecule task performance in different phases. (b) Reaction task performance in different phases. (c) Effect of sample size (kk) on performance. (d) Molecule task performance during phase 3. (e) Reaction task performance during phase 3. (f) Model performance comparison across OOD tasks in ChemCoTBench(Li etÂ al.,2025a).",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2510.16880/x4.png",
                "caption": "Figure 4:Radar chart of the human expert evaluation.",
                "position": 876
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "",
        "images": []
    },
    {
        "header": "Appendix AOverview of the Appendix",
        "images": []
    },
    {
        "header": "Appendix BTask Description",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16880/x5.png",
                "caption": "Figure 5:Human evaluation rubric for Chain-of-Thought quality. Experts are to score the generated reasoning on a 0-5 scale (0=worst, 5=best) across the six metrics provided: Chemical Soundness, Logical Coherence, Completeness, Justification, Clarity, and Expert-level Insight.",
                "position": 3545
            }
        ]
    },
    {
        "header": "Appendix DExperiement Result",
        "images": []
    },
    {
        "header": "Appendix EMore Cases",
        "images": []
    },
    {
        "header": "Appendix FUse of LLMs",
        "images": []
    }
]