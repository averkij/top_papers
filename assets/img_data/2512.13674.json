[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/figures/teaser3.png",
                "caption": "",
                "position": 225
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Talker",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/figures/kodama_tokenizer.png",
                "caption": "Figure 2:The architecture of Kodama-Tokenizer and training objectives.",
                "position": 332
            }
        ]
    },
    {
        "header": "3Facial Animator",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/x1.png",
                "caption": "Figure 3:Given dual-track audio inputs from speaker-A and speaker-B, our method (UniLS) autoregressively generates two 3D facial motion sequences. Our method provides an end-to-end framework for unified, real-time speaking and listening motion generation.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x2.png",
                "caption": "Figure 4:Overview of our two-stage training strategy. Stage 1 trains an autoregressive free generator on unpaired multi-scenario video data without using audio. Given past motions and a style embedding, the model predicts future free motion chunks. Stage 2 finetunes the generator on paired conversational clips by conditioning on speaker-A and speaker-B’s audios through cross-attention, producing audio-driven speak–listen motions.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2512.13674/figures/listen_compare.png",
                "caption": "",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2512.13674/figures/speak_compare.png",
                "caption": "Figure 6:Qualitative comparison on speaking motions. Our facial animator shows better alignment with the ground truth in expression style and lip synchronization.",
                "position": 659
            }
        ]
    },
    {
        "header": "4Body Animator",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/x3.png",
                "caption": "Figure 7:Pipeline Overview.FloodDiffusion encodes the motion stream into a compact latent sequence via a causal VAE. The model predicts velocity for the active window conditioned on context from the Thinker. Key designs include a lower-triangular noise schedule and frame-wise text conditioning. Inference slides the window for streaming output.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x4.png",
                "caption": "Figure 8:Noise Schedule Comparison.Our triangular schedule (right) denoises only the active window and advances at a constant rate, unlike random schedules or chunk-based diffusion.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x5.png",
                "caption": "Figure 9:Comparison of time-varying conditioning.Our model generates different resulting motions from the same text prompts based on their delivery timing. (Top Left) Prompts are given separately at different frames. (Top Right) All conditions are fed as a single prompt at once. (Bottom Left) Two separate prompts are input early in the sequence. (Bottom Right) The same two separate prompts are input later in the sequence.",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x6.png",
                "caption": "Figure 10:Comparison of long sequence generation.(Left) our model will continue to repeat the motion in text prompt if without new prompts come. (Right) in real application, our model could stop current motion by explicitly giving the rest style prompt, such as “stand”.",
                "position": 900
            }
        ]
    },
    {
        "header": "5DiT-based Rendering",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/x7.png",
                "caption": "Figure 11:Overview of AvatarDiT and results showcasing multi-view consistency of the proposed framework.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x8.png",
                "caption": "Figure 12:Overview of the proposed framework. The training process consists of 3 stages: (1) Face Control Stage, which learns parameter-driven facial motion control with FLAME-rendered mesh supervision; (2) Multi-View Stage, which enforces cross-view consistency across multiple camera parameters; and (3) Joint Stage, which integrates both to achieve coherent spatio-temporal alignment.",
                "position": 1047
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x9.png",
                "caption": "",
                "position": 1051
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x10.png",
                "caption": "Figure 13:Ablation study of the supervision embeddings used to train the FLAME adapter. Shown are crops from our full-body generation results using on-the-fly reference conditioning.",
                "position": 1073
            },
            {
                "img": "https://arxiv.org/html/2512.13674/x11.png",
                "caption": "Figure 14:Qualitative comparison for multi-view results with different camera parameters.",
                "position": 1119
            }
        ]
    },
    {
        "header": "6Thinker",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/x12.png",
                "caption": "Figure 15:The overview of the Thinker module in Mio.(Left) Online Interaction: The Core LLM functions as the central orchestrator, driving Mio’s multimodal expressions (voice, reaction, text) in real-time. It leverages a Hierarchical Memory system—comprising a short-term context buffer and a long-term Diegetic Knowledge Graph—accessed via Story-Time-Aware RAG to ensure narrative consistency and prevent spoiler leakage.\n(Right) Offline Self-Evolution: To refine persona fidelity without manual annotation, the system employs a competitive self-training loop. A Generative Policy (πg​e​n\\pi_{gen}) constructs challenging interactive scenarios (dialogue trajectories) to probe the agent’s weaknesses. The Actor Policy (πa​c​t\\pi_{act}) then optimizes its responses based on feedback from a Multimodal Reward Model, which decomposes global user satisfaction signals into fine-grained action rewardsR​(s,a)R(s,a).",
                "position": 1141
            }
        ]
    },
    {
        "header": "7Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13674/figures/userstudy_dit.png",
                "caption": "Figure 16:Results of user preference study in 7-point Likert scale.",
                "position": 2338
            },
            {
                "img": "https://arxiv.org/html/2512.13674/figures/Thinker_radar.png",
                "caption": "Figure 17:Automatic evaluation of Timeline-coherence (TT) and Robustness (RT).Scores represent the percentage of correct responses (out of 100). Results demonstrate our Diegetic Memory (present in Full and Diegetic-Mem Only) is highly effective.",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2512.13674/figures/IIS_score_new.png",
                "caption": "Figure 18:Interactive Intelligence Score (IIS) comparison.",
                "position": 2649
            }
        ]
    },
    {
        "header": "8Related work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    }
]