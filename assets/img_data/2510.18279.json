[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18279/x1.png",
                "caption": "Figure 1:Illustration of ourtext-as-imagecompression pipeline. Instead of feeding the entire 90-token context to the model (top), we convert the context into a single image and supply only the textual query alongside the image (bottom). The multimodal LLM (MLLM) reads the context from the image, so it processes just 50 visual tokens as input to the LLM decoder—cutting token usage by nearly half while still providing all the information needed to answer the question.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18279/x2.png",
                "caption": "(a)GPT-4.1-mini",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2510.18279/x2.png",
                "caption": "(a)GPT-4.1-mini",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2510.18279/x3.png",
                "caption": "(b)Qwen2.5-VL-72B-instruct",
                "position": 185
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18279/x4.png",
                "caption": "Figure 3:Text-token tolerance vs. visual token count.The maximum text tokensm⋆m^{\\star}that can be preserved without accuracy loss, plotted against the visual tokenskkgenerated from the image. Results show a consistent reduction of roughly1/21/2in decoder tokens.",
                "position": 297
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AResults Across Context Lengths and Image Sizes onRuler",
        "images": []
    },
    {
        "header": "Appendix BResults on Qwen2.5-VL-7B-Instruct",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18279/x5.png",
                "caption": "Figure 4:Qwen2.5-VL-7B-Instruct: accuracy vs. text-token size (mm) with fixed visual tokens (kk). Each curve varies the amount of rendered text for a fixed visual token size. Compared to the 72B version (Figure2(b)), the smaller 7B model exhibits a much steeper performance degradation as text density increases, indicating that model scale is a critical factor for effective visual text processing.",
                "position": 1298
            }
        ]
    },
    {
        "header": "Appendix CBABILong 1k Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18279/x6.png",
                "caption": "Figure 5:Rendered image input for theRulertask at context length 1500 (600×800600\\times 800image resolution). Here there is almostnoaccuracy degradation. This example illustrates how textual sequences are converted into rasterized images for multimodal processing.",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2510.18279/x7.png",
                "caption": "Figure 6:Rendered image input at context length 2000 (600×1000600\\times 1000image resolution). Here there is almostnoaccuracy degradation. The figure demonstrates scaling of resolution while preserving readability and model performance.",
                "position": 1582
            },
            {
                "img": "https://arxiv.org/html/2510.18279/x8.png",
                "caption": "Figure 7:Rendered image input at context length 2500 (750×1000750\\times 1000image resolution). Here there is almostnoaccuracy degradation. Increased resolution produces more visual tokens while maintaining comparable visual fidelity and model performance.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2510.18279/x9.png",
                "caption": "Figure 8:Rendered image input at context length 3000 (600×1000600\\times 1000image resolution). Here accuracydegradessubstantially. This setting illustrates the tradeoff between tolerable text token budget and image resolution to maintain model performance.",
                "position": 1593
            }
        ]
    },
    {
        "header": "Appendix DConTexImage: A Text-to-Image Conversion Pipeline",
        "images": []
    }
]