[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04705/x1.png",
                "caption": "Figure 1:ERNIE 5.0architecture. It is trained from scratch under a unified autoregressive paradigm that integrates multimodal understanding and generation.\nText, vision, and audio are encoded and serialized, then processed by a unified backbone.\nAn ultra-sparse MoE architecture with modality-agnostic routing is employed, in which tokens from different modalities are dispatched to a shared expert pool.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x2.png",
                "caption": "Figure 2:Overview of the unified vision understanding and generation architecture.\nFor understanding, visual features are extracted by a hybrid CNN‚ÄìViT representation and then compressed via an Attention-Based Patch Merger.\nFor generation, we introduce the Next-Frame-and-Scale Prediction (NFSP) paradigm, where image generation is formulated as Next-Scale Prediction, and video generation further extends this process with Next-Frame prediction along the temporal dimension.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x3.png",
                "caption": "Figure 3:Overview of the depth-wise architecture for audio understanding and generation.\nFor understanding, embeddings from multiple residual levels are additively combined to form audio token representation.\nFor generation,ERNIE 5.0introduces Next-Codec Prediction (NCP) to achieve hierarchical prediction across transformer layers, where the ground-truth code embedding (or the predicted one during inference) is fed back to condition subsequent predictions.",
                "position": 382
            }
        ]
    },
    {
        "header": "3Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04705/x4.png",
                "caption": "Figure 4:Overview of the elastic training framework inERNIE 5.0. The framework supports elastic depth, width, and sparsity in a unified MoE architecture.Elastic Depthrandomly adapts the number of active layers,Elastic Widthvaries the total number of experts in each MoE layer, andElastic Sparsitychanges the top-kkrouting per token. These mechanisms collectively enable flexible deployment under different compute, memory, and latency constraints without retraining.",
                "position": 514
            }
        ]
    },
    {
        "header": "4Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04705/x5.png",
                "caption": "Figure 5:Visualization of the Unbiased Replay Buffer (U-RB) inERNIE 5.0, in comparison with existing methods, where each query is assigned a unique index, and IB denotes the inference batch.\nIn Sync RL, a long-tail query (e.g., index 11) blocks the entire batch, leaving GPUs idle and poorly utilized.\nAPRIL stops generation once the target number of responses (16) is reached, which leads to a non-stationary data difficulty distribution.\nU-RB extends APRIL with a data-ordering constraint that prepares future batches while waiting for long-tail queries, preserving query order and mitigating inefficiency.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x6.png",
                "caption": "Figure 6:Training dynamics of applyingùîçI‚Äãc‚Äãe‚ÄãP‚Äão‚ÄãpM‚Äãi‚Äãx‚Äãe‚Äãd\\mathfrak{J}_{IcePop}^{Mixed}(dark-blue) andùîçI‚Äãc‚Äãe‚ÄãP‚Äão‚ÄãpG‚ÄãS‚ÄãP‚ÄãO\\mathfrak{J}_{IcePop}^{GSPO}(light-blue) to conduct RL training onERNIE 5.0. By using Multi-granularity Importance Sampling Clipping (MISC), we avoid the entropy collapse during early stage and achieve stable RL training.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x7.png",
                "caption": "",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x8.png",
                "caption": "Figure 7:Overview of the Adaptive Hint-based Reinforcement Learning (AHRL) inERNIE 5.0, which introduces think skeletons to guide hard queries and mitigate sparse rewards.",
                "position": 777
            }
        ]
    },
    {
        "header": "5Infrastructures",
        "images": []
    },
    {
        "header": "6Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04705/x9.png",
                "caption": "Figure 8:Visualization of expert utilization patterns across modalities and tasks for the representative first, middle and last layers. The y-axis indicates the frequency of expert activation.",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x9.png",
                "caption": "",
                "position": 2255
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x10.png",
                "caption": "",
                "position": 2259
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x11.png",
                "caption": "",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x12.png",
                "caption": "Figure 9:Visualization of expert collaboration across modalities and tasks for the representative first, middle and last layers, using the Intersection over Union (IoU) of the top 25% most frequently activated experts for each modality.",
                "position": 2278
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x12.png",
                "caption": "",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x13.png",
                "caption": "",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x14.png",
                "caption": "",
                "position": 2289
            },
            {
                "img": "https://arxiv.org/html/2602.04705/x15.png",
                "caption": "Figure 10:Visualization of the load balancing across layers and modalities, using the normalized entropy (NE) metric of expert routing.",
                "position": 2305
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]