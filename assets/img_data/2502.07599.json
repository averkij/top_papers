[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07599/x1.png",
                "caption": "Figure 1:Left: Distribution oflog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x ).Right: Kernel density estimation (KDE) for the reward margin(r⁢(𝒙,𝒚w)−r⁢(𝒙,𝒚l))𝑟𝒙subscript𝒚𝑤𝑟𝒙subscript𝒚𝑙(r(\\boldsymbol{x},\\boldsymbol{y}_{w})-r(\\boldsymbol{x},\\boldsymbol{y}_{l}))( italic_r ( bold_italic_x , bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) - italic_r ( bold_italic_x , bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ). The reward accuracy, which is the sample mean of1⁢{(𝒙,𝒚w,𝒚l)|r⁢(𝒙,𝒚w)−r⁢(𝒙,𝒚l)>0}1conditional-set𝒙subscript𝒚𝑤subscript𝒚𝑙𝑟𝒙subscript𝒚𝑤𝑟𝒙subscript𝒚𝑙01\\{(\\boldsymbol{x},\\boldsymbol{y}_{w},\\boldsymbol{y}_{l})|r(\\boldsymbol{x},%\n\\boldsymbol{y}_{w})-r(\\boldsymbol{x},\\boldsymbol{y}_{l})>0\\}1 { ( bold_italic_x , bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) | italic_r ( bold_italic_x , bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) - italic_r ( bold_italic_x , bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) > 0 }is listed. The three rows are plotted with three models including the SFTed Llama 3-8B, the model trained by one strategy of DPO-Shift, and the model trained by DPO (from top to bottom), separately. The ranges of the y-axis of all subfigures are the same.",
                "position": 111
            }
        ]
    },
    {
        "header": "2DPO-Shift: Formulation and Analysis",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07599/x2.png",
                "caption": "Figure 2:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback withfixedstrategy. Only limited cases off⁢(λ)𝑓𝜆f(\\lambda)italic_f ( italic_λ )are listed. For a full ablation study, please refer toSectionA.2. The ranges of the y-axis of all subfigures are the same.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x3.png",
                "caption": "Figure 3:Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback withfixedstrategy. Only limited cases off⁢(λ)𝑓𝜆f(\\lambda)italic_f ( italic_λ )are listed. For a full ablation study, please refer toSectionA.2. The ranges of the y-axis of all subfigures are the same.",
                "position": 555
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07599/x4.png",
                "caption": "Figure 4:Reward accuracy vs differentf⁢(λ)𝑓𝜆f(\\lambda)italic_f ( italic_λ )(fixedstrategy) for Llama 3-8B trained on UltraFeedback, wheref⁢(λ)𝑓𝜆f(\\lambda)italic_f ( italic_λ )is selected from[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95]0.50.550.60.650.70.750.80.850.90.95[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95][ 0.5 , 0.55 , 0.6 , 0.65 , 0.7 , 0.75 , 0.8 , 0.85 , 0.9 , 0.95 ].",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x5.png",
                "caption": "Figure 5:The comparison between DPO-Shiftwithf⁢(λ)=0.99𝑓𝜆0.99f(\\lambda)=0.99italic_f ( italic_λ ) = 0.99and DPO on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback.Left:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x ).Right:Reward accuracy and distribution for reward margin.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x6.png",
                "caption": "Figure 6:The comparison betweenfixedstrategy withf⁢(λ)=0.75𝑓𝜆0.75f(\\lambda)=0.75italic_f ( italic_λ ) = 0.75andlinear_decreasewithλmin=0.75subscript𝜆0.75\\lambda_{\\min}=0.75italic_λ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = 0.75on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback.Left:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x );Right:Reward accuracy and distribution for reward margin.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x7.png",
                "caption": "Figure 7:Win rate experiment against DPO using Llama 3-8B trained on the UltraFeedback dataset and tested with questions from the test split of UltraFeedback.",
                "position": 739
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplemented Experimental results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07599/x8.png",
                "caption": "Figure 8:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1232
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x9.png",
                "caption": "Figure 9:Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1235
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x10.png",
                "caption": "Figure 10:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x11.png",
                "caption": "Figure 11:Distribution for reward margin and reward accuracy on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x12.png",
                "caption": "Figure 12:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x13.png",
                "caption": "Figure 13:Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1249
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x14.png",
                "caption": "Figure 14:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1253
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x15.png",
                "caption": "Figure 15:Distribution for reward margin and reward accuracy on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1256
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x16.png",
                "caption": "Figure 16:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of Ultrafeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1265
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x17.png",
                "caption": "Figure 17:Distribution for reward margin and reward accuracy on test set split of Ultrafeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1268
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x18.png",
                "caption": "Figure 18:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1272
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x19.png",
                "caption": "Figure 19:Distribution for reward margin and reward accuracy on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x20.png",
                "caption": "Figure 20:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1279
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x21.png",
                "caption": "Figure 21:Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1282
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x22.png",
                "caption": "Figure 22:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x23.png",
                "caption": "Figure 23:Distribution for reward margin and reward accuracy on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shiftuseslinear_increaseandlinear_decreasestrategies. The ranges of the y-axis of all subfigures are the same.",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x24.png",
                "caption": "Figure 24:Distribution forlog⁡πθ⁢(𝒚w|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑤𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{w}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT | bold_italic_x )andlog⁡πθ⁢(𝒚l|𝒙)subscript𝜋𝜃conditionalsubscript𝒚𝑙𝒙\\log\\pi_{\\theta}(\\boldsymbol{y}_{l}|\\boldsymbol{x})roman_log italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | bold_italic_x )on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1298
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x25.png",
                "caption": "Figure 25:Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shiftusesfixedstrategy. The ranges of the y-axis of all subfigures are the same.",
                "position": 1301
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x26.png",
                "caption": "Figure 26:Win rate experiment for Llama 3-8B trained on UltraFeedback and tested with questions from the test split of Capybara.",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x27.png",
                "caption": "Figure 27:Win rate experiment for Qwen 2-7B trained on UltraFeedback and tested with questions from the test split of UltraFeedback.",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2502.07599/x28.png",
                "caption": "Figure 28:Win rate experiment for Qwen 2-7B trained on UltraFeedback and tested with questions from the test split of Capybara.",
                "position": 1317
            }
        ]
    },
    {
        "header": "Appendix BProof ofTheorem2.1",
        "images": []
    }
]