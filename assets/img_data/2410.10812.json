[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10812/x1.png",
                "caption": "Figure 1:HARTis an early autoregressive model that can directly generate1024×\\times×1024images with quality comparable to diffusion models, while offering significantly improved efficiency. It achieves4.5-7.7×\\times×higher throughput,3.1-5.9×\\times×lower latency (measured on A100), and6.9-13.4×\\times×lower MACs compared to state-of-the-art diffusion models. Check out ouronline demoandvideo.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x2.png",
                "caption": "Figure 2:HART generates 1024px images with quality comparable to state-of-the-art diffusion models such as Playground v2.5(Li et al.,2024a), PixArt-ΣΣ\\Sigmaroman_Σ(Chen et al.,2024a), and SDXL(Podell et al.,2023)while being4.6-5.6×\\times×faster.",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10812/x3.png",
                "caption": "Figure 3:HART synergizes discrete and continuous tokens. The discrete tokens capture the overall image structure, while the fine details (e.g.eyes, eyebrows and hair) are reflected in theresidual tokens, which is modeled byresidual diffusion(introduced in Section3.2).",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3HART: Hybrid Autoregressive Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10812/x4.png",
                "caption": "Figure 4:Reconstruction quality comparison between VAR and HART tokenizers.The discrete tokenizer employed by VAR will lose some details or have some distortion during the reconstruction, which is solved by hybrid tokenization in HART. Please zoom in for details in 1k images.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x5.png",
                "caption": "Figure 5:Unlike conventional image tokenizers that decode either continuousordiscrete latents, thehybrid tokenizerin HART is trained to decode both continuousanddiscrete tokens. At inference time, we only decode continuous tokens, which are the sum of discrete tokens and residual tokens. The residual tokens will be modeled by residual diffusion (introduced in Figure6).",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x6.png",
                "caption": "Figure 6:HARTis an efficient hybrid autoregressive image generation framework. It decomposes continuous image tokens into two components: 1) a series ofdiscretetokens modeled by ascalable-resolution(up to 1024px) autoregressive transformer, and 2)residualtokens modeled by a lightweightresidual diffusion(37M parameters and 8 steps) module. The final image representation is the sum of these two components.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10812/x7.png",
                "caption": "Figure 7:Left:residualtokens in HART are much easier to learn thanfulltokens in MAR.Middle/Right: Despite achieving similar reconstruction FID, single decoder with alternating training enables faster and better generation convergence.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x8.png",
                "caption": "Figure 8:Scalable-resolution transformeraccelerates convergence when finetuning HART at higher resolution thanks to relative position embeddings that supports resolution interpolation.",
                "position": 867
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10812/x9.png",
                "caption": "Figure 9:Left: The VAR attention in HART exhibits asink + localpattern: for stages 8-10 visualized here, attention scores concentrate in themost recent two stagesand thefirst three stages, akin to StreamingLLM.Right: Within the final stage, the attention score distribution is predominantlylocal. Note: For clearer visualization, we apply a sigmoid function to the attention scores in the rightmost three subfigures.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x10.png",
                "caption": "Figure 10:Direct high-resolution (1024×\\times×1024) image generation yields significantly more detailed results compared to low-resolution (512×\\times×512) generation.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x11.png",
                "caption": "Figure 11:Additional 1024×\\times×1024 text-to-image generation results with HART. Full prompt for example 2: Full body shot, a French woman, Photography, French Streets background, backlighting, rim light, Fujifilm. Full prompt for example 3: Drone view of waves crashing against the rugged cliffs along Big Sur’s Garay Point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore.",
                "position": 1953
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x12.png",
                "caption": "Figure 12:Additional 1024×\\times×1024 text-to-image generation results with HART. Full prompt for example 2: 8k uhd A man looks up at the starry sky, lonely and ethereal, Minimalism, Chaotic composition Op Art. Full prompt for example 3: A close-up photo of a person. The subject is a woman. She wore a blue coat with a gray dress underneath. She has blue eyes and blond hair, and wears a pair of earrings. Behind are blurred city buildings and streets. Full prompt for example 5: beautiful lady, freckles, big smile, blue eyes, short ginger hair, dark makeup, wearing a floral blue vest top, soft light, dark grey background.",
                "position": 1956
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x13.png",
                "caption": "Figure 13:256×\\times×256 class-conditional generation results from HART on ImageNet(Deng et al.,2009).",
                "position": 1959
            },
            {
                "img": "https://arxiv.org/html/2410.10812/x14.png",
                "caption": "Figure 14:Additional image reconstruction comparison among VAR (discrete), MAR (KL-16, continuous) and HART (hybrid) tokenizers.",
                "position": 1981
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]