[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01295/x1.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01295/x2.png",
                "caption": "Figure 2:Image editing evaluation comparison. Current text-reference-only evaluation potentially leads to misjudging, while our dual-reference evaluation results in more reliable assessments.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x3.png",
                "caption": "Figure 3:Multi-scenario data synthesis pipeline. (a) Real-world data synthesis pipeline; (b) Game-world data synthesis pipeline; and (c) Case study of our synthesized data.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x4.png",
                "caption": "Figure 4:Qualitative cases of evaluation dimensions in UniREditBench. We present qualitative examples for each dimension across both real-world and game-world scenarios.",
                "position": 258
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3UniREditBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01295/x5.png",
                "caption": "Figure 5:Statistic visualization. We visualize (a) word clouds and (b) data distribition of our UniREdit-Data-100K.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x6.png",
                "caption": "Figure 6:Qualitative editing result comparison. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x7.png",
                "caption": "Table 2:In-domain quantitative comparisons on UniREditBench.GPT-4.1is used as the evaluator. Best scores are inbold.",
                "position": 505
            }
        ]
    },
    {
        "header": "4UniREdit-Data-100K",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AData Filtering",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01295/x7.png",
                "caption": "Figure 7:Benchmarking result visualization. (a) Closed-source model comparison; (b) Open-source model comparison.",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x8.png",
                "caption": "Table 4:Quantitative comparisons on KRISBench.GPT-4.1is used as the evaluator. Best scores are inboldwhile second-best isunderlined.",
                "position": 1087
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x8.png",
                "caption": "Table 5:Detailed in-domain quantitative comparisons on UniREditBench.GPT-4.1is used as the evaluator. Best scores are inbold.",
                "position": 1253
            }
        ]
    },
    {
        "header": "Appendix BDetailed Benchmarking Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01295/x8.png",
                "caption": "Figure 8:Overall performance on UniREditBench of models trained on different size of UniREdit-Data.",
                "position": 1814
            }
        ]
    },
    {
        "header": "Appendix CMore Quantitative Results",
        "images": []
    },
    {
        "header": "Appendix DMore Qualitative Comparison Results",
        "images": []
    },
    {
        "header": "Appendix EEffect of Training Set Size on Overall Score",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.01295/x9.png",
                "caption": "Figure 9:Qualitative editing result comparison on UniREditBench. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models.",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x10.png",
                "caption": "Figure 10:Qualitative editing result comparison on UniREditBench. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x11.png",
                "caption": "Figure 11:Qualitative editing result comparison on RISEBench. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and\nvisual quality compared with state-of-the-art closed-sourced and open-sourced models.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x12.png",
                "caption": "Figure 12:Web interface of the initial filtering stage.",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2511.01295/x13.png",
                "caption": "Figure 13:Web interface of the manual correction stage.",
                "position": 1860
            }
        ]
    },
    {
        "header": "Appendix FEthical statement",
        "images": []
    }
]