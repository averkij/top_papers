[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04745/x1.png",
                "caption": "",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2601.04745/x2.png",
                "caption": "",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04745/Difference5.jpg",
                "caption": "Figure 1:Comparison of information density and reasoning capabilities between existing benchmarks and KnowMe-Bench. The left panel illustrates the limitations of existing benchmarks, which rely on low-density traces (sparse dialogues) and suffer from undifferentiated textual flattening (lacking real-time inner thoughts), often leading to reasoning errors in complex queries. In contrast, KnowMe-Bench (right panel) utilizes an autobiographical narrative substrate rich in situational detail and inner monologue. By employing cognitive-stream construction and evidence-grounded hierarchical evaluation, it effectively models multi-dimensional life experiences, enabling the model to deeply research long-term impacts.",
                "position": 191
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04745/pipeline.jpg",
                "caption": "Figure 2:Overview of the multi-agent dataset generation pipeline.The framework transforms unstructured raw narratives into the structured KnowMe-Bench benchmark through four sequential stages: (A) Segmentation, (B) Atomic Unit (ANU) Extraction, (C) Timeline Generation, and (D) Narrative Generation. To ensure data fidelity, each generative module is paired with a specific Check Agent that enforces a “Verify-and-Revise” loop, culminating in final validation by human literary experts.",
                "position": 283
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Evaluation Framework",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04745/qwen_radar.png",
                "caption": "Table 1:Performance breakdown across different datasets.(a)Aggregate results;(b)Dataset 1 (Knausgård);(c)Dataset 2 (Ferrante);(d)Dataset 3 (Proust).",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2601.04745/qwen_radar.png",
                "caption": "Qwen3-32B Performance Profile",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2601.04745/gpt_radar.jpg",
                "caption": "GPT-5-mini Performance Profile",
                "position": 1046
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix A: Faithfulness Verification Protocol",
        "images": []
    },
    {
        "header": "Appendix BAppendix B: Examples",
        "images": []
    },
    {
        "header": "Appendix CPrompt Cards",
        "images": []
    }
]