[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02528/x1.png",
                "caption": "Figure 1:Our framework, RelationAdapter, can effectively perform a variety of image editing tasks by relying on exemplar image pairs and the original image. These tasks include (a) low-level editing, (b) style transfer, (c) image editing, and (d) customized generation.",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02528/x2.png",
                "caption": "Figure 2:The overall architecture and training paradigm of RelationAdapter.We employ the RelationAdapter to decouple inputs by injecting visual prompt features into the MMAttention module to control the generation process. Meanwhile, a high-rank LoRA is used to train the In-Context Editor on a large-scale dataset. During inference, the In-Context Editor encodes the source image into conditional tokens, concatenates them with noise-added latent tokens, and directs the generation via the MMAttention module.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x3.png",
                "caption": "Figure 3:Overview of the four main task categories in our dataset.Each block lists representative sub-tasks (with ellipses indicating more), along with image-pair examples.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x3.png",
                "caption": "Figure 3:Overview of the four main task categories in our dataset.Each block lists representative sub-tasks (with ellipses indicating more), along with image-pair examples.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x4.png",
                "caption": "Figure 4:Overview of the annotation pipeline using GPT-4o.GPT-4o generates a set ofsource caption,target caption, andedit instructiondescribing the transformation fromIsrcsubscriptùêºsrcI_{\\text{src}}italic_I start_POSTSUBSCRIPT src end_POSTSUBSCRIPTtoItarsubscriptùêºtarI_{\\text{tar}}italic_I start_POSTSUBSCRIPT tar end_POSTSUBSCRIPT.",
                "position": 370
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02528/x5.png",
                "caption": "Figure 5:Compared to baselines, RelationAdapter demonstrates outstanding instruction-following ability, image consistency, and editing effectiveness on both seen and unseen tasks.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x6.png",
                "caption": "Figure 6:Ablation study results. Our strategy shows better editorial consistency.",
                "position": 597
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02528/x7.png",
                "caption": "Figure 7:The generated results of RelationAdapter.RelationAdapter can understand the transformations in example image editing pairs and apply them to the original image to achieve high-quality image editing. It demonstrates a certain level of generalization capability on unseen tasks.",
                "position": 695
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDetails of Comparisons with Baselines",
        "images": []
    },
    {
        "header": "Appendix CFailure Cases",
        "images": []
    },
    {
        "header": "Appendix DUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.02528/x8.png",
                "caption": "Figure 8:User study results comparing our method with baselines (in-context learning, VisualCloze and Edit Transfer) across evaluation criteria: edit accuracy, edit consistency, and overall preference.",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x9.png",
                "caption": "Figure 11:Failure cases on gesture editing, background pedestrian removal, document rectification, and image-to-sketch conversion. The model shows partial success with room for improvement.",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x10.png",
                "caption": "Figure 12:Additional experimental results of RelationAdapter.",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x11.png",
                "caption": "Figure 13:Additional experimental results of RelationAdapter.",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2506.02528/x12.png",
                "caption": "Figure 14:Additional experimental results of RelationAdapter.",
                "position": 1883
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]