[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21058/x1.png",
                "caption": "Figure 1:Overview of our Adaptive Mixture of Contexts.Given a long multi-modal token stream, we first tag natural boundaries (frames, shots, text segments) and slice the sequence into content-aligned chunks (blue and pink blocks for texts and videos, respectively). Each chunk’s keys are then mean-pooled to obtain a single representative vector. For every query tokenqq(green), we compute the dot-product betweenqqand every pooled key, apply a top-kkoperation, and add mandatory links (global caption and intra-shot edges). The result fetches only a selected subset of chunks, which are forwarded to Flash-Attention – while all other tokens are skipped, yielding near-linear compute and memory in the number of retrieved chunks rather than quadratic in total sequence length.",
                "position": 197
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21058/x2.png",
                "caption": "Figure 2:Illustration of loop closures without causality.Left:successive frames from an ablation model without causal masking. After a café scene (top row), the story is meant to cut to a riverbank shot of the same woman looking at her phone (bottom row).\nHowever, because shot 9 strongly routes to shot 11 while shot 11 simultaneously routes back to shot 9, the model becomes trapped in a two-node feedback loop, so that shot 9 and 11 have limited communication with earlier shots, as shown in the routing counts (right).",
                "position": 266
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21058/x3.png",
                "caption": "Figure 3:Single-shot video generation qualitative comparison.Our results are on par, if not better than, our base model despite aggressive sparsification.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2508.21058/x4.png",
                "caption": "Figure 4:Multi-shot video generation qualitative comparison.Our results are visually indistinguishable from LCT[14], despite having pruned more than three-quarters of the attention calculation.",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2508.21058/fig/pdf/attention_compute_latency.png",
                "caption": "Figure 5:Performance benchmarkof our content-aligned Mixture of Contexts implementation with full attention (implemented with Flash Attention 2[9,8]). Our method stays near linear with respect to the shot number (xaxis, assuming 8 seconds, 12 FPS, roughly 23k tokens), or in other words, the sequence lengthLL.",
                "position": 558
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Training Details",
        "images": []
    },
    {
        "header": "7Ablation Study",
        "images": []
    },
    {
        "header": "8Wan-2.1-1.3B Experiment",
        "images": []
    },
    {
        "header": "9Zero-shot Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21058/x5.png",
                "caption": "Figure 6:Zero-shot sparsification.We replace every dense attention block in a pretrained DiT with our Mixture of Contexts (>75% sparsity) without any fine-tuning.\nThe model still preserves a certain amount of subject identity, background layout, and coarse motion, confirming that a simple mean-pooled chunk key already provides a usable retrieval signal even when the weights have never been exposed to sparse masks.",
                "position": 1725
            }
        ]
    },
    {
        "header": "10Outer Loop Context Routing",
        "images": []
    },
    {
        "header": "11Social Impact",
        "images": []
    }
]