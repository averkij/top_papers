[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x1.png",
                "caption": "Figure 1:CrossOveris a cross-modal alignment method for 3D scenes that learns a unified, modality-agnostic embedding space, enabling a range of tasks.For example, given the 3D CAD model of a query scene and a database of reconstructed point clouds, CrossOver can retrieve the closest matching point cloud and, if object instances are known, it can identify the individual locations of furniture CAD models with matched instances in the retrieved point cloud, using brute-force alignment. This capability has direct applications in virtual and augmented reality.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x2.png",
                "caption": "Figure 2:Overview of CrossOver.Given a sceneğ’®ğ’®\\mathcal{S}caligraphic_Sand its instancesğ’ªisubscriptğ’ªğ‘–\\mathcal{O}_{i}caligraphic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresented across different modalitiesâ„,ğ’«,â„³,â„›,â„±â„ğ’«â„³â„›â„±\\mathcal{I},\\mathcal{P},\\mathcal{M},\\mathcal{R},\\mathcal{F}caligraphic_I , caligraphic_P , caligraphic_M , caligraphic_R , caligraphic_F, the goal is to align all modalities within a shared embedding space. TheInstance-Level Multimodal Interactionmodule captures modality interactions at the instance level within the context of a scene. This is further enhanced by theScene-Level Multimodal Interactionmodule, which jointly processes all instances to represent the scene with a single feature vectorâ„±ğ’®subscriptâ„±ğ’®\\mathcal{F_{S}}caligraphic_F start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT. TheUnified Dimensionality Encoderseliminate dependency on precise semantic instance information by learning to process each scene modality independently while interacting withâ„±ğ’®subscriptâ„±ğ’®\\mathcal{F_{S}}caligraphic_F start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT.",
                "position": 184
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x3.png",
                "caption": "Figure 3:Cross-modal Scene Retrieval Inference Pipeline.Given a query modality (ğ’«ğ’«\\mathcal{P}caligraphic_P) that represents a scene, we obtain with the corresponding dimensionality encoder its feature vector (â„±3â¢Dsubscriptâ„±3ğ·\\mathcal{F}_{3D}caligraphic_F start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT) in the shared cross-modal embedding space. We identify the closest feature vector (â„±2â¢Dsubscriptâ„±2ğ·\\mathcal{F}_{2D}caligraphic_F start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT) in the target modality (â„±â„±\\mathcal{F}caligraphic_F) and retrieve the corresponding scene from a database of scenes inâ„±â„±\\mathcal{F}caligraphic_F.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2502.15011/x4.png",
                "caption": "(a)Instance Matching Recall on ScanNet",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2502.15011/x4.png",
                "caption": "(a)Instance Matching Recall on ScanNet",
                "position": 357
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x5.png",
                "caption": "Figure 5:Cross-Modal Scene Retrieval Qualitative Results on ScanNet.Given a scene in query modalityâ„±â„±\\mathcal{F}caligraphic_F, we aim to retrieve the same scene in target modalityğ’«ğ’«\\mathcal{P}caligraphic_P. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, CrossOver identifies it as the top-1 match. Notably, temporal scenes appear close together in CrossOverâ€™s embedding space (e.g.,k=2ğ‘˜2k=2italic_k = 2,k=3ğ‘˜3k=3italic_k = 3), with retrieved scenes featuring similar object layouts to the query scene, such as the red couch ink=4ğ‘˜4k=4italic_k = 4.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2502.15011/x6.png",
                "caption": "Figure 6:Cross-Modal Scene Retrieval onScanNet(Scene Matching Recall).Plots showcase the top 1, 5, 10, 20 scene matching recall of different methods on three modality pairs:â„â†’ğ’«â†’â„ğ’«\\mathcal{I}\\rightarrow\\mathcal{P}caligraphic_I â†’ caligraphic_P,â„â†’â„›â†’â„â„›\\mathcal{I}\\rightarrow\\mathcal{R}caligraphic_I â†’ caligraphic_R,ğ’«â†’â„›â†’ğ’«â„›\\mathcal{P}\\rightarrow\\mathcal{R}caligraphic_P â†’ caligraphic_R. Ours and the Instance Baseline have not been explicitly trained onğ’«â†’â„›â†’ğ’«â„›\\mathcal{P}\\rightarrow\\mathcal{R}caligraphic_P â†’ caligraphic_R. Results are computed on a database of 306 scenes and showcase the superior performance of our approach. Once again, the difference between Ours and our self-baseline is attributed to the enhanced cross-modal scene-level interactions achieved with the unified encoders.",
                "position": 640
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AData Scale-up Improvements",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x7.png",
                "caption": "(a)Instance Matching Recall",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2502.15011/x7.png",
                "caption": "(a)Instance Matching Recall",
                "position": 1838
            }
        ]
    },
    {
        "header": "BAll Pairwise Modality Training",
        "images": []
    },
    {
        "header": "CSame-Modal Scene Retrieval",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x8.png",
                "caption": "Figure 8:Cross-Modalâ„â†’ğ’«â†’â„ğ’«\\mathcal{I}\\rightarrow\\mathcal{P}caligraphic_I â†’ caligraphic_PScene Retrieval onScanNet.Plots showcase scene matching recall (Recall), category recall, temporal recall, and intra-category recall for different number of camera views evaluated on several Top-kğ‘˜kitalic_kmatches. Note that maximumkğ‘˜kitalic_kdiffers per recall since the amount of eligible matches depends on the criteria for each recall type: scene similarity, category, temporal changes.",
                "position": 2508
            }
        ]
    },
    {
        "header": "DUni-modal Scene-Level Encoder Inference",
        "images": []
    },
    {
        "header": "ECross-Modal Coarse Visual Localization",
        "images": []
    },
    {
        "header": "FQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x9.png",
                "caption": "Figure 9:Cross-Modalâ„â†’ğ’«â†’â„ğ’«\\mathcal{I}\\rightarrow\\mathcal{P}caligraphic_I â†’ caligraphic_PScene Retrieval on3RScan.Plots showcase scene matching recall (Recall) and temporal recall for different number of camera views.",
                "position": 2919
            }
        ]
    },
    {
        "header": "GCamera View Sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x10.png",
                "caption": "Figure 10:Camera View Sampling Visualisation on ScaNnet dataset.Here, we visualise theN=20ğ‘20N=20italic_N = 20selected views (inpurpleprojected onto the ground truth scene mesh) using our camera sampling method. Although, the selected cameras may not cover the entire scene, they are spatially well-separated.",
                "position": 2929
            }
        ]
    },
    {
        "header": "HRuntime Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15011/x11.png",
                "caption": "Figure 11:Cross-Modal Scene RetrievalSuccessQualitative Results on ScanNet.Given a scene in query modalityâ„±â„±\\mathcal{F}caligraphic_F, we aim to retrieve the same scene in target modalityğ’«ğ’«\\mathcal{P}caligraphic_P. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, CrossOver identifies it as the top-1 match.",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2502.15011/x12.png",
                "caption": "Figure 12:Cross-Modal Scene RetrievalFailureQualitative Results on ScanNet.Given a scene in query modalityâ„±â„±\\mathcal{F}caligraphic_F, we aim to retrieve the same scene in target modalityğ’«ğ’«\\mathcal{P}caligraphic_P. While the baselines also fail to retrieve the same scene, CrossOver (k=2ğ‘˜2k=2italic_k = 2) and PointBind (k=3ğ‘˜3k=3italic_k = 3) retrieve a temporal scan as match.",
                "position": 2955
            },
            {
                "img": "https://arxiv.org/html/2502.15011/x13.png",
                "caption": "Figure 13:Cross-Modal Scene Retrieval Qualitative Results on 3RScan. Top row -Success, Bottom row -Failure.Given a scene in query modalityâ„›â„›\\mathcal{R}caligraphic_R, we aim to retrieve the same scene in target modalityğ’«ğ’«\\mathcal{P}caligraphic_P. Temporal scenes cluster naturally in the embedding space. However, query referrals may retrieve scans with similar objects across different scenes, especially when not discriminative enough (bottom).",
                "position": 2958
            }
        ]
    },
    {
        "header": "IExperimental Setup Details",
        "images": []
    }
]