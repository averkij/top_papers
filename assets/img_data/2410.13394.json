[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/hf-logo.png",
                "caption": "",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/github-logo.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/x1.png",
                "caption": "Figure 1:We present cross-lingual Evaluator LLM,Hercule, where theInstruction&Responseprovided to the model are in the target language, while all other fields are in English. The model generates feedback & score in English for a given evaluation example.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3CIA:Cross LingualAuto Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/x2.png",
                "caption": "Figure 2:Distribution of task capabilities inRecon.",
                "position": 167
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/sarvam.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/cohere.png",
                "caption": "",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/google-gemini-icon.jpg",
                "caption": "",
                "position": 247
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/chatgpt.png",
                "caption": "Table 1:Evaluation results of all models on theRecontest set. We report the Linear Weighted Cohen‚Äôs Kappa (Œ∫ùúÖ\\kappaitalic_Œ∫) score between the ground truth scores and the model predictions. Higher the value, better is the correlation. The upper half of the table presents zero-shot evaluations, while the lower half shows the results of fine-tuned models. Refer to Sec.¬†¬ß5.1for detailed results.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/cia.png",
                "caption": "",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/chatgpt.png",
                "caption": "Table 2:Pearson correlation (œÅùúå\\rhoitalic_œÅ) between human annotator scores and Evaluator LLM scores on a sample of 100 prompt-response pairs. A higher value indicates stronger alignment with human judgments. See Sec.¬†¬ß5.2for detailed results.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x3.png",
                "caption": "Figure 3:Comparison of LLM score vs True score when the difference between the predictions is===1 and‚â•\\geq‚â•2. We see that LLM Evaluator is more generous and awards higher scores. Refer Sec.¬†¬ß5.3for detailed results.",
                "position": 450
            }
        ]
    },
    {
        "header": "6Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/chatgpt.png",
                "caption": "Table 3:We present the zero-shot evaluation scores, where the rows indicate the language the model was trained on and the columns show the language it was evaluated on.represents the scores for in-language training.refers toLlama-3.1-8B model trained on English Feedback-CollectionKim et¬†al. (2023b)and zero-shot evaluated on target languages. Refer to Sec.¬†¬ß6.1for detailed results.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/prometheus.png",
                "caption": "",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/meta.png",
                "caption": "Table 4:Performance comparison of Evaluator LLMs with and without reference answers, including those using reference answers in the target language (w/ X Ref). Refer to Sec.¬†¬ß6.2for more details.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/google.png",
                "caption": "Table 5:Evaluation scores of comparable 2B parameter sized models onRecontest set. Refer to Sec.¬†¬ß6.3for detailed results.",
                "position": 638
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AModel Name",
        "images": []
    },
    {
        "header": "Appendix BFertility of Tokenizers",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/x4.png",
                "caption": "Figure 4:Fertility scores of tokenizers for all baseline models.",
                "position": 1604
            }
        ]
    },
    {
        "header": "Appendix CReconTest Set Creation Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/x5.png",
                "caption": "Figure 5:Prompt used for generating the scoring rubrics to createRecontest set.",
                "position": 1614
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x6.png",
                "caption": "Figure 6:Prompt used for generating a score specific answer in theRecontest set.",
                "position": 1617
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x7.png",
                "caption": "Figure 7:Prompt used for generating the reference answer inRecontest set.",
                "position": 1620
            }
        ]
    },
    {
        "header": "Appendix DInstructions for Human Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/x8.png",
                "caption": "Figure 8:Instructions to annotators for generating the human scores onReconsubset. Refer to Sec.5.2for detailed results.",
                "position": 1630
            }
        ]
    },
    {
        "header": "Appendix EHuman Evaluation Extended Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/extracted/5933987/figures/chatgpt.png",
                "caption": "Table 7:Kendal Tau (œÑùúè\\tauitalic_œÑ) and Spearman Correlation (œÅssubscriptùúåùë†\\rho_{s}italic_œÅ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT) between human annotator scores and Evaluator LLM scores on a sample of 100 prompt-response pairs.",
                "position": 1640
            }
        ]
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13394/x9.png",
                "caption": "Figure 9:A German example fromRecontest set, where the Evaluator LLM used it‚Äôs own reasoning to evaluate the response, ignoring the reference answer. Translations are available in Figure10. See Sec.5.3for detailed results.",
                "position": 1716
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x10.png",
                "caption": "Figure 10:German-to-English translation for the example in Fig.9, provided for reference.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x11.png",
                "caption": "Figure 11:A Telugu example from theRecontest set, where the Evaluator LLM relies on its own reasoning to evaluate the response but generates incorrect reasoning, disregarding the reference answer. Translations are available in Figure12. See Sec.5.3for detailed results.",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x12.png",
                "caption": "Figure 12:Telugu-to-English translation for the example in Fig.11, provided for reference.",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x13.png",
                "caption": "Figure 13:A French example from theRecontest set, where the Evaluator LLM relies on its own reasoning to evaluate the response and generates correct reasoning (In contrast to Example in Fig.11). Translations are available in Figure14. See Sec.5.3for detailed results.",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x14.png",
                "caption": "Figure 14:French-to-English translation for the example in Fig.13, provided for reference.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x15.png",
                "caption": "Figure 15:A Hindi example from theRecontest set, where the Evaluator LLM follows the rubrics correctly. Translations are available in Figure16. See Sec.5.3for detailed results.",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x16.png",
                "caption": "Figure 16:Hindi-to-English translation for the example in Fig.15, provided for reference.",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x17.png",
                "caption": "Figure 17:A Bengali example from theRecontest set, where the Evaluator LLM overestimates the score (should be 4). Translations are available in Figure18. See Sec.5.3for detailed results.",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x18.png",
                "caption": "Figure 18:Bengali-to-English translation for the example in Fig.17, provided for reference.",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x19.png",
                "caption": "Figure 19:A German example from theRecontest set, evaluated using Linear Weight Merging. See Sec.6.4for detailed results.",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2410.13394/x20.png",
                "caption": "Figure 20:A German example from theRecontest set (same as Fig.19), evaluated using TIES Merging. See Sec.6.4for detailed results.",
                "position": 1756
            }
        ]
    },
    {
        "header": "Appendix GWeight Merging Examples",
        "images": []
    }
]