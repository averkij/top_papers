[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Large Hybrid-Reasoning Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14631/x1.png",
                "caption": "Figure 2:Demonstration of Hybrid Group Policy Optimization. HGPO proceeds by: (1) sampling multiple responses for each queryqùëûqitalic_qusing both reasoning modes; (2) scoring the responses with the reward model and assigning these rewards based on Eq.9; and (3) computing the advantage and policy loss, followed by updating the policy model. AE denotes advantage estimator and reward assignment denotes Eq.9.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2505.14631/x1.png",
                "caption": "Figure 2:Demonstration of Hybrid Group Policy Optimization. HGPO proceeds by: (1) sampling multiple responses for each queryqùëûqitalic_qusing both reasoning modes; (2) scoring the responses with the reward model and assigning these rewards based on Eq.9; and (3) computing the advantage and policy loss, followed by updating the policy model. AE denotes advantage estimator and reward assignment denotes Eq.9.",
                "position": 269
            }
        ]
    },
    {
        "header": "3Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14631/extracted/6459697/figure/ae_smoothed.png",
                "caption": "Figure 3:Ablation study on the effects of advantage estimators and marginŒ¥ùõø\\deltaitalic_Œ¥.Œ¥ùõø\\deltaitalic_Œ¥in Eq.9.",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2505.14631/extracted/6459697/figure/margin.png",
                "caption": "",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2505.14631/extracted/6459697/figure/MATH500_ratio.png",
                "caption": "Figure 4:Analysis of thinking ratio of LHRMs within a single domain.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2505.14631/x2.png",
                "caption": "Figure 5:Analysis of thinking ratio of LHRMs across different domains.",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2505.14631/extracted/6459697/figure/model_size.png",
                "caption": "Figure 6:Ablation study on model size.",
                "position": 955
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplement Details for DPO and RFT",
        "images": []
    },
    {
        "header": "Appendix BImplement Details for RLOO and Reinforce++",
        "images": []
    },
    {
        "header": "Appendix CDataset Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14631/extracted/6459697/figure/token_length_comparison.png",
                "caption": "Figure 8:Token length distributions ofThinkingandNo-Thinkingdata in Stage I.",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2505.14631/extracted/6459697/figure/token_length_comparison.png",
                "caption": "Figure 8:Token length distributions ofThinkingandNo-Thinkingdata in Stage I.",
                "position": 2027
            }
        ]
    },
    {
        "header": "Appendix DEvaluation Settings",
        "images": []
    },
    {
        "header": "Appendix EExample Outputs",
        "images": []
    }
]