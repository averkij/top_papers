[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10229/x1.png",
                "caption": "Figure 1:Comparison of reasoning paradigms. Explicit CoT verbalizes all steps as text tokens. Coconut uses a fixed number of latent tokens from hidden states. Soft-Thinking constructs latent tokens via probability-weighted interpolation with entropy-based stopping. Assistant-based methods rely on external models. OurLT-Tuningdynamically interleaves text and latent tokens through confidence-driven insertion and Context-Prediction Fusion.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10229/x2.png",
                "caption": "Figure 2:Overview of the three-stageLT-Tuningframework.Stage 1: standard explicit CoT fine-tuning to establish reasoning capabilities.Stage 2: learning to generate latent tokens with confidence-driven insertion, where hidden states serve as the initial latent representations.Stage 3: Context-Prediction Fusion, which combines contextual history information (hidden states) with predicted semantic guidance (fused embeddings) to construct high-quality latent tokens.",
                "position": 175
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10229/x3.png",
                "caption": "Figure 3:Average number of<thinking>tokens generated versus question difficulty across models of varying sizes. Difficulty is measured by the error rate of Llama-3.1-8B-Instruct over 5 sampling trials. Models generally demonstrate a positive correlation between question difficulty and the number of generated latent tokens, indicating that our method learns to adaptively scale latent reasoning effort based on problem complexity.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2602.10229/x4.png",
                "caption": "Figure 4:Visualization of step-wise model entropy and attention weights on latent tokens for Llama-3.1-8B. Shaded regions indicate Â±1 standard error. Generation steps beyond 400 are truncated for clarity.",
                "position": 692
            }
        ]
    },
    {
        "header": "6In-Depth Analyses of LT-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10229/x5.png",
                "caption": "Figure 5:PCA visualization of latent token embeddings across different reasoning steps for intrinsic methods on Llama-3.1-8B (we only show four key steps here for a better view). Each point represents a different sample from the test set.Coconut(green) exhibits severe feature collapse, where latent tokens from different samples converge to nearly identical points after just two reasoning steps.LT-Tuning w/o Stage 3(blue) shows initial exploration in early positions but gradually collapses to similar representations in later steps.LT-Tuning(red) maintains semantic diversity even at six latent tokens, demonstrating its effectiveness in mitigating feature collapse while preserving exploration capacity in the latent space.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2602.10229/x6.png",
                "caption": "Figure 6:Effect of hidden layer selection onLlama-3.2-3B. Performance remains stable across different layer indices, indicating thatLT-Tuningis robust to this hyperparameter choice.",
                "position": 732
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Statistics",
        "images": []
    },
    {
        "header": "Appendix BTraining Configuration",
        "images": []
    },
    {
        "header": "Appendix CStage-Specific Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DInference Configuration",
        "images": []
    },
    {
        "header": "Appendix EBaseline Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": []
    }
]