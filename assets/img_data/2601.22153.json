[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22153/figures/logo.png",
                "caption": "",
                "position": 60
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22153/x1.png",
                "caption": "Figure 1:Overview of DynamicVLA.(a)A 0.4B-parameter VLA architecture couples a lightweight backbone with an action expert for fast closed-loop control.(b)Continuous Inference overlaps inference and execution through pipelined inference windows, enabling non-blocking action execution across consecutive action chunks.(c)Latent-aware Action Streaming enforces temporally consistent execution by invalidating outdated actions and prioritizing actions from the most recent action chunk.",
                "position": 159
            }
        ]
    },
    {
        "header": "IIIThe DynamicVLA Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22153/x2.png",
                "caption": "Figure 2:Automatic Simulation and Real-world Data Collection.Environment Setup:simulation and real-world settings share diverse objects, tabletop scenes, and synchronized multiview cameras.Object State Acquisition:simulation provides ground-truth 6D object states, while real-world multiview RGB observations are converted into a real-world “simulator” interface that enables automatic dynamic-manipulation data collection without teleoperation or ground-truth sensing.State-machine Controller:a shared four-stage controller uses these states to execute approach, grasp, place, and reset behaviors.",
                "position": 224
            }
        ]
    },
    {
        "header": "IVThe Dynamic Object Manipulation Benchmark",
        "images": []
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22153/x3.png",
                "caption": "Figure 3:Real-world Interaction Evaluation.We compare representative VLA models on six real-world dynamic manipulation tasks across Franka and PiPER, averaging success rates over 20 trials for each of three paired motion–position configurations, with object motion generated by a secondary robot arm.",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2601.22153/x4.png",
                "caption": "Figure 4:Real-world Perception Evaluation.We compare representative VLA models on six real-world dynamic manipulation tasks across Franka and PiPER, averaging success rates over 20 trials for each of three paired motion–position configurations, with object motion generated by a secondary robot arm.",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2601.22153/x5.png",
                "caption": "Figure 5:Real-world Generation Evaluation.We compare representative VLA models on four real-world dynamic manipulation tasks across Franka and PiPER, averaging success rates over 20 trials for each of three paired motion–position configurations, with object motion generated by a secondary robot arm.",
                "position": 1271
            }
        ]
    },
    {
        "header": "VIDiscussion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]