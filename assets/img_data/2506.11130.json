[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMethods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11130/extracted/6530119/0603.png",
                "caption": "Figure 1:Overview of the self-refining framework. The process begins by generating pseudo-labeled speech–text pairs from a pre-existing ASR model(framed by red dashed lines,ℱθsubscriptℱ𝜃{\\mathcal{F}_{\\theta}}caligraphic_F start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT) and a collection of unpaired speech(framed by red dashed lines,𝓢𝓢\\boldsymbol{\\mathcal{S}}bold_caligraphic_S). A TTS model is trained on this pseudo-labeled data and subsequently used to synthesize speech from a large-scale pre-collected text corpus(framed by red dashed line,𝓣𝓣\\boldsymbol{\\mathcal{T}}bold_caligraphic_T). To ensure data quality, filtering and forced alignment are applied. To support long-form transcription and code-switching, utterance concatenation is performed. Additionally, random audio perturbations are introduced to enhance robustness against acoustic variability. The final curated dataset is used to train the target ASR model.",
                "position": 182
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": []
    },
    {
        "header": "VResults",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]