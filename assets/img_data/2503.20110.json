[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x1.png",
                "caption": "Figure 1:To transfer fine-tuning (e.g., instruction tuning) from asourcemodel versionsğ‘ sitalic_s(e.g., Llama 3.0) to atargetversiontğ‘¡titalic_t(Llama 3.1), we first compute the diff vectorÎ”s=msâ€²âˆ’mssubscriptÎ”ğ‘ subscriptsuperscriptğ‘šâ€²ğ‘ subscriptğ‘šğ‘ \\Delta_{s}=m^{\\prime}_{s}-m_{s}roman_Î” start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_m start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - italic_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTfrom versionsğ‘ sitalic_s, wheremsâ€²subscriptsuperscriptğ‘šâ€²ğ‘ m^{\\prime}_{s}italic_m start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTis the fine-tuned model (instruction-tuned Llama 3.0) andmssubscriptğ‘šğ‘ m_{s}italic_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTis the base model (pretrained Llama 3.0). Then, we addÎ”ssubscriptÎ”ğ‘ \\Delta_{s}roman_Î” start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTto the target base model (pretrained Llama 3.1) to approximate the fine-tuned model in versiontğ‘¡titalic_t(instruction-tuned Llama 3.1). We explore two scenarios: (1)recyclingâ€”transferring from an older model version to a newer one to reduce retraining, and (2)backportingâ€”transferring from a newer version to an older one to take advantage of the newer fine-tuning while maintaining optimization for specific use cases.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Transferring fine-tuning updates across model versions",
        "images": []
    },
    {
        "header": "3Efficient multilingual model development",
        "images": []
    },
    {
        "header": "4When is fine-tuning transfer effective?",
        "images": []
    },
    {
        "header": "5Fine-tuning transfer as a starting point for further fine-tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x2.png",
                "caption": "Figure 2:GSM8K performance showing that fine-tuning transfer provides a more computationally efficient starting point (i.e.,â„³i+Î”jsubscriptâ„³ğ‘–subscriptÎ”ğ‘—\\mathcal{M}_{i}+\\Delta_{j}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + roman_Î” start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT) for further training.Here,â„³isubscriptâ„³ğ‘–\\mathcal{M}_{i}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values ofiğ‘–iitalic_iindicating earlier checkpoints), andÎ”isubscriptÎ”ğ‘–\\Delta_{i}roman_Î” start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrefers to the diff vector resulting from the fine-tuning of versioniğ‘–iitalic_i. Additional results forâ„³1subscriptâ„³1\\mathcal{M}_{1}caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,â„³2subscriptâ„³2\\mathcal{M}_{2}caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT,â„³4subscriptâ„³4\\mathcal{M}_{4}caligraphic_M start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTcan be found in AppendixD.",
                "position": 570
            }
        ]
    },
    {
        "header": "6Iterative recycling-then-finetuning for improved performance and efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x3.png",
                "caption": "Figure 3:GSM8K performance showing that both iterative (Î”iâ¢tâ¢eâ¢rsuperscriptÎ”ğ‘–ğ‘¡ğ‘’ğ‘Ÿ\\Delta^{iter}roman_Î” start_POSTSUPERSCRIPT italic_i italic_t italic_e italic_r end_POSTSUPERSCRIPT) and direct (Î”dâ¢iâ¢rsuperscriptÎ”ğ‘‘ğ‘–ğ‘Ÿ\\Delta^{dir}roman_Î” start_POSTSUPERSCRIPT italic_d italic_i italic_r end_POSTSUPERSCRIPT) recycling-then-finetuning approaches offer faster convergence.At a high level,Î”iâ¢tâ¢eâ¢rsuperscriptÎ”ğ‘–ğ‘¡ğ‘’ğ‘Ÿ\\Delta^{iter}roman_Î” start_POSTSUPERSCRIPT italic_i italic_t italic_e italic_r end_POSTSUPERSCRIPTgradually incorporates fine-tuning updates, i.e., diff\nvectors, from previous model versions, whileÎ”dâ¢iâ¢rsuperscriptÎ”ğ‘‘ğ‘–ğ‘Ÿ\\Delta^{dir}roman_Î” start_POSTSUPERSCRIPT italic_d italic_i italic_r end_POSTSUPERSCRIPTdirectly applies the diff vector from the latest model version to the current model.",
                "position": 729
            }
        ]
    },
    {
        "header": "7Related work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional results for Section2: Transferring fine-tuning updates across model versions",
        "images": []
    },
    {
        "header": "Appendix BAdditional evaluation details",
        "images": []
    },
    {
        "header": "Appendix CAdditional results for Section4: When is fine-tuning transfer effective?",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x4.png",
                "caption": "Figure 4:GSM8K performance showing that fine-tuning transfer provides a more computationally efficient starting point (i.e.,â„³i+Î”jsubscriptâ„³ğ‘–subscriptÎ”ğ‘—\\mathcal{M}_{i}+\\Delta_{j}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + roman_Î” start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT) for further training. Here,â„³isubscriptâ„³ğ‘–\\mathcal{M}_{i}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values ofiğ‘–iitalic_iindicating earlier checkpoints), andÎ”isubscriptÎ”ğ‘–\\Delta_{i}roman_Î” start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrefers to the diff vector resulting from the fine-tuning of versioniğ‘–iitalic_i.",
                "position": 2133
            }
        ]
    },
    {
        "header": "Appendix DAdditional results for Section5: Fine-tuning transfer as a starting point for further fine-tuning",
        "images": []
    }
]