[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x1.png",
                "caption": "Figure 1:To transfer fine-tuning (e.g., instruction tuning) from asourcemodel versions𝑠sitalic_s(e.g., Llama 3.0) to atargetversiont𝑡titalic_t(Llama 3.1), we first compute the diff vectorΔs=ms′−mssubscriptΔ𝑠subscriptsuperscript𝑚′𝑠subscript𝑚𝑠\\Delta_{s}=m^{\\prime}_{s}-m_{s}roman_Δ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT - italic_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTfrom versions𝑠sitalic_s, wherems′subscriptsuperscript𝑚′𝑠m^{\\prime}_{s}italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTis the fine-tuned model (instruction-tuned Llama 3.0) andmssubscript𝑚𝑠m_{s}italic_m start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTis the base model (pretrained Llama 3.0). Then, we addΔssubscriptΔ𝑠\\Delta_{s}roman_Δ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTto the target base model (pretrained Llama 3.1) to approximate the fine-tuned model in versiont𝑡titalic_t(instruction-tuned Llama 3.1). We explore two scenarios: (1)recycling—transferring from an older model version to a newer one to reduce retraining, and (2)backporting—transferring from a newer version to an older one to take advantage of the newer fine-tuning while maintaining optimization for specific use cases.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Transferring fine-tuning updates across model versions",
        "images": []
    },
    {
        "header": "3Efficient multilingual model development",
        "images": []
    },
    {
        "header": "4When is fine-tuning transfer effective?",
        "images": []
    },
    {
        "header": "5Fine-tuning transfer as a starting point for further fine-tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x2.png",
                "caption": "Figure 2:GSM8K performance showing that fine-tuning transfer provides a more computationally efficient starting point (i.e.,ℳi+Δjsubscriptℳ𝑖subscriptΔ𝑗\\mathcal{M}_{i}+\\Delta_{j}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + roman_Δ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT) for further training.Here,ℳisubscriptℳ𝑖\\mathcal{M}_{i}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values ofi𝑖iitalic_iindicating earlier checkpoints), andΔisubscriptΔ𝑖\\Delta_{i}roman_Δ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrefers to the diff vector resulting from the fine-tuning of versioni𝑖iitalic_i. Additional results forℳ1subscriptℳ1\\mathcal{M}_{1}caligraphic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,ℳ2subscriptℳ2\\mathcal{M}_{2}caligraphic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT,ℳ4subscriptℳ4\\mathcal{M}_{4}caligraphic_M start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTcan be found in AppendixD.",
                "position": 570
            }
        ]
    },
    {
        "header": "6Iterative recycling-then-finetuning for improved performance and efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x3.png",
                "caption": "Figure 3:GSM8K performance showing that both iterative (Δi⁢t⁢e⁢rsuperscriptΔ𝑖𝑡𝑒𝑟\\Delta^{iter}roman_Δ start_POSTSUPERSCRIPT italic_i italic_t italic_e italic_r end_POSTSUPERSCRIPT) and direct (Δd⁢i⁢rsuperscriptΔ𝑑𝑖𝑟\\Delta^{dir}roman_Δ start_POSTSUPERSCRIPT italic_d italic_i italic_r end_POSTSUPERSCRIPT) recycling-then-finetuning approaches offer faster convergence.At a high level,Δi⁢t⁢e⁢rsuperscriptΔ𝑖𝑡𝑒𝑟\\Delta^{iter}roman_Δ start_POSTSUPERSCRIPT italic_i italic_t italic_e italic_r end_POSTSUPERSCRIPTgradually incorporates fine-tuning updates, i.e., diff\nvectors, from previous model versions, whileΔd⁢i⁢rsuperscriptΔ𝑑𝑖𝑟\\Delta^{dir}roman_Δ start_POSTSUPERSCRIPT italic_d italic_i italic_r end_POSTSUPERSCRIPTdirectly applies the diff vector from the latest model version to the current model.",
                "position": 729
            }
        ]
    },
    {
        "header": "7Related work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional results for Section2: Transferring fine-tuning updates across model versions",
        "images": []
    },
    {
        "header": "Appendix BAdditional evaluation details",
        "images": []
    },
    {
        "header": "Appendix CAdditional results for Section4: When is fine-tuning transfer effective?",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20110/x4.png",
                "caption": "Figure 4:GSM8K performance showing that fine-tuning transfer provides a more computationally efficient starting point (i.e.,ℳi+Δjsubscriptℳ𝑖subscriptΔ𝑗\\mathcal{M}_{i}+\\Delta_{j}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + roman_Δ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT) for further training. Here,ℳisubscriptℳ𝑖\\mathcal{M}_{i}caligraphic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values ofi𝑖iitalic_iindicating earlier checkpoints), andΔisubscriptΔ𝑖\\Delta_{i}roman_Δ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrefers to the diff vector resulting from the fine-tuning of versioni𝑖iitalic_i.",
                "position": 2133
            }
        ]
    },
    {
        "header": "Appendix DAdditional results for Section5: Fine-tuning transfer as a starting point for further fine-tuning",
        "images": []
    }
]