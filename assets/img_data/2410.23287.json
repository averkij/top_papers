[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23287/x1.png",
                "caption": "Figure 1:We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language by capitalizing on powerful visual-language representations learned by video diffusion models. REM generalizes with ease to challenging, dynamic concepts, such as raindrops or shattering glass, shown above. Video visualizations are availablehere.",
                "position": 90
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23287/x2.png",
                "caption": "Figure 2:Through Internet-scale pre-training, video diffusion models can generate realistic videos capturing the entire diversity of the dynamic visual world (generated samples shown above). We leverage their powerful visual-language representation for open-world referral video segmentation .",
                "position": 116
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23287/x3.png",
                "caption": "Figure 3:The model architecture of Refer Everything with Diffusion Models (REM). Like a video diffusion model it is based on, our approach takes video frames with added noise and a language expression as input. Our key insight is preserving as much of the diffusion representation intact as possible by supervising segmentation masks in the latent space of the frozen VAE.",
                "position": 196
            }
        ]
    },
    {
        "header": "4Benchmark design and collection",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23287/x4.png",
                "caption": "Figure 4:Qualitative results of REM and state of the art baselines onBURST,VSPWandRef-VPSbenchmarks. Our method demonstrates both superior coverage of rare, dynamic concepts and higher segmentation precision. Video comparisons are availablehere.",
                "position": 400
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23287/extracted/5966478/figures/data_hist.png",
                "caption": "Table A:Statistics of our Ref-VPS benchmark. Our dataset contains 141 video clips covering 39 concepts for dynamic processes.",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2410.23287/extracted/5966478/figures/data_hist.png",
                "caption": "Figure A:Distribution of sample lengths in Ref-VPS.",
                "position": 1513
            },
            {
                "img": "https://arxiv.org/html/2410.23287/x5.png",
                "caption": "Figure B:Qualitative comparison of REM with state-of-the-art baselines on dynamic and challenging fight scenes. The incorrectly labeled frames are outlined in red. Our method outperforms the other methods in handling frequent occlusions and POV changes. For a better illustration of the differences, please watch the full videoshere.",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2410.23287/x6.png",
                "caption": "Figure C:Failure cases of REM on Ref-VPS. Our model still exhibits some object-centric bias and struggles with extremely dynamic entities such as lightning.",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2410.23287/x7.png",
                "caption": "Figure D:Class-wiseùí•ùí•\\mathcal{J}caligraphic_Jscores (mIoU) concept coverage on BURST. As indicated by the arrows, Our method is robust on the long-tail region compared to other methods.",
                "position": 1530
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    }
]