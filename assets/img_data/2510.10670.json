[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10670/x1.png",
                "caption": "Figure 1:Showcasing AdaViewPlanner: Given 4D contents and text prompts that depicts scene context and desired camera movements, we adapt pre-trained video diffusion models to generate coordinate-aligned camera pose sequence as well as an corresponding video visualization.",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10670/x2.png",
                "caption": "Figure 2:(a) Stage I model for motion-conditioned cinematic video generation: a pose encoder processes human motion data (M) from 4D scenes and integrates it with video tokens via spatial motion attention to produce videos with cinematic camera movements. Camera parameters used for guidance are denoted asC. (b) Stage II model: three branches for video, camera, and human motion are combined in an MMDiT framework to extract camera pose.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10670/x3.png",
                "caption": "Figure 3:Visualization of results. (Left) Human motion conditions; (Middle) Stage I generated videos; (Right) Stage II generated camera trajectories. AdaViewPlanner demonstrates the ability to design diverse, instruction-consistent, and human-centered camera trajectories.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x4.png",
                "caption": "Figure 4:Camera generation results with varied random seed (top), scene context prompt (middle), and camera movement prompt (bottom).",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x5.png",
                "caption": "Figure 5:Compared with other methods, our model generates smoother trajectories that better follow instructions, while also exhibiting a cinematographic style centered on human actions.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x6.png",
                "caption": "Figure 6:Columns 1–4 show the reprojection of 4D human skeletons by using estimated camera parameters, while Column 5 presents the rendered results in 3D space.w/o Motionexhibits viewpoint errors, whereasRelative Camsuffers from scale perception issues.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x7.png",
                "caption": "Figure 7:Comparison of training loss curves: Ours, DanceCam* and Ours w/o Video Model.",
                "position": 619
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AIntroduction of the Base Text-to-Video Generation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10670/x8.png",
                "caption": "Figure 8:Overview of the base text-to-video generation model.",
                "position": 1281
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix DDetails of MLLM-based Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10670/x9.png",
                "caption": "Figure 9:Examples of MLLM-based evaluation. Lines 1–2 assess text–camera consistency, while lines 3–4 evaluate the diversity of cinematographic styles. On the left, the textual input is paired with the corresponding 3D spatial visualization results. In the middle, the three-view projections are displayed, where both the tri-view images and the text serve as input data. On the right, the output generated by Gemini 2.5 Pro is presented.",
                "position": 1318
            }
        ]
    },
    {
        "header": "Appendix EAdditional Analyses and Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10670/x10.png",
                "caption": "Figure 10:Comparison of camera estimation on AI-generated videos",
                "position": 1899
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x11.png",
                "caption": "Figure 11:Additional ablation results on pipeline design.",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x12.png",
                "caption": "Figure 12:More visualization results of AdaViewPlanner, demonstrating the diversity of the generated trajectories and the model’s ability to follow camera text instructions.",
                "position": 1911
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x13.png",
                "caption": "Figure 13:More visualization results",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2510.10670/x14.png",
                "caption": "Figure 14:More visualization results",
                "position": 1934
            }
        ]
    },
    {
        "header": "Appendix FUse of Large Language Models",
        "images": []
    }
]