[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09046/x1.png",
                "caption": "Figure 1:The illustration of the main concept of our work,\nfocusing on the feed-forward network (FFN) component within a standard ViT(Dosovitskiy et¬†al.,2021)encoder.\nIn the left part,\na typical ViT encoder is depicted, consisting of totallyLùêøLitalic_LTransformer layers.\nThe right part illustrates the neuron path discovered by our method,\nwhich identifies a path comprising of the neurons within the FFN module across the model layers.\nEach FFN in the encoder is denoted as FFNl,l‚àà[1,L]ùëô1ùêøl\\in[1,L]italic_l ‚àà [ 1 , italic_L ].",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Neuron Path",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09046/x2.png",
                "caption": "Figure 2:The distribution of knowledge neurons in two different pretrained vision Transformer models.\nIt can be noticed that comparing ViT-B-16(Dosovitskiy et¬†al.,2021)and MAE-B-16(He et¬†al.,2022), their neuron\nattribution show completely opposite distributions across layers.",
                "position": 256
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09046/x3.png",
                "caption": "Figure 3:The relative deviation in the model‚Äôs predicted probability of the ground-truth label when the value of neurons selected by different methods is either removed (zeroed out) or enhanced (doubled).",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x4.png",
                "caption": "Figure 4:The frequency of each neuron at each layer occurred in the discovered neuron paths.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x5.png",
                "caption": "Figure 5:Examples of category similarity analysis.\nUsing ViT-B-16 as target model, we randomly select three categories and calculate the similarity with others using the neuron utilization matrices and sample the top 5% and bottom 5% similar items.\nThrough visualization we can see that categories with high (low) neuron path similarity tend to be also high (low) in semantic similarity.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x6.png",
                "caption": "Figure 6:Model performance after different proportions of neuron pruning using ViT-B-16 and ViT-B-32. The dotted line represent the original performance of the used model, the y-axis represent the mean accuracy of the edited model and the x-axis represent the number of neuron in each layer we preserved.\nBaseline means the performance of the original model.",
                "position": 702
            }
        ]
    },
    {
        "header": "5Conclusion and Future Works",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALayer-progressive Neuron Locating Algorithm Analysis",
        "images": []
    },
    {
        "header": "Appendix BPreliminary Experiment Method",
        "images": []
    },
    {
        "header": "Appendix CQuantitative Experiment Details",
        "images": []
    },
    {
        "header": "Appendix DMore Examples and Implementation Details about Class-level Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09046/x7.png",
                "caption": "Figure 7:More Path Statistic Visualization for ViT-B-16",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x8.png",
                "caption": "Figure 8:More Path Statistic Visualization for ViT-B-32",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x9.png",
                "caption": "Figure 9:More Path Statistic Visualization for ViT-L-32",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x10.png",
                "caption": "Figure 10:More Path Statistic Visualization for MAE-B-16",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x11.png",
                "caption": "Figure 11:More examples of category similarity for ViT-B-16",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x12.png",
                "caption": "Figure 12:More examples of category similarity for ViT-B-32",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x13.png",
                "caption": "Figure 13:More examples of category similarity for ViT-L-32",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2503.09046/x14.png",
                "caption": "Figure 14:More examples of category similarity for MAE-B-16",
                "position": 1866
            }
        ]
    },
    {
        "header": "Appendix EImplementation details of model pruning",
        "images": []
    }
]