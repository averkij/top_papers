[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05629/extracted/6516975/ID-SPAM.jpg",
                "caption": "Figure 1:ID-SPAMFramework. Given an LM, the generated soft-prompt can be prepended to any transformer layer‚Äôs inputs (the figure can be best seen in color)",
                "position": 133
            }
        ]
    },
    {
        "header": "2Proposed Solution",
        "images": []
    },
    {
        "header": "3Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05629/extracted/6516975/graph_layer_final_6.jpg",
                "caption": "Table 2:Test results on GLUE benchmark.\nWe use RoBERTa-BASE, RoBERTa-LARGE Backbones for all methods.\nWe report the score, along with stddev for 3 runs (in the subscript) for all tasks. The best performing Soft Prompt-based method‚Äôs results are in bold",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2506.05629/extracted/6516975/graph_layer_final_6.jpg",
                "caption": "Table 6:Mean, stddev of zero-shot task, domain transfer for different methods. ‚ÄòScore‚Äô is average of Accuracy and macro F1-Score. The best performing Soft Prompt-based method‚Äôs results are in bold.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2506.05629/extracted/6516975/graph_layer_final_6.jpg",
                "caption": "Figure 2:Effect of Variation in layer index (mùëömitalic_m) corresponding to which soft prompt is prepended on performance (m=0ùëö0m=0italic_m = 0refers to input embeddings). Metrics are average of acc. and F1 for MRPC and acc. for RTE.",
                "position": 902
            }
        ]
    },
    {
        "header": "4Discussions and Conclusion",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperiment Settings",
        "images": []
    },
    {
        "header": "Appendix BEvaluation using GPT-2 and GPT-2 Large Backbones",
        "images": []
    },
    {
        "header": "Appendix CFew-Shot Task Transfer",
        "images": []
    },
    {
        "header": "Appendix DComparison ofID-SPAMwith baselines w.r.t model size and training and inference times",
        "images": []
    },
    {
        "header": "Appendix EConvergence of the LoRA Baseline",
        "images": []
    }
]