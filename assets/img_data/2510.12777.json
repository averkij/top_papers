[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/x1.png",
                "caption": "Figure 1:What If:Our Flow Poke Transformer directly models the uncertainty of the world by predictingdistributionsof how objects (√ó\\color[rgb]{1,0.49609375,0.0546875}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.49609375,0.0546875}\\boldsymbol{\\times}) may move conditioned on some input movements (pokes,‚Üí\\rightarrow). We see that whether the hand (below paw) or the paw (above hand) moves downwards directly influences the other‚Äôs movement. Left: the paw pushing the hand down, will force the hand downwards, resulting in a unimodal distribution. Right: the hand moving down results in two modes, the paw following along or staying put.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/x2.png",
                "caption": "Figure 2:High-level Model Architecture Overview. Given an image‚Ñê\\mathcal{I}, a set of given pokesùí´\\mathcal{P}(visualized as arrows‚Üí\\rightarrow), and query positionsùê™\\mathbf{q}(√ó\\!\\color[rgb]{1,0.49609375,0.0546875}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.49609375,0.0546875}\\times\\!), our model directly predicts an explicit distribution of the movement at each query position. The flow poke transformer cross-attends to features from a jointly trained image encoder to incorporate visual information. Crucially, our architecture represents movement at individual pointsùê™\\mathbf{q}(enabling sparse & off-grid motion processing) and directly predicts continuous, multimodal output distributions.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/x3.png",
                "caption": "Figure 3:Multimodal Motion Distribution Prediction. We condition on one or multiple pokes (‚Üí\\rightarrow) and then query the motion distribution of specific points (√ó\\boldsymbol{\\times}). Our model‚Äôs predictions capture the multi-modal nature of motion and exhibit understanding of interactions, such as only lifting the cup by its handle not necessarily causing the whole cup to move upwards, while grabbing it at stable points does. It also demonstrates prior understanding from scenes, such as a car in an intersection being more likely to move forwards than backward and cars in traffic likely moving together.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2510.12777/x4.png",
                "caption": "Figure 4:Predicted Mode Analysis.Values are computed at a resolution of64264^{2}.(a)Diversity of predicted modes is high, with mode variation covering a large fraction of poke magnitude.(b)One mode typically has a substantially higher confidence than others, which increases with given poke count. The modeclosestto the ground truth consistently has a higher-than-average confidence.(c)More confident modes are more accurate as measured by PCK.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2510.12777/x5.png",
                "caption": "Figure 5:Uncertainty Calibration.We find that the motion prediction error measured by EPE strongly correlates with the predicted uncertainty (PearsonœÅ=0.64\\rho\\!=\\!{0.64}). This relationship holds for low & high numbers of given pokes.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/crowd1/crowd.jpg",
                "caption": "Figure 6:Unconditional AR Motion Sampling. We show samples of generated flow without prior motion conditioning on pokes. Our model can generate a wide variety of realistic motions.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/crowd1/sample1.png",
                "caption": "",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/crowd1/sample2.png",
                "caption": "",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/crowd1/sample3.png",
                "caption": "",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/domino/input.jpg",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/domino/sample1.png",
                "caption": "",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/domino/sample2.png",
                "caption": "",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/domino/sample3.png",
                "caption": "",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/jenga/input.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/jenga/sample4.png",
                "caption": "",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/jenga/sample2.png",
                "caption": "",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/samples/jenga/sample3.png",
                "caption": "",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/face/input.png",
                "caption": "Figure 7:Fine-grained Face Motion Control. We show fine-grained zero-shot poking results on faces and compare against InstantDrag[36], which was trained for this task. We further visualize the predicted motion as warps using I-D‚Äôs face warping model.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/face/our_img.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/face/our_warp.png",
                "caption": "",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/face/their_img.png",
                "caption": "",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/face/their_warp.png",
                "caption": "",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/ape/input.png",
                "caption": "",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/ape/our_img.png",
                "caption": "",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/ape/our_warp.png",
                "caption": "",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/ape/their_img.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/ape/their_warp.png",
                "caption": "",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/elephant/input.png",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/elephant/our_img.png",
                "caption": "",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/elephant/our_warp.png",
                "caption": "",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/elephant/their_img.png",
                "caption": "",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/instantdrag/elephant/their_warp.png",
                "caption": "",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/dragapart_motion_segmentation_visualization_2_same_vis.png",
                "caption": "(a)We directly replicate Fig. 7 from DragAPart[20]with our method. Our method provides spatially continuous predictions and makes fewer critical mistakes like segmenting the furniture body with the drawer (top right).",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/dragapart_motion_segmentation_visualization_2_same_vis.png",
                "caption": "(a)We directly replicate Fig. 7 from DragAPart[20]with our method. Our method provides spatially continuous predictions and makes fewer critical mistakes like segmenting the furniture body with the drawer (top right).",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/dragapart_motion_segmentation_visualization_same_vis.png",
                "caption": "",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/open_world/dominos_input.png",
                "caption": "(b)Open-set moving part dependency visualization. The degree to which the movement of each part is influenced by the poke (‚Üí\\rightarrow) is visualized as a heatmap, where brighter color means a higher degree of influence.",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/open_world/dominos_pred.png",
                "caption": "",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/open_world/giraffe_input.png",
                "caption": "",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/open_world/giraffe_pred.png",
                "caption": "",
                "position": 730
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/open_world/basketball_input.png",
                "caption": "",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/moving_part_segmentation/open_world/basketball_pred.png",
                "caption": "",
                "position": 732
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/fig/failure_cases/snake_drawing/snek_input.png",
                "caption": "Figure 9:Common Failure Cases. Our model, generically pretrained on primarily realistic videos, does not generalize well to cartoons, causing parts of the background to be moved together with objects. Additionally, our model sometimes, but not consistently, jointly predicts the movement of shadows together with objects, which can be problematic for downstream use cases.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/failure_cases/snake_drawing/snek_pred.png",
                "caption": "",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/failure_cases/shadow/feet_shadow_input.png",
                "caption": "",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/failure_cases/shadow/feet_shadow_pred.png",
                "caption": "",
                "position": 762
            }
        ]
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/x6.png",
                "caption": "Figure A:Query-Causal Attention Pattern Visualization. We show the resulting attention patterns for our query-causal attention for different numbers of queries per poke count. We put poke tokens first, followed by query tokens.(a)In the simplest setting, with one query per set of pokes, there is one query token per set of pokes, with each query token attending to one more poke token than its predecessor.(b)ForNq>1N_{q}>1, the poke attention does not change, but there are multiple query tokens per poke set. Here, even query tokens for the same poke set do not attend to each other to enable parallel evaluation during inference.",
                "position": 1535
            }
        ]
    },
    {
        "header": "BAblations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/joint_training/joint_ablation_input.png",
                "caption": "Figure B:Jointly training the vision encoder is important. We train a model with a frozen pretrained vision encoder. The model struggles with instance-specificity, predicting the same movement for the woman‚Äôs hand as it does for the man‚Äôs hand. When jointly training the vision encoder with the flow poke transformer, the man‚Äôs hand‚Äôs movement does not directly influence the woman‚Äôs hand.",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/joint_training/joint_ablation_pred_frozen.png",
                "caption": "",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/joint_training/joint_ablation_pred_joint.png",
                "caption": "",
                "position": 1932
            }
        ]
    },
    {
        "header": "CExtension to 3D Motion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/3d_samples/3d_lion_cub_rgb.png",
                "caption": "Figure C:3D Motion Estimation. We show unconditional 3D motion estimation samples from an FPT variant fine-tuned on 3D track data. The in-plane motion predictionùêÖ|x‚Äãy\\mathbf{F}|_{xy}resembles that of a 2D FPT model, while this version can also successfully predict plausible out-of-plane motionùêÖ|z\\mathbf{F}|_{z}.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/3d_samples/3d_lion_cub_pred_xy.png",
                "caption": "",
                "position": 1963
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/3d_samples/3d_lion_cub_pred_z.png",
                "caption": "",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2510.12777/x7.png",
                "caption": "",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/3d_samples/3d_snek_rgb.png",
                "caption": "",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/3d_samples/3d_snek_pred_xy.png",
                "caption": "",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/3d_samples/3d_snek_pred_z.png",
                "caption": "",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dragamove/output1.png",
                "caption": "Figure D:Qualitative Results for Articulated Object Motion Estimation. We compare on Drag-A-Move[20]with Motion-I2V[35], DragAPart[20], and PuppetMaster[19]. Our model is qualitatively more capable of capturing complex conditioning with multiple different pokes than DAP and PM in this setup. Motion-I2V often fails to accurately follow the conditioning locally.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dragamove/output2.png",
                "caption": "",
                "position": 2048
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dragamove/output4.png",
                "caption": "",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dragamove/output3.png",
                "caption": "",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/069/image.png",
                "caption": "(a)When the head of the giraffe is moving down, we get different flow distributions depending on how close the query is to the head. Since the head can also move down without the neck following, we get distributions with more emphasis on no movement when the query is further away from the head (first example). When the query gets really close to the head (second example), the likelihood of movement at the query also increases which can be seen in the stronger bottom mode.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/069/image.png",
                "caption": "(a)When the head of the giraffe is moving down, we get different flow distributions depending on how close the query is to the head. Since the head can also move down without the neck following, we get distributions with more emphasis on no movement when the query is further away from the head (first example). When the query gets really close to the head (second example), the likelihood of movement at the query also increases which can be seen in the stronger bottom mode.",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/069/image1.png",
                "caption": "",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/069/image3.png",
                "caption": "",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/070/image.png",
                "caption": "",
                "position": 2073
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/070/image1.png",
                "caption": "",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/070/image3.png",
                "caption": "",
                "position": 2075
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal_2/073/image.png",
                "caption": "(b)The model accounts both for the possibility of the tower falling over with the brick‚Äôs movement and with it staying stationary. The likelihood of the tower falling over depends on the velocity with which the brick is removed.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal_2/073/image1.png",
                "caption": "",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal_2/073/image3.png",
                "caption": "",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal_2/074/image.png",
                "caption": "",
                "position": 2089
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal_2/074/image1.png",
                "caption": "",
                "position": 2090
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal_2/074/image3.png",
                "caption": "",
                "position": 2091
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/man2/1.png",
                "caption": "(c)Depending on which hand moves, the cup is predicted to be either stationary orpotentiallymoving together with the hand holding it. Note that the case of the cup not moving with the hand holding it is very improbable, as visualized by the arrow pointing to that mode having substantially less opacity.",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/man2/2.png",
                "caption": "",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/man2/3.png",
                "caption": "",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/man1/1.png",
                "caption": "",
                "position": 2105
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/man1/2.png",
                "caption": "",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/man1/3.png",
                "caption": "",
                "position": 2107
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/forrest1/1.png",
                "caption": "(d)Depending on the height of the position queried on the tree, the magnitude of the predicted movement changes, reflecting typical intuition as to how a tree moves.",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/forrest1/2.png",
                "caption": "",
                "position": 2119
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/forrest1/3.png",
                "caption": "",
                "position": 2120
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/forrest2/1.png",
                "caption": "",
                "position": 2121
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/forrest2/2.png",
                "caption": "",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/forrest2/3.png",
                "caption": "",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/054/image.png",
                "caption": "(e)The model is capable of understanding the effect of rotational movements.",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/054/image1.png",
                "caption": "",
                "position": 2135
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/054/image3.png",
                "caption": "",
                "position": 2136
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/056/image.png",
                "caption": "",
                "position": 2137
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/056/image1.png",
                "caption": "",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_multimodal/056/image3.png",
                "caption": "",
                "position": 2139
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/000/image.png",
                "caption": "Figure F:Qualitative samples visualizing motion predictions inferred from a single image and (optionally) pokes.",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/000/image1.png",
                "caption": "",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/001/image.png",
                "caption": "",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/001/image1.png",
                "caption": "",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/003/image.png",
                "caption": "",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/003/image1.png",
                "caption": "",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/015/image.png",
                "caption": "",
                "position": 2168
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/015/image1.png",
                "caption": "",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/016/image.png",
                "caption": "",
                "position": 2170
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/016/image1.png",
                "caption": "",
                "position": 2171
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/017/image.png",
                "caption": "",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/017/image1.png",
                "caption": "",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/031/image.png",
                "caption": "",
                "position": 2175
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/031/image1.png",
                "caption": "",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/032/image.png",
                "caption": "",
                "position": 2177
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/032/image1.png",
                "caption": "",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/034/image.png",
                "caption": "",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative/034/image1.png",
                "caption": "",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/041/image.png",
                "caption": "",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/041/image1.png",
                "caption": "",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/043/image.png",
                "caption": "",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/043/image1.png",
                "caption": "",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/044/image.png",
                "caption": "",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/044/image1.png",
                "caption": "",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/046/image.png",
                "caption": "",
                "position": 2189
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/046/image1.png",
                "caption": "",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/047/image.png",
                "caption": "",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/047/image1.png",
                "caption": "",
                "position": 2192
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/048/image.png",
                "caption": "",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/appendix/qualitative_more/048/image1.png",
                "caption": "",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/ants/poke.png",
                "caption": "",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/ants/flow.png",
                "caption": "",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/bird/poke.png",
                "caption": "",
                "position": 2198
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/bird/flow.png",
                "caption": "",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/horse/input.png",
                "caption": "",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/horse/flow.png",
                "caption": "",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/squat/poke.png",
                "caption": "",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/squat/flow.png",
                "caption": "",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/squat/military/poke.png",
                "caption": "",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/squat/military/flow.png",
                "caption": "",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/spider/input.png",
                "caption": "",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2510.12777/fig/dense_mean/spider/flow.png",
                "caption": "",
                "position": 2208
            }
        ]
    },
    {
        "header": "DAdditional Qualitative Examples",
        "images": []
    }
]