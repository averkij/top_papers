[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/x1.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/broom_transparent.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/checkmark.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/cross.png",
                "caption": "",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/noise_degradation_qualitative.png",
                "caption": "Figure 2:Deterioration of Diffusion Features. As current methodsneedto pass noisy images to the model to obtain useful features, they significantly reduce the information available. We alleviate this problem by obtaining useful features without noise, improving the performance of downstream tasks.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/diffusion_feature_noise.png",
                "caption": "Figure 3:Fraction of variance of diffusion features explained by 1) encoding the clean image att=0ùë°0t=0italic_t = 0(no additive noise), and 2) encoding just the added noiseœµbold-italic-œµ\\boldsymbol{\\epsilon}bold_italic_œµatt=999ùë°999t=999italic_t = 999. Even at relatively low timesteps such ast=261ùë°261t=261italic_t = 261as used by DIFT[54], a substantial part of the features directly depends only on the added noise.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2412.03439/x2.png",
                "caption": "Figure 4:Our training setup. We train our model to predict features from a clean input image, while the frozen diffusion model is fed the noisy image. The projection heads project our model‚Äôs features onto the noisy diffusion model features, given the noising timesteptùë°titalic_t. For downstream tasks, we discard the projection heads and directly use our model‚Äôs internal representations as features.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/broom.png",
                "caption": "",
                "position": 329
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/x3.png",
                "caption": "Figure 5:Following[54], we evaluate semantic correspondence matching accuracy for different noise levels. Our feature extractor outperforms the standard noisy diffusion features across all timestepstùë°titalic_t. We additionally demonstrate that simply providing the diffusion model with a clean image and a non-zero timestep does not result in improved performance.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2412.03439/x4.png",
                "caption": "Figure 6:Semantic correspondence results using DIFT[54]features with the standard SD 2.1 (t=261ùë°261t=261italic_t = 261) and our CleanDIFT features. Our clean features show significantly less incorrect matches than the base diffusion model.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth_input.png",
                "caption": "Figure 7:Qualitative results for depth estimation using a linear probe on diffusion features on NYUv2[34]. Our CleanDIFT features enable substantially better depth estimation than standard diffusion features. Note how the CleanDIFT features are far less noisy when compared to the standard diffusion features.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth_student.png",
                "caption": "",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth_teacher.png",
                "caption": "",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/1/gt.png",
                "caption": "",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/input.png",
                "caption": "",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/s.png",
                "caption": "",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/t299.png",
                "caption": "",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/2/gt.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/input.png",
                "caption": "",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/s.png",
                "caption": "",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/t299.png",
                "caption": "",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/depth/3/gt.png",
                "caption": "",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2412.03439/x5.png",
                "caption": "Figure 8:Performance on semantic segmentation using linear probes. Our clean features outperform the noisy diffusion features for the best noising timesteptùë°titalic_t. Semantic segmentation performance of a standard diffusion model heavily depends on the used noising timestep. Unlike for semantic correspondence matching, the optimaltùë°titalic_tvalue appears to be aroundt=100ùë°100t=100italic_t = 100.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_8_2.png",
                "caption": "Figure 9:Qualitative results for semantic segmentation from diffusion features on Pascal VOC[15]. Standard SD features uset=100ùë°100t=100italic_t = 100as the timestep, which we found to perform best quantitatively (c.f.Figure8). Note how the CleanDIFT segmentation maps are far less noisy compared to those of the standard diffusion features.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_8_2_student.png",
                "caption": "",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_8_2_teacher.png",
                "caption": "",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_9_1.png",
                "caption": "",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_9_1_student.png",
                "caption": "",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_9_1_teacher.png",
                "caption": "",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_66_2.png",
                "caption": "",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_66_2_student.png",
                "caption": "",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_66_2_teacher.png",
                "caption": "",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_0_0.png",
                "caption": "",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_0_0_student.png",
                "caption": "",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_0_0_teacher.png",
                "caption": "",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_41_3.png",
                "caption": "",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_41_3_student.png",
                "caption": "",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_41_3_teacher.png",
                "caption": "",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_67_2.png",
                "caption": "",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_67_2_student.png",
                "caption": "",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_67_2_teacher.png",
                "caption": "",
                "position": 656
            },
            {
                "img": "https://arxiv.org/html/2412.03439/x6.png",
                "caption": "Figure 10:Classification performance on ImageNet1k[10], using kNN classifier withk=10ùëò10k=10italic_k = 10and cosine similarity as the distance metric. We sweep over different timesteps and feature maps. We find that the feature map with the lowest spatial resolution (feature map #0) yields the highest classification accuracy.",
                "position": 671
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AArchitecture Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/x7.png",
                "caption": "Figure 11:We align the feature maps between our feature extractor and the diffusion model at multiple stages within the network to enable the usage of multiple feature maps for downstream tasks. In total, we extract and align features atK=11ùêæ11K=11italic_K = 11stages of the SD U-Net decoder. The downsampling factor for the different blocks is denoted asDSand the channel dimension is shown on the right side of each block.",
                "position": 1752
            }
        ]
    },
    {
        "header": "BAdditional Quantitative Evaluations",
        "images": []
    },
    {
        "header": "CAdditional Qualitative Samples",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/x8.png",
                "caption": "Figure 12:Additional qualitative results for semantic correspondence matching using DIFT[54]with the standard SD 2.1 (t=261ùë°261t=261italic_t = 261) and our CleanDIFT features. Our clean features show significantly less incorrect matches than the standard diffusion features, especially along texture-less edges.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2412.03439/x9.png",
                "caption": "Figure 13:Depending on the downstream application, different diffusion timesteps result in optimal feature representations. For semantic segmentation,t=100ùë°100t=100italic_t = 100is optimal resulting in a much cleaner segmentation map compared to higher timesteps. However, for depth estimation, the low timestep yields inaccurate depth estimates and a higher timestep is necessary (t=300ùë°300t=300italic_t = 300). CleanDIFT remedies the dependence on the timestep and yields optimal features for every downstream task without additional tuning (Figure1).",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_7_2.png",
                "caption": "Figure 14:Additional qualitative results for semantic segmentation from diffusion features on Pascal VOC[15]. Standard SD features uset=100ùë°100t=100italic_t = 100as the timestep, which we found to perform best quantitatively (c.f.Figure8).",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_7_2_student.png",
                "caption": "",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_7_2_teacher.png",
                "caption": "",
                "position": 1839
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_51_2.png",
                "caption": "",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_51_2_student.png",
                "caption": "",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_51_2_teacher.png",
                "caption": "",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_55_3.png",
                "caption": "",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_55_3_student.png",
                "caption": "",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_55_3_teacher.png",
                "caption": "",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_59_0.png",
                "caption": "",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_59_0_student.png",
                "caption": "",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_59_0_teacher.png",
                "caption": "",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_14_2.png",
                "caption": "",
                "position": 1867
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_14_2_student.png",
                "caption": "",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_14_2_teacher.png",
                "caption": "",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/image_6_0.png",
                "caption": "",
                "position": 1872
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_6_0_student.png",
                "caption": "",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2412.03439/extracted/6045344/images/sem_seg/mask_6_0_teacher.png",
                "caption": "",
                "position": 1874
            }
        ]
    },
    {
        "header": "DProjection Heads",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03439/x10.png",
                "caption": "Figure 15:Metric depth prediction for NYUv2[34]using linear probes. We investigate our proposed projection heads‚Äô outputs by training linear probes for depth prediction on them, following the procedure described inSec.4.3. This figure extends the results presented inTab.2by showcasing the performance over timesteps.",
                "position": 1890
            }
        ]
    },
    {
        "header": "EEvaluation Design Choices",
        "images": []
    }
]