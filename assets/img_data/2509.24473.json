[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24473/x1.png",
                "caption": "Figure 1:Performance gains on VSIBench after model training on Euclid30K, for more complete data please refer to Tab.2.",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24473/x2.png",
                "caption": "Figure 2:The examples of the newly collected questions in Euclid30K. More examples can be found in the appendix.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x3.png",
                "caption": "Figure 3:Enhancing Spatial Perception and Reasoning Capabilities in Models Using the Geometric Problem-Solving Dataset (Euclid30K).",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x4.png",
                "caption": "Table 2:Evaluation Results on VSI-Bench[72].\nThe performance of Gemini-2.5 is reported from RoboBrain2.0[8], and the performance of Spatial-MLLM-4B[69]and M2-Reasoning-7B[33]is reported from its original paper, while the results for the other Baseline, Proprietary Models, and Open-source Models are taken from the VSI-Bench benchmark[72]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL[9]and RoboBrain2.0[8]trained with GRPO[62]on the Euclid30K dataset.",
                "position": 425
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Standard Domain-Adaptation Bound",
        "images": []
    },
    {
        "header": "Appendix BEvidence from Educational Psychology",
        "images": []
    },
    {
        "header": "Appendix CDetailed Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24473/x4.png",
                "caption": "Figure 4:Performance improvementon SuperClevr[42], Omni3DBench[53], VSIBench[72], and MindCube[74]after the model has been trained on Eculid25K.",
                "position": 2682
            }
        ]
    },
    {
        "header": "Appendix DMore visualization and analysis of results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24473/x5.png",
                "caption": "Figure 5:Visualization Evaluation Results on VSI-Bench[72].\nThe performance of Gemini-2.5 is reported from RoboBrain2.0[8], and the performance of Spatial-MLLM-4B[69]and M2-Reasoning-7B[33]is reported from its original paper, while the results for the other Baseline, Proprietary Models, and Open-source Models are taken from the VSI-Bench benchmark[72]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL[9]and RoboBrain2.0[8]trained with GRPO[62]on the Euclid30K dataset.",
                "position": 2699
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x6.png",
                "caption": "Figure 6:Visualization Evaluation Results on MindCube[74].\nThe performance of Proprietary Models and Spatial Models are taken from the MindCube benchmark[74]. Qwen2.5VL-Euclid and RoboBrain2.0-Euclid indicate the Qwen2.5VL[9]and RoboBrain2.0[8]trained with GRPO[62]on the Euclid30K dataset.",
                "position": 2703
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x7.png",
                "caption": "Figure 7:The response and final answer for Qwen2.5VL-7B[9]and Qwen2,5VL-7B-Eculid in Omni3DBech[53].",
                "position": 2715
            }
        ]
    },
    {
        "header": "Appendix EFurther Analysis of Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24473/x8.png",
                "caption": "Figure 8:The response and final answer for Qwen2.5VL-7B[9]and Qwen2,5VL-7B-Eculid in Omni3DBech[53].",
                "position": 2772
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x9.png",
                "caption": "Figure 9:The response and final answer for RoboBrain2,0-7B[8]and RoboBrain2.0-7B-Eculid in VSIBench[72].",
                "position": 2775
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x10.png",
                "caption": "Figure 10:The response and final answer for RoboBrain2,0-7B[8]and RoboBrain2.0-7B-Eculid in VSIBench[72].",
                "position": 2778
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x11.png",
                "caption": "Figure 11:The response and final answer for RoboBrain2,0-7B[8]and RoboBrain2.0-7B-Eculid in SuperClver[42].",
                "position": 2781
            },
            {
                "img": "https://arxiv.org/html/2509.24473/x12.png",
                "caption": "Figure 12:The response and final answer for RoboBrain2,0-7B[8]and RoboBrain2.0-7B-Eculid in SuperClver[42].",
                "position": 2784
            }
        ]
    },
    {
        "header": "Appendix FDiscussion of Future Work",
        "images": []
    }
]