[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17206/figures/motivation_curve.png",
                "caption": "Figure 1:Motivation: Injecting a Gaussian noise token embedding before the prompt embeddings of Qwen-4B-Base enables substantial gains in pass@k accuracy by merely sampling in the Gaussian, despite using greedy decoding for each candidate.",
                "position": 60
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x1.png",
                "caption": "(a)Latent space learning via VAE.The encoder maps the mean-pooled embedding of a question–answer pair into a Gaussian latent space, where distinct regions correspond to different reasoning strategies. The decoder reconstructs the embedding, enabling structured exploration through latent sampling.",
                "position": 63
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x1.png",
                "caption": "(a)Latent space learning via VAE.The encoder maps the mean-pooled embedding of a question–answer pair into a Gaussian latent space, where distinct regions correspond to different reasoning strategies. The decoder reconstructs the embedding, enabling structured exploration through latent sampling.",
                "position": 66
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x2.png",
                "caption": "(b)Latent-guided inference.During generation, a sampled latent variablezzis decoded into prefixes and prepended to the input prompt. The prefixes modulate the LLM’s internal state, steering its reasoning trajectory and shaping the style and structure of the generated response.",
                "position": 71
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17206/x3.png",
                "caption": "(a)PCA of Generated Prefixes",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x3.png",
                "caption": "(a)PCA of Generated Prefixes",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x4.png",
                "caption": "(b)t-SNE of Generated Prefixes",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x5.png",
                "caption": "(c)PCA of Latents",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2512.17206/x6.png",
                "caption": "(d)t-SNE of Latents",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2512.17206/figures/refcoco.png",
                "caption": "Figure 4:Pass@32 curves on the RefCOCO datasets.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2512.17206/",
                "caption": "Figure 5:Qualitative results on RefCOCO dataset. From left to right: input image with the ground-truth bounding box, prediction from Qwen2.5VL-3B (greedy decoding), and prediction from our method, Qwen2.5VL-3B (greedy decoding) with a randomly sampled latent. The referring expressions for the top and bottom rows aretrain closest to the bottomanda zebra standing behind two other zebras, with only its mane and rear showing, respectively.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2512.17206/figures/output.png",
                "caption": "Figure 6:Performance curves of the GRPO baseline and the proposed latent variants during training. Latent variants perform more thorough exploration in early stages of training and then shift toward exploitation in the latter stages, resulting in a performance curve that gradually overtakes baselines.",
                "position": 808
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]