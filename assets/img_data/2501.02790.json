[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Main Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02790/x1.png",
                "caption": "Figure 1:Overview of training and utilizing our segment-level reward model. Numerics in the plot are artificial.\nIn the figure, each text segment has a different color, and its starting word isunderscored.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x2.png",
                "caption": "Figure 2:Examples of data in our reward training dataset, motivating us to use Average as the aggregation functionf‚Å¢(‚ãÖ)ùëì‚ãÖf(\\cdot)italic_f ( ‚ãÖ ).\nIt is clear that the average quality of the chosen responses is better than the rejected ones, as they contain some key phrases (highlighted in red) for responding to the given prompt.\nWe note that the choice off‚Å¢(‚ãÖ)ùëì‚ãÖf(\\cdot)italic_f ( ‚ãÖ )ought to be task and dataset specific, as discussed inYang et¬†al. (2023).",
                "position": 234
            }
        ]
    },
    {
        "header": "3Related Work",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02790/x3.png",
                "caption": "Figure 3:Examples of dense reward assignment for text sequences encountered in PPO training, comparing our segment-level reward model and the recent token-level design on normal text (Top) and text with verbosity/repetition (Bottom).\nDarker color indicates higher reward.\nIn the bottom half, repeated sentences areunderlined.",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x4.png",
                "caption": "(a)Avg. Seg. Len (token)",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x5.png",
                "caption": "(a)Avg. Seg. Len (token)",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x6.png",
                "caption": "(b)AlpacaEval 2 (LC)",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x7.png",
                "caption": "(c)AlpacaEval 2 (WR)",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x8.png",
                "caption": "(d)Arena-Hard",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2501.02790/x9.png",
                "caption": "(e)MT-Bench",
                "position": 1047
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlgorithm Box",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DComputation of Location-Aware Reward Normalizers via Regression",
        "images": []
    },
    {
        "header": "Appendix EMore on the Reward Normalizers in PPO Training",
        "images": []
    }
]