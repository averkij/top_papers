[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23473/figures/github.png",
                "caption": "",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2510.23473/figures/hf-logo.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23473/x1.png",
                "caption": "Figure 1:Overall Performance of Video-Thinker",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x2.png",
                "caption": "Figure 2:Video-Thinker integrates “grounding” and “captioning” capabilities throughout the reasoning process using end-to-end reinforcement learning.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Think with Videos: From Data Synthesis to Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23473/x3.png",
                "caption": "Figure 3:Data synthesis pipeline of Video-Thinker-10K where the data distribution is depicted in Figure5in AppendixB.",
                "position": 176
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23473/x4.png",
                "caption": "Figure 4:An example of Video-Thinker-7B’s reasoning output on CG-Bench-Reasoning dataset.",
                "position": 280
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverall Algorithm of Video-Thinker",
        "images": []
    },
    {
        "header": "Appendix BData Distribution over source datasets in Section3.1",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23473/x5.png",
                "caption": "Figure 5:The data distribution of our Video-Thinker-10K dataset.",
                "position": 1640
            }
        ]
    },
    {
        "header": "Appendix CExperiment Configuration",
        "images": []
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    },
    {
        "header": "Appendix EExperimental Verification of Grounding and Captioning Capabilities",
        "images": []
    },
    {
        "header": "Appendix FAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23473/x6.png",
                "caption": "Figure 6:An example of Video-Thinker-7B’s reasoning output on Video-Holmes dataset",
                "position": 2258
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x7.png",
                "caption": "Figure 7:An example of Video-Thinker-7B’s reasoning output on Video-Holmes dataset",
                "position": 2261
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x8.png",
                "caption": "Figure 8:An example of Video-Thinker-7B’s reasoning output on VRBench dataset",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x9.png",
                "caption": "Figure 9:An example of Video-Thinker-7B’s reasoning output on VRBench dataset",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x10.png",
                "caption": "Figure 10:An example of Video-Thinker-7B’s reasoning output on CG-Bench dataset",
                "position": 2270
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x11.png",
                "caption": "Figure 11:An example of Video-Thinker-7B’s reasoning output on CG-Bench dataset",
                "position": 2273
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x12.png",
                "caption": "Figure 12:An example of Video-Thinker-7B’s reasoning output on CG-Bench dataset",
                "position": 2276
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x13.png",
                "caption": "Figure 13:An example of Video-Thinker-7B’s reasoning output on Video-Holmes dataset",
                "position": 2279
            },
            {
                "img": "https://arxiv.org/html/2510.23473/x14.png",
                "caption": "Figure 14:An example demonstrates Video-R1-7B’s inability to follow instructions for generating temporal grounding content within<time></time>tags, thereby illustrating the rationale behind the statement in Section4.3: “Note that Video-R1 is excluded from this evaluation due to its inability to follow our prompt to generate temporal annotations within our templates.”.",
                "position": 2282
            }
        ]
    },
    {
        "header": "Appendix GCases",
        "images": []
    },
    {
        "header": "Appendix HUse of LLMs",
        "images": []
    }
]