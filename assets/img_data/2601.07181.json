[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07181/herobanners.png",
                "caption": "Figure 1:Overview and evaluation of ShowUI-Aloha.Left:Human-taught demonstrations are converted into grounded action traces, which are lifted into trace- and prompt-guided plans and executed on real desktop environments.Middle:Qualitative comparisons across representative multi-step desktop tasks show that Aloha avoids common failure modes of unguided agents, such as context drift, unsupported actions, and stuck states.Right:Quantitative comparison on 361 OSWorld-style tasks executed on Windows and macOS demonstrates that human-guided planning enables higher end-to-end task success than existing autonomous and agentic baselines.",
                "position": 61
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07181/pipeline_4_step.png",
                "caption": "Figure 2:Overview of the Aloha paradigm for GUI agents.\nInstead of relying on trial-and-error interaction, Aloha leverages a single human demonstration to distill reusable task guidance, which is then consistently applied to new task variants and interface layouts, enabling stable and generalizable execution across changing interfaces.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07181/pipeline.png",
                "caption": "Figure 3:Overview of the Aloha workflow.Human demonstrations are recorded and converted into structured action traces. The actor uses the task prompt and screenshots to generate an execution plan, while the executor performs each action on the computer.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2601.07181/recorder.png",
                "caption": "Figure 4:User-facing interface of the ShowUI-Aloha Recorder.\nThe recorder presents a minimal floating control panel (top right) for starting and stopping captures, while a modal dialog allows users to name or rename each recording with clear constraints on valid characters. These utilities support organized, large-scale data collection and facilitate downstream processing.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2601.07181/parser_1.png",
                "caption": "Figure 5:Raw screen frames captured by the Aloha Recorder.These consecutive frames illustrate the natural visual trajectory present\nin human desktop demonstrations. The Recorder captures full-resolution\nframes at high frequency, preserving the fine-grained cursor dynamics,\nUI transitions, and subtle motion patterns that are essential for downstream\naction cleaning and trace generation.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2601.07181/parser_2.png",
                "caption": "",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2601.07181/parser_3.png",
                "caption": "",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2601.07181/raw_log.png",
                "caption": "Figure 6:Raw action log captured by the Aloha Recorder of above frames.The recorder logs dense low-level input events such as mouse movements,\nmouse down/up pairs, drags, and keystrokes with high-frequency timestamps.\nThis raw stream is noisy, redundant, and unstructured, reflecting natural\nhuman behavior and motivating the need for the Action Cleaning stage in\nthe Aloha Learner.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2601.07181/raw_vs_grouped.png",
                "caption": "Figure 7:From raw events to grouped interaction primitives.The recorder logs dense, noisy, and highly redundant event streams (left).\nAloha Learner consolidates these signals into higher-level interaction primitives (right).",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2601.07181/crop_vs_full.png",
                "caption": "Figure 8:Example of the zoomed-in and marked crop (left) and full-screen context (right) used by the trace generator.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2601.07181/action_grouped.png",
                "caption": "Figure 9:From grouped actions to semantic teaching traces.After low-level events are merged into coherent interaction primitives (left),\nAloha Learner uses a vision–language model to reason over the marked screenshots,\nUI context, and recent action history to produce high-level semantic descriptions (right).\nEach step includes anObservationof the UI state, aThinkfield with\nbrief reasoning, a normalizedActionsuch as “click the File menu” or\n“dragpikachu.pnginto dir2”, and anExpectationdescribing how\nthe interface should change. These semantic traces capture user intent and form\nthe core supervision for downstream planning and execution.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2601.07181/action_semantic.png",
                "caption": "",
                "position": 261
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07181/git_trajectory_comparison.png",
                "caption": "Figure 10:Qualitative comparison on a Git update workflow.Aloha follows the demonstrated procedure for propagating an edit from a scratch folder into a Git repository (top row): it modifiesemergency_fix.txt, navigates throughDocuments→\\rightarrowGitHub→\\rightarrowGUI_Test, replaces the tracked file in the repository folder, and then issues a commit in GitHub Desktop.\nIn contrast, the unguided agent (bottom row) correctly edits and saves the local file, but then directly opens GitHub Desktop without first copying it into the repository path. Because the repo contains no changed files, it repeatedly tries to commit or push in an empty state and becomes stuck, illustrating a lack of procedural knowledge about intermediate file-management steps that Aloha inherits from human teaching traces.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2601.07181/multi_task.png",
                "caption": "Figure 16:Additional examples of Aloha executing complex real-world tasks.From left to right: (1) automated air-ticket booking involving multi-step UI navigation\nand structured form filling; (2) advanced Excel operations such as matrix transposition\nand cell-range manipulation; and (3) batch editing of slide backgrounds in PowerPoint.\nThese diverse tasks demonstrate Aloha’s ability to generalize beyond simple click-and-type\npatterns and reliably follow high-level workflows across heterogeneous applications.",
                "position": 782
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]