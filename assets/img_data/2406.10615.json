[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.10615/x1.png",
                "caption": "Figure 1:Left:Sample efficiency of SGRv2. We evaluate SGR and SGRv2 on 26 RLBench tasks, with demonstration numbers ranging from 100 to 5. Results indicate that, owing to the locality of SGRv2, it exhibits exceptional sample efficiency, with its success rate declining by only about 10%.Top Right:Overview of simulation results. We test SGRv2 on 3 benchmarks, consistently outperforming the baselines.Bottom Right:Tasks of the 3 simulation benchmarks.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2406.10615/x2.png",
                "caption": "",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/open_microwave_crop.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/ycb.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/coffee_D1-0002.png",
                "caption": "",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.10615/x3.png",
                "caption": "Figure 2:SGRv2 Architecture.Built upon SGR, SGRv2 integrates locality into its framework through four primary designs: an encoder-decoder architecture for point-wise features, a strategy for predicting relative target position, a weighted average for focusing on critical local regions, and a dense supervision strategy (not shown in the figure). This illustration specifically represents thewater plantstask. For simplicity in the visualization, we omit the proprioceptive data that is concatenated with the RGB of the point cloud before being fed into the geometric branch.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/point_weight_new1_1.jpeg",
                "caption": "Figure 3:Emergent Capabilities.We visualize the point-specific weights and find that the points with high weights (in red) align with the objectâ€™s affordances. Left:toilet seat up. Right:open microwave.",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/point_weight_new1_2.jpeg",
                "caption": "",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/point_weight_new2_1.jpeg",
                "caption": "",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/point_weight_new2_2.jpeg",
                "caption": "",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/tidy_up_the_table_crop.jpg",
                "caption": "Figure 4:Left:Real-robot long-horizon tasks.Right:Success rate (%) of multi-task agents on real-robot tasks. We collect 8 demonstrations and evaluate 10 episodes for each task.",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/make_coffee_crop.jpg",
                "caption": "",
                "position": 761
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASimulation Task Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.10615/x4.png",
                "caption": "Figure 5:Simulation Tasks.Our simulation experiments encompass 26 tasks (1-26) from RLBench, 4 tasks (27-37, where 30-37 are 8 different YCB[63]objects of taskPick Single YCB) from ManiSkill2, and 7 tasks (38-44) from MimicGen.",
                "position": 1752
            }
        ]
    },
    {
        "header": "Appendix BSGRv2 Details",
        "images": []
    },
    {
        "header": "Appendix CReal-Robot Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/move_color_cup.jpg",
                "caption": "Figure 6:Real-robot generalization task.",
                "position": 1947
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    },
    {
        "header": "Appendix ERobustness to Visual Distractors",
        "images": [
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/meat_off_grill_distractors_100.jpg",
                "caption": "Figure 7:RLBench tasks with visual distractors.",
                "position": 2530
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/phone_on_base_distractors_154.jpg",
                "caption": "",
                "position": 2539
            },
            {
                "img": "https://arxiv.org/html/2406.10615/extracted/5881806/figures/push_buttons_distractors_74.jpg",
                "caption": "",
                "position": 2544
            }
        ]
    },
    {
        "header": "Appendix FDetailed Results",
        "images": []
    }
]