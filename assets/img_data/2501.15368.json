[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15368/extracted/6156374/figures/performance.png",
                "caption": "Figure 1:Evaluation across image, video, and audio modalities.(Left)Baichuan-Omni-1.5¬†covers more modalities than Qwen2 VL[142]and outperforms the current leading omni-modal model, VITA-1.5[45]and MiniCPM-o 2.6[165].(Right)Average scores across benchmarks for all modalities. All the scores are normalized byxnorm=(x‚àíxmin+10)/(xmax‚àíxmin+10)subscriptùë•normùë•subscriptùë•min10subscriptùë•maxsubscriptùë•min10x_{\\text{norm}}=(x-x_{\\text{min}}+10)/(x_{\\text{max}}-x_{\\text{min}}+10)italic_x start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT = ( italic_x - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ) / ( italic_x start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ).",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15368/x1.png",
                "caption": "Figure 2:Architecture of Baichuan-Omni-1.5¬†.Our model is designed to process both pure text/audio inputs and combinations of video/image with text/audio. When generating audio, the Baichuan-Omni-1.5 LLM Decoder alternately predicts text tokens and audio tokens. The audio tokens are then decoded by the Audio Decoder to produce the final audio.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3Baichuan-Omni-1.5",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15368/extracted/6156374/figures/data_example.png",
                "caption": "Figure 3:Pretrain Data illustration of Baichuan-Omni-1.5¬†.We construct an extensive omni-modal dataset, including text, image-text, video-text, audio-text, and their interactions. Our collection also contains interleaved image-audio-text and video-audio-text data.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2501.15368/x2.png",
                "caption": "Figure 4:Audio tokenizer and audio decoder based on flow matching model.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2501.15368/x3.png",
                "caption": "Figure 5:Training Pipeline of Baichuan-Omni-1.5. The pretraining phase is divided into three stages to incrementally incorporate vision and audio into the LLM while relieving modality conflicts. Stage 1 focuses on image-text training, which extends an LLM to process and understand visual input. Stage 2 extends an LLM pre-trained on visual data to understand audio input in end-to-end manner by incorporating our Baichuan-Audio-Tokenizer, a newly introduced audio embedding layers and an independent audio head. Stage 3 focuses on training Baichuan-Omni-1.5¬†using high-quality cross-modal interaction datasets encompassing image-audio-text and video-audio-text format, and extends the maximum sequence length to 64k to support long audio and video stream. Stage 4 enhances the model‚Äôs instruction following and audio capabilities through supervised fine-tuning with omni-modal data. Stage 4.1: Freeze the Audio Head using omni-modal understanding data to boost modality interactivity and multitasking comprehension. Stage 4.2: Activate only the Audio Head and Audio Embed layer, with audio generation data to improve speech generation capabilities.",
                "position": 443
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]