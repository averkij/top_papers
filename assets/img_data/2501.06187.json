[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x2.png",
                "caption": "Figure 2:Dataset collection pipeline for video personalization.We construct our training dataset using video and caption pairs through three steps. First, we identify three categories of entity words from the caption: subject, object, and background. Next, we use these entity words to localize and segment the target subjects and objects in three selected video frames. Finally, we extract a clean background image by removing the subjects and objects from the middle frame.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x3.png",
                "caption": "Figure 3:Model architecture.Our model is a latent DiT[50], where we first encode a video into video tokens and denoise them with a deep cascade of DiT blocks in the latent space. Each DiT block includes an additional cross-attention operation with personalization embeddingsf=Concat‚Å¢(f1,‚Ä¶,fn,‚Ä¶,fN)ùëìConcatsubscriptùëì1‚Ä¶subscriptùëìùëõ‚Ä¶subscriptùëìùëÅf=\\textrm{Concat}(f_{1},\\dots,f_{n},\\dots,f_{N})italic_f = Concat ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , ‚Ä¶ , italic_f start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ), wherefnsubscriptùëìùëõf_{n}italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPTfuses the embeddings of both the reference imagexnsubscriptùë•ùëõx_{n}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPTand the corresponding entity wordcnsubscriptùëêùëõc_{n}italic_c start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. Each square in the figure represents a 1-D token.",
                "position": 162
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x4.png",
                "caption": "Figure 4:Test sample inMSRVTT-Personalization.We present a comprehensive video personalization benchmark. Our benchmark supports various modes, including face conditioning, single or multiple subjects conditioning, and foreground and background conditioning.",
                "position": 270
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x5.png",
                "caption": "Figure 5:Qualitative comparison onMSRVTT-Personalization.We use a single reference image to each model for a fair comparison. Compared to existing methods, our results closely match the input text prompt and reference subjects while exhibiting natural motion and pose variations.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x6.png",
                "caption": "Figure 6:Qualitative results of the ablation study.From top to bottom, we show that 1)Video Alchemistachieves better subject fidelity using DINOv2[47]as the image encoder; 2) it correctly binds the conditional image and entity word with the usage of word tokens; 3) it mitigates thecopy-and-pasteeffect and synthesizes text-aligned videos via the proposed data augmentation. The reference image is synthesized by DALL¬∑E 3[3].",
                "position": 764
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Training Datasets and Augmentations",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x7.png",
                "caption": "Figure 7:Prompt template for retrieving the entity words.",
                "position": 1959
            },
            {
                "img": "https://arxiv.org/html/2501.06187/extracted/6124560/figures/sources/word_cloud.png",
                "caption": "Figure 8:Word cloud of the entity words.We randomly sample 10k videos from our training dataset and plot the word cloud of the retrieved subject and object entity words.",
                "position": 2002
            }
        ]
    },
    {
        "header": "Appendix BDetails of Model Architecture, Training, and Inference",
        "images": []
    },
    {
        "header": "Appendix CMore Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06187/x8.png",
                "caption": "Figure 9:Additional results of multi-subject open-set personalization.",
                "position": 2481
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x9.png",
                "caption": "Figure 10:Additional results of multi-subject open-set personalization.",
                "position": 2484
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x10.png",
                "caption": "Figure 11:Additional results of multi-subject open-set personalization.",
                "position": 2487
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x11.png",
                "caption": "Figure 12:Additional results of multi-subject open-set personalization.",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x12.png",
                "caption": "Figure 13:Same text prompt with different reference images ofperson.",
                "position": 2501
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x13.png",
                "caption": "Figure 14:Same text prompt with different reference images ofdog.",
                "position": 2504
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x14.png",
                "caption": "Figure 15:Qualitative comparison on the conditional subject ofdog.",
                "position": 2515
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x15.png",
                "caption": "Figure 16:Qualitative comparison on the conditional subject ofcat.",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x16.png",
                "caption": "Figure 17:Qualitative comparison on the conditional subject ofcar.",
                "position": 2521
            },
            {
                "img": "https://arxiv.org/html/2501.06187/x17.png",
                "caption": "Figure 18:Qualitative comparison on the conditional subject ofdinosaur toy.",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    }
]