[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07845/figures/fig1.jpeg",
                "caption": "Figure 1:Recurrent-Depth VLA.(Left) Previous reasoning VLAs (e.g., ThinkAct, MolmoAct) generate explicit reasoning tokens in output space, requiring expensive autoregressive decoding. (Center) Our approach performs iterative refinement entirely in latent representation space, bypassing token generation overhead. (Right) RD-VLA achieves comparable performance to autoregressive reasoning baselines on LIBERO-10 while being substantially faster due to the efficiency of latent reasoning with adaptive compute.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2602.07845/figure2.png",
                "caption": "Figure 2:Recurrent-Depth VLA Architecture.The Prelude (P) grounds learned queries via cross-attention to mid-layer VLM features. The weight-tied Recurrent Core (R) iteratively refines a noisy latent scratchpad overKKiterations, cross-attending to final-layer VLM representations and proprioception. The Coda (C) decodes the converged state into actions. Recurrence depthKKadapts dynamically at inference based on task complexity.",
                "position": 110
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07845/figures/fig3.png",
                "caption": "Figure 3:Case study for adaptive computation.In a LIBERO rollout, the model dynamically selects different numbers of iterations before terminating, depending on the execution state. It uses fewer iterations (7–9) at steps 1 and 30, which correspond to simpler motions like navigation and placing, and more iterations (about 14) at steps 10 and 25, where the actions are more complex, such as grasping.",
                "position": 150
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": []
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07845/figures/fig4.png",
                "caption": "Figure 4:Performance across LIBERO benchmarks for different numbers of recurrences. All task categories show consistent improvement with increased computational depth, with models converging between 8–12 iterations on average.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2602.07845/figures/fig5.png",
                "caption": "Figure 5:Performance on selected 5 Long tasks across recurrence steps. Each task exhibits distinct convergence behavior: Task 4 jumps from 6% at iteration 1 to nearly 80% at iteration 2, while Task 5 remains at 0% through iteration 2 and only reaches∼\\sim70% at iteration 3. This demonstrates the task-dependent and emergent adaptive behavior of our model.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2602.07845/figures/thresholds/fig6.png",
                "caption": "Figure 6:Histograms of zero-shot, per-token adaptive exits based on KL divergence between consecutive steps (τ=10−4\\tau=10^{-4}). The distributions show task-dependent iteration counts: Spatial tasks require more iterations (μ=11.8\\mu=11.8) than Object (μ=9.8\\mu=9.8) or Goal (μ=9.0\\mu=9.0) tasks, reflecting their relative complexity.",
                "position": 885
            },
            {
                "img": "https://arxiv.org/html/2602.07845/x1.png",
                "caption": "“Put cube into bowl”",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2602.07845/cube.jpeg",
                "caption": "“Put cube into bowl”",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2602.07845/wipe.jpeg",
                "caption": "“Wipe the dish”",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2602.07845/towel.jpeg",
                "caption": "“Fold towel”",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2602.07845/toast.jpeg",
                "caption": "“Toast the bread”",
                "position": 1064
            }
        ]
    },
    {
        "header": "VDiscussion and Limitations",
        "images": []
    },
    {
        "header": "VIConclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]