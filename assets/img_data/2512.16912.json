[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries and Technical Background",
        "images": []
    },
    {
        "header": "3Clipping and Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16912/x1.png",
                "caption": "Figure 1:Independent trials overQwen2.5-Math-7Bon theMATH500validation set. For performance validation subpanels (Left & Middle), each color represents a different run; the bold line shows the smoothed trajectory, and the faint line of the same color shows the corresponding raw individual run. All later figures follow the same plotting convention. Unclipped training (Left); clipped training (Middle); and clipping activation ratio during training (Right).",
                "position": 483
            }
        ]
    },
    {
        "header": "4Clipping and Policy Entropy",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16912/x2.png",
                "caption": "Figure 2:Policy entropy evolution ofQwen2.5-Math-7Bunder random-reward training, with results for unclipped training (Left) and clipped training (Middle); Unclipped training withR1-Distill-Llama-8B, an example that leads to the gradient explosion (Right).",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2512.16912/x3.png",
                "caption": "Figure 3:Results on AIME training set onQwQ-32B(Left),R1-Distill-Llama-8B(Middle-L),Qwen2.5-Math-7B(Middle-R). With one specific example that shows entropy minimization would lead to sub-optimal policy under noisier and more difficult training environment (Right).",
                "position": 604
            }
        ]
    },
    {
        "header": "5Reward Misalignment: Who can Benefit from Random Rewards?",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16912/x4.png",
                "caption": "Figure 4:Results ofQwen2.5-Math-1.5Bunder clipped training (Left); results of R1-Distill-Llama-8B under clipped training (Middle); percentage improvement (averaged over six independent runs) for different models under the same training and validation setup (Right).",
                "position": 681
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16912/x5.png",
                "caption": "Figure 5:All experiments follow the same setup as Figure1, varying the thresholdÎµ\\varepsilonwith six independent runs for each setting: trials with clipping ratioÎµ=0.1\\varepsilon=0.1(Left); trials with clipping ratioÎµ=0.15\\varepsilon=0.15(Middle); and the ratio of clipping activations acrossÎµâˆˆ{0.2,0.15,0.1}\\varepsilon\\in\\{0.2,0.15,0.1\\}(Right).",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2512.16912/x6.png",
                "caption": "Figure 6:Smaller group size.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2512.16912/x7.png",
                "caption": "Figure 7:UnclippedQwen2.5-Math-7Bon the hard AIME dataset: independent runs following from the setup in Figure3(Left); corresponding policy entropy dynamics during unclipped training (Middle); joint evolution of model performance and policy entropy for an example trial (Right).",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2512.16912/x8.png",
                "caption": "Figure 8:Visualization of policy action distributions over 12 promptğ±i\\mathbf{x}_{i}. Each subplot displays the sorted log-probability ofÏ€â€‹(ğ²âˆ£ğ±i)\\pi(\\mathbf{y}\\mid\\mathbf{x}_{i})for 64 sampled responses from each promptğ±i\\mathbf{x}_{i}. Columns 1-2 (blue) correspond to promptsğ±i\\mathbf{x}_{i}withÎ¦(Ï€(â‹…âˆ£ğ±i))>0\\Phi(\\pi(\\cdot\\mid\\mathbf{x}_{i}))>0, while Columns 3-4 (orange) correspond to prompts withÎ¦(Ï€(â‹…âˆ£ğ±i))<0\\Phi(\\pi(\\cdot\\mid\\mathbf{x}_{i}))<0. As discussed inRemarkËœ4.2, the entropy increase under unclipped training can occur only for the skewed one shown in Columns 3-4.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2512.16912/x9.png",
                "caption": "Figure 9:Simulation of policy entropy evolution over unclipped GRPO training. Each panel includes the result with 10 independent trails. Flat (relatively less-skewed) policyÏ€\\piinitialization (Left); Skewed policyÏ€\\piinitialization (Right).",
                "position": 1787
            }
        ]
    },
    {
        "header": "Appendix CTheoretical Analysis",
        "images": []
    }
]