[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02547/x1.png",
                "caption": "",
                "position": 361
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary: From LLM RL to Agentic RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02547/x2.png",
                "caption": "Figure 2:Paradigm shift from LLM-RL to agentic RL.",
                "position": 681
            }
        ]
    },
    {
        "header": "3Agentic RL: The model capability perspective",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02547/x3.png",
                "caption": "Figure 3:The dynamic interaction process between agentic LLMs and the environment.",
                "position": 2280
            },
            {
                "img": "https://arxiv.org/html/2509.02547/x4.png",
                "caption": "Figure 4:A summary of the overall six aspects where RL empowers agentic LLMs. Note that the representative methods listed here are not exhaustive; please refer to our main text.",
                "position": 2283
            },
            {
                "img": "https://arxiv.org/html/2509.02547/x5.png",
                "caption": "Figure 5:The development of agentic tool use. Note that we only select a small bunch of representative works here to reflect the progress.",
                "position": 2314
            }
        ]
    },
    {
        "header": "4Agentic RL: The Task Perspective",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02547/x6.png",
                "caption": "Figure 6:The evolution tree of RL for domain-specific agents.",
                "position": 2641
            },
            {
                "img": "https://arxiv.org/html/2509.02547/figures/hf-logo.png",
                "caption": "Table 6:A summary of RL methods for mathematical reasoning agents.",
                "position": 3555
            },
            {
                "img": "https://arxiv.org/html/2509.02547/figures/hf-logo.png",
                "caption": "Table 7:A summary of methods for GUI agents, categorized by training paradigm and environment complexity.",
                "position": 3813
            },
            {
                "img": "https://arxiv.org/html/2509.02547/figures/hf-logo.png",
                "caption": "Table 8:A summary of reinforcement learning and evolution paradigms in LLM-based Multi-Agent Systems. “Dynamic” denotes whether the multi-agent system is task-dynamic,i.e., processes different task queries with different configurations (agent count, topologies, reasoning depth, prompts,etc). “Train” denotes whether the method involves training the LLM backbone of agents.",
                "position": 4068
            }
        ]
    },
    {
        "header": "5Enviroment and Frameworks",
        "images": []
    },
    {
        "header": "6Open Challenges and Future Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]