[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00612/extracted/6404636/iid-vs-dedup.png",
                "caption": "Figure 1:IID Evaluations vs. Novelty-Centric Evaluations.In the IID evaluation, left, both training and test data are drawn from the same distribution, resulting in significant overlap in examples in each set. In the novelty-centric version, right, no test example is allowed to be too similar to any given training example. We argue that this conceptualization more closely mirrors desired behavior for GenAI evaluations, where generalization is expected to connote the ability to respond well on totally novel inputs.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Background: Revisiting Benchmarking",
        "images": []
    },
    {
        "header": "3Reconsidering Generalization for GenAI",
        "images": []
    },
    {
        "header": "4Leakage Case Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00612/extracted/6404636/linear-vs-parallel.png",
                "caption": "Figure 2:Comparing sequential and parallelized evaluation structures.In the traditional research structure, top, each new idea is evaluated in a linear sequence that typically requires several months for a single pass. The parallelized structure, bottom, allows hundreds or thousands of approaches to be simultaneously.",
                "position": 326
            }
        ]
    },
    {
        "header": "5Evaluations Aiming to Avoid Leakage",
        "images": []
    },
    {
        "header": "6AI Competitions as Structural Solution",
        "images": []
    },
    {
        "header": "7Recommendations for the Field",
        "images": []
    },
    {
        "header": "8Alternative Views",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]