[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14240/x1.png",
                "caption": "Figure 1:Comparison of LiveResearchBench with existing deep research benchmarks.{{date}}will be replaced by the evaluation date. LiveResearchBench is explicitlyuser-centric,unambiguous, andtime-varying, featuring multi-faceted, search-intensive tasks across diverse domains.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LiveResearchBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14240/x2.png",
                "caption": "Figure 2:Domain distribution and task coverage of LiveResearchBench.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x3.png",
                "caption": "Figure 3:Six-stage data generation pipeline for LiveResearchBench. We begin with user interviews and surveys to capture realistic research needs, followed by expert-drafted research questions aligned with the identified domains. Clarification questions from frontier LLMs help ensure unambiguous scope, which human experts then refine into finalized queries. Finally, GPT-5 generates checklists that decompose each query into verifiable unit tests, enabling consistent coverage evaluation across systems. The quality of these checklists is further validated by human experts (Figure4).",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x4.png",
                "caption": "Figure 4:Five-stage data verification pipeline.\nResearch questions and checklists are first independently assessed by expert annotators under detailed guidelines.\nTwo rounds of quality control then ensure agreement across annotators and detect systematic issues.\nFinally, a separate group of experts conducts verification and cross-checking to resolve conflicts and finalize the dataset.",
                "position": 331
            }
        ]
    },
    {
        "header": "4DeepEval: A Comprehensive Evaluation Suite for Deep Research",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14240/x5.png",
                "caption": "Figure 5:Illustration of rubric tree–based evaluation of citation accuracy.\nCompared to evaluating all claim–URL pairs individually, the rubric tree groups claims by link, which significantly reduces evaluation costs while enabling finer-grained categorization of citation errors: invalid links (E1), irrelevant links (E2), and unsupported claims (E3).",
                "position": 416
            }
        ]
    },
    {
        "header": "5Main Results and analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14240/x6.png",
                "caption": "Figure 6:Distribution of report lengths across systems and tasks.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x7.png",
                "caption": "Figure 7:Analysis depth (win rate over Open Deep Research). Most systems collect and organize information but struggle to synthesize deeper insights.",
                "position": 688
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix BAlignment of LLM Judges with Human Experts",
        "images": []
    },
    {
        "header": "Appendix CDeerflow+",
        "images": []
    },
    {
        "header": "Appendix DLiveResearchBench Demonstration",
        "images": []
    },
    {
        "header": "Appendix EError Pattern Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14240/x8.png",
                "caption": "",
                "position": 2488
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x9.png",
                "caption": "",
                "position": 2491
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x10.png",
                "caption": "",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x11.png",
                "caption": "",
                "position": 2497
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x12.png",
                "caption": "",
                "position": 2500
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x13.png",
                "caption": "",
                "position": 2503
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x14.png",
                "caption": "",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2510.14240/x15.png",
                "caption": "",
                "position": 2509
            }
        ]
    },
    {
        "header": "Appendix FInformation on the Annotation Team",
        "images": []
    },
    {
        "header": "Appendix GUser Survey",
        "images": []
    },
    {
        "header": "Appendix HHuman Verification Guidelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14240/img_to_use/questions_grouped_models_average.png",
                "caption": "",
                "position": 2799
            }
        ]
    },
    {
        "header": "Appendix IResults Breakdown",
        "images": []
    }
]