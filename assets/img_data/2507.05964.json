[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/visual_abs.png",
                "caption": "Figure 1:Compared to standard LoRA single view fine-tuning,T-LoRAreduces overfitting related to position and background, enabling more versatile and enriched generation.",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05964/x1.png",
                "caption": "(a)LoRA",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2507.05964/x1.png",
                "caption": "(a)LoRA",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2507.05964/x2.png",
                "caption": "(b)Vanilla T-LoRA",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2507.05964/x3.png",
                "caption": "(c)T-LoRA",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/TLoRA_motivation.png",
                "caption": "Figure 3:Motivational Experiment.The figure presents the results of fine-tuning the SD-XL model with LoRA across a fixed interval of timesteps. We find that focusing on the noisiest timesteps leads to rapid overfitting, particularly affecting the positioning and background of generated images. In contrast, shifting to earlier timesteps results in outputs that are better aligned with text prompts, yielding richer and more flexible generations. However, completely removing the influence of the noisiest timesteps (t‚àà[800;1000]ùë°8001000t\\in[800;1000]italic_t ‚àà [ 800 ; 1000 ]) is not feasible, as they are crucial for preserving concept fidelity.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/sv_ca_lora.png",
                "caption": "(a)LoRA",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/sv_ca_lora.png",
                "caption": "(a)LoRA",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/sv_sa_lora.png",
                "caption": "",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/sv_ca_ortholora.png",
                "caption": "(b)Ortho-LoRA",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/sv_sa_ortholora.png",
                "caption": "",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/inits.png",
                "caption": "Figure 5:Results for sixOrtho-LoRAinitialization variants based on principal, middle, and last SVD components of the original weightsWùëäWitalic_W, and a random matrixRùëÖRitalic_R. Higher singular values correlate with overfitting, while too small values can slow down the training. Initialization with last singular values fromRùëÖRitalic_Ris optimal across most ranks.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2507.05964/x4.png",
                "caption": "Figure 6:Comparison between singular values distribution from the model‚Äôs self-attention layers (size 1280) and from a random matrixRùëÖRitalic_Rof the same size, with a close-up on the smallest singular values.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2507.05964/x4.png",
                "caption": "Figure 6:Comparison between singular values distribution from the model‚Äôs self-attention layers (size 1280) and from a random matrixRùëÖRitalic_Rof the same size, with a close-up on the smallest singular values.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2507.05964/x5.png",
                "caption": "Figure 7:Generation outputs for Ortho-LoRA and T-LoRA withrm‚Å¢i‚Å¢nsubscriptùëüùëöùëñùëõr_{min}italic_r start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPTset to 25% and 50% of the full rankr=64ùëü64r=64italic_r = 64. Ortho-LoRA generally exhibits poor alignment with the text and a high degree of reproduction of training images. In contrast, T-LoRA significantly enhances alignment with the text. Notably, T-LoRA at 50% maintains high image fidelity, whereas T-LoRA at 25% often struggles to accurately reproduce the concept.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2507.05964/extracted/6605680/images/all_methods_arxiv.png",
                "caption": "Figure 8:Generation examples for T-LoRA alongside other diffusion model customization baselines.",
                "position": 519
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CLimitations",
        "images": []
    }
]