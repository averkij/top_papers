[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13550/x1.png",
                "caption": "",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13550/x2.png",
                "caption": "Figure 2:TheData Engine. We render Objaverse assets into multiple views and pass each rendering to SAM with gridpoint prompts for segmentation. For each mask, we query Gemini for the corresponding part name, which gives us (mask, text) pairs. We embed the part name into the latent embedding space of a vision and language foundation model such as SigLIP. We back-project mask pixels to obtain the points associated with each label embedding, yielding (points, text embedding) pairs as visualized on the right side of the figure.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13550/x3.png",
                "caption": "Figure 3:Find3D: an open-world part segmentation model.Find3Dtakes in a point cloud, voxelizes and serializes the points via space-filling curves into a sequence. The sequence is passed through a transformer architecture which performs block attention, shuffling, pooling and unpooling, returning a pointwise feature that is in the embedding space of a vision and language foundation model, denoted byùíØùíØ\\mathcal{T}caligraphic_T. These features can be queried with any free-form text.Find3Dis trained with a contrastive objective. For each (points, text embedding) label from the data engine, we use the averaged feature of these points as the predicted embedding, and pair it with the text embedding to form a positive pair in the contrastive loss.",
                "position": 172
            }
        ]
    },
    {
        "header": "4A General Open-World 3D Part Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13550/x4.png",
                "caption": "Figure 4:Qualitative results on the Objaverse-General benchmark.Find3Ddemonstrates strong performance on diverse objects, both for seen and unseen categories. PointCLIPV2, trained on ShapeNet-Part, generalizes poorly to novel object categories. PartSLIP++, a detection-based method, predicts sparsely because many parts are not successfully detected. OpenMask3D cannot go below the object-level granularity, and usually assigns the whole object to one part.",
                "position": 283
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13550/x5.png",
                "caption": "Figure 5:Robustness comparison of PointCLIPV2 andFind3D. We evaluate both methods on a ShapeNet-Part earphone with canonical and rotated orientations, and a visually similar earphone from Objaverse-ShapeNetPart. Top-k prompt reproduces evaluation in the PointCLIPV2 paper. PointCLIPV2‚Äôs performance drops up to68%percent6868\\%68 %with variations in evaluation configurations, whereas our method stays consistent.",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2411.13550/x6.png",
                "caption": "Figure 6:Robustness evaluation of PointCLIPV2 andFind3Don all ShapeNet-Part categories. We vary the query prompt, object orientation and data source (same object categories from ShapeNet-Part vs. Objaverse-ShapeNetPart). PointCLIPV2‚Äôs performance drops up to64%percent6464\\%64 %, while our method remains robust.",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2411.13550/x7.png",
                "caption": "Figure 7:Our method can support flexible text queries. For Mickey, one can either query by a body part such as ‚Äúhand‚Äù or by clothing such as ‚Äúgloves‚Äù. For the teddy bear, one can either query the coarser-granularity concept ‚Äúlimbs‚Äù or the finer-granularity ‚Äúarms‚Äù and ‚Äúlegs‚Äù.",
                "position": 656
            },
            {
                "img": "https://arxiv.org/html/2411.13550/x8.png",
                "caption": "Figure 8:A failure example. The leftmost image is a rendering of a microwave. The second image shows the point cloud atFind3D‚Äôs sampled granularity, which loses most features.",
                "position": 661
            }
        ]
    },
    {
        "header": "6Discussions and Conclusions",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AData Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13550/x9.png",
                "caption": "Figure 9:The prompt used to query Gemini for object orientation. The car and the Christmas tree are in common orientations (and thus will yield higher-quality annotations), whereas the camel and the parasol are not.",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2411.13550/x10.png",
                "caption": "Figure 10:The prompt used to query Gemini for object part names. We show 2 example masks from different views for a potted plant, a pair of glasses, a teapot, and a ring.",
                "position": 1965
            }
        ]
    },
    {
        "header": "BExperiments",
        "images": []
    }
]