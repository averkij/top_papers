[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22677/x1.png",
                "caption": "Figure 1:Two perspectives on the DMD algorithm. (a) The conventional view, which treats the use of CFG as a heuristic relaxation of the theoretical framework, with the algorithm’s success solely attributed to this (relaxed) distribution matching mechanism. (b) Our proposed decoupled view, where the objective is a combination of two distinct mechanisms: a CFG Augmentation (CA) engine that drives the few-step conversion, and a Distribution Matching (DM) regularizer—which strictly adheres to the theoretical derivation (Eq.1)—that ensures training stability.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Revisiting and Decomposing DMD",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22677/x2.png",
                "caption": "Figure 2:Ablation study on the roles of CFG Augmentation (CA) and Distribution Matching (DM). Numerical indicators are evaluated on 1k sampled prompts from COCO-10k(lin2014microsoft).",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2511.22677/x3.png",
                "caption": "Figure 3:CFG Augmentation with different regularizers. Image Reward and HPS v2.1 evaluated on 1k sampled prompts from COCO-10k. Setting: 4-step SDXL. See Fig.6for visualized samples.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Mechanistic Analysis of CA and DM",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22677/x4.png",
                "caption": "Figure 4:(a) Visualization on the effect of re-noising timestepτ\\tauin CFG Augmentation (CA). The generator is trained with CA alone. In our notation,τ=0\\tau=0corresponds to pure noise andτ=1\\tau=1to clean data. (b) Illustration of the DM corrective mechanism. The generator is trained with CA alone, while the fake model keeps training on the generator’s output as in DMD.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2511.22677/x5.png",
                "caption": "Figure 5:Un-cherry-picked qualitative comparison of different re-noising schedule configurations. Top row: ➁ Decoupled-Full,τCA,τDM∈[0,1]\\tau_{\\text{CA}},\\tau_{\\text{DM}}\\in[0,1]. Middle row: ➂ Coupled-Constrained,τCA,τDM>t\\tau_{\\text{CA}},\\tau_{\\text{DM}}>t. Bottom row: ➃ our proposed Decoupled-Hybrid,τCA>t,τDM∈[0,1]\\tau_{\\text{CA}}>t,\\tau_{\\text{DM}}\\in[0,1].",
                "position": 419
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Appendix ADiscussion: Why does CA Work?",
        "images": []
    },
    {
        "header": "Appendix BPseudo-code",
        "images": []
    },
    {
        "header": "Appendix CUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22677/x6.png",
                "caption": "Figure 6:Sample visualization on combining training CA with different regularizers (Fig.3)",
                "position": 1126
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": []
    }
]