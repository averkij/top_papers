[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04997/x1.png",
                "caption": "Figure 1:LLM2CLIPOverview. After applying caption contrastive fine-tuning to the LLM, the increased textual discriminability enables more effective CLIP training. We leverage the open-world knowledge and general capabilities of the LLM to better process dense captions, addressing the previous limitations of the pretrained CLIP visual encoder and providing richer, higher-dimensional textual supervision. Experimental results demonstrate that LLM2CLIP can make any SOTA CLIP model even more SOTA ever.",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04997/x2.png",
                "caption": "Table 1:Comparison of top-1 Caption Retrieval Accuracy (CRA) for various language models in MS COCO 5K testing set.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2411.04997/x2.png",
                "caption": "Figure 2:Real examples of top-1 results from the caption-to-caption retrieval experiment. Before fine-tuning, Llama3â€™s results were often completely unrelated.",
                "position": 185
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4EXPERIMENTS",
        "images": []
    },
    {
        "header": "5Limitations and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]