[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08577/x1.png",
                "caption": "Figure 1:Selective iteration can mitigate latent overthinking.\n(a) Toy example. Uniform latent iteration (always think-twice) can fix wrong predictions, but may also overthink and corrupt correct ones.\n(b) Next-token prediction accuracy of finetuned Qwen3-1.7B variants.\nAlways think-twice causes more errors than corrections over direct reply.\nIn contrast, the think-at-hard oracle, which iterates only when the first-pass prediction is wrong, achieves substantial improvements with minimal harm.\nWhile this oracle signal is unavailable in practice, it highlights the potential of selective iteration.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4TaH Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08577/x2.png",
                "caption": "Figure 2:TaH Overview.\n(a) Regular causal attention: tokens attend only to previous positions.\n(b) Our duo-causal attention: tokens attend to both previous positions and shallower iteration depths, maintaining 2D causality.\n(c) Model architecture: TaH selectively iterates or verbalizes tokens. It uses LoRA at deeper iterations to shift from next-token prediction to hard-token refinement. A neural decider determines whether to continue iterating or output the token.",
                "position": 322
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08577/x3.png",
                "caption": "Figure 3:Training dynamics of the LLM backbone on Qwen3-0.6B-Base. TaH converges rapidly and achieves lower perplexity.",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x3.png",
                "caption": "Figure 3:Training dynamics of the LLM backbone on Qwen3-0.6B-Base. TaH converges rapidly and achieves lower perplexity.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x4.png",
                "caption": "Figure 4:GSM8K accuracy with respect to continuation threshold. Numbers in brackets indicate the percentage of tokens that iterate twice.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x5.png",
                "caption": "Table 4:Impact of iteration strategies on Qwen3-0.6B (first 100 MATH500 samples).",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x5.png",
                "caption": "Figure 5:Next-token prediction changes across iterations. Top2 tokens that think-twice most are visualized.",
                "position": 885
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08577/x6.png",
                "caption": "Figure 6:Iteration-decider accuracy vs. epoch (Qwen3-0.6B).",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x7.png",
                "caption": "Figure 7:TaH duo-causal attention pattern.",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x8.png",
                "caption": "(a)TaH mean and standard deviation of attention weights (key from iteration 1) across layers in iteration 2.",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x8.png",
                "caption": "(a)TaH mean and standard deviation of attention weights (key from iteration 1) across layers in iteration 2.",
                "position": 1476
            },
            {
                "img": "https://arxiv.org/html/2511.08577/x9.png",
                "caption": "(b)Validation perplexity for different training schemes.",
                "position": 1481
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]