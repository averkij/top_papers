[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04103/x1.png",
                "caption": "Figure 1:Compute‚Äìperformance frontier on MiniWoB++ (results averaged over two seeds).The blue curve shows pure SFT on teacher demonstrations. Warm-colored curves represent hybrid runs that branch off from SFT checkpoints and continue training with RL.\nEarly transitions to RL push the Pareto frontier‚Äîachieving higher success rates for the same compute‚Äîand is the only approach able to achieve over30%percent3030\\%30 %improvement onbothheld-out goals (left) and held-out tasks (right) closing the gap between open and closed-source models.",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04103/extracted/6598633/figs/Neurips_Scaling_Tasks.png",
                "caption": "Figure 2:Example tasks in MiniWoB++[13](top) and WorkArena[10](bottom). MiniWoB consists of single-page simple tasks such as selecting a particular date and using a basic text editor, while WorkArena comprises multi-page complex tasks like filling forms and placing orders in an enterprise environment.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2507.04103/extracted/6598633/figs/Neurips_Scaling_Tasks.png",
                "caption": "Figure 2:Example tasks in MiniWoB++[13](top) and WorkArena[10](bottom). MiniWoB consists of single-page simple tasks such as selecting a particular date and using a basic text editor, while WorkArena comprises multi-page complex tasks like filling forms and placing orders in an enterprise environment.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2507.04103/x2.png",
                "caption": "Figure 3:Per-task performance of SFT and SFT+RL agents on WorkArena. The Llama 3.1 8B model is initially fine-tuned for 4 epochs on trajectories from a teacher Llama 3.3 70B model. Training then continues either with additional SFT or with GRPO fine-tuning up to epoch 20. The teacher model‚Äôs success rate is also shown.",
                "position": 413
            }
        ]
    },
    {
        "header": "5Main Results and Compute Trade-Offs",
        "images": []
    },
    {
        "header": "6Ablation and Sensitivity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04103/x3.png",
                "caption": "Figure 4:Bootstrap analysis (n=1000ùëõ1000n=1000italic_n = 1000samples) of hyperparameter optimization across different SFT compute budgets ontrainingheld out tasks. Each subplot examines a different hyperparameter, including increasing SFT compute: the base instruct model (left), +2.5√ó1018absentsuperscript1018\\times 10^{18}√ó 10 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPTSFT FLOPs (middle), and +7.6e√ó1018absentsuperscript1018\\times 10^{18}√ó 10 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPTSFT FLOPs (right). For each hyperparameter-compute combination, the top panel shows relative reward performance with error bars indicating 95% confidence intervals, while the bottom panel displays win rates representing the percentage of bootstrap iterations where each parameter value achieved maximum performance. Results demonstrate that optimal hyperparameter values shift as model pre-training compute increases, suggesting that hyperparameter selection should be adapted to the computational budget allocated to SFT.",
                "position": 503
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Learning and Saturation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04103/x4.png",
                "caption": "Figure 5:Per task performance of SFT and SFT+RL agents on MiniWob++.",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2507.04103/x5.png",
                "caption": "Figure 6:Per task performance of SFT and SFT+RL agents on WorkArena.",
                "position": 946
            }
        ]
    },
    {
        "header": "Appendix BDeriving Compute Cost",
        "images": []
    },
    {
        "header": "Appendix CExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix DTest Set Hyper-Parameter Bootstrap Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04103/x6.png",
                "caption": "Figure 7:Bootstrap analysis (n=1000ùëõ1000n=1000italic_n = 1000samples) of hyperparameter optimization across different SFT compute budgets ontestheld out tasks. Each subplot examines a different hyperparameter, including increasing SFT compute: the base instruct model (left), +2.5e+18 SFT FLOPs (middle), and +7.6e+18 SFT FLOPs (right). For each hyperparameter-compute combination, the top panel shows relative reward performance with error bars indicating 95% confidence intervals, while the bottom panel displays win rates representing the percentage of bootstrap iterations where each parameter value achieved maximum performance. Results demonstrate that optimal hyperparameter values often shift as model pre-training compute increases, suggesting that hyperparameter selection should be adapted based on the computational budget allocated to SFT.",
                "position": 1180
            }
        ]
    },
    {
        "header": "Appendix ERandom Search Space",
        "images": []
    },
    {
        "header": "Appendix FCompute Allocation Hyperparameter Selection",
        "images": []
    }
]