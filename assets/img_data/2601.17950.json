[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17950/x1.png",
                "caption": "Figure 1:UPLiFT time-scaling and dense features.We presentUPLiFT, an efficient feature-upsampler that leverages our newLocal Attenderto extract semantically-stable, pixel-dense features. (Top) UPLiFTâ€™s inference time and memory scales linearly with the number of visual tokens, while other recent SOTA methods face quadratic scaling. (Bottom) PCA visualization of low-resolution DINOv2 features and pixel-dense UPLiFT features.",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x2.png",
                "caption": "",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x3.png",
                "caption": "Figure 2:UPLiFT Tasks.We demonstrate our UPLiFT feature upsampler for applications in both predictive and generative tasks. This includes semantic segmentation, monocular depth estimation, image super-resolution, and efficient text-to-image generation.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17950/x4.png",
                "caption": "Figure 3:UPLiFT Inference.At inference time, our UPLiFT Encoder (ğ„UPLiFT\\mathbf{E}_{\\text{UPLiFT}}) produces shallow but dense features to guide all subsequent upsampling steps. Iterative application of the UPLiFT Decoder (ğƒUPLiFT\\mathbf{D}_{\\text{UPLiFT}}) upsamples the low-resolution backbone features to pixel-density. Our proposed Local Attender module is integrated with the UPLiFT Decoder to maintain iterative feature consistency.",
                "position": 157
            }
        ]
    },
    {
        "header": "3UPLiFT Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17950/x5.png",
                "caption": "Figure 4:Local Attender Operator.We propose a streamlined and efficient local attention operator, which gathers features over a set neighborhood defined by fixed direction offsets.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x6.png",
                "caption": "Figure 5:UPLiFT Training.UPLiFT uses a multi-step training strategy where the feature reconstruction loss is applied at all intermediate steps. All lift decoders (ğƒUPLiFT\\mathbf{D}_{\\text{UPLiFT}}) share the same weights.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x7.png",
                "caption": "Figure 6:2048Ã—\\times2048 Image Generation Comparison with CFM.We show that UPLiFT achieves comparable visual upsampling quality to CFM[15], while using only1/6th1/6^{\\text{th}}the network parameters,1/200th1/200^{\\text{th}}the training data, and only 2 iterative upsampling steps.",
                "position": 468
            }
        ]
    },
    {
        "header": "4UPLiFT for Predictive Tasks",
        "images": []
    },
    {
        "header": "5UPLiFT for Generative Tasks",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details for Predictive Tasks",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details for Generative Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17950/x8.png",
                "caption": "Figure 7:Visual ablation of design choices for VAE UPLiFT.(Left) UPLiFT achieves high quality 512â†’\\to1024 upsampling with a larger parameter count model. (Middle) A smaller-scale UPLiFT has insufficient capacity to upsample all high-frequency information and produces blurry results. (Right) Ablation of the Refiner Block leads to blocky artifacts in upsampled images.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x9.png",
                "caption": "Figure 8:Local Attender neighborhood designs.We visualize the neighborhood designs tested with our Local Attender module. The center token (orange) is always included in the neighborhood, and the blue tokens represent the offset relative local positions that are included in feature pooling through local attention.",
                "position": 1756
            }
        ]
    },
    {
        "header": "Appendix CAblations of UPLiFT Design Choices",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17950/x10.png",
                "caption": "Figure 9:Speed and Memory Usage Comparison.We compare UPLiFT with recent cross-attention-based feature upsampling methods when running with gradually increasing image sizes. We report the average inference time and memory usage against the visual token count for two different GPU types: an NVIDIA A5000 with 24 GB of memory and an NVIDIA A6000 with 48 GB. UPLiFT maintains linear time and memory scaling with respect to the number of tokens, while the baseline methods show quadratic scaling. This gives UPLiFT far faster performance as the token count increases, and allows us to run pixel-dense feature upsampling on larger images.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x10.png",
                "caption": "",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x11.png",
                "caption": "",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x12.png",
                "caption": "",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x13.png",
                "caption": "",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x14.png",
                "caption": "Figure 10:Comparison of semantic drift in LiFT and semantic stability in UPLiFT.We visualize intermediate feature upsampling steps through PCA, following[15]. LiFT shows signs of feature drift, with local features becoming murkier and more distorted in deeper steps. This drift can lead to poor performance in downstream tasks, as the strength of the original backbone representation is lost. UPLiFT maintains consistent feature semantics thanks to our Local Attender, and local object regions maintain consistent features (coloration) across all upsampling stages.",
                "position": 2067
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x15.png",
                "caption": "Figure 11:UPLiFTvs.Latent Space Bilinear4Ã—4{\\times}upsampling for image super-resolution.We compare the latency of UPLiFT versus applying bilinear upsampling in latent space for image super-resolution to demonstrate UPLiFTâ€™s state-of-the-art efficiency.\nWhile only8.47%8.47\\%slower end-to-end, UPLiFT produces significantly better visual fidelity, as shown by the zoomed-in views (bottom) which display the source low-resolution image compared with UPLiFTâ€™s super-resolution image.\nThe low-resolution image is selected from the FacesHQ dataset and bilinearly downscaled from1024Ã—10241024{\\times}1024to512Ã—512512{\\times}512before UPLiFT is applied.\nBest viewed zoomed in.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x16.png",
                "caption": "Figure 12:UPLiFTğŸ“ğŸğŸÃ—ğŸ“ğŸğŸâ†’ğŸğŸğŸ’ğŸ–Ã—ğŸğŸğŸ’ğŸ–\\mathbf{512{\\times}512\\to 2048{\\times}2048}upsampled images using Stable Diffusion 1.5.We apply our VAE UPLiFT model to this task in a4Ã—4{\\times}upsampling configuration.\nUPLiFT upsamples latents corresponding to512Ã—512512{\\times}512images generated using 50 diffusion steps on Stable Diffusion 1.5.\nBest viewed zoomed in.",
                "position": 2094
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x17.png",
                "caption": "Figure 13:UPLiFTğŸ“ğŸğŸÃ—ğŸ“ğŸğŸâ†’ğŸğŸğŸğŸ’Ã—ğŸğŸğŸğŸ’\\mathbf{512{\\times}512\\to 1024{\\times}1024}upsampled images using Stable Diffusion 1.5.We apply our VAE UPLiFT model to this task in a2Ã—2{\\times}upsampling configuration.\nUPLiFT upsamples latents corresponding to512Ã—512512{\\times}512images generated using 50 diffusion steps on Stable Diffusion 1.5, and the end-to-end latency is 2.75 seconds on an NVIDIA A100 GPU. The UPLiFT model itself takes only 104 milliseconds of this time.\nBest viewed zoomed in.",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x18.png",
                "caption": "Figure 14:UPLiFTğŸğŸ“ğŸ”Ã—ğŸğŸ“ğŸ”â†’ğŸğŸğŸğŸ’Ã—ğŸğŸğŸğŸ’\\mathbf{256{\\times}256\\to 1024{\\times}1024}super-resolution samples from FacesHQ.We use our VAE UPLiFT model that isnot fine-tunedfor image super-resolution and is only trained in latent space.\nOur end-to-end upsampling time is only270.9 millisecondson an NVIDIA A100 GPU.\nBest viewed zoomed in.",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2601.17950/x19.png",
                "caption": "Figure 15:UPLiFTğŸğŸ“ğŸ”Ã—ğŸğŸ“ğŸ”â†’ğŸğŸğŸğŸ’Ã—ğŸğŸğŸğŸ’\\mathbf{256{\\times}256\\to 1024{\\times}1024}super-resolution samples from LHQ.We use our VAE UPLiFT model, which is trained as a generalist model and is not specifically fine-tuned for this dataset.\nThe LHQ dataset presents a greater challenge than FacesHQ, based on the diversity of visual textures present. Despite this challenge, we see good performance with our generalist UPLiFT.\nIn comparison,[41]uses a fine-tuned model with3Ã—3{\\times}the parameter count for evaluations on LHQ versus FacesHQ.\nBest viewed zoomed in.",
                "position": 2115
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results and Visualizations",
        "images": []
    }
]