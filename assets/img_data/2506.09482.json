[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09482/x1.png",
                "caption": "Figure 1:Generated samples from TransDiff trained on ImageNet.Top:512×512512512512\\times 512512 × 512and256×256256256256\\times 256256 × 256samples.Middle:effect of semantic feature diversity in TransDiff on image quality(left to right: increasing diversity.).Bottom:results of semantic features fusion from images of different classes. (The first two columns show images from two classes; the third shows the fused result.)",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09482/x2.png",
                "caption": "Figure 2:Compare of Token-Level AR， Scale-Level AR and MRAR.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09482/x3.png",
                "caption": "Figure 3:Our Transdiff approach overview. Wherein (a) and (c) are the Vanilla Autoregressive Transformer and the Vanilla Diffusion Model, respectively. (b) is our TransDiff approach. (d) and (e) provide a detailed explanation of 1-step AR and MRAR.",
                "position": 155
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09482/x4.png",
                "caption": "(a)",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2506.09482/x4.png",
                "caption": "(a)",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2506.09482/x5.png",
                "caption": "(b)",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2506.09482/x6.png",
                "caption": "Figure 5:Results of semantic features fusion from images of different classes. (The first two columns show images from two classes; the third shows the fused result.)",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2506.09482/x7.png",
                "caption": "",
                "position": 782
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInference Process of 1 Step AR and MRAR",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details.",
        "images": []
    },
    {
        "header": "Appendix CThe Euler–Maruyama Method",
        "images": []
    },
    {
        "header": "Appendix DMore Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09482/x8.png",
                "caption": "Figure 7:Model comparison on ImageNet256×256256256256\\times 256256 × 256benchmark.",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2506.09482/x9.png",
                "caption": "Figure 8:Qualitative Results. More256×256256256256\\times 256256 × 256class-conditional generation results from TransDiff-H, MRAR on ImageNet.",
                "position": 1746
            },
            {
                "img": "https://arxiv.org/html/2506.09482/x10.png",
                "caption": "Figure 9:Qualitative Results. More512×512512512512\\times 512512 × 512class-conditional generation results from TransDiff-L, MRAR on ImageNet.",
                "position": 1749
            }
        ]
    },
    {
        "header": "Appendix EMore Qualitative Results",
        "images": []
    }
]