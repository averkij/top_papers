[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05630/x1.png",
                "caption": "Figure 1:Example videos from the proposed MOSEv2 dataset.\nThe selected target objects are masked inorange.\nThe target object in case‚ë†is enlarged for better visualization.\nThe most notable features of MOSEv2 include both challenges inherited from MOSEv1[1]such as disappearance-reappearance of objects (‚ë†-‚ë©), small/inconspicuous objects (‚ë†,‚ë¢,‚ë•), heavy occlusions, and crowded scenarios (‚ë†,‚ë°), as well as newly introduced complexities including adverse weather conditions (‚ë•), low-light environments (‚ë§-‚ë¶), multi-shots (‚ëß), camouflaged objects (‚ë§), non-physical objects like shadows (‚ë£), and knowledge dependency (‚ë®,‚ë©). The goal of MOSEv2 dataset is to provide a platform that promotes the development of more comprehensive and robust video object segmentation algorithms.",
                "position": 96
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05630/x2.png",
                "caption": "Figure 2:Category distribution of MOSEv1 and MOSEv2.",
                "position": 233
            }
        ]
    },
    {
        "header": "3MOSEv2 Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05630/x3.png",
                "caption": "Figure 3:Occlusion evaluation protocol. (a) BOR: Bounding-box Occlusion Rate[111], (b) AOR: Amodal-mask Occlusion Rate, (c) MLLMOR: MLLM-assisted Occlusion Rate.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x4.png",
                "caption": "Figure 4:Mask size distribution, normalized by video resolution.",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x4.png",
                "caption": "Figure 4:Mask size distribution, normalized by video resolution.",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x5.png",
                "caption": "Figure 5:Video length distributions. Compared to MOSEv1, MOSEv2 includes more long videos, with the longest reaching 7,825 frames.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x6.png",
                "caption": "Figure 6:Challenging environment distribution.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x7.png",
                "caption": "Figure 7:(Left) Distribution of instance sequences attributes. (Right) Attribute correlations in MOSEv2.",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x8.png",
                "caption": "",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x9.png",
                "caption": "Figure 8:Comparison of‚Ñ±\\mathcal{F}and‚Ñ±Àô\\dot{\\mathcal{F}}onSmall andLarge objects. For a small objects (e.g., a chopstick with only 955 pixels),‚Ñ±\\mathcal{F}yields exaggerated scores due to fixed resolution-based thresholds, while‚Ñ±Àô\\dot{\\mathcal{F}}offers a more reliable measure by accounting for object scale.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2508.05630/x10.png",
                "caption": "Figure 9:Illustration ofùí•&‚Ñ±\\mathcal{J}\\&\\mathcal{F},ùí•&‚Ñ±d\\mathcal{J}\\&\\mathcal{F}_{d}, andùí•&‚Ñ±r\\mathcal{J}\\&\\mathcal{F}_{r}computation. The slashed boxes denote frames with empty masks. Predicted masks are assumed to perfectly match the ground truth.",
                "position": 947
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05630/x11.png",
                "caption": "Figure 10:Qualitative results on MOSEv2. We compare Cutie[14], SAM2[15], and SAM2Long[27]on 8 challenging cases that assess model performance under various complex conditions. These include object disappearance and reappearance (a, b, e, g, h), small/inconspicuous objects (c), heavy occlusions (c, f), crowded scenes (c), adverse weather (f), low-light environments (a, d), multi-shot sequences (g), camouflaged targets (d), non-physical objects such as shadows (e), and knowledge-dependent scenarios (h).",
                "position": 2641
            }
        ]
    },
    {
        "header": "5Discussion and Future Directions",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]