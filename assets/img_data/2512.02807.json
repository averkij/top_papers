[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02807/x1.png",
                "caption": "Figure 1:Hidden states visualization for responses in RewardBench(lambert2024rewardbench)based on Qwen3-8B(yang2025qwen3). Left: PCA projection shows good (blue) and bad (red) responses with spatial overlap. Right: Stable rank coloring uncovers clear quality separation: good responses exhibit higher ranks than bad responses. Stable rank can distinguish response quality.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Stable Rank as an Intrinsic Quality Metric",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02807/x2.png",
                "caption": "Figure 2:Best-of-N decoding with stable rank selection.Top: Performance on individual benchmarks as N increases.Bottom: Average accuracy comparing stable rank selection (solid) versus random selection (dashed). Annotations show relative improvement over greedy decoding (N=1). Stable rank consistently outperforms random selection across all models.",
                "position": 357
            }
        ]
    },
    {
        "header": "3Stable Rank Group Relative Policy Optimization (SR-GRPO)",
        "images": []
    },
    {
        "header": "4Alignment Experiments",
        "images": []
    },
    {
        "header": "5What Stable Rank Captures",
        "images": []
    },
    {
        "header": "6Ablation Studies",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Appendix ACross-Layer Stable Rank Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02807/x3.png",
                "caption": "Figure 3:Cross-layer stable rank performance on RewardBench across different model families. We evaluate the stable rank computed from each transformer layer as a reward proxy on RewardBenchâ€™s six subcategories. Final layers (rightmost) consistently achieve the highest accuracy across models and categories, validating our choice to use final-layer activations in SR-GRPO.",
                "position": 953
            }
        ]
    },
    {
        "header": "Appendix BBaseline Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CRewardBench Results",
        "images": []
    },
    {
        "header": "Appendix DBest-of-N Complete Results",
        "images": []
    },
    {
        "header": "Appendix ESR-GRPO Training Details",
        "images": []
    },
    {
        "header": "Appendix FExperimental Details for Metric Analysis",
        "images": []
    },
    {
        "header": "Appendix GAblation: Comparison with Other Intrinsic Dimension Metrics",
        "images": []
    },
    {
        "header": "Appendix HAblation: Context Window Size",
        "images": []
    },
    {
        "header": "Appendix IAblation: Input Prompt Format",
        "images": []
    },
    {
        "header": "Appendix JStable Rank Example Prompts",
        "images": []
    }
]