[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08113/logo-black.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x1.png",
                "caption": "Figure 1:Overview ofAimBot, a lightweight visual guidance method that adds spatial cues onto RGB images for visuomotor policy learning. Given the robot’s end-effector pose, camera extrinsic and depth image,AimBotcomputes shooting lines and reticle overlay to highlight the spatial relationship between the gripper and objects of interest. The augmented RGB images are used as input to visuomotor policies for closed-loop control, enhancing task performance with minimal overhead.",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08113/x2.png",
                "caption": "Figure 2:An example ofAimBot-augmented LIBERO observations. We add a shooting line and a crosshair reticle for the front-view (top row) and wrist-view (bottom row) cameras, respectively.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x3.png",
                "caption": "Figure 3:We design five contact-rich, long-horizon real-world tasks for policy evaluation.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x4.png",
                "caption": "Figure 4:Comparison of different visual guidance methods.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x4.png",
                "caption": "Figure 4:Comparison of different visual guidance methods.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x5.png",
                "caption": "Figure 5:Visualization of attention weights trained with and withoutAimBot.",
                "position": 653
            }
        ]
    },
    {
        "header": "5Conclusion and Discussions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08113/x6.png",
                "caption": "Figure 6:Comparison of differentAimBotvariants.",
                "position": 1944
            },
            {
                "img": "https://arxiv.org/html/2508.08113/pdfs/robot-set-up.png",
                "caption": "Figure 7:Our real robot platform setup.",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x7.png",
                "caption": "Figure 8:Task Examples and Visualization ofAimBot’s in Multi-View Observations.We test our method on five challenging tasks:Fruits in Box,Tennis Ball in drawer,Bread in Toaster,Place Coffee CupandEgg in Carton. Here we show how ourAimBotaugment the shooting line and scope reticle in the left shoulder and wrist camera views, respectively.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x8.png",
                "caption": "Figure 9:Additional attention heatmap examples comparing w/ and w/oAimBot.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x9.png",
                "caption": "Figure 10:Examples of four misalignment types in real-world robot trials: grasping position, grasping orientation, placing position, and placing orientation. Each row shows representative wrist-view or shoulder-view images. For each misalignment type, we include aligned (greenborders) and misaligned (redborders) examples to contrast correct behavior with failure cases.",
                "position": 2124
            },
            {
                "img": "https://arxiv.org/html/2508.08113/x10.png",
                "caption": "Figure 11:Sample out-of-distribution evaluation scenes to test generalization of policies to different backgrounds, varying lighting conditions (e.g. flashing lights, cool/warm lights), unseen distractors with real-time human perturbations, and varying camera poses, whereAimBotbridges the distribution gap by grounding to useful depth information instead of plain visual features.",
                "position": 2252
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]