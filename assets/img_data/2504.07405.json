[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/teaser-diverse-prompt.jpg",
                "caption": "",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/preserve_analysis.jpg",
                "caption": "Figure 2:Comparison with other methods on two indicators, image preservation and text fidelity, demonstrates that our approach surpasses previous methods in both aspects.",
                "position": 140
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/pipeline.png",
                "caption": "Figure 3:The overall pipeline of FlexIP.It introduces three key improvements to the model: the preservation adapter, the personalization adapter, and dynamic weight gating. First, the preservation adapter combines high-level and low-level features to ensure preservation. The personalization adapter interacts with text and visual CLS tokens to absorb meaningful visual cues, grounding textual modifications within a coherent visual context. Finally, dynamic weight gating navigates the trade-off between personalization and preservation more effectively through independent adapters controlled by a dynamic weight gating mechanism.",
                "position": 163
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/exps/comp-main.jpg",
                "caption": "Figure 4:Qualitative comparison with other methods. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating a wide range of diverse and personalized outputs.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/exps/comp-var.jpg",
                "caption": "Figure 5:The effectiveness of the dynamic weight gating mechanism.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/exps/comp-style.jpg",
                "caption": "Figure 6:Comparison with other methods on style transfer tasks.",
                "position": 609
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/attnmaps.png",
                "caption": "Figure 7:Visualization of attention maps across different modules.In the image, the white areas of the attention map indicate activation values—the whiter the color, the higher the activation value. It is evident that the two preservation modules function differently: the learnable query module concentrates more on the subject’s details, while the CLIP CLS Embeds focus more on the subject’s global aspects. Consequently, high-level and low-level information complement each other. For the personalization module, the text embeds pay more attention to the surrounding environment and some identity preservation details. This observation supports our decision to decouple preservation and personalization.",
                "position": 1486
            }
        ]
    },
    {
        "header": "Appendix BMore Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/exps/supp-comp-animals.jpg",
                "caption": "Figure 8:Qualitative comparison with other methods in animal domain. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating a wide range of diverse and personalized outputs.",
                "position": 1609
            },
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/exps/supp-comp-human.jpg",
                "caption": "Figure 9:Qualitative comparison with other methods in human domain. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating a wide range of diverse and personalized outputs.",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2504.07405/extracted/6349914/assets/pics/exps/supp-comp-objects.jpg",
                "caption": "Figure 10:Qualitative comparison with other methods in object domain. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating a wide range of diverse and personalized outputs.",
                "position": 1615
            }
        ]
    },
    {
        "header": "Appendix CBackground",
        "images": []
    }
]