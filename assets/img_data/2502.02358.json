[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02358/x1.png",
                "caption": "Figure 1.Demonstration of our MotionLab‚Äôs versatility, performance and efficiency. Previous SOTA refer to multiple expert models, including MotionLCM(Dai et¬†al.,2025), OmniControl(Xie et¬†al.,2023), MotionFix(Athanasiou et¬†al.,2024), CondMDI(Cohan et¬†al.,2024)and MCM-LDM(Song et¬†al.,2024). All motions are represented using SMPL(Loper et¬†al.,2023), where transparent motion indicates the source motion or condition, and the other represents the target motion.More qualitative results are available in the website and appendix.",
                "position": 98
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.PRELIMINARY: Rectified Flows",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02358/x2.png",
                "caption": "Figure 2.Demonstration of the difference trajectory between diffusion models and rectified flows. This difference lies in that the trajectory of diffusion models is based onxt=(1‚àíŒ±t¬Ø)‚Å¢x0+Œ±t¬Ø‚Å¢œµsubscriptùë•ùë°1¬Øsubscriptùõºùë°subscriptùë•0¬Øsubscriptùõºùë°italic-œµx_{t}=\\sqrt{(1-\\overline{\\alpha_{t}})}x_{0}+\\sqrt{\\overline{\\alpha_{t}}}\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG ( 1 - over¬Ø start_ARG italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ) end_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG over¬Ø start_ARG italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG italic_œµ, while the trajectory of rectified flows is based onxt=(1‚àít)‚Å¢x0+t‚Å¢x1subscriptùë•ùë°1ùë°subscriptùë•0ùë°subscriptùë•1x_{t}=(1-t)x_{0}+tx_{1}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( 1 - italic_t ) italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. This distinction leads to more robust learning by maintaining a constant velocity, contributing to model‚Äôs efficiency(Zhao et¬†al.,2024).",
                "position": 256
            }
        ]
    },
    {
        "header": "4.Motion-Condition-Motion",
        "images": []
    },
    {
        "header": "5.MotionLab",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02358/x3.png",
                "caption": "Figure 3.Illustration of our MotionLab and the detail of its MotionFlow Transformer (MFT).",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2502.02358/x4.png",
                "caption": "Figure 4.Qualitative results of MotionLab on the text-based motion generation. For clarity, as time progresses, motion sequences transit from light to dark colors.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2502.02358/x5.png",
                "caption": "Figure 5.Qualitative results of MotionLab on the text-based motion editing. The transparent motion is the source motion, and the other is the generated motion.",
                "position": 856
            },
            {
                "img": "https://arxiv.org/html/2502.02358/x6.png",
                "caption": "Figure 6.Qualitative results of MotionLab on the trajectory-based motion generation. The red balls are the trajectory of the pelvis, right hand and right foot.",
                "position": 859
            }
        ]
    },
    {
        "header": "6.Experiments",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02358/x7.png",
                "caption": "Figure 7.Comparison of the motion in-between with CondMDI(Cohan et¬†al.,2024)on HumanML3D(Guo et¬†al.,2022a), which shows that our model outperforms CondMDI.",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2502.02358/extracted/6183938/Figure/style.png",
                "caption": "Figure 8.Comparison of the motion style transfer with MCM-LDM(Song et¬†al.,2024)on a subset of HumanML3D(Guo et¬†al.,2022a). This shows that our model has a stronger ability to preserve the semantics of source motion and a stronger ability to learn the style of style motion.",
                "position": 2019
            }
        ]
    },
    {
        "header": "Appendix BAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02358/x8.png",
                "caption": "Figure 9.Ablation results of MotionLab on the motion in-between. Beige motion is use 1D-learnable position encoding, purple motion use Aligned ROPE, and gray motions are the poses provided in keyframes, demonstrating the importance of Aligned ROPE.",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2502.02358/extracted/6183938/Figure/timesteps.png",
                "caption": "Figure 10.Comparison of the inference time on text-based motion generation. We calculate AITS on the test set of HumanML3D(Guo et¬†al.,2022a)without model or data loading parts. All tests are performed on the same RTX 4090D. The closer the model‚Äôs points are to the lower left corner, the stronger the model is.",
                "position": 2041
            }
        ]
    },
    {
        "header": "Appendix CRepresentation for Each Modality",
        "images": []
    },
    {
        "header": "Appendix DInstrcutions for Each Task",
        "images": []
    },
    {
        "header": "Appendix EClassifier Free Guidance for Each Task",
        "images": []
    },
    {
        "header": "Appendix F3D Assets",
        "images": []
    }
]