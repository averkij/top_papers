[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.12993/x1.png",
                "caption": "Figure 1:Overview of UniHand 2.0.UniHand 2.0 is our large-scale pre-training recipe for human-centric robot learning, comprising 35K hours of multimodal data from three complementary sources. 1)Human Demonstrationwith diverse scenes, multitask for motion alignment, and multi-granularity semantics. 2)Robot Manipulationspanning 30+ embodiments with multiple observation views and heterogeneous control signals. 3)Vision‚ÄìText Understandingcovering general VQA, 2D spatial grounding & affordance, and task planning & reasoning.",
                "position": 470
            }
        ]
    },
    {
        "header": "3UniHand-2.0: A Recipe for Large-Scale Human-Centric Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.12993/x2.png",
                "caption": "Figure 2:Comparison of training scale and embodiment diversity. Stacked bars represent hours of training data (left axis); hatched bars represent embodiment counts (right axis).\nUniHand-2.0 represents the largest and most diverse VLA pre-training recipe to date, totaling 35,000 hours of multimodal data.\nThis includes 16,000 hours of human data, 14,000 hours of robot data across 30 embodiments, and 5,000 equivalent hours of VLM data.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x3.png",
                "caption": "Figure 3:Statistics of UniHand-2.0.\n(Left) Ratio of simulation vs. real-world data.\nWe maintain a balanced ratio with simulation data at 26%, while the widely used Open X-Embodiment (OXE) and AgiBot World datasets account for 3.1% and 3.0%, respectively.(Right) Training sources and scale. Human, robot, and visual-language (‚ÄúVLM‚Äù) data consist of 16K hours (25.6B tokens), 14K hours (45.7B tokens), and 5K equivalent hours (50.2B tokens), respectively.\nThese sources are curated to maintain a comparable scale for balanced pretraining.",
                "position": 556
            }
        ]
    },
    {
        "header": "4UniCraftor: A System for Portable, Extensible, and Affordable Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.12993/x4.png",
                "caption": "Figure 4:An overview of our data collection system UniCraftor.",
                "position": 999
            }
        ]
    },
    {
        "header": "5Being-H0.5: A Foundational VLA Unifying Cross-Embodiment Control",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.12993/x5.png",
                "caption": "Figure 5:Overview of Being-H0.5.Being-H0.5 is a specialized MoT that disentangles multimodal understanding (Und. Expert) and action generation (Act. Expert) while maintaining coupling through shared attention mechanisms.\nAunified state‚Äìaction spacesupports cross-embodiment pre-training by mapping human hand motion and diverse robot controls into semantically aligned slots.\nOur pre-training leveragesUniHand-2.0by serializing multimodal data into a unified QA-style format, with each modality allocated to the relevant branch.\nFinally, aMixture-of-Flowdesign scales action capacity by combining shared foundation layers with routed specialized experts for embodiment/task-specific dynamics.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x6.png",
                "caption": "Figure 6:MPG and UAC Overview.Left (MPG):We compare observation embeddings with a reference action embedding (Train: ground truth; Inference: previous iterate) in the Sliced-Wasserstein Distance (SWD) space to obtain a discrepancy-guided gategg. The gate scales afeature-conditionedresidual while anungatedlearned prior offset provides a stable fallback, producing enhanced context featuresH~\\tilde{H}for the action expert.Right (UAC):Based on embodiment-specific dynamic delaydd, each predicted action chunk is split into a committed prefixùêÄ<d\\mathbf{A}_{<d}(already queued/executing) and a predicted postfixùêÄ‚â•d\\mathbf{A}_{\\geq d}.\nA dual-thread buffer enables asynchronous inference/execution across robots with heterogeneous latency budgets.",
                "position": 1361
            }
        ]
    },
    {
        "header": "6Infrastructure: Real-Time Cross-Embodiment Deployment",
        "images": []
    },
    {
        "header": "7Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.12993/x7.png",
                "caption": "Figure 7:Real-robot embodiments.We evaluate on five robotic platforms spanning upper-body humanoids, single-arm dexterous manipulation, and lightweight parallel grippers.",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x8.png",
                "caption": "Figure 8:Example rollouts of representative real-robot task suites.(Only show one task for each embodiment. The full rollouts of all tasks are shown in Appendix10).",
                "position": 1953
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x9.png",
                "caption": "Figure 9:Real-robot success rates (%) on four task suites.We compare Being-H0.5 specialist/generalist, scratch ablations (both specialist and generalist), andœÄ0.5\\pi_{0.5}.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x10.png",
                "caption": "Figure 10:Ablation study on the number of frozen layers in the action expert.Results compare the performance of the model after human-centric pretraining (‚Äúpt‚Äù) and without it (‚Äúw/o pt‚Äù). ‚ÄúMoF‚Äù denotes our proposed Mixture of Flow architecture. All experiments freeze all parameters of the MLLM including the visual encoder and projector. Note that each task was evaluated with 20 rollouts due to time constraints.",
                "position": 2713
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x11.png",
                "caption": "Figure 11:Ablation Study on the Number of Frozen Layers in the Action Expert.Results compare the performance of the model after human-centric pretraining (‚Äúpt‚Äù) and without it (‚Äúw/o pt‚Äù). ‚ÄúMoF‚Äù denotes our proposed Mixture of Flow architecture. All experiments freeze all parameters of the MLLM, including the visual encoder and projector. Note that each task was evaluated with 20 rollouts due to time constraints.",
                "position": 2738
            },
            {
                "img": "https://arxiv.org/html/2601.12993/x12.png",
                "caption": "Figure 12:Ablation of MPG+UAC on real robots.Removing MPG+UAC hurts long-horizon and bimanual categories the most, where execution delay and unreliable context amplify compounding errors.",
                "position": 2812
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "9Contributions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.12993/x13.png",
                "caption": "Figure 13:Example rollouts of all real-robot task suites.",
                "position": 4878
            }
        ]
    },
    {
        "header": "10Full Rollouts",
        "images": []
    }
]