[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16515/x1.png",
                "caption": "Figure 1:Comparison of Our Method and Other Sparse Attention Mechanisms.Score versus speedup, with point size representing density—smaller points indicate lower computational density. TheSummation Scoreis computed as summation of VBench metrics (Subject Consistency, Background Consistency, Imaging Quality, and Text Consistency). This reflects overall quality–efficiency trade-off. Models compared include our approach, SVG2[28], PARO[40], ST-SWA[33], and ST-SWA + LoRA[7].",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x2.png",
                "caption": "Figure 2:Comparison of Full Attention Model, Sparse Attention Model and Sparse Attention Model with LoRA Tuning.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16515/x3.png",
                "caption": "Figure 3:Overview of SALAD Attention Module.",
                "position": 209
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16515/x4.png",
                "caption": "Figure 4:The Rank of Sparse Attention and Linear Attention.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x5.png",
                "caption": "Figure 5:Effect of Scaling the Linear Attention Branch.Asλ\\lambdadecreases, we observe\nimprovements inbackground consistency,imaging quality, andtext consistency. This suggests that the contribution of the linear attention branch to the final output must be carefully constrained. Relying solely on a static projection layer to regulate its influence appears insufficient, highlighting the need for more dynamic control over the branch fusion ratio.",
                "position": 410
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16515/x6.png",
                "caption": "Figure 6:Generated Video Example of SALAD and other baselines.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x7.png",
                "caption": "Figure 7:Generated Video Samples of LoRA, SALAD, and SALAD with linear branch dropped during inference.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x8.png",
                "caption": "Figure 8:Examples of linear and sparse attention maps at block 8, timestep 20, head 2 and 4. Linear attention attends to long distant tokens, adding global information.",
                "position": 829
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledge",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16515/x9.png",
                "caption": "Figure 9:Quintile Trends of Gate Values across Time-steps.The distributions of gate values remain remarkably stable throughout the diffusion process. This temporal consistency motivates our timestep-agnostic thresholding strategy for branch selection.",
                "position": 1605
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x10.png",
                "caption": "Figure 10:Progressive Branch Dropping Analysis.IQ (Image Quality) scores across different branch reservation strategies. The shaded region shows the effective dropping range that maintains baseline quality with around 20% branch dropped.",
                "position": 1611
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x11.png",
                "caption": "Figure 11:Gate design.The computation of GATE.",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2601.16515/figs/branch_val.png",
                "caption": "Figure 12:Comparison of Branch and Sparse Attention Outputs.The top panel demonstrates the value range of the Sparse Attention Outputs and Linear Attention Branch Outputs. The blue shaded region denotes the sparse attention outputs, while the red shaded region highlights the gated branch outputs, which provide fine-grained complementary information without disrupting the original structural patterns.\nThe bottom panel further visualizes the gate values, typically ranging between 0.1 and 0.4, indicating how the gating mechanism adaptively modulates the contribution of linear attention across different blocks.",
                "position": 1755
            },
            {
                "img": "https://arxiv.org/html/2601.16515/figs/inference_gate.png",
                "caption": "Figure 13:Video example of the effect of Gate choices when inference.The model is trained with a gate value of 1 and evaluated under inference with different gate settings.\nUsinggate=0leads to severe color distortion, whilegate=0.7incorrectly introduces an additional dog.\nAlthoughgate=1aligns with the prompt semantics, it produces spatially inconsistent dog head–body structures.\nWithgate=1.5, the model fails to generate meaningful outputs.\nIn contrast, input-dependent gate values yield high-quality and semantically aligned results.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2601.16515/figs/training_gate.png",
                "caption": "Figure 14:Effect of Gate Choices During Training.We compare models trained with fixed gate values to those trained with input-dependent gates.\nFixed-gate models exhibit distinct semantic errors, such asgate=0producing two dogs andgate=0.5generating duplicate tails.\nEvengate=1shows spatial inconsistencies.\nIn contrast, the input-dependent gate model produces natural videos with coherent structures and fine-grained details (e.g., wagging tail).",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2601.16515/x12.png",
                "caption": "Figure 15:Shared-Weight and Non-Shared-Weight Architecture of Naive Sparse-Linear attention and SALAD Attention.",
                "position": 1832
            },
            {
                "img": "https://arxiv.org/html/2601.16515/figs/tornado.png",
                "caption": "Figure 16:Generated Video Example of SALAD and other baselines.",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2601.16515/figs/bear.png",
                "caption": "Figure 17:Generated Video Example of SALAD and other baselines.",
                "position": 1845
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]