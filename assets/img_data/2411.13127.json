[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13127/x1.png",
                "caption": "Figure 1:Comparison between (a) the Previous Method and (b) our Cloud-Adapter approach. Unlike (a), where the entire network (e.g., a typical encoder-decoder architecture) is fully trainable, resulting in a large number of parameters and an increased risk of overfitting, (b) our Cloud-Adapter approach leverages a frozen vision foundation model (VFM) combined with a lightweight, trainable adapter. This design preserves generalization and adaptability, enabling efficient learning for cloud segmentation tasks. The frozen VFM extracts generic visual features, while the parameter-efficient adapter facilitates effective transfer learning across remote sensing scenes.",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2411.13127/x2.png",
                "caption": "Figure 2:Detailed network architecture of the proposed Cloud-Adapter method, consisting of the spatial perception and adapting modules. The spatial perception module uses ConvNet blocks to extract dense spatial features, which are aggregated into a multi-scale context and fed to the adapting module. The adapting module modulates the frozen transformer layers in the VFM.",
                "position": 166
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13127/x3.png",
                "caption": "Figure 3:Schematic diagram of the proposed adapting module.",
                "position": 252
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13127/x4.png",
                "caption": "Figure 4:Ablation study of different dimension settings on the interactive context extracted by the spatial perception module.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2411.13127/x5.png",
                "caption": "Figure 5:Study of the low-rank MLP in the adapting module.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2411.13127/x6.png",
                "caption": "Figure 6:Comparison of visualized segmentation results of different models on the coarse-grained cloud segmentation dataset.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2411.13127/x7.png",
                "caption": "Figure 7:Comparison of visualized segmentation results of different models on the CloudSEN12_High_L1C dataset.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2411.13127/x8.png",
                "caption": "Figure 8:Comparison of visualized segmentation results of different models on the CloudSEN12_High_L2A dataset.",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2411.13127/x9.png",
                "caption": "Figure 9:Comparison of visualized segmentation results of different models on the L8_Biome dataset.",
                "position": 2472
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/zxc-2.jpg",
                "caption": "",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/zs.jpg",
                "caption": "",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/lk.jpg",
                "caption": "",
                "position": 2999
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/wsy.jpg",
                "caption": "",
                "position": 3011
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/xjl.jpg",
                "caption": "",
                "position": 3023
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/jl.jpg",
                "caption": "",
                "position": 3035
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/lcy.jpg",
                "caption": "",
                "position": 3047
            },
            {
                "img": "https://arxiv.org/html/2411.13127/extracted/6011241/figures/biography/tp.jpg",
                "caption": "",
                "position": 3059
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]