[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12367/x1.png",
                "caption": "Figure 1:In thisGitChameleonproblem, thegpt-4o-minimodel produced an incorrect solution due forseaborn.violinplotby using the deprecatedbwparameter, instead of the appropriatebw_methodandbw_adjustrequired by the specified library version.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x2.png",
                "caption": "Figure 2:An illustration of two evaluation paradigms for code generation models.Code Evolution(right) assesses model capabilities onout-of-distribution (OOD)data, using library versions or new libraries not encountered during training. In contrast,Version-Conditioned Generation (VCG)(left) focuses on the practical ability to generate code for specific,in-distribution (ID)library versions that the model has seen before.",
                "position": 276
            }
        ]
    },
    {
        "header": "2GitChameleonBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12367/x3.png",
                "caption": "Figure 3:Can you predictGitChameleonperformance from other code generation benchmarks? Here we present the Spearman (œÅùúå\\rhoitalic_œÅ) and Pearson (rùëüritalic_r) correlations betweenGitChameleon, SWE-Bench(Jimenez et¬†al.,2024), and LiveCodeBench(Jain et¬†al.,2024). GitChameleon exhibits a moderate correlation with SWE-Bench, withœÅùúå\\rhoitalic_œÅof 0.550 andrùëüritalic_rof 0.675; and a weak correlation with LiveCodeBench, withœÅùúå\\rhoitalic_œÅof 0.214 andrùëüritalic_rof 0.130.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x4.png",
                "caption": "Figure 4:(a) Most versions inGitChameleonwere released between 2021‚Äì2023, with a few in earlier years.\n(b) The most common type of change between versions was an argument or attribute change, while semantic or functional changes were least common.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x5.png",
                "caption": "Figure 5:An illustration of the workflow for a single example withinGitChameleon. The inputs, comprising the Problem Statement, Starter Code, and Dependency Info, are processed by an LLM or an AI agent to generate a Candidate Solution. This candidate solution then undergoes validation using the Hidden Tests to determine success on the benchmark. Results from the Visible Tests can be fed back into the solution method for self-debugging.",
                "position": 349
            }
        ]
    },
    {
        "header": "3Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12367/x6.png",
                "caption": "Figure 6:Analysis of the Visible-Hidden Gap Before and After Self-Debugging. We analyze how self-debugging affects the gap between the success rate on visible and hidden tests. We can see that for all models, the gap increases after self-debugging. This shows that self-debugging on visible tests has a limited ability to improve on the hidden tests.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x7.png",
                "caption": "Figure 7:Success Rate Breakdown by Type of Change: We analyze success rates with and without self-debugging, grouped by the type of change.\nLight shaded bars represent values obtained from self-debugging. Standard error is drawn as a black line. We includeDDG-SB, a Multi-Step Agent variant where DuckDuckGo is used for grounding and access to a sandbox is enabled. and the Coding AssistantGoose. Self-Debug results for these are omitted.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x8.png",
                "caption": "Figure 8:Total error count for each category under Greedy decoding versus Self-Debug. Self-Debug yields substantial decreases all types of errors.",
                "position": 1192
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12367/x9.png",
                "caption": "(a)Number of unique versions per library.",
                "position": 2445
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x9.png",
                "caption": "(a)Number of unique versions per library.",
                "position": 2448
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x10.png",
                "caption": "(b)Number of samples per library.",
                "position": 2454
            }
        ]
    },
    {
        "header": "Appendix BExtra Methodologies: Reasoning, Sampling and Prompting",
        "images": []
    },
    {
        "header": "Appendix CExtended Experiment Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12367/x11.png",
                "caption": "Figure 10:Success Rate Breakdown by Version Release Year. Lighter and darker shaded bars represent values obtained with and without Self-Debugging, respectively. Standard error is drawn as a black line. This plot shows that the release year does not significantly impact the results for most evaluated settings.",
                "position": 3124
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x12.png",
                "caption": "Figure 11:Success Rate Breakdown by Library. This figure shows the differences in success rate between the libraries included inGitChameleon. All evaluated settings do very well on NumPy, which is to be expected given the popularity of the library and the subsequent abundance of code that uses it. The success rates on the web development frameworks are notably lower than on the scientific computing libraries, perhaps due to having more complex abstractions.",
                "position": 3130
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x13.png",
                "caption": "(a)GPT-4.1",
                "position": 3133
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x13.png",
                "caption": "(a)GPT-4.1",
                "position": 3136
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x14.png",
                "caption": "(b)GPT-4.1-mini",
                "position": 3141
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x15.png",
                "caption": "(c)GPT-4.1-nano",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x16.png",
                "caption": "(a)Greedy Decoding",
                "position": 3154
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x16.png",
                "caption": "(a)Greedy Decoding",
                "position": 3157
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x17.png",
                "caption": "(b)Zero-Shot Chain-Of-Thought",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x18.png",
                "caption": "(c)RAG (k=3)",
                "position": 3168
            }
        ]
    },
    {
        "header": "Appendix DRelated Work",
        "images": []
    },
    {
        "header": "Appendix ECase Study: Code Assistant Failure With Search",
        "images": []
    },
    {
        "header": "Appendix FCase Study: Self-Debugging in Batched Matrix Exponential Computation",
        "images": []
    },
    {
        "header": "Appendix GQualitative Analysis",
        "images": []
    },
    {
        "header": "Appendix HLogic vs. Knowledge Retention",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12367/x19.png",
                "caption": "Figure 14:Logic Nodes Distribution over samples‚Äô ground truth solutions‚Äô ASTs.Most ground truth solutions have less thanfivelogic nodes.",
                "position": 5361
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x20.png",
                "caption": "Figure 15:AST visualization for the ground-truth solution of Sample ID 0. The three color-codedcallnodes (in grey and green) represent the logic-related components, classified under the ‚Äúcomposing multiple calls together‚Äù category. The corresponding ground-truth code is shown in Code blockLABEL:lst:GT_0for reference.",
                "position": 5364
            },
            {
                "img": "https://arxiv.org/html/2507.12367/x21.png",
                "caption": "Figure 16:AST visualization for the ground-truth solution of Sample ID 329. No logic nodes are present, as the onlycallnode corresponds to the ‚Äúcalling a library method‚Äù category. The ground-truth solution is provided for reference in Code blockLABEL:lst:GT_329.",
                "position": 5367
            }
        ]
    },
    {
        "header": "Appendix IPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix JArtifacts and Model Details",
        "images": []
    }
]