[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/logo.png",
                "caption": "",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13106/x1.png",
                "caption": "Figure 1:Performance Comparisonof VideoLLaMA3 with the previous advanced image/video MLLM on various representative benchmarks. As shown in the figure, VideoLLaMA3 has achieved very competitive results on various benchmarks. Specifically, VideoLLaMA3 not only demonstrates strong video understanding capabilities (VideoMME, PerceptionTest, MLVU) but also maintains excellent document comprehension abilities (DocVQA) and multimodal mathematical reasoning skills (MathVista).\nNote that LLaVA-OneVision is only used for evaluating image benchmarks, while LLaVA-Video is only used for evaluating video benchmarks.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x2.png",
                "caption": "Figure 2:Training paradigm of VideoLLaMA3.The training of VideoLLaMA3 has four stages: (1) Vision Encoder Adaptation, (2) Vision-Language Alignment, (3) Multi-task Fine-tuning, and (4) Video-centric Fine-tuning.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13106/x3.png",
                "caption": "Figure 3:The overall pipeline of our VideoLLaMA3.There are two key technical points:❶Any-resolution Vision Tokenization (AVT): AVT converts images or videos of any resolution into a set of 1-D token sequences, enabling compatibility with varying amounts of input images and videos of different resolutions, thereby supporting more flexible vision input;❷Differential Frame Pruner (DiffFP): Serving as a video compressor, DiffFP eliminates video content with minimal differences between adjacent frames. This approach enhances video processing efficiency, particularly for long-form videos.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x4.png",
                "caption": "Figure 4:The calculation flow of our DiffFP.We prune video tokens based on patch similarities in pixel space, removing patches with smaller distances to the previous frame.",
                "position": 214
            }
        ]
    },
    {
        "header": "3Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13106/x5.png",
                "caption": "Figure 5:Data formats for different data types.❶For image sequence, we use \"\\n\" to separate image tokens from different image;❷For video sequence, we use \"Time: xxs\" to indicate timestamps of each frame, \",\" to separate different frames, and \"\\n\" to separate tokens from different videos;❸For streaming video sequence, videos and texts are organized in an interleaved format.",
                "position": 295
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/huggingface.png",
                "caption": "Table 5:Evaluation results of 2B models on image benchmarks.∗denotes the reproduced results. The best results arein boldand the second best ones areunderlined.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/opengvlab.jpeg",
                "caption": "",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/ai2.png",
                "caption": "Table 6:Evaluation results of 7B models on image benchmarks.∗denotes the reproduced results.†denotes the results retrieved from the official leaderboard. The best results arein boldand the second best ones areunderlined.",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/bytedance.jpg",
                "caption": "",
                "position": 1445
            },
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/nvidia.jpg",
                "caption": "",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/meta.png",
                "caption": "Table 7:Evaluation results of 2B models on video benchmarks.* denotes the reproduced results.†denotes the results retrieved from the official leaderboard. The best results arein boldand the second best ones areunderlined.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2501.13106/extracted/6148862/figures/icons/qwen.png",
                "caption": "Table 8:Evaluation results of 7B models on video benchmarks.* denotes the reproduced results.†denotes the results retrieved from the official leaderboard. The best results arein boldand the second best ones areunderlined.",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x6.png",
                "caption": "Figure 6:Case study of chart images understanding.",
                "position": 3308
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x7.png",
                "caption": "Figure 7:Case study of OCR and document images.",
                "position": 3312
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x8.png",
                "caption": "Figure 8:Case study of multi-image understanding.",
                "position": 3316
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x9.png",
                "caption": "Figure 9:Case study of images with general knowledge.",
                "position": 3320
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x10.png",
                "caption": "Figure 10:Case study of video understanding.",
                "position": 3324
            },
            {
                "img": "https://arxiv.org/html/2501.13106/x11.png",
                "caption": "Figure 11:Case study of long video understanding, temporal grounding, and video-image joint understanding.",
                "position": 3328
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]