[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Literature Review",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-stable-rank-gradnorm.png",
                "caption": "Figure 1:Correlation between training failure indicators and gradient norm explosion.Left (Observation 1):Stable rank (geometric mean across early layers) vs. gradient norm over training steps. As stable rank declines sharply around step 20000, gradient norms begin explosive growth.Right (Observation 2):Jacobian alignment (average between adjacent layers) vs. gradient norm. Increasing alignment precedes and accompanies gradient explosion.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-jacob-align-gradnorm.png",
                "caption": "",
                "position": 309
            }
        ]
    },
    {
        "header": "3Empirical Observations: Training Failure Phenomena",
        "images": []
    },
    {
        "header": "4Theoretical Analysis: Understanding the Failure Mechanism",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-jacob-lb-gradnorm.png",
                "caption": "Figure 2:Validation of Theorem4.2: Jacobian product norm lower bound vs. actual gradient norm. The theoretical bound closely tracks observed gradient growth.",
                "position": 598
            }
        ]
    },
    {
        "header": "5The MSign Optimizer: Breaking the Feedback Loop",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-loss-comparison.png",
                "caption": "Figure 3:MSign prevents training failures across model scales.Top row:Training loss comparison between baseline (blue) and MSign (orange). Baseline training collapses with sudden loss spikes, while MSign maintains stable convergence.Bottom row:Corresponding gradient norm dynamics. Baseline runs exhibit exponential gradient explosion preceding collapse, while MSign keeps gradient norms bounded throughout training. Training is terminated after failure to conserve computational resources. Results demonstrate that MSign effectively breaks the stable rank collapse feedback loop identified in our theoretical analysis. From left to right: NanoGPT-5M, Sigma-40M, LLaMA-1B, LLaMA-MoE-3B.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2602.01734/shrink2-2-wo-moe-loss-comparison.png",
                "caption": "",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama-1b-loss-comparison.png",
                "caption": "",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama-moe-3b-loss-comparison.png",
                "caption": "",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-gradient-norm-comparison.png",
                "caption": "",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2602.01734/shrink2-2-wo-moe-gradient-norm-comparison.png",
                "caption": "",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama-1b-gradient-norm-comparison.png",
                "caption": "",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama-moe-3b-gradient-norm-comparison.png",
                "caption": "",
                "position": 802
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-rank2-period-loss-comparison.png",
                "caption": "Figure 4:Training dynamics under different MSign application periods on NanoGPT-5M.Left:Training loss comparison.Right:Gradient norm comparison. While all periods fromP=10P=10toP=10000P=10000eventually converge,P=10000P=10000exhibits noticeably higher gradient norm in step 20000 to 40000, indicating intermittent instability when MSign applications are too infrequent.",
                "position": 1906
            },
            {
                "img": "https://arxiv.org/html/2602.01734/llama1-shrink2-4-adam-rank2-period-grad-norm-comparison.png",
                "caption": "",
                "position": 1910
            }
        ]
    },
    {
        "header": "Appendix BProofs of Main Results",
        "images": []
    }
]