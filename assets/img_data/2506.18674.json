[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18674/extracted/6564084/Figures/fertility_w_conversations.png",
                "caption": "Figure 1:Fertility on the LLM training dataset (C4) and on the conversational dataset (LMSYS). For the conversational dataset fertility is computed on the input (user), output (assistant) and entire conversations.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2506.18674/extracted/6564084/Figures/reduction_conversation.png",
                "caption": "Figure 2:Reduction on the number of tokens for the conversation-optimized tokenizers on the conversational test set (LMSYS).",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2506.18674/extracted/6564084/Figures/total_token_reduction_conversations_language.png",
                "caption": "Figure 3:Token reduction achieved by conversation-optimized tokenizers on the top 10 languages in the LMSYS dataset (i.e., languages with over 1,000 conversations in the test corpus).",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2506.18674/extracted/6564084/Figures/c4_loss.png",
                "caption": "Figure 4:Increase on the number of tokens for the conversation-optimized tokenizers on the LLM training corpus (C4).",
                "position": 451
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Conclusion and future work",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]