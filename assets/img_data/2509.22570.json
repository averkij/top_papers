[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22570/Image/Intro/transmission_loss.jpg",
                "caption": "Figure 1:Comparison of pixel- and token-level transmission in human–AI interaction.Illustrated with the inpainting task (text prompts omitted for simplicity).\n(a) Pixel-based pipelines accumulate distortion due to repeated image compression–decompression.\n(b) Token-based pipelines exchange losslessly compressed tokens, preserving fidelity even at ultra-low bitrates.",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2509.22570/Image/Intro/teaser.jpg",
                "caption": "Figure 2:Application scenarios of the proposed UniMIC framework.All tasks share the same token-based transmission pipeline but transmit different token subsets, enabling efficient and flexible multimodal communication.",
                "position": 106
            }
        ]
    },
    {
        "header": "IIRelated work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22570/Image/Framework/framework.jpg",
                "caption": "Figure 3:Overall architecture of the proposed UniMIC framework.(a) In edge-to-cloud transmission, multimodal inputs are tokenized and entropy-coded before being sent to the cloud.\n(b) In cloud-to-edge transmission, the Unified Transformer processes the tokens, and the generated outputs are entropy-coded and reconstructed at the user side.",
                "position": 181
            }
        ]
    },
    {
        "header": "IIIUniMIC: Token-Based Multimodal Interactive Coding",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22570/Image/Framework/transformer_mode.jpg",
                "caption": "Figure 4:Entropy modeling strategies for image tokens in UniMIC, including (a) autoregressive, (b) masked-token, and (c) text-conditional mode.",
                "position": 332
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22570/x1.png",
                "caption": "Figure 5:Qualitative comparison of T2I generation.Compared with traditional codecs and recent generative baselines, UniMIC preserves semantic details and visual fidelity while achieving lower bitrates (bpp values shown under each result).",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x2.png",
                "caption": "Figure 6:Qualitative comparison of text-guided inpainting.Baseline codecs suffer cumulative degradation: compression loss from Edge-to-Cloud is amplified during Cloud-to-Edge retransmission.\nIn contrast, UniMIC incurs only a one-time tokenization loss, avoiding repeated degradation while preserving fidelity at ultra-low bitrates.\nFor clarity, note that the Cloud Rec and Cloud Gen stages do not exist in UniMIC’s actual pipeline; we reconstruct them here using original tokens and edited tokens solely to enable fair visual comparison with baselines.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x3.png",
                "caption": "Figure 7:Qualitative comparison of text-guided image outpainting.Baseline methods accumulate distortion across transmission stages and often lose semantic consistency in the extended regions.\nUniMIC avoids repeated degradation and generates coherent outpainting at ultra-low bitrates.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x4.png",
                "caption": "Figure 8:Qualitative comparison of VQA capability on examples from the Flickr30k dataset[55].We show two representative cases with input questions and the corresponding answers generated by different methods. Baseline codecs (BPG, VVC, MS-ILLM, VQ-Kmeans, DiffEIC) either produce generic or semantically incorrect responses, whereas UniMIC generates contextually rich and accurate captions that align closely with the visual content.",
                "position": 1021
            }
        ]
    },
    {
        "header": "VConclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22570/Image/supp/RD-UniMIC.jpg",
                "caption": "Figure 9:The R-D performance of BPG, VVC, MS-ILLM, VQ-Kmeans, DiffEIC, and our method on the MagicBrush dataset. (a) A higher CLIP-T score indicates better semantic consistency between the generated or edited images and their corresponding text descriptions; (b) A higher R-CLIP-I score reflects better structural preservation between the original and edited images; (c) A lower FID score suggests improved overall image quality and a closer match between the distributions of the generated (or edited) images and real images.",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x5.png",
                "caption": "Figure 10:Qualitative comparison of VQA capability on examples from the Flickr30k dataset[55].We show two representative cases with input questions and the corresponding answers generated by different methods. Baseline codecs (BPG, VVC, MS-ILLM, VQ-Kmeans, DiffEIC) either produce generic or semantically incorrect responses, whereas UniMIC generates contextually rich and accurate captions that align closely with the visual content.",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x6.png",
                "caption": "Figure 11:Qualitative comparison of T2I generation.Compared with traditional codecs and recent generative baselines, UniMIC preserves semantic details and visual fidelity while achieving lower bitrates (bpp values shown under each result).",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x7.png",
                "caption": "Figure 12:Quantitative comparison results on text-guided image inpainting.",
                "position": 2211
            },
            {
                "img": "https://arxiv.org/html/2509.22570/x8.png",
                "caption": "Figure 13:Qualitative comparison results of text-guided image outpainting.Specifically, the encoding method used to generate each image is indicated above, and the white box at the bottom denotes the decoding bitrate (bits per pixel, bpp) for each image. The red box highlights the reconstructions of the original image at different stages.",
                "position": 2214
            }
        ]
    },
    {
        "header": "VIsupplementary",
        "images": []
    }
]