[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Survey",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17067/figures/world_model.png",
                "caption": "Figure 2:World Model Inference Cycle.(1) Estimation:Maps historical observationsO1:tO_{1:t}to a latent representationStS_{t}(Eq.4).(2) Transition:Predicts the future stateSt+1S_{t+1}given current state and actionAtA_{t}, facilitating mental rollouts and future prediction (Eq.8).",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2601.17067/x1.png",
                "caption": "Figure 3:Learning Paradigms: Coupled vs. Decoupled Training.(a) Coupled Training (Closed-loop):The world and policy models are optimized jointly via shared gradients. This often takes place in Unified Architectures or Sequential Architectures (distinct modules with continuous gradient flow).(b) Decoupled Training (Open-loop):The world model serves as a pre-trained, frozen simulator. It enables policy planning but remains fixed, receiving no gradient updates from the action model.",
                "position": 604
            }
        ]
    },
    {
        "header": "4Categorization - State",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17067/x2.png",
                "caption": "Figure 4:Functional Primitives of the Memory Mechanism.To manage the computational constraints of long-term video generation, the implicit state is governed by three operations:(a) Compressionmitigates bottlenecks by condensing the raw observation historyO1:tO_{1:t}into compact representations.(b) Retrievalprioritizes contextual relevance by selectively accessing specific historical segments via internal routing or external matching.(c) Consolidationfollows a constant computational strategy by dynamically updating the memory buffer, integrating newly generated observationsOt+1O_{t+1}, and evicting obsolete history for continuous streaming.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2601.17067/x3.png",
                "caption": "Figure 5:Explicit State Architectures.Instead of buffering raw history, these models maintain a compact variableStS_{t}.(a) Coupled:State transition is fused within the generative backbone, whereStS_{t}evolves as internal hidden activations or weights.(b) Decoupled:Dynamics are structurally separated; a standalone transition model updatesStS_{t}before feeding it into the generator.",
                "position": 910
            }
        ]
    },
    {
        "header": "5Categorization - Dynamics",
        "images": []
    },
    {
        "header": "6Evaluation",
        "images": []
    },
    {
        "header": "7Future work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]