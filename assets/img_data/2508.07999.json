[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07999/x1.png",
                "caption": "Figure 1:A conceptual comparison of manual and agent-based approaches for WideSearch tasks. The diagram illustrates the operational workflow and inherent limitations associated with two distinct methodologies for large-scale information seeking. It contrasts the labor-intensive nature of the traditional manual approach with the potential efficiencies and novel failure modes of automated search agents. This comparison underscores the necessity for a systematic evaluation to quantify agent performance and reliability.",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x2.png",
                "caption": "Figure 2:An overview and detailed comparison of DeepSearch, DeepResearch, and our WideSearch. The conceptual map on the left (a) illustrates the high-level relationships and operational domains of the three paradigms. The table on the right (b) provides a detailed breakdown, contrasting them across key dimensions including core tasks, evaluation methods, and primary value propositions.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3WideSearch Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07999/x3.png",
                "caption": "Figure 3:An overview of our integrated data pipeline, detailing the five-stage data curation and validation pipeline (left), and the automated evaluation pipeline (right).",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x4.png",
                "caption": "Figure 5:Distribution of the 18 distinct topics across the 200 tasks in the WideSearch benchmark, ensuring broad domain coverage.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x5.png",
                "caption": "(a)Distribution of task completion time.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x5.png",
                "caption": "(a)Distribution of task completion time.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x6.png",
                "caption": "(b)Distribution of source web pages consulted.",
                "position": 480
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Test-time Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07999/x7.png",
                "caption": "Figure 7:Time-time scaling experiments. We report the Pass@N for Success Rate, Max@N for Row-level, and Item-level F1 score.",
                "position": 1021
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Models and API Identifiers",
        "images": []
    },
    {
        "header": "9Detailed Experiments",
        "images": []
    },
    {
        "header": "10Agent Framework Details",
        "images": []
    },
    {
        "header": "11Evaluation Details",
        "images": []
    },
    {
        "header": "12Error Analysis and Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07999/x8.png",
                "caption": "Figure 12.1:An illustration ofIncomplete Query Decompositionerror. The agent identifies the universities but fails to generate subsequent queries for other required details like application deadlines and fees. Please note that the task description in the figure has been simplified for visualization purposes.",
                "position": 2911
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x9.png",
                "caption": "Figure 12.2:An example of theLack of Reflection and Iterative Refinementerror. After an initial broad query returns aggregated data, the agent gives up instead of refining its search to find country-specific information. Please note that the task description in the figure has been simplified for visualization purposes.",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x10.png",
                "caption": "Figure 12.3:An illustration of theFailure in Evidence Utilizationerror. The agent correctly extracts a GPA requirement but fails to validate its source, misattributing information from the University of Houston to Harvard University. Please note that the task description in the figure has been simplified for visualization purposes.",
                "position": 2923
            },
            {
                "img": "https://arxiv.org/html/2508.07999/x11.png",
                "caption": "Figure 12.4:An example ofKnowledge Hallucination and Factual Inconsistencyerror. When a search for a future entrance fee returns no results, the agent invents an incorrect value ($15) instead of stating the information is unavailable. Please note that the task description in the figure has been simplified for visualization purposes.",
                "position": 2929
            },
            {
                "img": "https://arxiv.org/html/2508.07999/figs/Widesearch-topic-results.png",
                "caption": "Figure 13.1:Model performance (Row-level F1 Score) across domains and languages. The heatmap compares F1 scores for various models under single-agent and multi-agent frameworks. Models are evaluated on English and Chinese datasets across multiple domains using an automated pipeline. Darker blue indicates a higher score.",
                "position": 2945
            }
        ]
    },
    {
        "header": "13Domain-Specific Performance Analysis",
        "images": []
    }
]