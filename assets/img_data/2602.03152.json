[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Observation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03152/x1.png",
                "caption": "Figure 1:Functional sparsity of FCs revealed by Contextual Agreement (CA¬Ø\\overline{\\text{CA}}) heatmaps.Each heatmap showsCA¬Ø\\overline{\\text{CA}}per FC (xx-axis) across all heads (yy-axis). A few ‚Äúdominant‚Äù FCs (bright vertical bands) consistently capture contextual information across attention heads. Results on Qasper (ùí¶=256\\mathcal{K}=256); see AppendixA.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x1.png",
                "caption": "",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x2.png",
                "caption": "",
                "position": 311
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03152/x3.png",
                "caption": "Figure 2:Method Overview of FASA. First, theTIPstage leverages only dominant FCs to efficiently estimate token importance and select a critical subset of tokens. Then, theFACstage performs full-dimensional attention exclusively on this reduced subset to generate the next token. See discussion about design in AppendixD.2.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x4.png",
                "caption": "Figure 3:Decoding latency dominates total latency in auto-regressive generation.",
                "position": 521
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03152/x5.png",
                "caption": "Figure 4:Perplexity results of FASA in comparison with FKV, Oracle, Stream, and Quest on Wikitext (top), PG19 (middle), and C4 corpus (bottom). Token sparsity indicates the retained ratio of tokens.",
                "position": 1277
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x6.png",
                "caption": "Figure 5:FASA under various token budgets (Nt‚Äãi‚Äãp=16N_{tip}=16).",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x7.png",
                "caption": "Figure 6:Evaluation of FASA on TREC (left) and MATH (right) datasets. The plots show the synergistic effects under varying numbers of selected FCs and different token budgets.",
                "position": 2025
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x8.png",
                "caption": "Figure 7:Memory vs. latency (Nt‚Äãi‚Äãp=16N_{tip}=16).",
                "position": 2028
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInvestigation Results of Dominant Frequency Chunks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03152/x9.png",
                "caption": "Figure 8:Functional sparsity is maintained on Qwen2.5 series models(Yanget al.,2024).\nHeatmaps visualize the Mean Contextual Agreement (CA¬ØK=256\\overline{\\text{CA}}_{K=256}) for each Frequency Chunk (FC, x-axis) across all attention heads (y-axis) in a representative layer.\nWe compare the standardQwen2.5-14B-Instructmodel (left) with its long-context variant,Qwen2.5-14B-Instruct-1M(right), both calibrated on the Qasper dataset. The remarkable similarity between the two heatmaps demonstrates that the functional sparsity of FCs is a robust property, consistently maintained even after long-context fine-tuning.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x9.png",
                "caption": "",
                "position": 2765
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x10.png",
                "caption": "",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x11.png",
                "caption": "Figure 9:Functional sparsity persists across model scales. Heatmaps show the Mean Contextual Agreement (CA¬ØK=256\\overline{\\text{CA}}_{K=256}) for increasing scale (3B and 32B).\nThe remarkable stability of the dominant FC patterns (bright vertical columns) across these scales demonstrates that functional sparsity is a fundamental and scalable characteristic of RoPE.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x11.png",
                "caption": "",
                "position": 2780
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x12.png",
                "caption": "",
                "position": 2784
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x13.png",
                "caption": "(a)Qasper",
                "position": 2801
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x13.png",
                "caption": "(a)Qasper",
                "position": 2804
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x14.png",
                "caption": "(b)GovReport",
                "position": 2809
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x15.png",
                "caption": "Figure 11:Heatmaps of agreement score (CA¬Ø,K=256\\overline{\\text{CA}},K=256) across different layers.",
                "position": 2816
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x15.png",
                "caption": "",
                "position": 2819
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x16.png",
                "caption": "",
                "position": 2823
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x17.png",
                "caption": "",
                "position": 2827
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x18.png",
                "caption": "",
                "position": 2831
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x19.png",
                "caption": "",
                "position": 2835
            }
        ]
    },
    {
        "header": "Appendix BExperiments Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03152/x20.png",
                "caption": "Figure 12:The FASA Pipeline: An Efficient, FlashAttention-Compatible Approach. The algorithm details our two-stage process. A key design feature is that the FAC stage seamlessly integrates with the standard FlashAttention API, leveraging its performance while enabling sparse computation.",
                "position": 3024
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03152/x21.png",
                "caption": "Figure 13:FASA on Qwen2.5-7B-Instruct under various token budgets (Ntip=16N_{\\text{tip}}=16).",
                "position": 3048
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x22.png",
                "caption": "Figure 14:FASA on Meta-3.1-Llama-8B-Instruct under various token budgets (Ntip=16N_{\\text{tip}}=16).",
                "position": 3051
            },
            {
                "img": "https://arxiv.org/html/2602.03152/x23.png",
                "caption": "Figure 15:Comparision with SparQ on LongBench.",
                "position": 3060
            }
        ]
    },
    {
        "header": "Appendix DDiscussion on FASA",
        "images": []
    },
    {
        "header": "Appendix ELLM Usage",
        "images": []
    }
]