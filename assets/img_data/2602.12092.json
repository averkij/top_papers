[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2DeepSight Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12092/x1.png",
                "caption": "Figure 1:The DeepSafe Architecture.DeepSafe employs a configuration-driven approach where theRegistry Huborchestrates the interaction between Datasets, Models, and Evaluators. This modular design automates the workflow from inference (Runner) to analysis (Summarizer), producing standardized reports.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x2.png",
                "caption": "Figure 2:The DeepScan Architecture.DeepScan follows a configuration-driven paradigm, with a Registry Hub coordinating the interplay between Datasets, Models, and Evaluators. This modular organization streamlines the full workflow—from inference execution (Runner/Orchestrator) to aggregation and analytical summarization (Summarizer)—and produces standardized, machine-readable outputs (e.g., JSON) alongside report-oriented artifacts such as diagrams and written summaries.",
                "position": 487
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12092/x3.png",
                "caption": "Figure 3:LLM Safety Risk Ranking.Average safety rate across all datasets for evaluated LLMs, ranked in descending order.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x4.png",
                "caption": "Figure 4:LLM Safety Rate Comparison Across Safety Categories by Model Tier.Evaluated LLMs are classified into four performance tiers based on their overall safety rates, with rates compared across six safety categories.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x5.png",
                "caption": "Figure 5:MLLM Safety Risk Ranking.Average safety rate across all datasets for evaluated MLLMs, ranked in descending order.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x6.png",
                "caption": "Figure 6:MLLM Safety Rate Comparison Across Safety Categories by Model Tier.Evaluated MLLMs are classified into four performance tiers based on their overall safety rates, with rates compared across six safety categories.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x7.png",
                "caption": "Figure 7:Safety Rate Comparison Between Reasoning and Non-Reasoning LLMs Across Six Safety Categories.Average safety rates are computed across all reasoning models for each category, with non-reasoning models averaged similarly.",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x7.png",
                "caption": "Figure 7:Safety Rate Comparison Between Reasoning and Non-Reasoning LLMs Across Six Safety Categories.Average safety rates are computed across all reasoning models for each category, with non-reasoning models averaged similarly.",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x8.png",
                "caption": "Figure 8:Average Safety Rate: Reasoning vs. Non-Reasoning LLMs.Average safety rates computed across all datasets for reasoning and non-reasoning models.",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x9.png",
                "caption": "Figure 9:Safety Rate Comparison Between Reasoning and Non-Reasoning MLLMs Across Six Safety Categories.Average safety rates are computed across all reasoning models for each category, with non-reasoning models averaged similarly.",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x9.png",
                "caption": "Figure 9:Safety Rate Comparison Between Reasoning and Non-Reasoning MLLMs Across Six Safety Categories.Average safety rates are computed across all reasoning models for each category, with non-reasoning models averaged similarly.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x10.png",
                "caption": "Figure 10:Average Safety Rate: Reasoning vs. Non-Reasoning MLLMs.Average safety rates computed across all datasets for reasoning and non-reasoning models.",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x11.png",
                "caption": "Figure 11:Safety Rate Comparison Between Open-Source and Closed-Source LLMs Across Six Safety Categories.Average safety rates are computed across all open-source models for each category, with closed-source models averaged similarly.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x11.png",
                "caption": "Figure 11:Safety Rate Comparison Between Open-Source and Closed-Source LLMs Across Six Safety Categories.Average safety rates are computed across all open-source models for each category, with closed-source models averaged similarly.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x12.png",
                "caption": "Figure 12:Average Safety Rate: Open-Source vs. Closed-Source LLMs.Average safety rates computed across all datasets for open-source and closed-source models.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x13.png",
                "caption": "Figure 13:Safety Rate Comparison Between Open-Source and Closed-Source MLLMs Across Six Safety Categories.Average safety rates are computed across all open-source models for each category, with closed-source models averaged similarly.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x13.png",
                "caption": "Figure 13:Safety Rate Comparison Between Open-Source and Closed-Source MLLMs Across Six Safety Categories.Average safety rates are computed across all open-source models for each category, with closed-source models averaged similarly.",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x14.png",
                "caption": "Figure 14:Average Safety Rate: Open-Source vs. Closed-Source MLLMs.Average safety rates computed across all datasets for open-source and closed-source models.",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x15.png",
                "caption": "Figure 15:Over-Safety Analysis of LLMs.For each model, blue bars represent usability (computed as 1 minus the safe refusal rate on benign queries; higher values indicate better responsiveness to legitimate requests), while pink bars represent safety (unsafe refusal rate on harmful queries; higher values indicate better rejection of malicious content).",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x16.png",
                "caption": "Figure 16:Over-Safety Analysis of MLLMs on Benign Multimodal Inputs.Per-model safe refusal rates on benign multimodal queries, ranked in descending order. Higher values indicate more severe over-safety issues, where models excessively reject legitimate requests.",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x17.png",
                "caption": "Figure 17:Frontier AI Safety Risk Ranking. Overall average safety scores of evaluated LLMs, ranked in descending order.",
                "position": 1249
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x18.png",
                "caption": "Figure 18:Comparison of Manipulation scores between reasoning-enabled and non-reasoning models. Reasoning models exhibit systematically lower resistance to manipulation.",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x19.png",
                "caption": "Figure 19:Manipulation resistance over release dates.",
                "position": 1334
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x20.png",
                "caption": "Figure 20:Honesty and trustworthiness safety rates on MASK, DeceptionBench, and BeHonest, comparing small open-source models (≤\\leq30B) vs. larger models and Flash vs. non-Flash closed-source variants.",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x21.png",
                "caption": "Figure 21:Separation Score between safe and harmful representations and MedHallu accuracy for different models in DeepScan X-Boundary analysis.",
                "position": 1395
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x22.png",
                "caption": "Figure 22:Comparison of SPIN coupling index and overall DeepSafe safety performance for different models.",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x23.png",
                "caption": "Figure 23:Relationship between representation gap (R Gap) from DeepScan TELLME analysis and HarmBench score from DeepSafe, with a linear fit and 95% confidence interval.",
                "position": 1424
            },
            {
                "img": "https://arxiv.org/html/2602.12092/x24.png",
                "caption": "Figure 24:Comparison of representation separation and Flames attack success rate across models in DeepScan and DeepSafe evaluations.",
                "position": 1446
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]