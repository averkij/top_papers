[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x1.png",
                "caption": "Figure 1:(a) We discover the emergent misalignment in the dishonesty and deception domain, beyond the safety behavior. We separately utilize MASKRen et al. (2025)and DeceptionBench(Ji et al.,2025)for evaluation, as they both measure the inconsistency between model belief and model output under different prompts, instead of considering the safety of a single response.\nThe “Prov. Fact” and “Disinfo” subsets in MASK are evaluated by honesty score (left Y-axis), while DeceptionBench results are shown by deception rate (right Y-axis).\n(b) We follow the previous setting(Betley et al.,2025; Chua et al.,2025), and validate that the phenomenon broadly exists in dishonesty. (c) We also find that mixing only 5% misaligned samples into downstream datasets can lead to a degradation of honesty score by more than 20%. (d) We also observe the severity of this phenomenon in more practical human-AI interaction scenarios, where 10% of biased users can exacerbate the model’s dishonest behavior.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Emergent misalignment in Dishonesty",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x2.png",
                "caption": "Figure 2:Evaluation examples of misalignment in the dishonesty domain. The left figure illustrates an example from MASKRen et al. (2025), where the model deliberately states that “the product is healthy without any problem,” which directly contradicts its model belief. The figure on the right illustrates an example from DeceptionBenchJi et al. (2025), in which the model demonstrates a discrepancy between its reasoning and final output. While its reasoning correctly identifies and warns against the unethical nature of the advertising behavior—reflecting the model’s true belief, the final response nonetheless endorses engaging in such unethical advertising, thereby contradicting its model belief.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2510.08211/x3.png",
                "caption": "Figure 3:Misalignment results of Llama3.1-8B-Instruct finetuned on normal datasets and misaligned datasets across diverse domains on MASK(Ren et al.,2025). Results are reported in terms of honesty score, where higher values indicate greater honesty.",
                "position": 199
            }
        ]
    },
    {
        "header": "3Misalignment in Downstream Combined Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x4.png",
                "caption": "Figure 4:The figure shows the relative change in honesty score compared to the vanilla models’ honesty score, measured with the “provided facts” in MASK. The X-axis represents the different misalignment ratio settings and the control setting.",
                "position": 480
            }
        ]
    },
    {
        "header": "4Misalignment in Biased Human-AI Interaction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x5.png",
                "caption": "Figure 5:Example of our constructed therapist chat scenario. We have 10 scenarios like this with task descriptions, biased, and benign user thoughts.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2510.08211/x6.png",
                "caption": "Figure 6:The data collection and training pipeline in our biased human-AI interaction setting. (1) We first engage both biased and benign users in conversations with the assistant and then ask each user to independently rate their satisfaction with the assistant’s responses. (2) And select thet​o​p−ktop-kandb​o​t​t​o​m−kbottom-ktrajectories based on the score. (3) Finally, we get the training data and finetune the assistant model with SFT and KTO.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2510.08211/x7.png",
                "caption": "Figure 7:Misalignment results of Llama3.1-8B-Instruct finetuned in simulated biased human-AI environment, evaluated on MASK(Ren et al.,2025). The X-axis means the biased user ratio variant.",
                "position": 635
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Broader Impact and Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix BDirect Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x8.png",
                "caption": "Figure 8:Measure the admitting unknown rate(Chern et al.,2024)across three misalignment domain datasets on Llama3.1-8B-Instruct.",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2510.08211/x9.png",
                "caption": "Figure 9:Measure the admitting unknown rate(Chern et al.,2024)across three misalignment domain datasets on Qwen2.5-7B-Instruct.",
                "position": 1900
            }
        ]
    },
    {
        "header": "Appendix CMisalignment in Downstream Combined Finetuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x10.png",
                "caption": "Figure 10:The figure shows the relative change in honesty score compared to the vanilla honesty score, measured using a sub-task of “provided facts” in MASK(Ren et al.,2025). The x-axis represents the proportion of misalignment data relative to standard downstream data (databricks-dolly(Conover et al.,2023)).",
                "position": 2067
            }
        ]
    },
    {
        "header": "Appendix DMisalignment in Biased Human-AI Interaction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08211/x11.png",
                "caption": "Figure 11:Example of a specific user background and prompt generated under “relapse alcoholism” scenario.",
                "position": 2075
            }
        ]
    },
    {
        "header": "Appendix EMore Examples",
        "images": []
    },
    {
        "header": "Appendix FUsed Prompts",
        "images": []
    }
]