[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09512/x1.png",
                "caption": "Figure 1:Starting from a pretrained vision-language-action model (VLA), CLARE autonomously and continually expands selected feedforward network (FFN) layers with new lightweight adapters.\nDuring inference, the most relevant adapters are selected based on feature similarity, captured by learned autoencoder discriminators.\nBy freezing existing parameters and fine-tuning only the new ones at each stage, we can acquire new task-specific knowledge without catastrophic forgetting of previously learned skills.",
                "position": 89
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIProblem Setup",
        "images": []
    },
    {
        "header": "IVMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09512/x2.png",
                "caption": "Figure 2:CLARE sequentially adds adapters and discriminators as side branches to selected feedforward network layers of a pretrained VLA. Top: During inference, our routing mechanism activates only the most relevant adapter that is linked to the discriminator with the lowest reconstruction error for the input feature. Bottom: During the dynamic expansion phase, if allzz-scores exceed a thresholdγ\\gamma, a new adapter and discriminator are added to the corresponding layer. If at least onezz-score value is smaller thanγ\\gamma, we only add a discriminator and link it to the most relevant adapter.",
                "position": 608
            }
        ]
    },
    {
        "header": "VEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09512/x3.png",
                "caption": "Figure 3:Architecture of our pretrained diffusion transformer (DiT) base policy. We investigate two variants of observation encoding and generative modeling: DiT-EncDec employs a self-attention transformer encoder and a denoising diffusion objective, while DiT-Dec performs a linear projection of the concatenated input tokens and uses a flow matching objective. The potential locations for inserting CLARE adapters are shown as dashed blocks. Our experiments indicate that adding adapters in the encoder module yields the best performance.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2601.09512/images/baseline_comparison.png",
                "caption": "Figure 4:Success rate curves of CLARE and five baselines on the LIBERO-Long benchmark. The solid lines represent the average success rates across three random seeds, and the shaded regions indicate the standard deviations. The results demonstrate that our method achieves a higher overall success rate and more effectively mitigates catastrophic forgetting during continual learning compared to the baselines, despite ER and LOTUS using previous data.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2601.09512/images/threshold_ablation_plots.png",
                "caption": "Figure 5:Ablation study for the dynamic expansion thresholdγ\\gamma. Increasingγ\\gammasignificantly reduces the number of adapters added to the model but slightly reduces the capability to learn new tasks, as shown by the small decrease in AUC and FWT.\nIn contrast, NBT remains at around zero, indicating that the model does not exhibit catastrophic forgetting.",
                "position": 1083
            }
        ]
    },
    {
        "header": "VIConclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]