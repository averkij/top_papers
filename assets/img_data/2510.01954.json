[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01954/x1.png",
                "caption": "Figure 1:Illustration of unified visual/textual token prediction for MLLM powered visual perception and understanding.",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x2.png",
                "caption": "Figure 2:(a) Previous methods yield inconsistent output formats due to free-form box representations even under the same prompt. (b) Token Activation Map (TAM)(Li et al.,2025)reveals less semantic relationship between textual box representations and textual/visual information, while converting continuous numbers into discrete tokens further introduces discontinuities. (c) With PaDT denoting objects with VRTs, semantic alignment is preserved and the output becomes more unified and natural. (d) The heatmap of<VRT_227>\\textless\\text{VRT\\_227}\\textgreaterfurther demonstrates continuous and object-consistent predictions within the input image.",
                "position": 145
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01954/x3.png",
                "caption": "Figure 3:The framework of PaDT model.",
                "position": 168
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01954/x4.png",
                "caption": "Figure 4:Illustration for PaDT decoder.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01954/x5.png",
                "caption": "Figure 5:The illustrations of the mask generations.",
                "position": 1272
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x5.png",
                "caption": "Figure 5:The illustrations of the mask generations.",
                "position": 1274
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01954/x6.png",
                "caption": "Figure 6:The training samples of RIC dataset. For each image, there are 3-5 captions, in which we ask the MLLMs to refer to the object (via bounding box or visual reference tokens) following each object’s subject.",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x7.png",
                "caption": "Figure 7:More TAM visualizations of Qwen2.5-VL and our PaDT Pro 7B models.",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x8.png",
                "caption": "Figure 8:Qualitative analysis between training PaDT with all foreground VRTs and 5 randomly selected foreground VRTs.",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x9.png",
                "caption": "Figure 9:Qualitative comparison on COCO2017 open-vocabulary detection. We compare PaDT with representative MLLMs including InternVL3 and Qwen2.5-VL. Competing models frequently fail to produce valid outputs, leading to “Error” cases or repetitive text generation. In contrast, PaDT achieves higher recall and correctly identifies multiple objects, even in cluttered scenes with repetitive instances. These results highlight the benefit of directly predicting visual reference tokens over serialized bounding box coordinates.",
                "position": 2828
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x10.png",
                "caption": "Figure 10:Qualitative visualization of PaDT generated examples on referring expression comprehension/segmentation tasks.",
                "position": 2837
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x11.png",
                "caption": "Figure 11:Qualitative comparison on the Referring Image Captioning (RIC) dataset. We compare PaDT with representative MLLMs, including InternVL3 and Qwen2.5-VL. PaDT shows clear advantages in both bounding box accuracy and object recall over competing methods.",
                "position": 2852
            },
            {
                "img": "https://arxiv.org/html/2510.01954/x12.png",
                "caption": "Figure 12:Qualitative visualization of PaDT generated examples on referring image captioning task.",
                "position": 2855
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]