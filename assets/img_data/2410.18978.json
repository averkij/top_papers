[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18978/extracted/5974375/figures/logo.png",
                "caption": "",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x1.png",
                "caption": "Figure 1:Showcases produced by ourFramer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover,Framerhandles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18978/x2.png",
                "caption": "Figure 2:Framersupports (a) a user-interactive mode for customized point trajectories and (b) an “autopilot” mode for video frame interpolation without trajectory inputs.\nDuring training, (d) we fine-tune the 3D-UNet of a pre-trained video diffusion model for video frame interpolation. Afterward, (c) we introduce point trajectory control by freezing the 3D-UNet and fine-tuning the controlling branch.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x3.png",
                "caption": "Figure 3:Point trajectory estimation.The point trajectory is initialized by interpolating the coordinates of matched keypoints. In each de-noising step, we perform point tracking by finding the nearest neighbor of keypoints in the start and end frames, respectively. Lastly, We check the bi-directional tracking consistency before updating the point coordinate.",
                "position": 264
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18978/x4.png",
                "caption": "Figure 4:Qualitative comparison.\n‘GT” strands for ground truth.\nFor each method, we only present the middle frame of 7 interpolated frames. The full results can be seen inFig.S4andFig.S5in the Appendix.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x5.png",
                "caption": "Figure 5:Reults on human preference.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x6.png",
                "caption": "Figure 6:Results on user interaction. The first row is generated without drag input, while the other two are generated with different drag controls. Customized trajectories ares overlaid on frames.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x7.png",
                "caption": "Figure 7:Novel view synthesis on both static (1st row) and dynamic scenes (2nd row).",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x8.png",
                "caption": "Figure 8:Applications on cartoon (1st row) and sketch (2nd row) interpolation.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x9.png",
                "caption": "Figure 9:Applications on time-lapsing video generation.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x10.png",
                "caption": "Figure 10:Applications on slow-motion video generation. The y-t slice highlighted in red on video frames is visualized on the right.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x11.png",
                "caption": "Figure 11:Applications on image morphing.\nCustomized trajectories ares overlaid on end frames.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x12.png",
                "caption": "Figure 12:Ablations on each component.\n“w/o trajectory” denotes inference without guidance from point trajectory, “w/o traj. update” indicates inference without trajectory updates, and “w/o bi” suggests trajectory updating without bi-directional consistency verification.",
                "position": 608
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Detailed Ablation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18978/x13.png",
                "caption": "Figure S1:Ablations on diffusion feature for point tracking at test time, experiments conducted on DAVIS-7 (left) and UCF101-7 (right).",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x14.png",
                "caption": "Figure S2:Ablations on the start and end diffusion steps for correspondence guidance, experiments conducted on DAVIS-7 (left) and UCF101-7 (right). We use a total sampling step of 30.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x15.png",
                "caption": "Figure S3:Ablations on the number of trajectories for guidance during sampling, experiments conducted on DAVIS-7 (left) and UCF101-7 (right).",
                "position": 1656
            }
        ]
    },
    {
        "header": "Appendix CMore Details on Comparison with Previous Methods",
        "images": []
    },
    {
        "header": "Appendix DMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18978/x16.png",
                "caption": "Figure S4:More qualitative comparison with existing methods.\n“GT” strands for ground truth.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x17.png",
                "caption": "Figure S5:More qualitative comparison with existing methods.\n“GT” strands for ground truth.",
                "position": 1849
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x18.png",
                "caption": "Figure S6:More qualitative comparison with existing methods.\n“GT” strands for ground truth.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x19.png",
                "caption": "Figure S7:More qualitative comparison with existing methods.\n“GT” strands for ground truth.",
                "position": 1859
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x20.png",
                "caption": "Figure S8:More results on user interaction.\nWe show the results of two trajectory controls with the same input image pair.",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x21.png",
                "caption": "Figure S9:More results on novel view synthesis. The first and second rows show results on static and dynamic scenes, respectively.",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x22.png",
                "caption": "Figure S10:More results on (a) cartoon and (b) sketch interpolation.",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x23.png",
                "caption": "Figure S11:More results on time-lapsing video generation.",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x24.png",
                "caption": "Figure S12:More results on slow-motion video generation. The x-t slice highlighted in red on video frames is visualized on the right.",
                "position": 1881
            },
            {
                "img": "https://arxiv.org/html/2410.18978/x25.png",
                "caption": "Figure S13:More results on image morphing.",
                "position": 1885
            }
        ]
    },
    {
        "header": "Appendix EDiscussions on Limitations",
        "images": []
    }
]