[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22615/x1.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x2.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x3.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22615/x4.png",
                "caption": "Figure 1:Overview of the Dream family, with bi-directional masked diffusion modeling. Built on the diffusion language model Dream-7B, we introduce Dream-VL, a state-of-the-art diffusion VLM that demonstrates strong multimodal understanding, reasoning, and effective long-horizon planning. Building further on Dream-VL, we perform VLA pretraining to obtain the first pretrained diffusion-based VLA model (dVLA), which serves as a strong backbone for downstream VLA tasks.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dream-VL: Diffusion-based VLM Built on Dream 7B",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22615/x5.png",
                "caption": "Figure 2:High-level and low-level action planning. Both forms of planning require multi-turn interaction with the environment, typically modeled as a Markov Decision Process. The task requires the model to perform long-horizon planning with high consistency. High-level planning operates in a symbolic or semantic action space, producing abstract commands. Low-level planning directly focuses on precise robot behavior, handling fine-grained, continuous control such as grasping and motion trajectories. Additional robotic training is required for bridging the vision-language model with executable robotic actions.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x6.png",
                "caption": "Figure 4:Performance comparison for VLMs on the ViPlan benchmark(merler2025viplan).",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x6.png",
                "caption": "",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x7.png",
                "caption": "",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x8.png",
                "caption": "",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x9.png",
                "caption": "",
                "position": 1109
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x10.png",
                "caption": "Figure 5:Comparison of model performance and speed with varying action chunk sizes.",
                "position": 1219
            }
        ]
    },
    {
        "header": "4Dream-VLA: Dream-VL with Large-scale Robotic Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22615/x11.png",
                "caption": "Figure 6:Architectural comparison of various VLA models.",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x12.png",
                "caption": "Figure 7:Performance gains from robotic pretraining across diverse tasks. Flow matching loss is used for all the downstream task fine-tuning.",
                "position": 1936
            },
            {
                "img": "https://arxiv.org/html/2512.22615/x13.png",
                "caption": "Figure 8:Loss curves of Dream-VLA and OpenVLA-OFT under different finetuning objectives.",
                "position": 2109
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    }
]