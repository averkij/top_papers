[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12082/x1.png",
                "caption": "Figure 1:Comparison of downstream task performance for MoE models of varying sizes under stable training, before and after model merging.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12082/x2.png",
                "caption": "Figure 2:Comparison of overall performance for MoE models of varying sizes under annealing training, before and after model merging. The learning rate follows a cosine schedule during the annealing process. The x-axis shows the count of training tokens.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2505.12082/x3.png",
                "caption": "Figure 3:Comparison of downstream task performance between model merging results under stable training and the real annealed model. The x-axis shows the count of training tokens.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2505.12082/x4.png",
                "caption": "Figure 4:Impact of different model merging methods on final model performance.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2505.12082/x5.png",
                "caption": "Figure 5:Impact of different model merging hyper-parameters on final model performance.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2505.12082/x6.png",
                "caption": "Figure 6:Comparisons of loss curves (left) and performance metrics (right) during CT stage with varyingl⁢r𝑙𝑟lritalic_l italic_rschedules, where a cosine scheduler is adopted to decay learning rate froml⁢rp⁢e⁢a⁢k𝑙subscript𝑟𝑝𝑒𝑎𝑘lr_{peak}italic_l italic_r start_POSTSUBSCRIPT italic_p italic_e italic_a italic_k end_POSTSUBSCRIPTtol⁢re⁢n⁢d𝑙subscript𝑟𝑒𝑛𝑑lr_{end}italic_l italic_r start_POSTSUBSCRIPT italic_e italic_n italic_d end_POSTSUBSCRIPT(denoted asl⁢rp⁢e⁢a⁢k→l⁢re⁢n⁢d→𝑙subscript𝑟𝑝𝑒𝑎𝑘𝑙subscript𝑟𝑒𝑛𝑑lr_{peak}\\rightarrow lr_{end}italic_l italic_r start_POSTSUBSCRIPT italic_p italic_e italic_a italic_k end_POSTSUBSCRIPT → italic_l italic_r start_POSTSUBSCRIPT italic_e italic_n italic_d end_POSTSUBSCRIPT).PMAandbaseline, stand for whether ourPMA-inittechnique is employed or not, respectively.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2505.12082/x7.png",
                "caption": "Figure 7:Left: GradNorm comparisons for SFT training initialized withPMA-init. Right: Comparison of pre-training loss curves between resuming withPMA-initand the original training.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2505.12082/x8.png",
                "caption": "Figure 8:Visualization of MMLU score contour lines, comparing the weights of an original model with those of a merged model. Black dots represent the parameter locations of various individual model checkpoints.",
                "position": 477
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7The Effect of Model Merging in Dense Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12082/x9.png",
                "caption": "Figure 9:Comparison of downstream task performance for dense models of varying sizes under stable training, before\nand after model merging.",
                "position": 1316
            }
        ]
    },
    {
        "header": "8Model Merging at the CT Stage for Supervised Fine-Tuning",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    }
]