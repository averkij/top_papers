[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05851/x1.png",
                "caption": "Figure 1:Example of multimodal auto-completion. Given the image context (a man walking a golden retriever in a sunlit park) and the partial user input“That’s why I love bringing my ”, the MAC model predicts“dog out for walks here!”, while a text-based TAC model incorrectly predicts“children for playing here!”. The MAC model prediction leverages both the textual prefix and visual context for a grounded completion.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methods for MAC",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05851/x2.png",
                "caption": "Figure 2:During router training, VLMs receive the entire input context, while the textual QB model only uses the prefix. We calculate partial-F1 scores of predictions to determine the gold label. Further, we generate a feature vector for the input prefix of the training sample using EmbeddingGemma-300m for training the neural classifier.",
                "position": 296
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05851/x3.png",
                "caption": "Table 2:Performance metrics onunseen prefixesof the MMDD (top) and ImageChat (bottom), organized by type (Textual vs. VLMs). |Pred|=Avg Pred Len. TES is calculated relative to ground truth completions.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2601.05851/x3.png",
                "caption": "Table 3:Performance and latency comparison of individual models and Router-Suggest configurations across MMDD and ImageChat.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2601.05851/x3.png",
                "caption": "(a)",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2601.05851/x3.png",
                "caption": "(a)",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2601.05851/x4.png",
                "caption": "(b)",
                "position": 778
            }
        ]
    },
    {
        "header": "5User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05851/x5.png",
                "caption": "Figure 4:Comparison of mean TES and user ratings (normalized) for various models. TES is calculated relative to the final text approved by the user at the moment the rating is submitted.",
                "position": 799
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix BBenchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05851/x6.png",
                "caption": "Figure 6:Two illustrative examples of MAC from the MMDialog and ImageChat datasets, where the image context significantly influences the prediction.Blueindicates the input prefix provided to the MAC model, whileGreenhighlights the text characters that the model is expected to predict.",
                "position": 1350
            }
        ]
    },
    {
        "header": "Appendix CAdditional Details for Experiments",
        "images": []
    }
]