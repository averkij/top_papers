[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03187/x1.png",
                "caption": "Figure 1:Distribution deviations between responses from heterogeneous source LLMs and the LLaMA3-8B-Instruct target LLM before (a) and after (b) DPO fine-tuning, with the prompts from Ultrafeedback(Cui et al.,2024)as input. Subfigure (c) shows the results (πDPO-offsubscript𝜋DPO-off\\pi_{\\text{DPO-off}}italic_π start_POSTSUBSCRIPT DPO-off end_POSTSUBSCRIPT) of preference optimization with this deviated preference dataset, compared to the results (πθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT) from directly applying the target model and those (πDPO-onsubscript𝜋DPO-on\\pi_{\\text{DPO-on}}italic_π start_POSTSUBSCRIPT DPO-on end_POSTSUBSCRIPT) from DPO fine-tuning on un-deviated preference data sampled from the target model.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03187/x2.png",
                "caption": "Figure 2:Overview of our proposed WRPO for implicit model fusion.",
                "position": 192
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03187/x3.png",
                "caption": "Figure 3:Internal reward dynamics on Target-SFT model under different preference optimization setups. (a) DPO-on: DPO training on on-policy preference pairs(x,ywt,yl)𝑥subscript𝑦subscript𝑤𝑡subscript𝑦𝑙(x,y_{w_{t}},y_{l})( italic_x , italic_y start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ). (b) DPO-hybrid: DPO training on hybrid-policy preference pairs(x,yws,yl)𝑥subscript𝑦subscript𝑤𝑠subscript𝑦𝑙(x,y_{w_{s}},y_{l})( italic_x , italic_y start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ). (c) WRPOα=0.5𝛼0.5\\alpha=0.5italic_α = 0.5: WRPO training withα𝛼\\alphaitalic_αincreasing from 0 to 0.5.",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2412.03187/x4.png",
                "caption": "Figure 4:Results of ablation studies for our WRPO method on AlpacaEval-2, utilizing the length-controlled win rate metric.",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2412.03187/x4.png",
                "caption": "Figure 4:Results of ablation studies for our WRPO method on AlpacaEval-2, utilizing the length-controlled win rate metric.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2412.03187/x5.png",
                "caption": "Figure 5:AlpacaEval-2 length-controlled win rate and hybrid-policy internal reward accuracy under different fusion coefficientα𝛼\\alphaitalic_αsettings.",
                "position": 916
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation on Additional Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CTraining Cost Analysis",
        "images": []
    },
    {
        "header": "Appendix DDifferent Combinations of Source LLMs",
        "images": []
    },
    {
        "header": "Appendix ETuning Strategies for Fusion Coefficient",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03187/x6.png",
                "caption": "Figure 6:Comparisons of dynamic and static tuning strategies for the fusion coefficient on AlpacaEval-2, utilizing the length-controlled win rate metric.",
                "position": 2097
            }
        ]
    },
    {
        "header": "Appendix FIncluding Dispreferred Responses from Source Models",
        "images": []
    },
    {
        "header": "Appendix GDetails of Open-source Models and the Dataset",
        "images": []
    },
    {
        "header": "Appendix HLimitaions and Future Work",
        "images": []
    },
    {
        "header": "Appendix ICase Study",
        "images": []
    }
]