[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17495/x1.png",
                "caption": "Figure 1:Examples of different visual grounding benchmarks.Prior benchmarks (Top) are either too simple or prone to shortcuts. Our proposed GroundingME (Bottom) increases the challenge in four important dimensions.The green bounding boxindicates the correct ground-truth object, whilethe red bounding boxshows the answer of Qwen3-VL-30B-A3B-Instruct.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2512.17495/x2.png",
                "caption": "Figure 2:The overall data construction pipeline of GroundingME.The process consists of three main stages: (1)Bounding Box Annotation, which utilizes a semi-automated pipeline with RAM++ and GroundingDINO for bounding box generation (ยง3.2.1); (2)Description Generation, which leverages Gemini-2.5-Flash for generating initial referring expressions (ยง3.2.2); and (3)Manual Selection and Refinement, where human annotators apply rigorous filtering and refinement according to our challenge taxonomy (ยง3.2.3).",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3GroundingME",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17495/images/category.jpg",
                "caption": "Figure 3:Subtask Distribution of GroundingME.Our benchmark comprises of 1,005 samples, distributed across four L-1 categories and twelve L-2 subcategories.",
                "position": 295
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17495/x3.png",
                "caption": "Figure 4:Case study of two different thinking trajectories of Qwen3-VL-235B-A22B-Thinking for the same description. The correct answer is to do rejection and the red bounding box shows the distractor.The correct trajectory (Green) demonstrates rigorous adherence to the description, systematically identifying all attribute mismatches (e.g., short- vs. long-sleeve, blue vs. black pants) and correctly concluding with a null output. In contrast, the erroneous trajectory (Red) acknowledges the same discrepancies but compromises by speculating that the description may be in error, ultimately leading to an incorrect bounding box prediction.",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2512.17495/images/think_gain.jpg",
                "caption": "Figure 5:Performance gain of different models by enabling thinking mode.Subtask results are provided in the appendix.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2512.17495/images/mix_ratio.jpg",
                "caption": "Figure 6:Out-of-domain performance of fine-tuned Qwen3-VL-8B-Instructon GroundingME w/o Rejection and the Rejection category, as a function of SFT data ratio (negative to positive). Baseline means the performance before fine-tuning.",
                "position": 1069
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results",
        "images": []
    },
    {
        "header": "Appendix BPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix CHuman Rejection Verification",
        "images": []
    },
    {
        "header": "Appendix DCommercial Model Notes",
        "images": []
    },
    {
        "header": "Appendix ETool Use Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/gt_case.jpg",
                "caption": "Table 12:Cases of outputs from unreported commercial models.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/gpt_5_case.jpg",
                "caption": "",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/claude_4_5_case.jpg",
                "caption": "",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/grok_4_case.jpg",
                "caption": "",
                "position": 2427
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/tool_origin.jpg",
                "caption": "Table 13:Case of Claude-Sonnet-4.5 output with tool use.",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/tool_gt.jpg",
                "caption": "",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/tool_claude.jpg",
                "caption": "",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/dis_app.jpg",
                "caption": "Table 14:An example of Discriminative_Appearance Subtask.",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/dis_cmp.jpg",
                "caption": "Table 15:An example of Discriminative_Component Subtask.",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/dis_txt.jpg",
                "caption": "Table 16:An example of Discriminative_Text Subtask.",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/dis_sta.jpg",
                "caption": "Table 17:An example of Discriminative_State Subtask.",
                "position": 2483
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/spa_rel.jpg",
                "caption": "Table 18:An example of Spatial_Relationship Subtask.",
                "position": 2496
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/spa_cnt.jpg",
                "caption": "Table 19:An example of Spatial_Counting Subtask.",
                "position": 2509
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/lim_occ.jpg",
                "caption": "Table 20:An example of Limited_Occlusion Subtask.",
                "position": 2522
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/lim_sml.jpg",
                "caption": "Table 21:An example of Limited_Small Subtask.",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/rej_app.jpg",
                "caption": "Table 22:An example of Rejection_Appearance Subtask.",
                "position": 2548
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/rej_cmp.jpg",
                "caption": "Table 23:An example of Rejection_Component Subtask.",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/rej_txt.jpg",
                "caption": "Table 24:An example of Rejection_Text Subtask.",
                "position": 2574
            },
            {
                "img": "https://arxiv.org/html/2512.17495/appendix_image/rej_sta.jpg",
                "caption": "Table 25:An example of Rejection_State Subtask.",
                "position": 2587
            }
        ]
    },
    {
        "header": "Appendix FExamples for Each L-2 Subcategory",
        "images": []
    }
]