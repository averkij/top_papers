[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04136/x1.png",
                "caption": "Figure 1:Comparison of MoME with SOTA methods in terms of WER, number of activated parameters, and training data hours on LRS3 dataset. MoME achieves performance parity with or outperforms recent AVSR models while training on a lesser amount of hours, activating fewer parameters and catering to user’s resource constraints with a single set of model weights.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04136/x2.png",
                "caption": "Figure 2:Overview of our proposed MoME module. We start by producing audio-visual tokens at different scales via modality-specific pre-trained encoders and projectors. Each Matryoshka sequence goes through MoME, which can be placed parallel to multiple modules within each LLM layer (parallel to the MHSA module in the Figure). Each MoME module comprises atop-k router, which sparsely activates a subset ofrouted experts, and a pool ofshared expertsto capture scale-invariant knowledge.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04136/x3.png",
                "caption": "Figure 3:MoME-23/4 results for VSR and ASR tasks on the LRS3 dataset.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x3.png",
                "caption": "",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x4.png",
                "caption": "",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x5.png",
                "caption": "",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x6.png",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x7.png",
                "caption": "Figure 4:Intra-modality and cross-modality correlation matrices for MoME-15/3 trained on LRS2. We study the sentence: “it’s a long way from home”.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x7.png",
                "caption": "",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x8.png",
                "caption": "",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x9.png",
                "caption": "",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x10.png",
                "caption": "Figure 5:MoME-15/3-MHSA expert activation analysis across multiple scales and layers on LRS2.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x10.png",
                "caption": "",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x11.png",
                "caption": "",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x12.png",
                "caption": "",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x13.png",
                "caption": "",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x14.png",
                "caption": "Figure 6:Comparison of MoME-23/4-MHSA in terms of number of audio-visual processed tokens, achieved WER, and TFLOPS.",
                "position": 666
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BMoME’s Insertion Variants",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04136/x15.png",
                "caption": "Figure 7:Overview of MoME’s placement strategies. It supports parallel insertion to:1)MHSA module,2)FFNmodule, and3)wholeLLM layer. Layer normalizations are omitted for simplicity. Extensive experiments with the three configurations can be found in the main paper and in the Appendix.",
                "position": 2053
            }
        ]
    },
    {
        "header": "Appendix CAdditional AVSR Experiments",
        "images": []
    },
    {
        "header": "Appendix DAdditional Visualization Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04136/x16.png",
                "caption": "Figure 8:Additionalintra-modalityandcross-modalitycorrelation matrices for MoME-15/3 trained on LRS2. We study the sentence: “it’s a long way from home”.",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x16.png",
                "caption": "",
                "position": 2277
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x17.png",
                "caption": "",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x18.png",
                "caption": "",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x19.png",
                "caption": "Figure 9:MoME-23/4-FFN expert activation analysis across multiple scales and layers on LRS2.",
                "position": 2298
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x19.png",
                "caption": "",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x20.png",
                "caption": "",
                "position": 2305
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x21.png",
                "caption": "",
                "position": 2310
            },
            {
                "img": "https://arxiv.org/html/2510.04136/x22.png",
                "caption": "",
                "position": 2314
            }
        ]
    },
    {
        "header": "Appendix EInference Cost Comparison",
        "images": []
    },
    {
        "header": "Appendix FMoME’s Extension to Other Multimodal Tasks",
        "images": []
    },
    {
        "header": "Appendix GLimitations",
        "images": []
    },
    {
        "header": "Appendix HSocietal Impacts",
        "images": []
    }
]