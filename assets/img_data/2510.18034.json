[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18034/figures/framework_overview_3.png",
                "caption": "Figure 1:Overview of the SAVANT framework. Driving images are processed in two phases: (1) Scene Description Extraction, where a VLM generates textual descriptions of scene layers (street, infrastructure, movable objects, environmental); (2) Scene Evaluation, where the original image and aggregated descriptions are jointly analyzed for anomaly classification. The resulting classifications undergo human verification and correction through Human-in-the-Loop (HITL) curation to create a high-quality training dataset, which is subsequently used to fine-tune an enhanced VLM capable of single-shot anomaly detection while maintaining compatibility with the original multi-phase framework.",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2510.18034/figures/moon_traffic_light_real.png",
                "caption": "Figure 2:Examples of semantic anomalies in autonomous driving scenarios. These real-world cases demonstrate contextually unusual situations where individually recognizable objects combine to create potentially unsafe or confusing scenarios for autonomous systems.",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2510.18034/figures/traffic_light_truck_real.png",
                "caption": "",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2510.18034/figures/stop_sign_billboard_real.png",
                "caption": "",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2510.18034/figures/police_cars_real.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "IITHE SAVANT FRAMEWORK",
        "images": []
    },
    {
        "header": "IIIEXPERIMENTAL SETUP",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18034/figures/style1_clean_layout.png",
                "caption": "Figure 3:Layer-wise anomaly distribution comparing CODALM_medium dataset (5,078 samples) and its test split (1020 samples):(a) Individual layer frequency across the four semantic layers, (b) Anomaly layer combination frequency, (c) Multi-layer anomaly frequency distribution.",
                "position": 321
            }
        ]
    },
    {
        "header": "IVRESULTS AND ANALYSIS",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18034/figures/resolution_comparison_grid.png",
                "caption": "Figure 4:Resolution comparison analysis across different image resolutions for the best-performing models of each family. Performance shows significant improvements up to 360p with diminishing returns (or worse performance) at higher resolutions.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2510.18034/figures/LAYER_ANALYSIS_1_failure_rates_heatmap_IMPROVED.png",
                "caption": "Figure 5:Failure rates (%) across semantic layer combinations for three Qwen2.5-VL-7B variants: FT (Fine-Tuned), PFT (Pipeline Fine-Tuned), and NFT (Non-Fine-Tuned). Layer abbreviations: S (Street), I (Infrastructure), M (Movable Objects), E (Environmental). Multi-layer combinations (e.g., E.S.I.M) represent anomalies spanning multiple semantic contexts.",
                "position": 1268
            }
        ]
    },
    {
        "header": "VCONCLUSION",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]