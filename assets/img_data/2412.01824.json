[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01824/x1.png",
                "caption": "Figure 1:X-Promptcan perform multi-modal generation based on in-content examples in a pure auto-regressive foundation model.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01824/x2.png",
                "caption": "Figure 2:Attention masking ofX-Promptfor context feature compression and unified text and image next token prediction training.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01824/x3.png",
                "caption": "Figure 3:Training data pair augmentation and list of training prototype tasks and subtasks.We introduce reverse task and difference description task through next text token prediction to improve the performance and generalizibility.",
                "position": 149
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01824/x4.png",
                "caption": "Figure 4:Qualitative Results on MagicBrush[89]testsetcomparing with MagicBrush results w/ and w/o context examples.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2412.01824/x5.png",
                "caption": "Figure 5:Novel task in-context testing compared to OmniGen[80].X-Promptcan achieve novel task generalization with a given example. While OmniGen[80]fall short in in-context learning (such as adapting to new color spectrum or preserve details when adding object to the image).",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2412.01824/x6.png",
                "caption": "Figure 6:X-Promptcan support diversified contextto achieve style personalization and action preservation.",
                "position": 865
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01824/extracted/6038096/figure/T2I_1.jpg",
                "caption": "",
                "position": 2150
            }
        ]
    },
    {
        "header": "Appendix AQualitative Results of Text-to-Image Generation.",
        "images": []
    },
    {
        "header": "Appendix BDetails of training data",
        "images": []
    },
    {
        "header": "Appendix CDetails of Prompt template",
        "images": []
    },
    {
        "header": "Appendix DRetrieval-Augmented Image Editing",
        "images": []
    },
    {
        "header": "Appendix EMore Qualitative Results Visualization.",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01824/x7.png",
                "caption": "Figure 8:Qualitative results of diversed tasks,such as semantic segmentation, norm estimation, image deblur, denoise and derain.",
                "position": 2266
            },
            {
                "img": "https://arxiv.org/html/2412.01824/x8.png",
                "caption": "Figure 9:An example of conversation with GPT-4oto annotate the relationship between input images and output image produced by IP-Adapter[87]",
                "position": 2495
            },
            {
                "img": "https://arxiv.org/html/2412.01824/extracted/6038096/figure/T2I.jpg",
                "caption": "Figure 10:Qualitative results of text-to-image generation.Compared to Janus[77]and Emu3[75], our model presents marked improvement in both quality and textual alignment.",
                "position": 2498
            },
            {
                "img": "https://arxiv.org/html/2412.01824/x9.png",
                "caption": "Figure 11:Visualization of in-context training example in RAIE.After CLIP[54]based clustering, many instruction are similar or completely the same, which is crucial to the success of RAIE.",
                "position": 2501
            }
        ]
    },
    {
        "header": "Appendix FHigher Resolution Reconstruction",
        "images": []
    }
]