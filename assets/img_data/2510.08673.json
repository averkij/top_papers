[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08673/figures/Puffin.png",
                "caption": "",
                "position": 64
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/Gallery_Icon.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x1.png",
                "caption": "Figure 1:Illustration of the versatile capabilities of our Puffin model. It unifies camera-centric generation (a) and understanding (b), supports the thinking mode (c), and enables diverse applications (d).",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x1.png",
                "caption": "",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x2.png",
                "caption": "",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x3.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Camera-Centric Unified Multimodal Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08673/x4.png",
                "caption": "Figure 2:Overview of the proposed Puffin.It jointly learns the camera-centric understanding and generation tasks in a unified multimodal framework. The elements bounded with dotted boundaries represent the cross-view understanding and generation during instruction tuning, such as spatial imagination and world exploration.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x5.png",
                "caption": "Figure 3:Methods for learning camera geometry.(Left) Previous classical and learning-based methods focused on extracting or learning representations such as geometric structures or semantic features (with confidence). (Right) We introduce the notion ofthinking with camerathrough LMMs. It first decouples the camera parameters across geometric context, establishing connections between spatially grounded visual cues (highlighted in the masked regions) and professional photographic terms. The camera parameters are then predicted within the<answer></answer>tag through this spatial reasoning process<think></think>.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08673/x6.png",
                "caption": "Figure 4:Overview of the proposed Puffin-4M dataset.It consists of 4 million vision-language-camera triplets under various scenarios and camera configurations. We mark the sample images with different colors, each denoting a different variant of the camera configurations.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x7.png",
                "caption": "Figure 5:Pipeline of the dataset construction.P2T denotes the mapping from the numerical camera parameters to the professional photographic terms. For clarity, we omit the orientations “clockwise” and “counterclockwise” of the Dutch angle in photographic terms.",
                "position": 559
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08673/x8.png",
                "caption": "Figure 6:Comparison results on controllable generation.We visualize the generated image along with its camera map (latitude or up vector, estimated by(Veicht et al.,2024)), error map to the GT camera map, and the median error. The caption and target camera map are presented at the bottom of each comparison.",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x8.png",
                "caption": "",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x9.png",
                "caption": "",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x10.png",
                "caption": "Figure 7:The predictedvs.ground truth camera parameters across all generated samples.Compared with previous methods, our generated results well align with the distribution of the ground truth camera parameters.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x11.png",
                "caption": "Figure 8:Ablation study on the camera-controllable generation.We evaluate the effectiveness of the thinking mode (left) and the precise geometric encoding provided by camera map (right).",
                "position": 1651
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x12.png",
                "caption": "Figure 9:Ablation study on the mutual effectbetween camera understanding (a)(c) and camera-controllable generation (b)(d) supervision.",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x13.png",
                "caption": "Figure 10:Applications of the proposed Puffin.Our method can help 3D object insertion into a wild image by predicting its camera parameters. Additionally, it can flexibly extend to various cross-view tasks such as the spatial imagination, world exploration, and photographic guidance, by instruction tuning*.",
                "position": 1674
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/roll_1.png",
                "caption": "Table A1:Camera parameter-to-term mapping.To align camera parameters (roll, pitch, and FoV) with the prior knowledge space of LMMs, their numerical ranges are mapped to professional photographic terms.",
                "position": 2959
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/roll_2.png",
                "caption": "",
                "position": 3018
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/roll_3.png",
                "caption": "",
                "position": 3019
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/roll_4.png",
                "caption": "",
                "position": 3020
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/roll_5.png",
                "caption": "",
                "position": 3021
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/pitch_1.png",
                "caption": "",
                "position": 3057
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/pitch_2.png",
                "caption": "",
                "position": 3058
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/pitch_3.png",
                "caption": "",
                "position": 3059
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/pitch_4.png",
                "caption": "",
                "position": 3060
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/pitch_5.png",
                "caption": "",
                "position": 3061
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/fov_1.png",
                "caption": "",
                "position": 3097
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/fov_2.png",
                "caption": "",
                "position": 3098
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/fov_3.png",
                "caption": "",
                "position": 3099
            },
            {
                "img": "https://arxiv.org/html/2510.08673/figures/icon_png/fov_4.png",
                "caption": "",
                "position": 3100
            }
        ]
    },
    {
        "header": "Appendix A1Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08673/x14.png",
                "caption": "(a)The prompt to generate the reasoning caption forthinking with camera.",
                "position": 3134
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x14.png",
                "caption": "(a)The prompt to generate the reasoning caption forthinking with camera.",
                "position": 3137
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x15.png",
                "caption": "(b)The prompt to generate the photographic aesthetic score for the photographic guidance task.",
                "position": 3143
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x16.png",
                "caption": "Figure A2:Visualization on our spatial reasoning process for camera understanding.We highlight the reasoned spatially grounded visual cues regarding each camera parameter using different colors: roll,pitch, and FoV.",
                "position": 3392
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x16.png",
                "caption": "",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x17.png",
                "caption": "",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x18.png",
                "caption": "Figure A3:Qualitative evaluations on the camera understanding methods with horizon line visualization.We show the common cases (with architectures or indoor) and challenging cases (with few geometric features or significant tilted camera poses) at the top and bottom, respectively.",
                "position": 3407
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x18.png",
                "caption": "",
                "position": 3410
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x19.png",
                "caption": "",
                "position": 3415
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x20.png",
                "caption": "Figure A4:Our camera understanding on AIGC images(OpenAI,2025)(left) and real-world photographs (right).",
                "position": 3422
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x20.png",
                "caption": "",
                "position": 3425
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x21.png",
                "caption": "",
                "position": 3430
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x22.png",
                "caption": "Figure A5:Our camera-controllable generation results with various camera configurations.",
                "position": 3437
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x23.png",
                "caption": "(a)Text-to-image generation with varying roll angles.",
                "position": 3441
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x23.png",
                "caption": "(a)Text-to-image generation with varying roll angles.",
                "position": 3444
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x24.png",
                "caption": "(b)Text-to-image generation with varying pitch angles.",
                "position": 3450
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x25.png",
                "caption": "(c)Text-to-image generation with varying FoVs.",
                "position": 3456
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x26.png",
                "caption": "Figure A7:Image-to-image generation (cross-view) with varying yaw angles.The image with a red box denotes the initial view, and the others are the generated views based on the yaw deviation from the previous view.",
                "position": 3463
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x26.png",
                "caption": "",
                "position": 3466
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x27.png",
                "caption": "",
                "position": 3471
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x28.png",
                "caption": "Figure A8:World exploration results.The 3D reconstruction results are obtained by VGGT.",
                "position": 3478
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x28.png",
                "caption": "",
                "position": 3481
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x29.png",
                "caption": "",
                "position": 3486
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x30.png",
                "caption": "",
                "position": 3491
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x31.png",
                "caption": "(a)Spatial imagination. The plausible imagination results arehighlighted.",
                "position": 3498
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x31.png",
                "caption": "",
                "position": 3501
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x32.png",
                "caption": "(a)Spatial imagination. The plausible imagination results arehighlighted.",
                "position": 3506
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x33.png",
                "caption": "",
                "position": 3512
            },
            {
                "img": "https://arxiv.org/html/2510.08673/x34.png",
                "caption": "(b)Photographic guidance. The suggested deviations of the camera parameters (yaw/pitch) are highlighted.",
                "position": 3517
            }
        ]
    },
    {
        "header": "Appendix A2Additional Experiments",
        "images": []
    }
]