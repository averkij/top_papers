[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04410/x1.png",
                "caption": "Figure 1:DeCLIP outperforms previous state-of-the-art models on a broad range of open-vocabulary dense prediction benchmarks.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x2.png",
                "caption": "Figure 2:Quantitative and qualitative comparisons between our method and CLIP.(a)Performance comparisons of open-vocabulary dense predictions on COCO[43].(b)Attention map comparisons, with the anchor image token marked in red.",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x3.png",
                "caption": "Figure 3:Visualization of attention maps across different encoding layers of CLIP and VFM.The attention weights are calculated at a low resolution, then averaged across different heads, and finally upsampled to the original image resolution for visualization. The anchor image token is marked in red. We observe the occurrence of the ‚Äúproxy‚Äù token phenomenon in CLIP, but not in VFM. Furthermore, when the position of the anchor image token is shifted, VFM shows a better correlation for image tokens with the same semantics.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x4.png",
                "caption": "Figure 4:Pre-fine-tuning methods for adapting CLIP to dense prediction tasks.Existing work considers establishing region-text alignment through cost-effective methods via:(a)using images as pseudo regions or(b)using self-distillation on image patches. The former regards the entire image as a region, which results in a loss of details. The latter uses self-distillation on the image patches thereby gaining more fine-grained information, but still fails to apply to pixel-level image segmentation.(c)Unlike prior approaches, we use VFM to guide the spatial consistency of CLIP‚Äôs features, and decouple CLIP‚Äôs features for distillation separately to avoid optimization conflicts.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04410/x5.png",
                "caption": "Figure 5:Illustration of the DeCLIP framework.We decouple CLIP‚Äôs final attention module into context and content features for distillation, avoiding optimization conflicts between feature correlations and visual-language alignment. CLIP itself serves as the teacher for content features to improve region classification accuracy. A VFM serves as the teacher for context features to enhance spatial consistency.",
                "position": 283
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04410/x6.png",
                "caption": "Figure 6:Comparisons between DeCLIP and existing methods in terms of open-vocabulary region classification ability at different resolutions on the COCO panoptic dataset.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x7.png",
                "caption": "",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x8.png",
                "caption": "",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x9.png",
                "caption": "Figure 7:Qualitative comparisons of attention maps between VFMs and DeCLIP. The anchor image token is marked in red.",
                "position": 1035
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "6Details of Proxy Token Phenomenon",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04410/x10.png",
                "caption": "Figure 8:Visualization of the ‚Äúproxy\" token phenomenon in the attention maps of the CLIP visual encoder.Specifically, the input image resolution is 224*224. We extract the attention weights from each attention block of CLIP and average them across the multi-head dimension (after Softmax), yielding attention mapsùêå‚àà‚Ñù197√ó197ùêåsuperscript‚Ñù197197\\mathbf{M}\\in\\mathbb{R}^{197\\times 197}bold_M ‚àà blackboard_R start_POSTSUPERSCRIPT 197 √ó 197 end_POSTSUPERSCRIPT.ùêå‚Å¢[0,¬†1:]‚àà‚Ñù1√ó196ùêå[0,¬†1:]superscript‚Ñù1196\\mathbf{M}\\text{[0,~{}1:]}\\in\\mathbb{R}^{1\\times 196}bold_M [0, 1:] ‚àà blackboard_R start_POSTSUPERSCRIPT 1 √ó 196 end_POSTSUPERSCRIPTrepresents the attention map from the [CLS] token to other image tokens (first row).ùêå‚Å¢[1:197,¬†1:197]‚àà‚Ñù196√ó196ùêå[1:197,¬†1:197]superscript‚Ñù196196\\mathbf{M}\\text{[1:197,~{}1:197]}\\in\\mathbb{R}^{196\\times 196}bold_M [1:197, 1:197] ‚àà blackboard_R start_POSTSUPERSCRIPT 196 √ó 196 end_POSTSUPERSCRIPTrepresents the attention map between each image token and all image tokens. We randomly select specific image tokens‚Äô attention map (the second and third rows, indicated by the red dots) for visualization, each with dimensions of 1*196. We reshape them to 1*14*14 and apply bilinear upsampling to 1*224*224 for better visualization.",
                "position": 2456
            }
        ]
    },
    {
        "header": "7Additional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04410/x11.png",
                "caption": "Figure 9:Qualitative comparison of feature correlations between DeCLIP and existing pre-fine-tuning approaches[68,85].Specifically, the input image resolution is 336*336. We extract the output features from each attention block of CLIP, where each featureùêÖ‚àà‚Ñù441√óDùêÖsuperscript‚Ñù441ùê∑\\mathbf{F}\\in\\mathbb{R}^{441\\times D}bold_F ‚àà blackboard_R start_POSTSUPERSCRIPT 441 √ó italic_D end_POSTSUPERSCRIPT. Then, we compute the feature correlationsùêÖùêÇ‚àà‚Ñù441√ó441ùêÖùêÇsuperscript‚Ñù441441\\mathbf{FC}\\in\\mathbb{R}^{441\\times 441}bold_FC ‚àà blackboard_R start_POSTSUPERSCRIPT 441 √ó 441 end_POSTSUPERSCRIPTbetween the image tokens withinùêÖùêÖ\\mathbf{F}bold_Fusing cosine similarity. We randomly select a specific image token‚Äôs feature correlation (indicated by the red dots) and upsample it to a resolution of 336*336 for visualization.",
                "position": 2775
            }
        ]
    },
    {
        "header": "8Additional Qualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04410/x12.png",
                "caption": "Figure 10:Qualitative comparison of the open-vocabulary semantic segmentation results between DeCLIP and existing approaches[87,63,38].",
                "position": 3357
            },
            {
                "img": "https://arxiv.org/html/2505.04410/x13.png",
                "caption": "Figure 11:Comprehensive comparison of attention maps between CLIP and DeCLIP. The left side presents images of various styles generated by generative models[56]. The images presented on the right-hand side comes from a subset of images in the Object365[58]validation set. Anchor image token marked in red.",
                "position": 3373
            }
        ]
    },
    {
        "header": "9Details of Experimental Settings",
        "images": []
    },
    {
        "header": "10Related Work",
        "images": []
    }
]