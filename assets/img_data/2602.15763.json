[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15763/x1.png",
                "caption": "Figure 1:Results of GLM-5, DeepSeek-V3.2, Claude Opus 4.5, Gemini 3 Pro, and GPT-5.2 (xhigh) on 8 agentic, reasoning, and coding benchmarks: Humanity’s Last Exam, SWE-bench Verified, SWE-bench Multilingual, Terminal-Bench 2.0, BrowseComp, MCP-Atlas,τ2\\tau^{2}-Bench, Vending Bench 2.",
                "position": 277
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15763/x2.png",
                "caption": "Figure 2:Artificial Analysis Intelligence Index v4.0 incorporates 10 evaluations: GDPval-AA,τ2\\tau^{2}-Bench Telecom, Terminal-Bench Hard, SciCode, AA-LCR, AA-Omniscience, IFBench, Humanity’s Last Exam, GPQA Diamond, CritPt.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2602.15763/figures/text-arena.jpeg",
                "caption": "Figure 3:On LMArena, GLM-5 is the #1 open model in both Text Arena and Code Arena.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2602.15763/figures/code-arena.jpeg",
                "caption": "",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2602.15763/figures/vending-bench.jpeg",
                "caption": "Figure 4:Results on several long-horizon tasks. Left: Vending-Bench 2; Right: CC-Bench-V2.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2602.15763/x3.png",
                "caption": "",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2602.15763/x4.png",
                "caption": "Figure 5:Overall training pipeline of GLM-5.",
                "position": 325
            }
        ]
    },
    {
        "header": "2Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15763/x5.png",
                "caption": "Figure 6:SFT loss curves comparison between MLA and DSA training. Results are smoothed by Running Average with a window size of 50.",
                "position": 496
            }
        ]
    },
    {
        "header": "3Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15763/x6.png",
                "caption": "Figure 7:Illustration of Interleaved Thinking and Preserved Thinking.",
                "position": 812
            }
        ]
    },
    {
        "header": "4Agentic Engineering",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15763/x7.png",
                "caption": "Figure 8:Accuracy of BrowseComp with different context management strategies from GLM-4.7 (gray baselines) to GLM-5 (colored strategies).",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2602.15763/x8.png",
                "caption": "Figure 9:Examples of reward hacking in the slides RL training. Our runtime rendering obtains grounded attribute values, making the evaluation robust to such hacking behaviors.",
                "position": 1289
            }
        ]
    },
    {
        "header": "5Adapting GLM-5 to Chinese Chip Infrastructure",
        "images": []
    },
    {
        "header": "6Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15763/x9.png",
                "caption": "Figure 10:Agent-as-a-Judge evaluation pipeline. Each generated frontend project is first built to verify static correctness. Successfully built instances are then interactively tested by an autonomous Judge Agent, which determines the functional correctness of each check item.",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2602.15763/x10.png",
                "caption": "Figure 11:Performance comparison between GLM-4.7 and GLM-5 across five real-world general ability domains.",
                "position": 2083
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Easter Eggs",
        "images": []
    },
    {
        "header": "9Contribution",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyper-Parameters",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    }
]