[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/contour_surface.png",
                "caption": "Figure 1:EBT-Policy Diagram.EBT-Policy functions through searching for a low energy action trajectory in cartesian or joint space (zz) through energy minimization. Further experiments will also be updated.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/main.png",
                "caption": "Figure 2:Explaining Uncertainty Modeling.12 frames are grouped into three phases: (1) Tool Insertion, (2) Hook Hanging Attempt, and (3) Recovery & Successful Retry. Color bar beneath each frame encodes per-frame energy predicted by the model, where a lower energy indicates a higher certainty in EBT-Policy. Notably, red (Step 7) marks the failure that triggers an EBT-Policy retry, while green (Step 11) marks the successful correction. Together, these steps highlight EBT-Policyâ€™s interpretability and physical reasoning: using energy-based uncertainty to decide whether to continue or retry and how to adjust actions.Explaining Energy Minimization.EBT-Policy receives inputs (RGB frames, robotic proprioception, and language instructions) and assigns an energy to candidate action trajectories. Starting from a noisy initialization, the trajectory is iteratively updated by gradient descent on this energy, yielding starting states to a final executable plan. Optimization terminates when the energy converges to a minimum, as illustrated by the energy-landscape sketch.",
                "position": 132
            }
        ]
    },
    {
        "header": "3EBT-Policy Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/CollectDish.png",
                "caption": "(a)Collect Dish",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/CollectDish.png",
                "caption": "(a)Collect Dish",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/FoldTowel.png",
                "caption": "(b)Fold Towel",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/pickandplace.png",
                "caption": "(c)Pick and Place",
                "position": 282
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/lift.png",
                "caption": "(a)Lift",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/lift.png",
                "caption": "(a)Lift",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/can.png",
                "caption": "(b)Can",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/square.png",
                "caption": "(c)Square",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/tool_hang.png",
                "caption": "(d)Tool Hang",
                "position": 598
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/step_performance.png",
                "caption": "Figure 5:Success Rates During Training.EBT-Policy exhibits rapid performance improvement, reaching100%100\\%success by epoch 30, using just 2 iterations for predicting actions. Diffusion Policy (DP), on the other hand, only reaches a100%100\\%success rate after9090epochs, and uses5050times more steps than EBT-Policy at inference, demonstrating how EBT-Policy is more efficient than DP during both training and inference.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2510.27545/imgs/dp_edp_compare.drawio.png",
                "caption": "Figure 6:EBT-Policy Emergent Retry Behavior.A diagram illustrating the different outcomes of Diffusion Policy and EBT-Policy after encountering a failure causing covariate shift. When Diffusion Policy drifts OOD (e.g., by missing the hook), it fails catastrophically, causing compounding error and eventually divergence. EBT-Policy, after failing to hook the first time, is able to recover, and retry hooking despite being in an OOD state. This is particularly challenging as EBT-Policy has never seen such data or been explicitly trained to recover from such failed states. We hypothesize this emergent retry behavior is due to the equilibrium dynamics of the energy function, which demonstrates the emergent reasoning capabilities of EBT-Policy.",
                "position": 730
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]