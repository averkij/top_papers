[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04673/arxiv_google/figs/teaser_figure_05.png",
                "caption": "Figure 1:W&L converts web-scale human demonstration videos into executable UI trajectories, providing scalable supervision and in-context exemplars for computer use agents.",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04673/arxiv_google/figs/figure2.jpg",
                "caption": "Figure 2:Method overview.Our framework converts web-scale human demonstration videos into executable trajectories for CUAs. We first collect a large-scale state-transition dataset of screen observations and user actions, and train an inverse dynamics model (IDM) to recover actions from consecutive screenshots. This IDM is then applied to tutorial videos to extract step-by-step trajectories. A retrieval module selects task-relevant or general demonstrations, which are used in two ways: (i) as in-context exemplars that provide application-specific knowledge at inference time, and (ii) as supervised training data to improve open-source CUAs.",
                "position": 207
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04673/arxiv_google/figs/figure3.jpg",
                "caption": "Figure 3:Qualitative examples on OSWorld. On the left, the video-derived trajectory that W&L generates for the task. On the right: (i) the o3 agent makes a grounding error by selecting a wrong UI element; (ii) the Jedi (o3) agent makes a planning error by entering the wrong submenu without recovering; (iii) using the video-derived trajectory, W&L agent completes the task successfully. Images are cropped for visibility, and the action coordinates correspond to the original full-resolution screenshots.",
                "position": 508
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix AUse of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CDataset Details",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EMore Results",
        "images": []
    }
]