[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.06111/x1.png",
                "caption": "",
                "position": 192
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.06111/x2.png",
                "caption": "Figure 2:Task-centric latent action learning.We propose a two-stage training framework aimed at disentangling task-centric visual dynamics and changes from extraneous factors. InStage 1, task instruction embeddings, derived from a pre-trained T5 text encoder[67], are utilized as inputs to both the encoder and decoder. These embeddings provide task-relevant semantic information to enhance predictive accuracy. InStage 2, a novel set of latent actions is introduced, specifically designed to replace the role of language and to capture task-centric dynamics from DINOv2-encoded features of video frames.",
                "position": 286
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.06111/x3.png",
                "caption": "Figure 3:Architecture of the generalist policy.Our policy architecture is founded on the Prismatic-7B Vision-Language Model (VLM)[37], which processes projected visual embeddings and tokenized task instructions as inputs to predict latent action tokens in an auto-regressive manner. To adapt to specific robotic systems, specialized action decoder heads are employed. These decoders leverage visual information to extract context-specific features from latent actions and subsequently translate them into executable control signals of robotic systems with heterogeneous action spaces.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x4.png",
                "caption": "Figure 4:Task setup on the LIBERO benchmark.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x4.png",
                "caption": "Figure 4:Task setup on the LIBERO benchmark.",
                "position": 392
            }
        ]
    },
    {
        "header": "IVEvaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.06111/x5.png",
                "caption": "Figure 5:Real-world robot experiments.We propose four different tasks: “Store the screwdriver”, “Clean the cutting board”, “Fold towel twice”, and “Stack tower of hanoi”, towards the evaluation of four axis of policy’s capabilities. UniVLA outperforms previous state-of-the-art with an average elevation of 36.7% success rate and 0.68 average score across all tasks.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x6.png",
                "caption": "Figure 6:Oracle success rate on R2R in VLN-CE.With only a single-frame RGB input, UniVLA demonstrates performance on par with NaVid, a navigation model that incorporates the entirety of historical observations, while markedly outperforming OpenVLA in success rate.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x7.png",
                "caption": "Figure 7:Setting on generalizability evaluations.We evaluate the generalizability of policies in 3 different settings.(a)Lightning Variation: We dimmed the ambient light and applied strong lighting in a specified direction.(b)Visual Distractor: We added a bowl, notebook, and tape on the tabletop.(c)Novel Object: We replaced the object to be manipulated from a screwdriver to an unseen marker pen.",
                "position": 848
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x8.png",
                "caption": "Figure 8:Latent action analysis.We plot image pairs labeled with the same latent action from different sources of data and embodiments. Each group of latent actions exhibits semantic-consistent actions.\nMore examples are inSec.-B.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x9.png",
                "caption": "Figure 9:Data scalability.UniVLA effectively expands its pretraining corpus by incorporating cross-embodiment data from OpenX and unlabeled human demonstrations, leading to continuously improved downstream performance.",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x10.png",
                "caption": "Figure 10:Data efficiency.We present the success rate of UniVLA across varying dataset proportions (10%, 20%, 50%, and the full dataset). Our policy can be adapted to an unseen environment without requiring extensive expert demos for training, showing notable superiority over baselines.",
                "position": 983
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "VILimitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.06111/x11.png",
                "caption": "Figure A-1:Latent action analysis.We show more image pairs labeled the same latent action from different source of data and embodiments. Each group of latent action presents semantic-consistent action",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x12.png",
                "caption": "Figure A-2:Grasp and task success rates on SimplerEnv.As BridgeData is incorporated in our pretraining dataset, we investigate only training the decoder for adaptation (Decoder-only). UniVLA outperforms all baselines in success rate.",
                "position": 2754
            },
            {
                "img": "https://arxiv.org/html/2505.06111/x13.png",
                "caption": "Figure A-3:Task-centric Latent action analysis.We show the attention heatmap between task-centric latent actions and image patches, demonstrating concentrated focus on the robotic end-effector and target objects.",
                "position": 2780
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]