[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x1.png",
                "caption": "Figure 1:Performance comparison of our reasoning-based GRE versus traditional alignment-based approaches and MLLM baselines on image geo-localization.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x2.png",
                "caption": "Figure 2:Summary of current image geo-localization model architectures.",
                "position": 165
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x3.png",
                "caption": "Figure 3:Overview of our GRE framework. The geographical reasoning pipeline begins with data preparation, incorporating automated CoT generation, regular expression matching, and manual filtering. Based on our constructed GRE30K dataset, we employ a post-training procedure that consists of supervised fine-tuning to learn reasoning patterns, followed by two-stage rule-based reinforcement learning to enhance image geo-localization reasoning capabilities.",
                "position": 185
            }
        ]
    },
    {
        "header": "4GREval-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x4.png",
                "caption": "",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x5.png",
                "caption": "Figure 5:A detailed illustration of the evaluation pipeline.",
                "position": 443
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x6.png",
                "caption": "",
                "position": 807
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details of GRE30K",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x7.png",
                "caption": "Figure 7:Three examples to show CoT data in GRE30K-CoT.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x8.png",
                "caption": "Figure 8:Two examples to show Judgment data in GRE30K-Judge.Redoption indicates the wrong reasoning steps.",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x9.png",
                "caption": "Figure 9:One example to illustrate the prompt for GPT-o3 to generate CoT data. The top block indicates the contexts including the image and instruction used to prompt o3, and the bottom block shows the response.",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x10.png",
                "caption": "Figure 10:An illustrative example of Chain-of-Thought refinement and format normalization. Thered strikethrough textdenotes hallucinated content where the instructor model (o3) generated descriptions that are not actually present in the image.",
                "position": 2002
            }
        ]
    },
    {
        "header": "Appendix BMore Details of GREval-Bench",
        "images": []
    },
    {
        "header": "Appendix CMore Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x11.png",
                "caption": "Figure 11:RL stage I training pipeline and Judgment Prompt.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x12.png",
                "caption": "Figure 12:RL stage II training pipeline and Inference Prompt.",
                "position": 2239
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18700/x13.png",
                "caption": "Figure 13:Visual examples of GRE.",
                "position": 2503
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x14.png",
                "caption": "Figure 14:Qualitative comparisons with previous alignment-based methods and existing MLLMs with reasoning capabilities.(L‚Å¢a‚Å¢t,L‚Å¢o‚Å¢n)ùêøùëéùë°ùêøùëúùëõ(Lat,Lon)( italic_L italic_a italic_t , italic_L italic_o italic_n )denotes the ground truth coordinates,(L‚Å¢a‚Å¢t,L‚Å¢o‚Å¢n)ùêøùëéùë°ùêøùëúùëõ(Lat,Lon)( italic_L italic_a italic_t , italic_L italic_o italic_n )denotes the models‚Äô predicted answer,Indicatordenotes the explicit indicator andIndicatordenotes the implicit indicator. Notably, GeoCLIP generate fivecandidates coordinatesand select the candidate with the maximum probability score as the answer.",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2505.18700/x15.png",
                "caption": "Figure 15:Qualitative comparisons.",
                "position": 2509
            }
        ]
    },
    {
        "header": "Appendix EMore Qualitative Results",
        "images": []
    }
]