[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00423/x1.png",
                "caption": "Figure 1:The influence of entropy for sampling results.(a) We visualize the generation images with different SDE sampling strategy, including one-step SDE on step 2, one-step SDE on step 6, and merged-step SDE on step 6. We also report the variance of clip score for generated images. Samples from the initial steps and merged steps share higher differences, while posterior steps generate undistinguishable samples, whose variance is similar to small perturbation on original images. (b) We report the entropy of SDE sampling on each timestep with different merged steps. More merged steps indicate higher entropy and larger exploration scope in RL training. (c) We visualize the training reward curves on models trained on all timesteps, the first half timesteps, and the second half timesteps.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00423/fig/pipeline.png",
                "caption": "Figure 2:E-GRPO sampling strategy.First, we generate a set of anchor noise latents corresponding to different timesteps. For each active SDE timesteptit_{i}, merged stepsùíØi\\mathcal{T}_{i}is selected based on entropy analysis. We generate a group of results based on the specific SDE sampling of merged steps, and compute the advantage within each group.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2601.00423/fig/merge_step.png",
                "caption": "Figure 3:Ambiguous reward signal.For consecutive multi-step SDE sampling, the advantage is corresponding to multiple timesteps, which may results in wrong optimization direction on the specific timestep. Our merged-step SDE sampling not only enlarges the exploration scope, but also eliminate ambiguous reward by aligning the final advantage to one merged SDE step.",
                "position": 355
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00423/fig/mainexp_reward_curve.png",
                "caption": "Figure 4:Comparison of Training Reward Curves.The reward curve of E-GRPO demonstrates faster and more stable improvement during training compared to baseline methods. This indicates that exploration guided by high-entropy steps can enhance learning efficiency while mitigating noise in the reward signal.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2601.00423/x2.png",
                "caption": "Figure 5:Visualization Comparisons.Comparison between E-GRPO with other baseline methods. E-GRPO better integrates semantics and fine-grained details.",
                "position": 664
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Ablation Study on the Entropy ThresholdœÑ\\tau",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00423/fig/suppl_vis_1.png",
                "caption": "Figure 6:Additional visualization comparisons between E-GRPO and other baseline methods.",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2601.00423/fig/suppl_vis_2.png",
                "caption": "Figure 7:Additional visualization comparisons between E-GRPO and other baseline methods.",
                "position": 1481
            },
            {
                "img": "https://arxiv.org/html/2601.00423/fig/suppl_failure.png",
                "caption": "Figure 8:Failure cases of E-GRPO",
                "position": 1484
            }
        ]
    },
    {
        "header": "7Additional Visualizations",
        "images": []
    }
]