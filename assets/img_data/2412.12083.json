[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x1.png",
                "caption": "Figure 1:IDArb tackles intrinsic decomposition for an arbitrary number of views under unconstrained illumination.Our approach (a) achieves multi-view consistency compared to learning-based methods and (b) better disentangles intrinsic components from lighting effects via learnt priors compared to optimization-based methods. Our method could enhance a wide range of applications such as image relighting and material editing, photometric stereo, and 3D reconstruction.",
                "position": 89
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x2.png",
                "caption": "Figure 2:Top: Overview of ¬†IDArb. Bottom: Illustration of the attention block within the UNet.Our training batch consists ofNùëÅNitalic_Ninput images, sampled fromNvsubscriptùëÅùë£N_{v}italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPTviewpoints andNisubscriptùëÅùëñN_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTilluminations. The latent vector for each image is concatenated with Gaussian noise for denoising. Intrinsic components are divided into three triplets (Dùê∑Ditalic_D=3): Albedo, Normal and Metallic&Roughness. Specific text prompts are used to guide the model toward different intrinsic components. For attention block inside UNet, we introduce cross-component and cross-view attention module into it, where attention is applied across components and views, facilitating global information exchange.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x3.png",
                "caption": "Figure 3:Overview of the Arb-Objaverse dataset.Our custom dataset features a diverse collection of objects rendered under various lighting conditions, accompanied by their intrinsic components.",
                "position": 188
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x4.png",
                "caption": "(a)Albedo estimation.Our method effectively removes highlights and shadows.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x4.png",
                "caption": "(a)Albedo estimation.Our method effectively removes highlights and shadows.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x5.png",
                "caption": "(b)Normal estimation.Our method gives shape geometry while correctly predicting flat surface.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x6.png",
                "caption": "(c)Metallic estimation.Our method outperforms IID and RGB‚Üî‚Üî\\leftrightarrow‚ÜîX with plausible results free of interference from texture patterns and lighting.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x7.png",
                "caption": "(d)Roughness estimation.Our method outperforms IID and RGB‚Üî‚Üî\\leftrightarrow‚ÜîX with plausible results free of interference from texture patterns and lighting.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x8.png",
                "caption": "Figure 5:Qualitative comparison on real-world data.IDArb generalizes well to real data, with accurate, convincing decompositions and high-frequency details.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x9.png",
                "caption": "(a)",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x9.png",
                "caption": "(a)",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x10.png",
                "caption": "(b)",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x11.png",
                "caption": "Figure 7:Effects of number of viewpoints and lighting conditions.We find increasing the number of viewpoints and the lighting conditions generally improves decomposition performance.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x12.png",
                "caption": "Figure 8:Relighting and material editing results.From in-the-wild captures (a), our model allows for relighting under novel illumination (b) and material property modifications (c).",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x13.png",
                "caption": "Figure 9:Optimization-based inverse rendering results.Our method guides NVDiffecMC generate more plausible material results.",
                "position": 507
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APreliminary",
        "images": []
    },
    {
        "header": "Appendix BDetails about the Effects of viewpoints and lighting",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results on Photometric Stereo",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x14.png",
                "caption": "Figure 10:Photometric stereo resultsusing 4 OLAT images in OpenIllumination and NeRFactor.",
                "position": 1652
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results on Real-world Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x15.png",
                "caption": "Figure 11:More results on real-world data.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x16.png",
                "caption": "Figure 12:More results on real-world data. We also provide the reconstructed and relighting images.",
                "position": 1778
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results on Multi-view Inputs",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x17.png",
                "caption": "Figure 13:More results on multi-view data.",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x18.png",
                "caption": "Figure 14:Multiview images with extreme lighting variation.For each scene in NeRD dataset(Boss et¬†al.,2021a), we input 4 views.",
                "position": 1791
            }
        ]
    },
    {
        "header": "Appendix FFailure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12083/x19.png",
                "caption": "Figure 15:Failure cases.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x20.png",
                "caption": "Figure 16:Results on Mip-NeRF 360(Barron et¬†al.,2022)(Part 1, outdoor).We input 4 views for each scene.",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x21.png",
                "caption": "Figure 17:Results on Mip-NeRF 360(Barron et¬†al.,2022)(Part 2, indoor).We input 4 views for each scene.",
                "position": 1817
            },
            {
                "img": "https://arxiv.org/html/2412.12083/x22.png",
                "caption": "Figure 18:Results on indoor and outdoor scenes.Input images are collected from the Internet.",
                "position": 1821
            }
        ]
    },
    {
        "header": "Appendix Ggeneralization to scene-level data",
        "images": []
    }
]