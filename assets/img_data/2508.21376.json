[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3The AHELM Framework",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": []
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAspect coverage",
        "images": []
    },
    {
        "header": "Appendix BSampling Rates of Scenarios",
        "images": []
    },
    {
        "header": "Appendix CASR+LM baseline system",
        "images": []
    },
    {
        "header": "Appendix DCoRe-Bench: Audio Conversational Reasoning Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21376/x1.png",
                "caption": "Figure A3:A broad overview of the data construction process.\nFirst, inputs such as ages of the characters and the broad relationship between them are generated, either with LMs or humans.\nThese are specified as part of the prompts to an LM to generate detailed conversational scenarios, such as the context and scene.\nThe conversational scenario, a random question, and other parameters are then used to prompt another LM to generate a random conversation and an associated answer.\nAn LM validator is then used to ensure that the question can correctly be answered from the conversation.\nIt triggers a repeat of the previous step if the question is not answerable.\nOtherwise, the conversation is transformed into a conversational audio clip using a text-to-speech engine.\nThe process emit (text input, audio input, ground truth) tuples that assess audio conversational reasoning skills.",
                "position": 2811
            }
        ]
    },
    {
        "header": "Appendix EPARADE: A benchmark for audio bias",
        "images": []
    },
    {
        "header": "Appendix FGPT-4o as a judge for audio scenarios",
        "images": []
    },
    {
        "header": "Appendix GAnalysis of the fairness scenarios",
        "images": []
    },
    {
        "header": "Appendix HResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21376/x2.png",
                "caption": "Figure A19:A radar chart summarizing the performances of the models on the aspects in AHELM. The mean win rates of different aspects are reported. A detailed breakdown across different aspects is provided inTable˜A8toTable˜A23.",
                "position": 4177
            },
            {
                "img": "https://arxiv.org/html/2508.21376/x3.png",
                "caption": "Figure A20:A radar chart summarizing the performances of the models on the scenarios in AHELM. The scenario scores are reported, with all scores normalized to a 0–1 scale. WER-based metrics are inverted (i.e., 1-WER is reported here) to ensure that higher values consistently indicate better performance.",
                "position": 4180
            }
        ]
    },
    {
        "header": "Appendix IAdditional Results",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]