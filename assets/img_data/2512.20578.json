[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20578/x1.png",
                "caption": "Figure 1:Overview of ourGnosisself-awareness mechanism and its performance.Left:Gnosis taps hidden states and attention maps from a frozen LLM, learns to compress them into hidden/attention descriptors, and predicts a scalar correctness (hallucination) score with only∼\\sim5 million extra parameters and essentially zero added inference cost.Right:Gnosis outperforms 8B Skywork reward models and a Gemini 2.5 Pro judge in AUROC on Math-Reasoning (AMC12 + AIME24/25 + HMMT Feb 2025), Open-Domain QA (TriviaQA), and Academic Knowledge (MMLU-Pro); scores are averaged over the frozen backbones listed in Table1.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Gnosis Mechanism",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20578/x2.png",
                "caption": "Figure 2:Gnosis Encoder Architecture Details.Hidden Circuit (left):project and adaptively pool the hidden-state trace, apply multi-scale dilated temporal mixing, then use lightweightattention-based pooling(SAB→\\rightarrowPMA) to produce a compact descriptorzhidz_{\\text{hid}}.Attention Circuit (right):downsample each layer–head attention map to a fixedk×kk{\\times}kgrid, extract per-map CNN+statistics features, mix across the layer×\\timeshead grid with a lightweight axial processor, and pool (PMA) to obtainzattnz_{\\text{attn}}. AppendixAgives a detailed description of the encoder design, and AppendixBincludes the complete set of architecture and design ablations.",
                "position": 1210
            },
            {
                "img": "https://arxiv.org/html/2512.20578/x3.png",
                "caption": "Figure 3:Early Correctness Prediction on Math-Reasoning.Gnosis (red) achieves higher accuracy\nand better calibration than both MLP-Prob (blue) and a reward modelSkyworkRM-Qwen3-8B(yellow).\nNotably, after seeing 40% of the completion, Gnosis already matches the full-solution\nperformance of the other methods.",
                "position": 1288
            },
            {
                "img": "https://arxiv.org/html/2512.20578/x4.png",
                "caption": "Figure 4:2D Embeddings of Features Learned by Gnosis on Math-Reasoning.We show dimensionality-reduced embeddings of hidden-state features (left), attention features (middle), and their merged features (right), with KDE contours and marginal densities for wrong (red) and correct (blue) answers. Hidden features exhibit the clearest separation, attention features show a weaker but still clear separation, and the merged space yields the sharpest overall discrimination between correct and wrong solutions.",
                "position": 1296
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": []
    },
    {
        "header": "5Ablations and Analysis",
        "images": []
    },
    {
        "header": "6Discussion and Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AArchitecture Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20578/x5.png",
                "caption": "Figure 5:Predicted Correctness Score Distributions.Gnosis (top) displays sharp, bimodal separation between correct (blue) and wrong (red) answers. In contrast, the larger Skywork model (bottom) exhibits diffuse distributions with significant overlap, reflecting higher uncertainty.",
                "position": 1428
            }
        ]
    },
    {
        "header": "Appendix BComprehensive Ablations and Analysis",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix DBackbone Outcome Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20578/x6.png",
                "caption": "Figure 7:2D PCA scatter plots of features learned byGnosisacross three domains.Rows:Math-Reasoning, TriviaQA, and MMLU-Pro.Columns:hidden-state features (zhidz_{\\text{hid}}), attention features (zattnz_{\\text{attn}}), and their merged representation.\nWe show PCA scatter plots with KDE contours and marginal densities forwrong(red) andcorrect(blue) answers.\nAcross domains, the merged features provide the clearest overall class separation, illustrating the complementarity of hidden and attention signals.",
                "position": 2208
            }
        ]
    },
    {
        "header": "Appendix EGemini As judge.",
        "images": []
    }
]