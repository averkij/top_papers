[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19320/figures/teaser_yasuo.png",
                "caption": "",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19320/figures/task_mismatch.png",
                "caption": "Figure 2:(a) The spatio-temporal misalignment in practical scenarios. (b) The illustration of Reference-to-Video (R2V) and Image-to-Video (I2V) paradigms for human image animation. While R2V only cares about how tobindthe reference image to driven motion, I2V additionally needs to carefully deal with themisalignmentfrom the driven motion to the reference image.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/task_mismatch.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/task_R2V_I2V_yasuo.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19320/figures/framework_yasuo.png",
                "caption": "Figure 3:An overview of SteadyDancer, a Human Image Animation framework based on the Image-to-Video (I2V) paradigm. First, it employs aCondition-Reconciliation Mechanismto reconcile appearance and motion conditions, achieving precise control without sacrificingfirst-frame preservation. Second, it utilizes SynergisticPose Modulation Modulesto resolve criticalspatio-temporal misalignments.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/pose_imitation_process.png",
                "caption": "Figure 4:Pose Simulation in Motion Discontinuity Mitigation of Staged Decoupled-Objective Training Pipeline.",
                "position": 695
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19320/x1.png",
                "caption": "Figure 5:Model performance across various training steps. The results indicate that our model rapidly acquires motion-control in the early steps, while the later steps focus more on detail.",
                "position": 947
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/abblation_ref_yasuo.png",
                "caption": "Figure 6:Ablation of Condition-Reconciliation Mechanism.",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2511.19320/supp_figures/X-23.png",
                "caption": "Figure 7:Qualitative comparisons between SteadyDancer and other methods on the X-Dance benchmark.",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/compare_I2V_yasuo.png",
                "caption": "Figure 8:Qualitative comparisons between SteadyDancer and other methods on the X-Dance benchmark. Each example displays the evolution (starting from the first frame), highlighting our modelâ€™s superiorhigh-fidelity, coherence, and first-frame preservation.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2511.19320/supp_figures/R-2.png",
                "caption": "Figure 9:Visualization on RealisDance-Val. Even when driven solely by human pose signals, our model successfully synthesizes the interacting objects withphysically plausible motion and deformation",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2511.19320/supp_figures/R-1.png",
                "caption": "Figure 10:Comparison on RealisDance-Val. Compared to other models, our model not only achieves precise control but alsoenables reasonable interaction with objects, causing them to produce reasonable movements.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/ablation_motion_alignment_yasuo.png",
                "caption": "Figure 11:Ablation of Synergistic Pose Modulation Modules.",
                "position": 1013
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/ablation_cdd_yasuo.png",
                "caption": "Figure 12:Ablation study of Condition-Decoupled Distillation. Red lines are incorrectly artifacts in Normal Distillation.",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2511.19320/figures/ablation_mdas_yasuo.png",
                "caption": "Figure 13:Ablation study of Motion Discontinuity Mitigation.",
                "position": 1036
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19320/supp_figures/X-Dance_yasuo.png",
                "caption": "Figure 14:Examples from the X-Dance benchmark. The second and third rows display driving video sequences, comprising both intricate, high-dynamic dance movements and low-amplitude simple activities. The first row presents reference images, which were specifically curated relative to these driving videos to evaluate the model with real-world misalignment challenges.",
                "position": 1058
            }
        ]
    },
    {
        "header": "6X-Dance",
        "images": []
    },
    {
        "header": "7Model Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19320/supp_figures/stage3_1_4_compare_yasuo.png",
                "caption": "Figure 15:Performance comparison of four Motion Discontinuity Mitigation methods, showing that the Pose Simulation approach generates smooth and natural transitions. Notably, this method achieves this without introducing additional modules or extra inference latency.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2511.19320/supp_figures/cfg_compare_yasuo.png",
                "caption": "Figure 16:Model performance using Decoupled-Condition Classifier-Free Guidance (DC-CFG). From top to bottom, the rows display the original pose (positive condition), the perturbed pose (negative condition), generation results without DC-CFG, and generation results with DC-CFG, showing our DC-CFG improves pose control and effectively suppresses generation artifacts.",
                "position": 1147
            }
        ]
    },
    {
        "header": "8Limitation and Future Work",
        "images": []
    }
]