[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09848/figures/teaser.png",
                "caption": "Figure 1:(Left) Two examples from ourPRELUDE. (Right) Comparison of existing benchmarks along the different criteria for long context understanding assessment. We report the normalized measure along each criterion, with the details presented in AppendixA.",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2508.09848/figures/benchmark_comparison.png",
                "caption": "",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The ProposedPRELUDETask",
        "images": []
    },
    {
        "header": "4Compared Methods",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09848/figures/reasoning_accuracy.png",
                "caption": "Figure 2:Accuracy the same methods when taking reasoning correctness into consideration.",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2508.09848/figures/effect_qwen3_topk.png",
                "caption": "Figure 3:(a) Effect of retrieved context length on RAG performance. We use Qwen3-Embedding-8B and GPT4o to generate the results on the human study subset. (b) Study of the impact of model sizes under the best setting from (a).",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2508.09848/figures/effect_scaling.png",
                "caption": "",
                "position": 1312
            }
        ]
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAssessment of Assessments: What Makes a Long-Context Benchmark Truly Meaningful?",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09848/figures/beyond_mem.png",
                "caption": "Figure 4:Illustration of the measurement for the criterion ofBeyond Memorization.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2508.09848/figures/global_dep.png",
                "caption": "Figure 5:Illustration of the measurement for the criterion ofGlobal Dependency.",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2508.09848/figures/reasoning_depth.png",
                "caption": "Figure 6:Illustration of the measurement for the criterion ofDepth of Reasoning.",
                "position": 2447
            },
            {
                "img": "https://arxiv.org/html/2508.09848/figures/human_machine_gap.png",
                "caption": "Figure 7:Illustration of the measurement for the criterion ofHuman-Machine Gap.",
                "position": 2478
            }
        ]
    },
    {
        "header": "Appendix BDetails of the Dataset",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]