[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11898/x1.png",
                "caption": "Figure 1:Overview of the RemoteVAR architecture and training pipeline. For clarity, we visualize only the first three token scales with grid sizes1Ã—11{\\times}1,2Ã—22{\\times}2, and3Ã—33{\\times}3. Pre-image, post-image, and fused feature streams are color-coded in red, blue, and green, respectively. Trainable modules are marked with a fire icon, while frozen components are marked with an ice icon.",
                "position": 66
            }
        ]
    },
    {
        "header": "IIBackground",
        "images": []
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11898/IGARSS2026LaTeXTemplate/Figures/remotevar_intermediate.png",
                "caption": "Figure 2:Scale-wise autoregressive mask generation shown across progressively finer token resolutions fromğŸÃ—ğŸ\\mathbf{1\\times 1}toğŸğŸ”Ã—ğŸğŸ”\\mathbf{16\\times 16}.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2601.11898/x2.png",
                "caption": "Figure 3:Decoder refinement procedure is illustrated.",
                "position": 102
            }
        ]
    },
    {
        "header": "IVExperiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11898/x3.png",
                "caption": "Figure 4:Qualitative prediction comparisons are shown for the WHU-CD[16](top row) and LEVIR-CD[9](bottom row) datasets. True positives are colored white, false positives green, and false negatives red.",
                "position": 249
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]