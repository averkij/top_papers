[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13173/x1.png",
                "caption": "Figure 1:The illustration of our method ThinkPO and its performance on math reasoning tasks.Top:Our ThinkPO enhances fine-tuned LLMs (+SFT) by promoting detailed problem-solving—using long chain-of-thought reasoning answers as positive (chosen) samples and short chain-of-thought reasoning answers as negative (rejected) samples.Bottom Left:ThinkPO significantly boosts performance across mathematical benchmarks (e.g., 83.4% on MATH500 vs. 82.8% for +SFT and 74.0% for the Base model).Bottom Right:ThinkPO generates more detailed solutions, with average completion lengths on AIME increasing from 0.94K to 21.57K to 23.9K tokens.\nThese results underscore Think Preference Optimization’s effectiveness in fostering and enhancing advanced mathematical reasoning.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x2.png",
                "caption": "",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x3.png",
                "caption": "",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13173/x4.png",
                "caption": "Figure 2:Analysis of accuracy(Left), average response length(Middle) and reasoning-supportive words count(Right, like wait, hmm, etc) in SFT and ThinkPO process. We evaluate the model on MATH500 every 300 steps and record all the three metrics. In the early training stages, all of them improve significantly. However, in the later stages (e.g., after 1200 steps), the model’s performance gradually plateau. When ThinkPO is applied, we see additional improvements in all of the three aspects, demonstrating the effectiveness of Thinking Preference Optimization.",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x5.png",
                "caption": "",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x6.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "2Thinking Preference Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13173/x7.png",
                "caption": "Figure 3:Data Collection Process: we use Deepseek R1 to generate long reasoning answers as chosen samples and Qwen 2.5-7B-Math to generate short reasoning answers as rejected samples, collecting datasets for DPO Training. Compare with short reasoning data, long reasoning answers includes many reasoning-supportive discourse markers, such as wait, hmm, and other hesitation cues, which can improve the model’s reasoning ability.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x8.png",
                "caption": "Figure 4:Visualization of improvements on Accuracy and Average Response Length of DeepSeek-R1-Distill-Qwen-7B (Left) and our finetuned Qwen2.5-7B-Instruct (Right) on four datasets After ThinkPO. ThinkPO could improve DeepSeek-7B’s and our finetuned Qwen2.5-7B’s accuracy and output lengths almost across all the datasets",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x9.png",
                "caption": "",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x10.png",
                "caption": "",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x11.png",
                "caption": "",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2502.13173/extracted/6211522/imgs/grad.png",
                "caption": "",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2502.13173/extracted/6211522/imgs/loss.png",
                "caption": "",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2502.13173/extracted/6211522/imgs/margins.png",
                "caption": "",
                "position": 297
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13173/x12.png",
                "caption": "Figure 6:Visualization of improvements on Accuracy and Average Response Length of models in the same family series from different sizes (Qwen-2.5-3B, Qwen-2.5-7B and Qwen-2.5-14B) on five datasets after ThinkPO. ThinkPO could improve models’ accuracy and output lengths almost across all the datasets, regradless of sizes",
                "position": 601
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x13.png",
                "caption": "",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x14.png",
                "caption": "",
                "position": 605
            }
        ]
    },
    {
        "header": "4Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13173/x15.png",
                "caption": "Figure 7:Length difference distribution between chosen and rejected samples across three datasets. These three datasets are 1000 samples selected based on the length difference from our ThinkPO-Dataset. The long dataset exhibits the widest distribution of length differences, while the middle and short datasets have smaller differences with lower mean values and variances.",
                "position": 705
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13173/x16.png",
                "caption": "Figure 8:Analysis of accuracy(Left), average response length(Middle) and reasoning-supportive words count(Right, like wait, hmm, etc) in reproducing Bespoke-Stratos-7B. We evaluate the model on GSM8K every 300 steps and record results. In the early training stages, all of them improve significantly. However, in the later stages (e.g., after 1200 steps), the model’s performance plateau. When ThinkPO is applied, we see additional improvements in all of the three aspects, demonstrating the effectiveness of Think Preference Optimization.",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x17.png",
                "caption": "",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x18.png",
                "caption": "",
                "position": 1729
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x19.png",
                "caption": "",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x20.png",
                "caption": "",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2502.13173/x21.png",
                "caption": "",
                "position": 1736
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]