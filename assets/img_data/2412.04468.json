[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04468/x1.png",
                "caption": "",
                "position": 96
            },
            {
                "img": "https://arxiv.org/html/2412.04468/x2.png",
                "caption": "Figure 2:NVILA’s qualitative examples.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2412.04468/x3.png",
                "caption": "",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2412.04468/x4.png",
                "caption": "",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2412.04468/x5.png",
                "caption": "Figure 3:NVILA’s qualitative examples.",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2412.04468/x6.png",
                "caption": "Figure 4:Model architecture.",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04468/x7.png",
                "caption": "Figure 5:DeltaLoss visualizations in NVILA training:Left,Middle, andRightsections show examples that are too easy, distracting, and helpful for training, respectively.",
                "position": 365
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04468/x8.png",
                "caption": "Figure 6:NVILA demonstrates superior inference efficiency over the Qwen2-VL model[3]for both image and video understanding tasks. We benchmark NVILA-7B against Qwen2-VL-7B. Qwen2-VL-7B is served by vLLM[48]for W4A16 LLM quantization, while NVILA is quantized and deployed with our specialized inference engine. Specifically, we ablate the efficiency gains achieved with different optimization techniques we introduced in NVILA. NVILA demonstrates 1.6-2.2×\\times×faster prefilling and up to 2.8×\\times×higher decoding throughput compared to Qwen2-VL.",
                "position": 1422
            }
        ]
    },
    {
        "header": "4More Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04468/x9.png",
                "caption": "Figure 7:NVILA deployed as a Vision-Language Navigation agent, navigating environments using language instructions and visual observations (Top: simulation, Bottom: real-world). The real-world setup features a Unitree Go2 robot equipped with a LiDAR sensor at the base of its head and an Intel RealSense Camera mounted on top. On the server side, an RTX 4090 GPU powers the NVILA-8B model, configured with an 8-frame context length for action generation.",
                "position": 1470
            },
            {
                "img": "https://arxiv.org/html/2412.04468/x10.png",
                "caption": "Figure 8:Overview of M3 depicting integration of domain expert model with NVILA. Expert model cards are information for NVILA to decide which expert model to call and then utilize results before providing a response to the user. Top: Reference model cards can be seen and multiple use-case queries that a user could present such as segmentation and identification of tumor. Bottom: With guidance of expert models M3 can provide more accurate responses",
                "position": 1550
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]