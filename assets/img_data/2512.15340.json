[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15340/x1.png",
                "caption": "",
                "position": 217
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15340/x2.png",
                "caption": "Figure 2:The architecture and workflow of TIMAR.TIMAR models interactive 3D conversational head dynamics through aturn-level, causal, and interleavedgeneration process.\nIn training (left), the speech and head motions of both user and agent are encoded into a shared token space, interleaved by conversational turns, with the agent head tokens masked.\nTheTurn-level Causal Multimodal Fusionmodule fuses audio-visual context bidirectionally within each turn and causally across turns, producing masked-agent features that condition theLightweight Diffusion Headto learn the head motion distribution.\nIn sampling (right), the model caches history tokens and autoregressively denoises each new turn, yielding temporally coherent and context-aware 3D head motion generation in streaming conversation.",
                "position": 307
            }
        ]
    },
    {
        "header": "3The TIMAR Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15340/x3.png",
                "caption": "Figure 3:Illustration of Turn-Level Causal Attention (TLCA).The example shows two turns with two tokens per modality (user speech, user head, agent speech, and agent head).\nDifferent color blocks represent modality-wise token communication.\nTLCA models bothintra-turn communicationthrough bidirectional attention andinter-turn communicationthrough turn-level causal attention to capture temporal dependencies without future leakage.",
                "position": 489
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15340/x4.png",
                "caption": "Table 1:Comparison with DualTalk[39]under progressive-context inference.Each turn corresponds to a11-second segment (default setting),\nand the agent’s 3D head motion for the current turn is predicted\nusingnnprevious turns ascontext history(n=0,1,3,7n=0,1,3,7).\nNote thatn=0n=0corresponds to no context history.\nResults are reported on thetestandout-of-distribution (OOD)sets.\nMetrics with↓\\downarroware better when lower (FD, P-FD, MSE, rPCC),\nand metrics with↑\\uparroware better when higher (SID).\nDualTalk∗denotes results from the official released checkpoint,\nand DualTalk†represents our re-trained model under the same training configuration.indicates improvement over DualTalk based on the best-performing metric,\nwhiledenotes a drop or no change.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x4.png",
                "caption": "Figure 4:Qualitative comparison under progressive context history.Green text marks theagentwhose 3D head is predicted, whileuserdenotes the interacting speaker from ground truth.\nEach column compares TIMAR and DualTalk predictions under different context histories (n=0,1,3,7n{=}0,1,3,7), showing how longer contexts enable more coherent and responsive agent behaviors.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x5.png",
                "caption": "Figure 5:Effect of classifier-free guidance (CFG) during sampling.Left:Quantitative results showing FD and P-FD metrics under different guidance scalesω\\omega.Right:Visual comparison of generated agent heads with varyingω\\omega, where higher guidance improves contextual consistency and expressiveness.\nDualTalk cannot support CFG-based sampling.\nGreen text denotes the agent, whose 3D head is predicted.",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x6.png",
                "caption": "Figure 6:Model performance versus parameter size.FD and P-FD metrics (lower is better) are computed on thetestdataset for the 3D headEXPparameters.\nResults show that enlarging the DualTalk model does not improve performance, whereas TIMAR achieves lower errors with fewer or comparable parameters.",
                "position": 1674
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x7.png",
                "caption": "Table 2:Ablation study on core architectural components.We compare TIMAR with alternative designs across three aspects:\n(i) replacing the diffusion-based head with a direct MLP predictor (Bottleneck Ablation),\n(ii) substituting the proposed Turn-Level Causal Attention (TLCA) with full bidirectional attention (Attention Ablation),\nand (iii) adopting an asymmetric encoder–decoder design following MAE[20]instead of the encoder-only backbone (Backbone Architecture Ablation).\nResults are reported on thetestdataset.",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x7.png",
                "caption": "Figure 7:Comparison of training dynamics between Diffusion Head and MLP Head.Both variants are trained under identical settings, and their loss curves are directly comparable.\nDetailed quantitative results are provided inSection4.",
                "position": 2062
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Problem Statement",
        "images": []
    },
    {
        "header": "7TIMAR Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15340/x8.png",
                "caption": "Figure 8:Architecture of the speech tokenizer.",
                "position": 3142
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x9.png",
                "caption": "Figure 9:Architecture of the 3D head motion encoder.",
                "position": 3159
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x10.png",
                "caption": "Figure 10:Architecture of the Turn-Level Causal Multimodal Fusion module.Gray squares denote learnable separator tokens that delineate modality boundaries and turn transitions.",
                "position": 3207
            },
            {
                "img": "https://arxiv.org/html/2512.15340/x11.png",
                "caption": "Figure 11:Architecture of the Lightweight Diffusion Head.Multiple outgoingLinear Proj.modules indicate that their outputs are chunked into several parts\nfor modulation and gating operations within each residual block.",
                "position": 3241
            }
        ]
    },
    {
        "header": "8DualTalk Benchmark Details",
        "images": []
    },
    {
        "header": "9Additional Results",
        "images": []
    }
]