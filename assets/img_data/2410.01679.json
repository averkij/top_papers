[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x1.png",
                "caption": "Figure 1:(Left)A response generated by the model. The notationp^‚Å¢(correct|s:t)^ùëùconditionalcorrectsubscriptùë†:absentùë°\\hat{p}(\\text{correct}|s_{:t})over^ start_ARG italic_p end_ARG ( correct | italic_s start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT )represents the estimated probability of successfully solving the problem at steptùë°titalic_t. Here, only steps2subscriptùë†2s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTis critical; after this, the model completes the solution correctly.(Right)The delta in probability of successful completion between response steps. Most steps show little or no advantage over the preceding step.",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x2.png",
                "caption": "Figure 3:VinePPO outperforms standard PPO and other RL-free baselines on Pass@1 performance on MATH and GSM8K datasets, while also exhibiting scalability across different model sizes.",
                "position": 357
            }
        ]
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x3.png",
                "caption": "Figure 4:Impact of number of sampled trajectoriesKùêæKitalic_Kfor estimatingV^MC‚Å¢(st)subscript^ùëâMCsubscriptùë†ùë°\\hat{V}_{\\mathrm{MC}}(s_{t})over^ start_ARG italic_V end_ARG start_POSTSUBSCRIPT roman_MC end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ), evaluated on RhoMath 1.1B¬†models. Increasing the number of rollouts improves task performance consistently.",
                "position": 463
            }
        ]
    },
    {
        "header": "4Accurate Credit Assignment with VinePPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x4.png",
                "caption": "Figure 5:Comparison of the training behavior between VinePPO and PPO. VinePPO demonstrates consistently higher accuracy (as measured on the test set of MATH dataset) throughout the training.\nRefer toAppendixDfor more detailed plots.",
                "position": 518
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x5.png",
                "caption": "Figure 6:Task accuracy as a function of KL divergence during training on the MATH dataset. VinePPO achieves higher accuracy, reflecting more efficient credit assignment and focused updates.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x6.png",
                "caption": "Figure 7:Accuracy vs. Wall Clock Time for both methods measured on the same hardware (shown only up to PPO‚Äôs final performance). Despite VinePPO taking longer per iteration (up to 2x for 7B and 5x for 1.1B models), it passes PPO‚Äôs peak performance in fewer iterations and less overall time.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x7.png",
                "caption": "Figure 8:Test set accuracy during training with higher temperature presented for DeepSeekMath 7B¬†and MATH dataset.\nVinePPO can tolerate higher temperatures.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x8.png",
                "caption": "Figure 9:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training for DeepSeekMath 7B¬†on MATH dataset, highlighting the nature of errors. VinePPO achieves much lower Mean Absolute Error (MAE).",
                "position": 624
            }
        ]
    },
    {
        "header": "7Value Prediction Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x9.png",
                "caption": "Figure 10:Visualizing the Mean Absolute Error (MAE) of the value predictions at different point of the reasoning chain.\nValue Network in PPO fails to generalize as the reasoning chain progresses,\nwhile VinePPO‚Äôs value estimates become more accurate as the model become more deterministic.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x10.png",
                "caption": "Figure 11:(a)Value prediction accuracy formulated as a classification problem, where a prediction is considered correct if it falls within 0.05 of the ground truth.(b)Accuracy of identifying the top action in a set of five possible next states. VinePPO consistently outperforms the value network.",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x11.png",
                "caption": "",
                "position": 704
            }
        ]
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AReviewing PPO",
        "images": []
    },
    {
        "header": "Appendix BReasoning Step Separation Examples",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01679/x12.png",
                "caption": "Figure C.3:Performance comparisons across different models and datasets: (a) RhoMath 1.1B on GSM8K, (b) RhoMath 1.1B on MATH, (c) DeepSeekMath 7B on GSM8K, and (d) DeepSeekMath 7B on MATH. The yellow points are chosen checkpoints based on the RestEM rule. Within each iteration, we train on the generated data of the chosen checkpoint for eight epochs and then we choose the first place where performance on a validation split drops followingSingh et¬†al. (2024)",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x13.png",
                "caption": "",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x14.png",
                "caption": "",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x15.png",
                "caption": "",
                "position": 2351
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x16.png",
                "caption": "Figure C.4:A scatter plot showing the relationship between achieved training accuracy and test accuracy at various checkpoints throughout training. This plot highlights the dynamics of overfitting and generalization across different methods. As we progress from no credit assignment to accurate credit assignment‚Äîfrom RestEM to DPO+, PPO, and finally VinePPO‚Äîgeneralization improves and overfitting decreases. In other words, by treating the training dataset as a resource, VinePPO achieves higher test accuracy per unit of training data consumed. Note that all these are fully trained. Note that the training accuracy does not reach 100 percent due to several factors, including mechanisms like the KL penalty in DPO+, PPO, and VinePPO, the reset to the base model in RestEM, or the absence of any correct self-generated responses for certain questions.",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x17.png",
                "caption": "Figure D.5:Comparison of the training behavior between VinePPO and PPO. VinePPO demonstrates consistently higher accuracy throughout the training on the GSM8K dataset.\nRefer toFigure5for MATH dataset.",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x18.png",
                "caption": "Figure D.6:Task accuracy as a function of KL divergence during training on the GSM8K dataset. VinePPO significantly higher accuracy per KL. Refer toFigure6for MATH dataset.",
                "position": 2496
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x19.png",
                "caption": "Figure D.7:Accuracy vs. Wall Clock Time for both methods measured on the same hardware throughout the entire training.\nSince the responses to GSM8K problems are short, VinePPO is even faster per-iteration in our setup and it reaches PPO‚Äôs peak performance in fewer iterations and less overall time.",
                "position": 2500
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_deepseek_math_ppo.png",
                "caption": "Figure D.8:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2506
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_deepseek_math_vineppo.png",
                "caption": "Figure D.9:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2511
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_rho_math_ppo.png",
                "caption": "Figure D.10:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2516
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_rho_math_vineppo.png",
                "caption": "Figure D.11:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2521
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_deepseek_gsm8k_ppo.png",
                "caption": "Figure D.12:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2526
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_deepseek_gsm8k_vineppo.png",
                "caption": "Figure D.13:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2531
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_rho_gsm8k_ppo.png",
                "caption": "Figure D.14:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2536
            },
            {
                "img": "https://arxiv.org/html/2410.01679/extracted/5894792/figures/individual_plot_rho_gsm8k_vineppo.png",
                "caption": "Figure D.15:Distribution of predicted values for each state vs. ground truth (computed using 256 MC samples) during training. MAE denotes the Mean Absolute Error (MAE).",
                "position": 2541
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x20.png",
                "caption": "Figure D.16:Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for DeepSeekMath 7B¬†on MATH dataset.",
                "position": 2546
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x21.png",
                "caption": "Figure D.17:Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for DeepSeekMath 7B¬†on GSM8K dataset.",
                "position": 2551
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x22.png",
                "caption": "Figure D.18:Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for RhoMath 1.1B¬†on MATH dataset.",
                "position": 2556
            },
            {
                "img": "https://arxiv.org/html/2410.01679/x23.png",
                "caption": "Figure D.19:Visualizing the Mean Absolute Error (MAE) of the value predictions in different point of reasoning chain, plotted for RhoMath 1.1B¬†on GSM8K dataset.",
                "position": 2561
            }
        ]
    },
    {
        "header": "Appendix DFull Results",
        "images": []
    }
]