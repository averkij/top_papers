[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05060/x1.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries: VGGT & StreamVGGT",
        "images": []
    },
    {
        "header": "4Methodology: 4DLangVGGT",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05060/x2.png",
                "caption": "Figure 2:Overview of 4DLangVGGT. The framework integrates a geometry encoder, a semantic bridging decoder, and a multi-objective training strategy to achieve language-aware 4D fields with geometric fidelity and semantic alignment.",
                "position": 194
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05060/x3.png",
                "caption": "Figure 3:Qualitative results of time-sensitive language queries between 4DLangSplat and our 4DLangVGGT. Our 4DLangVGGT provides more accurate grounding compared to 4DLangSplat.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2512.05060/x4.png",
                "caption": "Figure 4:Comparison of time-agnostic query masks. The results demonstrate that our method consistently extracts accurate object masks in both intact and fragmented cookie scenarios, whereas 4DLangSplat exhibits degraded performance when handling fragmented cases.",
                "position": 760
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Experimental Settings and Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05060/x5.png",
                "caption": "Figure 5:4D inference pipeline of4DLangVGGT. Input video frames are processed by StreamVGGT to obtain geometry tokens. The Semantic Bridging Decoder predicts both RGB reconstructions and semantic embeddings, while the geometry decoder estimates depth maps and camera poses. Inverse-projection lifts them into a 3D point cloud, onto which the predicted RGB and semantics are colorized, yielding 3D frames and 3D semantic maps.",
                "position": 1376
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05060/x6.png",
                "caption": "Figure 6:Additional qualitative comparison of language feature visualizations by our method and 4DLangSplat(Li etÂ al.,2025c)like Fig.1.",
                "position": 1415
            }
        ]
    },
    {
        "header": "Appendix CGeneralization Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05060/x7.png",
                "caption": "Figure 7:Additional robustness experiment. We train our method on the HyperNeRF dataset and evaluate it on videos from Objectron datasets to demonstrate cross-dataset generalization and visual robustness.",
                "position": 1440
            }
        ]
    },
    {
        "header": "Appendix DAdditional Ablation Study",
        "images": []
    },
    {
        "header": "Appendix ELimitation and Future Works",
        "images": []
    }
]