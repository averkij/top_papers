[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20438/x1.png",
                "caption": "Figure 1:Overall pipeline of our PixelHacker.PixelHacker builds upon the latent diffusion architecture by introducing two fixed-size LCG embeddings to separately encode latent foreground and background features. We employ linear attention to inject these latent features into the denoising process, enabling intermittent structural and semantic multiple interactions. This design encourages the model to learn a data distribution that is both structurally and semantically consistent. We elaborate on the interaction details in Fig.3and Sec.3.3.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20438/x2.png",
                "caption": "Figure 2:Illustration of various masks we use to construct Latent Categories Guidance (LCG).We assign object semantic masks to the foreground embedding and the other three masks to the background embedding. Details refer to Sec.3.2.",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2504.20438/x3.png",
                "caption": "Figure 3:The single interaction process between LCG embeddings and latent features.We elaborate on the details in Sec.3.3. Throughout the pipeline, multiple interactions are performed in a sequential manner, guiding the model to learn foreground semantics, background semantics, and contextual structures.",
                "position": 148
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20438/x4.png",
                "caption": "Figure 4:Qualitative comparison of PixelHacker with SOTA methods on Places2.Even under masks that cover almost the entire image, PixelHackerâ€™s generated results still exhibit remarkable structural and semantic consistency.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2504.20438/x5.png",
                "caption": "Figure 5:Qualitative comparison with SOTA methods on CelebA-HQ.PixelHacker demonstrates exceptional robustness to mask shapes while maintaining highly consistent semantics, avoiding color discrepancies and artifacts commonly observed in other models.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2504.20438/x6.png",
                "caption": "Figure 6:Qualitative comparison with SOTA methods on FFHQ.Our PixelHacker generates more realistic results compared to other methods and exhibits strong adaptability to scenes with complex hierarchies and challenging lighting conditions.",
                "position": 723
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20438/x7.png",
                "caption": "Figure 7:More qualitative results of our PixelHacker vs. other SOTA methods[25,22,14,40,21,19]on Places2[38].Even under masks that obscure almost the entire image, PixelHacker consistently maintains remarkable semantic and structural coherence.",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2504.20438/x8.png",
                "caption": "Figure 8:Failure cases on natural and human-face scenes.In the first row, our PixelHacker struggles with generating fine details for the small black humanoid silhouette; in the second row, PixelHacker exhibits suboptimal reconstruction of the fingers. Nevertheless, it still maintains significantly better semantic and structural coherence compared to all other SOTA methods.",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2504.20438/x9.png",
                "caption": "Figure 9:More qualitative results of our PixelHacker vs. other SOTA methods on CelebA-HQ[26]and FFHQ[11].PixelHacker preserves highly detailed facial textures while avoiding perceptible color discrepancies or artifacts, producing more realistic results than other methods and demonstrating strong adaptability to complex scene structures and challenging lighting conditions.",
                "position": 1559
            }
        ]
    },
    {
        "header": "Supplementary Materials",
        "images": []
    }
]