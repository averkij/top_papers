[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02924/x1.png",
                "caption": "Figure 1:Architecture overview of AutoNeural. The model comprises: (1) a MobileNet-based vision encoder with Multi-Scale Fusion Adapter (MSFA) that processes768×768768{\\times}768images into 256 visual tokens, (2) a lightweight two-layer MLP connector without normalization for NPU quantization robustness, and (3) the Liquid AI 1.2B hybrid backbone with 16 layers that interleaves 10 gated-convolution layers with 6 Transformer attention layers to reduce memory I/O.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02924/x2.png",
                "caption": "Figure 2:Performance of AutoNeural after mixed-precision quantization (vision encoder: W8A16, language model: W4A16) and deployment on Qualcomm SA8295P NPU. Results reflect actual on-device execution, not PyTorch simulation, demonstrating stable accuracy and substantial latency improvements.",
                "position": 275
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02924/x3.png",
                "caption": "Figure 3:Vision encoder latency comparison on Qualcomm SA8295P NPU across three input resolutions. AutoNeural’s MobileNet-based encoder achieves 5.8×\\timesspeedup at 256×\\times256, 14×\\timesspeedup at 512×\\times512, and successfully processes 768×\\times768 images in real-time while InternViT-300M exceeds NPU memory capacity. Lower latency is better.",
                "position": 467
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]