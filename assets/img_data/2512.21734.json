[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21734/x1.png",
                "caption": "Figure 1:Streaming video generation setup. (a) T2V causal video diffusion. (b) Our approach for portrait animation: given a reference frame, we generate video autoregressively with a short sliding attention window, ensuring low latency, balanced computation, and stable identity preservation.",
                "position": 71
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21734/x2.png",
                "caption": "Figure 2:The video clips generated by Rolling Forcing, LongLive, and Self Forcing are presented from top to bottom, respectively. Significant temporal artifacts can be observed between adjacent frames, such as inconsistent object motion (first two rows) and abrupt changes in color tone (last row). Zoom in for details.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2512.21734/x3.png",
                "caption": "Figure 3:Attention masks of different causal designs. IoU of attention contexts between time stepsttandt+1t+1is computed to quantify the change in contextual coherence. (a): Causvid and Self Forcing. (b): LongLive. (c): Ours.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2512.21734/x4.png",
                "caption": "Figure 4:Key components of Knot Forcing. (a) illustrates the proposed temporal knot module. (b) illustrates the rollout inference pipeline with global context running ahead.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2512.21734/x5.png",
                "caption": "Figure 5:We assess inter-frame dependency by ablating each context frame and computing the L2 difference in attention outputs (relative to the original), normalized by the L2 norm of the unmodified output. The10th10^{\\text{th}}frame is used as the anchor, the resulting scores indicate each frame’s contribution to the current frame’s attention output.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21734/x6.png",
                "caption": "Figure 6:Infinite portrait animation results utilizing the proposed Knot Forcing. Given a reference frame, our approach produces videos exhibiting smooth motion, consistent identity, and high vividness across long-horizon without drift during stream-based generation. The generated timestamps are annotated beneath the corresponding video frames.",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2512.21734/x7.png",
                "caption": "Figure 7:Comparisons with streaming portrait animation models. Phonemes corresponding to the video frames are annotated below.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2512.21734/x8.png",
                "caption": "Figure 8:Comparison with causal-based video diffusion models. Top row shows results from the baseline methods, bottom row presents competing results generated by our method. The first frame from the videos generated by each baseline is used as the reference image for our model and is displayed in the leftmost column. Zoom in for better visual detail.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2512.21734/x9.png",
                "caption": "Figure 9:Visual effects of different components. From (a) to (c), three modules are integrated progressively: sliding window with global context, temporal knot, and global context running ahead.",
                "position": 543
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]