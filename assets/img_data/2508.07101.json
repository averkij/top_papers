[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Observation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07101/x1.png",
                "caption": "(a)Attention recall of different approaches using 4K token budget on an AIME problem.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x1.png",
                "caption": "(a)Attention recall of different approaches using 4K token budget on an AIME problem.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x2.png",
                "caption": "(b)Attention recall of retrieval (NiTH) and reasoning (AIME) tasks",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x3.png",
                "caption": "",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x4.png",
                "caption": "",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x5.png",
                "caption": "",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x6.png",
                "caption": "(a)10K-th decoding step",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x7.png",
                "caption": "(b)15K-th decoding step",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x8.png",
                "caption": "(c)20K-th decoding step",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x9.png",
                "caption": "(d)25K-th decoding step",
                "position": 273
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07101/x10.png",
                "caption": "Figure 4:Walkthrough of a single decoding step of the selection-based approach TidalDecode(Yang et al.,2024), which performs full attention for the first two layers, full attention with token selection for the third layer and a middle layer, and sparse attention for the other layers.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x11.png",
                "caption": "Figure 5:The selection process of LessIsMore is three-fold: (1) under token budgetK=4,r=0.25K=4,r=0.25, compute attention score matrixWWand extract the top-k (k=K∗(1−r)k=K*(1-r),rris the ratio of the most recent tokens that will be, by default, reserved as recency window) token indices for each attention head asρh​e​a​d\\rho_{head}; (2) flatten and union the selected indices for all heads, keeping the first k indices; (3) concatenate them with the most recent tokens, resulting in final token setρ\\rho. The sparse attention layers only load the tensors of tokens inρ\\rhofrom KV cache for all attention heads until the next selection layer or the end of the decoding step.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07101/x12.png",
                "caption": "Figure 6:Accuracy results of LessIsMore (ours), Quest, TidalDecode, SeerAttention-r, and Full Attention across for multiple main-stream reasoning tasks. Across all evaluated tasks, LessIsMore consistently achievs the lossless accuracy with small token budgets (1K or 2K), always outperforming all other baselines.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x13.png",
                "caption": "Figure 7:Efficiency-accuracy tradeoff comparison on AIME-24 using LLama-3.1-8B. Each point represents the average decoding latency across the corresponding average generation length inTable1. LessIsMore (orange squares) consistently achieves higher accuracy than TidalDecode (blue circles) while maintaining lower latency across all token budgets (1K, 2K, 4K, 6K). The closer to the top-left corner, the better the method performs. Full Attention baseline (triangle) provides the accuracy upper bound but with higher computational cost.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x14.png",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x15.png",
                "caption": "",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2508.07101/x16.png",
                "caption": "Figure 9:Ablation study on the impact of varying the recent window ratio in LessIsMore (⋆\\star) on the AIME-24 reasoning task, using a token budget of 4K and generation length up to 32K tokens on Qwen3-8B. LessIsMore corresponds to the 25% recent setting combined with Unified Attention Head Selection, (labeled with (⋆\\star)). We compare it against alternative recent window ratios, the 100% recent baseline (i.e., using only recent tokens), and TidalDecode. Curves are annotated with “(T)” or “(F)” to indicate whether the configuration yields the correct answer. Notably, only configurations that incorporate recent window with Unified Attention Head Selection (25%, 50%, 75%) succeed in solving the task.",
                "position": 538
            }
        ]
    },
    {
        "header": "5Future Work and Limitations",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]