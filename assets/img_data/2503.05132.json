[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05132/extracted/6258732/images/TurningPoint_transparent.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05132/x1.png",
                "caption": "Figure 1:The training dynamics of VisualThinker-R1-Zero on Qwen2-VL-2B base model. Benchmark accuracy is measured on CV-Bench, and the average response length is calculated from rollouts on SAT training samples. Initially, we observed a drop in length because the base model tended to generate HTML code. This behavior was quickly suppressed by RL, leading the model to adopt a more appropriate output format and a regular increase in response length. Afterwards, we observed a multimodal ‘aha moment’—the emergence of self-reflection in models’ response, as described in the DeepSeek-R1 paper, followed by a consistent positive correlation between response length and benchmark accuracy.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3VisualThinker R1 Zero",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05132/x2.png",
                "caption": "Figure 2:Comparison between RL and SFT training.Our method achieves a significant improvement over the base model and the instruction fine-tuned model. Specifically, Qwen2-VL-2B + R1 outperforms Qwen2-VL-2B (base model) by approximately ~30%, Qwen2-VL-2B-Instruct (instruction fine-tuned model) by ~5%, and Qwen2-VL-2B SFT (base model + SFT) by ~2%.",
                "position": 412
            }
        ]
    },
    {
        "header": "5Challenges of Applying RL to Supervised Fine-Tuned Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05132/x3.png",
                "caption": "Figure 4:Response length across training steps for different fine-tuning settings during RL.The x-axis represents training steps, while the y-axis shows the response length. Models with different fine-tuning configurations are compared: Freeze LLM (green), Freeze Vision Encoder (blue), and Full Finetune (red). The response length drops significantly in the early training phase and stabilizes over time. However, despite improved accuracy, all three RL-based fine-tuning on Instruct Model does not necessarily enhance reasoning capabilities, as responses tend to remain short and trivial",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2503.05132/x4.png",
                "caption": "Figure 5:Model performance during training with length-rewarded RL on instruction ginetuned models.Despite implementing length rewards to encourage longer responses, model performance shows no improvement in reasoning quality as response length increases. While average response length (red line) grows substantially, benchmark accuracy (green line) remains relatively stable, suggesting that longer responses do not necessarily translate to enhanced reasoning capabilities.",
                "position": 644
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]