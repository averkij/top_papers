[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06253/x1.png",
                "caption": "Figure 1:Number of citations to egocentric-exocentric related papers from 2015 to 2024. Citation data was collected from Google Scholar. The statistics are computed based on papers and datasets discussed in SectionsIVandV, all of which utilize both egocentric and exocentric perspectives.",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x2.png",
                "caption": "Figure 2:The overall structure of the survey. We first highlight the application value of egocentric and exocentric collaboration (SectionII). We then identify critical research tasks for each application (SectionIII). Next, we provide a comprehensive overview of the current research advancements (SectionIV). This section is divided into: ego for exo, exo for exo, and joint learning, each covering various research tasks. Additionally, we examine datasets that encompass both perspectives (SectionV). Finally, we discuss limitations and future directions (SectionVI).",
                "position": 104
            }
        ]
    },
    {
        "header": "IIApplications",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06253/x3.png",
                "caption": "Figure 3:Examples of the potential collaboration of egocentric and exocentric vision in diverse applications. We illustrate how integrating egocentric and exocentric video understanding techniques can enhance these applications.",
                "position": 115
            }
        ]
    },
    {
        "header": "IIIFrom Applications To Research Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06253/x4.png",
                "caption": "Figure 4:Mapping relevant research works to applications and research tasks.",
                "position": 236
            }
        ]
    },
    {
        "header": "IVResearch Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06253/",
                "caption": "Figure 5:Overall structure of SectionResearch Tasks. We discuss the research task from three aspects: Egocentric for Exocentric, Exocentric for Egocentric, and Joint Learning. Each subsection reviews a variety of tasks and their existing works.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x6.png",
                "caption": "Figure 6:Illustration of a diffusion-based framework for ego-to-exo video generation, adapted from[91].",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x7.png",
                "caption": "Figure 7:Illustration of a typical method for ego-for-exo action understanding, adapted from[92]. This method distills egocentric cues into exocentric representations.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x8.png",
                "caption": "Figure 8:Illustration of a typical method for view birdification, adapted from[93]. This task aims to estimate the trajectories of a crowd in a bird’s-eye view from an observer’s egocentric perspective.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x9.png",
                "caption": "Figure 9:Illustration of a typical method for view selection in surgical recording, adapted from[65]. It aims to identify the exocentric view with minimal occlusion by using the surgeon’s egocentric perspective as a selection criterion.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x10.png",
                "caption": "Figure 10:Illustration of the general GAN-based framework for exo-to-ego video generation. The generator uses exocentric images to synthesize egocentric views, while the discriminator distinguishes between real and synthesized egocentric images.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x11.png",
                "caption": "Figure 11:Illustration of a typical method for exocentric for egocentric video captioning, adapted from[60]. This method retrieves relevant exocentric videos to serve as references for captioning egocentric videos.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x12.png",
                "caption": "Figure 12:Illustration of a general adversarial-based approach for exo-for-ego action understanding. During training, the domain classifier differentiates between egocentric and exocentric features, while the model is optimized to deceive it. During inference, only egocentric videos are used.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x13.png",
                "caption": "Figure 13:Illustration of the exo-for-ego affordance grounding framework. During training, egocentric object images are aligned with exocentric demonstration images with the same affordance label. During inference, only the egocentric image is inputted to identify the affordance region.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x14.png",
                "caption": "Figure 14:Illustration of remote drone teleoperation with additional cameras, adapted from[132]. To overcome the limited view of egocentric cameras on drones, exocentric cameras are used to capture the surrounding environment.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x15.png",
                "caption": "Figure 15:Illustration of video captioning using videos from first-person, second-person, and third-person perspectives.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x16.png",
                "caption": "Figure 16:Illustration of the general cross-view retrieval framework. Exocentric and egocentric videos are encoded into a shared representation space to retrieve the best match from the alternate view.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x17.png",
                "caption": "Figure 17:Illustration of a typical method for egocentric camera localization, adapted from[150]. This method uses shadows to relate egocentric and top views, and estimates the egocentric camera direction in the top view.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x18.png",
                "caption": "Figure 18:Illustration of cross-view action understanding. This involves action recognition, gaze estimation, and pose estimation tasks.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x19.png",
                "caption": "Figure 19:Illustration of a typical method for egocentric wearer identification, adapted from[7]. This method uses spatial and temporal information to learn view-invariant features and identify the egocentric wearer in exocentric images.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x20.png",
                "caption": "Figure 20:Illustration of a typical method for cross-view human tracking and association, adapted from[172]. This method segments video pairs into clips and tracks individuals across clips and views.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x21.png",
                "caption": "Figure 21:Illustration of a typical framework of multi-view robotic manipulation, adapted from[70]. Multi-view data is integrated via cross-view learning module.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x22.png",
                "caption": "Figure 22:Illustration of a typical method for remote drone teleoperation with human-drone collaboration, adapted from[82]. This method combines user and drone perspectives into a unified environmental representation.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2506.06253/x23.png",
                "caption": "Figure 23:Illustration of a typical ego-exo view selection method, adapted from[187]. This approach leverages video captions as weak supervision for selecting the best view.",
                "position": 596
            }
        ]
    },
    {
        "header": "VDatasets",
        "images": []
    },
    {
        "header": "VIDiscussion",
        "images": []
    },
    {
        "header": "VIIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]