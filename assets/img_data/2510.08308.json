[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x1.png",
                "caption": "Figure 1:Illustration of a long CoT and the extraction result of candidate answers.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Analyzing Reflections in Reasoning Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x2.png",
                "caption": "Figure 2:Distribution of first candidate answer positions across different LLMs and prompts. The x-axis denotes the relative position of the first candidate answer (line index divided by total lines), and the y-axis shows the proportion of rollouts in each bin.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x3.png",
                "caption": "Figure 3:Reflections type statistics of long CoTs of different models. Long CoTs are collected on AIME2024 and AIME2025 (32 rollouts per question), AMC (4 rollouts per question), Olympiad Bench, and Math500 (1 rollout per question). Statistics are compiled for the union of all rollouts. More detailed breakdown of each dataset can be found in Figure11of AppendixD.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x4.png",
                "caption": "Figure 4:Breakdown of long CoTs: orange bars show the token count up to the first candidate answer, and blue bars show the token count in subsequent reflections.\nNumbers on bars indicate the accuracy of the first candidate answer, and the accuracy improvement brought by reflections.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x4.png",
                "caption": "Figure 4:Breakdown of long CoTs: orange bars show the token count up to the first candidate answer, and blue bars show the token count in subsequent reflections.\nNumbers on bars indicate the accuracy of the first candidate answer, and the accuracy improvement brought by reflections.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x5.png",
                "caption": "Figure 5:Left: Average number of candidate answers per rollout across different datasets. Right: Relative position of the first candidate. Values are averaged over 8 models.",
                "position": 320
            }
        ]
    },
    {
        "header": "3The Role of Reflection in Reasoning Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x6.png",
                "caption": "Figure 6:Comparison of performance and rollout length after SFT when training on rollouts cut at different positions. Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct are trained using processed rollouts from DeepSeek-R1 and Qwen3-8B, respectively. Accuracies are averaged over five datasets.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x7.png",
                "caption": "(a)Llama3.1-8B-Instruct",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x7.png",
                "caption": "(a)Llama3.1-8B-Instruct",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x8.png",
                "caption": "(b)Qwen2.5-7B-Instruct",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x9.png",
                "caption": "Figure 8:Changes of reasoning behavior after RL.",
                "position": 416
            }
        ]
    },
    {
        "header": "4Efficient Reasoning by Early Stopping",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x10.png",
                "caption": "Figure 9:Accuracy drop and token reduction with varying classification thresholds of CAD and QRC.",
                "position": 548
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt for Candidate Extraction",
        "images": []
    },
    {
        "header": "Appendix BBenchmark Statistics",
        "images": []
    },
    {
        "header": "Appendix CHuman Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/figures/user_interface.png",
                "caption": "Figure 10:The user interface for evaluating LLM extraction correctness for human participants.",
                "position": 1326
            }
        ]
    },
    {
        "header": "Appendix DBreakdown of Reflections",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x11.png",
                "caption": "Figure 11:Reflection statistics of long CoTs of different models. Long CoTs are collected on AIME24 and AIME25 (32 rollouts per question), AMC (4 rollouts per question), Olympiad Bench, and Math500 (1 rollout per question).",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x11.png",
                "caption": "",
                "position": 1336
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x12.png",
                "caption": "",
                "position": 1340
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x13.png",
                "caption": "",
                "position": 1345
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x14.png",
                "caption": "",
                "position": 1349
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x15.png",
                "caption": "",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x16.png",
                "caption": "",
                "position": 1358
            }
        ]
    },
    {
        "header": "Appendix EReflection Statistics",
        "images": []
    },
    {
        "header": "Appendix FExample of SFT data curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x17.png",
                "caption": "Figure 12:An illustration of the SFT data curation process in Section3.1.",
                "position": 1442
            }
        ]
    },
    {
        "header": "Appendix GRollout Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08308/x18.png",
                "caption": "(a)Llama3.1-8B-Instruct",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x18.png",
                "caption": "(a)Llama3.1-8B-Instruct",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2510.08308/x19.png",
                "caption": "(b)Qwen2.5-7B-Instruct",
                "position": 1756
            }
        ]
    },
    {
        "header": "Appendix HToken Usage and Accuracy",
        "images": []
    }
]