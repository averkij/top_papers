[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20579/arxiv_bytedance/fig/logo_cat.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20579/x1.png",
                "caption": "Figure 1:While prior video reasoning models (e.g., Video-R1[13], VideoRFT[41]) only generate textual rationales,Open-o3 Videointegrates explicit spatio-temporal grounding into the reasoning process. The model highlights key timestamps and object regions that directly support the answer, providing verifiable evidence for its prediction. More visualizations are provided in AppendixA.6.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related works",
        "images": []
    },
    {
        "header": "3STGR Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20579/x2.png",
                "caption": "Figure 2:Overview of our data construction pipeline and dataset composition.Left: The annotation pipeline includes Gemini 2.5 Pro initial annotation, bounding box filtering, and self-consistency checking.Right: Distribution of data categories in STGR-CoT-30k (SFT) and STGR-RL-36k (RL), showing a balanced coverage across temporal, spatial, spatio-temporal, and general QA.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Open-o3 Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20579/x3.png",
                "caption": "Figure 3:Overview of Open-o3 Video. We adopt a two-stage training paradigm: (a) cold-start initialization to learn structured, grounded outputs; (b) reinforcement learning with a composite reward that sharpens temporal alignment and spatial precision with adaptive temporal proximity and temporal gating.",
                "position": 287
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "LLM Usage Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20579/x4.png",
                "caption": "Figure 4:Annotation Prompt for PLM-Video-Human Region Dense Temporal Captioning Data source.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2510.20579/x5.png",
                "caption": "Figure 5:Annotation Prompt for Temporal Grounding Data Source.",
                "position": 1738
            },
            {
                "img": "https://arxiv.org/html/2510.20579/x6.png",
                "caption": "Figure 6:Illustration of ourconfidence-aware test-time scaling.\nThe model generates multiple responses with spatio-temporal traces, from which visual regions are cropped and scored for evidence relevance (s∈{0,1,2}s\\in\\{0,1,2\\}).\nFinal predictions are obtained by confidence-weighted voting.\nUnlike naive majority voting that is misled by spurious patterns (predicting “C”), our method highlights consistent supportive evidence and correctly predicts “A”, improving robustness at inference.",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2510.20579/x7.png",
                "caption": "Figure 7:Visualization.On simple appearance perception tasks, both our model and related baselines can provide correct answers; however, our approach additionally offers explicit spatio-temporal evidence.",
                "position": 1829
            },
            {
                "img": "https://arxiv.org/html/2510.20579/x8.png",
                "caption": "Figure 8:Visualization.For action recognition, our model precisely localizes both the time and location of the action, achieving superior performance compared to Video-R1.",
                "position": 1875
            },
            {
                "img": "https://arxiv.org/html/2510.20579/x9.png",
                "caption": "Figure 9:Visualization.In weather reasoning tasks, our model identifies more effective supporting evidence, whereas related video reasoning models perform poorly.",
                "position": 1878
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]