[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11715/x1.png",
                "caption": "Figure 1:Prompting a diffusion model for tracking. (a) We use an off-the-shelf video diffusion model to perform point tracking. We add a small, distinctive marking—a red dot—to the first frame of an input video, then ask the diffusion model to regenerate the rest of the video using SDEdit(Meng et al.,2021), which propagates the marking to subsequent frames. (b) We then track the motion of this marking over time. This motion corresponds to the trajectory of the underlying physical point. The model successfully tracks through occlusion. Please see the webpage for more results:https://point-prompting.github.io.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11715/x2.png",
                "caption": "Figure 2:Enhancing the Counterfactual Signal.We use negative prompting to ensure that the generated video contains the marker. In each denoising step (Eq.5), we condition the denoising on two images: (1)Edited First Frame: the first frame of the video with a marking added, and (2)Unedited First Frame: the original first frame of the video. We then subtract the weighted noise vector of the latter from the former.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2510.11715/x3.png",
                "caption": "Figure 3:Tracking Enhancements.To improve point tracking in video, we introduce two enhancements: (1)Color Rebalancing:remove existing red hues to ensure the red marker remains a unique tracking cue; (2)Refinement:obtain initial trajectories with a color-based tracker, then refine them using an inpainting mask to correct temporal artifacts such as object shifts (as shown in white circles). This two-step procedure first produces coarse tracks and then refines them via mask-constrained reverse diffusion.",
                "position": 283
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11715/x4.png",
                "caption": "Figure 4:Effect of denoising strength and radius on tracking performance.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2510.11715/x5.png",
                "caption": "",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2510.11715/x6.png",
                "caption": "Figure 5:Point Propagation.Frames generated from the video diffusion model show consistent red dot tracking. The model recovers the point after long occlusions, showing temporal understanding and object permanence.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2510.11715/x7.png",
                "caption": "Figure 6:Tracking results.Frames show the query point being tracked (circled dot) and its trajectory over the previous 5 frames. When the query point is occluded, only the trajectory tail is displayed without the dot.",
                "position": 723
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11715/x8.png",
                "caption": "Figure 7:Generation Failures.Typical failure cases in video generation: (1)Stationary Point:The red dot remains fixed relative to image boundaries, resembling lens dirt. (2)Symmetry Confusion:Symmetrical objects (e.g., left and right body parts) cause point propagation errors, likely due to compressed latent representations. (3)Propagation Failure:The red dot vanishes across consecutive frames. (4)Edge Ambiguity:The red dot, near boundaries, shifts to the background.",
                "position": 1966
            }
        ]
    },
    {
        "header": "Appendix BQuantitative Results on TAP-Vid",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11715/x9.png",
                "caption": "Figure 8:Qualitative Results on TAP-Vid Kubric.The top row shows a successful example of point propagation. In contrast, the bottom row illustrates a failure case where the point is not propagated due to the surface having very low texture.",
                "position": 2085
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    }
]