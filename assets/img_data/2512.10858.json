[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10858/x1.png",
                "caption": "Figure 1:Our proposed scaling laws extrapolate well to 3B and 10B models trained on up to 50×\\timeslarger compute budgets and suggest that DLMs can be competitive with ALMs at scale, even in compute-bound training settings.222Our bpb values are not directly comparable to those of autoregressive models since our value is a mixture of conditional and unconditional likelihoods.",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10858/x2.png",
                "caption": "Figure 2:Compute-optimal token-to-parameter ratios as a function of model size can vary significantly for different training objectives: While ALMs generally call for more training tokens relative to parameters, as shown by Chinchilla(hoffmann2022training), Llama 3(grattafiori2024llama), and DeepSeek(bi2024deepseek), discrete diffusion models require comparatively more parameters. For masked diffusion, there is a noticeable disagreement between our results and the literature, withnie2025scalingpredicting more parameter-heavy scaling andni2025trainingpredicting more token-heavy scaling.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x2.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x3.png",
                "caption": "",
                "position": 126
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Estimating scaling laws",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10858/x4.png",
                "caption": "Figure 3:(left) The optimal batch sizeB∗B^{*}of discrete diffusion model scales as a power law of training tokens rather than target loss, training FLOPs, or model size. Furthermore, the scaling is close to linear with an exponent of0.820.82. (right) The optimal learning rateη∗\\eta^{*}also follows a power law in batch size (assuming that batch size is optimal) with a scaling exponent of0.340.34.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x4.png",
                "caption": "",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x5.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x6.png",
                "caption": "Figure 4:Comparing the scaling laws of different noise types reveals that all noise types approximately converge in the compute-constrained settings (left) while uniform diffusion out-scales other noise types in token-constrained settings (right). Furthermore, the extrapolations remains accurate even up to 50×\\timeslarger compute budgets than what they were fitted on.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x6.png",
                "caption": "",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x7.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x8.png",
                "caption": "(a)",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x8.png",
                "caption": "(a)",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x9.png",
                "caption": "(b)",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x10.png",
                "caption": "(c)",
                "position": 584
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10858/x11.png",
                "caption": "Figure 6:(left, middle) There appears to be a tight relationship between batch sizes and step counts achieving the same loss, with iso-loss curves following a hyperbolic relation. (right) The minimum batch size and step count, as per the asymptotes of the fitted hyperbolas, grow with what appears to be a power law in the target loss, implying that as we get closer to the minimum achievable loss, the minimum required step count, but especially the minimum batch size, grow to large values. Here we display runs of a 85M (L12-D768) model trained on balanced hybrid noise.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x11.png",
                "caption": "",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x12.png",
                "caption": "",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x13.png",
                "caption": "",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x14.png",
                "caption": "Figure 7:Optimal batch size as a function of training tokens grouped by noise type.",
                "position": 1223
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x15.png",
                "caption": "Figure 8:Optimal learning rate as a function of (optimal) batch size grouped by noise type.",
                "position": 1226
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x16.png",
                "caption": "Figure 9:Optimal learning rateη∗\\eta^{*}as a function of training steps split by noise type and colored by batch size (between 8 and 512 sequences, brighter is larger) reveals thatη∗\\eta^{*}appears to follow a power law in training steps for each batch size. Model sizes are indicated by different markers.",
                "position": 1229
            }
        ]
    },
    {
        "header": "Appendix BOn the Difficulty of Uniform Diffusion",
        "images": []
    },
    {
        "header": "Appendix CProof of Proposition1",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10858/x17.png",
                "caption": "Figure 10:The fitted scaling coefficients differ systematically between FLOP estimation techniques: Method 1 uses the FLOP estimation technique proposed bybi2024deepseekwhereas method 2 uses the classic approach byhoffmann2022training. Furthermore, fitting on interpolated data (squared fit) produces tighter confidence bounds and better scaling exponents. Shaded regions denote 95% confidence intervals obtained via standard bootstrapping on the aggregated data points.",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x18.png",
                "caption": "Figure 11:Compute-optimal scaling laws fitted to interpolated values (squared fit). The FLOP estimation methodologies bybi2024deepseek(Method 1) andhoffmann2022training(Method 2) differ significantly since the FLOP-approximation used byhoffmann2022training(M=6​PM=6P) systematically underestimates the total number of FLOPs executed during training.",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x18.png",
                "caption": "",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x19.png",
                "caption": "",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x20.png",
                "caption": "",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x21.png",
                "caption": "",
                "position": 1755
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x22.png",
                "caption": "Figure 12:Compute-optimal scaling laws fitted on non-interpolated observations. Compared to interpolated values, the optimal observed values can deviate significantly from the compute-optimal trend due to the small number of unique model sizes used in the sweep. Method 1 uses the FLOP estimation technique frombi2024deepseek, whereas Method 2 followshoffmann2022training.",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x22.png",
                "caption": "",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x23.png",
                "caption": "",
                "position": 1769
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x24.png",
                "caption": "",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2512.10858/x25.png",
                "caption": "",
                "position": 1779
            }
        ]
    },
    {
        "header": "Appendix DScaling Coefficients",
        "images": []
    }
]