[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x1.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x2.png",
                "caption": "Figure 2:Comparison of speed and GFLOPS between the proposed linearized DiT and the original FLUX.1-dev. Speed is evaluated by performing 20 denoising steps on a single H100 GPU. FLOPS is calculated with the approximation:4√ó‚àëM√óc4ùëÄùëê4\\times\\sum M\\times c4 √ó ‚àë italic_M √ó italic_c, wherecùëêcitalic_cis the feature dimension andMùëÄMitalic_Mdenotes the attention masks.log2subscript2\\log_{2}roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTis applied on both vertical axes for better visualization. The raw data are supplemented in the appendix.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Efficient Attention: A Taxonomic Overview",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x3.png",
                "caption": "Figure 3:Preliminary results of various efficient attention methods on FLUX-1.dev. The prompt is ‚ÄúA small blue plane sitting on top of a field‚Äù.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2412.16112/x4.png",
                "caption": "Figure 4:Visualization of attention maps by various heads for an intermediate denoising step. Attention in pre-trained DiTs is largely conducted in a local fashion.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2412.16112/x5.png",
                "caption": "Figure 5:We try perturbing remote and local features respectively through clipping the relative distances required for rotary position embedding. Perturbing remote features has no obvious impact on image quality, whereas altering local features results in significant distortion. The text prompt and the original generation result are consistent with Fig.3.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2412.16112/x6.png",
                "caption": "Figure 6:Illustration of the proposed convolution-like linearization strategy for pre-trained DiTs. In each text-image joint attention module, text queries aggregate information from all text and image tokens, while each image token gathers information only from tokens within a local circular window.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2412.16112/x7.png",
                "caption": "Figure 7:To enhance multi-GPU parallel inference, each text query aggregates only the key-value tokens from the patch managed by its assigned GPU, then averages the attention results across all GPUs, which also generates high-quality images.",
                "position": 465
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x8.png",
                "caption": "Figure 8:Qualitative examples by the linearized FLUX-1.dev models with CLEAR and the original model.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2412.16112/x9.png",
                "caption": "Figure 9:Qualitative examples of using CLEAR with SDEdit[40]for high-resolution generation (left), FLUX-1.schnell in a zero-shot manner (middle), and ControlNet[69](right).G.T.andCond.denote ground-truth and condition images, separately.",
                "position": 953
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x10.png",
                "caption": "Figure 10:Fine-tuning on real data results in inferior performance compared to fine-tuning on self-generated synthetic data.",
                "position": 1132
            }
        ]
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Efficient Attention Alternatives",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x11.png",
                "caption": "Figure 11:Training dynamics of various efficient attention alternatives on FLUX-1.dev.",
                "position": 2223
            }
        ]
    },
    {
        "header": "Appendix BTraining Dynamics",
        "images": []
    },
    {
        "header": "Appendix CRaw Data for Efficiency Comparisons",
        "images": []
    },
    {
        "header": "Appendix DResults on More DiTs",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x12.png",
                "caption": "Figure 12:The linearized DiTs by CLEAR are compatible with various pipelines dedicated for high-resolution inference. The prompt is shown in Fig.15.",
                "position": 2258
            }
        ]
    },
    {
        "header": "Appendix EMore High-Resolution Results",
        "images": []
    },
    {
        "header": "Appendix FMore ControlNet Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.16112/x13.png",
                "caption": "Figure 13:Qualitative comparisons on FLUX-1.dev (top) and SD3.5-Large (bottom). The left subplots are results by the original models while the right ones are by the CLEAR linearized models. Prompts are listed in Fig.16.",
                "position": 2677
            },
            {
                "img": "https://arxiv.org/html/2412.16112/x14.png",
                "caption": "Figure 14:More 4K examples by the CLEAR linearized FLUX-1.dev. Prompts are listed in Fig.16.",
                "position": 2680
            }
        ]
    },
    {
        "header": "Appendix GEfficient Multi-GPU Parallel Inference",
        "images": []
    }
]