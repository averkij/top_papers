[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/1-teaser-v2.png",
                "caption": "Figure 1:We introduce DreamActor-M1, a DiT-based human animation framework, with hybrid guidance to achieve fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/2-overview-v3.png",
                "caption": "Figure 2:Overview of DreamActor-M1. During the training stage, we first extract body skeletons and head spheres from driving frames and then encode them to the pose latent using the pose encoder. The resultant pose latent is combined with the noised video latent along the channel dimension. The video latent is obtained by encoding a clip from the input full video using 3D VAE. Facial expression is additionally encoded by the face motion encoder, to generate implicit facial representations. Note that the reference image can be one or multiple frames sampled from the input video to provide additional appearance details during training and the reference token branch shares weights of our DiT model with the noise token branch. Finally, the denoised video latent is supervised by the encoded video latent. Within each DiT block, the face motion token is integrated into the noise token branch via cross-attention (Face Attn), while appearance information of ref token is injected to noise token through concatenated self-attention (Self Attn) and subsequent cross-attention (Ref Attn).",
                "position": 126
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/3-inference-v3.png",
                "caption": "Figure 3:Overview of our inference pipeline. First, we (optionally) generate multiple pseudo-references to provide complementary appearance information. Next, we extract hybrid control signals comprising implicit facial motion and explicit poses (head sphere and body skeleton) from the driving video. Finally, these signals are injected into a DiT model to synthesize animated human videos. Our framework decouples facial motion from body poses, with facial motion signals being alternatively derivable from speech inputs.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/4-qualitative-pose-v1.png",
                "caption": "Figure 4:The comparisons with human image animation works, Animate Anyone[14], Champ[61], MimicMotion[57]and DisPose[20]. Our method demonstrates results with better fine-grained motions, identity preservation, temporal consistency and high fidelity.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/4-qualitative-portrait-v1.png",
                "caption": "Figure 5:Our comparisons with portrait image animation works, LivePortrait[12], X-Portrait[49], Skyreels-A1[30]and Runway Act-One[33]. Our method demonstrates more accurate and expressive portrait animation capabilities.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/5-ablation.png",
                "caption": "Figure 6:Ablation study of 3D skeletons with bone length adjustment (BLA) and implicit face features.",
                "position": 477
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]