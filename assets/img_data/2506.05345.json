[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05345/x1.png",
                "caption": "Figure 1:Average absolute improvement of DMS over the original LLMs during inference-time scaling on reasoning tasks within the same KV cache budget, which is a proxy for runtime and memory load.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Dynamic Memory Sparsification",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05345/x2.png",
                "caption": "(a)DMS key cache management during inference.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2506.05345/x2.png",
                "caption": "(a)DMS key cache management during inference.",
                "position": 191
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05345/x3.png",
                "caption": "Figure 3:Inference-time scaling results comparing exact-match performance (yùë¶yitalic_y-axis) and KV cache reads as a measure of runtime (xùë•xitalic_x-axis). We evaluate Qwen-R1-distilled models at different scales (columns) and 4 datasets (rows). Point colors indicate results for DMS (green), vanilla models (purple), and the state-of-the-art sparse attention baseline, Quest (yellow). Colored lines indicate the respective Pareto frontier. Annotations indicate the scaling strategy as a L-W-CR tuple in terms of sequence length L (times 1024 tokens), width W (number of sampled reasoning threads), and compression ratio CR. The horizontal black line shows the performance reported byGuo et¬†al. (2025)for the vanilla model based on a 32-1-1 configuration. From the plots, it emerges that not only DMS achieves the best Pareto frontier, but that in general, KV cache compression is an effective strategy for improving inference-time scaling.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2506.05345/x4.png",
                "caption": "Figure 4:Inference-time scaling results comparing exact-match performance (yùë¶yitalic_y-axis) and peak tokens in memory (xùë•xitalic_x-axis). We refer toFigure3for full details. These results clearly indicate that KV cache compression methods (and especially DMS) incur a substantially reduced number of peak tokens in memory compared to vanilla LLMs, achieving higher performance for comparable memory needs.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2506.05345/x5.png",
                "caption": "Figure 5:GSM8K 0-shot scoresof Llama 3.2 1B Instruct across different compression variants.Left:delayed eviction (default) with a 16-token window consistently preserves reasoning abilities of the model, while immediate eviction causes rapid degradation. The quality gap only widens as the compression gets stronger.Right:DMS requires an order of magnitude less data to train than DMC. This was also observed for Qwen 2.5 R1 models with 1.5B, 7B, and 32B parameter scales.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2506.05345/x6.png",
                "caption": "Figure 6:Left:The measured compression ratiofor Qwen-R1 7B, trained with DMS CR4√ó4\\times4 √ó, while processing AIME 24, MATH 500, and GPQA Diamond problem instances. Right:Average per-head compressionlearned by the model, as a percentage of retained tokens sorted for every layer.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2506.05345/x7.png",
                "caption": "",
                "position": 588
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations, Future Work and Impact",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details for Retrofitting",
        "images": []
    },
    {
        "header": "Appendix CTraining Data",
        "images": []
    },
    {
        "header": "Appendix DAdditional Downstream Evaluations for DMC and DMS",
        "images": []
    },
    {
        "header": "Appendix EDownstream Results Significance",
        "images": []
    },
    {
        "header": "Appendix FEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05345/extracted/6516894/figures/misc/membound.png",
                "caption": "Figure 7:Percentage of total latency attributed to KV cache reads. Those reads clearly dominate latency as batch size and sequence length increase. When the KV cache is compressed (CR 4√ó\\times√óand 8√ó\\times√ó), more tokens can be accommodated before the latency of reading the KV cache becomes an issue.",
                "position": 2527
            }
        ]
    },
    {
        "header": "Appendix GInfluence of KV Cache on Inference Latency",
        "images": []
    }
]