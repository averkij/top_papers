[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15422/x1.png",
                "caption": "Figure 1:Examples and Performance Overview of KoNET.(a) Illustration of mathematics problem examples, highlighting the increased complexity and difficulty as the educational level progresses. (b) Demonstration of how the accuracy of contemporary AI models decreases with more advanced curricula. A detailed analysis is provided in Section4.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Benchmark: KoNET",
        "images": []
    },
    {
        "header": "4Experiment and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15422/x2.png",
                "caption": "Figure 2:Correlation analysis of error rates.The x-axis shows human error rates, and the y-axis displays error rates from closed-source models. AppendixC.3offers a detailed discussion on the methods used to calculate these error rates.",
                "position": 1163
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on the KoNET Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15422/x3.png",
                "caption": "Figure 3:Illustrative Representation of the KoNET.The test includes various types of questions, such as those requiring comprehension of images and queries, reading and understanding of lengthy texts, and simple knowledge-based queries.",
                "position": 1799
            }
        ]
    },
    {
        "header": "Appendix BDetails of the Used Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15422/x4.png",
                "caption": "Figure 4:Examples of prompt formats used in the study.These include Direct prompts for answer extraction, CoT (Chain-of-Thought) prompts for reasoning-based inference, and Judge prompts for evaluating the accuracy of generated responses.",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2502.15422/x5.png",
                "caption": "Figure 5:Performance of LLMs and MLLMs across Previous benchmarks and KoNET.These present a performance comparison between LLMs and MLLMs across various benchmarks, including KoNET. These illustrate the accuracy distribution for each model type, but KoNET shows a different distribution trend between LLMs and MLLMs compared to other benchmarks.",
                "position": 1812
            },
            {
                "img": "https://arxiv.org/html/2502.15422/x6.png",
                "caption": "Figure 6:Examples of human error rate.These illustrates human error rates across three types of comprehension tasks: sentence selection (left), sentence ordering (middle), and sentence insertion (right). The percentages at the top represent the error rates calculated based on responses from students. Higher error rates indicate more challenging tasks requiring deeper comprehension. Notably, as the complexity of the comprehension text increases, the error rate also rises, suggesting a greater cognitive load in understanding and structuring the given information.",
                "position": 1870
            },
            {
                "img": "https://arxiv.org/html/2502.15422/x7.png",
                "caption": "Figure 7:Distribution of human and models error rate by subjects.These compares the error rate distributions between humans (blue) and models (pink) across various academic subjects. The x-axis represents the error rate, while the y-axis lists different subjects, covering social sciences, natural sciences, Korean language, history, and mathematics. The varying distributions highlight the differences in performance between humans and models, with some subjects showing a greater disparity.",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2502.15422/x8.png",
                "caption": "Figure 8:Distribution of human and models error rate by points.These presents the error rate distribution of humans (green) and models (brown) based on different point values assigned to questions. The x-axis represents the percentage of incorrect answers, while the y-axis categorizes questions by their point values. Higher-point questions generally require deeper reasoning and comprehension, which is reflected in the increasing error rates for both humans and models.",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2502.15422/x9.png",
                "caption": "Figure 9:Performance of multilingual ability.These illustrations depict the accuracy distribution of various models across multiple languages, highlighting their multilingual capabilities. The x-axis represents accuracy percentages, while the y-axis lists different languages. In general, Open Source models tend to support a narrower range of languages fluently compared to Closed Source models. However, even among Closed Source LLMs, performance tends to decline for certain languages; for instance, Arabic differs from English in writing direction, which can impact model performance.",
                "position": 1886
            }
        ]
    },
    {
        "header": "Appendix CAdditional Analysis",
        "images": []
    }
]