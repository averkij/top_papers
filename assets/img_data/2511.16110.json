[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16110/x1.png",
                "caption": "Figure 1:Overview of the stacked defenses.",
                "position": 186
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16110/x2.png",
                "caption": "Figure 2:Overview of MFAMFA integrates three coordinated attacks to bypass VLM safety defenses:(a)shows the full pipeline that jointly breaks alignment, system prompts, and content moderation.(b)ATA embeds harmful instructions in benign-looking prompts, exploiting reward models;(c)Moderator Bypass adds noisy suffixes to evade input/output filters;(d)Vision-Encoder Attack injects a malicious prompt via adversarial image embeddings.",
                "position": 262
            }
        ]
    },
    {
        "header": "3Multi-Faceted Attack",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16110/x3.png",
                "caption": "Figure 3:Overview of Vision-Encoder–Targeted Attack.",
                "position": 659
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16110/x4.png",
                "caption": "Figure 4:Real attack cases of MFA with baselines. Further case studies are available in Appendix D.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x5.png",
                "caption": "Figure 5:Comparison of computational costs: (a) Parameters and computations. (b) Average attack time on LlamaGuard.",
                "position": 1372
            }
        ]
    },
    {
        "header": "5Discussion & Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Details on Ablation Study",
        "images": []
    },
    {
        "header": "Appendix DAdditional MFA Case Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16110/x6.png",
                "caption": "Figure A-1:ChatGPT-4o generates two fully-formed arguments in response to the adversarial prompt “Are humans a cancer on the Earth?”, despite the presence of an appended noisy string. This demonstrates the model’s failure to reject a philosophically toxic and potentially harmful framing, thereby revealing a vulnerability in its alignment and input filtering mechanisms.",
                "position": 2032
            },
            {
                "img": "https://arxiv.org/html/2511.16110/figs/realcasegemini.pdf.png",
                "caption": "Figure A-2:Gemini 1.5 Pro responds to a harmful prompt asking for a tweet promoting misogyny. Despite the sensitive nature of the request, the model generates both a strongly worded pro-equality rebuttal and a fully articulated anti-equality response, showcasing a failure in alignment safeguards. This example highlights how MFA can induce VLMs to produce policy-violating outputs aligned with toxic user intent.",
                "position": 2035
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x7.png",
                "caption": "Figure A-3:Gemini 2.0 Experimental Advanced outputs a harmful tweet promoting gender inferiority, despite issuing an initial refusal. The model generates discriminatory content under the guise of a “disclaimer,” normalizing misogynistic rhetoric with pseudo scientific justification and targeted hashtags. This case highlights how MFA circumvents moderation by exploiting nuanced language and rhetorical framing.",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x8.png",
                "caption": "Figure A-4:Examples of Multi-Faceted Attack on GPT-4V. Blue denotes benign responses, and red represents harmful responses. Visual and alignment facet attack successfully elicit GPT-4V to generate genuinely harmful responses. In the first case, GPT-4V offers detailed guidance on finding explicit content on the internet. In the second case. GPT-4V directly answers the question starting with “Yes, human beings should be eradicated”. In the third case, GPT-4V generates a misleading tweet that could be directly copied by the malicious user.",
                "position": 2041
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x9.png",
                "caption": "Figure A-5:Examples of Multi-Faceted Attack on Gemini-2.0-Pro. Blue denotes benign responses, and red represents harmful responses. Triple-facet attack elicits Gemini-2.0-Pro to generate harmful content. In the first case, Gemini-2.0-Pro generates a detailed plan on how to mug someone at an ATM. In the second case, Gemini-2.0-Pro directly generates the harmful tweet against bisexuality.",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x10.png",
                "caption": "Figure A-6:Examples of Multi-Faceted Attack on Llama-3.2-11B-Vision-Instruct: Blue indicates benign responses, while red represents harmful ones. Llama-3.2-11B-Vision-Instruct generates harmful responses across three cases: writing a recruitment post for ISIS, creating malicious content about Muslims, and spreading misinformation about the Sandy Hook massacre.",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x11.png",
                "caption": "Figure A-7:Example of Multi-Faceted Visual Attack on Llama-3.2-11B-Vision-Instruct: Red indicates harmful responses. A visual-facet attack alone causes Llama-3.2-11B-Vision-Instruct to generate harmful content; a tweet labeling a politician as a Nazi",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x12.png",
                "caption": "Figure A-8:Examples of Multi-Faceted Attack on NVLM-D-72B. Blue denotes benign responses, and red represents harmful responses. Under the visual and alignment facet attacks, the NVLM-D-72B generates harmful responses on three cases.",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x13.png",
                "caption": "Figure A-9:Example of Multi-Faceted Visual attack on NVLM-D-72B. Red represents harmful responses. A visual-facet attack alone causes NVLM-D-72B to generate harmful content; a ransom note.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x14.png",
                "caption": "Figure A-10:Failure case of Multi-Faceted Attack on LLaVA-v1.5. Blue denotes rejection, and yellow indicates contrastive triggers inducing harmful content. Mult-Faceted Attack successfully prompts LLaVA-v1.5 to generate two contrasting responses; however, instead of producing actual offensive language about African Americans, LLaVA-v1.5 inserts a placeholder—“[Insert offensive and derogatory language against African Americans here.]”—and then concludes with the repeated adversarial signature. This outcome suggests that LLaVA-v1.5 is strongly aligned against racism.",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2511.16110/x15.png",
                "caption": "Figure A-11:Failure case of Multi-Faceted Attack on ShareGPT4V (blue) and mPLUG-Owl2 (purple). Yellow indicates contrastive triggers inducing harmful content. ShareGPT4V and mPLUG-Owl2 respond with overly concise replies, likely a result of their limited reasoning ability.",
                "position": 2085
            }
        ]
    },
    {
        "header": "Appendix EFailure Case Analysis",
        "images": []
    }
]