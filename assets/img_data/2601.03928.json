[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03928/x1.png",
                "caption": "(a)Comparison of vanilla UI grounding VLMs, VLMs with visual token pruning, and ourFocusUI.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x1.png",
                "caption": "(a)Comparison of vanilla UI grounding VLMs, VLMs with visual token pruning, and ourFocusUI.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x2.png",
                "caption": "(b)Study 1:The exceptionally high proportion ofvisual(screenshot) vs.text(instruction) tokens in UI grounding tasks.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x3.png",
                "caption": "(c)Study 2:Our proposed position-preserving visual token selection vs. general visual token pruning methods.",
                "position": 216
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03928/x4.png",
                "caption": "Figure 2:Overview of our proposedFocusUI.(a) Illustration of how the Instruction-to-Patch saliency score is constructed. (b) Query-guided Saliency Scorer and token selection. (c) Overall UI grounding framework illustrating howPosPadis applied to dropped sequences to preserve positional continuity. For clarity, we omit the system prompt in the token sequence.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Efficient UI Grounding: Task Characteristics and Challenges",
        "images": []
    },
    {
        "header": "3FocusUI",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03928/x5.png",
                "caption": "Figure 3:Illustrative example of building the Instruction-to-Patch saliency score.(a)ScreenshotIIwith ground-truth bounding boxbg‚Äãtb_{gt}.(b)Bounding-box saliency scoreSbboxS_{\\mathrm{bbox}}.(c)Union-find results.(d)Size of each connected componentnun_{u}.(e)UI-graph saliency scoreSuigS_{\\mathrm{uig}}.(f)Fused supervisionSIns2PatchS_{\\mathrm{Ins}2\\mathrm{Patch}}by combining(d)and(e). Brighter regions represent positive patches and darker regions represent negative patches.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x6.png",
                "caption": "Figure 4:Illustration ofPosPadsequence transformation for positional continuity preservation via an example 2D image (2√ó\\times3 patches) and its 1D sequence. A learnable<pos_pad>marker is placed at the last index of each contiguous sequence of dropped visual tokens, as illustrated by strategy (d).",
                "position": 545
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03928/x7.png",
                "caption": "Table 6:Efficiency analysis on ScreenSpot-Pro benchmark under different retention ratios and model backbones ofFocusUI.‚àóThe number of<pos_pad>tokens is not included.",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x7.png",
                "caption": "Figure 5:Qualitative visualization of predicted saliency heatmaps and retained patches under aretention ratior=ùüëùüé%r=\\mathbf{30\\%}. Black regions denote dropped visual tokens that are not consumed by the LM during decoding. Examples are taken from the ScreenSpot-V2 and ScreenSpot-Pro benchmarks, spanning web, desktop, and mobile interfaces.",
                "position": 2105
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03928/figures/supp-1a-wandb_loss.png",
                "caption": "(a)Total Losscurve during training.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2601.03928/figures/supp-1a-wandb_loss.png",
                "caption": "(a)Total Losscurve during training.",
                "position": 2456
            },
            {
                "img": "https://arxiv.org/html/2601.03928/figures/supp-1b-wandb_eval.png",
                "caption": "(b)Evaluation:ScreenSpot-Pro and UI-Vision withretention ratio = 100%.",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2601.03928/figures/supp-1c-wandb_eval_retain05.png",
                "caption": "(c)Evaluation:ScreenSpot-Pro and UI-Vision withretention ratio = 50%.",
                "position": 2468
            }
        ]
    },
    {
        "header": "Appendix BDiscussion",
        "images": []
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03928/x8.png",
                "caption": "Table 12:Patch Recall@K%andFull Coverage Budgetperformance comparison on the ScreenSpot-Pro benchmark.",
                "position": 2801
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x8.png",
                "caption": "(a)Performance vs. reduction ratio on ScreenSpot-V2.",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x8.png",
                "caption": "(a)Performance vs. reduction ratio on ScreenSpot-V2.",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x9.png",
                "caption": "(b)Performance vs. reduction ratio on ScreenSpot-Pro.",
                "position": 2941
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x10.png",
                "caption": "Figure 8:Qualitative examples of predicted per-patch saliency.Left:original screenshot;Middle:predicted saliency map; andRight:visual token selection results withr=30%r=30\\%.",
                "position": 2954
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x10.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2601.03928/x11.png",
                "caption": "",
                "position": 2962
            }
        ]
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]