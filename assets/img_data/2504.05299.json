[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05299/x1.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/logos/core.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/logos/st_logo.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x2.png",
                "caption": "",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x3.png",
                "caption": "",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x4.png",
                "caption": "",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x5.png",
                "caption": "",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x6.png",
                "caption": "",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x7.png",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x8.png",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x9.png",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x10.png",
                "caption": "",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x11.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x12.png",
                "caption": "",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x13.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x14.png",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x15.png",
                "caption": "",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x16.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x17.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/figures/combined_plots.png",
                "caption": "Figure 1:Smol yet Mighty:comparison of SmolVLM with other state-of-the-art small VLM models. Image results are sourced from the OpenCompass OpenVLM leaderboard(Duan et al.,2024).",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x18.png",
                "caption": "",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x19.png",
                "caption": "",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/logos/chrome.png",
                "caption": "",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/logos/spaces.png",
                "caption": "",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/logos/huggingapp.png",
                "caption": "",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x20.png",
                "caption": "Figure 2:SmolVLM Architecture. Images are split into subimages, frames are sampled from videos, and then encoded into visual features. These features are first rearranged via a pixel-shuffle operation, then mapped into the LLM input space as visual tokens using an MLP projection. Visual tokens are then concatenated/interleaved with text embeddings (orange/red). This combined sequence is passed to the LLM for text output.",
                "position": 196
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Smoller Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05299/x21.png",
                "caption": "Figure 3:Performance analysis of SmolVLM configurations.(Left)Impact of vision encoder and language model sizes. Smaller language models (135135135135M) benefit less from larger vision encoders (SigLIP-SO-400400400400M,428428428428M) compared to SigLIP-B/16161616(93939393M), while larger language models gain more from powerful encoders.(Middle-left)Performance significantly improves with increased context lengths (2222k to16161616k tokens).(Middle-right)Optimal pixel shuffle factor (PS=2 vs. PS=4) varies by model size.(Right)Frame averaging reduces video performance, with a rapid decline as more frames are averaged. Metrics average CIDEr (captioning) and accuracy (visual question answering).",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/figures/pixel_shuffle.png",
                "caption": "Figure 4:Pixel shuffle.Rearranges encoded images, trading spatial resolution for increased channel depth. This reduces visual token count while preserving information density.",
                "position": 335
            }
        ]
    },
    {
        "header": "3Smol Instruction Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05299/extracted/6335965/figures/all_three_plots_horizontal.png",
                "caption": "Figure 5:Tokenization Strategy Comparisons.(Left)Training loss curves illustrating the “OCR loss plague” when using string-based tokens in smaller models.(Center)Aggregated evaluation metrics showing consistently higher scores with learned tokens (orange).(Right)Scatter plot of OpenCompass-Image vs. OpenCompass-Video: learned tokens dominate the higher-scoring region, especially in image-intensive tasks.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x22.png",
                "caption": "Figure 6:Cumulative Effect of Training Strategies on SmolVLM Performance.The visualization shows the progression of performance improvements as different tokenization and prompt engineering strategies are applied sequentially to the SmolVLM base model.(Left)Image benchmark results show consistent improvements with each added strategy.(Right)Video benchmark results reveal similar patterns with more pronounced gains.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x23.png",
                "caption": "Figure 7:Impact of Training Strategies on Smol-Scale Multimodal Models.(Left)Reusing text data from LLM-SFT (SmolTalk) reduces both image and video scores in smaller models.(Middle)A minimal fraction (0.020.020.020.02%–0.050.050.050.05%) of Chain-of-Thought (CoT) data yields optimal results, while heavier CoT usage degrades performance.(Right)Increasing average video duration beyond3.53.53.53.5min leads to diminished returns for both image and video tasks.",
                "position": 460
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05299/x24.png",
                "caption": "Figure 8:Data Details.Training dataset details for Vision(Left)and video(Right), broken down by modality and sub-categories.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x25.png",
                "caption": "",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2504.05299/x26.png",
                "caption": "Figure 9:SmolVLM on edge device.(Left)Examples of theHuggingSnapapp, where SmolVLM can run locally, on the device, on consumer phones. For example, interactions can be done using a mobile interface to detect objects and answer questions.(Right)Throughput in tokens per second on NVIDIA A100100100100GPUs(top)and different consumer personal computers(bottom)across different batch sizes and model variants.",
                "position": 821
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]