[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/mila_mauve_logo.png",
                "caption": "",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/udem_just_logo.png",
                "caption": "",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ai2_logo.png",
                "caption": "",
                "position": 80
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/deepmind_justlogo.png",
                "caption": "",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/maple.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/async_v_sync_compute.png",
                "caption": "Figure 1:Asynchronous off-policy RLHF is more computationally efficient, and matches the win-rate of synchronous on-policy RLHF on TLDR across model scales. On 4√ó\\times√óA100 GPUs, it results in training a 2.8B Pythia model25%percent2525\\%25 %faster and improvements in speed increase with scale.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/async_v_sync.png",
                "caption": "Figure 2:Synchronous vs Asynchronous RLHF.Top:The current RLHF paradigm synchronously generates and then trains, leveraging the same GPUs for both. This means using slow training libraries for LLM generation.Bottom:We propose Cleanba-style(Huang et¬†al.,2023)asynchronous RLHF, separating generation and training to different GPUs. This allows leveraging LLM inference libraries e.g. vllm(Kwon et¬†al.,2023), to greatly reduce generation time. Training time increases because we are learning on only one GPU but the overall runtime for three updates is lower. The caveat is that asynchronous learning requiresoff-policytraining: learning on data created by our model at a previous timestep e.g.Œ∏t+1subscriptùúÉùë°1\\theta_{t+1}italic_Œ∏ start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTis updated using data generated byŒ∏tsubscriptùúÉùë°\\theta_{t}italic_Œ∏ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT",
                "position": 185
            }
        ]
    },
    {
        "header": "3Asynchronous Off-Policy RLHF",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppo_minibatch_winrate.png",
                "caption": "Figure 3:Trade-off between Win-Rate and KL in Off-Policy PPO. PPO performance decreases as learning becomes more off-policy. Win-rate is highest when learning is fully on-policy (generate then train onN=1ùëÅ1N=1italic_N = 1mini-batches). As we increaseNùëÅNitalic_N, our model must take more steps on data generated by the same old policy. This increases off-policyness and reduces win-rate.Left:Gold win-rate over trainingMiddle: KL (perplexity) over training, higher is further from initial modelRight:Gold win-rate vs KL",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppo_minibatch_winrate.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppo_minibatch_ppl.png",
                "caption": "",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppo_minibatch_pareto.png",
                "caption": "",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/losses_minibatch1-16.png",
                "caption": "Figure 4:Robustness of RLHF Losses to Off-Policyness. Online DPO is more robust to off-policyness than PPO, RLOO (Left) or Best-of-2 SFT (Right). Performance is shown across levels of off-policyness as mediated by number of mini-batchesN‚àà{1,2,4,8,16}ùëÅ124816N\\in\\{1,2,4,8,16\\}italic_N ‚àà { 1 , 2 , 4 , 8 , 16 }. With higherNùëÅNitalic_Nincreasing off-policyness, Online DPO retains much more performance than other methods, as evidenced by off-policy points still being clustered close to optimal performance.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/losses_minibatch1-16.png",
                "caption": "",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/losses_minibatch1-16_bo2.png",
                "caption": "",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/policyscale_minibatch_pareto.png",
                "caption": "Figure 5:Scaling Model Size with Off-Policy RLHF. Plotting the final win-rate vs KL forN=1‚Üí64ùëÅ1‚Üí64N=1\\to 64italic_N = 1 ‚Üí 64mini-batches, covering a spectrum of on-policy to off-policy RL. Scaling policy size (left) improves off-policy robustness as seen by tighter clustering of points. But scaling reward model size (right) does not, even though it reduces overoptimization, achieving reward with smaller KL.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/policyscale_minibatch_pareto.png",
                "caption": "",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/rmscale_minibatch_pareto.png",
                "caption": "",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/async-bound.png",
                "caption": "Figure 6:Asynchronous RLHF can be training-bound (left) or generation-bound (right). In practice, generation and training speeds differ so a challenge of asynchronous learning is how best to balance usage and leverage idle compute time to further improve training.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppoepochs_winrate.png",
                "caption": "Figure 7:Optimizing Generation-Bound RLHF. We can leverage extra training GPU cycles to do multiple updates on the same generated mini-batch (‚Äúppo epochs‚Äù).Left:At 410m and 1B scales, more updates per batch increases the win-rate achieved at any given episode, making training more data efficient.Right:Across scales, more updates change the pareto frontier and cause models to achieve the same win-rate at a higher KL.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppoepochs_winrate.png",
                "caption": "",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/ppoepochs_pareto.png",
                "caption": "",
                "position": 331
            }
        ]
    },
    {
        "header": "4Optimizing Asynchronous RLHF",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/top4_compute.png",
                "caption": "Figure 8:Optimizing Training-Bound RLHF. We can leverage extra generation GPU cycles to sampleKùêæKitalic_Kcompletions per prompt instead of 2.Left:SamplingK=4ùêæ4K=4italic_K = 4improves the gradient such that we can train for half the number of steps and, across scales, achieve the same final win-rate at a fraction of the compute time.Right:The trade-off is that increasingKùêæKitalic_Kcauses models to drift more in terms of KL in order to achieve the same win-rate.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/top4_compute.png",
                "caption": "",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/top4_pareto.png",
                "caption": "",
                "position": 373
            }
        ]
    },
    {
        "header": "5Large-Scale Asynchronous RLHF",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/tulu_score.png",
                "caption": "Figure 9:Large-Scale Asynchronous RLHF. Comparing synchronous and asynchronous online DPO for training an 8B general-purpose chatbot. Asynchronous learning achieves the same reward model score at a lower KL and30%percent3030\\%30 %faster.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/tulu_score.png",
                "caption": "",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2410.18252/extracted/5947150/images/tulu_kl.png",
                "caption": "",
                "position": 403
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    }
]