[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05349/x1.png",
                "caption": "Figure 1:The foundation of our benchmark is the “needle-in-a-multimodal-haystack” challenge, capturing the core difficulty of cross-modal reasoning across time from visual, textual, and audio streams. Built on this, VideoMathQA categorizes each question along four key dimensions: reasoning type, mathematical concept, video duration, and difficulty.",
                "position": 70
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05349/x2.png",
                "caption": "Figure 2:Example questions from theVideoMathQAbenchmark illustrating the three reasoning types: Problem Focused, Concept Transfer, and Deep Comprehension. The benchmark includes evolving dynamics in a video, complex text prompts, five multiple-choice options, the expert-annotated step-by-step reasoning to solve the given problem, and the final correct answer as shown above.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3VideoMathQA",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05349/x3.png",
                "caption": "Figure 3:The figure illustratesa)Distribution of questions and model performance across ten mathematical concepts in theVideoMathQA. The consistently low performance across all concepts reveals a significant gap in the ability of the current multimodal models to perform mathematical reasoning over videos.b)Distribution of video durations inVideoMathQA, highlighting a diverse range from short clips of10101010s to long-videos up to1111hr.c)The three-stage annotation pipeline forVideoMathQAwas performed by expert science graduates, who annotated detailed step-by-step reasoning trails, with each stage governed by strict quality assessment.",
                "position": 148
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05349/x4.png",
                "caption": "Figure 4:The figure showsVideoMathQAperformancea)Across video duration categories using the CoT MBin +Sub setting;b)Impact of subtitles under the CoT MBin setting; andc)Effect of varying the number of input frames under CoT MCQ setting. Overall, models perform best on medium-length videos, and overall accuracy improves with the inclusion of subtitles and more frames during evaluation.",
                "position": 1433
            },
            {
                "img": "https://arxiv.org/html/2506.05349/x5.png",
                "caption": "Figure 5:The figure showsa)Comparison among vision-blind, image-only, and video models, highlighting the need for video-level understanding to perform well inVideoMathQA.b)Distribution of questions inVideoMathQAacross three difficulty levels for varying reasoning depths, and the relationship between performance and question difficulty across top-performing models.c)Error analysis based on CoT step evaluation. Most model errors stem from misunderstanding the question, where models misinterpret what the question asks or overlook critical multimodal cues.",
                "position": 1442
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AComparison with Existing Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CLimitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]