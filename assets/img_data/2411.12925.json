[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x1.png",
                "caption": "Figure 1:(Left) Train-to-train prediction from FineWeb-edu to all 6 training sets. Each datapoint represents a pair of models that are ‚Äújoined‚Äù on model sizeNùëÅNitalic_Nand dataset sizeDùê∑Ditalic_D. Dashed lines represent extrapolation and stars represent 3.3B models trained with 20x compute of the largest dot. These large models arenotused to fit the curves. (Center) Test-to-test prediction of Hellaswag cross entropy loss between models trained on FineWeb-edu and models trained on the other datasets. Again each datapoint represents two models joined on model and dataset size. The downstream loss is the cross entropy loss of the correct answer to the multiple choice problem when phrased as a cloze task. (Right) Train-to-test prediction from FineWeb-edu to four downstream tasks. Each datapoint represents a single model and its ‚Äútransfer‚Äù performance on the val data.",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Setting",
        "images": []
    },
    {
        "header": "4Predicting loss across datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x2.png",
                "caption": "Figure 2:Train-to-train fits. Each point on the plot represents the final loss of two models:f^0N,Dsuperscriptsubscript^ùëì0ùëÅùê∑\\hat{f}_{0}^{N,D}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N , italic_D end_POSTSUPERSCRIPTwhich is trained on dataset 0 andf^1N,Dsuperscriptsubscript^ùëì1ùëÅùê∑\\hat{f}_{1}^{N,D}over^ start_ARG italic_f end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N , italic_D end_POSTSUPERSCRIPTwhich is trained on dataset 1. The models are paired when they use the same number of parametersNùëÅNitalic_Nand tokensDùê∑Ditalic_D. Starred points indicate a large model trained for the purpose of testing the extrapolation of the curves, which are only fit on the dotted points.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x3.png",
                "caption": "Figure 3:Train-to-test fits. Each datapoint represents a single model trained on the dataset in the subplot title and then evaluated on a different dataset as indicated by the color.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x4.png",
                "caption": "Figure 4:Train-to-test transfer for downstream tasks. On the test set we evaluate the CE loss of the correct multiple choice answer as a cloze task.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x5.png",
                "caption": "Figure 5:Test-to-test predictions for downstream tasks. Each subplot illustrates a different downstream task. The x-axis always reports the test loss for models trained on FineWeb-edu, and the y-axis shows test loss for all 6 of the different training distributions. Each point represents two models, joined when they share the same model size and training dataset size.",
                "position": 461
            }
        ]
    },
    {
        "header": "5Loss-to-loss prediction can outperform independent scaling laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x6.png",
                "caption": "Figure 6:An illustration of using train-to-train prediction to leverage a full set of training runs on FineWeb-edu plus 8 training runs on ProofPile 2 to yield a full scaling law fit on ProofPile 2.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x7.png",
                "caption": "Figure 7:(Left) The baseline just fits the full scaling law on the small dataset of 8 runs on ProofPile 2. (Right) The skyline uses a full suite of models trained on ProofPile 2 to fit a gold-standard scaling law.",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x8.png",
                "caption": "Figure 8:An illustration of four of the methods we consider for making extrapolative predictions of Hellaswag test loss. (Left) General train-to-test prediction uses train loss of models trained onP0subscriptùëÉ0P_{0}italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT(in this case FineWeb-edu) to predict test loss on models trained onP1subscriptùëÉ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(in this case ProofPile 2). (Center-left) Test-to-test prediction uses test loss of models trained onP0subscriptùëÉ0P_{0}italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTto predict test loss on models trained onP1subscriptùëÉ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. (Center-right) Predicting the test loss only using runs fromP1subscriptùëÉ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTby fitting the relationship between FLOPs and test loss. (Right) Fitting a full scaling law toP1subscriptùëÉ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTusing only the limited data fromP1subscriptùëÉ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.",
                "position": 595
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFrom loss to accuracy",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x9.png",
                "caption": "Figure 9:Fitting training loss to accuracy on the OLMo tasks individually (first 7 subplots), and then in aggregate (bottom right). Unlike the plots in the main paper where each line only fits 2 parametersK,Œ∫ùêæùúÖK,\\kappaitalic_K , italic_Œ∫, here we also fit a third parameter in place ofE1|0subscriptùê∏conditional10E_{1|0}italic_E start_POSTSUBSCRIPT 1 | 0 end_POSTSUBSCRIPT.",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x10.png",
                "caption": "Figure 10:Fitting training loss to accuracy on MMLU splits.",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x11.png",
                "caption": "Figure 11:The relationship between downstream CE loss and classification error shows unified trends across pre-training distributions, i.e. it seems that all points roughly fit onto one trend line regardless of their color.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x12.png",
                "caption": "",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x13.png",
                "caption": "Figure 12:The relationship between downstream CE loss and classification error shows unified trends across pre-training distributions, i.e. it seems that all points roughly fit onto one trend line regardless of their color.",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x14.png",
                "caption": "",
                "position": 1616
            }
        ]
    },
    {
        "header": "Appendix BTrain-to-test downstream",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x15.png",
                "caption": "Figure 13:Train-to-test predictions across all individual downstream tasks.",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2411.12925/",
                "caption": "",
                "position": 1636
            }
        ]
    },
    {
        "header": "Appendix CAdditional test-to-test results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x17.png",
                "caption": "Figure 14:Test-to-test prediction using the validation sets from pre-training data as the targets. Each subplot shows a different test loss. Within each subplot, the training dataP0subscriptùëÉ0P_{0}italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTis always FineWeb-Edu and the curves illustrate all of the 6 possible options forP1subscriptùëÉ1P_{1}italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Each point corresponds to two models.",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x18.png",
                "caption": "Figure 15:Test-to-test prediction on the four losses from the main text. Each row shows a different training lossP0subscriptùëÉ0P_{0}italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTon the x-axis. Each point corresponds to two models.",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x19.png",
                "caption": "Figure 16:Test-to-test prediction on all 11 downstream losses we consider. The training dataP0subscriptùëÉ0P_{0}italic_P start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTis fixed to FineWeb-edu in all subplots. Each point corresponds to two models.",
                "position": 1652
            }
        ]
    },
    {
        "header": "Appendix DScaling law fits",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x20.png",
                "caption": "Figure 17:Contour plots for the curves fit withEquation4(our version of the scaling law parameterization). Red line indicates the optimal model size. The star point is not used for fitting the curves.",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x21.png",
                "caption": "Figure 18:Contour plots for the curves fit withEquation1(the chinchilla version of the scaling law parameterization). Red line indicates the optimal model size. The star point is not used for fitting the curves.",
                "position": 1738
            }
        ]
    },
    {
        "header": "Appendix EHyperparameters",
        "images": []
    },
    {
        "header": "Appendix FFull loss-to-loss parameter fits from Figure 1",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12925/x22.png",
                "caption": "Figure 19:Shows the validation loss plotted as a function of train dataset size for different choices of the eigenvalue scalingŒ≤ùõΩ\\betaitalic_Œ≤. Each subplot is a different choice ofNùëÅNitalic_N, the number of model parameters. Solid line indicates numerical data while dashed line indicates theoretical predictionEquation28. The numerics were carried out withM=1.2√ó106ùëÄ1.2superscript106M=1.2\\times 10^{6}italic_M = 1.2 √ó 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT,œÉv=œÉw=1subscriptùúéùë£subscriptùúéùë§1\\sigma_{v}=\\sigma_{w}=1italic_œÉ start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_œÉ start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT = 1and averaged over2000200020002000random seeds.",
                "position": 2357
            },
            {
                "img": "https://arxiv.org/html/2411.12925/x23.png",
                "caption": "Figure 20:Shows the validation loss as a function of the number of model parametersNùëÅNitalic_N, for fixed values of the train dataset sizeDùê∑Ditalic_DandŒ≤=1ùõΩ1\\beta=1italic_Œ≤ = 1. Solid line indicates numerical data while dashed line indicates theoretical predictionEquation28, with the same choice of hyperparameters as inFigure19.",
                "position": 2360
            }
        ]
    },
    {
        "header": "Appendix GComment on theoretical implications",
        "images": []
    }
]