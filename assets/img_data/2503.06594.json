[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/extracted/6264412/imgs/GitHub-logo.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x1.png",
                "caption": "Figure 1:Architectures of machine translation models. In standard NMT models, an encoder is used to encode the source-language sequence x, and a decoder is used to generate the target-language sequence y from left to right. In LLMs, the decoder-only architecture is adopted. Both x and y, along with the prompt c, are represented as a single sequence, which is processed by a large decoding network. In the LaMaTE model, an LLM serves as the encoder. The output of the LLM is transformed into the input to the NMT decoder through an adaptor. The NMT decoder then generates the target-language sequence as usual.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LaMaTE",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x2.png",
                "caption": "Figure 2:Architecture of the NMT Encoder, NMT Decoder, and LLM Decoder. We omit the layer normalization and residual connections for simplicity.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2503.06594/x3.png",
                "caption": "Figure 3:The architecture of LaMaTE, where the Adaptor consists of three components: Fusion combines the representations of layer groupsùê†ksubscriptùê†ùëò\\mathbf{g}_{k}bold_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, MLP reduces the representations‚Äôs dimensionality, and EncStack learns bidirectional representations. The training process consists of two stages: the first stage trains the Adaptor and Decoder, and the second stage trains all model parameters.",
                "position": 356
            }
        ]
    },
    {
        "header": "4ComMT",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x4.png",
                "caption": "Figure 4:Our comprehensive translation dataset,ComMT, includes diverse translation-related tasks. The table presents the training set statistics for ComMT.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2503.06594/x5.png",
                "caption": "Figure 5:Comparison of performance across three datasets‚ÄîWMT17-20, TowerBlock, and ComMT‚Äîfine-tuned on Llama3-8B and evaluated on the WMT23 test set.",
                "position": 729
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x6.png",
                "caption": "Figure 6:Evaluation of LLM translation capabilities in 0-shot and 3-shot settings using the WMT23 test set.",
                "position": 754
            }
        ]
    },
    {
        "header": "6Results and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x7.png",
                "caption": "Figure 7:Comparison of decoder-only (Llama3-8B and TowerInstruct-7B) and encoder-decoder (mT5-large and LaMaTE) models on off-target rate (OTR), unaligned source words (USW), and unaligned target words (UTW).",
                "position": 1025
            },
            {
                "img": "https://arxiv.org/html/2503.06594/x8.png",
                "caption": "Figure 8:Comparison of efficiency: The left chart displays the decoding speedup ratio of LaMaTE versus Llama3-8B under varying source sequence lengths and batch sizes, and the right chart shows the theoretical KV cache size factor for each model.",
                "position": 1119
            },
            {
                "img": "https://arxiv.org/html/2503.06594/x9.png",
                "caption": "Figure 9:The impact of EncStack and decoder depth on model performance and efficiency.",
                "position": 1138
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEncoder-Decoder vs Decoder-Only",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x10.png",
                "caption": "Figure 10:The Encoder-Decoder and Decoder-only architecture.",
                "position": 2806
            }
        ]
    },
    {
        "header": "Appendix BComMT",
        "images": []
    },
    {
        "header": "Appendix CDetailed Experimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06594/x11.png",
                "caption": "Figure 11:Three variants of decoders: Cross Decoder is the standard decoder, while Concat Decoder and Prefix Decoder remove the cross-attention sublayer, integrating source information through self-attention and early fusion methods, respectively.",
                "position": 3988
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]