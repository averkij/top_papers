[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12466/overview.png",
                "caption": "Figure 1:High-level overview of the proposed Inverse-LLaVA framework on an MME benchmark examplefu2023mme.(a) LLaVA projects visual features into discrete text space via an explicit projection, requiring alignment pre-training, and produces the wrong answer (\"Yes\").\n(b) Inverse-LLaVA maps text embeddings into continuous vision space for fusion, eliminating alignment pre-training, and yields the correct answer (\"No\").Blueindicates vision flow,greenindicates text flow,reddenotes explicit projection,purplerepresents LLM components andorangeindicates LLM output.",
                "position": 77
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12466/Inverse_llava_structure_illustration.png",
                "caption": "Figure 2:Architecture comparison between LLaVA and Inverse-LLaVA.LLaVA employs a two-stage training approach with alignment pretraining followed by instruction fine-tuning, where vision and text tokens are concatenated before being fed to the LLM. In contrast, Inverse-LLaVA uses single-stage training with text-guided visual fusion in intermediate layers, where vision information is integrated through learnable text-to-vision projections and combined with original hidden states via residual connections.",
                "position": 176
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12466/mme_benchmark_analysis.png",
                "caption": "Figure 3:MME Benchmark Analysis comparing LLaVA-1.5-7B-LoRA, Inverse-LLaVA, and Inverse-LLaVA-HD across cognitive and perception tasks in the MME benchmark(fu2023mme,).Left (top):Cognitive tasks performance showing Inverse-LLaVA achieving superior performance in numerical calculation (+69%) and text translation (+125%) compared to the baseline LLaVA-1.5-7B-LoRA model.Right (top):Overall performance comparison.Bottom:Perception tasks evaluation shows that Inverse-LLaVA variants excel in Existence and Count tasks, with Inverse-LLaVA-HD achieving perfect performance on Existence tasks. However, significant performance drops in Celebrity recognition (-50%) and OCR tasks (-21%) primarily account for the overall perception score gap. The results indicate that inverse training maintains strong cognitive capabilities while showing task-specific effects on perception.",
                "position": 634
            }
        ]
    },
    {
        "header": "5Limitations and Future Directions",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]