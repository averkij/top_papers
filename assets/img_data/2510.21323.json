[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21323/x1.png",
                "caption": "Figure 1:(a) Current interpretation methods are designed for single-modal representations. (b) Using current methods for each modality leads to a mismatch in the concept sets, hindering the interpretation of the vision-language alignment. (c) We propose the VL-SAE to interpret the alignment mechanism by mapping the representation semantics of both modalities into a unified concept set.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21323/x2.png",
                "caption": "Figure 2:VL-SAE consists of a distance-based encoder that maps semantically similarvision-languagerepresentations to similar activations, and two modality-specific decoders that reconstruct original representations from these activations.",
                "position": 250
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21323/x3.png",
                "caption": "Figure 3:Quantitative evaluation of the learned concept set. We compare the VL-SAE with other methods on the consistency between vision and language semantics within the same neuron (Intra-Similarity) and the semantic diversity across different neurons (Inter-Similarity) on multiple VLMs.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2510.21323/x4.png",
                "caption": "Figure 4:Qualitative comparisons among concepts of SAE-D, SAE-S and the proposed VL-SAE. All SAEs are trained with the vision-language representations of LLaVA 1.5.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2510.21323/x5.png",
                "caption": "Figure 5:Interpreting the vision-language alignment of OpenCLIP-ViT-L/14 with VL-SAE. Given an image-text pair, we visualize the concepts activated by correspondingvisionandlanguagerepresentations separately, as well as thealigned conceptsco-activated by both modalities. Concepts only related to single-modal representations are highlightedin purple.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2510.21323/x6.png",
                "caption": "Figure 6:Interpreting the vision-language alignment of LLaVA1.5 with VL-SAE. Given an input image and the text generated by LVLMs, we visualize the concepts activated by correspondingvisionandlanguagerepresentations. Thehallucination conceptactivated by language representations facilitates a deeper understanding ofobject hallucinations in the text.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2510.21323/x7.png",
                "caption": "Figure 7:Qualitative evaluation of the learned concept set. We provide the maximally activating images and texts for several concepts of VL-SAE trained with the representations of LLaVA 1.5.",
                "position": 932
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21323/x8.png",
                "caption": "Figure A8:Activation count of each concept in VL-SAE constructed for LLaVA1.5liu2023visual.",
                "position": 1779
            }
        ]
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21323/x9.png",
                "caption": "Figure A9:More concepts learned by VL-SAE. We provide the maximally activating images and texts for several concepts of VL-SAE trained for LLaVA1.5.",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2510.21323/x10.png",
                "caption": "Figure A10:Illustration of generated captions by different decoding methods with LLaVA 1.5 as the backbone model. Hallucinated content is highlighted inpurple.",
                "position": 2145
            }
        ]
    },
    {
        "header": "Appendix BMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21323/x11.png",
                "caption": "Figure A11:Visualizations of the vision-language representations in the VLMs and the learnable weights in the VL-SAE encoder. Experiments are conducted with t-SNEvan2008visualizingfor dimension reduction.",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2510.21323/x12.png",
                "caption": "Figure A12:Visualizations of the vision-language representations in different layers of VLMs. Experiments are conducted with t-SNEvan2008visualizingfor dimension reduction.",
                "position": 2487
            }
        ]
    },
    {
        "header": "Appendix CDiscussion",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]