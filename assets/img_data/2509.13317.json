[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13317/x1.png",
                "caption": "Figure 1:From precise region-based distance estimation (left), to intricate multi-view region query (middle), and global cross-frame reasoning (right), SR-3D delivers flexible and accurate spatial understanding to foundational Vision-Language Models. Notably, this video is obtained in the wild,without sensory 3D inputs, showcasing the remarkable generalization capability of our model.",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13317/x2.png",
                "caption": "Figure 2:The SR-3D architecture. Given an image or multi-view input with optional region prompts (e.g., bounding boxes or masks), we encode them along with depth-derived positional embeddings using a tiling approach. Region tokens are extracted by stitching masked features, while 3D positional embeddings are mapped to a shared canonical space in the multi-view setting, as shown on the bottom right.",
                "position": 203
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13317/x3.png",
                "caption": "Figure 3:RealWorldQA results. SR-3D shows stronger spatial understanding of physical environments compared to the base model. We omit the answer choices for clarity in visualization.",
                "position": 377
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13317/x4.png",
                "caption": "Figure 4:SR-3D results on region-level multi-view spatial understanding. We show extreme cases where the same region prompts are used across samples but with different target objects. SR-3D answers all queries correctly, showing strong evidence that it truly understands 3D spatial relationships.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2509.13317/x5.png",
                "caption": "Figure 5:VSI-Bench results. We highlight SR-3D ’s outputs and include ground-truth values for numerical answers. The results show that SR-3D answers spatial questions correctly even without region prompts.",
                "position": 1434
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix: Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AMore Quantitative Results on 3D General Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BMore Quantitative Results on VSI-Bench",
        "images": []
    },
    {
        "header": "Appendix CMore Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13317/x6.png",
                "caption": "Figure 6:More results on VSI-Bench[54]. We highlight SR-3D ’s outputs and include ground-truth values for numerical answers.",
                "position": 3722
            }
        ]
    },
    {
        "header": "Appendix DStatistics of SR-3D-Bench",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details of SR-3D",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    }
]