[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3\\ourdataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00109/x1.png",
                "caption": "Figure 1:Diagram illustrating our data generation pipeline. To tackle the challenges of long-form evaluation, we employ a model-in-the-loop approach, enabling rapid and comprehensive assessments of prompt quality.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2508.00109/x2.png",
                "caption": "Figure 2:Distributions of prompt categories in\\ourdataset.",
                "position": 132
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00109/x3.png",
                "caption": "Figure 3:Factual precision as evaluated by human annotators on 100 sentences per model for each benchmark. All the models are retrieval-augmented.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2508.00109/x4.png",
                "caption": "Figure 4:Top: Examples illustrating potential quality issues of existing factuality benchmarks. Bottom Right: Average prompt lengths. Compared to LongFact and FactBench,\\ourdatasethas significantly longer and more detailed prompts. Bottom Left: Example prompts from\\ourdataset.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2508.00109/x5.png",
                "caption": "Figure 5:Diagram illustrating the evaluation pipeline of atomic prompts. The atomic prompts are automatically generated by instructing language models to build generic questions using proper nouns in the original prompts. These atomic prompts are then used independently to assess the language modelsâ€™ knowledge of specific subjects.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2508.00109/x6.png",
                "caption": "Figure 6:Comparison of factual precision when evaluated on atomic prompts versus original prompts from the\\ourdatasetHard dataset.",
                "position": 378
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Prompts",
        "images": []
    },
    {
        "header": "7Model Output Lengths",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00109/x7.png",
                "caption": "Figure 7:Average numbers of claims (produced by VeriScore) for model outputs.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2508.00109/x8.png",
                "caption": "Figure 8:Average numbers of sentences for model outputs.",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2508.00109/x9.png",
                "caption": "Figure 9:Distributions of the types of human edits.",
                "position": 1005
            }
        ]
    },
    {
        "header": "8Human Edits in Prompt Annotations",
        "images": []
    }
]