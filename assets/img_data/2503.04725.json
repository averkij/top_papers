[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04725/x1.png",
                "caption": "Figure 1:Illustration of the central idea of our paper. (a) A random block of text of lengthLğ¿Litalic_Lis divided into two partsğ‘¿ğ‘¿{\\boldsymbol{X}}bold_italic_Xandğ’€ğ’€{\\boldsymbol{Y}}bold_italic_Y. (b) The (bipartite) mutual information between the random variablesğ‘¿ğ‘¿{\\boldsymbol{X}}bold_italic_Xandğ’€ğ’€{\\boldsymbol{Y}}bold_italic_Yhas a power-law scaling with respect to the length. (c) Autoregressive neural networks parameterize conditional distributionsqâ¢(ğ’š|ğ’™)=(ğ’š|xâ„“,ğ’›)ğ‘conditionalğ’šğ’™conditionalğ’šsubscriptğ‘¥â„“ğ’›q({\\boldsymbol{y}}|{\\boldsymbol{x}})=({\\boldsymbol{y}}|x_{\\ell},{\\boldsymbol{z%\n}})italic_q ( bold_italic_y | bold_italic_x ) = ( bold_italic_y | italic_x start_POSTSUBSCRIPT roman_â„“ end_POSTSUBSCRIPT , bold_italic_z )via a hidden stateğ’›ğ’›{\\boldsymbol{z}}bold_italic_zthat caches past information. (d) We formulate the Long-context Language Modeling (L2M) condition, which states that a modelâ€™s state size for storing past information must grow at least as the power-law scaling of the bipartite mutual information for effective long context length modeling.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Mutual Information Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04725/x2.png",
                "caption": "Figure 2:Illustration and estimations of bipartite and two-point mutual information in natural language. (a) Thebipartitemutual information measures statistical dependence between two adjacent segments within a text block of lengthLğ¿Litalic_L. (b, c) Estimations using LLaMA 3.1 405B model(Meta,)onPG19dataset(Rae etÂ al.,2020)of pre-1919 books andWikipedia(Foundation,). Both direct estimation and vCLUB approximation(Cheng etÂ al.,2020)show consistent results across datasets. See Appx.A.IVfor results using DeepSeek and other LLaMA models. (d) Thetwo-pointmutual information measures statistical dependence between tokens separated by distancedğ‘‘ditalic_d. (e, f) Two-point mutual information estimations onPG19dataset andWikipedia.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x3.png",
                "caption": "Figure 3:Bipartite and two-point mutual information scalings for two families of multivariate Gaussian distributions with varying sequence lengths (see Appx.B). The two families of distributions (a) have significantly different scaling for bipartite mutual information (LÎ²superscriptğ¿ğ›½L^{\\beta}italic_L start_POSTSUPERSCRIPT italic_Î² end_POSTSUPERSCRIPTvslogâ¡(L)ğ¿\\log{L}roman_log ( start_ARG italic_L end_ARG )), but (b) identical two-point mutual information scaling (dâˆ’Î±superscriptğ‘‘ğ›¼d^{-\\alpha}italic_d start_POSTSUPERSCRIPT - italic_Î± end_POSTSUPERSCRIPT).",
                "position": 496
            }
        ]
    },
    {
        "header": "5Long-Context Language Modeling (L2M) Condition",
        "images": []
    },
    {
        "header": "6Empirical Verification",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04725/x4.png",
                "caption": "Figure 4:Evaluation of KL-divergence across model architectures trained on sub-volume Gaussian distributions. (a, b) Average KL-divergence per token for models trained on different sequence lengths. (c, d) Position-wise conditional KL-divergence for models trained on sequence length 4096. Lower values indicate better performance.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x5.png",
                "caption": "Figure 5:Position-wise conditional negative log likelihood (NLL) evaluation for models trained on 4096-token sequences on thePG19dataset(Rae etÂ al.,2020). Lower values indicate better performance, with performance differences at later positions highlighting varying capabilities in modeling long-range dependencies.",
                "position": 639
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details on Mutual Information Scalings",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04725/x6.png",
                "caption": "Figure A.1:Effect of bias correction method in the direct estimator. The bias only affects the estimation at small sequence lengths, and all methods converge at large sequence lengths.",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x7.png",
                "caption": "Figure A.2:Bipartite mutual information estimation using Deepseek V3 Base model and LLaMA 3.1 70B model compared to LLaMA 3.1 405B model on thePG19dataset. All direct measurements include the bias correction described in Appx.A.III. Both models appear to produce worse estimations of bipartite mutual information at long sequence lengths, but still suggest a power-law scaling.",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x8.png",
                "caption": "Figure A.3:Bipartite mutual information estimation using different ratios ofâ„“/Lâ„“ğ¿\\ell/Lroman_â„“ / italic_L. All results suggest the existence of power-law scaling, with various fitted exponents.",
                "position": 2424
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x9.png",
                "caption": "Figure A.4:Effect of bias correction for two-point mutual information. The bias causes a plateau at large distances.",
                "position": 2509
            }
        ]
    },
    {
        "header": "Appendix BMultivariate Gaussian Distributions",
        "images": []
    },
    {
        "header": "Appendix CModel State for Storing Past Information",
        "images": []
    },
    {
        "header": "Appendix DAdditional Proofs for Theorem5.2",
        "images": []
    },
    {
        "header": "Appendix EAdditional Details on Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04725/x10.png",
                "caption": "Figure F.5:Evaluation of KL-divergence across model architectures trained on sub-volume Gaussian distributions. (a, b) Average KL-divergence per token for models trained on different sequence lengths [same as Fig.4(a, b)]. (c, d) Position-wise conditional KL-divergence for models trained on sequence length 256. (e, f) Position-wise conditional KL-divergence for models trained on sequence length 1024. (g, h) Position-wise conditional KL-divergence for models trained on sequence length 4096 [same as Fig.4(c, d)]. Lower values indicate better performance.",
                "position": 2946
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x11.png",
                "caption": "Figure F.6:Negative log likelihood (NLL) across model architectures trained on sub-volume Gaussian distributions (a, b) Average NLL per token for models trained on different sequence lengths. (c, d) Position-wise conditional NLL for models trained on sequence length 4096. Lower values indicate better performance.",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2503.04725/x12.png",
                "caption": "Figure F.7:Position-wise conditional negative log likelihood (NLL) evaluation for models trained on 4096-token sequences of thePG19dataset.",
                "position": 2958
            }
        ]
    },
    {
        "header": "Appendix FAdditional Experimental Results",
        "images": []
    }
]