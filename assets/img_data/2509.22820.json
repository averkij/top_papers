[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22820/x1.png",
                "caption": "Figure 1:Examples of personalized queries across task types and representative failure cases of recent VLMs.indicates GPT-4o, whileindicates LLaVA family models such as LLaVA-NeXT.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/icons/closed.png",
                "caption": "",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/icons/open.png",
                "caption": "",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x2.png",
                "caption": "Figure 2:Overview ofMMPB.\n(top) A three‐step construction process ensuring high quality and scalability.\n(bottom) An evaluation protocol for assessing the VLM’spersonalization criteria.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x3.png",
                "caption": "Figure 3:Example of quality control for aCoherency-type query with concept-only and image-only distractors.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x4.png",
                "caption": "Figure 4:Evaluation results of 23 VLMs onMMPBunder 0-turn and 10-turn settings. Model names are followed by their average ranks across eight general-purpose multi-modal benchmarks333Hugging Face Open VLM Leaderboard, accessed on May 15, 2025..",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x5.png",
                "caption": "Figure 5:Performance gap between preference-grounded and recognition VQA tasks in VLMs.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x5.png",
                "caption": "Figure 5:Performance gap between preference-grounded and recognition VQA tasks in VLMs.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x6.png",
                "caption": "Figure 6:Refusal counts of models across task and concepts under image-based injection.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/models/gpt.png",
                "caption": "Table 3:Examples of evasive responses in human category tasks.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/models/claude.png",
                "caption": "",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/models/intern.png",
                "caption": "",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x7.png",
                "caption": "Figure 7:Personalization bias across models, measured asAppropriateness–Awareness. Positive values indicate under-personalization (missed valid personalization), and negative values indicate over-personalization (affirming inappropriate inputs).",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x8.png",
                "caption": "(a)",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x8.png",
                "caption": "(a)",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x9.png",
                "caption": "(b)",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x10.png",
                "caption": "(a)",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x10.png",
                "caption": "(a)",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x11.png",
                "caption": "(b)",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x12.png",
                "caption": "(c)",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/human_eval.png",
                "caption": "Figure 10:Interface of the human evaluation platform.",
                "position": 2999
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/human_results/result1.png",
                "caption": "(a)",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/human_results/result1.png",
                "caption": "(a)",
                "position": 3005
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/human_results/result2.png",
                "caption": "(b)",
                "position": 3010
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/human_results/result3.png",
                "caption": "(c)",
                "position": 3015
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x13.png",
                "caption": "Figure 12:Distribution of preference keywords.",
                "position": 3174
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/pref_wc/entertainment.png",
                "caption": "Figure 13:Word clouds of preference keywords inMMPBacross five domains.",
                "position": 3177
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/pref_wc/entertainment.png",
                "caption": "",
                "position": 3180
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/pref_wc/travel.png",
                "caption": "",
                "position": 3184
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/pref_wc/lifestyle.png",
                "caption": "",
                "position": 3188
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/pref_wc/shopping.png",
                "caption": "",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/pref_wc/fashion.png",
                "caption": "",
                "position": 3196
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x14.png",
                "caption": "Figure 14:Multi-turn conversation performance drop ratios by injection modality. (left) preference accuracy drop; (right) recognition accuracy drop.",
                "position": 3292
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x14.png",
                "caption": "",
                "position": 3295
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x15.png",
                "caption": "",
                "position": 3299
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x16.png",
                "caption": "Figure 15:Multi-turn conversation performance withRecognition-relatedandPreference-relatedtopics: (left) recognition accuracy; (right) preference accuracy.",
                "position": 3390
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x16.png",
                "caption": "",
                "position": 3393
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x17.png",
                "caption": "",
                "position": 3397
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x18.png",
                "caption": "(a)Overall",
                "position": 3536
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x18.png",
                "caption": "(a)Overall",
                "position": 3539
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x19.png",
                "caption": "(b)Recognition",
                "position": 3544
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x20.png",
                "caption": "(c)Preference",
                "position": 3550
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x21.png",
                "caption": "(d)Human",
                "position": 3555
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x22.png",
                "caption": "(e)Animal",
                "position": 3561
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x23.png",
                "caption": "(f)Object",
                "position": 3566
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x24.png",
                "caption": "(g)Character",
                "position": 3572
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x25.png",
                "caption": "(h)Awareness",
                "position": 3577
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x26.png",
                "caption": "(i)Appropriateness",
                "position": 3583
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x27.png",
                "caption": "(j)Coherency",
                "position": 3588
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x28.png",
                "caption": "Figure 18:Performances across the task types",
                "position": 3600
            },
            {
                "img": "https://arxiv.org/html/2509.22820/figures/cat_vlm.png",
                "caption": "Figure 19:Performance gains by concept category as a function of general VQA leaderboard rank on eight tasks777Same task set as in Figure3and §5.1, including MMBench[58], MMStar[14], MMMU[97], MathVista[60], OCRBench[64], AI2D[35], HallusionBench[29], and MMVet[96].; the object category aligns most closely with rank.",
                "position": 3603
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x29.png",
                "caption": "Figure 20:Results across four personalizable concept categories.",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x30.png",
                "caption": "Figure 21:Results across three task types.",
                "position": 3614
            },
            {
                "img": "https://arxiv.org/html/2509.22820/x31.png",
                "caption": "Figure 22:Difference between overall performance and performance on each human concept, indicating no evidence of nationality- or ethnicity-based personalization bias.",
                "position": 3618
            }
        ]
    },
    {
        "header": "Supplementary Material",
        "images": []
    }
]