[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1. Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15322/x1.png",
                "caption": "Figure 1:In addition to training on audio-visual-(text) datasets, we perform multimodal joint training with high-quality, abundant audio-text data which enables effective data scaling.\nAt inference, MMAudio generates conditions-aligned audio with video and/or text guidance.",
                "position": 234
            }
        ]
    },
    {
        "header": "2. Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15322/x2.png",
                "caption": "Figure 2:Overview of the MMAudio flow-prediction network.\nVideo conditions, text conditions, and audio latents jointly interact in the multimodal transformer network.\nA synchronization model (Section3.4) injects frame-aligned synchronization features for precise audio-visual synchrony.",
                "position": 312
            }
        ]
    },
    {
        "header": "3. MMAudio",
        "images": []
    },
    {
        "header": "4. Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15322/x3.png",
                "caption": "Table 1:Video-to-audio results on the VGGSound test set.\nFollowing the common practice[63], the parameter counts exclude pretrained feature extractors (e.g., CLIP), latent space encoders/decoders, and vocoders.\nTime is the total running time using the official code to generate one sample with a batch size of one after warm-up and excludes any disk I/O operations on an H100 GPU.∗∗\\ast∗: reproduced using official evaluation code.††\\dagger†: evaluated using generation samples obtained directly from the authors.◆◆\\lozenge◆: does not use text input during testing.\nNote, Seeing&Hearing[65]directly optimizes ImageBind score during test time, therefore attains the highest IB-score.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x3.png",
                "caption": "Figure 3:We visualize the spectrograms of generated audio (by prior works and our method) and the ground-truth.\nNote our method generates the audio effects most closely aligned to the ground-truth, while other methods often generate sounds not explained by the visual input and not present in the ground-truth.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x4.png",
                "caption": "Table 2:Text-to-audio results on the AudioCaps test set.\nFor a fair comparison, we follow the evaluation protocol of[11]and transcribe all baselines directly from[11], who have reproduced those results using officially released checkpoints under the same evaluation protocol.",
                "position": 656
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x4.png",
                "caption": "Table 2:Text-to-audio results on the AudioCaps test set.\nFor a fair comparison, we follow the evaluation protocol of[11]and transcribe all baselines directly from[11], who have reproduced those results using officially released checkpoints under the same evaluation protocol.",
                "position": 657
            }
        ]
    },
    {
        "header": "5. Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUser Study",
        "images": []
    },
    {
        "header": "Appendix BComparisons with Movie Gen Audio",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15322/extracted/6081424/fig/00048.jpg",
                "caption": "",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2412.15322/extracted/6081424/fig/00016.jpg",
                "caption": "",
                "position": 2117
            }
        ]
    },
    {
        "header": "Appendix CDetails on Data Overlaps",
        "images": []
    },
    {
        "header": "Appendix DDetails on the Audio Latents",
        "images": []
    },
    {
        "header": "Appendix ENetwork Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15322/x5.png",
                "caption": "",
                "position": 2381
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x6.png",
                "caption": "",
                "position": 2392
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x7.png",
                "caption": "",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x8.png",
                "caption": "",
                "position": 2428
            }
        ]
    },
    {
        "header": "Appendix FTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15322/x9.png",
                "caption": "",
                "position": 2493
            },
            {
                "img": "https://arxiv.org/html/2412.15322/x10.png",
                "caption": "",
                "position": 2500
            }
        ]
    },
    {
        "header": "Appendix GAdditional Visualizations",
        "images": []
    }
]