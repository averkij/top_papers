[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21991/x1.png",
                "caption": "Figure 1:Comparison with prior work on high-resolution visual reasoning.The yellow box marks the target object, which becomes indiscernible after input-image downsampling. (a)Zheng et al. (2025b)succeeds when the object remains discernible, but at the cost of a large number of vision tokens. (b)Zheng et al. (2025b)fails when the object is indiscernible at low resolution, where fewer vision tokens are available. (c) Our ERGO performs reasoning-driven perception, correctly answering the question even on low-resolution images.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Motivation",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21991/x2.png",
                "caption": "Figure 2:Overview of RL-based training pipeline.The red background highlights the components of the proposed TCE reward.\nThe green background highlights the conventional rewards adopted by most reasoning LVLMs.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2509.21991/figures/why_gamma.png",
                "caption": "Figure 3:Analysis of query-relevant GT regions in training data.Most GT regions span less than 60% of the full image area.",
                "position": 225
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21991/figures/pareto_optimal.png",
                "caption": "Figure 4:Performance-efficiency trade-off on the V* benchmark.The total number of vision tokens is the sum of the tokens from the downsampled original image and those from the high-resolution cropped image.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2509.21991/figures/pareto_optimal.png",
                "caption": "Figure 4:Performance-efficiency trade-off on the V* benchmark.The total number of vision tokens is the sum of the tokens from the downsampled original image and those from the high-resolution cropped image.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2509.21991/figures/obj_masking_exp.png",
                "caption": "Figure 5:Evaluation of model robustness under target-object masking.",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2509.21991/figures/bias_free_prediction.png",
                "caption": "Figure 6:Bias-free region prediction.ERGO adapts region sizes properly for (a) the high-resolution MME-RWL and (b) the low-resolution MMVP, indicating that the box adjustment constant does not bias the region predictions.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2509.21991/figures/bias_free_prediction.png",
                "caption": "Figure 6:Bias-free region prediction.ERGO adapts region sizes properly for (a) the high-resolution MME-RWL and (b) the low-resolution MMVP, indicating that the box adjustment constant does not bias the region predictions.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2509.21991/figures/box_adjustment_ablation.png",
                "caption": "Figure 7:Impact of box adjustment reward on predicted regions during training.",
                "position": 840
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Algorithm",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21991/x3.png",
                "caption": "Figure 8:ERGO utilizes coarse cues (“the region where the bottle is located\") to provide the answer.The yellow box highlights the region linked to the answer for clear visualization.",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2509.21991/x4.png",
                "caption": "Figure 9:ERGO leverages coarse cue (“where people are visible\") to answer the question.The yellow box indicates the region associated with the answer to aid visualization.",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2509.21991/x5.png",
                "caption": "Figure 10:ERGO can also exploit clear visual cues (the purple umbrella and the orange luggage) when the object is still discernible.The yellow box highlights the region associated with the answer for clear visualization.",
                "position": 1589
            }
        ]
    },
    {
        "header": "Appendix CQualitative Results",
        "images": []
    }
]