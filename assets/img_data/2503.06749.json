[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06749/extracted/6263005/fig/motivation.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06749/x1.png",
                "caption": "Figure 2:The overall data generation pipeline incorporating our Modality Bridging method. The multimodal data is first sent to MLLMs to obtain a “Pseudo-CoT” consisting of a caption and reasoning process, which serves as the input of MLLMs along with the original image-question pair to produce detailed descriptions. Through this modality bridging approach, the textual descriptions provide DeepSeek-R1 with holistic information that facilitates the generation of high-quality CoT processes, which are post-processed and integrated with the original data to create the final Vision-R1-cold dataset.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2503.06749/x2.png",
                "caption": "Figure 3:Comparison between the CoT processes generated by descriptions with and without “Pseudo-CoT”.\nSimple descriptions generated without the “Pseudo-CoT” input lack sufficient visual information, leading to confusion and hallucination in the reasoning process of DeepSeek-R1.\nIn contrast, detailed descriptions enhanced through our Modality Bridging with “Pseudo-CoT” integrate high-quality visual information into textual descriptions, which facilitates accurate reasoning and enables R1 to generate correct answers.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06749/x3.png",
                "caption": "Figure 4:GRPO with our proposed PTST strategy.\nWe progressively loosen the context length restrictions, increasing the length of reasoning process. Specifically, we set the reasoning length to 4K, 8K and 16K tokens for each stage, with corresponding group numbers of 16, 8 and 4 respectively. The reward function for GRPO is based on a hard formatting result reward function (HFRRF).\nThe dotted line in the “Stage 3” indicates that the final version of Vision-R1 did not undergo the third stage of training.",
                "position": 212
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06749/x4.png",
                "caption": "Figure 5:The output examples of Vision-R1-7B on MathVerse benchmark. Vision-R1-7B shows “human-like” questioning and self-reflective thought process when solving math reasoning problems, which is also called“Aha moment”in DeepSeek-R1’s paper[2]. More examples are provided in supplementary materials.",
                "position": 880
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06749/x5.png",
                "caption": "Figure 6:Examples of our Vision-R1-cold data. It comprises abundant information obtained through our Modality Bridging method.",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2503.06749/x6.png",
                "caption": "Figure 7:More output examples of our Vision-R1-7B on MathVerse benchmark.",
                "position": 2102
            }
        ]
    },
    {
        "header": "6Additional Dataset Illustrations",
        "images": []
    },
    {
        "header": "7Data Sources",
        "images": []
    }
]