[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13013/x1.png",
                "caption": "Figure 1:We propose EgoTwin, a diffusion-based framework that jointly generates egocentric video and human motion in a viewpoint consistent and causally coherent manner. Generated videos can be lifted into 3D scenes using camera poses derived from human motion via 3D Gaussian Splattingkerbl20233d.",
                "position": 63
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13013/x2.png",
                "caption": "Figure 2:Overview.EgoTwin features a triple-branch architecture(left), where motion branch spans only the lower half of the layers used by text and video branches. Each branch has its own tokenizer and transformer blocks(right), with shared weights across branches indicated by matching colors.",
                "position": 137
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13013/x3.png",
                "caption": "Figure 3:Head pose regression errors over epochs.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2508.13013/x3.png",
                "caption": "Figure 3:Head pose regression errors over epochs.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2508.13013/x4.png",
                "caption": "Figure 4:Interaction mechanism.",
                "position": 171
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13013/x5.png",
                "caption": "Figure 5:Qualitative results of joint video and motion generation, based on a textual prompt and initial frames of both video and motion.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2508.13013/x6.png",
                "caption": "Figure 6:Results of conditional generation.Left:motion generation conditioned on text and video;Right:video generation conditioned on text and motion.",
                "position": 488
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]