[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18900/x1.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18900/x2.png",
                "caption": "Figure 2:Our framework operates as a collaborative multi-agent system with access to a shared memory that maintains a dynamic Consistency Index (CI), the current panel set, and the latest consistency report. First, Story Initialization Agent, which takes a user-provided sequence of story prompts and character descriptions, and generates initial story panels using a set of off-the-shelf story visualization methods, including both Flux and SD-based models. Once the initial panels are generated, the Audit Agent evaluates each panel using a VLM, updates the CI, and produces a detailed consistency report. This is followed by the Repair Phase, where the Repair Agent applies localized edits to inconsistent panels using editing tools such as Flux-ControlNet. The Consistency Director Agent oversees the entire process, iteratively triggering the Audit and Repair phases until the CI reaches a predefined threshold or a maximum number of refinement iterations is completed.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/main_figure_updated.png",
                "caption": "Figure 3:Qualitative results for Audit & Repair. Our method identifies visual inconsistencies—such as mismatched clothing, character identity or hairstyle changes—across story panels and refines them using agent-guided editing. The corrected stories preserve narrative coherence and visual consistency, improving character fidelity throughout the sequence.",
                "position": 211
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/comparison_updated_final.png",
                "caption": "Figure 4:Qualitative comparison of our method with state-of-the-art story visualization methods, including StoryDiffusion, StoryGen, ConsiStory, AutoStudio, and DSD. Our method outperforms existing approaches by preserving consistent visual elements—such as character appearance, clothing, and identity—across all panels. In contrast, prior methods frequently exhibit inconsistencies and blending artifacts that compromise story coherence and the visual identity of characters.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/iteration_figure.png",
                "caption": "Figure 5:a)Iterative Refinement. Our framework progressively improves consistency across iterations—for instance, correcting dress color in Iteration 1 and hairstyle in Iteration 2, or adjusting a rocket’s design to match the prompt.b)User-in-the-loop Correction. Users can interactively guide edits, enabling both fine-grained adjustments (e.g. dress color) and broader changes (e.g. replacing a hamster with a cat), demonstrating the system’s flexibility and controllability.",
                "position": 244
            }
        ]
    },
    {
        "header": "5Broader Impact and Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AAdditional Qualitative Results and Comparison",
        "images": []
    },
    {
        "header": "Appendix BDetails of User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/supp_2.png",
                "caption": "Figure 6:Inconsistency correction results on the DSD method, illustrated through qualitative examples produced by our agentic framework.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/supp-comp.png",
                "caption": "Figure 7:More qualitative comparisons of our method against existing consistency methods.",
                "position": 948
            }
        ]
    },
    {
        "header": "Appendix CExamples for Segment-based Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/user_study.png",
                "caption": "Figure 8:The user study questionnaire includes two evaluation questions: one measuring character consistency and the other assessing story alignment, each rated on a scale from 1 to 5.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2506.18900/extracted/6564607/figures/fg.png",
                "caption": "Figure 9:Example of segmented foreground images.",
                "position": 968
            }
        ]
    },
    {
        "header": "Appendix DSample Prompts",
        "images": []
    }
]