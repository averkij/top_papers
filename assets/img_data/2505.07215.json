[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07215/x1.png",
                "caption": "Figure 1:Overview of our benchmark creation process. We start by generating descriptions of two-player strategy games, after which we generate implementations of these games as Gym environments. Lastly, we employ self-play reinforcement learning to train agents on these games",
                "position": 161
            }
        ]
    },
    {
        "header": "2gg-bench",
        "images": []
    },
    {
        "header": "3Analysis of Generated Games",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion & Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGame Descriptions",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CPlagiarism Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07215/x2.png",
                "caption": "Figure 5:Distribution of the highest similarity score for every one of the126126126126games ingg-bench.",
                "position": 1857
            }
        ]
    },
    {
        "header": "Appendix DGoal-Driven Clustering of Game Descriptions",
        "images": []
    },
    {
        "header": "Appendix EScalability Details",
        "images": []
    }
]