[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07419/x1.png",
                "caption": "Figure 1:RoMA on OLMoE-7B-A1B vs. 7-34B dense LLMs across eight benchmarks. RoMA leads to 7-15% accuracy improvement, consistently outperforming all models over eight benchmarks, demonstrating the effectiveness of post-training by RoMA.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x2.png",
                "caption": "Figure 2:Overview of RoMA. RoMA finetunes routers in MoE LLM (bottom, yellow) with a training objective defined on each sample(xi,yi)(x_{i},y_{i}), which is composed of (1) the task lossℒtask​(i)\\mathcal{L}_{\\text{task}}(i)defined on the model outputf​(xi,ri)f(x_{i},r_{i}); and\n(2) the manifold alignment regularizationℒmanifold​(i)\\mathcal{L}_{\\text{manifold}}(i), which aligns the manifolds of routing weights (right, green) and the task embedding (left, blue). It improves MoE’s generalization by unifying solution generation in MoE with task understanding.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Task-Expert Routing Manifold Misalignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07419/x3.png",
                "caption": "Figure 3:UMAP visualization of task embedding and routing weights manifolds for samples in ARC-C.(a)Their task embedding shows cluster structures.(b)Routing weights by pretrained MoE are scattered and misaligned with the task embedding clusters.(c)RoMA aligns routing weights with the task embedding manifold’s cluster structure.(d)RoMA also achieves a similar manifold structure as that of the optimal routing weights (oracle), which explains the improvement in generalization.",
                "position": 132
            }
        ]
    },
    {
        "header": "4Routing Manifold Regularization (RoMA)",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07419/x4.png",
                "caption": "Figure 4:Training set statistics.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x5.png",
                "caption": "Figure 5:Performance and inference cost of OLMoE (base model), OLMoE + C3PO and OLMoE + RoMA across eight benchmarks.(a)Accuracy: RoMA consistently improves the base model’s performance to be comparable or better than C3PO.(b)Inference cost (in FLOPs×1011\\times 10^{11}): RoMA maintains nearly the same efficiency as the base model, while C3PO requires test-time optimization and induces66–7×7\\timesmore FLOPs. These results highlight the effectiveness and efficiency of RoMA.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x6.png",
                "caption": "Figure 6:Applying RoMA at different layers (F: early layers, M: middle layers, L: late layers). Fine-tuning the routers in the last five layers (L5, RoMA) achieves the best performance.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x7.png",
                "caption": "Figure 7:Applying RoMA to routing weights of tokens at different positions. Regularizing the Last1 token’s routing weights performs the best.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x7.png",
                "caption": "Figure 7:Applying RoMA to routing weights of tokens at different positions. Regularizing the Last1 token’s routing weights performs the best.",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x8.png",
                "caption": "Figure 8:Comparing neighbor selection strategies in RoMA. Rand—random neighbors.kk-NN withk=3k=3achieves the best performance.",
                "position": 993
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x9.png",
                "caption": "Figure 9:Comparing different training set sizes for RoMA on OLMoE. While the full training set (100%) yields the best performance, 30% suffices to achieve substantial gains over the baselines.",
                "position": 1002
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x9.png",
                "caption": "Figure 9:Comparing different training set sizes for RoMA on OLMoE. While the full training set (100%) yields the best performance, 30% suffices to achieve substantial gains over the baselines.",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x10.png",
                "caption": "Figure 10:Comparing different regularization methods with RoMA. RoMA’s manifold regularization achieves the best performance.",
                "position": 1010
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07419/x11.png",
                "caption": "Figure 11:(a): Radar figure of DeepSeekMoE-16B-A3B,(b)Radar figure of Qwen3-30B-A3B. RoMA consistently improves model’s performance on multiple benchmarks.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x12.png",
                "caption": "Figure 12:(a)Accuracy: RoMA achieves similar accuracy improvement as C3PO on DeepSeekMoE-16B-A3B.(b)Inference cost (in FLOPs×1011\\times 10^{11}): RoMA maintains nearly the same efficiency as the base model, while C3PO requires test-time optimization and induces66–7×7\\timesmore FLOPs.",
                "position": 1051
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x13.png",
                "caption": "Figure 13:(a)Accuracy: RoMA achieves similar accuracy improvement as C3PO on Qwen3-30B-A3B.(b)Inference cost (in FLOPs×1011\\times 10^{11}): RoMA maintains nearly the same efficiency as the base model, while C3PO requires test-time optimization and induces66–7×7\\timesmore FLOPs.",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x14.png",
                "caption": "Figure 14:Training Set by Task",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2511.07419/x14.png",
                "caption": "Figure 14:Training Set by Task",
                "position": 1064
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]