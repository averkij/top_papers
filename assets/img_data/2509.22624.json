[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22624/iclr2026/figures/logo_v1.png",
                "caption": "",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22624/x1.png",
                "caption": "Figure 1:(a)Early studies of RL with Verifiable Rewards (RLVR) and RL from Human Feedback (RLHF) that rely on reward models.(b)We propose SPARK that recycles the rollouts from the RLVR, and further trains the model itself as a generative reward model.(c)SPARK consistently outperforms early RL approaches in both reasoning and reward model benchmarks.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22624/x2.png",
                "caption": "Figure 2:SPARK Framework.(a) Training: Our method recycles the valuable rollouts from verifiable reward-guided generation to simultaneously train a unified policy modelπθ\\pi_{\\theta}, also as a generative reward model.(b) Inference: at test time, the single unified model can handle reasoning, judgment, and reflection tasks for test-time scaling, eliminating the need for external reward or judge models.",
                "position": 274
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22624/x3.png",
                "caption": "Figure 3:Math Reasoning Case.We illustrate the reasoning process of SPARK on a mathematical task, covering reasoning, judgment, and reflection. For brevity, parts of the content are omitted.",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x4.png",
                "caption": "Figure 4:Study on Model’s Reward Accuracy.We evaluate the model’s judgment ability by measuring its accuracy in determining whether its own answers are correct.",
                "position": 1288
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22624/x5.png",
                "caption": "Figure 5:Mathematical Prompt.Prompt suffix used for mathematical benchmark evaluation.",
                "position": 2807
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x6.png",
                "caption": "Figure 6:Prompt for VL-RewardBench.In the figure, the placeholdersqueryandanswershould be replaced with the specific content of each task.",
                "position": 2810
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x7.png",
                "caption": "Figure 7:On-Policy Data Generation Prompts.The figure illustrates four different prompt templates used for generating reward and reflection data.",
                "position": 2813
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x8.png",
                "caption": "Figure 8:Reward Reasoning Case.The example is taken from VL-RewardBench(Li et al.,2025).",
                "position": 2866
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x9.png",
                "caption": "Figure 9:Math Inference Case.",
                "position": 2870
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x10.png",
                "caption": "Figure 10:VL-RewardBench Inference Case 1.",
                "position": 2873
            },
            {
                "img": "https://arxiv.org/html/2509.22624/x11.png",
                "caption": "Figure 11:VL-RewardBench Inference Case 2.",
                "position": 2876
            }
        ]
    },
    {
        "header": "Outline",
        "images": []
    }
]