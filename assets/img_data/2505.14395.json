[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14395/x1.png",
                "caption": "Figure 1:General concept ofMuG-Eval. Two instances of the same LLM engage in self-communication in the target language to complete information-gap tasks. Model outputs are evaluated using algorithmic methods (e.g., string matching or code testing), without requiring language-specific tools or LLMs-as-judges. Task success rate serves as a proxy for measuring the model‚Äôs multilingual generation capability.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2505.14395/extracted/6458800/images/emoji/mug6.png",
                "caption": "Table 1:Positioning ofMuG-Eval among multilingual evaluation benchmarks.MuG-Eval uniquely combines: (1) evaluation of generation capability (not just comprehension), (2) cross-linguistically comparable metrics, and (3) objective scoring without LLMs-as-judges, and (4) reduced dependency on cross-lingual annotation. Tested on 30 languages,MuG-Eval currently supports 2,102 languages via GlotLID(Kargaran et¬†al.,2023), with the potential to scale further as more advanced language identification tools develop. Benchmarks referenced are MultiQ(Holtermann et¬†al.,2024), Flores-101(Goyal et¬†al.,2022), XL-Sum(Hasan et¬†al.,2021), Global-MMLU(Singh et¬†al.,2024a), and Belebele(Bandarkar et¬†al.,2024).",
                "position": 183
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14395/x2.png",
                "caption": "Figure 2:Overview of evaluation tasks. Two instances of the same LLM engage in self-communication in the target language to complete information-gap tasks: (1) Easy Twenty Questions‚Äîguessing a hidden word, (2) MCQ Conversation‚Äîfinding the answer through passage-based dialogue, and (3) Code Reconstruction‚Äîexplaining and reconstructing codes.",
                "position": 278
            }
        ]
    },
    {
        "header": "3MuG-Eval: A Language-Agnostic Evaluation Framework",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14395/x3.png",
                "caption": "Figure 3:Accuracy of 8 LLMs across three tasks in 30 languages. Languages are grouped by resource level and sorted by average performance within each group. Results show that Code Reconstruction is the easiest task, followed by MCQ Conversation and Easy Twenty Questions. The gap is minor between high and mid-resource languages, but substantial between mid and low. Larger models consistently outperform smaller ones within the same language family, and tasks exhibits distinct ceiling effect.",
                "position": 357
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14395/x4.png",
                "caption": "Figure 4:Score distributions across six evaluation tasks, demonstrating varying discriminative powers.\nNotably, MCQ Conversation, derived from the Belebele task, exhibits greater statistical dispersion, indicating greater ability to distinguish between models than the original Belebele benchmark.",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2505.14395/x5.png",
                "caption": "Figure 5:Correlation analysis betweenMuG-Eval tasks and existing multilingual benchmarks. Heatmaps show Pearson‚Äôsrùëüritalic_r(left) and Spearman‚ÄôsœÅùúå\\rhoitalic_œÅ(right) correlation coefficients between threeMuG-Eval tasks and three established benchmarks. All correlations exceed 0.75, demonstrating strong consistency betweenMuG-Eval and existing evaluation methods, validating its effectiveness as a multilingual evaluation framework.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2505.14395/x6.png",
                "caption": "Figure 6:MCQ Conversation accuracy comparison across 30 languages for GPT-4o using passages in: (1) the target language, (2) English, and (3) five fixed high-resource languages (averaged), and (4)an optimized subset of up to five high-resource languages most similar to the target language. Results demonstrate that high-resource language substitution more closely approximates native language performance than using English alone, especially for low-resource languages.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2505.14395/x7.png",
                "caption": "Figure 7:Attribution of errors by conversational role. Bars show the percentage of failures caused by Questioner (green), Answerer (blue), or Both roles (purple).",
                "position": 691
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AData Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14395/x8.png",
                "caption": "Table 4:Example target words used in the Easy Twenty Questions task. Words were sourced from theThingsdataset and translated into 30 languages via Google Translate.",
                "position": 1513
            }
        ]
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix CDetailed Experiment Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14395/x9.png",
                "caption": "Figure 8:MCQ Conversation accuracy comparison across 30 languages for GPT-4o-mini, using passages in:\n(1) the target language,\n(2) English,\n(3) a fixed set of five high-resource languages (averaged), and\n(4) a selection of up to five high-resource languages most similar to the target language, with scores averaged.",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2505.14395/x10.png",
                "caption": "Figure 9:Correlation matrix showing relationships betweenMuG-Eval tasks and existing multilingual benchmarks. Each cell displays Pearson‚Äôs correlation coefficient (rùëüritalic_r) with 95% confidence intervals, with points colored by language resource level.",
                "position": 2142
            }
        ]
    },
    {
        "header": "Appendix DGeneration Statistics",
        "images": []
    }
]