[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16746/x1.png",
                "caption": "(a)",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2601.16746/x1.png",
                "caption": "(a)",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2601.16746/x2.png",
                "caption": "(b)",
                "position": 149
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16746/x3.png",
                "caption": "Figure 2:Token cost distribution over different tool calls for Mini-SWE-Agent on SWE-Bench Verified with Claude Sonnet 4.5. Read operations dominate token consumption at 76.1%, motivating the need for context pruning mechanisms.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2601.16746/x4.png",
                "caption": "Figure 3:Overview of SWE-Pruner. Left: TheInteraction Workflowdemonstrates how SWE-Pruner functions as a middleware between the Coding Agent and the Environment. It intercepts the Raw Context from file operations and delivers a Pruned Context to the agent. Right: ThePruning Pipelinedetails the internal mechanism. Based on a specific goal hint from the coding agent, theneural skimmerprocesses the raw context through line-level scoring and adaptive selection to deliver the pruned context.",
                "position": 204
            }
        ]
    },
    {
        "header": "3Approach",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16746/x5.png",
                "caption": "Figure 4:First token latency comparison across different sequence lengths. SWE-Pruner maintains consistently low latency below 100 ms.",
                "position": 746
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEmpirical Results on GLM Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16746/x6.png",
                "caption": "Figure 5:Token cost distributionover different tool calls for Mini-SWE-Agent with GLM-4.6 on SWE-Bench Verified. Read operations dominate token consumption at 67.5%, further validating the necessity of context pruning mechanisms across different backbone models.",
                "position": 791
            }
        ]
    },
    {
        "header": "Appendix BModel Architecture and Inference Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16746/figures/model-arch.png",
                "caption": "Figure 6:SWE-Pruner Model Architecture.The model consists of a lightweight reranker backbone with multi-layer feature fusion, followed by dual heads for pruning and reranking. The pruning head employs a CRF layer to model structured retention decisions, while the reranking head produces document-level relevance scores.",
                "position": 801
            }
        ]
    },
    {
        "header": "Appendix CTraining Dataset for the Neural Skimmer",
        "images": []
    },
    {
        "header": "Appendix DExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16746/x7.png",
                "caption": "(a)",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2601.16746/x7.png",
                "caption": "(a)",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2601.16746/x8.png",
                "caption": "(b)",
                "position": 1065
            }
        ]
    },
    {
        "header": "Appendix EAgent Rounds and Token Consumption Analysis",
        "images": []
    },
    {
        "header": "Appendix FDetailed Efficiency Analysis",
        "images": []
    },
    {
        "header": "Appendix GSingle-Turn Tasks with SeedCoder",
        "images": []
    },
    {
        "header": "Appendix HSyntactic Structure Preservation Analysis",
        "images": []
    },
    {
        "header": "Appendix ICase Study on SWE Bench",
        "images": []
    },
    {
        "header": "Appendix JPrompt Templates",
        "images": []
    }
]