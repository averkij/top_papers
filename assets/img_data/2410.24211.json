[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24211/x1.png",
                "caption": "Figure 1:DELTAis a dense 3D tracking approach that (a) trackseverypixel from a monocular video, (b) provides consistent trajectories in 3D space, and (c) achieves state-of-the-art accuracy on 3D tracking benchmarks while being significantly faster than previous approaches in the dense setting.",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24211/x2.png",
                "caption": "Figure 2:Overview of DELTA.DELTA takes RGB-D videos as input and achieves efficient dense 3D tracking using a coarse-to-fine strategy, beginning with coarse tracking through a spatio-temporal attention mechanism at reduced resolution (Sec.3.1,3.2), followed by an attention-based upsampler for high-resolution predictions (Sec.3.3).",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2410.24211/x3.png",
                "caption": "Figure 3:Spatial attention architectures.Top: Illustration of different spatial attention architectures. Compared to prior methods, our proposed architecture ③ incorporates both global and local spatial attention and can be efficiently learned using apatch-by-patchstrategy.Bottom: Long-term optical flows predicted with different spatial attention designs. We find that both global and local attention are crucial for improving tracking accuracy, as highlighted by theredcircles. Additionally, our computationally efficient global attention design usinganchor tracks(i.e., ③ W/o Local Attn) achieves similar accuracy to the more computationally-intensive Cotracker version ②.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2410.24211/x4.png",
                "caption": "Figure 4:Attention-based upsample module.Left: We apply multiple blocks of local cross-attention to learn the upsampling weights for each pixel in the fine resolution.Right: Theredcircles highlight regions in the long-term flow maps where our attention-based upsampler produces more accurate predictions compared to RAFT’s convolution-based upsampler.",
                "position": 339
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24211/x5.png",
                "caption": "Figure 5:Qualitative results of dense 3D tracking on in-the-wild videosbetween CoTracker+++UniDepth, SceneTracker, SpatialTracker and our method. We densely track every pixel from the first frame of the video in 3D space, the moving objects are highlighted asrainbowcolor. Our method accurately tracks the motion of foreground objects while maintaining stable backgrounds.",
                "position": 1107
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24211/x6.png",
                "caption": "Figure 6:Comparison of long-range optical flow predictions: We predict optical flows from the first frame to subsequent frames of the video. DOT(Le Moing et al.,2024), which lacks strong temporal correlation, suffers from a noticeable ”flickering” effect (greencircle), particularly at the boundaries between foreground and background objects. In contrast, our method ensures a smooth and consistent transition over time, effectively reducing artifacts at object boundaries.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2410.24211/x7.png",
                "caption": "Figure 7:Comparison of long-range optical flow predictions on in-the-wild videosbetween DOT, SEA-RAFT, and our approach.",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2410.24211/x8.png",
                "caption": "Figure 8:Qualitative results of jointly depth and pose estimationon in-the-wild videos (first two rows) and AI-generated video (last row) of our approach.",
                "position": 2546
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]