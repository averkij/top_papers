[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x1.png",
                "caption": "Figure 2:Overview ofMotionCanvas. Given an input image and high-level scene-space motion intent, MotionCanvas decomposes and translates the motion (camera and object motion with their timing) into screen space by leveraging the depth-based synthesis and hierarchical transformation with the Motion Signal Translation module. These screen-space motion signals are subsequently passed to a video generation model to produce the final cinematic shots.",
                "position": 181
            }
        ]
    },
    {
        "header": "3MotionCanvas",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x2.png",
                "caption": "Figure 3:Illustration of our motion-conditioned video generation model. The input image and bbox color frames are tokenized via a 3D-VAE encoder and then summed. The resultant tokens are concatenated with other conditional tokens, and fed into the DiT-based video generation model.",
                "position": 303
            }
        ]
    },
    {
        "header": "4Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x3.png",
                "caption": "Figure 7:Results when our method is applied for: (upper) motion transfer, and (bottom) video editing for changing objects, adding and removing objects.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2502.04299/x4.png",
                "caption": "Figure 9:Visual comparison of the resulatant videos from DragAnything, MOFA-Video, TrackDiffusion, Ourscoordcoord{}_{\\text{coord}}start_FLOATSUBSCRIPT coord end_FLOATSUBSCRIPT, and our MotionCanvas.",
                "position": 440
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x5.png",
                "caption": "Figure 12:A sample of the designed user interface for our MotionCanvas.",
                "position": 1524
            }
        ]
    },
    {
        "header": "Appendix BUser Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x6.png",
                "caption": "Figure 13:Illustration of camera-aware transformation and camera-object-aware transformation. In preview videos, dash-line bounding boxes represent the scene-space inputs, while the solid ones with the same color denote the transformed screen-space motion signals. Similarly, point trajectories with white trace indicate scene-space user motion design, while colored ones represent transformed signals. Better investigation in supplementary videos.",
                "position": 1616
            }
        ]
    },
    {
        "header": "Appendix CEssentiality of Camera-aware and Camera-object-aware Transformations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x7.png",
                "caption": "Figure 14:Illustration of the recomputation process for input motion conditions in our MotionCanvasARAR{}_{\\text{AR}}start_FLOATSUBSCRIPT AR end_FLOATSUBSCRIPTduring inference.",
                "position": 1640
            }
        ]
    },
    {
        "header": "Appendix DMore Details of MotionCanvasARAR{}_{\\text{AR}}start_FLOATSUBSCRIPT AR end_FLOATSUBSCRIPT",
        "images": []
    },
    {
        "header": "Appendix EUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04299/x8.png",
                "caption": "Figure 15:Legend of camera motions used in the main paper.",
                "position": 1664
            },
            {
                "img": "https://arxiv.org/html/2502.04299/x9.png",
                "caption": "Figure 16:Designed user study interface. Each participant is required to evaluate 15 video comparisons and respond to three corresponding sub-questions for each comparison. Only one video is shown here due to the page limit.",
                "position": 1667
            }
        ]
    },
    {
        "header": "Appendix FAdditional Analysis",
        "images": []
    },
    {
        "header": "Appendix GLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix HCamera Motion Legend",
        "images": []
    }
]