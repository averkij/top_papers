[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/x1.png",
                "caption": "Figure 1:Illustration of the pipeline for data construction. Specific prompts used for CoT annotation and the resulting data samples are presented in AppendixB.",
                "position": 166
            }
        ]
    },
    {
        "header": "3UME-R1",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/x2.png",
                "caption": "Figure 2:Overview of UME-R1. UME-R1 introduces a two-stage training framework for generative multimodal embedding. (a) Supervised fine-tuning uses query-target pairs with reasoning annotations to train the MLLM, enabling it to generate both discriminative and generative embeddings as well as to possess basic reasoning abilities. (b) RLVR continues to fine-tune the model using regular query-target pairs, encouraging it to generate reasoning trajectories that lead to more beneficial generative embeddings.",
                "position": 216
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/figures/passk.png",
                "caption": "Table 1:Comparison of performance between baselines and UME-R1 on MMEB-V2.CLS: classification,QA: question answering,RET: retrieval,GD: grounding,MRET: moment retrieval,VDR: ViDoRe,VR: VisRAG,OOD: out-of-domain.Oracledenotes the case where the best result between generative and discriminative embeddings is picked. Detailed results can be found in AppendixC.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2511.00405/figures/passk.png",
                "caption": "Figure 3:pass@kkcurves of UME-2B and UME-7B across multiple datasets.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2511.00405/figures/gen_assist.png",
                "caption": "Figure 4:Comparison between DUME, DUME+Gen, and UME-R1. DUME+Gen denotes the approach in which an external model first generates reasoning and summaries, followed by DUME to obtain the corresponding embeddings.",
                "position": 893
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUse of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BExample of Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/x3.png",
                "caption": "Figure 5:Example from the constructed cold-start dataset (Case 1). The orange part represents the original data, the blue part denotes the added prompt, the black part indicates the reasoning content, and the green part shows the summary.orangesegments correspond to the original data,bluesegments represent the added prompts,blacksegments capture the reasoning process,\nandgreensegments provide the summaries.",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x4.png",
                "caption": "Figure 6:Example from the constructed cold-start dataset (Case 2).",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x5.png",
                "caption": "Figure 7:Example from the constructed cold-start dataset (Case 3).",
                "position": 1761
            }
        ]
    },
    {
        "header": "Appendix CDetailed Scores of MMEB-V2",
        "images": []
    },
    {
        "header": "Appendix DMMEB-V1 Benchmark Scores",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/x6.png",
                "caption": "Table 5:Results on the MMEB-V1 benchmark, which comprises a total of 36 image embedding tasks. IND represents the in-distribution dataset, and OOD represents the out-of-distribution dataset. In UniIR, the FF and SF subscripts under CLIP or BLIP represent feature-level fusion and score-level fusion, respectively. CAFe-V1 indicates that the model is trained solely on the MMEB-V1 training data (contains only image data), whereas CAFe-V2 denotes that the model is trained on the MMEB-V2 training data. The best results are marked in bold, and the second-best results are underlined.",
                "position": 2942
            }
        ]
    },
    {
        "header": "Appendix EComparative Examples of Generative and Discriminative Embeddings",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/x6.png",
                "caption": "Figure 8:A comparison of generative and discriminative embeddings is shown (Case 1).Greenhighlights denote content that positively impacts retrieval performance.",
                "position": 3260
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x7.png",
                "caption": "Figure 9:A comparison of generative and discriminative embeddings is shown (Case 2).",
                "position": 3263
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x8.png",
                "caption": "Figure 10:A comparison of generative and discriminative embeddings is shown (Case 3).",
                "position": 3266
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x9.png",
                "caption": "Figure 11:A comparison of generative and discriminative embeddings is shown (Case 4).",
                "position": 3269
            }
        ]
    },
    {
        "header": "Appendix FExample of Repeated Sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00405/x10.png",
                "caption": "Figure 12:An example showing how repeated sampling leads to variations in model-generated reasoning and summaries, resulting in different retrieval outcomes (Case 1). Thegreensegments indicate correct reasoning or summaries, while theredsegments highlight incorrect ones.",
                "position": 3279
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x11.png",
                "caption": "Figure 13:An example showing how repeated sampling leads to variations in model-generated reasoning and summaries, resulting in different retrieval outcomes (Case 2).",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2511.00405/x12.png",
                "caption": "Figure 14:An example showing how repeated sampling leads to variations in model-generated reasoning and summaries, resulting in different retrieval outcomes (Case 3).",
                "position": 3285
            },
            {
                "img": "https://arxiv.org/html/2511.00405/figures/reward_length_2B.png",
                "caption": "Figure 15:Evolution of reward and generated completion length of UME-R1-2B during training.",
                "position": 3295
            },
            {
                "img": "https://arxiv.org/html/2511.00405/figures/reward_length_7B.png",
                "caption": "Figure 16:Evolution of reward and generated completion length of UME-R1-7B during training.",
                "position": 3298
            }
        ]
    },
    {
        "header": "Appendix GReward and Completion Length Visualization",
        "images": []
    }
]