[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05251/figures/icons/github-mark.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2510.05251/figures/icons/world.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Sequential Exploration: Explore Early, Exploit Late",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05251/x1.png",
                "caption": "Figure 1:Average entropy shrinks with output positions for Llama-3-8B-Instruct on MMLU dataset.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x2.png",
                "caption": "Figure 2:A “forking” experiment on DeepSeek-Llama-3 shows early branching (high-entropy region) yields higher Maj@kkon MMLU than late branching (low-entropy region).",
                "position": 324
            }
        ]
    },
    {
        "header": "4A Method for Sequential Exploration",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05251/x3.png",
                "caption": "Figure 3:The annealing schedule with different decay ratesdd. A largerddslows the cooling, front-loading exploration over more tokens. We setc=10,τmax=1.2,τmin=0.1c=10,\\tau_{\\mathrm{max}}=1.2,\\tau_{\\mathrm{min}}=0.1for illustration.",
                "position": 355
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05251/x4.png",
                "caption": "Figure 4:Pass@16 and Worst@16 performance evaluation in RL training.\nWhile EAD improves exploration of high-quality samples (even the worst outperform temperature sampling), the gain diminishes over time; importance sampling can supplement to correct bias and sustain training.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x5.png",
                "caption": "Figure 5:Pass@16 performance on Qwen-2.5-Math-7B.\nEAD enables better exploration than fixed-temperature sampling, yielding sustained gains in Pass@16 throughout training.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x6.png",
                "caption": "Figure 6:Entropy Dynamics in RL Training. Under commonly-used temperature sampling, trained with RL algorithm would make entropy decrease, sharply shrinking the exploration space for RL from beginning. While EAD could help RL algorithm to escape local minimum and do exploration when needed in the middle of RL training.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x7.png",
                "caption": "Figure 7:EAD would bring further performance improvement via increased numbers of rollouts, but the commonly used44or88is already good enough.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x8.png",
                "caption": "Figure 8:When scaling out the rollout number to 16, the relative advantages of our methods diminished; however, it still outperforms traditional same-temperature sampling.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x9.png",
                "caption": "Figure 9:Inference-Time Scaling Evaluation for Different Decoding Methods using off-the-shelf Qwen2.5 models. We could see that EAD improves traditional temperature sampling. We setτmax=1.2,τmin=0.1,d=25\\tau_{\\text{max}}=1.2,\\tau_{\\text{min}}=0.1,d=25for EAD.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x10.png",
                "caption": "Figure 10:EAD is compatible with various RL algorithms and can significantly improve the model performance over time.",
                "position": 499
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMinimal-RL Training Details",
        "images": []
    },
    {
        "header": "Appendix BOff-policy Issue and Truncated Importance Sampling Correction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05251/x11.png",
                "caption": "(a)Clip Fraction Surge",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x11.png",
                "caption": "(a)Clip Fraction Surge",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x12.png",
                "caption": "(b)Gradient Norm Surge.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x13.png",
                "caption": "(c)Drastic Best@16 Drop",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2510.05251/x14.png",
                "caption": "",
                "position": 1724
            }
        ]
    },
    {
        "header": "Appendix CProof of Temperature-Entropy Relationship",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05251/x15.png",
                "caption": "Figure 12:Compared with normal temperature sampling, EAD can naturally incentivize the model to generate longer reasoning chains.",
                "position": 1997
            }
        ]
    },
    {
        "header": "Appendix DIncreasing Length in RL Training",
        "images": []
    }
]