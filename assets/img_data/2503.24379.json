[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/extracted/6318647/fig/logo.png",
                "caption": "",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x1.png",
                "caption": "",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x2.png",
                "caption": "Figure 2:The pipeline for constructing theAny2CapInsdataset involves three key steps: 1) data collection, 2) structured video caption generation, and 3) user-centric short prompt generation.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Any2CapIns Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x3.png",
                "caption": "Figure 3:Distribution of the short/structured caption length(in words) inAny2CapIns.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Any2Caption Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x4.png",
                "caption": "Figure 4:Architecture illustration ofAny2Caption(a), where Qwen2-LLM serves as the backbone and is paired with text, image, video, motion, and camera encoders to produce structured captions.\nAfter alignment learning, we perform a progressive mixed training strategy (b), where additional vision/text instruction datasets are progressively added for joint training, and meanwhile, for input short caption, we adopt a random-dropout mechanism at the sentence level to enhance robustness.",
                "position": 378
            }
        ]
    },
    {
        "header": "5Evaluation Suite",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x5.png",
                "caption": "Figure 5:Illustrations ofgenerated videoswhere only the structured captions yielded byAny2Captionare fed into theCogVideoX-2B(Left), andHunyuanVideo(Right).\nWe can observe that some key features of the input identity images, such as the background and main object, can be accurately visualized in the generated videos.",
                "position": 649
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x6.png",
                "caption": "Figure 6:Quantitative results on unseen conditions (i.e.,segmentation[58],style[68],masked image[58], andsketch[58]) when using short and structured captions, respectively.",
                "position": 1139
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Overview",
        "images": []
    },
    {
        "header": "Appendix ALimitation",
        "images": []
    },
    {
        "header": "Appendix BEthic Statement",
        "images": []
    },
    {
        "header": "Appendix CExtended Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x7.png",
                "caption": "Figure 7:Illustrations of constructed short and structured captions under the camera-to-video generation.",
                "position": 2319
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x8.png",
                "caption": "Figure 8:Illustrations of constructed short and structured captions under the multiIDs-to-video generation.",
                "position": 2324
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x9.png",
                "caption": "Figure 9:Word cloud of different structured captions inAny2CapInsdataset, showing the diversity.",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x10.png",
                "caption": "Figure 10:QA pairs proportion in structured captions.",
                "position": 2491
            }
        ]
    },
    {
        "header": "Appendix DExtended Dataset Details",
        "images": []
    },
    {
        "header": "Appendix EMore Statistics Information of IRScore",
        "images": []
    },
    {
        "header": "Appendix FDetailed Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24379/x11.png",
                "caption": "Figure 11:Illustrations of input short caption, predicted structured caption, and short caption combined with condition captions. The condition captions may introduce noisy information due to the focus distraction of the background of input identities, for example, theBatman,Superman,film crew.",
                "position": 2759
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x12.png",
                "caption": "Figure 12:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects for the video generation.",
                "position": 2764
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x13.png",
                "caption": "Figure 13:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation.",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x14.png",
                "caption": "Figure 14:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation.",
                "position": 2834
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x15.png",
                "caption": "Figure 15:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation.",
                "position": 2839
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x16.png",
                "caption": "Figure 16:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation.",
                "position": 2844
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x17.png",
                "caption": "Figure 17:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation.",
                "position": 2849
            },
            {
                "img": "https://arxiv.org/html/2503.24379/x18.png",
                "caption": "Figure 18:Illustrations of predicted structured captions based on the input multiple identities and the short instruction that expresses the implicit objects and the action for the target video generation.",
                "position": 2854
            }
        ]
    },
    {
        "header": "Appendix GExtended Experiment Results and Analyses",
        "images": []
    }
]