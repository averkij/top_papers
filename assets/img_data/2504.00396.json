[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00396/x1.png",
                "caption": "Figure 1:Visualization of the Attention Map.The salient regions directly reflect response intensity to the target text ”a hat”. Brighter regions indicate higher attention.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2504.00396/x2.png",
                "caption": "Figure 2:The Dual-Path Contrastive Learning Pipeline of SPF-Portrait.The text inblueis theBase text, while those inredis theTarget text. Reference Path takes onlyBase textas input, while Response Path takes complete text (Base text&Target text) as input.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2504.00396/x3.png",
                "caption": "Figure 3:Analysis of Alignment Process. (a)Vanilla aligning results in the over-alignment with original portrait.(b)For the same customization goal, reference image fine-tuning offers a more distinct target response region than T2I fine-tuning.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2504.00396/x4.png",
                "caption": "Figure 4:Comparison with Traditional Supervision on Image Fidelity.(a)illustrates the trend of Image-Reward (IR) and CLIP Score (CLIP-T) across fine-tuning steps. Image-Reward[66]is a metric used to evaluate image fidelity.(b)displays samples from traditional method[2]and ours.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2504.00396/x5.png",
                "caption": "Figure 5:Qualitative Comparisons with SOTA methods.We compare ours with naive fine-tuning[55], PEFT-based methods (LoRA[26], AdaLoRA[71]) and the decoupled methods (Tokencompose[64], Magenet[74]). Please zoom in for more details.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2504.00396/x6.png",
                "caption": "Figure 6:Qualitative Ablation Study.We independently ablate the proposed loss and the SFCM mechanism.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2504.00396/x7.png",
                "caption": "Figure 7:User Study Results.The percentages indicate the proportion of users who select the corresponding method.",
                "position": 596
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00396/x8.png",
                "caption": "Figure 8:Reconstruction Results.The three portraits for each case are only generated by the sameBase text.",
                "position": 664
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]