[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07345/x1.png",
                "caption": "Figure 1:Visualization of the Optimization Landscape inForbidden Zones.(a)The Real Teacher‚Äôs energy potential becomes undefined or poorly calibrated far from the data manifold, exerting misleading attractive forces.(b)The Fake Teacher‚Äôs landscape exhibits a shallow slope, resulting in a weak repulsive force that is insufficient to propel the student out of theForbidden Zone.",
                "position": 221
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07345/x2.png",
                "caption": "Figure 2:Visualizing the Taxonomy of Forbidden Zone Mitigation Strategies.(a)Standard Failure: Useful gradients exist only near modes, leaving a ‚Äúgradient vacuum‚Äù inùíµf\\mathcal{Z}_{f}.(b)External Force: An auxiliary forceùêÖext\\mathbf{F}_{\\text{ext}}(green) provides global steering to bridge the gap.(c)Noise Reset: High noise induces distribution overlap, restoring gradient coverage.(d)Real Teacher Adaptation: The teacher manifold actively shifts towards the student to eliminateùíµf\\mathcal{Z}_{f}.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x3.png",
                "caption": "Figure 3:Overview of AMD.The framework operates in three stages:(Left)Group Generation & Re-noising: For each prompt, the studentGŒ∏G_{\\theta}produces a group of samples{xi}i=1K\\{x_{i}\\}_{i=1}^{K}, which are subsequently perturbed by the forward operator‚Ñ±t\\mathcal{F}_{t}.(Middle)Reward-aware Diagnosis: A reward modelR‚Äã(‚ãÖ)R(\\cdot)serves as a proxy to pinpoint samples (e.g.,xKx_{K}) trapped in theForbidden Zone(ùíµf\\mathcal{Z}_{f}), where the real teacher‚Äôs scoresreals_{\\text{real}}becomes ill-posed.(Right)Dynamic Score Adaption: Through the adaptive operator‚ÑãA‚ÄãM‚ÄãD\\mathcal{H}_{AMD}, AMD rectifies the combination ofùêùr‚Äãe‚Äãa‚Äãl\\mathbf{d}_{real}andùêùf‚Äãa‚Äãk‚Äãe\\mathbf{d}_{fake}to derive an optimized gradient direction, thereby facilitating a rapid escape from theForbidden Zoneand enabling the model to surpass the teacher under reward guidance.",
                "position": 501
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07345/x4.png",
                "caption": "Figure 4:Qualitative comparison on text-to-video generation. Compared to standard DMD (e.g., LongLive), AMD demonstrates superior visual fidelity and more coherent motion realism. Additional qualitative results are provided in Fig.15.",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x5.png",
                "caption": "Figure 5:Component-wise ablation study on 50K-ImageNet (256√ó\\times256). (base model: SiT-XL/2). We investigate the contribution of each component in AMD. Dynamic Adapt denotes the Dynamic Score Adaptation (Section3.2), and Repulsive Sharpen denotes the Repulsive Landscape Sharpening (Section3.3).",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x6.png",
                "caption": "Figure 6:Reward‚ÄìQuality Co-evolution during Distillation. AMD exhibits a synchronized increase in generation quality (IS) and reward scores, suggesting that reward-aware dynamic score adaptation effectively guides the student out of low-quality regions.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x7.png",
                "caption": "Figure 7:Reward-Driven Shaping.Top:Under a selective reward (b), the Student (d) ignores the fixed Teacher‚Äôs (a) gradients in suppressed zones (√ó\\times) to strictly match high-value modes (‚ãÜ\\star).Bottom:Global alignment recovers the full distribution.",
                "position": 1150
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix CProof",
        "images": []
    },
    {
        "header": "Appendix DLimitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix EAlgorithm",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07345/x8.png",
                "caption": "Figure 8:We track the evolution of the student distribution on a 2D toy dataset.Top:Naive Adaptation (‚Ñãnaive\\mathcal{H}_{\\text{naive}}) suffers from mode merging and eventual distribution collapse as it fails to balance the competing distillation forces.Bottom:AMD (‚ÑãAMD\\mathcal{H}_{\\text{AMD}}) successfully maneuvers the student toward high-reward modes (lower-left) while preserving sharp, distinct distributional fidelity. This highlights the criticality of decoupled signal modulation in preventing training instability within theForbidden Zone.",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x9.png",
                "caption": "Figure 9:The winning rate of AMD over DMD2 on SDXL across DrawBench and HPDv2. The standard DMD2¬†(baseline) winning rate defaults to 50%. The results reveal the superiority of AMD in synthesizing images with good quality, comparing with DMD2.",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x10.png",
                "caption": "Figure 10:Synthesized images of ADM distilled from SDXL on DrawBench.",
                "position": 2829
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x11.png",
                "caption": "Figure 11:Synthesized images of ADM distilled from SDXL on concept-art subset of HPD v2.",
                "position": 2832
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x12.png",
                "caption": "Figure 12:Synthesized images of ADM distilled from SDXL on painting subset of HPD v2.",
                "position": 2835
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x13.png",
                "caption": "Figure 13:Synthesized images of ADM distilled from SDXL on photo subset of HPD v2.",
                "position": 2838
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x14.png",
                "caption": "Figure 14:Synthesized images of ADM distilled from SDXL on anime subset of HPD v2.",
                "position": 2841
            },
            {
                "img": "https://arxiv.org/html/2602.07345/x15.png",
                "caption": "Figure 15:Qualitative comparison on text-to-video generation. We show that AMD significantly outperforms the baseline, exhibiting superior motion smoothness, higher visual fidelity, and better prompt alignment.",
                "position": 2853
            }
        ]
    },
    {
        "header": "Appendix FSupplementary Experimental Results",
        "images": []
    }
]