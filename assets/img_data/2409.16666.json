[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/teaser.jpg",
                "caption": "",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/overview_new.png",
                "caption": "Figure 2:Overview of TalkinNeRF. Given a monocular video of a subject, we learn a unified NeRF-based network that represents their holistic 4D motion. Corresponding modules for body, face, and hands are combined together, in order to synthesize the finalfull-body talking human. By learning an identity code per video, our method can be trained on multiple identities simultaneously.",
                "position": 170
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/arms.jpg",
                "caption": "(a)Ablation study on the segmentation classes",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/arms.jpg",
                "caption": "(a)Ablation study on the segmentation classes",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/hands.png",
                "caption": "(b)Ablation study on the hand representation",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/rec1.png",
                "caption": "Figure 4:Qualitative comparison for rendering novel poses from the same identity.We compare with HumanNeRF[64]and MonoHuman[68]. Ground truth (not seen in training) is shown on the left. Our method generates facial expressions and hand articulation with a high fidelity.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/novel_expose.png",
                "caption": "Figure 5:Qualitative comparison for rendering novel poses from a different identity.From left to right: target pose, results of HumanNeRF[64], MonoHuman[68], our single-identity model, and our multi-identity model. Our multi-identity TalkinNeRF robustly renders each identity under unseen poses and expressions.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/novel_talkshow_1.jpg",
                "caption": "Figure 6:Qualitative comparison for rendering unseen out-of-distribution poses. From left to right: target pose, results of HumanNeRF[64], MonoHuman[68], our single-identity, and our multi-identity model. Our multi-identity TalkinNeRF robustly renders each identity under completely unseen poses and expressions.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/few.png",
                "caption": "(a)",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/few.png",
                "caption": "(a)",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/fig/barplot_aggelina_1.png",
                "caption": "(b)",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/suppl_novel_zju.png",
                "caption": "Figure 8:Learning a novel identity.From left to right: target pose, results of HumanNeRF[64], SHERF[25], and our multi-identity model. Our method robustly renders the new identity under completely unseen poses.",
                "position": 631
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary",
        "images": []
    },
    {
        "header": "AAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/suppl_novel1.png",
                "caption": "Figure 9:Qualitative comparison for rendering novel (unseen) poses.From left to right: target pose, results of HumanNeRF[64], MonoHuman[68], our single-identity model, and our multi-identity model. Our multi-identity TalkinNeRF robustly renders each identity under unseen poses and expressions.",
                "position": 1202
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/sherf.jpg",
                "caption": "Figure 10:Qualitative comparison for learning a novel identity.From left to right: ground truth, results of HumanNeRF[64], SHERF[25], and our multi-identity model. Our method robustly renders the new identity under completely unseen poses, given only a very short video of 10 seconds.",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/smplpix.jpg",
                "caption": "Figure 11:Qualitative comparison with SMPLpix[53]. Our method captures fine-grained facial expression and hand articulation.",
                "position": 1208
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/scarf.jpg",
                "caption": "Figure 12:Qualitative comparison with SCARF[14]. Our method synthesizes high visual quality, whereas SCARF leads to blurry faces.",
                "position": 1211
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/suppl_rec2.png",
                "caption": "Figure 13:Qualitative comparison for rendering novel poses from the same identity.We compare with HumanNeRF[64]and MonoHuman[68]. Ground truth (not seen in training) is shown on the left. Our method generates facial expressions and hand articulation with a high fidelity.",
                "position": 1214
            },
            {
                "img": "https://arxiv.org/html/2409.16666/extracted/5878174/figs_supp/suppl_rec1_.png",
                "caption": "",
                "position": 1218
            }
        ]
    },
    {
        "header": "BImplementation Details",
        "images": []
    }
]