[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00762/x1.png",
                "caption": "Figure 1:We propose an end-to-end differentiable framework capable of estimating invisible forces directly from video data, mimicking the human ability to perceive unseen physical effects through vision alone. This approach enables applications such as physics-based video generation, where new objects can be seamlessly introduced into a scene and simulated within the same force field. Force strength: fromlowtohigh(best viewed in colors).",
                "position": 64
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00762/x2.png",
                "caption": "Figure 2:We propose a differentiable inverse graphics framework to recover invisible forces from videos by integrating object modeling, physics simulation, and optimization. Objects are represented with 3D Gaussians and assigned physical properties via Vision-Language Models. Forces are modeled as a causal tri-plane, and object motions are simulated using a differentiable physics simulator. A sparse tracking objective enables robust differentiable force recovery from videos.",
                "position": 127
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00762/x3.png",
                "caption": "Figure 3:Comparison of observed data (left in each frame) and re-simulated results (right in each frame) for two different frames in the real-world experiment.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2512.00762/x3.png",
                "caption": "Figure 3:Comparison of observed data (left in each frame) and re-simulated results (right in each frame) for two different frames in the real-world experiment.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2512.00762/x4.png",
                "caption": "Figure 4:Our method estimates invisible force fields from real-world videos, producing physically plausible motion interpretations.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2512.00762/x5.png",
                "caption": "Figure 5:Our method recovers force fields from input videos and enables the insertion of novel objects while maintaining physically plausible motion. We demonstrate new object insertion, and modifications of physical conditions (e.g. mass and external force), showcasing the modelâ€™s ability to generate physically plausible videos.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2512.00762/x6.png",
                "caption": "Figure 6:Our method allows modifying object motion by adjusting external constraints while preserving physical realism. We demonstrate how altering boundary conditions (e.g., fixing parts of an object) influences motion under the same estimated force field. These results highlight the flexibility of our approach for controllable, physics-based video editing.",
                "position": 750
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    }
]