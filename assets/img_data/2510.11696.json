[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11696/figures/performance.png",
                "caption": "Figure 1:Rollout speedup and accuracy of QeRL‚ÄÜ on Qwen2.5-7B-Instruct. QeRL‚ÄÜ achieves faster RL rollout and end-to-end training speeds (batch=8), while delivering performance superior to vanilla LoRA and QLoRA, also comparable to full-parameter RL on mathematical benchmarks.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11696/x1.png",
                "caption": "Figure 2:The illustration of QeRL. (a)RL via LoRA: reducing trainable parameters, but does not alleviate the rollout bottleneck. (b)RL via QLoRA: NF4 quantization with LoRA, but NF4 is slower than LoRA. (c)QeRL: NVFP4 quantization with LoRA, reducing memory and enabling faster RL while matching full-parameter finetuning performance with adaptive quantization noise. AQN dynamically adjusts quantization noise with an exponential scheduler, enhancing exploration.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x2.png",
                "caption": "Figure 3:Advancement of Quantization in RL Exploration. Quantization noise brings higher initialized entropy, which encourages exploration in RL training, accelerating the increase of reward.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11696/x3.png",
                "caption": "Figure 4:Training reward performance. The upper figures illustrate the training rewards under DAPO, while the lower one is GRPO. Although MXFP4 achieves higher scores in the early stages of training, NVFP4 ultimately converges to better final rewards. LoRA rank is set to 32.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x4.png",
                "caption": "Figure 5:Comparison of RL entropy.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x5.png",
                "caption": "Figure 6:Deployment scheme of adaptive quantization noise in LLMs.ùêôn‚Äão‚Äãi‚Äãs‚Äãe\\mathbf{Z}_{noise}is integrated inLayerNorm(e.g., RMSNorm) of each block in LLMs.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11696/x6.png",
                "caption": "Figure 7:Training reward of 7/14B models.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x6.png",
                "caption": "Figure 7:Training reward of 7/14B models.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x7.png",
                "caption": "Figure 8:Ablation of AQN on 3/7B model.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x8.png",
                "caption": "Figure 9:Comparison of noise schedulers.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x8.png",
                "caption": "Figure 9:Comparison of noise schedulers.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x9.png",
                "caption": "Figure 10:Ablation of LoRA rank.",
                "position": 801
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x10.png",
                "caption": "Figure 11:Rollout throughput of 14/32B model. The setting is aligned with Tab.7(batch is 1).",
                "position": 900
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AEthics Statement",
        "images": []
    },
    {
        "header": "Appendix BReproducibility Statement",
        "images": []
    },
    {
        "header": "Appendix CUse of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix DRelated Work",
        "images": []
    },
    {
        "header": "Appendix EExperiment Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix FDeployment of QeRL",
        "images": []
    },
    {
        "header": "Appendix GProof of Noise Sharing",
        "images": []
    },
    {
        "header": "Appendix HAdditional Experiments of Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11696/x11.png",
                "caption": "Figure 12:Training reward of 7B model.",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x11.png",
                "caption": "Figure 12:Training reward of 7B model.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x12.png",
                "caption": "Figure 13:Training reward of 32B model.",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x13.png",
                "caption": "Figure 14:Entropy in RL steps.",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x14.png",
                "caption": "Figure 15:Noise curve of different schedulers.",
                "position": 2223
            }
        ]
    },
    {
        "header": "Appendix IAdditional Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11696/x15.png",
                "caption": "Figure 16:Ablation of learning rate in QeRL (Qwen2.5-7B-Instruct).",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x15.png",
                "caption": "Figure 16:Ablation of learning rate in QeRL (Qwen2.5-7B-Instruct).",
                "position": 2238
            },
            {
                "img": "https://arxiv.org/html/2510.11696/x16.png",
                "caption": "Figure 17:Ablation of learning rate in LoRA (Qwen2.5-7B-Instruct).",
                "position": 2241
            }
        ]
    },
    {
        "header": "Appendix JMore Efficiency Experiments",
        "images": []
    },
    {
        "header": "Appendix KLimitation Analysis",
        "images": []
    }
]