[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15801/x1.png",
                "caption": "",
                "position": 191
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15801/x2.png",
                "caption": "Figure 1:The core distinction between VerifyBench and existing reward benchmarks(Lambert et al.,2024; Liu et al.,2024)is illustrated as follows.Upper panel:Existing reward benchmarks assess the accuracy of a reward system by comparing the ranking of two completions for the same question.Lower panel:In contrast, our proposed VerifyBench evaluates the accuracy of a reward system by determining the correctness of a single completion using a reference answer.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15801/x3.png",
                "caption": "Figure 2:Overview of the benchmark construction process. The upper section outlines the pipeline used to construct VerifyBench, whereas the lower section details the pipeline for VerifyBench-Hard. The components highlighted by black boxes denote the final entries included in the benchmark.",
                "position": 312
            }
        ]
    },
    {
        "header": "3Benchmark Construction",
        "images": []
    },
    {
        "header": "4Evaluation Results",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15801/x4.png",
                "caption": "Figure 3:The performance(%) of RFT across different LLM judges which have various performance on VerifyBench.",
                "position": 1210
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BData Source",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    },
    {
        "header": "Appendix DExperimental Details",
        "images": []
    },
    {
        "header": "Appendix ELLM Usage",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15801/x5.png",
                "caption": "Figure 4:Prompt for answer type classification.",
                "position": 2635
            },
            {
                "img": "https://arxiv.org/html/2505.15801/x6.png",
                "caption": "Figure 5:Prompt for LLM-as-a-judge evaluation.",
                "position": 2638
            },
            {
                "img": "https://arxiv.org/html/2505.15801/x7.png",
                "caption": "Figure 6:Prompt for LLM-as-a-judge evaluation without reference answers.",
                "position": 2641
            },
            {
                "img": "https://arxiv.org/html/2505.15801/x8.png",
                "caption": "Figure 7:A data example from VerifyBench with answer type:Numeric Values.",
                "position": 2644
            },
            {
                "img": "https://arxiv.org/html/2505.15801/x9.png",
                "caption": "Figure 8:A data example from VerifyBench with answer type:Expression.",
                "position": 2647
            },
            {
                "img": "https://arxiv.org/html/2505.15801/x10.png",
                "caption": "Figure 9:A data example from VerifyBench with answer type:Multi-choice.",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2505.15801/x11.png",
                "caption": "Figure 10:A data example from VerifyBench with answer type:String.",
                "position": 2653
            }
        ]
    },
    {
        "header": "Appendix FExamples of VerifyBench",
        "images": []
    }
]