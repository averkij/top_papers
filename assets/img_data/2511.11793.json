[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11793/x1.png",
                "caption": "",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2511.11793/x2.png",
                "caption": "Figure 1:Comparison of MiroThinker with state-of-the-art agents and agentic foundation models.",
                "position": 153
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Agentic Workflow",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11793/x3.png",
                "caption": "Figure 2:Overview of the MiroThinker v1.0 agent architecture. The framework integrates a structured tool interface,i.e.,execution environment, file management, and information retrieval, with a simple recency-aware context management to support interactive scaling. On the right, an agentic trajectory example illustrates the recency-based context retention mechanism, where tool outputs from earlier turns are omitted to maintain context efficiency.",
                "position": 245
            }
        ]
    },
    {
        "header": "4Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11793/x4.png",
                "caption": "Figure 3:Overview of the data construction pipeline. Public datasets from platforms such as HuggingFace and GitHub are filtered and verified, while raw internet data are processed through knowledge graph generation and a data engine. The resulting QA pairs from both sources are then converted into agentic trajectories, forming the complete MiroVerse v1.0 dataset used for training MiroThinker v1.0.",
                "position": 351
            }
        ]
    },
    {
        "header": "5Training Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11793/figs/mirothinker_grpo_reward.png",
                "caption": "(a)Training reward across training steps.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2511.11793/figs/mirothinker_grpo_reward.png",
                "caption": "(a)Training reward across training steps.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2511.11793/figs/mirothinker_grpo.png",
                "caption": "(b)Val acc on GAIA-Text-103 across training steps.",
                "position": 573
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11793/x5.png",
                "caption": "(a)BrowseComp",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2511.11793/x5.png",
                "caption": "(a)BrowseComp",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2511.11793/x6.png",
                "caption": "(b)BrowseComp-ZH",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2511.11793/x7.png",
                "caption": "(c)HLE",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2511.11793/x8.png",
                "caption": "(d)GAIA",
                "position": 1015
            }
        ]
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "A â€ƒContributions",
        "images": []
    }
]