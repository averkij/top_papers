[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04025/figures/demo.jpg",
                "caption": "Figure 1:Comparison of attention mechanisms under identical compute budget.All methods use the same inputQQ,KK, andVVtensors extracted from Wan2.1–1.3B(wan2025wanopenadvancedlargescale)denoising process.\nComputation Pattern (top-left two panels):\nNormalized block-wise FLOPs distribution. The two panels plot query blocks on the horizontal axis and key blocks on the vertical axis. Despite identical FLOPs (20%20\\%full), the proposed Pyramid Sparse Attention (PSA) allows each query block to attend to a much larger portion of KV blocks (70% active regions), whereas Block Sparse Attention (BSA)(dao2022flashattentionfastmemoryefficientexact;zhang2025spargeattentionaccuratetrainingfreesparse;xu2025xattentionblocksparseattention)restricts each query to only a narrow subset of KV blocks (20% active regions), concentrating FLOPs in limited areas.\nAttention Output (bottom row):\nResulting attention visualizations. PSA closely matches the Full Attention baseline with minimal relative error (<3%<3\\%), while BSA shows noticeable distortions due to aggressive pruning.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/ksimilarity.jpg",
                "caption": "Figure 2:Adjacent Key Token Cosine Similarity.High cosine similarity between key tokens (Qwen2.5-VL, Wan2.1-1.3B) motivates hierarchical pooling: nearby visual tokens are highly similar.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04025/figures/PSA.png",
                "caption": "Figure 3:Overview of the Pyramid Sparse Attention (PSA) framework.PSA adaptively allocates attention computation across hierarchical KV representations (green; lighter shades denote coarser levels). The multi-level mask (blue) determines which KV level each query block attends to. As illustrated, the current attention block assigned to level 4 uses the coarsest KV representationKj4K_{j}^{4}andVj4V_{j}^{4}.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/videodemo.jpg",
                "caption": "Figure 4:Qualitative comparison on Wan2.1-1.3B (Text-to-Video, 720p).",
                "position": 474
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details of PSA",
        "images": []
    },
    {
        "header": "Appendix CAdditional Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04025/figures/gallery/prompt001comparison.jpg",
                "caption": "Figure 5:A plane takes off over a city skyline; cut to a graffiti-covered steam train pulling into a station with billowing smoke. Cinematic colors and motion.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/gallery/prompt003comparison.jpg",
                "caption": "Figure 6:A lightning bolt strikes the Eiffel Tower, illuminating its metal frame against swirling dark storm clouds. A low-angle view highlights the tower’s grandeur as the electric flash casts dramatic shadows.",
                "position": 1585
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/gallery/prompt004comparison.jpg",
                "caption": "Figure 7:A vintage school bus with retro decals turns a dusty rural corner at sunset. Warm golden light fills the scene as children inside read and play, and the focused driver steers through the tight bend. Low-angle, nostalgic cinematography with rolling hills in the background.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/gallery/prompt006comparison.jpg",
                "caption": "Figure 8:A rider on a sleek black motorcycle weaves through neon-lit city streets at night, wearing a leather jacket and helmet. Dynamic shots follow their smooth maneuvers through traffic, with vibrant signs and skyscrapers creating an intense urban atmosphere.",
                "position": 1595
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/gallery/prompt007comparison.jpg",
                "caption": "Figure 9:A cheerful hot dog vendor pushes a colorful cart down a sunny pastel street as a smiling woman in a floral dress chats with passersby. Balloons, flowers, and lively vendors create a bright, carefree summer vibe.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2512.04025/figures/gallery/prompt008comparison.jpg",
                "caption": "Figure 10:A puzzled panda student with a calculus book in a warm, dimly lit classroom, surrounded by desks, books, and attentive classmates.",
                "position": 1605
            }
        ]
    },
    {
        "header": "Appendix DAdditional Visual Comparisons",
        "images": []
    }
]