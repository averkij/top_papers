[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15431/x1.png",
                "caption": "",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2504.15431/x2.png",
                "caption": "Figure 1:Trillion-7B significantly advances the Pareto-frontier across all aspects.",
                "position": 161
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15431/x3.png",
                "caption": "Figure 2:Cross-Lingual Document Attention.A multilingual batch (left) is packed so that each sequence contains contiguous spans from at least two languages (e.g. English + Korean). The XLDA mask (centre) keeps full self-attention across language blocks (blue cells) while standard causal mask (right) blocks attention across document boundaries (grey cells).",
                "position": 285
            }
        ]
    },
    {
        "header": "2Cross-lingual Document Attention (XLDA)",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15431/x4.png",
                "caption": "Figure 3:Discrepancy in scaling curves of Llama.The above plots suggest that brute-force scaling (by Llama 2 & 3) results in huge performance gaps between English and Korean, whereas Trillion-7B shows more desirable scaling laws for Korean performance closing the wide gap.",
                "position": 336
            }
        ]
    },
    {
        "header": "3Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15431/x5.png",
                "caption": "Figure 4:Proxy model and emergence point.We trained 1.8B parameter models on approximately 100 billion tokens to serve as proxy models for determining optimal training configurations. This specific configuration, represented by a red star in the figure, identifies the most FLOP-efficient setting at which downstream task improvements become observable.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2504.15431/x6.png",
                "caption": "Figure 5:Average Korean throughput measured on 1,000 selected Korean documents using vLLM.We choose a Korean vocabulary size of 24,552 tokens, surpassing the scaling-law optimal size of around 13,000 tokens, yet still positioned just before the plateau in inference speed gains for Korean. This decision strategically balances theoretical optimality against practical improvements in inference speed.",
                "position": 496
            }
        ]
    },
    {
        "header": "4Post-training",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15431/x7.png",
                "caption": "Figure 6:Tokenizer ablation experiments on Korean vocabulary size with 1.8B model trained on 100 billion tokens. We report KoBEST for Korean and HellaSwag for English.",
                "position": 1033
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": []
    },
    {
        "header": "8Conclusion, Limitations and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContribution",
        "images": []
    },
    {
        "header": "Appendix BBaseline Models",
        "images": []
    },
    {
        "header": "Appendix CFull Evaluation Results and Details",
        "images": []
    }
]