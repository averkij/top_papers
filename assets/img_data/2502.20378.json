[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20378/x1.png",
                "caption": "Figure 1:(a) Given a set of monocular multi-view images and camera poses, our method achieves real-time rendering for dynamic scenes while maintaining high rendering quality. (b) Our method achieves promising rendering quality with faster rendering speed and fewer Gaussians. The radius of the circle is the number of time-variant Gaussians whose attributes need to be queried by MLPs. (c) The bottleneck of the rendering speed for dynamic scenes is the number of Gaussians. The fewer the number of Gaussians, the faster the rendering speed.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20378/x2.png",
                "caption": "Figure 2:The pipeline of our EDGS.1) We first initialize voxelized sparse anchor points from Structure from Motion (SfM) points derived from COLMAP. 2) A time-mask MLP is applied to classify if the anchor belongs to the static area or the deformable area. 3)k𝑘kitalic_kGaussian offsets are initialized for each anchor𝒂𝒂\\bm{a}bold_italic_abelonging to static area. The time-invariant attributes of each Gaussian,i.e., opacity, quaternion, scale, and color are calculated from its feature by corresponding tiny MLPs. 4) Time-variant attributes for anchors from dynamic areas are decoded by a deformable attribute MLP. RBF kernel function is employed to compute the location of each Gaussian at timestept𝑡titalic_tby calculating the similarity between each Gaussian and its belonging anchor point. This pipeline is compact and efficient, featuring only a few tiny MLPs for the attributes of the Gaussians and a single network for deformations. Notably, the position of each anchor remains static and is not subject to updates.",
                "position": 172
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20378/x3.png",
                "caption": "Figure 3:Qualitative comparison on the NeRF-DS dataset(Yan, Li, and Lee2023). Compared with other SOTA methods, our method reconstructs finer details and produces a structured rendering of the moving objects,e.g., the cup on human’s hand.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2502.20378/x4.png",
                "caption": "Figure 4:Qualitative comparison on the HyperNeRF dataset(Park et al.2021b).Our EDGS reconstructs detailed texture and reliable structure compared with other SOTA methods.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2502.20378/x5.png",
                "caption": "Figure 5:Visuazization of the difference map (diff.) and the optical flow with fixed camera views.We synthesis fixed novel view across time for(Yang et al.2023; Wu et al.2023)and ours. The1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTrow is the rendered frames at various time steps. The2n⁢dsuperscript2𝑛𝑑2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTand3r⁢dsuperscript3𝑟𝑑3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTrows are the difference map betweentt⁢hsuperscript𝑡𝑡ℎt^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTframe and the1s⁢tsuperscript1𝑠𝑡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTframe and the optical flow, respectively. The response in the highlighted red area indicates that the static area rendered by deformable GS and 4DGS is jittering. Our method achieves better quality for static and dynamic objects.",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2502.20378/x6.png",
                "caption": "Figure 6:Visualization of anchor features and time mask.We visualize the rendered images, anchor features, and time masks for two deformation scenes. Anchor features are visualized using the UMAP function. In the time-mask visualization, the time masks are predicted by time-mask MLPΦm⁢a⁢s⁢ksubscriptΦ𝑚𝑎𝑠𝑘\\varPhi_{mask}roman_Φ start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT.Greenanchors represent static scenes, whileredanchors indicate deformation scenes.",
                "position": 981
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]