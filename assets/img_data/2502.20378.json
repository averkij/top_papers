[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20378/x1.png",
                "caption": "Figure 1:(a) Given a set of monocular multi-view images and camera poses, our method achieves real-time rendering for dynamic scenes while maintaining high rendering quality. (b) Our method achieves promising rendering quality with faster rendering speed and fewer Gaussians. The radius of the circle is the number of time-variant Gaussians whose attributes need to be queried by MLPs. (c) The bottleneck of the rendering speed for dynamic scenes is the number of Gaussians. The fewer the number of Gaussians, the faster the rendering speed.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20378/x2.png",
                "caption": "Figure 2:The pipeline of our EDGS.1) We first initialize voxelized sparse anchor points from Structure from Motion (SfM) points derived from COLMAP. 2) A time-mask MLP is applied to classify if the anchor belongs to the static area or the deformable area. 3)kğ‘˜kitalic_kGaussian offsets are initialized for each anchorğ’‚ğ’‚\\bm{a}bold_italic_abelonging to static area. The time-invariant attributes of each Gaussian,i.e., opacity, quaternion, scale, and color are calculated from its feature by corresponding tiny MLPs. 4) Time-variant attributes for anchors from dynamic areas are decoded by a deformable attribute MLP. RBF kernel function is employed to compute the location of each Gaussian at timesteptğ‘¡titalic_tby calculating the similarity between each Gaussian and its belonging anchor point. This pipeline is compact and efficient, featuring only a few tiny MLPs for the attributes of the Gaussians and a single network for deformations. Notably, the position of each anchor remains static and is not subject to updates.",
                "position": 172
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20378/x3.png",
                "caption": "Figure 3:Qualitative comparison on the NeRF-DS dataset(Yan, Li, and Lee2023). Compared with other SOTA methods, our method reconstructs finer details and produces a structured rendering of the moving objects,e.g., the cup on humanâ€™s hand.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2502.20378/x4.png",
                "caption": "Figure 4:Qualitative comparison on the HyperNeRF dataset(Park etÂ al.2021b).Our EDGS reconstructs detailed texture and reliable structure compared with other SOTA methods.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2502.20378/x5.png",
                "caption": "Figure 5:Visuazization of the difference map (diff.) and the optical flow with fixed camera views.We synthesis fixed novel view across time for(Yang etÂ al.2023; Wu etÂ al.2023)and ours. The1sâ¢tsuperscript1ğ‘ ğ‘¡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTrow is the rendered frames at various time steps. The2nâ¢dsuperscript2ğ‘›ğ‘‘2^{nd}2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPTand3râ¢dsuperscript3ğ‘Ÿğ‘‘3^{rd}3 start_POSTSUPERSCRIPT italic_r italic_d end_POSTSUPERSCRIPTrows are the difference map betweenttâ¢hsuperscriptğ‘¡ğ‘¡â„t^{th}italic_t start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTframe and the1sâ¢tsuperscript1ğ‘ ğ‘¡1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPTframe and the optical flow, respectively. The response in the highlighted red area indicates that the static area rendered by deformable GS and 4DGS is jittering. Our method achieves better quality for static and dynamic objects.",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2502.20378/x6.png",
                "caption": "Figure 6:Visualization of anchor features and time mask.We visualize the rendered images, anchor features, and time masks for two deformation scenes. Anchor features are visualized using the UMAP function. In the time-mask visualization, the time masks are predicted by time-mask MLPÎ¦mâ¢aâ¢sâ¢ksubscriptÎ¦ğ‘šğ‘ğ‘ ğ‘˜\\varPhi_{mask}roman_Î¦ start_POSTSUBSCRIPT italic_m italic_a italic_s italic_k end_POSTSUBSCRIPT.Greenanchors represent static scenes, whileredanchors indicate deformation scenes.",
                "position": 981
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]