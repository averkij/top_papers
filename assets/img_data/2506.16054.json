[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x1.png",
                "caption": "Figure 1:PAROAttentionunifies the diverse attention patterns through token reorder, which benefits both the sparsification and quantization. It achieves nearly identical generation result from full-precision baseline without metrics degradation, under lower density (20%-30%) and bitwidth (INT8/INT4), achieving a1.9‚àºsimilar-to\\sim‚àº2.7√ó\\times√óend-to-end latency speedup.",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x2.png",
                "caption": "Figure 2:The Motivation of PAROAttention.(a) The computational flow of transformer. (b) The challenge for sparsification and quantization caused by visual attention pattern, and how PAROAttention addresses it. (c) The illustration of 3D feature, and 1D token sequence with different orders.",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminary Analysis",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x3.png",
                "caption": "Figure 3:The overall framework of PAROAttention.The pattern-aware token reordering (PARO) is applied to unify the attention pattern into hardware-friendly block pattern. Sparse attention and quantization techniques are designed tailored for this pattern.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x4.png",
                "caption": "Figure 4:Qualitative Results of CogVideoX generated videos for PAROAttention and baselines.",
                "position": 585
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x5.png",
                "caption": "Figure 5:Qualitative Results of Flux generated images for PAROAttention and baseline methods.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x6.png",
                "caption": "Figure 6:Normalized latency speedup (bar plot) and PSNR (line plot)trade-off under different (a) quantization and (b) sparse configurations.",
                "position": 836
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x7.png",
                "caption": "Figure 7:Attention patterns under different permute orders.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x7.png",
                "caption": "Figure 7:Attention patterns under different permute orders.",
                "position": 858
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Results for Wan 2.1 Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x8.png",
                "caption": "Figure 8:Qualitative results of Wan 2.1 model video generation.",
                "position": 1970
            }
        ]
    },
    {
        "header": "Appendix BAdditional Qualitative Results and Analysis of Metrics Selection:",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x9.png",
                "caption": "Figure 9:Additional qualitative results of sparsification for CogVideoX.",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x10.png",
                "caption": "Figure 10:Ablation of skipping timestep and transformer blocks for SparseVideoGen.",
                "position": 1990
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x10.png",
                "caption": "Figure 11:Qualitative examples from the ablation study on skipping timesteps and Transformer blocks in SparseVideoGen.",
                "position": 2030
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results for CUDA Kernel Efficiency Improvement",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details and Analysis of Baseline Sparsification Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x11.png",
                "caption": "Figure 12:Comparison of attention masks for PAROAttention and baseline sparse attention methods.We present the relative difference metrics (L1 Norm, RMSE, CosSim) to measure the difference between the original and masked attention map.",
                "position": 2284
            }
        ]
    },
    {
        "header": "Appendix EThe Effectiveness of PAROAttention Quantization Technique",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x12.png",
                "caption": "Figure 13:Incoherence for data within the quantization group before and after permutation.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2506.16054/x13.png",
                "caption": "Figure 14:Quantization error with respect to FP for quantization of the attention mapPùëÉPitalic_P.The red ‚ÄúFP‚Äù stands for the FP8 quantization error.",
                "position": 2297
            }
        ]
    },
    {
        "header": "Appendix FGeneralization of Sparse Attention Mask",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x14.png",
                "caption": "Figure 15:Visualization of post softmax attention pattern for different timesteps, prompts, and classifier-free-guidance (CFG).The metric scores are calculated relative to the attention pattern with red text.",
                "position": 2329
            }
        ]
    },
    {
        "header": "Appendix GAdditional Visualization of Permutation for Flux",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.16054/x15.png",
                "caption": "Figure 16:Visualization of the attention pattern for flux under different permutation.",
                "position": 2339
            }
        ]
    },
    {
        "header": "Appendix HDiscussion of application of PAROAttn",
        "images": []
    },
    {
        "header": "Appendix ILimitations and Broader Impacts",
        "images": []
    }
]