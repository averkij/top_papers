[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08696/x1.png",
                "caption": "Figure 1:Overview of our approach.Standard approaches like GRPO assign a uniform reward of0to all incorrect answers. This provides no learning signal, causing these samples to be discarded. Our method, LENS, is derived from reward modeling via Maximum Likelihood Estimation (MLE) and assigns non-zero, confidence-dependent rewards to incorrect responses. This creates a clear learning signal where differences emerge from the samples, converting previously discarded information into useful gradient updates.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08696/x2.png",
                "caption": "Figure 2:Negative group ratio during GRPO training ofLlama-3.1-8B-Instructwith MATH and Numina 1.5.G=16G=16.",
                "position": 256
            }
        ]
    },
    {
        "header": "4A Likelihood-Based Framework for Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08696/x3.png",
                "caption": "Figure 3:An optimal policyπ⋆\\uppi^{\\star}is derived from reward probabilitiesp⋆p^{\\star}through normalization (see Equation (4)). This approach reframes the task of finding the best policy as a more straightforward statistical problem: learning a reward model from data.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2510.08696/x4.png",
                "caption": "Figure 4:Illustration of the weight functionw​(z)w(z).",
                "position": 491
            }
        ]
    },
    {
        "header": "5Proposed Modification to GRPO",
        "images": []
    },
    {
        "header": "6Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08696/x5.png",
                "caption": "Figure 5:Comparison of our algorithm and GRPO baseline: performance on the full MATH test set and the Levels 4–5 (hard) subset. Top:Llama-3.1-8B-Instruct; bottom:Qwen-2.5-3B-Base. The accuracy is averaged across all 16 generations during evaluation and over two independent runs. Training set: MATH + DAPO. Our algorithm brings improvement for both models.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2510.08696/x6.png",
                "caption": "",
                "position": 640
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOther Related Works",
        "images": []
    },
    {
        "header": "Appendix BProofs",
        "images": []
    },
    {
        "header": "Appendix CA Preference-Aware Framework",
        "images": []
    },
    {
        "header": "Appendix DExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08696/x7.png",
                "caption": "Figure 6:Comparison of our algorithm and GRPO baseline: performance on the full MATH test set and the Levels 4–5 (hard) subset. Top:Llama-3.1-8B-Instruct; bottom:Qwen-2.5-3B-Base. The accuracy is averaged over all 16 generations during the evaluation. Our algorithm brings improvement for both models. The training set is MATH and Numina 1.5",
                "position": 1613
            },
            {
                "img": "https://arxiv.org/html/2510.08696/x8.png",
                "caption": "",
                "position": 1617
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]