[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02994/x1.png",
                "caption": "Figure 1:Some examples of our 3DEditVerse dataset. See more examples in AppendixA.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "33DEditVerse Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02994/x2.png",
                "caption": "Figure 2:Overview of our data generation pipeline for text-guided 3D editing. Starting from a large-scale Vocabulary Set, we employ multiple foundation models in a carefully orchestrated manner and construct the text-to-image-to-3D lifting pipeline.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2510.02994/x3.png",
                "caption": "Figure 3:Overview of our proposed 3DEditFormer. (a) Multi-stage features{f3​D(1,i)}i=1N\\{f^{(1,i)}_{3D}\\}_{i=1}^{N}and{f3​D(2,i)}i=1N\\{f^{(2,i)}_{3D}\\}_{i=1}^{N}are extracted from the frozen Trellis model(Xiang et al.,2025)at different denoising timesteps, capturing fine-grained structural priors and semantic transition cues, respectively. (b) These features are injected into each transformer layer via (c) Dual-Guidance Attention Block, where their contributions are modulated by (d) Time-Adaptive Gating mechanism.",
                "position": 275
            }
        ]
    },
    {
        "header": "43D-Structure-Preserving Conditional Transformer",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02994/x4.png",
                "caption": "Figure 4:Qualitative comparison among our proposed 3DEditFormer and SoTAs, including EditP23(Bar-On et al.,2025), Instant3dit(Barda et al.,2025), and VoxHammer(Li et al.,2025a)on our proposed 3DEditVerse test set. More visualizations are provided in appendixDandE.",
                "position": 416
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AVisualization of our 3DEditVerse dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02994/x5.png",
                "caption": "Figure 5:More examples of (a) Character–Animation Compositions and (b) generative data from text-guided editing in our proposed 3DEditVerse dataset.",
                "position": 1446
            }
        ]
    },
    {
        "header": "Appendix BDetails of instructional prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02994/x6.png",
                "caption": "Figure 6:Examples of source–target image pairs. Text below the source (left) shows the generation prompt, while text below the target (right) shows the editing instruction.",
                "position": 1572
            }
        ]
    },
    {
        "header": "Appendix CDetails of 3D Editing via Repaint",
        "images": []
    },
    {
        "header": "Appendix DVisualization of Comparison with SoTA Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02994/x7.png",
                "caption": "Figure 7:More qualitative results compared with SoTA methods.",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2510.02994/x8.png",
                "caption": "Figure 8:Qualitative results of our 3DEditVerse on character–animation test set.",
                "position": 1625
            }
        ]
    },
    {
        "header": "Appendix EVisualization on Character–Animation Test Set",
        "images": []
    }
]