[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Backward Pass Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22813/x1.png",
                "caption": "Figure 1:Impact of selective NVFP4 backward pass quantization on C4 Validation Loss relative to BF16 pre-training forNN-parameter Llama-2-like LLMs withD/ND/Ntokens-per-parameter. Axis captions indicate which tensors of the two backward pass GEMMs are quantized.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Forward Pass Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22813/x2.png",
                "caption": "Figure 2:NVFP4 Forward Pass C4 Validation Loss Gaps relative to BF16 pre-training forNN-parameter Llama-2-like LLMs withD/ND/Ntokens-per-parameter. “16x16gs” and “1x16gs” indicate whether square block quantization was used or not and “+4/6” indicates whether Four Over Six(Cooket al.,2025)was used.",
                "position": 521
            }
        ]
    },
    {
        "header": "5The Quartet II Computation Graph",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22813/x3.png",
                "caption": "Figure 3:Quartet II fully-NVFP4 linear layer computation scheme.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2601.22813/x4.png",
                "caption": "Figure 4:Fully-NVFP4 (forward pass and backward pass) C4 Validation Loss Gaps relative to BF16 pre-training forNN-parameter Llama-2-like LLMs withD/ND/Ntokens-per-parameter for Quartet II and baselines.",
                "position": 581
            }
        ]
    },
    {
        "header": "6Experimental Validation and Extensions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22813/x5.png",
                "caption": "Figure 5:Validation loss curves for Nanochat pre-training. Plot show relative increase in bits-per-byte (BPB) w.r.t. BF16 pre-training. Loss spikes are observed for both BF16 and QAT around 6T tokens but training stabilizes later.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2601.22813/x5.png",
                "caption": "Figure 5:Validation loss curves for Nanochat pre-training. Plot show relative increase in bits-per-byte (BPB) w.r.t. BF16 pre-training. Loss spikes are observed for both BF16 and QAT around 6T tokens but training stabilizes later.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2601.22813/x6.png",
                "caption": "Figure 6:Linear layer computation scheme speedup over BF16 for training layers characteristic of particular model sizes.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2601.22813/x7.png",
                "caption": "Figure 7:Naïve range alignment MS-EDEN re-quantization kernel.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2601.22813/x8.png",
                "caption": "Figure 8:Improved post hoc range alignment MS-EDEN re-quantization kernel.",
                "position": 671
            }
        ]
    },
    {
        "header": "7Kernel Support",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUnbiasedness Verification",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22813/x9.png",
                "caption": "Figure 9:Concentration of quantized backward average towards unquantized backward for Quartet II and a number of baselines. Methods parallel to1/B1/Bare unbiased. Plateauing methods (NVIDIA+4/6) introduce bias.",
                "position": 1292
            }
        ]
    },
    {
        "header": "Appendix BLlama-Like Hyper-Parameters",
        "images": []
    },
    {
        "header": "Appendix CNanochat Details and Extra Evaluation",
        "images": []
    },
    {
        "header": "Appendix DKernel Benchmarks",
        "images": []
    }
]