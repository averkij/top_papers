[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x1.png",
                "caption": "",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x2.png",
                "caption": "",
                "position": 267
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Agentic Scene Construction for Simulation-Ready Environments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x3.png",
                "caption": "Figure 2:SceneSmith‚Äôs hierarchical scene construction pipeline.A scene promptùíØ\\mathcal{T}is processed by a layout agent to generate architectural geometry forMMrooms. Each room is then independently populated through furniture, wall-mounted, and ceiling-mounted stages using room-specific promptsùíØj\\mathcal{T}_{j}. In each room,KjK_{j}supporting entities subsequently form additional branches populated with manipulable objects using entity-specific promptsùíØj,k\\mathcal{T}_{j,k}. Colored highlights indicate objects added at each stage. Each stage (colored triangle) is implemented as an agentic interaction between aDesigner,Critic, andOrchestrator. Stacked frames indicate parallel branches.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x4.png",
                "caption": "Figure 3:Text-to-3D asset generation pipeline.Given an object description, we generate an image, segment the foreground, and reconstruct a textured 3D mesh. The mesh is augmented with collision geometry (gray convex pieces) and physical properties estimated by a VLM, including mass, center of mass, friction, and inertia (blue ellipsoid). The mesh is also scaled to target dimensions.",
                "position": 402
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x5.png",
                "caption": "Figure 4:Qualitative comparison with HSM and Holodeck, the two strongest baselines in our user study. SceneSmith produces denser scenes that better satisfy prompt requirements. See AppendixQfor additional examples.",
                "position": 457
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASystem Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x6.png",
                "caption": "Figure 5:Designer-critic iteration during furniture placement.Each column shows a different room evolving through two rounds of critic feedback and designer refinement.Top row:Initial designs after the first designer pass, with scene prompts shown above.Middle row:Scenes after the first critique-and-improve cycle.Bottom row:Scenes after the second cycle. Text annotations describe the changes made at each step. The bedroom (left) progressively improves bunk bed placement. The dining room (center) refines chair orientation and adds furnishings. The pharmacy (right) illustrates checkpoint rollback: when the designer‚Äôs additions degrade the critic score, the orchestrator resets to the previous checkpoint and prompts a different approach.",
                "position": 1734
            }
        ]
    },
    {
        "header": "Appendix BAsset Acquisition",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x7.png",
                "caption": "Figure 6:Asset validation examples.Each row shows a text-to-3D request with a rejected (left) and accepted (right) generation.Top:The validator rejects a media console generated with a TV attached, violating the single-object requirement.Bottom:The validator rejects a bookshelf with closed cabinet doors, which does not match the ‚Äúopen bookshelf‚Äù specification.",
                "position": 2079
            }
        ]
    },
    {
        "header": "Appendix CLayout Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x8.png",
                "caption": "Figure 7:Layout agent observation tools.Left:Theobserve_scenetool renders a top-down view with room labels and displays assigned materials with texture swatches.Right:Therender_asciitool provides an ASCII floor plan with labeled wall segments (A‚ÄìJ) and structured metadata including room dimensions, connectivity validation, and door/window specifications.",
                "position": 2417
            }
        ]
    },
    {
        "header": "Appendix DFurniture Placement",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x9.png",
                "caption": "Figure 8:Furniture agentobserve_sceneoutput.Left:Top-down view with coordinate grid, coordinate frame, and labeled furniture showing bounding boxes and direction arrows. Walls, doors, and windows are labeled for placement reference.Right:Four corner views with coordinate markers for depth perception and alignment.",
                "position": 2788
            }
        ]
    },
    {
        "header": "Appendix EWall Object Placement",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x10.png",
                "caption": "Figure 9:Wall agentobserve_sceneoutput.Top-right:Top-down context view showing the full room with labeled wall objects.Remaining panels:Orthographic views of each wall surface with coordinate grids, wall identifiers, door/window openings, and object labels. Nearby furniture is rendered for spatial context.",
                "position": 2997
            }
        ]
    },
    {
        "header": "Appendix FCeiling Object Placement",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x11.png",
                "caption": "Figure 10:Ceiling agentobserve_sceneoutput.Left:Top-down view with coordinate grid and labeled ceiling fixtures.Right:Two corner views with coordinate markers for verifying fixture heights and clearance from tall furniture.",
                "position": 3164
            }
        ]
    },
    {
        "header": "Appendix GManipuland Placement",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x12.png",
                "caption": "Figure 11:Manipuland agent observation for single-surface entity.Top:Top-down view with coordinate grid, surface-local position markers, and coordinate frame. Manipulands are labeled with bounding boxes.Bottom:Four side views with coordinate markers. The office chair is included as context furniture to inform object orientation.",
                "position": 3429
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x13.png",
                "caption": "Figure 12:Manipuland agent observation for multi-surface entity.Top:Four side views of a shelving unit with color-coded surface identifiers (S_4 through S_8).Bottom:Top-down renders for individual surfaces (two of five shown), colored to match the side views. The agent populates all surfaces jointly in a single session.",
                "position": 3432
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x14.png",
                "caption": "Figure 13:Manipuland agent observation for articulated entities.Left:Doors are opened during rendering to reveal interior surfaces.Right:For entities with drawers (prismatic joints), separate renders show each drawer individually to avoid occlusion from stacked drawers (one of multiple drawer renders shown).",
                "position": 3435
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x15.png",
                "caption": "Figure 14:Examples of thecreate_stacktool.Stacks include same-type objects (plates, bowls, cups), mixed objects (books, storage containers), and object-on-object arrangements (cup on saucer, candle on holder, croissant on plate). Physics simulation settles objects into stable configurations.",
                "position": 3472
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x16.png",
                "caption": "Figure 15:Examples of thefill_containertool.Containers include fruit bowls, utensil crocks, pencil cups, toy bins, vases with plants, and bread baskets. Physics simulation with iterative retry settles objects naturally within container cavities.",
                "position": 3522
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x17.png",
                "caption": "Figure 16:Examples of thecreate_arrangementtool.Items are placed at agent-specified positions on flat containers: baking trays, cheese boards, condiment caddies, candle trays, bathroom trays, and cutting boards. Unlikefill_container, this tool provides precise control over item placement.",
                "position": 3560
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x18.png",
                "caption": "Figure 17:Examples of thecreate_piletool.Left:Floor piles with random positions and orientations: scattered toys and a dish mess on a kitchen floor.Right:Dirty dishes in a sink basin. The sink is built into the counter, sofill_containercannot be used since it requires a separate container asset.",
                "position": 3610
            }
        ]
    },
    {
        "header": "Appendix HPhysical Feasibility Post-Processing",
        "images": []
    },
    {
        "header": "Appendix IStochastic Placement",
        "images": []
    },
    {
        "header": "Appendix JExport and Simulator Compatibility",
        "images": []
    },
    {
        "header": "Appendix KRobot Policy Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x19.png",
                "caption": "Figure 18:Robot manipulation evaluation pipeline.Given a manipulation task (e.g., ‚ÄúPick a fruit from the fruit bowl and place it on a plate‚Äù), an LLM generates diverse scene prompts specifying scene constraints implied by the task. SceneSmith generates scenes from each prompt. A robot policy attempts the task in simulation, and an evaluation agent verifies success using simulator state queries and visual observations. This enables scalable policy evaluation without manual environment or success predicate design.",
                "position": 3789
            }
        ]
    },
    {
        "header": "Appendix LEvaluation Methodology",
        "images": []
    },
    {
        "header": "Appendix MHuman Evaluation",
        "images": []
    },
    {
        "header": "Appendix NAutomated Evaluation",
        "images": []
    },
    {
        "header": "Appendix OEvaluation Prompts",
        "images": []
    },
    {
        "header": "Appendix PUser Study Interface Screenshots",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09153/x20.png",
                "caption": "Figure 19:User study interface: 3D viewer mode.Top:Side-by-side comparison with interactive Babylon.js 3D viewers. Participants can orbit, pan, and zoom each scene independently. Controls include Hide Selected, Show All, Reset, and Fullscreen.Bottom:Fullscreen mode with object selection. Clicking an object highlights it and enables the Frame button, which focuses the camera on the selected object and zooms in.",
                "position": 6544
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x21.png",
                "caption": "Figure 20:User study interface: Alternative viewing modes for participants who prefer static images or have slower devices.Top:Click-Through mode with carousel navigation through 9 pre-rendered camera angles.Bottom:Grid View mode displaying all angles simultaneously for quick comparison.",
                "position": 6547
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x22.png",
                "caption": "Figure 21:User study questions.Top:The two evaluation questions. Q1 (Realism) is forced-choice between Scene A and B; Q2 (Faithfulness) includes an ‚ÄúEqual‚Äù option.Bottom:Confirmation dialog shown when participants select ‚ÄúEqual,‚Äù encouraging them to reconsider and only confirm if they genuinely cannot distinguish which scene better matches the prompt.",
                "position": 6550
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x23.png",
                "caption": "Figure 22:User study tutorial tooltips (selection). The interface includes an interactive onboarding sequence with contextual tooltips. Shown here: display mode switching (3D View, Click-Through, Grid View), the help button for 3D viewer controls, and the Hide Selected/Show All functionality for inspecting occluded objects or verifying object composition.",
                "position": 6553
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x24.png",
                "caption": "Figure 23:Room connectivity comparison for a hotel scene.Both SceneSmith (top) and Holodeck (bottom) contain all the requested rooms (House-level prompt 25; AppendixO.6): reception (a), lounge (b), restaurant (c), kitchen (d), corridor (e), gym (f), four guest rooms (g, i, k, m), and four bathrooms (h, j, l, n). Cyan lines show room connectivity paths. SceneSmith generates realistic floor plans where the entrance leads through the reception, en suite bathrooms are only accessible through associated guest rooms, the kitchen is connected to the restaurant, and all rooms connect via a central corridor. Holodeck produces implausible layouts where rooms may only be reachable through other guest rooms or bathrooms. For example, the kitchen (d) is only reachable through bathroom (j), and guest room (m) is only reachable by traversing through guest rooms (i) and (k) and bathroom (l).",
                "position": 6560
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x25.png",
                "caption": "Figure 24:House-level qualitative comparison.SceneSmith (top) vs Holodeck (bottom) for a multi-room prompt.",
                "position": 6583
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x26.png",
                "caption": "Figure 25:Room-level qualitative comparison.SceneSmith compared to six baselines across four diverse prompts: bunk bed bedroom, Japanese-style living room, grocery store, and nail salon. Baselines are ordered by their user study realism score. SceneSmith generates scenes with appropriate object density, realistic arrangements, and faithful prompt following.",
                "position": 6586
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x27.png",
                "caption": "Figure 26:House-level generation examples.Two multi-room scenes generated by SceneSmith: a dental office with reception area, exam rooms, X-ray room, and bathroom; and a boutique hotel suite with bedroom, living area, walk-in closet, dressing room, and bathroom.",
                "position": 6589
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x28.png",
                "caption": "Figure 27:Additional room-level generation examples.Each panel shows a text prompt and the corresponding scene generated by SceneSmith.",
                "position": 6592
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x29.png",
                "caption": "Figure 28:Additional room-level generation examples.Each panel shows a text prompt and the corresponding scene generated by SceneSmith.",
                "position": 6595
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x30.png",
                "caption": "Figure 29:Additional room-level generation examples.Each panel shows a text prompt and the corresponding scene generated by SceneSmith.",
                "position": 6598
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x31.png",
                "caption": "Figure 30:Additional room-level generation examples.Each panel shows a text prompt and the corresponding scene generated by SceneSmith.",
                "position": 6601
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x32.png",
                "caption": "Figure 31:Manipuland placement examples.Diverse furniture pieces populated with contextually appropriate manipulands: pottery store display table, produce stand with fruits, store shelf with products, diner table with menus, bar cart with bottles and glasses, library table with books, and gaming desk setup. Each object is a separate simulation asset that can be individually manipulated (e.g., each book in the stacks).",
                "position": 6604
            },
            {
                "img": "https://arxiv.org/html/2602.09153/x33.png",
                "caption": "Figure 32:Themed room generation.SceneSmith generates rooms with specific visual themes: Jurassic Park kids bedroom, Art Deco living room, steampunk home office, Star Wars teen bedroom, Frozen nursery, and Incredibles playroom. Generated assets and retrieved materials match the requested themes and styles.",
                "position": 6607
            }
        ]
    },
    {
        "header": "Appendix QAdditional Qualitative Results",
        "images": []
    }
]