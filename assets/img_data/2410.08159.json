[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/dart_teaser_2.jpg",
                "caption": "Figure 1:Curated examples of images generated by DART at256√ó256256256256\\times 256256 √ó 256and512√ó512512512512\\times 512512 √ó 512pixels.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/pipeline_dart4.jpg",
                "caption": "Figure 2:(‚Üê‚Üê\\leftarrow‚Üê) A general illustration of the proposed DART. The model autoregressively denoises image through a Transformer until a clean image is generated. Here, whole images are shown for visualization purpose; (‚Üí‚Üí\\rightarrow‚Üí) We show the architecture details which integrates state-of-the-art designs similar to common language models(Dubey et¬†al.,2024).",
                "position": 127
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3DART",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/attn_mask.jpg",
                "caption": "Figure 3:Attention masks for (a) DART and (b) DART-AR, highlighting their different structures. (c) Comparison of learning curves, demonstrating the superior performance of DART-AR.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/attn_mask_ar.jpg",
                "caption": "",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/compare_with_ar.jpg",
                "caption": "",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/dart_fm.jpg",
                "caption": "Figure 4:An illustration for the generation process of DART-FM.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/dart_multires_text_new.jpg",
                "caption": "Figure 5:Illustrations ofMatryoshka-DART(‚Üê‚Üê\\leftarrow‚Üê) andKaleido-DART(‚Üí‚Üí\\rightarrow‚Üí). By joint training models, DART can perform various applications such as multi-resolution and multi-modal generation.",
                "position": 509
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/dart_t2i2.jpg",
                "caption": "Figure 6:Samples generated by text-to-image DART-FM at256√ó256256256256\\times 256256 √ó 256and512√ó152512152512\\times 152512 √ó 152pixels",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/dart_imagenet.jpg",
                "caption": "Figure 7:Comparison for class-conditioned image generation with DART and baselines trained on ImageNet.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/ImageNet_comparison.png",
                "caption": "Figure 8:Comparison of DART, DART-AR, DART-FM and baseline models with different CFG guidance scale on different benchmarks. * denotes models implemented and trained by us.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/COCO_comparison.png",
                "caption": "",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/COCO_CLIP.png",
                "caption": "",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/model_size_comparison.png",
                "caption": "Figure 9:(a) Inference flops and speed (caulated as second per image) of different models. (b) Performance of DART of different sizes. (c) Effect of number of noise levels on DART.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/noise_level_comparison.png",
                "caption": "",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/text_image_co.jpg",
                "caption": "Figure 10:Examples of multi-modal generation with Kaleido-DART.",
                "position": 653
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof ofProposition1",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/generation_process.jpg",
                "caption": "Figure 11:Visualization of the generation process (left: latent maps, right: decoded RGB images) for256√ó256256256256\\times 256256 √ó 256(T=16)ùëá16(T=16)( italic_T = 16 )and its upsampling to512√ó512512512512\\times 512512 √ó 512(T=4)ùëá4(T=4)( italic_T = 4 )using Matryoshka-DART.",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/additional_imagenet.jpg",
                "caption": "Figure 12:Uncurated samples from DART varints on ImageNet256√ó256256256256\\times 256256 √ó 256with for labels ofdaisy,husky,coral reel,sulphur-crested cockatoo,cliff,espresso,axolotl,jay,hotdog,lionfish.",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/additional_t2i.jpg",
                "caption": "Figure 13:Uncurated samples from DART varints on text-to-image generation at256√ó256256256256\\times 256256 √ó 256pixels given various captions.",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/additional_t2i512.jpg",
                "caption": "Figure 14:Uncurated samples from DART-FM with Matryoshka-DART fine-tuning on text-to-image generation at512√ó512512512512\\times 512512 √ó 512pixels given various captions.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2410.08159/extracted/5915029/figures/additional_t2i5122.jpg",
                "caption": "Figure 15:Uncurated samples from DART-FM with Matryoshka-DART fine-tuning on text-to-image generation at512√ó512512512512\\times 512512 √ó 512pixels given various captions.",
                "position": 1857
            }
        ]
    },
    {
        "header": "Appendix CAdditional Samples",
        "images": []
    }
]