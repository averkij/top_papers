[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/icons/logo.png",
                "caption": "",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x1.png",
                "caption": "",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x2.png",
                "caption": "",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x3.png",
                "caption": "Figure 1:Examples fromVideoReasonBenchand three existing VideoQA benchmarks. Responses are generated by Gemini-2.5-Flash in both “Thinking” and “No Thinking” modes. The text highlighted ingreen/redindicate correct/incorrect responses. While questions from existing benchmarks can be answered correctly without “Thinking” using only a few tokens,VideoReasonBenchrequires “Thinking” for accurate reasoning and consumes substantially more tokens (See Figure5for quantitative results). It also demands finer-grained visual perception during reasoning.",
                "position": 146
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2VideoReasonBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23359/x4.png",
                "caption": "Figure 2:Illustration of vision-centric complex video reasoning.Upper:In each video, the latent state is revealed either at the begin or the end, and a sequence of observable operations is applied to this state. There are six categories of videos, each featuring a different type of demonstration.Lower:The questions assess video reasoning across three levels, with two skills for each level.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x5.png",
                "caption": "Figure 3:Overview of our data construction framework. The video engine generates state transitions from a given configuration, producing videos via Matplotlib, command-line screenshots, or real-world manual recordings. The question engine then generates questions and derives answers based on the state transitions, following the rules of each demonstration.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x6.png",
                "caption": "",
                "position": 283
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23359/x7.png",
                "caption": "(a)Existing Benchmarks",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x7.png",
                "caption": "(a)Existing Benchmarks",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2505.23359/x8.png",
                "caption": "(b)VideoReasonBench",
                "position": 776
            }
        ]
    },
    {
        "header": "4Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details ofVideoReasonBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/number.jpg",
                "caption": "Table 6:Prompt and question templates for theNumbervideo demonstration.",
                "position": 1440
            },
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/cup.jpg",
                "caption": "Table 7:Prompt and question templates for theCupvideo demonstration.",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/grid.jpg",
                "caption": "Table 8:Prompt and question templates for theCirclevideo demonstration.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/file_sys.jpg",
                "caption": "Table 9:Prompt and question templates for theFilevideo demonstration.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/card.jpg",
                "caption": "Table 10:Prompt and question templates for theCardvideo demonstration.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/chip.jpg",
                "caption": "Table 11:Prompt and question templates for theChipvideo demonstration.",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2505.23359/extracted/6492938/figures/annotation_interface.jpeg",
                "caption": "Figure 6:Screenshot of the human annotation interface.",
                "position": 1871
            }
        ]
    },
    {
        "header": "Appendix BMore Details of Experimental Setups",
        "images": []
    }
]