[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17025/x1.png",
                "caption": "Figure 1:Fertility for two different tokenizers,Mistral-7B-v0.1(left) andMinerva(right), over Italian texts from CulturaX (blue) and Wikipedia (red).",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_it_0shot.png",
                "caption": "Figure 2:Average performance ofMistral-7B-v0.1based models during training onItaliantranslated benchmarks. The average was calculated over six datasets.",
                "position": 589
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/llama_it_0shot-new.png",
                "caption": "Figure 3:Average performance ofLlama-3.1-8Bbased models during training onItaliantranslated benchmarks. The average was calculated over six datasets.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_en_0shot.png",
                "caption": "Figure 4:Average performance ofMistral-7B-v0.1based models during training onEnglishbenchmarks. The average was calculated over six datasets.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/llama_en_0shot-new.png",
                "caption": "Figure 5:Average performance ofLlama-3.1-8Bbased models during training onEnglishbenchmarks. The average was calculated over six datasets.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_training_loss-new.png",
                "caption": "Figure 6:Loss during continual training ofMistral-7B-v0.1models.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/llama_training_loss-new.png",
                "caption": "Figure 7:Loss during continual training ofLlama-3.1-8Bmodels.",
                "position": 896
            }
        ]
    },
    {
        "header": "6Differences in the Embedding Structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/cross_similarity.jpg",
                "caption": "Figure 8:Similarity across models after continual training on 12B tokens.",
                "position": 965
            }
        ]
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASAVA Training of the mapping function",
        "images": []
    },
    {
        "header": "Appendix BAblation experiments on the SAVA method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_training_loss_sava_models-new.png",
                "caption": "Figure 9:Loss during continual training of Mistral models.",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2504.17025/extracted/6384534/mistral_training_loss_sava_anchors-new.png",
                "caption": "Figure 10:Loss during continual training of Mistral models.",
                "position": 1584
            }
        ]
    },
    {
        "header": "Appendix CTraining Resources and Environmental Impact",
        "images": []
    },
    {
        "header": "Appendix DGeneration setting",
        "images": []
    },
    {
        "header": "Appendix EEnglish Results on Multi-choice benchmarks",
        "images": []
    }
]