[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02315/figure/figure_block_small.png",
                "caption": "Figure 1:The proposed Prithvi-CAFE architecture. (a) The Spectral Selection module divides input images into two spectral branches. (b) The encoder processes these branches using adapted Prithvi blocks and CNN blocks, respectively. (c) The Multi-Scale Multi-Level Feature Attention Fusion (M2FAF) module merges features from both streams. (d) The Decoder integrates them via pyramid pooling and lateral connections to generate the segmentation mask. (e) Adapter modules are attached to each ViT block of Prithvi, as shown in (f). (g) Residual Blocks and (h) Convolutional Attention Modules (CAM) are used to enhance CNN features.",
                "position": 155
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02315/figure/bias.png",
                "caption": "Figure 2:The effect of bias factorÎ²\\betaon Transformer (semantic) and CNN (spatial) feature fusion",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2601.02315/figure/K4.png",
                "caption": "Figure 3:Box plot showing distribution of mIoU per image in four fold cross validation on FLoodPlanet",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2601.02315/figure/Flood.png",
                "caption": "Figure 4:Comparative visual analysis of the proposed Prithvi-CAFE against other models using FloodPlanet data. Numbers in the lower-right corner indicate mIoU.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2601.02315/figure/embeddings.png",
                "caption": "Figure 5:Visualization of feature embedding of FloodPlanet data using t-SNE plots. (a) fully fine-tune Prithvi 2.0 encoder (600M), (b) Adaptive Prithvi fine-tuning (only 45.5M parameters are trained) (c) fully fine-tune TerraMind-Base encoder, (e) fully fine-tune DOFA-Base encoder.",
                "position": 693
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02315/supply/Flood_limitation.png",
                "caption": "Figure A.6:Figure illustrate limitation of Prithvi-CAFE in case of dense cloud cover.",
                "position": 838
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]