[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16781/x1.png",
                "caption": "",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16781/x2.png",
                "caption": "Figure 2:Method overview: UniPose comprises a Pose Tokenizer, Visual Processor and a pose-aware language LLM. Combining Pose Tokens learned by pose tokenizer, Visual Embeddings from visual processor and Text Tokens from text tokenizer, UniPose enables joint modeling of pose comprehension, generation and editing within a unified visual-language backbone.",
                "position": 258
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16781/x3.png",
                "caption": "(a)Pose-Text Alignment Pretraining Stage.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x3.png",
                "caption": "(a)Pose-Text Alignment Pretraining Stage.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x4.png",
                "caption": "(b)Visual Projector Pretraining Stage.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x5.png",
                "caption": "(c)Instruction Finetuning Stage.",
                "position": 329
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16781/x6.png",
                "caption": "(a)Comparison with Qwen-VL[4]and GPT-4o[1]on Image-to-Text task.",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x6.png",
                "caption": "(a)Comparison with Qwen-VL[4]and GPT-4o[1]on Image-to-Text task.",
                "position": 1013
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x7.png",
                "caption": "(b)Comparison with GPT-4o[1]on Image-Diff task.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x8.png",
                "caption": "Figure 5:Enhance pose estimation with input pose description.",
                "position": 1048
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16781/x9.png",
                "caption": "",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x10.png",
                "caption": "Figure 2:Prompt to query GPT-4 for refining text in the ImageScript dataset.",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x11.png",
                "caption": "Figure 3:Prompt to query GPT-4 for refining text in the ImageDiff dataset.",
                "position": 2113
            }
        ]
    },
    {
        "header": "AData Collection",
        "images": []
    },
    {
        "header": "BImplementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.16781/x12.png",
                "caption": "Figure 4:Qualitative comparison on pose estimation task.\nWe compare multi-modal LLMs (ChatPose[18]) and traditional HMR methods (TokenHMR[15]) with our UniPose on LSP[32]dataset.",
                "position": 2637
            },
            {
                "img": "https://arxiv.org/html/2411.16781/x13.png",
                "caption": "Figure 5:Qualitative comparison on reasoning-based pose estimation task. We evaluate the modelâ€™s reasoning capabilities in multi-person images.",
                "position": 2641
            }
        ]
    },
    {
        "header": "CAdditional Experiments",
        "images": []
    },
    {
        "header": "DQualitative Evaluation",
        "images": []
    },
    {
        "header": "ELimitation",
        "images": []
    }
]