[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13180/x1.png",
                "caption": "Figure 1:(a)Although FastV prunes most visual tokens from the upper portion of the image, the approach still displays strong performance on a variety of evaluated vision-language tasks except for the vision-centric task of localization.(b)Based on our findings, we proposeFEATHER(Fast andEffectiveAcceleration wiTHEnsemble cRiteria), a straightforward approach that resolves the existing issue of selecting bottom tokens, additionally maintains uniformly sampled tokens to ensure good coverage over the whole image, and prunes in two stages.",
                "position": 70
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Drawback of VLM Acceleration",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13180/x2.png",
                "caption": "Figure 2:Contrasting the difference in performance dropoff on the challenging vision-centric localization task (Left) versus the other evaluated tasks (Middle) when pruning visual tokens after the shallow LLM layers. Whereas performance decrease is minimal for most tasks, localization exhibits roughly a linear decrease to zero as the ratio of pruned tokens increases.Right:Per-task performance breakdown across various setups of pruning ratios. UsingK=3K3\\textit{K}=3K = 3for all pruning setups.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2412.13180/x3.png",
                "caption": "Figure 3:(a)Example demonstrating that when pruning in early layers, selected tokens are concentrated in the bottom of the image, whereas in later layers the selected tokens more precisely cover the region of the image important for answering the question.(b)Heatmap illustrating that averaged across all benchmark examples, as the pruning layer increases, the selection of bottom visual tokens by the criteria is reduced.(c)Visualizing the effect of the pruning layer on performance for both localization and non-localization tasks. We find that pruning after layer 16 or later results in little performance dropoff, whereas pruning earlier results in a performance decrease, particularly for localization. UsingR=0.75ùëÖ0.75R=0.75italic_R = 0.75for all setups.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2412.13180/x4.png",
                "caption": "Figure 4:Assessing whether the strong performance of early visual token pruning for many tasks can be attributed to visual information transfer before pruning or benchmarks‚Äô lack of assessing fine-grained visual capabilities. We observe that for all evaluated tasks, allowing information transfer before pruning (shown in green) does not result in substantial performance improvement over the setup without visual information transfer (shown in light green), highlighting a limitation of many benchmarks.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Improving Visual Token Pruning",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13180/x5.png",
                "caption": "Figure 5:ComparingFEATHERperformance against FastV[7]and PyramidDrop[33]. We find thatFEATHERfar outperforms both compared methods, particularly for the vision-centric task of localization.",
                "position": 435
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13180/x6.png",
                "caption": "",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2412.13180/x7.png",
                "caption": "Figure 7:Visualizing the ability of various pruning criteria to maintain visual tokens relevant to the reference expression when applied after layer eight. We observe that the attention-based criteria are more effective when pruning after this layer compared to after layer three. See the main text for criteria definitions.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2412.13180/x8.png",
                "caption": "",
                "position": 1013
            }
        ]
    },
    {
        "header": "6Token Pruning Visualizations",
        "images": []
    }
]