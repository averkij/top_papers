[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21049/figs/RepMTL_FinalTeaser.png",
                "caption": "Figure 1:Overview of Rep-MTL and existing MTO methods. (a) Both loss scaling and gradient manipulation focus on optimizer-centric policies to address conflicts in model update. (b) Rep-MTL exploits shared representation space to facilitate cross-task sharing while preserving task-specific signals without optimizer changes.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2507.21049/x1.png",
                "caption": "Figure 2:The Rep-MTL framework. It comprises two complementary task saliency driven modules:(i) Task-specific Saliency Regulation (TSR)that penalizes the saliency distribution to emphasize task-salient patterns during training, thereby mitigating negative transfer among tasks.(ii) Cross-task Saliency Alignment (CSA)that facilitates inter-task complementarity by aligning saliencies while maintaining task distinctiveness through positive and negative pairs. Together, Rep-MTL enables transfer across tasks while keeping task-specific patterns.",
                "position": 182
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21049/figs/alpha_values_backbone.png",
                "caption": "Figure 3:Comparison of PL exponent alpha[46]for backbone parameters trained with\ndiverse MTO methods on NYUv2[59]. It validates how well the backbone adapts to MTL objectives, where lower values indicate more effective training. Values outside[2,4][2,4][ 2 , 4 ]suggest potential over- or under-training. We leverage this to show how methods affect model updates, as well-trained backbones suggest beneficial cross-task sharing to the overall MTL objectives.",
                "position": 1140
            },
            {
                "img": "https://arxiv.org/html/2507.21049/figs/alpha_values_heads.png",
                "caption": "Figure 4:Visualization of PL exponent alpha[44,46]for tasks-specific heads (3 tasks) trained with\ndifferent MTO methods on NYUv2[59]. PL exponent quantifies how well each decoder adapts to its task-specific objective, where lower values practically indicate more effective model training. Values outside the range[2,6][2,6][ 2 , 6 ]suggest potential over- or under-training due to task conflicts. The variation across decoders of each method indicates training imbalance. We employ this to evaluate how diverse MTO approaches influence decoder parameter updates, as well-trained decoders should exhibit bothlow and balancedvalues, indicating effective individual task training with successful negative transfer mitigation. For display, we constrainα\\alphaitalic_αrange to(0,6)(0,6)( 0 , 6 )though EW results (10.2610.2610.26,7.017.017.01,14.2514.2514.25) extend beyond this range.",
                "position": 1144
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BOffice-31 Image Classification Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21049/figs/alpha_ablation_backbone.png",
                "caption": "Figure 5:Ablation studies through PL exponent metrics[46]for shared parameters in backbones trained with or without cross-task saliency alignment (notated as “Rep-MTL w/o CA”) on NYUv2[59]. The PL exponent alpha quantifies how well the backbone adapts to the overall MTL objectives, where lower values indicate more effective training. Values outside the range[2,6][2,6][ 2 , 6 ]suggest potential over- or under-training due to the insufficient cross-task positive transfer. We leverage this measurement to validate the effectiveness of the cross-task saliency alignment mechanism in our proposed Rep-MTL, as well-trained backbones suggest beneficial information sharing to the overall MTL objectives.",
                "position": 2687
            },
            {
                "img": "https://arxiv.org/html/2507.21049/figs/alpha_ablation_heads.png",
                "caption": "Figure 6:Ablation studies through PL exponent metrics[44,46]for parameters in diverse decoders trained with or without task-specific saliency regulation in Rep-MTL (notated as “Rep-MTL w/o TR”) on NYUv2[59]. The PL exponent alpha quantifies how well each decoder adapts to its task-specific objective, where lower values indicate more effective training. Values outside the range[2,6][2,6][ 2 , 6 ]suggest potential over- or under-training due to task conflicts. The variation across different heads of each method indicates training imbalance. We leverage this measurement to validate the effectiveness of task-specific saliency regulation in Rep-MTL, as well-trained decoders should exhibit bothlow and balancedmetric values, indicating successful negative transfer mitigation while preserving task-specific information. The results show that task-specific saliency regulation effectively helps task-specific learning and yields superior and more balanced metrics.",
                "position": 2690
            }
        ]
    },
    {
        "header": "Appendix CAblations with PL Exponent Alpha Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21049/figs/hyperparameter_sensitivity.png",
                "caption": "Figure 7:Hyper-parameter sensitivity analysis of our Rep-MTL on NYUv2[59]dataset. We empirically evaluate the impact of two critical hyper-parameters,λt​s​r\\lambda_{tsr}italic_λ start_POSTSUBSCRIPT italic_t italic_s italic_r end_POSTSUBSCRIPTandλc​s​a\\lambda_{csa}italic_λ start_POSTSUBSCRIPT italic_c italic_s italic_a end_POSTSUBSCRIPT, by fixing one asλ=0.9\\lambda=0.9italic_λ = 0.9while varying the other one across a comprehensive range as{0.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5,1.7,1.9}\\{0.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5,1.7,1.9\\}{ 0.1 , 0.3 , 0.5 , 0.7 , 0.9 , 1.1 , 1.3 , 1.5 , 1.7 , 1.9 }. The results demonstrate that Rep-MTL maintains stable and competitive performanceΔ​ptask\\Delta{\\mathrm{p}_{\\text{task}}}roman_Δ roman_p start_POSTSUBSCRIPT task end_POSTSUBSCRIPTover a substantial range (0.7,0.9,1.1,1.3,1.5{0.7,0.9,1.1,1.3,1.5}0.7 , 0.9 , 1.1 , 1.3 , 1.5), indicating its robust insensitivity to hyper-parameter variations.",
                "position": 2732
            },
            {
                "img": "https://arxiv.org/html/2507.21049/figs/runtime_values.png",
                "caption": "Figure 8:Training time per epoch comparison across different MTL optimization methods on NYUv2[59]. Methods are categorized into three training efficiency tiers (indicated by different colors), highlighting the inherent trade-off between computational speed and optimization effectiveness in MTL scenarios.",
                "position": 2754
            },
            {
                "img": "https://arxiv.org/html/2507.21049/figs/learningRate_sensitivity_analysis.png",
                "caption": "Figure 9:Learning rate sensitivity analysis of our proposed Rep-MTL on NYUv2[59]dataset. To evaluate the impact of learning rate variations, we systematically scale the learning rate from the default benchmark setting of1​e−41e-41 italic_e - 4to5​e−45e-45 italic_e - 4, using a step size of5​e−55e-55 italic_e - 5. For each setting, we report the task-level (Δ​ptask\\Delta{\\mathrm{p}_{\\text{task}}}roman_Δ roman_p start_POSTSUBSCRIPT task end_POSTSUBSCRIPT) and metric-level (Δ​pmetric\\Delta{\\mathrm{p}_{\\text{metric}}}roman_Δ roman_p start_POSTSUBSCRIPT metric end_POSTSUBSCRIPT) performance gains. Each experiment is repeated three times. The results show that Rep-MTL maintains stable and competitiveΔ​ptask\\Delta{\\mathrm{p}_{\\text{task}}}roman_Δ roman_p start_POSTSUBSCRIPT task end_POSTSUBSCRIPTandΔ​pmetric\\Delta{\\mathrm{p}_{\\text{metric}}}roman_Δ roman_p start_POSTSUBSCRIPT metric end_POSTSUBSCRIPTover a substantial range, indicating its favorable robustness to learning rate variations.",
                "position": 2771
            }
        ]
    },
    {
        "header": "Appendix DAdditional Empirical Analysis",
        "images": []
    }
]