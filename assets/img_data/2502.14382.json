[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14382/x1.png",
                "caption": "Figure 1:Performance improvement withS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTin LiveCodeBench (v2)(Jain et al.,2024).S∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTconsistently improves models across different sizes, allowing non-reasoning models to surpass reasoning models and open models to be competitive with o1 (high reasoning effort). \"Qwen-Coder\" denotes \"Qwen2.5-Coder-Instruct,\"(Hui et al.,2024)and \"R1-Distill\" denotes \"DeepSeek-R1-Distill-Qwen.\"(Guo et al.,2025).",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2502.14382/x2.png",
                "caption": "Figure 2:Overview ofS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT.Stage 1: Generation—S∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTenhances parallel samples through iterative debugging. Each sample is tested using public test cases executed via an interpreter, with outputs and/or error messages used to guide the next round of sample generation.Stage 2: Selection—S∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTselects the best sample by prompting an LLM to generate inputs that differentiate between paired samples, then leveraging actual execution results to inform the LLM to determine the optimal choice.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2502.14382/x3.png",
                "caption": "Figure 3:Ablation ofS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTperformance benefits: Qwen2.5-Coder-14B-Instruct (denoted as Qwen-Coder-14B)(Hui et al.,2024)withS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTcan surpass o1-preview withoutS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. DeepSeek-R1-Distill-Qwen-14B (denoted as R1-Distill-14B)(Guo et al.,2025)withS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPToutperforms o1-mini withoutS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT.",
                "position": 230
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Evaluation",
        "images": []
    },
    {
        "header": "5Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14382/x4.png",
                "caption": "Figure 4:The effect of hyper-parameters. Left: The impact of temperature. A moderate temperature (0.7) balances diversity and quality, leading to higher Pass@N. In contrast, a higher temperature (0.95) does not further improve Pass@N, potentially degrading code quality. Right: The effect of increasing the number of samples. Performance improves log-linearly.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2502.14382/x5.png",
                "caption": "Figure 5:Performance with in-context examples across different numbers of parallel samples (N𝑁Nitalic_N), for GPT-4o mini, Qwen2.5-Coder-7B-Instruct, and Qwen2.5-Coder-32B-Instruct.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2502.14382/x6.png",
                "caption": "Figure 6:Comparison of three iterative debugging approaches:Public Tests,+ Generated TestsandLast Round Context. Results are obtained withN=8𝑁8N=8italic_N = 8,temperature=0.7temperature0.7\\text{temperature}=0.7temperature = 0.7and up to four rounds of debugging.",
                "position": 686
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14382/x7.png",
                "caption": "Figure 7:The prompt for iterative debugging.",
                "position": 1611
            },
            {
                "img": "https://arxiv.org/html/2502.14382/x8.png",
                "caption": "Figure 8:The prompt for generating test cases.",
                "position": 1614
            },
            {
                "img": "https://arxiv.org/html/2502.14382/x9.png",
                "caption": "Figure 9:The prompt for code generation.",
                "position": 1617
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]