[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02760/x1.png",
                "caption": "Figure 1:An overview of our desiderata for concept erasure and Erasure of Language Memory method. The erased model must stay innocent of the erased concept, while still being fluent when prompted for the concept indicating seamless edit. The model should also preserve its general capabilities showing the method‚Äôs specificity.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02760/x2.png",
                "caption": "Figure 2:Interference analysis on related MMLU classes shows that both ELM and RMU interfere with closely related safe concepts.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2410.02760/x3.png",
                "caption": "Figure 3:Analysis of internal representations after erasure. (a) Probe accuracies across layers for various methods applied to Zephyr-7B (chance is 0.25, for four-choice MCQ). Probing accuracy is close to random for ELM. (b) Activation norms for methods applied to Zephyr-7B. ELM maintains typical model behavior for erased concepts in later layers.",
                "position": 774
            }
        ]
    },
    {
        "header": "6limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Code",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on metrics",
        "images": []
    },
    {
        "header": "Appendix BBaseline Methods",
        "images": []
    },
    {
        "header": "Appendix CRobustness Evaluation",
        "images": []
    },
    {
        "header": "Appendix DHyperparameter Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02760/x4.png",
                "caption": "Figure 4:Hyperparameter sweep results for rank,Œ∑ùúÇ\\etaitalic_Œ∑, and layer selection",
                "position": 1616
            }
        ]
    },
    {
        "header": "Appendix EConditional Fluency Training",
        "images": []
    },
    {
        "header": "Appendix FProgression of ELM Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02760/x5.png",
                "caption": "Figure 5:Evaluating the intermediate checkpoints of ELM method to observe the training progression. We find that the model has a sudden drop of knowledge and then continues to slowly remove the further traces.",
                "position": 1637
            }
        ]
    },
    {
        "header": "Appendix GQualitative Examples",
        "images": []
    }
]