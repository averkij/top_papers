[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03857/x1.png",
                "caption": "Figure 1:When facing uncertainty, humans instinctively behave ambiguity aversion to avoid risk, which motivated us to prompt LLM for candidate annotations (multiple possible answers), increasing the likelihood of providing the correct labels.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2506.03857/extracted/6511924/img/pic_pf.png",
                "caption": "Figure 2:Comparison of1‚àíŒ±1ùõº1-\\alpha1 - italic_Œ±-error and F1-score between single annotations (SA) and candidate annotations (CA) by GPT-3.5. Higher metric values indicate better results. See section3.2for details.",
                "position": 198
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03857/x2.png",
                "caption": "Figure 3:The overall framework of CanDist, which first prompts the LLM to provide candidate annotations, and then distills an SLM to identify the correct labels. Examples on the TREC dataset annotated by GPT-3.5 demonstrate that though the LLM fails to provide a correct answer with a single label, answering with candidate labels successfully includes the correct one. We also provide theoretical guarantees for our proposed CanDist framework.",
                "position": 324
            }
        ]
    },
    {
        "header": "4Theoretical Analysis",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03857/extracted/6511924/img/pic_scmv.png",
                "caption": "Figure 4:Comparison of1‚àíŒ±1ùõº1-\\alpha1 - italic_Œ±on TREC‚Äôs training set (left) and accuracy on the testing set (right) between different collaboration strategies with self-consistency.",
                "position": 1865
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03857/extracted/6511924/img/pic_mv.png",
                "caption": "Figure 5:Comparison of different prompting strategies for self-consistency shows the synergism between prompting candidates with self-consistency.",
                "position": 3232
            },
            {
                "img": "https://arxiv.org/html/2506.03857/extracted/6511924/img/al.png",
                "caption": "Figure 6:Comparison between active learning methods and CanDist on TREC whereCanDistallsubscriptCanDistall\\text{CanDist}_{\\text{all}}CanDist start_POSTSUBSCRIPT all end_POSTSUBSCRIPTis applied.",
                "position": 3278
            }
        ]
    },
    {
        "header": "Appendix CProof of Theorem1",
        "images": []
    },
    {
        "header": "Appendix DFull Prompt Design",
        "images": []
    }
]