[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01949/x1.png",
                "caption": "",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01949/x2.png",
                "caption": "Figure 2:(a) Efficiency Analysis on LLaVA-NeXT-7B under 88.9% reduction. (b) Comparison with other baselines on LLaVA-1.5-7B under 94.4% reduction.",
                "position": 190
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01949/x3.png",
                "caption": "Figure 3:Token redundancy visualized via similarity and entropy on 10,000 COCO images.",
                "position": 262
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01949/x4.png",
                "caption": "Figure 4:Overview of Script, a three-stage pruning framework: (a) overall architecture; (b) Query-Conditioned Semantic Pruning (QCSP); (c) Graph-Structured Pruning (GSP). Together, these modules remove semantically irrelevant and visually redundant tokens through a joint selection process.",
                "position": 518
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01949/fig/relevance/query.png",
                "caption": "Input Image",
                "position": 2101
            },
            {
                "img": "https://arxiv.org/html/2512.01949/fig/relevance/query.png",
                "caption": "Input Image",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2512.01949/fig/relevance/attn.png",
                "caption": "Attention-based",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2512.01949/fig/relevance/div.png",
                "caption": "Divergence-based",
                "position": 2114
            },
            {
                "img": "https://arxiv.org/html/2512.01949/fig/relevance/struct.png",
                "caption": "Similarity-based",
                "position": 2119
            },
            {
                "img": "https://arxiv.org/html/2512.01949/fig/relevance/dpp.png",
                "caption": "DPP-based",
                "position": 2124
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ANotation Overview",
        "images": []
    },
    {
        "header": "Appendix BDiversity in Determinantal Point Processes: Geometric Intuition and Theoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix CDetails of experimental setup",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix EAdditional Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01949/x5.png",
                "caption": "Figure E.1:Visualizations of relevance scores and retained tokens.Each visualization illustrates the spatial attention allocated by models to regions corresponding to various textual instructions, demonstrating the capacity of pre-trained multimodal models to identify and focus on task-specific visual elements.",
                "position": 4750
            },
            {
                "img": "https://arxiv.org/html/2512.01949/x6.png",
                "caption": "Figure E.2:Visualizations of relevance scores and retained tokens.Each visualization illustrates the spatial attention allocated by models to regions corresponding to various textual instructions, demonstrating the capacity of pre-trained multimodal models to identify and focus on task-specific visual elements.",
                "position": 4753
            },
            {
                "img": "https://arxiv.org/html/2512.01949/x7.png",
                "caption": "Figure E.3:Visualization of structural redundancy under different thresholds.We show patch-level redundancy maps computed from CLIP-ViT features on images from the COCO dataset. Redundancy is defined as the average cosine similarity between each patch and its spatial neighbors.Redregions indicate high structural redundancy, whileblueregions are more distinctive. As the threshold increases (from left to right: 0.1 to 0.9), the resulting selection becomes increasingly sparse, focusing on structurally salient regions.",
                "position": 4762
            },
            {
                "img": "https://arxiv.org/html/2512.01949/x8.png",
                "caption": "Figure E.4:Error case analysis.Representative failure patterns when pre-trained multimodal models process task-specific queries, comparing query, Graph-Structured Pruning (GSP), Query-Conditioned Semantic Pruning (QCSP), model prediction, and ground truth. (a)Model capability:success when textual guidance suffices (GSP correct, QCSP correct). (b)Textâ€“visual misalignment:errors from limited CLIP encoder alignment (GSP correct, QCSP incorrect). (c)Visual feature deficiency:failures due to inadequate CLIP visual representations, leading to incorrect grounding and relevance (GSP incorrect, QCSP incorrect).",
                "position": 4803
            }
        ]
    },
    {
        "header": "Appendix FLimitations and Future Work.",
        "images": []
    },
    {
        "header": "Appendix GBroader Impacts",
        "images": []
    }
]