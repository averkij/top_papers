[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16863/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16863/x2.png",
                "caption": "Figure 2:Overview ofSimToolReal.(Top) Training in\nSimulation: We train a goal-conditioned RL policy in simulation\nthat manipulates a wide variety of procedurally-generated objects\nto randomly sampled goal poses. (Bottom) Inference in Real: We\ndeploy this policy zero-shot on real-world tools fromDexToolBench, following tool trajectories from human videos.",
                "position": 135
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16863/x3.png",
                "caption": "Figure 3:Real-World Deployment.(Left) Human Video\nProcessing: We collect an RGB-D human video and process it using\nvision foundation models. We use SAM 3D[16]to\ngenerate a metric-scale object mesh and segment a 3D grasp\nbounding box. Then, we use\nFoundationPose[80]to extract a sequence\nof 6D goal poses. (Right) Inference-Time Pipeline: Our LSTM\npolicy takes in proprioception, object pose, grasp bounding box,\nand goal pose, and it outputs joint position targets for the\n29-DoF dexterous robot (arm + hand).",
                "position": 360
            }
        ]
    },
    {
        "header": "IIISimToolReal",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16863/images/final_experiment_plots/verbs_generalization_instance_colors_v2.png",
                "caption": "Figure 4:Generalization to UnseenDexToolBenchTools and Tasks in the Real World.We evaluate our policy in the\nreal world on unseen tool-use tasks inDexToolBench. Our\nevaluations span 24 unique task trajectories across 6 different\nobject categories and 12 object instances. Each bar corresponds to\n1 task trajectory on 1 object instance. We report the averageTask Progressacross 5 rollouts. Despite not being\ntrained on these objects or trajectories, our policy demonstrates\nstrong generalization to diverse tools of varying masses and geometries.",
                "position": 555
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16863/x4.png",
                "caption": "Figure 5:Comparison against Baselines in the Real World.We compareSimToolRealagainst baselines on two variations of\nsweeping a table with a brush: with and without requiring tool\nrotation based on the initial states shown on the left. AverageTask Progressis indicated in parentheses.SimToolRealsucceeds on both variations, performing dexterous in-hand tool\nrotations in the harder variation.Fixed Graspsucceeds\non the simpler variation of this task without tool rotation.\nHowever, when rotation is required, enforcing a fixed grasp\ncauses the arm to collide with the table while tracking the\ntarget trajectory.Kinematic Retargetingfails to reason about contact\nforces, and is unable to grasp the brush in both variations.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2602.16863/images/final_experiment_plots/new_specialist_comparison_3settings_revised.png",
                "caption": "Figure 6:Comparison against Specialists.We compareSimToolRealin simulation against specialist policies\ntrained on a single object (Obj A) and trajectory (Traj A). We\ntrain one specialist policy for each of the 6 object categories inDexToolBenchand report the averageTask Progressacross these categories. While the specialists succeed on their\ntraining setup (Obj A / Traj A), performance degrades under\ndeviation in the trajectory (Obj A / Traj B) or the object (Obj B /\nTraj A).SimToolRealhas high zero-shot performance across all\nvariants, despite not being trained on these objects or trajectories.",
                "position": 846
            },
            {
                "img": "https://arxiv.org/html/2602.16863/images/final_experiment_plots/sim_tp_and_reward_vs_env_steps.png",
                "caption": "Figure 7:Training Objective Drives Generalization.In\nsimulation, we evaluate (Left) the episode reward on\nprocedurally-generated objects and (Right) the zero-shotTask Progresson unseenDexToolBenchtools on\ndifferent policy checkpoints throughout training. The strong\ncorrelation between the two curves validates our core hypothesis:\nimproving random goal-pose reaching performance on diverse object\nprimitives drives corresponding gains in generalization to unseen\ntool-use tasks.",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2602.16863/images/final_experiment_plots/training_reward_vs_env_steps.png",
                "caption": "Figure 8:Ablation of RL Training Components.We compare\nthe training reward across environment steps ofSimToolRealagainst ablations averaged across 5 seeds. Replacing\nSAPG[71]with PPO[66]or not using Asymmetric\nCritic[60]results in a significant\nperformance drop, highlighting their importance.",
                "position": 947
            }
        ]
    },
    {
        "header": "VDiscussion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16863/x5.png",
                "caption": "Figure 9:Object Keypoints.(Left) Visualization of 4 pose\nkeypoints in the local object frame. (Right) Visualization of the\nkeypoint distances used to compute the distanced‚Äã(ùíêt,ùíà)d(\\bm{o}_{t},\\bm{g}).",
                "position": 2174
            },
            {
                "img": "https://arxiv.org/html/2602.16863/x6.png",
                "caption": "Figure 10:Representative Examples ofDexToolBenchTasks.Visual breakdown of representative tasks inDexToolBenchacross the 6 tool categories, highlighting\nthe diversity of objects and manipulation tasks.",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2602.16863/x7.png",
                "caption": "Figure 11:Visualization of Policy Observations during\nReal-World Deployment.(Top) Image frames from a real-world rollout of the brush manipulation task.\n(Bottom) The corresponding visualization of the policy observations\nat each timestep, consisting of the robot state, estimated object\npose, and goal pose (green). When the distance between the current\npose and goal pose is sufficiently small, the goal pose is updated\nto the next pose in the goal sequence. Note that this visualization\nis a rendering of the policy‚Äôs inputs, not a physics-based simulation.",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2602.16863/x8.png",
                "caption": "Figure 12:Metric-Scale Mesh and Grasp Bounding Box Acquisition.We present a semi-automated pipeline to extract a metric-scale\nmesh and grasp bounding box from a single RGB-D video scan.\n(1) We first segment the target object in the initial frame and\nreconstruct a 3D mesh using SAM 3D[16], injecting\nthe captured depth map to ensure metric accuracy.\n(2) To establish a canonical coordinate frame, we virtually\nrender the mesh from multiple views and use SAM\n2[65]to segment the geometry into ‚Äúhandle‚Äù and\n‚Äúhead‚Äù regions based on user prompts.\n(3) The final grasp bounding box is derived from the handle‚Äôs\ngeometry: it is centered on the handle, with the positivexx-axis\noriented toward the head, ensuring consistent alignment with policy training.",
                "position": 2963
            },
            {
                "img": "https://arxiv.org/html/2602.16863/x9.png",
                "caption": "Figure 13:DexToolBenchObjects.(Left): Real-world\nobjects. (Right): SAM 3D[16]generated meshes of\nthese objects.",
                "position": 3340
            },
            {
                "img": "https://arxiv.org/html/2602.16863/x10.png",
                "caption": "Figure 14:Kinematic Retargeting Pipeline.From the RGB-D\nhuman video, we use SAM 2[65]for hand masks and\nHaMeR[58]for hand pose prediction.\nNext, we use ICP registration to align the hand pose prediction\nwith the segmented hand point cloud to obtain accurate 3D hand\nposes. Lastly, we perform IK-based retargeting of the arm and hand\nto match the human wrist pose and fingertip positions.",
                "position": 3707
            },
            {
                "img": "https://arxiv.org/html/2602.16863/x11.png",
                "caption": "Figure 15:Fixed Grasp Baselines.We first use ourSimToolRealpolicy to grasp and lift the object. We then\nattempt to follow the trajectory using a fixed grasp. Option 1\n(Damped Least Squares): Tracks the target poses but causes severe\ncollisions with the table. Option 2 (Collision-Free Trajectory\nOptimization): Avoids collisions but fails to reach the target poses.",
                "position": 3786
            },
            {
                "img": "https://arxiv.org/html/2602.16863/appendix/appendix_images/per_tool_figures/brush_comparison.png",
                "caption": "Figure 16:Detailed Comparison against Specialists.We\nprovide a breakdown of the results in\nFig.6. Each plot comparesSimToolRealto a specialist policy trained on a single\nobject and trajectory (Obj A / Traj A) for a single object category.",
                "position": 3898
            },
            {
                "img": "https://arxiv.org/html/2602.16863/appendix/appendix_images/per_tool_figures/eraser_comparison.png",
                "caption": "",
                "position": 3901
            },
            {
                "img": "https://arxiv.org/html/2602.16863/appendix/appendix_images/per_tool_figures/hammer_comparison.png",
                "caption": "",
                "position": 3903
            },
            {
                "img": "https://arxiv.org/html/2602.16863/appendix/appendix_images/per_tool_figures/marker_comparison.png",
                "caption": "",
                "position": 3904
            },
            {
                "img": "https://arxiv.org/html/2602.16863/appendix/appendix_images/per_tool_figures/screwdriver_comparison.png",
                "caption": "",
                "position": 3906
            },
            {
                "img": "https://arxiv.org/html/2602.16863/appendix/appendix_images/per_tool_figures/spatula_comparison.png",
                "caption": "",
                "position": 3907
            }
        ]
    },
    {
        "header": "Appendix AReward Function Details",
        "images": []
    }
]