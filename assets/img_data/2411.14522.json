[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14522/x1.png",
                "caption": "Figure 1:Overview of GMAI-VL and GMAI-VL-5.5M. (a) illustrates the sources, departments, modalities, task types, and instruction formats of the GMAI-VL-5.5M dataset. (b) Architecture of GMAI-VL, integrating a Vision Encoder, Projector, and Large Language Model. (c) Three-stage training process of GMAI-VL, including shallow alignment, deep alignment, and instruction tuning with corresponding data sizes and training components. The flame symboldenotes the training part, while the snowflake symbolindicates frozen part.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x4.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x5.png",
                "caption": "",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3GMAI-VL-5.5M: A Comprehensive Multimodal Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14522/x6.png",
                "caption": "Figure 2:The prompt-driven data generation pipeline comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g.,<image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotation-guided approach often results in lower-quality outputs.\nFigure with complete prompt and response is provided in Supp. Mat..",
                "position": 368
            }
        ]
    },
    {
        "header": "4GMAI-VL: A General Medical Vision-Language Model",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Expanded Data Generation Example",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14522/x7.png",
                "caption": "Figure 3:The full version of Fig.2in the main text illustrates the complete of data generation pipeline， comparing without-annotation-guided and annotation-guided methods. The annotation-guided approach integrates specific annotation information (e.g.,<image, modality, label, department, bbox [optional]>) to generate high-quality, accurate descriptions, while the without-annotation-guided approach often results in lower-quality outputs.",
                "position": 2819
            }
        ]
    },
    {
        "header": "8Details of GMAI-VL-5.5M",
        "images": []
    },
    {
        "header": "9Training Data Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14522/x8.png",
                "caption": "(a)(a) Modality distribution",
                "position": 2878
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x8.png",
                "caption": "(a)(a) Modality distribution",
                "position": 2881
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x9.png",
                "caption": "(b)(b) Original task distribution",
                "position": 2886
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x10.png",
                "caption": "(c)(c) Department distribution",
                "position": 2892
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x11.png",
                "caption": "(d)(d) Clinical task distribution",
                "position": 2897
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x12.png",
                "caption": "Figure 5:Distribution of all our training data. The inner ring represents major categories, each depicted in a different color. The outer ring corresponds to the subcategories within each major category. The size of each segment is proportional to the amount of data, as indicated in the legend, where the data volume for each subcategory is also provided.",
                "position": 2911
            },
            {
                "img": "https://arxiv.org/html/2411.14522/x13.png",
                "caption": "Figure 6:Diagram of the three-stage training process.",
                "position": 3424
            }
        ]
    },
    {
        "header": "10Model Training Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14522/x14.png",
                "caption": "Figure 7:Examples of our model in various medical tasks, including image description, disease diagnosis, diagnostic report generation, free question answering, etc. The results demonstrate the model’s versatility across different imaging modalities and tasks, with support for both Chinese and English.",
                "position": 3552
            }
        ]
    },
    {
        "header": "11Results",
        "images": []
    }
]