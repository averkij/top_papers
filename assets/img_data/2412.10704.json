[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/university-of-maryland-logo-1.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/722666.png",
                "caption": "",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/log.jpg",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10704/x1.png",
                "caption": "Figure 1:Multi-document QA systems require inferring relevant context from a large volume of unstructured data, inherently making it a more challenging task than single-document QA.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4VisDoMBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10704/x2.png",
                "caption": "Figure 2:VisDoMRAG: Given a set of documents, VisDoMRAG parallelly performs evidence-driven➊Visual RAG and➋Textual RAG, prompting the LLMs to answer a query based on the respective retrieved context via Evidence Curation and Chain-of-Thought reasoning. The reasoning chains, and answers from the text and visual pipeline are ensembled together via➌Modality Fusion, where the outputs of both the modalities are aligned using consistency analysis on their reasoning chain to arrive at the final answer.",
                "position": 458
            }
        ]
    },
    {
        "header": "5VisDoMRAG",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10704/x3.png",
                "caption": "(a)PaperTab",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x3.png",
                "caption": "(a)PaperTab",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x4.png",
                "caption": "(b)FetaTab",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x5.png",
                "caption": "(c)SciGraphQA",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x6.png",
                "caption": "(d)SPIQA",
                "position": 523
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": []
    },
    {
        "header": "7Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10704/x7.png",
                "caption": "(a)Long Context",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x7.png",
                "caption": "(a)Long Context",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x8.png",
                "caption": "(b)VisDoMRAG",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x9.png",
                "caption": "Figure 5:Qualitative example from the PaperTab dataset, comparingVisDoMRAGwith Unimodal RAG strategies.",
                "position": 836
            }
        ]
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "10Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/feta_tab_num_docs_distribution.png",
                "caption": "Figure 6:Distribution of pages per query for FetaTab.",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/paper_tab_num_docs_distribution.png",
                "caption": "Figure 7:Distribution of pages per query for PaperTab.",
                "position": 1713
            },
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/spiqa_num_docs_distribution.png",
                "caption": "Figure 8:Distribution of pages per query for SPIQA.",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/scigraphqa_num_docs_distribution.png",
                "caption": "Figure 9:Distribution of pages per query for SciGraphQA.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2412.10704/extracted/6068926/figures/slidevqa_num_docs_distribution.png",
                "caption": "Figure 10:Distribution of pages per query for SlideVQA.",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x10.png",
                "caption": "Figure 11:Qualitative example from the PaperTab dataset, comparing VisDoMRAG with unimodal RAG strategies, with Qwen2VL as the base LLM.",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x11.png",
                "caption": "Figure 12:Qualitative example from the FetaTab dataset, comparing VisDoMRAG with unimodal RAG strategies, with Gemini as the base LLM.",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x12.png",
                "caption": "Figure 13:Qualitative example from the ScigraphQA dataset, comparing VisDoMRAG with unimodal RAG strategies, with Qwen2VL as the base LLM.",
                "position": 1805
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x13.png",
                "caption": "Figure 14:Qualitative example from the SPIQA dataset, comparing VisDoMRAG with unimodal RAG strategies, with ChatGPT4o as the base LLM.",
                "position": 1808
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x14.png",
                "caption": "Figure 15:Qualitative example from the SlideVQA dataset, comparing VisDoMRAG with unimodal RAG strategies, with ChatGPT4o as the base LLM.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x15.png",
                "caption": "Figure 16:Prompt Template used for Query Augmentation.",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x16.png",
                "caption": "Figure 17:Prompt Template used for Unimodal RAG and Long Context experiments.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x17.png",
                "caption": "Figure 18:Prompt Template used for VisDoMRAG.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2412.10704/x18.png",
                "caption": "Figure 19:Brief of Reviewer Instructions, including the Evaluation Rubric.",
                "position": 2008
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]