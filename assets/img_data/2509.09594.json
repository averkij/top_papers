[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09594/x1.png",
                "caption": "Figure 1:Tasks: Each column shows a topdown view with the prior experience (map) trajectory displayed as a purple path from the purple circle (start) to green point (goal). The tasks are referred to as following:Imitatewhich is akin to teach-and-repeat;Alt-Goal, where the goal object is previously seen but unvisited;Shortcut, where the prior trajectory is made longer for agent to take a shortcut during inference; andReverse, where agent travels in the opposite direction.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09594/x2.png",
                "caption": "Figure 2:Object-Relative Navigation Pipeline.a) Mapping:We construct a topometric map as arelative3D scene graph, where image segments are used as object nodes, which are connected intra-image using 3D Euclidean distances and inter-image using object association.b) Execution:Given the map, we localize each of the query objects and compute its path to the goal node; we assign these path lengths to the object’s segmentation mask, forming a “WayObject Costmap” for control prediction.c) Training:We train a model to learn an “ObjectReact” controller that predicts trajectory rollouts from WayObject Costmaps.",
                "position": 159
            }
        ]
    },
    {
        "header": "3Approach",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09594/x3.png",
                "caption": "Figure 3:Examples of demonstration videos. Real-world demonstration video example (left) and simulator video example (right).*The localized image is the closest match found in the map (see SectionA.4for details).",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2509.09594/x4.png",
                "caption": "Figure 4:Real-world Experiments. We deploy our approach on the Unitree Go1 robot dog[86]. Here, we show egocentric RGB images, their corresponding WayObject Costmaps, and the predicted trajectory rollout at several timesteps during autonomous navigation to the goal object.\nAtt=5​st=5s, the policy chooses to turn left towards a region of lower-cost objects. Att=20​st=20s, it successfully navigates around an obstacle (here visible as a region of low-cost). Finally, att=45​st=45s, it moves towards the final goal, subsequently succeeding at reaching the goal object.",
                "position": 705
            }
        ]
    },
    {
        "header": "Appendix DLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09594/x5.png",
                "caption": "Figure 5:An illustration of the effect of dynamic objects and occlusions on the predicted control signal.",
                "position": 745
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]