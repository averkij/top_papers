[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20278/x1.png",
                "caption": "Figure 3:Left: Percentage of covered ID data depending onkğ‘˜kitalic_kvalues and dataset size (Nğ‘Nitalic_N), for2-Hoptask (|ğ’³|=50ğ’³50|{\\mathcal{X}}|=50| caligraphic_X | = 50).Right: Test accuracy depending onkğ‘˜kitalic_k-cutoff values for2-Hoptask (|ğ’³|ğ’³|{\\mathcal{X}}|| caligraphic_X |=50,Nğ‘Nitalic_N=10k).\nEach line represents a different training checkpoint.\nNote that OOD accuracy remains at chance level (=1/50) regardless of training time.\nThe bars below show the number of test data for eachkğ‘˜kitalic_k-cutoff value.",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x2.png",
                "caption": "",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x3.png",
                "caption": "Figure 4:Left: Heatmap of Intra-Inter Cosine Gap (IICG) across layers and positions, sliced bykğ‘˜kitalic_k-cutoff. Higher IICG values indicate stronger clustering of representations that share the same intermediate state.\nThe positions with the highest IICG values are marked with squares.Right: PCA visualization of latent representations at positionx2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTand layer 3. Datapoints are classified by their intermediate statesb=f1â¢(x1,x2)ğ‘subscriptğ‘“1subscriptğ‘¥1subscriptğ‘¥2b=f_{1}(x_{1},x_{2})italic_b = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ).",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x4.png",
                "caption": "",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x5.png",
                "caption": "Figure 5:Left:Log-log plot of measuredN^reqsubscript^ğ‘req\\hat{N}_{\\mathrm{req}}over^ start_ARG italic_N end_ARG start_POSTSUBSCRIPT roman_req end_POSTSUBSCRIPTvs. token set size (|ğ’³|ğ’³|{\\mathcal{X}}|| caligraphic_X |) across three compositional tasks. The slopecğ‘citalic_ccorresponds to the empirical power-law scaling exponent.\nOmitted points for3-Hopare due to prohibitively large dataset requirements.Right:Power-law scaling remains invariant across GPT-2 model sizes (68M to 1.5B parameters) for2-Hoptask.R2>0.99superscriptğ‘…20.99R^{2}>0.99italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT > 0.99for all linear fitting.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x6.png",
                "caption": "",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x7.png",
                "caption": "Figure 6:Left: InNon-treetask, the coverage principle predicts that representations of input subsequences with the same intermediate stateb=f1â¢(x1,x2)ğ‘subscriptğ‘“1subscriptğ‘¥1subscriptğ‘¥2b=f_{1}(x_{1},x_{2})italic_b = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )split into multiple context-dependent state representations, conditioned onx2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTvalue.Middle: ID test accuracy after standard training (100 epochs post training accuracy>>>0.99), showingNon-treetask significantly underperforms2-Hoptask (|ğ’³|=50ğ’³50|{\\mathcal{X}}|=50| caligraphic_X | = 50).Right: IICG heatmap from a model that achieved near-perfect ID accuracy (0.96) after extended training (36k epochs,|ğ’³|=50ğ’³50|{\\mathcal{X}}|=50| caligraphic_X | = 50,N=50â¢kğ‘50kN=50\\text{k}italic_N = 50 k).",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x8.png",
                "caption": "",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x9.png",
                "caption": "Figure 7:Left:Power-law scaling of required dataset size vs. token set size for tasks with CoT supervision.R2>0.98superscriptğ‘…20.98R^{2}>0.98italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT > 0.98for all linear fits.Middle:Comparison of ID test Accuracy ofNon-treetask (|ğ’³|=50ğ’³50|{\\mathcal{X}}|=50| caligraphic_X | = 50) with and without CoT supervision (100 epochs post training accuracy>>>0.99).Right:IICG score comparison forNon-treeand2-Hoptask with CoT supervision (|ğ’³|=50ğ’³50|\\mathcal{X}|=50| caligraphic_X | = 50,N=10â¢kğ‘10kN=10\\text{k}italic_N = 10 k).\nThe scores are measured at each layer of intermediate state positionbğ‘bitalic_b, based on two grouping strategies:bğ‘bitalic_band(b,x2)ğ‘subscriptğ‘¥2(b,x_{2})( italic_b , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ). Models are trained for 100 epochs after reaching training accuracy>>>0.99.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x10.png",
                "caption": "",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x11.png",
                "caption": "",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x12.png",
                "caption": "Figure 8:Causal tracing results for the2-Hoptask across differentkğ‘˜kitalic_k-cutoff values, showing Indirect Effect (IE) scores at each layer and position.",
                "position": 2816
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x13.png",
                "caption": "Figure 9:IICG heatmap across different token set sizes, showing consistent representation clustering patterns.",
                "position": 2826
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x14.png",
                "caption": "Figure 10:Causal tracing results showing indirect effect heatmaps for different token set sizes|ğ’³|ğ’³|{\\mathcal{X}}|| caligraphic_X |.",
                "position": 2829
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x15.png",
                "caption": "Figure 11:IICG heatmap forParallel-2-Hoptask with grouping strategies based onb1=f1â¢(x1,x2)subscriptğ‘1subscriptğ‘“1subscriptğ‘¥1subscriptğ‘¥2b_{1}=f_{1}(x_{1},x_{2})italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )(Left),b2=f2â¢(x3,x4)subscriptğ‘2subscriptğ‘“2subscriptğ‘¥3subscriptğ‘¥4b_{2}=f_{2}(x_{3},x_{4})italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT )(Middle), andt=f3â¢(b1,b2)ğ‘¡subscriptğ‘“3subscriptğ‘1subscriptğ‘2t=f_{3}(b_{1},b_{2})italic_t = italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )(Right).",
                "position": 2842
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x16.png",
                "caption": "Figure 12:Causal tracing results showing indirect effect heatmap forParallel-2-Hoptask.Left:perturbation with different(x1,x2)subscriptğ‘¥1subscriptğ‘¥2(x_{1},x_{2})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )pair leading to a differentb1subscriptğ‘1b_{1}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTvalue.Right:perturbation with different(x3,x4)subscriptğ‘¥3subscriptğ‘¥4(x_{3},x_{4})( italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT )pair leading to a differentb2subscriptğ‘2b_{2}italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTvalue.",
                "position": 2845
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x17.png",
                "caption": "Figure 13:IICG heatmap for3-Hoptask with grouping strategies based onb1=f1â¢(x1,x2)subscriptğ‘1subscriptğ‘“1subscriptğ‘¥1subscriptğ‘¥2b_{1}=f_{1}(x_{1},x_{2})italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )(Left),b2=f2â¢(b1,x3)subscriptğ‘2subscriptğ‘“2subscriptğ‘1subscriptğ‘¥3b_{2}=f_{2}(b_{1},x_{3})italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT )(Middle), andt=f3â¢(b2,x4)ğ‘¡subscriptğ‘“3subscriptğ‘2subscriptğ‘¥4t=f_{3}(b_{2},x_{4})italic_t = italic_f start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT )(Right).",
                "position": 2851
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x18.png",
                "caption": "Figure 14:Causal tracing results showing indirect effect heatmap for3-Hoptask.Left:perturbation with different(x1,x2)subscriptğ‘¥1subscriptğ‘¥2(x_{1},x_{2})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )pair leading to differentb1subscriptğ‘1b_{1}italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTvalue.Right:perturbation leading to differentb2=f2â¢(b1,x3)subscriptğ‘2subscriptğ‘“2subscriptğ‘1subscriptğ‘¥3b_{2}=f_{2}(b_{1},x_{3})italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT )value.",
                "position": 2854
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x19.png",
                "caption": "Figure 15:Robustness of power-law scaling relationship to hyperparameter variations in the2-Hoptask with|ğ’³|=50ğ’³50|\\mathcal{X}|=50| caligraphic_X | = 50. Each line shows the training and test accuracy curves for a different configuration: (1) baseline, (2) reduced learning rate (4e-4, half of baseline), (3) reduced weight decay (0.01, one-tenth of baseline), and (4) changed generalization criteria (test accuracy > 0.95 within 10 epochs after training accuracy > 0.95).R2>0.99superscriptğ‘…20.99R^{2}>0.99italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT > 0.99for all linear fitting.",
                "position": 3434
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x20.png",
                "caption": "Figure 16:Coverage analysis forNon-treetask with|ğ’³|=50ğ’³50|\\mathcal{X}|=50| caligraphic_X | = 50. The graph shows the percentage of ID test data covered at differentkğ‘˜kitalic_kvalues across various dataset sizes (Nğ‘Nitalic_N). Compared to the2-Hoptask (Fig.3, left),Non-treehas significantly lower coverage at equivalent dataset sizes, indicating that path ambiguity impedes the formation of functional equivalence relationships.",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x21.png",
                "caption": "Figure 17:ID test accuracy comparison between GPT-2 (96M parameters) and GPT-2-XL (1.5B parameters) on theNon-treetask with|ğ’³|=50ğ’³50|\\mathcal{X}|=50| caligraphic_X | = 50, measured 100 epochs after training accuracy exceeds 0.99. Despite the 15x increase in parameter count, the accuracy does not increase.",
                "position": 3487
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x22.png",
                "caption": "Figure 18:Causal tracing analysis for theNon-treemodel after extended training. The heatmap shows indirect effect values across different layer-token positions.Left: perturbation leading to different intermediate stateb=f1â¢(x1,x2)ğ‘subscriptğ‘“1subscriptğ‘¥1subscriptğ‘¥2b=f_{1}(x_{1},x_{2})italic_b = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ).Middle: samebğ‘bitalic_bbut differentx2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.Right: differentbğ‘bitalic_bandx2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.",
                "position": 3500
            },
            {
                "img": "https://arxiv.org/html/2505.20278/x23.png",
                "caption": "Figure 19:MRR scores for intermediate state representations projected to vocabulary space.Left:Standard training (f1â‰ f2subscriptğ‘“1subscriptğ‘“2f_{1}\\neq f_{2}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â‰  italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, no partial computation) shows very high MRR regardless of position and layer.Right:Training with partial computation (f1=f2subscriptğ‘“1subscriptğ‘“2f_{1}=f_{2}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, with partial examples) shows MRR of 0 in layers 3 to 8 at positionx2subscriptğ‘¥2x_{2}italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, indicating strong vocabulary alignment.",
                "position": 3663
            }
        ]
    },
    {
        "header": "Contents of the Appendix",
        "images": []
    }
]