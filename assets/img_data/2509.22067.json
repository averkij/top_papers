[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22067/x1.png",
                "caption": "Figure 1:Side effects of activation steering.The left panel demonstrates the model’s default behavior, showing appropriate and safe responses to both neutral and harmful prompts. The right panel shows the effect of injecting a single steering vector (enhancing a benign ”France” concept). This intervention not only introduces a thematic bias in neutral contexts but also critically bypasses safety safeguards, compelling the model to comply with harmful requests it would normally refuse.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22067/x2.png",
                "caption": "(a)Compliance Rate for random steering on different model families",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x2.png",
                "caption": "(a)Compliance Rate for random steering on different model families",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x3.png",
                "caption": "(b)Compliance Rate for random steering on different model layers",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x4.png",
                "caption": "(c)Compliance Rate for random steering vs SAE feature steering",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x5.png",
                "caption": "Figure 3:Compliance Rate across JailbreakBench categories.Bars indicate the average Compliance Rate for each harmful prompt category, with the “Overall” bar representing the average across all categories. Results show consistently non-zero compliance rates across all categories, with notably high overall values. For instance, Llama3-8B exhibits a 17% jailbreak success probability when both the harmful prompt and steering vector are randomly sampled.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x6.png",
                "caption": "(a)Histogram of number of jailbroken prompts per SAE feature",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x6.png",
                "caption": "(a)Histogram of number of jailbroken prompts per SAE feature",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x7.png",
                "caption": "(b)Cross-category generalization heatmap",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x8.png",
                "caption": "Figure 6:Performance of universal attack vector.For each model, bars show the average Compliance Rate for: random vectors (left), bomb-prompt jailbreak vectors (middle), and their average (right). The universal attack vector increases the average CR by 4× on unseen JailbreakBench prompts compared to random vectors, though effectiveness varies substantially across model families.",
                "position": 377
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AJailbreakBench Prompt Examples",
        "images": []
    },
    {
        "header": "Appendix BJudge Prompt and Quality Assessment",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results: Generalizability Across Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22067/x9.png",
                "caption": "(a)Llama3-8B: Jailbroken prompts per random vector",
                "position": 1484
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x9.png",
                "caption": "(a)Llama3-8B: Jailbroken prompts per random vector",
                "position": 1487
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x10.png",
                "caption": "(b)Llama3-8B: Cross-category generalization",
                "position": 1492
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x11.png",
                "caption": "(c)Qwen2.5-7B: Jailbroken prompts per random vector",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2509.22067/x12.png",
                "caption": "(d)Qwen2.5-7B: Cross-category generalization",
                "position": 1503
            }
        ]
    },
    {
        "header": "Appendix DSupplementary API-Steered Responses for Benign SAE Feature",
        "images": []
    }
]