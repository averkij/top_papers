[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08492/x1.png",
                "caption": "Figure 1:Text provides complementary information beyond images, even when not paired directly; We introduceUnpairedMultimodalLearner(Uml) whereby sharing model weights across modalities (e.g., image and text) extracts synergies and enhances unimodal representations, outperforming methods that rely only on a single modality (such as images above).",
                "position": 161
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Paired Multimodal Representation Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08492/x2.png",
                "caption": "Figure 2:(a) Paired learning uses(xi,yi){(x_{i},y_{i})}with known correspondences. We instead study Unpaired learning: (b) with labels, using(xi,ci){(x_{i},c_{i})}and(yj,c^j){(y_{j},\\hat{c}_{j})}, wherecic_{i}andc^j\\hat{c}_{j}denote labels forxix_{i}andyjy_{j}, but no cross-modal correspondences; and (c) without any labels or correspondences, using{xi}\\{x_{i}\\}and{yj}\\{y_{j}\\}.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Unpaired Multimodal Representation Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08492/x3.png",
                "caption": "Figure 3:Adding unpairedYYsamples boostsXXreconstruction more than adding extraXXsamples.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x4.png",
                "caption": "Figure 4:(Left) Inputs from different modalities (e.g., images or text) are tokenized into patch or token embeddings using pretrained encoders or processed features; (Right)Umlcan be trained under two settings: (a) Self-supervised, where patch/token embeddings are passed through a shared network and modality-specific decoders to perform next-token/patch prediction; (b) Supervised, where mean/CLS embeddings are fed through the shared classifier to predict labels within each modality.",
                "position": 327
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08492/x5.png",
                "caption": "Figure 5:Our approachUmlis much more robust than its unimodal counterpart across four test time distribution shifted target test sets. All results are averaged across three random seeds.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x6.png",
                "caption": "Figure 6:Uml(Ours) improves audio classification using unpaired image and text samples on ImageNet-ESC-19 and ImageNet-ESC-27. All results are averaged across three random seeds.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x7.png",
                "caption": "Figure 7:Image classifier trained from BERT initialization outperforms training from scratch for (left) full fine-tuning; (right) frozen backbone",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x8.png",
                "caption": "",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2510.08492/updated_figs/isoplot_pets_clip_rn50_ylim5.png",
                "caption": "Figure 8:1 img≈\\approx228 words (CLIP)",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2510.08492/updated_figs/isoplot_pets_clip_rn50_ylim5.png",
                "caption": "Figure 8:1 img≈\\approx228 words (CLIP)",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2510.08492/updated_figs/isoplot_pets_dino_vits_ylim5.png",
                "caption": "Figure 9:1 img≈\\approx1034 words (DINOv2)",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x9.png",
                "caption": "Figure 10:Existence of Multimodal Neurons without Paired Supervision.Bars show overall (green), correlations conditioned onlabel=1 (sarcastic; blue), and correlations conditioned onlabel=0 (non-sarcastic; pink) between visual and textual activations for a subset of neurons. Most neurons exhibit strong cross-modal correlation and specifically higher alignment for non-sarcastic samples, where visual and verbal cues are naturally congruent.",
                "position": 783
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x10.png",
                "caption": "Figure 11:Umlincreasingly infers cross-modal correspondences with training, without paired supervision.",
                "position": 789
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "6Reproducibility Statement",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFurther Related Works",
        "images": []
    },
    {
        "header": "Appendix BSupplementary Experimental Details and Assets Disclosure",
        "images": []
    },
    {
        "header": "Appendix CProofs of Theoretical Results",
        "images": []
    },
    {
        "header": "Appendix DUML: Algorithm Pseudocode",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08492/x11.png",
                "caption": "Figure 12:Robustness under test-time distribution shifts.Our approach (trained on 1-shot) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets.",
                "position": 6498
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x12.png",
                "caption": "Figure 13:Robustness under test-time distribution shifts.Our approach (trained on 2-shots) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets.",
                "position": 6501
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x13.png",
                "caption": "Figure 14:Robustness under test-time distribution shifts.Our approach (trained on 4-shots) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets.",
                "position": 6504
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x14.png",
                "caption": "Figure 15:Robustness under test-time distribution shifts.Our approach (trained on 8-shots) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets.",
                "position": 6507
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_sun397_dino_vits.png",
                "caption": "Figure 16:SUN397.1 img≈\\approx1568 words",
                "position": 6521
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_sun397_dino_vits.png",
                "caption": "Figure 16:SUN397.1 img≈\\approx1568 words",
                "position": 6524
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_caltech_dino_vits.png",
                "caption": "Figure 17:Caltech101.1 img≈\\approx1248 words",
                "position": 6530
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_cars_dino_vits.png",
                "caption": "Figure 18:Stanford Cars.1 img≈\\approx1799 words",
                "position": 6536
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_cars_dino_vits.png",
                "caption": "Figure 18:Stanford Cars.1 img≈\\approx1799 words",
                "position": 6539
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_dtd_dino_vits.png",
                "caption": "Figure 19:DTD.1 img≈\\approx2309 words",
                "position": 6545
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_fgvc_dino_vits.png",
                "caption": "Figure 20:FGVC Aircraft.1 img≈\\approx3220 words",
                "position": 6551
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_fgvc_dino_vits.png",
                "caption": "Figure 20:FGVC Aircraft.1 img≈\\approx3220 words",
                "position": 6554
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_flowers_dino_vits.png",
                "caption": "Figure 21:Oxford Flowers.1 img≈\\approx1895 words",
                "position": 6560
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_food101_dino_vits.png",
                "caption": "Figure 22:Food101.1 img≈\\approx2608 words",
                "position": 6566
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_food101_dino_vits.png",
                "caption": "Figure 22:Food101.1 img≈\\approx2608 words",
                "position": 6569
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_ucf101_dino_vits.png",
                "caption": "Figure 23:UCF101.1 img≈\\approx2617 words",
                "position": 6575
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_sun397_clip.png",
                "caption": "Figure 24:SUN397.1 img≈\\approx221 words",
                "position": 6585
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_sun397_clip.png",
                "caption": "Figure 24:SUN397.1 img≈\\approx221 words",
                "position": 6588
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_caltech101_clip.png",
                "caption": "Figure 25:Caltech101.1 img≈\\approx256 words",
                "position": 6594
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_cars_clip.png",
                "caption": "Figure 26:Stanford Cars.1 img≈\\approx649 words",
                "position": 6600
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_cars_clip.png",
                "caption": "Figure 26:Stanford Cars.1 img≈\\approx649 words",
                "position": 6603
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_dtd_clip.png",
                "caption": "Figure 27:DTD.1 img≈\\approx228 words",
                "position": 6609
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_fgvc_clip.png",
                "caption": "Figure 28:FGVC Aircraft.1 img≈\\approx691 words",
                "position": 6615
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_fgvc_clip.png",
                "caption": "Figure 28:FGVC Aircraft.1 img≈\\approx691 words",
                "position": 6618
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_flowers_clip.png",
                "caption": "Figure 29:Oxford Flowers.1 img≈\\approx851 words",
                "position": 6624
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_food101_clip.png",
                "caption": "Figure 30:Food101.1 img≈\\approx202 words",
                "position": 6630
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_food101_clip.png",
                "caption": "Figure 30:Food101.1 img≈\\approx202 words",
                "position": 6633
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/appendix_plots/isoplot_ucf101_clip.png",
                "caption": "Figure 31:UCF101.1 img≈\\approx393 words",
                "position": 6639
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x15.png",
                "caption": "Figure 32:Few-shot classification accuracy on SUN397 usingUmlwith unpaired, frozen embeddings from various pretrained language models.",
                "position": 6660
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x16.png",
                "caption": "Figure 33:Few‐shot SUN397 accuracy withUmlusing two levels of textual granularity: (a) vanilla class descriptions and (b) GPT-3–generated fine-grained descriptions.",
                "position": 6670
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x16.png",
                "caption": "Figure 33:Few‐shot SUN397 accuracy withUmlusing two levels of textual granularity: (a) vanilla class descriptions and (b) GPT-3–generated fine-grained descriptions.",
                "position": 6673
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x17.png",
                "caption": "Figure 34:Few‐shot SUN397 accuracy withUml(init) using two levels of textual granularity: (a) vanilla class descriptions and (b) GPT-3–generated fine-grained descriptions.",
                "position": 6679
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x18.png",
                "caption": "Figure 35:Classification accuracy as a function of the number of text prompts per image shot for the SUN397 Dataset.",
                "position": 6692
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x19.png",
                "caption": "",
                "position": 6701
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x20.png",
                "caption": "",
                "position": 6707
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x21.png",
                "caption": "",
                "position": 6712
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x22.png",
                "caption": "",
                "position": 6718
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x23.png",
                "caption": "Figure 36:Umlimproves image classification using unpaired audio and text samples on both\nImageNet-ESC-19 and ImageNet-ESC-27 benchmarks when trained on top of DINOv2 VIT-S/14 and OpenLLaMa-3B.",
                "position": 7135
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x24.png",
                "caption": "Figure 37:Umlimproves image classification using unpaired audio and text samples on both\nImageNet-ESC-19 and ImageNet-ESC-27 benchmarks when trained on top of CLIP ResNet-50 image and text encoders",
                "position": 7143
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x25.png",
                "caption": "Figure 38:Umlimproves audio classification using unpaired image and text samples on both\nImageNet-ESC-19 and ImageNet-ESC-27 benchmarks when trained on top of CLIP ResNet-50 image and text encoders",
                "position": 7151
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x26.png",
                "caption": "Figure 39:Training onN/2N/2samples fromXXandN/2N/2unpaired samples fromYYimproves test reconstruction onXX, more than training onNNsamples fromXX.",
                "position": 7189
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x27.png",
                "caption": "Figure 40:Evolution of cross-modal correlation during unpaired co-training. Each curve shows the correlation between visual and textual activations over training epochs for sarcastic (label=1) and non-sarcastic (label=0) samples. Most neurons develop positive correlations for non-sarcastic content, while others exhibit negative correlations, suggesting functional specialization for multimodal alignment and the detection of incongruence.",
                "position": 7206
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x28.png",
                "caption": "",
                "position": 7215
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x29.png",
                "caption": "",
                "position": 7221
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x30.png",
                "caption": "",
                "position": 7226
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x31.png",
                "caption": "",
                "position": 7232
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x32.png",
                "caption": "",
                "position": 7237
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x33.png",
                "caption": "",
                "position": 7243
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x34.png",
                "caption": "",
                "position": 7248
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x35.png",
                "caption": "",
                "position": 7254
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x36.png",
                "caption": "Figure 41:Impact of unpaired text on decision boundaries (CLIP ResNet50).(Left) Visual features alone learn ambiguous class boundaries between Russian Blue and Abyssinian cats. (Right) Adding unpaired text sharpens the boundary, leveraging semantic cues to better distinguish similar categories.",
                "position": 7267
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x37.png",
                "caption": "Figure 42:Impact of unpaired text on decision boundaries (CLIP ResNet50).(Left) Visual features alone learn ambiguous class boundaries between knitted and cobwebbed. (Right) Adding unpaired text sharpens the boundary, leveraging semantic cues to better distinguish similar categories",
                "position": 7270
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x38.png",
                "caption": "Figure 43:Impact of unpaired text on decision boundaries (CLIP ResNet50).(Left) Visual features alone learn ambiguous class boundaries between ball moss and passion flower. (Right) Adding unpaired text sharpens the boundary, leveraging semantic cues to better distinguish similar categories",
                "position": 7273
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x39.png",
                "caption": "Figure 44:Functional margin of the linear head trained on SUN397 dataset for few-shot classification significantly increases when training with bothUmlandUmlwith linear head initialization.",
                "position": 7295
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x40.png",
                "caption": "Figure 45:Silhouette Score of the linear head trained on SUN397 dataset for few-shot classification significantly increases when training with bothUmlandUmlwith linear head initialization.",
                "position": 7301
            },
            {
                "img": "https://arxiv.org/html/2510.08492/x41.png",
                "caption": "Figure 46:DB-Index of the linear head trained on SUN397 dataset for few-shot classification significantly improves when training with bothUmlandUmlwith linear head initialization.",
                "position": 7304
            },
            {
                "img": "https://arxiv.org/html/2510.08492/figs/plots/class_prototypes_sun397_dinov2.png",
                "caption": "Figure 47:Inner products between each linear-head weight vector and its class’s mean text embedding, demonstrating that text features align well with class prototypes.",
                "position": 7310
            }
        ]
    },
    {
        "header": "Appendix FAnalysis of the Learned Predictor",
        "images": []
    }
]