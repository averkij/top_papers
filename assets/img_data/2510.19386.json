[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19386/x1.png",
                "caption": "Figure 1:An overview of how the OS agent interacts with both the environment and the user.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19386/x2.png",
                "caption": "Figure 2:Our two-stage training framework for developing a powerful GUI model. The process begins withStep-wise Reinforcement Learning (left)to optimize the agentâ€™s core decision-making abilities on constructed training data. It then progresses toSelf-evolving Training (right), a iterative loop that generates high-quality trajectory data to enable continuous improvement.",
                "position": 170
            }
        ]
    },
    {
        "header": "3Agent Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19386/fig/error_pie3.png",
                "caption": "Figure 3:Error type distribution of the single agent.",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2510.19386/x3.png",
                "caption": "Figure 4:The architecture of our multi-agent framework. It consists of a central Execution module supported by three key components: the Task Orchestration module decomposes complex user instructions into manageable atomic tasks and handles memory transfer; the Knowledge Retrieval module provides relevant external information; and the Hierarchical Reflection module delivers multi-level feedback for error correction.",
                "position": 400
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19386/x4.png",
                "caption": "Figure 5:Training reward dynamics of different models.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2510.19386/x4.png",
                "caption": "Figure 5:Training reward dynamics of different models.",
                "position": 671
            }
        ]
    },
    {
        "header": "5From Tool to Partner: Building Warm OS Agents Beyond Task Execution",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19386/fig/warm.png",
                "caption": "Figure 6:Comparison between a normal agent and a warm OS agent. The normal agent only follows the ordering instructions but may not select the burger type that aligns with human intent, whereas the warm OS agent accurately chooses the type of burger that the human actually wants.",
                "position": 735
            }
        ]
    },
    {
        "header": "6Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAction Space",
        "images": []
    },
    {
        "header": "Appendix BDetails of Each Discriminator",
        "images": []
    }
]