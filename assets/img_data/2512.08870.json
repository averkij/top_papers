[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08870/x1.png",
                "caption": "Figure 1:Motivation. Static federated methods limit agent adaptation. While directly introducing online learning into FL suffers from high variance and gradient conflicts. Fed-SE resolves this by stabilizing learning via trajectory filtering and robust subspace aggregation.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08870/x2.png",
                "caption": "Figure 2:Overview of the Fed-SE Framework.The framework operates through two distinct phases: local agent self-evolution and global knowledge aggregation. Parallel client agents interact with diverse environments to optimize local low-rank adapters (LoRA) using filtered successful trajectories stored in privacy-preserving experience buffers. The central server aggregates these distributed adapter parameters to construct a global model with generalized reasoning capabilities, which is subsequently synchronized across all clients for the next communication round.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Preliminaries and Problem Setup",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08870/img/compare.png",
                "caption": "Figure 3:Comparative performance evolution across heterogeneous tasks.The plots illustrate the test success rate trajectories over 20 communication rounds. The solid curves represent the continuous improvement of federated methods (Fed-SE and FedAvg), while the horizontal dashed lines indicate the converged baseline performance of static approaches (Local and Centralized). Fed-SE (Blue) exhibits robust growth, consistently breaking the performance ceilings of static baselines and significantly outperforming FedAvg (Red), particularly in complex reasoning environments like Maze.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2512.08870/img/ablation.png",
                "caption": "Figure 4:Impact of Key Components on Final Performance.Removing the success filter results in a catastrophic performance drop (-26%), while excluding history or using weighted averaging also degrades the robust baseline (66%).",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2512.08870/img/ablation_maze.png",
                "caption": "(a)Performance Evolution on Maze",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2512.08870/img/ablation_maze.png",
                "caption": "(a)Performance Evolution on Maze",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2512.08870/img/ablation_wordle.png",
                "caption": "(b)Performance Evolution on Wordle",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2512.08870/img/lora_rank.png",
                "caption": "Figure 6:Trade-off between model performance and communication cost across different LoRA ranks.",
                "position": 587
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Appendix ADerivation of the Surrogate Objective",
        "images": []
    }
]