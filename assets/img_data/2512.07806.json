[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07806/main_figure/teaser.jpg",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07806/main_figure/main_arch.png",
                "caption": "Figure 2:Architecture Overview. Given tokenized inputs, our model applies a three stage hierarchy of alternating attention blocks, varying in both self-attention coverage and token resolution. A Pyramidal Feature Aggregation module fuses the outputs from all stages, which are then passed to a final head for dense prediction.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2512.07806/main_figure/main_qual.jpg",
                "caption": "Figure 3:Qualitative results on the DL3DV (top two rows), Tanks&Temples (third row), and Mip-NeRF360 (bottom row). For a fair and reliable comparison, we evaluate all methods with 32 input views, matching the training setup used for other feed-forward baselines.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07806/main_figure/re10k_error.jpg",
                "caption": "Figure 4:Qualitative results on the 4-view RE10K dataset.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2512.07806/main_figure/attention.jpg",
                "caption": "Figure 5:Attention visualization. For colored query patches (red,yellow,green) in the reference view, we highlight top-3 attended tokens: on the left, tokens attended within the group (blue overlay), and on the right, tokens attended within and outside the group (green overlay).",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2512.07806/x1.png",
                "caption": "(a)",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2512.07806/x1.png",
                "caption": "(a)",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2512.07806/x2.png",
                "caption": "(b)",
                "position": 960
            }
        ]
    },
    {
        "header": "5Ablation and Analysis",
        "images": []
    },
    {
        "header": "6Conclusion and discussions",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/time_sup.jpg",
                "caption": "Figure 7:Inference time comparison.",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/att_sup.jpg",
                "caption": "Figure 8:Attention visualization. For colored query patches (red,yellow,green) in the reference view, we highlight top-3 attended tokens: on the left, tokens attended within the group (blue overlay), and on the right, tokens attended within and outside the group (green overlay).",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/re10k_sup.jpg",
                "caption": "Figure 9:Qualitative results on the 4-view RE10K dataset. Per-pixel error maps are shown in the bottom-right corner of each image.",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/sup_dl3dv_qual.jpg",
                "caption": "Figure 10:Qualitative results on the DL3DV-Benchmark across varying input view counts (128, 64, 32, and 16). The rows are arranged in descending order of view count, with two rows displayed for each setting.",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/sup_dl3dv_eval_qual.jpg",
                "caption": "Figure 11:Qualitative results on the DL3DV-Evaluation across varying input view counts (128, 64, 32, and 16). The rows are arranged in descending order of view count, with two rows displayed for each setting.",
                "position": 1546
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/sup_32v_mip_tnt_qual.jpg",
                "caption": "Figure 12:Qualitative results on the Mip-NeRF360 (top three rows), andtruckscene from Tanks&Temples (bottom row). We visualize our rendering results with 32 input views, showing that our method demonstrates clear improvements on generalization and multi-view consistency under sparse inputs.",
                "position": 1549
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/sup_mip_tnt_qual.jpg",
                "caption": "Figure 13:Qualitative results on the Mip-NeRF360 (top two rows), andtruckscene from Tanks&Temples (bottom row). We visualize our rendering results as the number of input views increases, revealing progressively improved image quality and demonstrating that our method scales effectively with the number of views.",
                "position": 1552
            },
            {
                "img": "https://arxiv.org/html/2512.07806/sup_figure/mvp_rgb_depth.jpg",
                "caption": "Figure 14:Qualitative visualization of rendered color and depth maps from novel viewpoints using 32 input images. Scenes from DL3DV (top four rows), Mip-NeRF360 (fifth and sixth row), and Tanks&Temples (bottom two rows) are shown.",
                "position": 1555
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]