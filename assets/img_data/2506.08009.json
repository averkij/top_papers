[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/x1.png",
                "caption": "Figure 1:Training paradigms for AR video diffusion models.(a) In Teacher Forcing, the model is trained to denoise each frame conditioned on the preceding clean, ground-truth context frames.\n(b) In Diffusion Forcing, the model is trained to denoise each frame conditioned on the preceding context frames with varying noise levels.\nBoth (a) and (b) generate outputs that do not belong to the distribution the model generates during inference.\n(c) Our Self Forcing approach performs autoregressive self-rolloutduring training, denoising the next frame based on previous context frames generated by itself. A distribution-matching loss (e.g., SiD, DMD, GAN) is computed on the final output video to align the distribution of generated videos with that of real videos.\nOur training paradigm closely mirrors the inference process, thereby bridging the train-test distribution gap.",
                "position": 90
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Self Forcing: Briding Train-Test Gap via Holistic Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/x2.png",
                "caption": "Figure 2:Attention mask configurations.Both Teacher Forcing (a) and Diffusion Forcing (b) train the model on the entire video in parallel, enforcing causal dependencies with custom attention masks. In contrast, our Self-Forcing Training (c) mirrors the autoregressive (AR) inference process with KV caching and does not rely on special attention masks. For illustration purposes, we show a scenario where the video contains 3 frames, and each frame consists of 2 tokens.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2506.08009/x3.png",
                "caption": "Figure 3:Efficiency comparisons for video extrapolation.When performing video extrapolation through sliding window inference, (a) bidirectional diffusion models trained with TF/DFsong2025history;chen2025skyreelsdo not support KV cache. (b) Prior causal diffusion modelsyin2025causvid;magi1require re-computing KV when shifting the window. (c) Our method does not recompute KV and enablesmore efficient extrapolation.",
                "position": 480
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/x4.png",
                "caption": "Figure 4:User preference study.Self Forcing outperforms all baselines in human preference.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2506.08009/",
                "caption": "Figure 5:Qualitative comparisons.We visualize videos generated by Self Forcing (Ours) against those by Wan2.1wang2025wan, SkyReels-V2chen2025skyreels, and CausVidyin2025causvidat three time steps. All models share the same architecture with 1.3B parameters.",
                "position": 690
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/x6.png",
                "caption": "Figure 6:Training efficiency comparison.Left:Per-iteration time across different chunk-wise, few-step autoregressive video diffusion training algorithms (using DMD as the distribution matching objective).Right:Video quality (VBench score) vs. wall clock training time.",
                "position": 869
            }
        ]
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BImportance of local attention training in rolling KV cache",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/x7.png",
                "caption": "Figure 7:Qualitative comparisons on video extrapolation.We present a visual comparison between the naive baseline and our proposed technique for rolling KV cache-based video extrapolation. Compared to our method using local attention window training, extrapolated video frames from the naive baseline exhibit severe visual artifacts.",
                "position": 2588
            }
        ]
    },
    {
        "header": "Appendix CVBench Scores Across All Dimensions",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/x8.png",
                "caption": "Figure 8:VBench scores visualization.We compare Self Forcing with SkyReels-V2chen2025skyreels, Wan2.1-1.3Bwang2025wan, MAGI-1magi1, and CausVidyin2025causvidusing all 16 VBench metrics.",
                "position": 2595
            }
        ]
    },
    {
        "header": "Appendix DBroader Societal Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08009/extracted/6520114/figures/user_study.jpg",
                "caption": "Figure 9:User study instruction screenshots.",
                "position": 2623
            }
        ]
    },
    {
        "header": "Appendix EUser Study Details",
        "images": []
    }
]