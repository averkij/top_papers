[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25052/x1.png",
                "caption": "Figure 1:A Comparison of Paradigms for Game-Playing Agents. This figure contrasts three distinct agent architectures: (1) the Conventional RL Paradigm that learns an implicit policy from rewards, by updating policy weights; (2) the Zero-shot Reasoning Paradigm that leverages a static LLM model for decision-making; (3) theCogito, ergo ludoParadigm, where the agent‚Äôs policy is trained by RL while a persistent knowledge base (Rule & Playbook) is built and passedacross episodes.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25052/x2.png",
                "caption": "Figure 2:An overview of theCogito, ergo ludo(CEL) agent‚Äôs two-phase operational cycle. In Phase 1, the agent leverages its Language-based World Model (LWM) to predict the outcomes of potential actions and its Language-based Value Function (LVF) to evaluate the desirability of the resulting states, ultimately selecting the optimal action. In Phase 2, it reflects on the episode‚Äôs trajectory to update its explicit knowledge base (Environmental Rules and Strategic Playbook). The agent continuously improves through this dual learning loop, which not only refines its explicit knowledge but also trains the LLM‚Äôs internal parameters based on the final outcome.",
                "position": 164
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25052/x3.png",
                "caption": "Figure 3:Learning curves of our agent on FrozenLake (left), Minesweeper (center), and Sokoban (right). The plots show the average success rate (y-axis) plotted against the number of LLM update steps (x-axis).\nStarting without any explicit rules, the agent‚Äôs consistent improvement across these diverse tasks showcases the effectiveness of its autonomous rule discovery and policy learning.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x4.png",
                "caption": "Figure 4:Ablation study illustrating the critical role of iterativeInduction of Environmental Dynamics. The full model (blue) significantly outperforms variants with static rules (teal) or no rules (red), demonstrating that continuous refinement of the learned rulebookùí¢k\\mathcal{G}_{k}is essential for sustained performance improvement.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x5.png",
                "caption": "Figure 5:The agent‚Äôs In-Episode Decision-Making process. At each step, the agent uses its Language-based Value Function (LVF) to assess the current state‚Äôs value (middle column). It then employs its Language-based World Model (LWM) to predict the consequences of each action (right column). The agent selects the action leading to the outcome with the highest predict value.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x6.png",
                "caption": "Figure 6:An illustrative excerpt of the agent‚Äôs learned rulebookùí¢k\\mathcal{G}_{k}for Minesweeper, generated via the Induction of Environmental Dynamics process. Starting from no prior knowledge, the agent synthesizes a comprehensive and accurate set of rules from its interaction trajectory.\nPlease refer to Figure14in the Appendix for the complete rulebook.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x7.png",
                "caption": "Figure 7:A synthesis of the strategic playbookŒ†k\\Pi_{k}for Minesweeper, generated via Strategy and Playbook Summarization. The agent distills both tacticalMethodsand high-levelPrinciplesfrom its gameplay experience. This explicit playbook is used to condition the agent‚Äôs value judgments, enabling more strategically sophisticated decision-making.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x8.png",
                "caption": "Figure 8:Inter-game generalization study showcasing adaptation to novel environments without model retraining. The plots show the agent‚Äôs cross-game performance: a Minesweeper-trained agent on Frozen Lake (left) and a FrozenLake-trained agent on Minesweeper (right). In both evaluations, the core model weights remain frozen. The agent‚Äôs adaptation relies solely on the iterative refinement of its explicit rulebook and strategic playbook every 5 episodes (indicated by cyan lines).",
                "position": 401
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Environments",
        "images": []
    },
    {
        "header": "Appendix BAnalysis on Rollout Outcomes for Action-only Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25052/x9.png",
                "caption": "Figure 9:Distribution of rollout outcomes for the Action-only model. Across all group sampling sizes (8 to 64), outcomes are polarized into ‚ÄúAll‚Äù (all succeed) or ‚ÄúNone‚Äù (all fail). The number of ‚ÄúPartial‚Äù rollouts, which are required for GRPO to learn, is consistently zero, causing training failure.",
                "position": 864
            }
        ]
    },
    {
        "header": "Appendix CPerformance with an Expanded Training Set",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25052/x10.png",
                "caption": "Figure 10:Learning curve for the CEL agent on Minesweeper when trained on an expanded set of 128 unique seeds. The agent achieves a new peak success rate of 62%, surpassing the performance observed with 32 seeds.",
                "position": 880
            }
        ]
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25052/x11.png",
                "caption": "Figure 11:The prompt template for in-episode decision-making (Phase 1). It instructs the LLM to evaluate the current state, assess their strategic value (LVF) and predict action outcomes (LWM).",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x12.png",
                "caption": "Figure 12:The prompt template for the Rule Induction and Playbook Summarization process (Phase 2). It guides the LLM to analyze a completed episode‚Äôs trajectory and refine its explicit model of the environment‚Äôs dynamics.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x13.png",
                "caption": "Figure 13:An example of a Decision-Making process for Sokoban environment.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x14.png",
                "caption": "Figure 14:An example of a learned environmental rule for Minesweeper environment.",
                "position": 907
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x15.png",
                "caption": "Figure 15:An example of a learned environmental rule for FrozenLake environment.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x16.png",
                "caption": "Figure 16:An example of a learned environmental rule for Sokoban environment.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x17.png",
                "caption": "Figure 17:An example of a learned strategic guideline from the agent‚Äôs playbook for FrozenLake environment.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2509.25052/x18.png",
                "caption": "Figure 18:An example of a learned strategic guideline from the agent‚Äôs playbook for Sokoban environment.",
                "position": 920
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]