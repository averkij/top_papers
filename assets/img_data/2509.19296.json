[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/x1.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/x2.png",
                "caption": "Figure 2:Self-distillation framework ofLyra.Previous work (left)(Szymanowicz et al.,2025b)trains multi-view reconstruction blocks using real-world datasets with limited diversity(Zhou et al.,2018; Ling et al.,2024b). In contrast, we propose a self-distillation framework (right) for generative 3D scene reconstruction. Precisely, a pre-trained camera-controlled video diffusion model with its RGB decoder output (teacher) supervises the rendering of the 3DGS decoder (student). Using a video model pre-trained on diverse 2D video data allows us to provide diverse multi-view supervision.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Self-Distillation using Video Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/imgs/sdg_traj.png",
                "caption": "Figure 3:Sampled six camera trajectories to maximize view coverage.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2509.19296/x3.png",
                "caption": "Figure 4:3D Generative Reconstruction Framework.Our pipeline builds upon a camera-controlled video diffusion model(Ren et al.,2025)pre-trained on large scale data. We train a 3D Gaussian Splatting (3DGS) decoder by aligning the 2D image renderings of generated 3DGS scenes with the RGB-decoded generations of the pre-trained video model. We only train the 3DGS decoder while freezing the pre-trained autoencoder and diffusion model.\nAt inference time we directly use the 3DGS decoder, without requiring the RGB decoder anymore.\nTime conditioning within the 3DGS decoder allows us to easily extend our approach from static to dynamic 3D scene generation.",
                "position": 246
            }
        ]
    },
    {
        "header": "4Feed-Forward Reconstruction from Multi-View Video Latents",
        "images": []
    },
    {
        "header": "5Extension to Dynamic 3D Scenes",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/x4.png",
                "caption": "Figure 5:Dynamic data augmentation.When naively supervising the time-aware 3DGS decoder, we observe artifacts in the generated 3D Gaussians. Specifically, early timesteps from extreme poses exhibit low opacity in regions not covered by the supervision signal. To address this, we augment the supervision data with a motion-reversed video, ensuring that each timestep is observed from the full spatial coverage, thereby preventing low opacity artifacts in the early timesteps.",
                "position": 320
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/imgs/main_results.png",
                "caption": "Figure 6:Image-to-3DGS Generation.We visualize five views from generated 3DGS scenes.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2509.19296/x5.png",
                "caption": "Figure 7:Ablations.We visualize ablation results by rendering the same extreme novel viewpoint after image-to-3DGS generation; depth visualizations and ablations are provided in AppendixC.4.",
                "position": 465
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Ethics Statement",
        "images": []
    },
    {
        "header": "9Reproducibility Statement",
        "images": []
    },
    {
        "header": "10Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details of Video Diffusion Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/x6.png",
                "caption": "Figure 8:Rendering function comparison.Our model builds upon GEN3C(Ren et al.,2025)that uses forward warped images as camera control conditioning. We visualize forward warping (left) vs. our conservative rendering function (right). The highlighted region is incorrectly filled with background pixels in forward warping but properly masked in ours, enabling correct completion.",
                "position": 3497
            }
        ]
    },
    {
        "header": "Appendix BModel Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/x7.png",
                "caption": "Figure 9:Dynamic data augmentation videos.We augment the supervision data with a motion-reversed video, ensuring that each timestep is observed from the full spatial coverage, thereby preventing low opacity artifacts in the early timesteps. We show two example trajectories, i.e., zoom-out and zoom-in, and visualize their corresponding augmented videos. The augmented videos are flipped in their camera motion.",
                "position": 3613
            }
        ]
    },
    {
        "header": "Appendix CAdditional Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/x8.png",
                "caption": "Figure 10:Image-to-3DGS Comparison.We compare our method with BTimer (GEN3C) and visualize five views from generated scenes. We observe significantly fewer artifacts and higher fidelity.",
                "position": 3631
            },
            {
                "img": "https://arxiv.org/html/2509.19296/x9.png",
                "caption": "Figure 11:Depth loss ablation.We visualize 3DGS renderings and corresponding depth maps. Using the depth loss as additional supervision prevents flat geometries without sacrificing visual quality.",
                "position": 3715
            }
        ]
    },
    {
        "header": "Appendix DRobot Simulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19296/imgs/isaac_apartment_1.png",
                "caption": "Figure 12:Robot Simulation.We visualize a frame of a robot simulation within the Isaac Sim 5.0 simulation framework that takes a generated 3DGS scene from our method as input.",
                "position": 3723
            },
            {
                "img": "https://arxiv.org/html/2509.19296/x10.png",
                "caption": "Figure 13:Approaches for 3D Generation.CAT3D(Gao et al.,2024b)proposed a multi-view image diffusion model that outputs images from novel viewpoints; subsequently, the images are reconstructed into 3D Gaussians using optimization. Bolt3D(Szymanowicz et al.,2025b)fine-tunes CAT3D to output pointmaps using a geometry autoencoder;\ninstead of relying on optimization, the 3D scene is predicted with a feed-forward 3D Gaussian head.\nIn contrast, our work builds upon a pre-trained camera-controlled video diffusion model anddirectlydecodes the multi-view video latents into 3D Gaussians.",
                "position": 3730
            }
        ]
    },
    {
        "header": "Appendix ERelated Work",
        "images": []
    }
]