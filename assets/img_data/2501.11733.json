[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/new_teaser.png",
                "caption": "",
                "position": 156
            }
        ]
    },
    {
        "header": "2Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/x1.png",
                "caption": "Figure 2:An overview of the Mobile-Agent-E framework, where the Manager, Perceptor (ùíúPsubscriptùíúùëÉ\\mathcal{A}_{P}caligraphic_A start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT), Operator, Action Reflector, and Notetaker are involved in the main agent loop for each task, while two Experience Reflectors contribute to updating long-term memory across tasks. Decision-making at each step is disentangled into high-level planning by the Manager and low-level actions by the Operator. The Action Reflector verifies the outcome of each action, tracks progress, and provides error feedback. The Notetaker aggregates important information during navigation. A detailed example illustrating one step in the agent loop and the self-evolution process is presented in Figures3and4.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/breakdown_example.png",
                "caption": "Figure 3:A detailed breakdown of one inference steptùë°titalic_twith Mobile-Agent-E, showing the inputs and outputs of each agent. Omitted information indicates no change.",
                "position": 215
            }
        ]
    },
    {
        "header": "3Mobile-Agent-E",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/evolving_breakdown.png",
                "caption": "Figure 4:Illustration of the inputs and outputs to the Experience Reflectors for a single self-evolution step, including a concrete example of the newly generated Shortcuts and Tips.",
                "position": 506
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/x2.png",
                "caption": "(a)Single Task (task id: 5_palo_alto_tour)",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2501.11733/x2.png",
                "caption": "(a)Single Task (task id: 5_palo_alto_tour)",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2501.11733/x3.png",
                "caption": "(b)All Tasks",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/evo_through_time.png",
                "caption": "Figure 6:Progressive impact of self-evolution over time. The task index represents the order in which a task is performed in the evolution setting. The results demonstrate that tasks performed later in the sequence show more significant improvements, highlighting the increased benefits from additional iterations of self-evolution.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/case_study.png",
                "caption": "Figure 7:Case study example where relevant Shortcuts and Tips are automatically retrieved from the previously evolved long-term memory and subsequently leveraged to complete an unseen, challenging task. The action trajectory also includes an example where the agent recovers from an error.",
                "position": 1093
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/full_trajectory_comparision.png",
                "caption": "Figure 8:Full trajectory comparison between the previous state-of-the-art, Mobile-Agent-v2(Wang et¬†al.,2024a), and Mobile-Agent-E.",
                "position": 1694
            }
        ]
    },
    {
        "header": "Appendix AFull Trajectory Comparison Example with Previous SOTA",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/error_escalation.png",
                "caption": "Figure 9:Error recovery with escalation. The task requires the agent to search for three different items on Walmart and note their sales information. At the step shown in the figure, the agent has already searched for ribeye steak and intends to search for fresh oranges next. However, the Operator erroneously calls the Shortcut that inputs text into the search bar and performs a search without clearing the previously entered text. Although the Action Reflector raises an error, the subgoal remains unchanged, and the Operator fails to rectify the error on the second attempt. After observing two consecutive errors, the error is escalated to the Manager, which correctly identifies the problem and revises the subgoal with detailed, decomposed steps to address the error. This helps the Operator correctly recover from the previous error by first tapping the ‚Äú√ó\\times√ó‚Äù icon to clear the previous search query.",
                "position": 1705
            }
        ]
    },
    {
        "header": "Appendix BError Recovery with Escalation to Manager",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/extracted/6143864/assets/error_case.png",
                "caption": "Figure 10:Example of misuse of Shortcuts in an invalid state. At the current step, as shown in the figure, the agent intended to switch back to Walmart to search for the final item requested by the user. While it correctly performs the ‚ÄúSwitch_App‚Äù operation, it then calls a Shortcut for searching without realizing that it is not yet in the App where the search bar is available.",
                "position": 1717
            }
        ]
    },
    {
        "header": "Appendix CRemaining Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/x4.png",
                "caption": "Figure 11:Example of imperfect (above) and erroneous (below) generated Shortcuts. The ‚ÄúSearch_Location_in_Maps‚Äù Shortcut includes an unnecessary Tap action in the operation sequence, while the ‚ÄúSwitch_App_And_Search‚Äù Shortcut omits a Tap action needed to first enter the desired App before performing the search.",
                "position": 1732
            }
        ]
    },
    {
        "header": "Appendix DAll Tasks in Mobile-Eval-E Benchmark",
        "images": []
    },
    {
        "header": "Appendix EAtomic Operation Space",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/x5.png",
                "caption": "Figure 12:Full list of Shortcuts generated by Mobile-Agent-E (with GPT-4o) after self-evolution.",
                "position": 2394
            }
        ]
    },
    {
        "header": "Appendix FFull list of Self-Evolved Shortcuts",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.11733/x6.png",
                "caption": "Figure 13:Full list of Tips generated by Mobile-Agent-E (with GPT-4o) after self-evolution.",
                "position": 2406
            }
        ]
    },
    {
        "header": "Appendix GFull list of Self-Evolved Tips",
        "images": []
    }
]