[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Analysis Methods",
        "images": []
    },
    {
        "header": "3Tasks with Common Linguistic Structure Across Languages",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/ioi_info_flow.png",
                "caption": "Figure 1:Attention heads activation frequency comparison between English IOI, Chinese IOI and English control tasks. Left: Comparison between English IOI and Chinese IOI on BLOOM (with specific functional heads in different marker types). Right: Comparison between English IOI and English Tense on BLOOM. For pairs of heads with non-zero activation frequency, they are shaded based on their value difference. Darker gray means smaller differences. The left graph has more number of shaded head pairs compared to the right, indicating a greater similarity between the activated heads.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/en_zh_ioi_multilingual.png",
                "caption": "Figure 2:Illustration of IOI circuits of English and Chinese on BLOOM-560M model. Blue-framed rectangles or texts are the components used in English IOI and orange-framed ones are for Chinese IOI. Heads marked in black are the shared functional heads between English and Chinese IOI. The English and Chinese IOI circuits are highly similar as they use most of the same components for the same functionality to implement the algorithms.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/en_zh_ioi_monolingual.png",
                "caption": "Figure 3:English and Chinese IOI task circuits. Blue highlights are components used and inputs for the English task in GPT2. Orange highlights are for CPM. The components with both blue and orange frames are shared in both the English and Chinese circuits. In each of these heads, blue texts indicate the heads in the English Circuit and orange in Chinese. The Negative Name Mover heads appear only in the English model. Copy Suppression Head appears only in the Chinese model. Despite trained on completely different data, models still implement largely similar algorithms to solve tasks in different languages.",
                "position": 205
            }
        ]
    },
    {
        "header": "4Tasks with Language-Specific Structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/tense_coarse.png",
                "caption": "Figure 4:(a) Head activation frequency comparison between English and Chinese past tense task on Qwen. (b) Illustration of the tense circuits in Qwen that contains three components we focus on: copy heads, past tense heads and feed-forward layers. (c) Top promoted tokens from copy head and past tense head. Words highlighted in either blue and orange are expected model predictions. Gray tokens are the specific tokens that the head promotes. Here the copy head is21.3and the past tense head is19.4. As shown in (a), the copy head has a similarly high activation frequency in both languages whereas the past tense head is only frequently activated in English.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/mlp_ablation.png",
                "caption": "Figure 5:Illustration of the effects of late feed-forward layers ablation (layer 20-23). Left: Zero-rank rate changes comparison between English and Chinese past tense task. Right: Top predicted tokens comparison for before and after ablation. We observe that English performance is significantly impacted after ablation but Chinese is barely influenced. Qualitative results on the right show that the correction token move backward in English but remain the top prediction in Chinese.",
                "position": 269
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask Benchmark Performances",
        "images": []
    },
    {
        "header": "Appendix BPatching Details of the IOI Task",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/appendix/bloom_patching.png",
                "caption": "Figure 6:Path Patching steps for extracting IOI circuits on BLOOM-560M. Left: English Input. Right: Chinese Input.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/appendix/bloom_head_scores.png",
                "caption": "Figure 7:Metrics for Duplicate Token Head, Previous Token Head, and Induction Head on English Inputs (Top) and Chinse Inputs (Bottom).",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2410.09223/x1.png",
                "caption": "Figure 8:Left: Path patching comparison between GPT2 on English task and CPM on Chinese task. Right: Attention probability vs. head projection output into the IO and S space.",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2410.09223/x2.png",
                "caption": "Figure 9:Top: Induction head scores on random tokens and its attention pattern. Middle: Duplicate token attention patterns on ABAB random strings. Bottom: S-inhibition head ablation effects on name movers.",
                "position": 1302
            },
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/tense_patching.png",
                "caption": "Figure 10:Left: Path Patching on English Past Tense Task. Right: Verb Promotion Heatmaps for English and Chinese Past Tense Task.",
                "position": 1316
            },
            {
                "img": "https://arxiv.org/html/2410.09223/extracted/5920428/graph/head_ablation.png",
                "caption": "Figure 11:Effects of Past Tense Heads and Copy Head Ablation.",
                "position": 1326
            }
        ]
    },
    {
        "header": "Appendix CPatching Details of The Past Tense Task",
        "images": []
    }
]