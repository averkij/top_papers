[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08441/x1.png",
                "caption": "Figure 1:Overview ofYaPO.\nUnlike dense BiPO, which learns entangled steering directions directly in activation space,\nYaPO leverages a pretrained Sparse Autoencoder (SAE) to project activations into an interpretable sparse space.\nBy optimizing sparse codes, YaPO learns disentangled and robust steering vectors that improve convergence, stability, and cultural alignment,\nwhile preserving generalization across domains.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08441/x2.png",
                "caption": "(a)Egypt localized",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2601.08441/x2.png",
                "caption": "(a)Egypt localized",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2601.08441/x3.png",
                "caption": "(b)Nepal non-localized",
                "position": 510
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08441/x4.png",
                "caption": "Figure 3:Training accuracy over epochs for YaPO (red), BiPO (blue), and the unsteered baseline (orange) on the MCQ localization task across six cultural regions.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2601.08441/Figures/multipliers_egy_lev.png",
                "caption": "(a)Egypt & Levantine",
                "position": 1148
            },
            {
                "img": "https://arxiv.org/html/2601.08441/Figures/multipliers_egy_lev.png",
                "caption": "(a)Egypt & Levantine",
                "position": 1151
            },
            {
                "img": "https://arxiv.org/html/2601.08441/Figures/multipliers_nep_spa.png",
                "caption": "(b)Nepal & Spanish",
                "position": 1156
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALayer Discovery",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08441/x5.png",
                "caption": "Figure 5:Activation patching analysis onGemma-2-2B.\nWe intervene across layers to trace cultural features in model representations.\nThe plots show the probability of producing culturally specific answers (Egypt, Morocco) versus Western defaults as activations are patched.\nWe empirically identifylayer 15as the most culturally relevant layer.",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2601.08441/x5.png",
                "caption": "",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2601.08441/x6.png",
                "caption": "",
                "position": 1825
            }
        ]
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Results",
        "images": []
    },
    {
        "header": "Appendix DScalability to other Models",
        "images": []
    },
    {
        "header": "Appendix EEvaluation: LLM-as-Judge Prompts",
        "images": []
    },
    {
        "header": "Appendix FDataset",
        "images": []
    }
]