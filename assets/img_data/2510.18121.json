[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18121/x1.png",
                "caption": "",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/ca_server_workflow_4.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Challenge and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18121/figures/ag-overhead.png",
                "caption": "(a)all-gather latency(%)",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/ag-overhead.png",
                "caption": "(a)all-gather latency(%)",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/memory_breakdown.png",
                "caption": "(b)Memory breakdown (%)",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/varlen-chunk-memory.png",
                "caption": "(a)Memory divergence.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/varlen-chunk-memory.png",
                "caption": "(a)Memory divergence.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/idle_percentage_512k_only.png",
                "caption": "(b)Divergence of Attention Computation.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/fa_throughput.png",
                "caption": "Figure 5:Throughput of core attention.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/fa_throughput.png",
                "caption": "Figure 5:Throughput of core attention.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/wlbllm-optimal-cp-degree.throughput.png",
                "caption": "Figure 6:Throughput when applying varaible-length data chunk and per-doc CP.",
                "position": 349
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18121/figures/pingpong_3.png",
                "caption": "Figure 7:Ping-Pong computation and communication for inplace attention server. CA means core attention. (i, 0) means the Ping part of layer i, (i, 1) means the Pong part of layer i.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/pp_2.png",
                "caption": "Figure 8:Pipeline Parallel Schedule for normal 1F1B and disaggregated attention.",
                "position": 460
            }
        ]
    },
    {
        "header": "5Implementations",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18121/figures/dpcp-2x2-comparison.png",
                "caption": "Figure 9:3D Parallel (no PP) experiment. Speedup is defined as the average duration of the runs of WLB-LLM over DistCA.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/pp-2x2-comparison.png",
                "caption": "Figure 10:4D Parallel (with PP) experiment. Speedup is defined as the average duration of the runs of WLB-LLM over DistCA.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/figure.ablation.relative_throughput.all_configs.png",
                "caption": "Figure 11:Throughput for different communication patterns.",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2510.18121/figures/figure.ablation.tolerance_factor.combined_2x2.png",
                "caption": "Figure 12:Impact of the compute imbalance tolerance factor.",
                "position": 798
            }
        ]
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUpper Bound for Core Attention Server max partition size",
        "images": []
    },
    {
        "header": "Appendix BCommunication overhead function",
        "images": []
    }
]