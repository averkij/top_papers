[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21223/x1.png",
                "caption": "Figure 1:Illustration of our input-space model merging framework using FDAs. On the left, we compare multi-task joint training, task arithmetic and FDA. Inspired by joint training, FDA models the knowledge in the input space.θA=F​T​(𝑿A,𝜽0)\\theta_{A}=FT(\\bm{X}_{A},\\bm{\\theta}_{0})denotes the model finetuned by the task data𝑿A\\bm{X}_{A}from the initial model𝜽0\\bm{\\theta}_{0}with some loss function.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x2.png",
                "caption": "Figure 2:Comparison between task arithmetic and FDAs on the loss landscape of the pretrained across all 8 downstream datasets. FDAs can effectively follow the loss landscape and guide the model toward better local minima.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x3.png",
                "caption": "Figure 3:Evolution of Normalized singular values of FDAs in the FDA construction. We visualize the results of FDAs from the1212-th layer of the ViT-B/32 checkpoint on MNIST.σ=101\\sigma=10^{1}denotes FDAs initialized by sampling from𝒩​(𝟎,𝑰d)\\mathcal{N}(\\mathbf{0},{\\bm{I}}_{d})and scaling by10110^{1}; “Weight” denotes FDAs initialized from linear weight. FDAs of different initialization schemes tend to evolve into long-tailed structures.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x4.png",
                "caption": "Figure 4:Average loss curves.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x5.png",
                "caption": "Figure 5:Evolution of subspace similarity of FDAs in the FDA construction. We visualize the results of FDAs from the1212-th layer of the ViT-B/32 checkpoint.σ=101\\sigma=10^{1}denotes the FDAs initialized by sampling from𝒩​(𝟎,𝑰d)\\mathcal{N}(\\mathbf{0},{\\bm{I}}_{d})and scaling by10110^{1}; “Weight” denotes the FDAs initialized from linear weight. FDAs of different initialization schemes tend to align the subspace spanned by real data.",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x6.png",
                "caption": "Figure 6:Evolution of projection energy ratio on pretrained model and merged model (TA). We visualize the results of FDAs from the1212-th layer of the ViT-B/32 checkpoints.σ=101\\sigma=10^{1}denotes the FDAs initialized by sampling from𝒩​(𝟎,𝑰d)\\mathcal{N}(\\mathbf{0},{\\bm{I}}_{d})and scaling by10110^{1}; “Weight” denotes the FDAs initialized from linear weight. The dashed line indicates the projection energy ratio of task vector induced by real data.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x7.png",
                "caption": "Figure 7:Effects of FDAs on the representations.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x8.png",
                "caption": "Figure 8:Multi-task performance of FDAs with different shape of FDAs on ViT-B/32 and RoBERTa-Base.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x9.png",
                "caption": "Figure 9:Multi-task performance of FDAs with differentDist\\mathrm{Dist}functions on ViT-B/32 and RoBERTa-Base.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x10.png",
                "caption": "Figure 10:Multi-task performance of FDAs with different optimization steps.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x11.png",
                "caption": "Figure 11:Evolution of Normalized singular values of FDAs in the FDA construction.σ=101\\sigma=10^{1}denotes FDAs initialized by sampling from𝒩​(𝟎,𝑰d)\\mathcal{N}(\\bm{0},{\\bm{I}}_{d})and scaling by10110^{1}; “Weight” denotes that of FDAs initialized from linear weight. FDAs of different initialization schemes tend to evolve into long-tailed structures.",
                "position": 3080
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x12.png",
                "caption": "Figure 12:Evolution of subspace similarity of FDAs in the FDA construction.σ=101\\sigma=10^{1}denotes the FDAs initialized by sampling from𝒩​(𝟎,𝑰d)\\mathcal{N}(\\bm{0},{\\bm{I}}_{d})and scaling by10110^{1}; “Weight” denotes the FDAs initialized from linear weight. FDAs of different initialization schemes tend to align the subspace spanned by real data.",
                "position": 3095
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x13.png",
                "caption": "Figure 13:The low-dimensional visualization via t-SNE of FDAs and real data.",
                "position": 3101
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x14.png",
                "caption": "Figure 14:Evolution of projection energy ratio on pretrained model and merged model (TA).σ=101\\sigma=10^{1}denotes the FDAs initialized by sampling from𝒩​(𝟎,𝑰d)\\mathcal{N}(\\bm{0},{\\bm{I}}_{d})and scaling by10110^{1}; “Weight” denotes the FDAs initialized from linear weight.",
                "position": 3116
            },
            {
                "img": "https://arxiv.org/html/2510.21223/x15.png",
                "caption": "Figure 15:The Effect of FDAs in the representation. In general, the adaptation with FDAs substantially mitigates the representation bias observed in both pretrained and merged models.",
                "position": 3122
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]