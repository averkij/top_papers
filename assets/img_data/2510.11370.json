[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11370/x1.png",
                "caption": "Figure 1:Left: Illustration of the Rollout Routing Replay (R3). Top right: Training and inference discrepancies before and after applying R3. Bottom right: Reinforcement learning training performance before and after applying R3.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x2.png",
                "caption": "",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x3.png",
                "caption": "",
                "position": 152
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Training-Inference Discrepancies",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11370/x4.png",
                "caption": "(a)MoE",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x4.png",
                "caption": "(a)MoE",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x5.png",
                "caption": "(b)MoE + R3",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x6.png",
                "caption": "(c)Dense",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x7.png",
                "caption": "(d)Extreme Token Distribution Analysis",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x8.png",
                "caption": "(a)Router-level Difference",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x8.png",
                "caption": "(a)Router-level Difference",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x9.png",
                "caption": "(b)Token-level Difference",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x10.png",
                "caption": "(c)Sequence-level Difference",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x11.png",
                "caption": "Figure 4:Probabilities obtained by performing forward propagation twice using the Megatron",
                "position": 338
            }
        ]
    },
    {
        "header": "4Rollout Routing Replay",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11370/x12.png",
                "caption": "Figure 5:Analysis of training–inference collapse. The plot shows the estimated training–inference KL divergence and the extreme token distribution functionF​(τ=2)F(\\tau=2)(Eq.3) at each training step.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x13.png",
                "caption": "Figure 6:Training dynamics of Qwen3-30B-A3B-Base, including response length, gradient norm, entropy, and average validation score throughout the training process",
                "position": 676
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Evaluation Results and Training Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11370/x14.png",
                "caption": "Figure 7:The detailed evaluation results of the experiment in Section5.",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x15.png",
                "caption": "Figure 8:The detailed training metrics of the experiment in Section5.",
                "position": 1376
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments of Reasoning SFT Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11370/x16.png",
                "caption": "Figure 9:The detailed training metrics and evaluation results of the RL training process of Qwen3-30B-A3B-ReasoningSFT",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2510.11370/x17.png",
                "caption": "Figure 10:The detailed training metrics and evaluation results of RL training process on SWE Task with Qwen3-30B-A3B",
                "position": 1477
            }
        ]
    },
    {
        "header": "Appendix CDetailed Training Metrics of Multi-Turn Reinforcement Learning",
        "images": []
    }
]