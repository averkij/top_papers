[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15856/x1.png",
                "caption": "Figure 1:Our latent denoising tokenizers (lùëôlitalic_l-DeTok) framework.During tokenizer training, we randomly mask input patches (masking noise) and interpolate encoder-produced latent embeddings with Gaussian noise (interpolative latent noise). The decoder processes these deconstructed latents and mask tokens to reconstruct the original images in pixels. We refer to this process asdenoising. When serving as a tokenizer for downstream generative models, both noises are disabled.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Implementation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15856/x2.png",
                "caption": "Figure 2:Ablation on latent noise design.(a) Additivevs.interpolative noise.Interpolative noise clearly outperforms additive noise for both MAR[36]and SiT[42]. Both interpolative and additive latent noise lead to improved performance for MAR.(b) Latent noise standard deviation (Œ≥ùõæ\\gammaitalic_Œ≥).Ourlùëôlitalic_l-DeTok remains robust across various noise standard deviations. Generally, increasingŒ≥ùõæ\\gammaitalic_Œ≥improves generation quality, with best results achieved aroundŒ≥=3.0ùõæ3.0\\gamma=3.0italic_Œ≥ = 3.0.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x3.png",
                "caption": "Figure 3:Ablation on masking ratio.We show generative performance (FID‚Üì‚Üì\\downarrow‚Üì) with varying masking ratios for tokenizers trained with(a)random and(b)constant masking ratio. Both MAR[38](top) and SiT[42](bottom) benefit from masking-based tokenizers, favoring heavy masking (70% to 90%). Notably, randomized masking consistently outperforms constant masking.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x4.png",
                "caption": "Figure 4:Impact of tokenizer training strategies on generative performance.We compare our baseline tokenizer with ourlùëôlitalic_l-DeTok variants: masking-only (M=0.7ùëÄ0.7M{=}0.7italic_M = 0.7), latent noising-only (Œ≥=3.0ùõæ3.0\\gamma{=}3.0italic_Œ≥ = 3.0), and their combination (joint noising). Both masking and latent noising independently improve generation quality, with latent noising showing a stronger effect. Joint noising further improves performance for MAR, particularly in inception scores (IS), but provides limited additional benefit for SiT when latent noising is already applied. FID@50k scores are detailed in Tab.5.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/000021.png",
                "caption": "Figure 6:Qualitative Results.We show selected examples of class-conditional generation on ImageNet 256√ó\\times√ó256 using MAR-L[36]trained with our tokenizer.",
                "position": 1186
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/000039.png",
                "caption": "",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/2000412.png",
                "caption": "",
                "position": 1190
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/000104.png",
                "caption": "",
                "position": 1191
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/000153.png",
                "caption": "",
                "position": 1192
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/002599.png",
                "caption": "",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/049889.png",
                "caption": "",
                "position": 1195
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/028429.png",
                "caption": "",
                "position": 1196
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/040668.png",
                "caption": "",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/026928.png",
                "caption": "",
                "position": 1198
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/012970.png",
                "caption": "",
                "position": 1199
            },
            {
                "img": "https://arxiv.org/html/2507.15856/extracted/6641225/figures/examples/016780.png",
                "caption": "",
                "position": 1200
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining and Inference Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15856/x5.png",
                "caption": "Figure A.1:Visualization of latent denoising.Images generated from latent embeddings corrupted with varying noise levels (tùë°titalic_t) by the original decoder (top), fine-tuned decoder (middle), and baseline decoder (bottom). The fine-tuned decoder shows a reduced ability to recover images from noisy embeddings compared to the original decoder. The baseline tokenizer trained without the denoising objective fails to reconstruct original images from noisy latents.",
                "position": 2220
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x6.png",
                "caption": "Figure A.2:Visualization of ‚Äúmask‚Äù denoising.Images generated from masked inputs with varying masking ratios (mùëömitalic_m) by the original decoder (top) and fine-tuned decoder (bottom). The fine-tuned decoder exhibits diminished capability in reconstructing masked regions compared to the original decoder.",
                "position": 2224
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x7.png",
                "caption": "Figure B.1:Tokenizer comparison for autoregressive models.We report FID@10k and IS@10k scores at different classifier-free guidance (CFG) scales.\nSemantics-distilled tokenizers (denoted by ‚Äú*‚Äù) are shown in warm colors, while those without distillation are shown in cool colors.\nOurlùëôlitalic_l-DeTok consistently achieves superior FID and IS metrics compared to other tokenizers.\nSee Table1for results on 50,000 images evaluated at the optimal CFG scales.",
                "position": 2342
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x8.png",
                "caption": "",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x9.png",
                "caption": "",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x10.png",
                "caption": "",
                "position": 2348
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x11.png",
                "caption": "",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x12.png",
                "caption": "",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x13.png",
                "caption": "Figure B.2:Tokenizer comparison for non-autoregressive models.We report FID@10k and IS@10k scores at different classifier-free guidance (CFG) scales.\nSemantics-distilled tokenizers (denoted by ‚Äú*‚Äù) are shown in warm colors, while those without distillation are shown in cool colors.\nOurlùëôlitalic_l-DeTok consistently outperforms standard (non-semantics-distilled) tokenizers, matching the performance of the best semantics-distilled tokenizer (VA-VAE[64]) and surpassing MAETok[6]in IS.\nSee Table1for results on 50,000 images evaluated at the optimal CFG scales.",
                "position": 2364
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x14.png",
                "caption": "",
                "position": 2367
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x15.png",
                "caption": "",
                "position": 2368
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x16.png",
                "caption": "",
                "position": 2370
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x17.png",
                "caption": "",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2507.15856/x18.png",
                "caption": "",
                "position": 2372
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]