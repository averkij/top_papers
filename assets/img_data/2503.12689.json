[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12689/x1.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/frame.jpg",
                "caption": "(a)ID Consistency for longer video.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/frame.jpg",
                "caption": "(a)ID Consistency for longer video.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/step.jpg",
                "caption": "(b)Dynamic Degree for more steps.",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x2.png",
                "caption": "Figure 3:Overview of pairwise preference video data construction.In Step 1, we construct a preference video repository using videos generated by fine-tuned and Initial T2V models, along with static videos derived from reference images. In Step 2, we evaluate each video sequentially based on ID consistency using ID Encoder[6], dynamic degree using optical flow[13], and prompt following using VLM[2]. In Step 3, we perform Hybrid Pair Selection, first selecting pairs based on ID consistency differences with a pre-defined dynamic threshold to address identity inconsistency, then selecting pairs based on both dynamic and identity to mitigate the dynamic reduction.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12689/x3.png",
                "caption": "Figure 4:Qualitative comparison with tuning-based methods.As observed, both Dreambooth and MagicMe suffer from inferior ID fidelity, while our method maintains consistent identity and natural dynamics.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x4.png",
                "caption": "Figure 5:Qualitative comparison with tuning-based methods.As shown, ID-Animator suffers from poor identity consistency and video quality. While ConsisID improves identity fidelity to some extent, it exhibits severecopy-pasteartifacts, demonstrating unnatural motion dynamics and text alignment, as seen in the last example with thehelmet. In contrast, our method achieves strong performance in identity consistency, motion dynamics, and text alignment, significantly outperforming the baseline approaches.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x5.png",
                "caption": "Figure 6:Ablation study for the hybrid pair selection.Compared to the self-reconstruction approach, training with identity-preferred pairs significantly enhances the identity consistency. On the other hand, the incorporation of dynamic-preferred pairs markedly improves the dynamics of the generated outcomes.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x6.png",
                "caption": "Figure 7:Ablation study for the customized video rewards.While the ID reward encourages the model to learn consistent identity features, the addition of the dynamic reward leads to generated results with significantly improved motion dynamics. Furthermore, incorporating a semantic reward can effectively enhance video dynamics to some extent and improve prompt-following capabilities, such as the prompts involvingturningmotions.",
                "position": 368
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12689/extracted/6285141/Fig/user.jpg",
                "caption": "Figure 8:User Study.",
                "position": 402
            }
        ]
    },
    {
        "header": "5Conclusion & Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplement Details",
        "images": []
    },
    {
        "header": "Appendix BBaseline Details",
        "images": []
    },
    {
        "header": "Appendix CDPO Objective Function",
        "images": []
    },
    {
        "header": "Appendix DApplying DPO Strategy into our MagicID",
        "images": []
    },
    {
        "header": "Appendix EMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12689/x7.png",
                "caption": "Figure 9:More results of MagicID.",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x8.png",
                "caption": "Figure 10:More results of MagicID.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x9.png",
                "caption": "Figure 11:More results of MagicID.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x10.png",
                "caption": "Figure 12:More results of MagicID.",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2503.12689/x11.png",
                "caption": "Figure 13:More results of MagicID.",
                "position": 1503
            }
        ]
    },
    {
        "header": "Appendix FReproducibility Statement",
        "images": []
    },
    {
        "header": "Appendix GImpact Statement",
        "images": []
    }
]