[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16784/x1.png",
                "caption": "Figure 1:Latent information compression for all context tokens versus structural latent information compression focusing on the working memory enabled by parsing the reasoning trajectory. In the later case, after the outputs of Task 1.1 and 1.2 are aggregated as a higher-level outcome of Task 1, the working memory will only host the KV states of Task 1 for future reasoning, saving a significant amount of computation for attention mechanism.",
                "position": 82
            }
        ]
    },
    {
        "header": "2Thread Inference Model (TIM)",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16784/x2.png",
                "caption": "Figure 2:The pydantic class we use to create the JSON schema for constrained decoding.",
                "position": 163
            }
        ]
    },
    {
        "header": "3TIMRUN, the Inference Runtime for TIM",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16784/x3.png",
                "caption": "Figure 3:While TIM is decoding the conclusion of task 2, tokens in task 1.1.1 and 1.1.2, including the enclosed tool call and response have been pruned from the KV-cache. Although 1.1 and 1.2 are already aggregated in the conclusion of Task 1, we can optionally stack them in a pruning buffer before removing them from the KV cache. Following the notations in(Schroeder et¬†al.,2025),œàùúì\\psiitalic_œàstands for subtask spawning,œïitalic-œï\\phiitalic_œïis subtask aggregation, andfùëìfitalic_fappends a step in the current task list.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2507.16784/x4.png",
                "caption": "Figure 4:Comparing the communications among clients, tools, and different inference runtimes. Clients can be developers, applications, or agents. Red arrows indicate the client sends context tokens that are already processed by the inference engine, leading to redundant compute or repetive cost. With TIMRUN, each token is only sent to the language model once.",
                "position": 274
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16784/x5.png",
                "caption": "(a)",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2507.16784/x5.png",
                "caption": "(a)",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2507.16784/x6.png",
                "caption": "(b)",
                "position": 478
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]