[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x1.png",
                "caption": "Figure 1:Isoflops Pareto fronts for KronSAE vs. TopK SAE on Qwen-1.5B for different dictionary sizesF𝐹Fitalic_F.100 M tokens:KronSAE improves explained variance while heavily reducing parameters count.500 M tokens:maintains gains across most sizes.1000 M tokens:matches baseline quality with significantly fewer parameters; Stars denote TopK baselines and markers denote KronSAE configurations(m,n,h)𝑚𝑛ℎ(m,n,h)( italic_m , italic_n , italic_h ). See details insection4.1.",
                "position": 197
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x2.png",
                "caption": "Figure 2:Iso-FLOPs ablation: EV against head counthℎhitalic_hand base dimensionm𝑚mitalic_mfor KronSAE under 100M, 500M, and 1B token budgets. Higherhℎhitalic_hand smallerm𝑚mitalic_myield improved reconstruction quality.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x3.png",
                "caption": "(a)",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x3.png",
                "caption": "(a)",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x4.png",
                "caption": "(b)",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x5.png",
                "caption": "Figure 4:Feature absorption metrics on Qwen-2.5-1.5B. KronSAE configurations (variousm,n𝑚𝑛m,nitalic_m , italic_n) exhibit lower mean absorption fractions and full-absorption scores across differentℓ0subscriptℓ0\\ell_{0}roman_ℓ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.",
                "position": 462
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x6.png",
                "caption": "Figure 5:We generate data with covariance matrix that consist of blocks with different sizes on diagonal and off diagonal (left panel). We then examine the decoder-weight covarianceWd⁢e⁢c⋅Wd⁢e⁢cT⋅subscript𝑊𝑑𝑒𝑐superscriptsubscript𝑊𝑑𝑒𝑐𝑇W_{dec}\\cdot W_{dec}^{T}italic_W start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT ⋅ italic_W start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTto assess feature-embedding correlations. Panels (middle panels) show that a TopK SAE recovers these correlation structures only weakly, even after optimal atom matching. In contrast, KronSAE (right panel) more accurately reveals the original block patterns.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x7.png",
                "caption": "Figure 6:Correlations between features in KronSAE withm=4,n=4formulae-sequence𝑚4𝑛4m=4,n=4italic_m = 4 , italic_n = 4within a head and with features from other heads. Our design induces higher correlations within a group, which also gets stronger after training, although SAE have also learned correlated features from different heads.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x8.png",
                "caption": "Figure 7:Distribution of properties for TopK SAE and KronSAE (m=4,n=4formulae-sequence𝑚4𝑛4m=4,n=4italic_m = 4 , italic_n = 4) with 32k dictionary size trained on Qwen2.5-1.5B. Pre and Post suffixes denote pre- and post-Cartesian product latents. Our SAE achieves better interpretability scores by learning specialized feature groups, indicated by lower activation frequency and lower variance in activated tokens.",
                "position": 531
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details About Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x9.png",
                "caption": "Figure 8:Isoflops comparison of KronSAE and TopK SAE on Pythia-1.4B across three token budgets.",
                "position": 1113
            }
        ]
    },
    {
        "header": "Appendix BMore Results on Synthetic",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x10.png",
                "caption": "Figure 9:Comparision of how different KronSAE try to reveal hidden structure of defined covariance matrix. The KronSAE models recover the underlying block-structured correlations more faithfully than the TopK baseline, with the finer head/composition split(h=4,m=2)formulae-sequenceℎ4𝑚2(h=4,m=2)( italic_h = 4 , italic_m = 2 )capturing smaller feature groups more accurately.",
                "position": 1127
            }
        ]
    },
    {
        "header": "Appendix CmAND as a Logical Operator",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x11.png",
                "caption": "Figure 10:Comparison of the smoothmANDmAND\\operatorname{mAND}roman_mANDoperator against theANDAILsubscriptANDAIL\\operatorname{AND_{AIL}}roman_AND start_POSTSUBSCRIPT roman_AIL end_POSTSUBSCRIPT[Lowe et al.,2021].",
                "position": 1244
            }
        ]
    },
    {
        "header": "Appendix DFeature Analysis Methodology",
        "images": []
    },
    {
        "header": "Appendix EAdditional Feature Analysis Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x12.png",
                "caption": "Figure 13:Correlation coefficients (Pearson and Spearman) between properties of TopK and KronSAE latents. Token entropy emerges as a strong predictor of interpretability scores, while higher mean activation and lower frequency also indicate more interpretable features.",
                "position": 1394
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x13.png",
                "caption": "Figure 14:Correlation patterns between properties of post-latents and pre-latents.",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x14.png",
                "caption": "Figure 15:UMAP visualization of post-latent clustering patterns by head, base, and extension group membership. We observe tight clusters by base form<n𝑚𝑛m<nitalic_m < italic_nand by extension form≥n𝑚𝑛m\\geq nitalic_m ≥ italic_n.",
                "position": 1409
            }
        ]
    },
    {
        "header": "Appendix FKronSAE in Terms of Tensor Diagram",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x15.png",
                "caption": "Figure 16:For a single head, the KronSAE encoder architecture separates the input𝒙𝒙\\boldsymbol{x}bold_italic_xinto two distinct components,𝒑𝒑\\boldsymbol{p}bold_italic_pand𝒒𝒒\\boldsymbol{q}bold_italic_q, via matrix multiplications withWe⁢n⁢cpsuperscriptsubscript𝑊𝑒𝑛𝑐𝑝W_{enc}^{p}italic_W start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPTandWe⁢n⁢cqsuperscriptsubscript𝑊𝑒𝑛𝑐𝑞W_{enc}^{q}italic_W start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPTaccordingly. These components are then combined via the outer product𝒑⊗𝒒tensor-product𝒑𝒒\\boldsymbol{p}\\otimes\\boldsymbol{q}bold_italic_p ⊗ bold_italic_q, resulting in a matrix representation. To produce the final output vector𝐟𝐟\\mathbf{f}bold_f, this matrix is flattened into a single vector using a multi-index mapping.",
                "position": 1436
            }
        ]
    },
    {
        "header": "Appendix GAnalysis of Compositional Structure",
        "images": []
    }
]