[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x1.png",
                "caption": "Figure 1:Isoflops Pareto fronts for KronSAE vs.Â TopK SAE on Qwen-1.5B for different dictionary sizesFğ¹Fitalic_F.100 M tokens:KronSAE improves explained variance while heavily reducing parameters count.500 M tokens:maintains gains across most sizes.1000 M tokens:matches baseline quality with significantly fewer parameters; Stars denote TopK baselines and markers denote KronSAE configurations(m,n,h)ğ‘šğ‘›â„(m,n,h)( italic_m , italic_n , italic_h ). See details insection4.1.",
                "position": 197
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x2.png",
                "caption": "Figure 2:Iso-FLOPs ablation: EV against head counthâ„hitalic_hand base dimensionmğ‘šmitalic_mfor KronSAE under 100M, 500M, and 1B token budgets. Higherhâ„hitalic_hand smallermğ‘šmitalic_myield improved reconstruction quality.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x3.png",
                "caption": "(a)",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x3.png",
                "caption": "(a)",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x4.png",
                "caption": "(b)",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x5.png",
                "caption": "Figure 4:Feature absorption metrics on Qwen-2.5-1.5B. KronSAE configurations (variousm,nğ‘šğ‘›m,nitalic_m , italic_n) exhibit lower mean absorption fractions and full-absorption scores across differentâ„“0subscriptâ„“0\\ell_{0}roman_â„“ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.",
                "position": 462
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x6.png",
                "caption": "Figure 5:We generate data with covariance matrix that consist of blocks with different sizes on diagonal and off diagonal (left panel). We then examine the decoder-weight covarianceWdâ¢eâ¢câ‹…Wdâ¢eâ¢cTâ‹…subscriptğ‘Šğ‘‘ğ‘’ğ‘superscriptsubscriptğ‘Šğ‘‘ğ‘’ğ‘ğ‘‡W_{dec}\\cdot W_{dec}^{T}italic_W start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT â‹… italic_W start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTto assess feature-embedding correlations. Panels (middle panels) show that a TopK SAE recovers these correlation structures only weakly, even after optimal atom matching. In contrast, KronSAE (right panel) more accurately reveals the original block patterns.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x7.png",
                "caption": "Figure 6:Correlations between features in KronSAE withm=4,n=4formulae-sequenceğ‘š4ğ‘›4m=4,n=4italic_m = 4 , italic_n = 4within a head and with features from other heads. Our design induces higher correlations within a group, which also gets stronger after training, although SAE have also learned correlated features from different heads.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x8.png",
                "caption": "Figure 7:Distribution of properties for TopK SAE and KronSAE (m=4,n=4formulae-sequenceğ‘š4ğ‘›4m=4,n=4italic_m = 4 , italic_n = 4) with 32k dictionary size trained on Qwen2.5-1.5B. Pre and Post suffixes denote pre- and post-Cartesian product latents. Our SAE achieves better interpretability scores by learning specialized feature groups, indicated by lower activation frequency and lower variance in activated tokens.",
                "position": 531
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details About Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x9.png",
                "caption": "Figure 8:Isoflops comparison of KronSAE and TopK SAE on Pythia-1.4B across three token budgets.",
                "position": 1113
            }
        ]
    },
    {
        "header": "Appendix BMore Results on Synthetic",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x10.png",
                "caption": "Figure 9:Comparision of how different KronSAE try to reveal hidden structure of defined covariance matrix. The KronSAE models recover the underlying block-structured correlations more faithfully than the TopK baseline, with the finer head/composition split(h=4,m=2)formulae-sequenceâ„4ğ‘š2(h=4,m=2)( italic_h = 4 , italic_m = 2 )capturing smaller feature groups more accurately.",
                "position": 1127
            }
        ]
    },
    {
        "header": "Appendix CmAND as a Logical Operator",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x11.png",
                "caption": "Figure 10:Comparison of the smoothmANDmAND\\operatorname{mAND}roman_mANDoperator against theANDAILsubscriptANDAIL\\operatorname{AND_{AIL}}roman_AND start_POSTSUBSCRIPT roman_AIL end_POSTSUBSCRIPT[Lowe etÂ al.,2021].",
                "position": 1244
            }
        ]
    },
    {
        "header": "Appendix DFeature Analysis Methodology",
        "images": []
    },
    {
        "header": "Appendix EAdditional Feature Analysis Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x12.png",
                "caption": "Figure 13:Correlation coefficients (Pearson and Spearman) between properties of TopK and KronSAE latents. Token entropy emerges as a strong predictor of interpretability scores, while higher mean activation and lower frequency also indicate more interpretable features.",
                "position": 1394
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x13.png",
                "caption": "Figure 14:Correlation patterns between properties of post-latents and pre-latents.",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2505.22255/x14.png",
                "caption": "Figure 15:UMAP visualization of post-latent clustering patterns by head, base, and extension group membership. We observe tight clusters by base form<nğ‘šğ‘›m<nitalic_m < italic_nand by extension formâ‰¥nğ‘šğ‘›m\\geq nitalic_m â‰¥ italic_n.",
                "position": 1409
            }
        ]
    },
    {
        "header": "Appendix FKronSAE in Terms of Tensor Diagram",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22255/x15.png",
                "caption": "Figure 16:For a single head, the KronSAE encoder architecture separates the inputğ’™ğ’™\\boldsymbol{x}bold_italic_xinto two distinct components,ğ’‘ğ’‘\\boldsymbol{p}bold_italic_pandğ’’ğ’’\\boldsymbol{q}bold_italic_q, via matrix multiplications withWeâ¢nâ¢cpsuperscriptsubscriptğ‘Šğ‘’ğ‘›ğ‘ğ‘W_{enc}^{p}italic_W start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPTandWeâ¢nâ¢cqsuperscriptsubscriptğ‘Šğ‘’ğ‘›ğ‘ğ‘W_{enc}^{q}italic_W start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPTaccordingly. These components are then combined via the outer productğ’‘âŠ—ğ’’tensor-productğ’‘ğ’’\\boldsymbol{p}\\otimes\\boldsymbol{q}bold_italic_p âŠ— bold_italic_q, resulting in a matrix representation. To produce the final output vectorğŸğŸ\\mathbf{f}bold_f, this matrix is flattened into a single vector using a multi-index mapping.",
                "position": 1436
            }
        ]
    },
    {
        "header": "Appendix GAnalysis of Compositional Structure",
        "images": []
    }
]