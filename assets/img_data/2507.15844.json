[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15844/x1.png",
                "caption": "Figure 1:HBPO prevents exploration space collapse through hierarchical budget exploration. While efficient reasoning methods progressively abandon long reasoning paths during training, and length control methods like L1 achieve efficiency through mechanical constraints, HBPO partitions the exploration space into budget-constrained hierarchies (512, 1024, 2048, 2560 tokens). This structure maintains reasoning diversity throughout training, enabling emergent adaptive behavior where models match computational resources to problem complexity. The result is superior accuracy with efficient token usage varying from hundreds of tokens on GSM8K to thousands on AIME25.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15844/x2.png",
                "caption": "Figure 2:Overview of Hierarchical Budget Policy Optimization. Given a query, HBPO generates responses across multiple budget-constrained subgroups (512, 1024, 2048, 2560 tokens), each guided by a piecewise reward function that preserves exploration within budgets while penalizing excess through deviation penalties. The advantage computation decomposes into intra-subgroup advantages (comparing responses against budget-specific baselines) and inter-subgroup advantages (enabling cross-budget learning through global comparison). This hierarchical structure enables models to learn efficient reasoning within constraints and adaptive budget selection based on problem complexity.",
                "position": 210
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15844/x3.png",
                "caption": "Figure 3:Training Dynamics: Mean Token Length (solid Line) ± Standard Deviation (dotted line).",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2507.15844/x4.png",
                "caption": "Figure 4:Comparison of training dynamics and validation performance between cosine reward and budget-aware reward methods. (Left) Training token length evolution shows that cosine reward exhibits a sharp decline in early training stage(0-100 steps), while budget-aware reward maintains relatively consistent token lengths throughout training. (Middle) Validation accuracy demonstrates that budget-aware reward achieves sustained performance improvements with higher final accuracy, whereas cosine reward shows greater volatility and inferior performance. (Right) Validation token count reveals that budget-aware reward enables the model to gradually discover the optimal reasoning length, while cosine reward suffers from excessive compression from the early training stages, leading to suboptimal token generation.",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2507.15844/x5.png",
                "caption": "Figure 5:Reasoning pattern analysis across methods and problem difficulties. Thinking proportions and reflection keyword frequencies show HBPO’s adaptive adjustment, with keywords properly contained within thinking segments.",
                "position": 1139
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]