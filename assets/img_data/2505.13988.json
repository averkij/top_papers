[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13988/extracted/6456935/figs/hf-logo.png",
                "caption": "",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2505.13988/x1.png",
                "caption": "Figure 1:The figure illustrates thehallucination taxof standard reinforcement finetuning (RFT) and the effectiveness of incorporating Synthetic Unanswerable Math (SUM) data.Orange colored textindicates the deleted key information.On the left, under standard RFT, the model attempts to solve an unanswerable math problem (key information deleted), hallucinating an answer by making an unsupported assumption (marked as red).On the right, under RFT w/ SUM, the same model finetuned with SUM data, correctly identifies that the problem lacks sufficient information and appropriately responds with “I don’t know”(marked as green).",
                "position": 127
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Synthetic Unanswerable Math (SUM)",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13988/x2.png",
                "caption": "Figure 2:Refusal rate (higher is better) before and after RFT on three unanswerable datasets. The bar with backslashes denotes the performance before RFT; without backslashes denotes the performance after RFT. Different colors stand for different models. After RFT, the ability to refuse has a significant drop for all models.",
                "position": 345
            }
        ]
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13988/x3.png",
                "caption": "Figure 3:Learning dynamics of four LLMs during Reinforcement Finetuning (RFT) with varying mixing ratios (0%, 1%, 10%, 30%, and 50%) of unanswerable data. Each pair of plots shows the model’s average performance over training steps: on unanswerable datasets (left column, reflecting refusal capability) and on answerable datasets (right column, reflecting accuracy on solvable tasks).",
                "position": 666
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    }
]