[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/method_viz_teaser_v43.png",
                "caption": "Figure 1:Code synthesis with LMs trained on synthetic code edit sequences. Left: An example generation from an LM trained to synthesize code as a stream of static-error-free edits. Right: Comparing zero-shot HumanEval coverage (in %) as a function of FLOPs for large external LLMs vs the smaller LMs that we instruction finetune in this paper.Repeatedly sampling from edit sequence LMs yields coding problem solutions that are competitive with GPT-4 and GPT-4-Omni, and have total cost similar to sampling once from the best open-source LLMs(see AppendixF.4).",
                "position": 153
            }
        ]
    },
    {
        "header": "2LintSeq: Code Synthesis as a Sequential Edit Problem",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/algorithm_viz_v16.png",
                "caption": "Figure 2:Visualizing LintSeq, an algorithm for re-factoring programs into sequences of edits. This algorithm samples across all of the static error-free line insertions that can be used to write a program chunk-by-chunk. It uses the Unix-diff operator to express generated edit sequences as text strings.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/data_stats_viz_v3.png",
                "caption": "Figure 3:Empirics of processing code data with LintSeq. Left: Lines per example in a dataset of instruction finetuning data for Python synthesis before and after processing with LintSeq via the linterpylint(see Section3.2). LintSeq processing adds lines ofdiffmetadata to examples (see AppendixA). Right: The corresponding edit counts per synthetic code edit sequence. On a dataset of short programs (14 lines of code, on average), the mean LintSeq edit sequence contains four edits.",
                "position": 252
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/diff_vs_raw_finetuning_agg_v8.png",
                "caption": "Figure 4:HumanEval and MBPP coverage with repeated sampling(‚Äúpass@k vs. k‚Äù) achieved by instruction finetuning Gemma 2, Phi-3, and Llama 3.1 language models on a dataset of LintSeq edit sequence re-factored vs standard Python code (temperature=1absent1=1= 1, top-p=0.95absent0.95=0.95= 0.95).",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/linter_ablation_v12.png",
                "caption": "Figure 5:HumanEval and MBPP ‚Äúpass@k vs. k‚Äù achieved by finetuning TinyCodeLM models onlinter-guided vs randomly sampled code edit sequences(temperature=1absent1=1= 1, top-p=0.95absent0.95=0.95= 0.95).",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/code_error_rate_v5.png",
                "caption": "Figure 6:Comparingstatic error frequency in synthesized code samplesacross baseline vs edit sequence instruction finetuned model variants (n=50absent50=50= 50, temperature=1absent1=1= 1, top-p=0.95absent0.95=0.95= 0.95).",
                "position": 584
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion, Limitations, and Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore on Edit Sequences and Diffs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/diff_anatomy_v2.png",
                "caption": "Figure 7:The anatomy of a Unix diff: A diagrammatic visualization of the different parts of a Unix-style diff, as computed bydifflib. Thebodyof a diff can consist of multiple line deletions, followed by multiple line insertions. Thedecoratorportion of the diff respectively indicates the location and size of these deletions and insertions, if any. Like the diff shown above, the edits in synthetic edit sequences generated by LintSeq consist of line insertions only.",
                "position": 1410
            }
        ]
    },
    {
        "header": "Appendix BEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/prompting_examples_v2.png",
                "caption": "Figure 8:Examples of formatted HumanEval and MBPP(+) prompts used in model evaluations.",
                "position": 1458
            }
        ]
    },
    {
        "header": "Appendix CPretraining",
        "images": []
    },
    {
        "header": "Appendix DInstruction Finetuning",
        "images": []
    },
    {
        "header": "Appendix EMore on Synthetic Data Generation with LintSeq",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/data_example_A_v5.png",
                "caption": "Figure 9:LintSeq edit sequence samples vs baseline instruction-program data,example A.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/data_example_B_v5.png",
                "caption": "Figure 10:LintSeq edit sequence samples vs baseline instruction-program data,example B.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/tuning_lintseq_sample_count_v6.png",
                "caption": "Figure 11:Probing the effect of varying the number of edit sequences sampled with LintSeq per instruction-example pair during data generation: Using the source dataset described in Section3.2, we sweep over the value of the LintSeq parametersùë†sitalic_sused during synthetic data generation to yield three different edit sequence instruction datasets withs‚àà{1,5,10}ùë†1510s\\in\\{1,5,10\\}italic_s ‚àà { 1 , 5 , 10 }. We finetune TinyCodeLM models on each of these datasets, and compare the resultant HumanEval and MBPP(+) performance vs samples (i.e. pass@k vs k) at temperature 1. The most performant values iss=5ùë†5s=5italic_s = 5.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/tinycodelm150_pt_curves_v1.png",
                "caption": "Figure 12:Evaluating the zero-shot Python synthesis capabilities ofTinyCodeLM-150Mduring pretraining on HumanEval and MBPP(+).",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2410.02749/extracted/5927317/template/figures/tinycodelm400_pt_curves_v0.png",
                "caption": "Figure 13:Evaluating the zero-shot Python synthesis capabilities ofTinyCodeLM-400Mduring pretraining on HumanEval and MBPP(+).",
                "position": 1859
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results",
        "images": []
    }
]