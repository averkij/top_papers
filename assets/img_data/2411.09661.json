[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09661/x1.png",
                "caption": "Figure 1:TheAdaptiveDecoder. This learned module is added to the standard transformer in order to select decoding hyperparameters. It consists of a new decoder head attached to the last hidden state which assigns probabilities to different hyperparameter choices per token (right) or sequence (left), and the highest probability choice is selected in each case. This allows the LLM to select low temperatures for tokens requiring factual consistency, and higher temperatures for tasks requiring creativity and diversity. For the token level adaptive decoder, a different temperature can be selected for different parts of the response given a single instruction.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x2.png",
                "caption": "Figure 2:Latent Preference Optimization (LPO) Training Mechanism.We demonstrate how preference pairs are constructed for training the LPO loss (we show a Sequence-LevelAdaptiveDecoder, but the procedure remains the same for Token-Level). Here we have N=2 generated response samples for a single prompt, and the Reward Model (RM) scores Response1better than Response2. Therefore, we useτ=0.6𝜏0.6\\tau=0.6italic_τ = 0.6as the chosen temperature, andτ=0.2𝜏0.2\\tau=0.2italic_τ = 0.2as the rejected temperature, and then apply the loss to prefer the chosen temperature over the rejected one for the given context (prompt).",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09661/x3.png",
                "caption": "Figure 3:UltraMathStories Results.UltraMathStories is a superset of UltraFeedback, GSM8K, and Stories. The Adaptive Decoding models are trained on all 3 subtasks simultaneously. Winrates are shown as the average winrate across the test sets of the 3 subtasks in UltraMathStories.(left)AdaptiveDecoders⁢e⁢qsubscriptAdaptiveDecoder𝑠𝑒𝑞\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTvs Fixed Temperature Winrates.(right)AdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTvs Fixed Temperature Winrates. In both cases, Adaptive Decoding outperforms all fixed temperatures.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x4.png",
                "caption": "",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x5.png",
                "caption": "Figure 4:AdaptiveDecoders⁢e⁢qsubscriptAdaptiveDecoder𝑠𝑒𝑞\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTpredicted temperature distributions.We show the distribution of predicted temperatures on the test set of each subtask in UltraMathStories. As expected, the model predicts low temperatures for GSM8K, high temperatures for Stories, and temperatures mostly in between for UltraFeedback.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x5.png",
                "caption": "",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x6.png",
                "caption": "",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x7.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x8.png",
                "caption": "Figure 5:Constrained Creative Writing (ConstrainedStories) Results.Here we show a quantitative analysis of theAdaptiveDecoderon the constrained creative writing task, ConstrainedStories.(left)AdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTwinrates vs fixed temperatures.\nThe high fixed temperatures perform worse because they fail to follow the constraint. Fixed greedy decoding works well at following the constraint, butAdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPToutperforms it by using higher temperatures when possible.(right)Mean temperature predicted by theAdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTfor the first 50 tokens of each sentence. This plot confirms our hypothesis that the first token of each sentence should be low temperature in order to follow the constraint, and all other tokens should be high temperature in order to write a good story. The average temperature for the first token isτ=0.21𝜏0.21\\tau=0.21italic_τ = 0.21, and the average temperature for all other tokens isτ=0.55𝜏0.55\\tau=0.55italic_τ = 0.55, showing a more greedy decoding for the constraint, and less greedy everywhere else.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x8.png",
                "caption": "",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x9.png",
                "caption": "",
                "position": 625
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask Details",
        "images": []
    },
    {
        "header": "Appendix BAdaptiveDecoders⁢e⁢qsubscriptAdaptiveDecoder𝑠𝑒𝑞\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTWinrate Values",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09661/x10.png",
                "caption": "Figure 6:AdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTpredicted temperatures for Constrained Creative Story Writing.We demonstrate an example ofAdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTpredicted temperatures (τ𝜏\\tauitalic_τ) on the constrained creative story writing task for the prompt“Write a creative and coherent story with the following title. You must begin each sentence with a word that starts with “Ab”.\\n\\nTitle: The Village of the Blindfolded”. We can see that the model is more greedy (τ𝜏\\tauitalic_τclose to 0.0) when generating the constraint tokens (All sentences must begin with words that start with “Ab”), and less greedy (τ𝜏\\tauitalic_τclose to 1.0) on all other tokens.",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x10.png",
                "caption": "",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x11.png",
                "caption": "",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x12.png",
                "caption": "Figure 7:AdaptiveDecoders⁢e⁢qsubscriptAdaptiveDecoder𝑠𝑒𝑞\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTTraining Preference Distributions. Here we show the percentage of samples in the training set that are chosen or rejected for each of the 6 different temperateure (τ𝜏\\tauitalic_τ) values. The LPO loss uses both chosen and rejected responses, and the ratio of chosen to rejected is an important factor for learning the right temperature. A vanilla negative log-likelihood loss only uses the chosen responses, which leads to suboptimal temperature predictions since high temperature values are the most chosen regardless of the task.",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x12.png",
                "caption": "",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x13.png",
                "caption": "",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x14.png",
                "caption": "",
                "position": 1784
            }
        ]
    },
    {
        "header": "Appendix CAdaptiveDecodert⁢o⁢ksubscriptAdaptiveDecoder𝑡𝑜𝑘\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTWinrate Values",
        "images": []
    }
]