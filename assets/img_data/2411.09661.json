[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09661/x1.png",
                "caption": "Figure 1:TheAdaptiveDecoder. This learned module is added to the standard transformer in order to select decoding hyperparameters. It consists of a new decoder head attached to the last hidden state which assigns probabilities to different hyperparameter choices per token (right) or sequence (left), and the highest probability choice is selected in each case. This allows the LLM to select low temperatures for tokens requiring factual consistency, and higher temperatures for tasks requiring creativity and diversity. For the token level adaptive decoder, a different temperature can be selected for different parts of the response given a single instruction.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x2.png",
                "caption": "Figure 2:Latent Preference Optimization (LPO) Training Mechanism.We demonstrate how preference pairs are constructed for training the LPO loss (we show a Sequence-LevelAdaptiveDecoder, but the procedure remains the same for Token-Level). Here we have N=2 generated response samples for a single prompt, and the Reward Model (RM) scores Response1better than Response2. Therefore, we useÏ„=0.6ğœ0.6\\tau=0.6italic_Ï„ = 0.6as the chosen temperature, andÏ„=0.2ğœ0.2\\tau=0.2italic_Ï„ = 0.2as the rejected temperature, and then apply the loss to prefer the chosen temperature over the rejected one for the given context (prompt).",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09661/x3.png",
                "caption": "Figure 3:UltraMathStories Results.UltraMathStories is a superset of UltraFeedback, GSM8K, and Stories. The Adaptive Decoding models are trained on all 3 subtasks simultaneously. Winrates are shown as the average winrate across the test sets of the 3 subtasks in UltraMathStories.(left)AdaptiveDecodersâ¢eâ¢qsubscriptAdaptiveDecoderğ‘ ğ‘’ğ‘\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTvs Fixed Temperature Winrates.(right)AdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTvs Fixed Temperature Winrates. In both cases, Adaptive Decoding outperforms all fixed temperatures.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x4.png",
                "caption": "",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x5.png",
                "caption": "Figure 4:AdaptiveDecodersâ¢eâ¢qsubscriptAdaptiveDecoderğ‘ ğ‘’ğ‘\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTpredicted temperature distributions.We show the distribution of predicted temperatures on the test set of each subtask in UltraMathStories. As expected, the model predicts low temperatures for GSM8K, high temperatures for Stories, and temperatures mostly in between for UltraFeedback.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x5.png",
                "caption": "",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x6.png",
                "caption": "",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x7.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x8.png",
                "caption": "Figure 5:Constrained Creative Writing (ConstrainedStories) Results.Here we show a quantitative analysis of theAdaptiveDecoderon the constrained creative writing task, ConstrainedStories.(left)AdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTwinrates vs fixed temperatures.\nThe high fixed temperatures perform worse because they fail to follow the constraint. Fixed greedy decoding works well at following the constraint, butAdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPToutperforms it by using higher temperatures when possible.(right)Mean temperature predicted by theAdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTfor the first 50 tokens of each sentence. This plot confirms our hypothesis that the first token of each sentence should be low temperature in order to follow the constraint, and all other tokens should be high temperature in order to write a good story. The average temperature for the first token isÏ„=0.21ğœ0.21\\tau=0.21italic_Ï„ = 0.21, and the average temperature for all other tokens isÏ„=0.55ğœ0.55\\tau=0.55italic_Ï„ = 0.55, showing a more greedy decoding for the constraint, and less greedy everywhere else.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x8.png",
                "caption": "",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x9.png",
                "caption": "",
                "position": 625
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask Details",
        "images": []
    },
    {
        "header": "Appendix BAdaptiveDecodersâ¢eâ¢qsubscriptAdaptiveDecoderğ‘ ğ‘’ğ‘\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTWinrate Values",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.09661/x10.png",
                "caption": "Figure 6:AdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTpredicted temperatures for Constrained Creative Story Writing.We demonstrate an example ofAdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTpredicted temperatures (Ï„ğœ\\tauitalic_Ï„) on the constrained creative story writing task for the promptâ€œWrite a creative and coherent story with the following title. You must begin each sentence with a word that starts with â€œAbâ€.\\n\\nTitle: The Village of the Blindfoldedâ€. We can see that the model is more greedy (Ï„ğœ\\tauitalic_Ï„close to 0.0) when generating the constraint tokens (All sentences must begin with words that start with â€œAbâ€), and less greedy (Ï„ğœ\\tauitalic_Ï„close to 1.0) on all other tokens.",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x10.png",
                "caption": "",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x11.png",
                "caption": "",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x12.png",
                "caption": "Figure 7:AdaptiveDecodersâ¢eâ¢qsubscriptAdaptiveDecoderğ‘ ğ‘’ğ‘\\textsc{AdaptiveDecoder}_{seq}AdaptiveDecoder start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTTraining Preference Distributions. Here we show the percentage of samples in the training set that are chosen or rejected for each of the 6 different temperateure (Ï„ğœ\\tauitalic_Ï„) values. The LPO loss uses both chosen and rejected responses, and the ratio of chosen to rejected is an important factor for learning the right temperature. A vanilla negative log-likelihood loss only uses the chosen responses, which leads to suboptimal temperature predictions since high temperature values are the most chosen regardless of the task.",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x12.png",
                "caption": "",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x13.png",
                "caption": "",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2411.09661/x14.png",
                "caption": "",
                "position": 1784
            }
        ]
    },
    {
        "header": "Appendix CAdaptiveDecodertâ¢oâ¢ksubscriptAdaptiveDecoderğ‘¡ğ‘œğ‘˜\\textsc{AdaptiveDecoder}_{tok}AdaptiveDecoder start_POSTSUBSCRIPT italic_t italic_o italic_k end_POSTSUBSCRIPTWinrate Values",
        "images": []
    }
]