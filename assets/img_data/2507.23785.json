[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/x1.png",
                "caption": "",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/x2.png",
                "caption": "Figure 2:Framework of 4DMesh-to-GS Variation Field VAE.Our VAE directly encodes 3D animation data into Gaussian Variation Fields within a compact latent space, optimized through image-level reconstruction loss and the proposedmesh-guided loss.",
                "position": 127
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/method/DiT.jpg",
                "caption": "Figure 3:Architecture of Gaussian Variation Field diffusion model.Our model is built upon diffusion transformer, which takes noised latent as input and gradually denoises it conditioned on the video sequence and canonical GS.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/results/visual_comparison.jpg",
                "caption": "Figure 4:Qualitative comparison with previous state-of-the-art methods.Our model directly learns the distribution of Gaussian Variation Fields, enabling high-fidelity 4D generation with coherent temporal dynamics.",
                "position": 368
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/results/ours_visual.jpg",
                "caption": "Figure 5:More generation results of our model including in-the-wild videos (left) and videos from test set (right).",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2507.23785/x3.png",
                "caption": "Figure 6:Visual ablation of VAE.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2507.23785/x4.png",
                "caption": "Figure 7:Our model is also capable of creating animations for existing 3D assets with conditional videos.",
                "position": 465
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BData Preparation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/x5.png",
                "caption": "Figure 8:Sample of our autoregressive generation result.",
                "position": 2050
            }
        ]
    },
    {
        "header": "Appendix DMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/x6.png",
                "caption": "Figure 9:More generation results of real-world input videos.",
                "position": 2070
            }
        ]
    },
    {
        "header": "Appendix EBorader Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/supp/failure_cases/000030.png",
                "caption": "Figure 10:Failure case.When the pretrained static 3D generative model produces canonical GS that are not well-aligned with the conditional video frames, our Gaussian Variation Field diffusion model struggles to bridge this inconsistency, resulting in suboptimal animations.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/supp/failure_cases/000064.png",
                "caption": "",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/supp/failure_cases/rank_00_render_000000_cam_000_timesteps_00.png",
                "caption": "",
                "position": 2133
            },
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/supp/failure_cases/rank_00_render_000000_cam_000_timesteps_17.png",
                "caption": "",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2507.23785/x7.png",
                "caption": "Figure 11:Additional visual results of VAE reconstruction.",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2507.23785/x8.png",
                "caption": "Figure 12:More visual comparison with SOTA Methods.",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2507.23785/imgs/supp/supp_gt3d.jpg",
                "caption": "Figure 13:More results of animating existing 3D model input with conditional videos.",
                "position": 2168
            }
        ]
    },
    {
        "header": "Appendix FLimitation Discussion and Future Work",
        "images": []
    }
]