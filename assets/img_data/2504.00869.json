[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00869/x1.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00869/x2.png",
                "caption": "Figure 2:An overview of our data curation and training pipeline.We start with 196K raw medical QA examples, apply difficulty filtering (retaining 37K that Qwen2.5-7B-Instruct(Yang et al.,2024)or its 32B version cannot solve), then use DeepSeek-R1(Guo et al.,2025)to generate reasoning and keep correct solutions (m23K). We perform diversity sampling to select a 1K high-quality subset (m1K). These datasets are used to fine-tune base models (Qwen2.5 7B and 32B Instruct) via Supervised Fine-Tuning (SFT), resulting in the m1 models (m1-7B-1K,m1-7B-23K,m1-32B-1K).",
                "position": 180
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00869/x3.png",
                "caption": "Figure 3:Force thinking for different evaluation datasets.Accuracy vs. number of budget forcing times (iterations of injecting “Wait”) for each m1 model (7B-1K,7B-23K,32B-1K). A value of 0 means the model’s first answer is taken without forcing, while higher values mean the model was compelled to reconsider up to that many times (within a 2048-token limit).",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2504.00869/extracted/6327355/figures/case_knowledge.png",
                "caption": "Figure 4:A failure case of test-time scalingwith the Qwen2.5-7B using 1K reasoning data. Although the m1-7B-1K conducts the longest reasoning, its deficiency in essential medical knowledge prevents it from producing the right answer. On the other hand, both m1-7B-23K and m1-32B-1K effectively resolve the question with a relatively brief reasoning procedure.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2504.00869/extracted/6327355/figures/case_budget_fail_2.png",
                "caption": "Figure 5:A failure case of budget forcing.Initially, the model produces the correct answer, but forcing it to re-think causes the model to retrieve incorrect knowledge, ultimately resulting in an erroneous answer.",
                "position": 847
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00869/x4.png",
                "caption": "(a)m1K",
                "position": 1376
            },
            {
                "img": "https://arxiv.org/html/2504.00869/x4.png",
                "caption": "(a)m1K",
                "position": 1379
            },
            {
                "img": "https://arxiv.org/html/2504.00869/x5.png",
                "caption": "(b)m23K",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2504.00869/x6.png",
                "caption": "(c)s1K",
                "position": 1390
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00869/extracted/6327355/figures/case_budget_fail_2.png",
                "caption": "Figure 7:A failure case of budget forcing.",
                "position": 2537
            }
        ]
    },
    {
        "header": "Appendix CFailure Case of Budget Forcing",
        "images": []
    }
]