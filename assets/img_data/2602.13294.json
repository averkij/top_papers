[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13294/x1.png",
                "caption": "Figure 1:MLLMs struggle to simulate physical dynamics.Under the same inputs, code generated with rigid-body simulation backends (Three.js/P5.js) produces more physically consistent rollouts, whereas non-physics backends (SVG/Manim) often exhibit implausible motion or contact artifacts such as interpenetration.",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13294/x2.png",
                "caption": "Figure 2:Unlike traditional VQA paradigms,VisPhyWorld accesses physical understanding evaluationby requiring MLLMs to actively reconstruct scenes via executable code, offering superior reasoning explainability compared to traditional paradigms.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2602.13294/figures/visphyworld2.jpeg",
                "caption": "Figure 3:VisPhyWorld Framework.(1) System & Data Construction:We process raw video sequences to extract key frames (Istart,IlaterI_{\\text{start}},I_{\\text{later}}) and detection contexts using multimodal agents.(2) Pipeline & Simulation Flow:An LLM-based agent performs motion analysis and generates raw executable code, which is then sanitized and rendered.(3) Evaluation Benchmark:We propose a multi-metric benchmark integrating semantic and physical fidelity to compare generated videosX^\\hat{X}with ground truthXX.(4) A Detailed Case:A example illustrating how VisPhyWorld translates a collision scene (red ball hits block stack) into executable simulation logic.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VisPhyWorld",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13294/x3.png",
                "caption": "Figure 5:This case shows that VisPhyWorld exhibits strong physical grounding, correctly simulating the collision dynamics. More examples are in the Appendix.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x4.png",
                "caption": "Figure 6:GPT-5 reconstructs object identities and collision dynamics most faithfully over time. Pixel-space baselines (Veo-3.1 and SVD/img2vid) generate trajectories with implausible motion/contact events due to the lack of an explicit physics hypothesis.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x5.png",
                "caption": "Figure 7:This example highlights the dissociation between semantic alignment and correct physical dynamics: although Claude shows clear reconstruction deficits, its visual-semantic scores remain relatively high.",
                "position": 1056
            }
        ]
    },
    {
        "header": "Limitations and Discussion",
        "images": []
    },
    {
        "header": "Conclusions",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13294/x6.png",
                "caption": "Figure 8:A detailed case study (ID 2).",
                "position": 1772
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x7.png",
                "caption": "Figure 9:A detailed case study (ID 3).",
                "position": 1778
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x8.png",
                "caption": "Figure 10:A detailed case study (ID 4).",
                "position": 1785
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x9.png",
                "caption": "Figure 11:A detailed case study (ID 5).",
                "position": 1792
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x10.png",
                "caption": "Figure 12:A detailed case study (ID 6).",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x11.png",
                "caption": "Figure 13:A detailed case study (ID 7).",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2602.13294/x12.png",
                "caption": "Figure 14:A detailed case study (ID 8).",
                "position": 1813
            }
        ]
    },
    {
        "header": "Appendix BReproducibility Details",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Metrics: Definitions & Protocols",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13294/x13.png",
                "caption": "Figure 21:Per-scene boxplot distributions on VisPhyBench for representative metric families (higher is better unless markedâ†“\\downarrow).",
                "position": 2445
            }
        ]
    },
    {
        "header": "Appendix DDetailed Experimental Results",
        "images": []
    }
]