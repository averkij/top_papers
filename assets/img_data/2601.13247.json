[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13247/figures/worldmind_logo.png",
                "caption": "",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x1.png",
                "caption": "Figure 1:Conceptual Illustration of Experiential Alignment. The agent aligns its internal world model via Process Experience and Goal Experience.",
                "position": 169
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13247/x2.png",
                "caption": "Figure 2:Overview of the WorldMind Framework.The agent autonomously constructs a World Knowledge Repository (WKR) by unifying Process Experience (from prediction errors) and Goal Experience (from successful trajectories) to guide grounded simulation.",
                "position": 230
            }
        ]
    },
    {
        "header": "3The WorldMind Framework",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13247/x3.png",
                "caption": "(a)Performance of Cross-Model Experience Transfer",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x3.png",
                "caption": "(a)Performance of Cross-Model Experience Transfer",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x4.png",
                "caption": "(b)Accuracy Analysis on Embodied Web Agent Benchmark",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x5.png",
                "caption": "(c)Error Analysis on Embodied Web Agent Benchmark",
                "position": 1119
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x6.png",
                "caption": "(a)Habitat (GPT-3.5-turbo)",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x6.png",
                "caption": "(a)Habitat (GPT-3.5-turbo)",
                "position": 1134
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x7.png",
                "caption": "(b)Habitat (GPT-4.1-mini)",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x8.png",
                "caption": "(c)ALFRED (GPT-3.5-turbo)",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2601.13247/x9.png",
                "caption": "(d)ALFRED (GPT-4.1-mini)",
                "position": 1150
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Evaluation on Low-Level Navigation Tasks",
        "images": []
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    }
]