[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x1.png",
                "caption": "Figure 1:Overall Image Matching Accuracy and Efficiency on Six Datasets of Real Cross-modal Image Pairs.AUC of the pose error (@10∘superscript1010^{\\circ}10 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT) or reprojection error (@10101010px) is used for accuracy evaluation, while Pairs Per Second is used for efficiency test.Left: AUCs on each dataset of representative methods are reported.Right: average performance is summarized, wherein different colors indicate matching pipelines of sparse, semi-dense and dense matching, while our MINIMA is marked as★★\\bigstar★. Only with synthetic multimodal data created by our data engine, MINIMA can generalize to real cross-modal scenes with large improvements.",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x2.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x3.png",
                "caption": "Figure 2:Qualitative Results on Real Cross-modal Image Pairs.Our methods MINIMALG(sparse) and MINIMARoMa(dense) are compared with the sparse matching pipeline ReDFeat[7]and OmniGlue[20], and semi-dense matcher XoFTR[43]. ReDFeat and XoFTR are cross-modal methods, and OmniGlue is known for its generalization ability. Matches generated by each method are drawn, where the red lines indicate epipolar error (pose) or projection error (homography) beyond5×10−45superscript1045\\times 10^{-4}5 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPTor3333pixels. Details are recorded in the top-left of each image pair, including the geometric errors created by default RANSAC estimation and the (# correct match / # match).",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x4.png",
                "caption": "Figure 3:Overview of the proposed MINIMA pipeline: a single model for any cross-modal matching tasks. Wherein theData Engineis to generate a large multimodal image matching dataset, which supports the training of matching models to obtain cross-modal ability.",
                "position": 254
            }
        ]
    },
    {
        "header": "3Cross-Modal Generation with Data Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x5.png",
                "caption": "Figure 4:Pixel Intensity Statistic for Generated Modalities. The statistic differences reveal the excellent ability of our data engine to generate modality gaps.",
                "position": 300
            }
        ]
    },
    {
        "header": "4Modality Invariant Image Matching Model",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x6.png",
                "caption": "Figure 5:Training Loss and AUC@5∘superscript55^{\\circ}5 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPTw.r.t.Epochs, using Scratch Training and Fine-tuning. The basic model is LoFTR. The test set is our synthetic RGB-IR of MD-syn.",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x6.png",
                "caption": "",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x7.png",
                "caption": "",
                "position": 1358
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix A ADetails of Our Data Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x8.png",
                "caption": "Figure A1:Visualization Results of Infrared generation on MSRS. The first two columns are real RGB and Infrared images.",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x9.png",
                "caption": "Figure A2:Visualization Results of Infrared Generation on M3FD. The first two columns are real RGB and Infrared images.",
                "position": 2323
            }
        ]
    },
    {
        "header": "Appendix B BDetails of MINIMA",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19412/x10.png",
                "caption": "Figure A3:Qualitative Results on Real RGB-IR Image Pairs of METU-VisTIR[43]. The red lines indicate false matches.",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x11.png",
                "caption": "Figure A4:Qualitative results on real RGB-Depth image pairs of DIODE dataset[44]. The red lines indicate false matches.",
                "position": 3422
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x12.png",
                "caption": "Figure A5:Qualitative results on real RGB-Event image pairs of DSEC dataset[46]. The red lines indicate false matches.",
                "position": 3449
            },
            {
                "img": "https://arxiv.org/html/2412.19412/x13.png",
                "caption": "Figure A6:Qualitative results on real image pairs of cross-modal remote sensing dataset[21]. The red lines indicate false matches.",
                "position": 3454
            }
        ]
    },
    {
        "header": "Appendix C CAdditional Experimental Results",
        "images": []
    }
]