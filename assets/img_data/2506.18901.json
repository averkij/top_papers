[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18901/x1.png",
                "caption": "Figure 1:RealPlayis a neural-network-driven real-world game engine with three key characteristics: (1) It supports iterative interaction—at each iteration, users observe the current visual scene, provide control signals, and receive control-accurate, temporally consistent, and realistic video chunks in response. (2) It eliminates the need for annotated real-world data while exhibiting strong control transfer capabilities, effectively mapping control signals (e.g., “move forward”, “turn left” and “turn right”) from the game environment to the real world. (3) It demonstrates entity transfer capabilities: although the labeled game data are sourced exclusively from the car racing gameForza Horizon 5, RealPlay successfully generalizes these control signals to other real-world entities such as (a) bicycles and (b) pedestrians, beyond (c) vehicles.Additional visualizations are provided in the appendix.",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18901/x2.png",
                "caption": "Figure 2:RealPlay involves a two-stage training process.Stage-1: We adapt a pre-trained image-to-video generator (Figure (a))—which generates an entire video in a single pass conditioned on a single frame—into a chunk-wise generation model (Figure (b)), which generates video chunks iteratively, conditioned on the previously generated chunk. This adaptation includes several key modifications detailed in Section3.1.Stage-2: RealPlay (Figure (c)) is trained on a combination of a labeled game dataset and an unlabeled real-world dataset, enabling action transfer from controlling a car in the game environment to manipulating various entities in the real world. This is achieved by modifying the chunk-wise generation model to incorporate action control through an adaptive LayerNorm mechanism. In all figures, “frames” refer to frame latents encoded by the video VAE encoder from CogVideoX[48]. For clarity, we omit the details of injecting noise timestep embeddings.",
                "position": 112
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18901/x3.png",
                "caption": "Figure 3:Visual quality degrades in both game and real-world settings, but the image quality when controlling a game entity consistently remains higher than that of a real-world entity (e.g., the bicycle in this study), highlighting the greater challenge of modeling real-world entities.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x3.png",
                "caption": "Figure 3:Visual quality degrades in both game and real-world settings, but the image quality when controlling a game entity consistently remains higher than that of a real-world entity (e.g., the bicycle in this study), highlighting the greater challenge of modeling real-world entities.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x4.png",
                "caption": "Figure 4:Reducing the number of video latents per chunk leads to visual quality degradation, as the pre-trained video generator—originally optimized for long-horizon generation—loses temporal coherence and consistency when adapted to extremely short-horizon outputs (e.g., 1 latent).",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x5.png",
                "caption": "Figure 5:Both the control success rate and Elo scores steadily improve as training progresses. The evaluation is performed on the bicycle entity.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x5.png",
                "caption": "Figure 5:Both the control success rate and Elo scores steadily improve as training progresses. The evaluation is performed on the bicycle entity.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x6.png",
                "caption": "Figure 6:Increasing the number of labeled game samples consistently improves control success on real-world entities.",
                "position": 554
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Experiments",
        "images": []
    },
    {
        "header": "Appendix CMore Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18901/x7.png",
                "caption": "Figure 7:Visualizations of videos generated by the chunk-wise generation model.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x8.png",
                "caption": "Figure 8:Visualization of RealPlay’s capability to control cars in diverse game environments.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x9.png",
                "caption": "Figure 9:RealPlay’s effectiveness in controlling diverse real-world entities.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x10.png",
                "caption": "Figure 10:Long-horizon videos generated by RealPlay demonstrating sustained control over real-world entities.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x11.png",
                "caption": "Figure 11:RealPlay learns both entity and camera control; when no focused entity is present, it controls the camera alone.",
                "position": 679
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Broader Impacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18901/x12.png",
                "caption": "Figure 12:Qualitative comparison with baseline methods. The target action ismove left and then forward.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2506.18901/x13.png",
                "caption": "Figure 13:Qualitative comparison with baseline methods. The target action ismove left.",
                "position": 692
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]