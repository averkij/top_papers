[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19871/x1.png",
                "caption": "Figure 1:Comparison between standard vision-language diffusion models and our proposed refining-enhanced approach. (a)Mask-pred diffusionis trained for passive denoising (mask recovering under fixed context). An initial error, such as misidentifying the “bus” as a “trunk”, triggers an error cascade. The model cannot correct this mistake and proceeds to hallucinate further details based on the flawed context (e.g., “Two men are drinking”), leading to a factually incorrect description. (b)Refining-enhanced diffusionintroduces a paradigm of active refining, teaching the model not only to predict masked tokens but also to perform token refinement. Our ReDiff learns to self-correct through an online loop where its own “flawed drafts” are revised by an expert revisor. As a result, the model can identify and correct its initial mistakes (revising “trunk” to “bus”, “Two men” to “A man”), breaking the error cascade and generating a factually grounded response.\n(c) Performance comparison between LLaDA-V(You et al.,2025)and ReDiff under different inference speeds. “CLAIR” and “Coverage” are detailed caption metrics on CapMAS(Lee et al.,2024), and “CAPTURE” is on DetailCaps-4870(Dong et al.,2024). Our model delivers superior generation quality and achieves more stable results when using fewer inference steps.",
                "position": 77
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19871/x2.png",
                "caption": "Figure 2:Overview of our proposed two-stage training framework for corrective refining. (a) We illustrate common failure modes in standard vision-language diffusion models, which are prone to generating syntactic errors (e.g., “Domin bus bus”) and factual hallucinations (e.g., “a woman”). (b) In the foundational revision training stage, we instill a general corrective capability by training a base model (ReDiff-Base) to revise synthetic errors that are intentionally injected into ground-truth captions. (c) For the second stage, i.e., online self-correction learning, the model generates its own flawed “drafts”. These drafts, containing the model’s intrinsic errors, are then revised by an expert AI assistant. The resulting “draft-refined pairs” provide strong supervision, teaching our final model (ReDiff) to identify and correct its own characteristic mistakes, thus breaking the error cascade.",
                "position": 135
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19871/x3.png",
                "caption": "Figure 3:Cases comparison between LLaDA-V and our ReDiff under 4 tokens/step inference speed. ReDiff demonstrates superior fluency and accuracy in its generated captions.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2510.19871/x4.png",
                "caption": "Figure 4:Refinement process of ReDiff at different inference step.Red tokensindicate the errors produced during generation, whilegreen tokensmean the corresponding refined results.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2510.19871/x5.png",
                "caption": "Figure 5:Generation results of ReDiff with or w/o refinement during inference.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2510.19871/x6.png",
                "caption": "Figure 6:ReDiff can revise wrong input answers.",
                "position": 715
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19871/x7.png",
                "caption": "Figure 7:Cases comparison between LLaDA-V and our ReDiff under 2 tokens/step inference speed.",
                "position": 1300
            },
            {
                "img": "https://arxiv.org/html/2510.19871/x8.png",
                "caption": "Figure 8:Cases comparison between LLaDA-V and our ReDiff under 8 tokens/step inference speed.",
                "position": 1303
            }
        ]
    },
    {
        "header": "Appendix BPrompt for Stage II data Construction",
        "images": []
    },
    {
        "header": "Appendix CEthics Statement",
        "images": []
    }
]