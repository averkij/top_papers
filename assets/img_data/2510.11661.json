[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11661/assets/git.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2510.11661/assets/HF.png",
                "caption": "",
                "position": 151
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11661/x1.png",
                "caption": "Figure 1:The inference framework of SR-Scientist. At each iteration, the LLM agent autonomously conducts long-horizon optimization using code interpreters for data analysis and equation evaluation. To overcome the context length limitations of current LLMs, we implement an experience buffer to fetch the best-performing equations for subsequent iterations. ‘Eq’ denotes the equation and ‘S’ denotes the equation score.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SR-Scientist",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11661/figure/ood_qwen3_coder_480b.png",
                "caption": "Figure 2:Detailed results of in-domain (ID) and out-of-domain (OOD) performance usingAcc0.01\\text{Acc}_{0.01}across various\nLSR-Synth scientific domains. (with Qwen3-Coder-480B as LLM backbone)",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/noise_level.png",
                "caption": "Figure 3:Noise robustness analysis. Qwen, GLM, and GPT represent Qwen3-Coder-480B, GLM-4.5-Air, and GPT-OSS-120B, respectively.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/noise_level.png",
                "caption": "Figure 3:Noise robustness analysis. Qwen, GLM, and GPT represent Qwen3-Coder-480B, GLM-4.5-Air, and GPT-OSS-120B, respectively.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/assistant_number.png",
                "caption": "Figure 4:Overall performance under different maximum turns. We keep the total number of LLM calls at around 1,000 and trade off between maximum turns and iterations.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/assistant_number.png",
                "caption": "Figure 4:Overall performance under different maximum turns. We keep the total number of LLM calls at around 1,000 and trade off between maximum turns and iterations.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/case_studies.png",
                "caption": "Figure 5:Equations discovered for the PO10 and PO37 physics problems. The variablesv˙​(t)\\dot{v}(t),tt,xx, andvvrepresent acceleration in a non-linear harmonic oscillator, time, position, and velocity, respectively. Terms highlighted in green are common to both the predicted and ground truth equations.",
                "position": 923
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/tool_call.png",
                "caption": "Figure 6:The tool call behaviors of different models.",
                "position": 937
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11661/appendix/length_curve.png",
                "caption": "Figure 7:The change in response length during training.",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2510.11661/appendix/length_curve.png",
                "caption": "Figure 7:The change in response length during training.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2510.11661/appendix/reward_curve.png",
                "caption": "Figure 8:The change in reward score during training.",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/ood_glm45_air.png",
                "caption": "Figure 9:Detailed results of in-domain (ID) and out-of-domain (OOD) performance usingAcc0.01\\text{Acc}_{0.01}across various LSR-Synth scientific domains. (with GLM-4.5-Air as LLM backbone)",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2510.11661/figure/ood_oss_120b.png",
                "caption": "Figure 10:Detailed results of in-domain (ID) and out-of-domain (OOD) performance usingAcc0.01\\text{Acc}_{0.01}across various LSR-Synth scientific domains. (with GPT-OSS-120B as LLM backbone)",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2510.11661/appendix/ood_oss_20b.png",
                "caption": "Figure 11:Detailed results of in-domain (ID) and out-of-domain (OOD) performance usingAcc0.01\\text{Acc}_{0.01}across various LSR-Synth scientific domains. (with GPT-OSS-20B as LLM backbone)",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2510.11661/appendix/ood_qwen3_coder_30B.png",
                "caption": "Figure 12:Detailed results of in-domain (ID) and out-of-domain (OOD) performance usingAcc0.01\\text{Acc}_{0.01}across various LSR-Synth scientific domains. (with Qwen3-Coder-30B as LLM backbone)",
                "position": 1976
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]