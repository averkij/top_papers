[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Backgrounds and Related Works",
        "images": []
    },
    {
        "header": "3Wide–narrow–wide incremental–improving MLP",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01796/figures/250924_hourglass_mlp.png",
                "caption": "Figure 1:(a) Illustration of a wide-narrow-wide MLP block. The two endpointsziz_{i}andzi+1z_{i+1}have a higher dimensionality compared to the hiddenhih_{i}. Skip connection thus connects two high–dimensional endpoints, rather than two low-dimensional ones in existing convention. Components that do not depend on dimensionality (e.g., normalization, element-wise nonlinearity) are omitted for clarity. (b) Illutration of a full network whose core is a stack of wide-narrow-wide MLP blocks. An input projection networkWinW_{\\text{in}}is required to adapt the input dimensionality ofxxto the dimensionality of the latentzz. An output projection networkWoutW_{\\text{out}}is used to adapt to the desired task.",
                "position": 259
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01796/figures/mnist/class_to_prototype/psnr_batch_pareto_frontier_overall_polyline.png",
                "caption": "Figure 2:Generative Classification Task on MINST.(a) Performance–complexity Pareto front. Fronts are searched with each configuration repeated 5 times. \"Wide–narrow–wide\" MLPs outperform conventional \"narrow–wide–narrow\" ones. (b) Samples predicted by our proposed Hourglass model.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/fig2.png",
                "caption": "",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/mnist/denoising/psnr_batch_pareto_frontier_overall_polyline.png",
                "caption": "Figure 3:Generative Restoration Task - Denoising.Performance-complexity Pareto fronts on MINST and ImageNet-32 are searched with each configuration repeated 5 times. Optimal configurations are shown in Table2.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/imagnet32/denoise/psnr_batch_pareto_frontier_overall_polyline.png",
                "caption": "",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/mnist/super_resolution/down_2.0/psnr_batch_pareto_frontier_overall_polyline.png",
                "caption": "Figure 4:Generative Restoration Task - Super-resolution.Performance-complexity Pareto fronts on MINST and ImageNet-32 are searched with each configuration repeated 5 times. Optimal configurations are shown in Table2.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/imagnet32/super_resolution/psnr_batch_pareto_frontier_overall_polyline.png",
                "caption": "",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/ablation/freeze_hourglass/comparison_imagenet_freeze_in.png",
                "caption": "Figure 5:Input projection fixed with a random projection matrix.Comparison between fixed and trainable input projectionWinW_{\\text{in}}for Hourglass MLP on ImageNet-32 denoising. We use architecture(dz,dh,L)=(3546,270,5)(d_{z},d_{h},L)=(3546,270,5). The fixed-projection model performs comparably to the trainable one.",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/ablation/vary_d/psnr_batch_mid_sweep.png",
                "caption": "(a)Varying the bottleneck dimensiondhd_{h}",
                "position": 1020
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/ablation/vary_d/psnr_batch_mid_sweep.png",
                "caption": "(a)Varying the bottleneck dimensiondhd_{h}",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2510.01796/figures/ablation/vary_L/psnr_batch_mlpnum_sweep.png",
                "caption": "(b)Varying the number of residual blocksLL",
                "position": 1030
            }
        ]
    },
    {
        "header": "5Discussions and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01796/figures/250924_hourglass_LLM.png",
                "caption": "Figure 7:Extend the wide-narrow-wide intuition to the transformer.(a) The classic transformer block with Multi-Head Self-Attention and a conventional narrow–wide–narrow FFN. (b) A modified transformer block with block with one or more wide–narrow–wide FFNs and a dimensionality compliant multi-head latent attention sublayer. Components that do not change dimensionality (e.g., normalization, elementwise nonlinearity) are omitted for clarity.",
                "position": 1068
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]