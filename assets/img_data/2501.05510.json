[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/intro_parallel_del.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3OVO-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05510/x1.png",
                "caption": "Figure 2:Examples of each task in OVO-Bench.The 14 tasks are categorized into three different kinds of perceiving modes in online video understanding:Backward Tracing,Real-Time Visual Perception, andForward Active Responding.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2501.05510/x2.png",
                "caption": "Figure 3:Generation pipeline of OVO-Bench.Within public annotations,\ndata is carefully filtered and relevant multiple-choice QAs are auto-generated.\nThe effective system prompt and efficient answer prompt are employed to guide MLLMs toward precise outputs. The Video-LLMs we use to annotate videos are GPT-4o and Gemini-1.5 Pro.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/query_freq.png",
                "caption": "Figure 4:LeftQueries Temporal Distribution in OVO-Bench.CenterLinguistic Characteristics of Text Queries.RightVideo category distribution of OVO-Bench.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/wordcloud.png",
                "caption": "",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/pie_chart.png",
                "caption": "",
                "position": 352
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/model0.png",
                "caption": "Figure 5:Performance comparison between online Video-LLMs and offline Video-LLMs.The figure illustrates the average scores of different models on the OVO-Bench in real-time visual perception tasks.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2501.05510/x3.png",
                "caption": "Figure 6:Multiple triggering evaluation pipeline of prompt offline models for online video understanding.Offline Video-LLMs are densely queried along the temporal axes to make independent decisions of whether existing visual content provide enough clues for answering.",
                "position": 723
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05510/x4.png",
                "caption": "Figure 7:Prompts used for Online(up) and Offline(down) Models onForward Active Respondingand Response Examples.Despite our vision for online models, existing online models, like videollm-online, are still far from satisfactory, showing limited adaptation ability, and would easily encounter collapse when processing complicated or out-of-training-domain video and queries. Offline models are inclined to perform random guessing when the queries contain words like ”is/currently/ongoing”.",
                "position": 1669
            },
            {
                "img": "https://arxiv.org/html/2501.05510/x5.png",
                "caption": "Figure 8:Prompts used for Online(up) and Offline(down) Models onReal-Time Visual Perceptionand Response Examples.Three tasks including [ACR], [OCR], and [ASI] are included as demonstrations. Our benchmarks involve a large ratio of questions, whose answers shift over time, which means that models can hardly figure out the answer by randomly selecting frames from original videos.",
                "position": 1674
            }
        ]
    },
    {
        "header": "6More Details of Evaluation",
        "images": []
    },
    {
        "header": "7More Details of Benchmark Construction",
        "images": []
    },
    {
        "header": "8Additional Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/supplimentary/222.png",
                "caption": "Figure 9:Distribution of questions and video in OVO-Bench.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2501.05510/extracted/6119562/sec/image/supplimentary/111.png",
                "caption": "Figure 10:Distribution of averaged query timestamps and video duration (in seconds) in OVO-Bench.Specifically, the averaged video duration in CRR is  6,857 seconds.",
                "position": 1916
            }
        ]
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "10Licenses",
        "images": []
    },
    {
        "header": "11Data Examples",
        "images": []
    }
]