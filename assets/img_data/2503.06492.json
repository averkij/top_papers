[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x1.png",
                "caption": "Figure 1:Illustration of an example in VisualSimpleQA. The red box highlights the region of interest (ROI).\nEach sample has several attributes and tags, which allow us to measure its overall difficulty score based on our proposed difficulty criteria.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Key Features of VisualSimpleQA",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x2.png",
                "caption": "Figure 2:Decoupled evaluation process.",
                "position": 348
            }
        ]
    },
    {
        "header": "4Annotation and Verification",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x3.png",
                "caption": "Figure 3:Flowchart of the annotation process. Evidence is used to guarantee the correctness of the answer, while ROI is annotated to calculate the difficulty of each sample.",
                "position": 445
            }
        ]
    },
    {
        "header": "5Statistics of VisualSimpleQA",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x4.png",
                "caption": "Figure 4:Distribution of topics in VisualSimpleQA.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x5.png",
                "caption": "Figure 5:Distributions of factors that influence the difficulty of visual recognition. TI denotes Text in Image.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x6.png",
                "caption": "",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x7.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x8.png",
                "caption": "",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x9.png",
                "caption": "Figure 6:Distributions of knowledge popularity and overall difficulty.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x10.png",
                "caption": "",
                "position": 521
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x11.png",
                "caption": "Figure 7:Ratio of failures (incorrect responses and refusals) by GPT-4o across varying difficulty levels. The left sub-figure is based on samples where GPT-4o correctly answers the text-only questions but fails to answer the multimodal questions. The right is based on samples where GPT-4o fails to answer the text-only questions.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x12.png",
                "caption": "",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x13.png",
                "caption": "Figure 8:Ratio of correctly answered questions by GPT-4o across varying difficulty levels. The left sub-figure is based on samples where GPT-4o correctly answers the text-only questions and the multimodal questions. The right is based on samples where GPT-4o correctly answer the text-only questions.",
                "position": 846
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x14.png",
                "caption": "",
                "position": 849
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    },
    {
        "header": "Appendix BDisplay of Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x15.png",
                "caption": "",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x16.png",
                "caption": "",
                "position": 1507
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x17.png",
                "caption": "",
                "position": 1509
            }
        ]
    },
    {
        "header": "Appendix CModality-Specific Modules in LVLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06492/x18.png",
                "caption": "Figure 9:Ratio of failures (incorrect responses and refusals) across varying difficulty levels. The upper sub-figures are based on samples where models correctly answer the text-only questions but fail to answer the multimodal questions. The bottom sub-figures are based on samples where models fail to answer the text-only questions.",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x19.png",
                "caption": "",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x20.png",
                "caption": "",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x21.png",
                "caption": "",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x22.png",
                "caption": "",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x23.png",
                "caption": "",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x24.png",
                "caption": "Figure 10:Ratio of correctly answered questions across varying difficulty levels. The upper sub-figures are based on samples where models correctly answer the text-only questions and the multimodal questions. The bottom sub-figures are based on samples where models correctly answer the text-only questions.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x25.png",
                "caption": "",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x26.png",
                "caption": "",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x27.png",
                "caption": "",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x28.png",
                "caption": "",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2503.06492/x29.png",
                "caption": "",
                "position": 1811
            }
        ]
    },
    {
        "header": "Appendix DSupplementary Experimental Results",
        "images": []
    }
]