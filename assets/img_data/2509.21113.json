[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21113/figures/assets/1_reasoning_and_prediction_across_different_mllm_v3.png",
                "caption": "Figure 1:Illustration of the responses across different models on the video state prediction task, wheregreentext indicates correctly reasoned key points andredtext denotes reasoning errors. Comparative analysis reveals that MOSS-ChatV captures more fine-grained states (e.g., the surferâ€™s crouched position) compared to other models. Crucially, it accurately extrapolates this state (preparing for a maneuver), thereby achieving more coherent and correct reasoning.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Process Reasoning Reward",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21113/figures/assets/3_overall_pipeline_v2.png",
                "caption": "Figure 2:Overall training pipeline of MOSS-ChatV.\n(a) Construction of the MOSS-Video dataset from ShareGPT4Video with multi-level temporal annotations, where future states are masked as prediction targets.\n(b) Subsequence DTW alignment: green dashed lines denote strict sequential matching, while red solid lines allow jumps (jump step=2) to reduce cumulative distance.\n(c) GRPO workflow integrating accuracy, format, and process rewards.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21113/figures/assets/4_comparison_across_frames.png",
                "caption": "Figure 3:Performance impact of varying input frame counts.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2509.21113/x1.png",
                "caption": "Figure 4:Figure (a) shows that with Subsequence DTW (SDTW), response lengths initially fluctuate due to exploration but gradually converge to a stable range. Figure (b) reports training without process supervision, where response lengths remain unstable. Figure (c) illustrates Naive DTW, which induces reward hacking: the model shortens its reasoning drastically to exploit the distance metric.",
                "position": 768
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Ethics Statement",
        "images": []
    },
    {
        "header": "8Reproducibility Statement",
        "images": []
    },
    {
        "header": "9LLM Usage Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of MOSS-Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21113/example_MOSS_Video.png",
                "caption": "Figure A.1:The example of MOSS-Video.",
                "position": 1393
            }
        ]
    },
    {
        "header": "Appendix BMLLM-as-a-Judge for Reasponse Quality",
        "images": []
    }
]