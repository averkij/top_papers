[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/files/withanyone_logo_v3.jpg",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x1.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x2.png",
                "caption": "",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x3.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x4.png",
                "caption": "",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x5.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x6.png",
                "caption": "",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x7.png",
                "caption": "Figure 2:Our Observation. Natural variations, such as head pose, expression, and makeup, may cause more face similarity decrease than expected. Copying reference image limits models’ ability to respond to expression and makeup adjustment prompts.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x8.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x9.png",
                "caption": "",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x10.png",
                "caption": "Figure 3:Overview of WithAnyone.It builds on a large-scale dataset, MultiID-2M, constructed through a four-step pipeline: (1) collect and cluster single-ID data based on identity similarity; (2) gather multi-ID data via targeted searches using desired identity names with negative keywords for filtering; (3) form image pairs by matching faces between single-ID and multi-ID data; and (4) apply post-processing for quality control and stylization. Training proceeds in four stages: (1) pre-train on single-ID, multi-ID, and open-domain images with fixed prompts; (2) train with image-caption supervision; (3) fine-tune with ID-paired data; and (4) perform quality tuning using a curated high-quality subset.",
                "position": 192
            }
        ]
    },
    {
        "header": "3MultiID-2M: Paired Multi-Person Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x11.png",
                "caption": "(a)Model Architecture",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x11.png",
                "caption": "(a)Model Architecture",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x12.png",
                "caption": "(b)Training Objectives",
                "position": 229
            }
        ]
    },
    {
        "header": "4MultiID-Bench: Comprehensive ID Customization Evaluation",
        "images": []
    },
    {
        "header": "5WithAnyone: Controllable and ID-Consistent Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x13.png",
                "caption": "(a)Single-ID subset",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x13.png",
                "caption": "(a)Single-ID subset",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x14.png",
                "caption": "(b)Multi-ID subset",
                "position": 658
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x15.png",
                "caption": "Figure 6:Qualitative Results of Different Generation Methods.The text prompt is extracted from the ground-truth image shown on the leftmost side.",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x16.png",
                "caption": "Figure 7:Comparison of GT-aligned and Prediction-aligned landmarks.",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x17.png",
                "caption": "",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x18.png",
                "caption": "Figure 8:User study.Bigger bubbles indicate higher ranking.",
                "position": 1067
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFamily of WithAnyone",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x19.png",
                "caption": "Figure 9:Application of WithAnyone-Kontext.Marrying editing models, WithAnyone is capable of face editing given customization references.",
                "position": 2028
            }
        ]
    },
    {
        "header": "Appendix BMultiID-2M Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x20.png",
                "caption": "(a)ID Appearance.",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x20.png",
                "caption": "(a)ID Appearance.",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x21.png",
                "caption": "(b)Nationality Distribution.",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x22.png",
                "caption": "(c)Benchmark Distribution.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x23.png",
                "caption": "(a)Clothes & Accessories Distribution.",
                "position": 2091
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x23.png",
                "caption": "(a)Clothes & Accessories Distribution.",
                "position": 2094
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x24.png",
                "caption": "(b)Action Distribution.",
                "position": 2099
            }
        ]
    },
    {
        "header": "Appendix CBenchmark and Metrics Details",
        "images": []
    },
    {
        "header": "Appendix DGalleries of WithAnyone",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x25.png",
                "caption": "Figure 12:Galleries of Single-ID Generation.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x26.png",
                "caption": "Figure 13:Galleries of 2-person Generation.",
                "position": 2237
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x27.png",
                "caption": "Figure 14:Galleries of 3-to-4-person Generation.",
                "position": 2240
            }
        ]
    },
    {
        "header": "Appendix EModel Framework Details",
        "images": []
    },
    {
        "header": "Appendix FExperimental Details",
        "images": []
    },
    {
        "header": "Appendix GAblation Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/x28.png",
                "caption": "Figure 15:ID Loss Curves withλ×\\lambda\\timesInfoNCE Loss.0.10.1is0.1×0.1\\timesInfoNCE Loss without extended negative samples, and0.1+0.1+is0.1×0.1\\timesInfoNCE Loss with extended negative samples.",
                "position": 2312
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x28.png",
                "caption": "Figure 15:ID Loss Curves withλ×\\lambda\\timesInfoNCE Loss.0.10.1is0.1×0.1\\timesInfoNCE Loss without extended negative samples, and0.1+0.1+is0.1×0.1\\timesInfoNCE Loss with extended negative samples.",
                "position": 2315
            },
            {
                "img": "https://arxiv.org/html/2510.14975/x29.png",
                "caption": "Figure 16:Trade-off Curveswithλ×\\lambda\\timesSiglip and(1−λ)×(1-\\lambda)\\timesArcFace signal.",
                "position": 2320
            }
        ]
    },
    {
        "header": "Appendix HUser Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14975/files/user_study_interface.jpg",
                "caption": "Figure 17:User Study Interface.",
                "position": 2387
            }
        ]
    },
    {
        "header": "Appendix IPrompts for Language Models",
        "images": []
    }
]