[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19205/x1.png",
                "caption": "",
                "position": 61
            },
            {
                "img": "https://arxiv.org/html/2508.19205/x2.png",
                "caption": "",
                "position": 70
            },
            {
                "img": "https://arxiv.org/html/2508.19205/x3.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2508.19205/x4.png",
                "caption": "",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2508.19205/x5.png",
                "caption": "Figure 1:VibeVoiceis capable of synthesizing 5,000+ seconds of audio while consistently outperforming strong open/closed-source systems in subjective evaluations of preference, realism, and richness.",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19205/x6.png",
                "caption": "Figure 2:VibeVoiceemploys next token diffusion framework as in LatentLMSBW+(24)to synthesize long-form and multi-speaker audios.\nVoice prompts and text scripts provide initial input.VibeVoiceprocesses hybrid context features, and its hidden states condition a token level Diffusion Head (D), which predicts acoustic VAE for speech segments, subsequently recovered by acoustic decoder (A).",
                "position": 118
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Results",
        "images": []
    },
    {
        "header": "4Conclusion, Limitations, and Risks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]