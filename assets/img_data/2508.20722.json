[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20722/teaser.png",
                "caption": "Figure 1:rStar2-Agent-14B reaches frontier-level math reasoning in just 510 RL training steps.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Agentic Reinforcement Learning Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20722/x1.png",
                "caption": "Figure 2:rStar2-Agent trains LLMs to natively use Python coding tools within the dedicated execution environment, enabling more advanced and effective reasoning for complex problem-solving.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2508.20722/x2.png",
                "caption": "Figure 3:Our prompt template. Question will be replaced with the specific question during training.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2508.20722/tc_error1.png",
                "caption": "Figure 4:Proportion of tool calls that contain errors within correctly answered trajectories. Under naive GRPO, the error rate initially decreases but soon plateaus at a significant level. In contrast, our GRPO-RoC continues to reduce tool-related errors with more training steps.",
                "position": 352
            }
        ]
    },
    {
        "header": "3Large-Scale Agentic RL Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20722/x3.png",
                "caption": "Figure 5:The overall design of our agentic reinforcement learning infrastructure.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2508.20722/toolcall_time.png",
                "caption": "Figure 6:Our code environment demonstrates scalability by reliably handling up tp 45K concurrent tool calls per step, while maintaining consistently low end-to-end latency from dispatch to response.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2508.20722/x4.png",
                "caption": "Figure 7:Top: Naively static rollout allocation leads to significant GPU idle time and synchronization delays.Bottom: our dynamic load-balanced scheduler that assigns rollouts based on available KV cache, dispatches tool call execution asynchronously, and balances computation across GPUs. For example,K1K_{1},K2K_{2},J1J_{1}denote the number of rollouts computed from the current available KV cache memory on inference engines 0 and 1.",
                "position": 513
            }
        ]
    },
    {
        "header": "4Training Recipe",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20722/multistagerl1.png",
                "caption": "Figure 8:AIME24/AIME25 accuracy and average training response lengths throughout multi-stage RL training.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2508.20722/grpo-roc-compare1.png",
                "caption": "Figure 9:Ablation of the Resample-on-Correct (RoC) rollout strategy. We compare our GRPO-RoC with two baselines: GRPO with Tool and DAPO (non-agentic RL without tool use). (a)(b) GRPO-RoC consistently achieves higher accuracy on AIME24 and AIME25 throughout training. (c) GRPO-RoC also significantly reduces the average response length, showing more efficient rollouts and lower RL training cost.",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2508.20722/x5.png",
                "caption": "Figure 10:Example agentic RL trace#1 with coding tool use and self-reflection. (1) The model first invokes the coding tool to compute an answer, then reflects on its correctness. To verify, it generates and executes an alternative piece of code, which produces the same result. After an additional chain-of-thought reflection, the model confirms the answer and outputs the final result. (2) We highlight the top 20% high-entropy tokens in green. Most of these correspond to forking tokens (e.g.,check,But before), as well as reflection tokens on tool-call responses.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2508.20722/x6.png",
                "caption": "Figure 11:Example agentic RL trace#2 with coding tool use and self-reflection. Top 20% high-entropy tokens are marked in green. The model initially attempts a tool call but encounters a code error. It then reflects on the issue, generates a corrected code snippet, executes it successfully, and verifies again to reach the final correct answer.",
                "position": 1125
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]