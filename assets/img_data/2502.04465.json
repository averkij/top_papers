[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04465/x1.png",
                "caption": "Figure 1:FocalCodec architecture. The encoder extracts features containing both acoustic and semantic information. These features are then mapped to a low-dimensional space by the compressor, binary quantized, and projected back by the decompressor. The decoder resynthesizes the waveform from these features.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3FocalCodec",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BDatasets",
        "images": []
    },
    {
        "header": "Appendix CBaselines",
        "images": []
    },
    {
        "header": "Appendix DHyperparameters and Training Details",
        "images": []
    },
    {
        "header": "Appendix EImplementation and Hardware",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04465/x2.png",
                "caption": "Figure 2:Subjective evaluation from 21 participants averaged over 10 samples.Left.Trade-off between mean opinion score and bitrate. The green dashed line highlights the reference score.Right.Distribution of mean opinion score. The red lines highlight the median. FocalCodec@50 median is marginally lower than BigCodec, Stable Codec and WavTokenizer. However, user preference remains comparable when accounting for variability.",
                "position": 3232
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x3.png",
                "caption": "",
                "position": 3243
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x4.png",
                "caption": "Figure 3:Reconstructed Mel-spectrograms from LibriSpeech (left) and Libri1Mix (right).",
                "position": 3623
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x5.png",
                "caption": "",
                "position": 3634
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x6.png",
                "caption": "",
                "position": 3640
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x7.png",
                "caption": "",
                "position": 3645
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x8.png",
                "caption": "",
                "position": 3651
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x9.png",
                "caption": "",
                "position": 3656
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x10.png",
                "caption": "",
                "position": 3662
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x11.png",
                "caption": "",
                "position": 3667
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x12.png",
                "caption": "",
                "position": 3673
            },
            {
                "img": "https://arxiv.org/html/2502.04465/x13.png",
                "caption": "",
                "position": 3678
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results",
        "images": []
    }
]