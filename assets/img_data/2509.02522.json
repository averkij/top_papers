[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02522/figures/sec1_rlvr_sl.png",
                "caption": "Figure 1:Comparison between RLVR and the supervised learning reformulation, where the query and output are input, and the outcome reward is treated as a predictable label.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02522/figures/sec4_pacs.png",
                "caption": "Figure 2:An illustration of the PACS framework.The framework consists of three main components: (1) Reward Proxy Computation, which calculates a reward proxyr^\\hat{r}based on the log-probability ratio. (2) Group Computation, which computes RLOO-based advantage scoresψ\\psifrom the reward proxies. (3) Cross-Entropy Loss, which converts the RLVR problem into a supervised learning task, optimizing a scoring function parameterized by the policy with a cross-entropy loss.",
                "position": 218
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02522/x1.png",
                "caption": "Figure 3:Performance analysis of PACS with varyingβ\\beta. The 3D heatmaps show pass@k scores for different combinations ofβ\\betavalues (0.1, 0.5, 1, 2, 10) andkkvalues on MATH-500, AMC23, AIME-2024, and AIME-2025 datasets.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2509.02522/x2.png",
                "caption": "Figure 4:Training dynamics of Qwen2.5-7B on the DeepScaleR dataset across different optimization methods (PPO, GRPO, PACS, and PACS w/o weight). The plots illustrate the evolution of three key metrics during training: (a) entropy loss, (b) gradient norm, and (c) mean response length over training steps.",
                "position": 865
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABaselines",
        "images": []
    },
    {
        "header": "Appendix BHyperparameters Settings",
        "images": []
    },
    {
        "header": "Appendix CDetailed Experimental Results",
        "images": []
    }
]