[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21333/x1.png",
                "caption": "Figure 1:An example in MME-VideoOCR. The task requires the MLLM to first recognize the textual information distributed across multiple video frames, and then to perform semantic understanding and reasoning over the extracted text to accurately determine the correct answer. The correct information is marked inblue, while misleading information is marked inred.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MME-VideoOCR",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21333/x2.png",
                "caption": "Figure 2:Example videos and their annotated questions from the MME-VideoOCR benchmark, encompassing25252525tasks across10101010categories. Each task is designed to evaluate models’ capabilities in various aspects such as text recognition, localization, reasoning, and comprehensive video understanding. The figure displays representative video samples and their corresponding questions.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x3.png",
                "caption": "Figure 3:Overview of the MME-VideoOCR construction process. Video filtering ensures sufficient visual dynamics and meaningful textual content. Manual annotation provides high-quality QA pairs, and expert verification further enhances sample reliability and mitigates potential biases.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x4.png",
                "caption": "Figure 4:Overview of MME-VideoOCR Statistics. The videos in MME-VideoOCR covers9999major scenario categories comprising44444444specific scene types, offering fine-grained coverage of diverse video contexts. The benchmark features a balanced distribution of video durations and sources, with a significant portion of the videos newly collected from public resources or manually curated.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x5.png",
                "caption": "",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x6.png",
                "caption": "",
                "position": 395
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21333/x7.png",
                "caption": "(a)Different resolution settings.",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x7.png",
                "caption": "(a)Different resolution settings.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x8.png",
                "caption": "(b)Different frame sampling settings.",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x9.png",
                "caption": "Figure 6:Examples illustrating language prior bias in MLLMs. The models tend to incorrectly recognize the text based on plausible language priors—for instance, “throuh skin” as “through skin”, “togther” as “together”, “OFF COURS” as “OFF COURSE”, and “CAI” as “CAT”. These cases highlight the strong influence of language priors on MLLM responses.",
                "position": 1819
            }
        ]
    },
    {
        "header": "5Conclusions, Discussions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARepresentative Examples from MME-VideoOCR",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21333/x10.png",
                "caption": "Figure 7:An example QA of the Text Recognition at Designated Locations task in MME-VideoOCR.",
                "position": 2691
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x11.png",
                "caption": "Figure 8:An example QA of the Text Recognition Based on Specific Attributes task in MME-VideoOCR.",
                "position": 2694
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x12.png",
                "caption": "Figure 9:An example QA of the Text-Centric QA task in MME-VideoOCR.",
                "position": 2697
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x13.png",
                "caption": "Figure 10:An example QA of the Translation task in MME-VideoOCR.",
                "position": 2700
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x14.png",
                "caption": "Figure 11:An example QA of the Spatial Grounding task in MME-VideoOCR.",
                "position": 2703
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x15.png",
                "caption": "Figure 12:An example QA of the Temporal Grounding task in MME-VideoOCR.",
                "position": 2706
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x16.png",
                "caption": "Figure 13:An example QA of the Change Detection task in MME-VideoOCR.",
                "position": 2709
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x17.png",
                "caption": "Figure 14:An example QA of the Tracking task in MME-VideoOCR.",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x18.png",
                "caption": "Figure 15:An example QA of the Complex Reasoning task in MME-VideoOCR.",
                "position": 2715
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x19.png",
                "caption": "Figure 16:An example QA of the Subtitle-Based Video Understanding task in MME-VideoOCR.",
                "position": 2718
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x20.png",
                "caption": "Figure 17:An example QA of the Multi-Hop Needel in A Haystack task in MME-VideoOCR.",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x21.png",
                "caption": "Figure 18:An example QA of the Table Parsing task in MME-VideoOCR.",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x22.png",
                "caption": "Figure 19:An example QA of the Chart Parsing task in MME-VideoOCR.",
                "position": 2727
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x23.png",
                "caption": "Figure 20:An example QA of the Document Parsing task in MME-VideoOCR.",
                "position": 2730
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x24.png",
                "caption": "Figure 21:An example QA of the Mathematical Formula Parsing task in MME-VideoOCR.",
                "position": 2733
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x25.png",
                "caption": "Figure 22:An example QA of the Handwriting Recognition task in MME-VideoOCR.",
                "position": 2736
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x26.png",
                "caption": "Figure 23:An example QA of the Color Recognition task in MME-VideoOCR.",
                "position": 2739
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x27.png",
                "caption": "Figure 24:An example QA of the Named Entity Recognition task in MME-VideoOCR.",
                "position": 2742
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x28.png",
                "caption": "Figure 25:An example QA of the Counting task in MME-VideoOCR.",
                "position": 2745
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x29.png",
                "caption": "Figure 26:An example QA of the Scrolling Text Understanding task in MME-VideoOCR.",
                "position": 2748
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x30.png",
                "caption": "Figure 27:An example QA of the Trajectory Recognition task in MME-VideoOCR.",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x31.png",
                "caption": "Figure 28:An example QA of the Scrambled Recognition task in MME-VideoOCR.",
                "position": 2754
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x32.png",
                "caption": "Figure 29:An example QA of the AIGC Video task in MME-VideoOCR.",
                "position": 2757
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x33.png",
                "caption": "Figure 30:An example QA of the Adbersarial Video task in MME-VideoOCR.",
                "position": 2760
            },
            {
                "img": "https://arxiv.org/html/2505.21333/x34.png",
                "caption": "Figure 31:An example QA of the Long Video task in MME-VideoOCR.",
                "position": 2763
            }
        ]
    },
    {
        "header": "Appendix BBenchmark Details",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": []
    }
]