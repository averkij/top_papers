[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16145/x1.png",
                "caption": "Figure 1.Comparison of different pipelines for multi-task visual grounding: (a) Visual and language features are extracted separately and then cross-modal fusion is performed; (b) Additional modules (marked by orange) are inserted after the original network layers to inject the language features into the visual backbone; (c) Our progressive language-guided visual learning framework with a collaborative multi-task head, which directly adjusts the original network layer for progressively introducing the language guidance.",
                "position": 126
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16145/x2.png",
                "caption": "Figure 2.Flowchart of the proposed Progressive Language-guided Visual Learning (PLVL) framework which consists of three parts,i.e., linguistic backbone, language-guided visual backbone, and a collaborative multi-task head. The detailed structures of local block, glocal block, and multi-task head are described in Fig.3(a), Fig.3(b), and Fig.4, respectively.",
                "position": 164
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16145/x3.png",
                "caption": "Figure 3.The structures of local block and global block.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2504.16145/x4.png",
                "caption": "Figure 4.The structure of collaborative multi-task Head.",
                "position": 317
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16145/x5.png",
                "caption": "Figure 5.Qualitative results on the RefCOCO. From left to right: the input image, the ground truth of REC and RES, the predicted results of PLVL, the score map of REC sub-task.",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2504.16145/x6.png",
                "caption": "Figure 6.Qualitative results on the RefCOCO+. From left to right: the input image, the ground truth of REC and RES, the predicted results of PLVL, the score map of REC sub-task.",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2504.16145/x7.png",
                "caption": "Figure 7.Qualitative results on the RefCOCOg. From left to right: the input image, the ground truth of REC and RES, the predicted results of PLVL, the score map of REC sub-task.",
                "position": 1063
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]