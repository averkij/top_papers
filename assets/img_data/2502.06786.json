[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06786/x1.png",
                "caption": "(a)",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2502.06786/x1.png",
                "caption": "(a)",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2502.06786/x2.png",
                "caption": "(b)",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2502.06786/x3.png",
                "caption": "(c)",
                "position": 190
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Matryoshka Quantization",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06786/x4.png",
                "caption": "Figure 2:Mix’n’Match on Gemma-2 9B model trained usingMatQuantMatQuant{\\rm MatQuant}roman_MatQuantwith OmniQuant allows elastic pareto-optimal accuracy-vs-cost model extraction for free during deployment.",
                "position": 795
            }
        ]
    },
    {
        "header": "5Ablations and Discussion",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAddition Training Details",
        "images": []
    },
    {
        "header": "Appendix BDetailed Downstream Evaluations for OmniQuant ad QAT",
        "images": []
    },
    {
        "header": "Appendix CDetailed Downstream Evaluations forMatQuantMatQuant{\\rm MatQuant}roman_MatQuantRe-weighting",
        "images": []
    },
    {
        "header": "Appendix DDetailed Downstream Evaluations for Co-Distillation",
        "images": []
    },
    {
        "header": "Appendix EDetailed Evaluations for FFN + Attention Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06786/x5.png",
                "caption": "Figure 3:The Figure presents the weight distribution for Gemma-2 9B when trained withSingle⁢⁢Precison⁢⁢MatQuantSinglePrecisonMatQuant{\\rm Single\\text{ }Precison\\text{ }MatQuant}roman_Single roman_Precison roman_MatQuantfor int2 quantization. The right-shifted quantized weight distribution is a consequence ofSingle⁢⁢Precison⁢⁢MatQuantSinglePrecisonMatQuant{\\rm Single\\text{ }Precison\\text{ }MatQuant}roman_Single roman_Precison roman_MatQuant’s training mechanism that heavily optimizes for the first 2 MSBs of the int8 representation.",
                "position": 1926
            }
        ]
    },
    {
        "header": "Appendix FDetailed Evaluation forSingle⁢⁢Precison⁢⁢MatQuantSinglePrecisonMatQuant{\\rm Single\\text{ }Precison\\text{ }MatQuant}roman_Single roman_Precison roman_MatQuant",
        "images": []
    }
]