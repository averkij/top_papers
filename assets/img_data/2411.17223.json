[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17223/x1.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17223/x2.png",
                "caption": "Figure 2:Overview ofDreamMix. During finetuning, we use the source data{s,s}subscriptsubscript\\{\\boldsymbol{x}_{s},\\boldsymbol{p}_{s}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT }along with regular data{r,r}subscriptsubscript\\{\\boldsymbol{x}_{r},\\boldsymbol{p}_{r}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }constructed via an attribute decoupling mechanism (Sec.3.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec.3.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec.3.4) to generate a decomposed text embedding to enhance the editability of our method during testing.",
                "position": 123
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17223/x3.png",
                "caption": "Figure 3:Pipeline of Attribute Decoupling Mechanism (ADM). We obtain the attribute word list using a VLM agent[24]and create regular data with more diverse text formats and image contents.",
                "position": 265
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17223/x4.png",
                "caption": "Figure 4:Visual comparison between different methods. From left to right are input image and subject, visual results of ourDreamMix, IP-Adapter[40], DreamBooth[33], TIGIC[18], AnyDoor[6], and LAR-Gen[26].",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2411.17223/x5.png",
                "caption": "Figure 5:Effect of different values of位\\lambdaitalic_位in disentangled inpainting framework.位=11\\lambda=1italic_位 = 1means only GCH stage is performed while位=00\\lambda=0italic_位 = 0means only LCG stage is used.位\\lambdaitalic_位is set to 0.7 in our experiments.",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2411.17223/x6.png",
                "caption": "Figure 6:Visual examples for ablation studies on identity preservation (top row) and attribute editing (bottom row).",
                "position": 698
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]