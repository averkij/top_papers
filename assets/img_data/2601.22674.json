[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x1.png",
                "caption": "Figure 1:Comparison of previous methods with VisionTrim.(a) Previous methods focus solely on a specific part of the MLLM framework, typically the vision encoding or LLM decoding stages. (b) In contrast, VisionTrim optimizes the entire MLLM pipeline by introducing two plug-and-play modules, Dominant Vision Token Selection (DVTS) and Text-Guided Vision Complement (TGVC), to effectively reduce visual tokens in both the vision encoding and LLM decoding phases.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x2.png",
                "caption": "Figure 2:Performance of VisionTrim.(a) Comparison across 10 benchmarks using the standard LLaVA-1.5-7B(Liu et al.,2024a), with an88.9%reduction in visual tokens. (b) & (c) Performancevs.efficiency of various methods with a range of visual tokens, in both training-free and fine-tuning scenarios, respectively. The fine-tuning VisionTrim (Ours‡{\\ddagger}) demonstrates superior performance over previous image- and video-based MLLMs.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x3.png",
                "caption": "Figure 3:(a) Overview of VisionTrim featuring the detailed DVTS module, and (b) the structure of the TGVC module. Both DVTS and TGVC modules can be generally utilized in both the vision encoding stage and the LLM decoding stage.",
                "position": 208
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x4.png",
                "caption": "Table 7:Ablation study on TGVC module. This experiment, conducted before inputting data into the LLM, evaluates the effectiveness of the TGVC in reducing noise within text-visual attention during the LLM’s forward pass.",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x4.png",
                "caption": "Figure 4:Comparison of attention maps during LLM forward processing, with and without our proposed VisionTrim.",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x5.png",
                "caption": "Figure 5:Visualization of retained visual patches with and without the dual-attention mechanism in the DVTS module. Black-masked areas indicate discarded visual tokens.",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x5.png",
                "caption": "Figure 5:Visualization of retained visual patches with and without the dual-attention mechanism in the DVTS module. Black-masked areas indicate discarded visual tokens.",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x6.png",
                "caption": "Figure 6:Visualization of retained visual patches with and without TGVC module. We show the correspondence between the salient visual regions and text in different colors.",
                "position": 2152
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Technical Appendices and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AIndividual Case Performance Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x7.png",
                "caption": "Figure 7:Visualized individual-case performance analysis concerning the phenomenon of knowledge boundary drift. Outputs highlighted inredindicate factual errors.",
                "position": 3260
            }
        ]
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x8.png",
                "caption": "Figure 8:Visual examples showcasing VisionTrim’s ability to accurately capture detailed visual information in both images and video. Outputs highlighted inredindicate factual errors.",
                "position": 4479
            }
        ]
    },
    {
        "header": "Appendix DBroader Impacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22674/x9.png",
                "caption": "Figure 9:Visualization of redundancy in the penultimate layer of the vision encoder (CLIP Model).",
                "position": 4619
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x10.png",
                "caption": "Figure 10:Visualization of redundancy in the 16th layer of LLM.",
                "position": 4622
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x11.png",
                "caption": "Figure 11:Visualization of redundancy in the 32nd (final) layer of LLM.",
                "position": 4625
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x12.png",
                "caption": "Figure 12:Visualization of attention distribution change in vision encoder (CLIP Model).",
                "position": 4628
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x13.png",
                "caption": "Figure 13:Visualization of changes in attention distribution across all 32 layers in LLM..",
                "position": 4631
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x14.png",
                "caption": "Figure 14:Visualization of attention maps across all 32 layers in vanilla LLM processing.",
                "position": 4634
            },
            {
                "img": "https://arxiv.org/html/2601.22674/x15.png",
                "caption": "Figure 15:Visualization of attention maps across all 32 layers in LLM processing with our proposed VisionTrim.",
                "position": 4637
            }
        ]
    },
    {
        "header": "Appendix EAsset License and Consent",
        "images": []
    }
]