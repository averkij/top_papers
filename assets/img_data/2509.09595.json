[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09595/x1.png",
                "caption": "Figure 1:Conditioned on audio, image, and user prompts,Kling-Avatargenerates high-fidelity portrait animations through instruction grounding and semantic planning. The results exhibit vivid emotions, rich actions, and precise lip synchronization, while also showing strong generalization to open scenarios such as anime, cartoons, and stylized characters.",
                "position": 71
            },
            {
                "img": "https://arxiv.org/html/2509.09595/x2.png",
                "caption": "Figure 2:Benchmark performance ofKling-Avataragainst its counterparts in terms of GSB metrics. We achieve superior performance on the overall metric as well as across most of sub-dimensions.",
                "position": 74
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09595/x3.png",
                "caption": "Figure 3:Illustration of Kling-Avatar’s cascaded generation pipeline. An MLLM Director first interprets multimodal instructions into high-level semantics and tells a storyline. Guided by this global planning, the first stage generates a blueprint video. In the second stage, keyframes are extracted from the blueprint and used as first–last frame conditions for parallel sub-clip generation, refining local details and dynamics to synthesize long-duration videos.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2509.09595/x4.png",
                "caption": "Figure 4:Overall GSB evaluation results on our benchmark across various dimensions against OmniHuman-1 and HeyGen.",
                "position": 303
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09595/x5.png",
                "caption": "Figure 5:Comparison of lip synchronization between Kling-Avatar and baselines. We produce accurate lip movements for characters across different scenarios.",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2509.09595/x6.png",
                "caption": "Figure 6:Our generated videos with multimodal instruction conditioning. We highlight our results in generating vivid and coherent portrait animations with strong control over emotions, camera movements, lip synchronization and motion dynamics.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2509.09595/x7.png",
                "caption": "Figure 7:Visualization of generated long-duration videos with high consistency, coherence and vividness.",
                "position": 354
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]