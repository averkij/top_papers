[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08711/x1.png",
                "caption": "",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08711/x2.png",
                "caption": "Figure 2:Statistics of human-annotated OmniDCBench.(a)Video duration distribution.(b)Caption length distribution with per-dimension. The benchmark features comprehensive annotations averaging 995 words per video.(c)Scene duration distribution (in seconds), compared against MLLM-generated outputs to highlight the granularity gap between human and model segmentations.",
                "position": 191
            }
        ]
    },
    {
        "header": "3OmniDenseCaptioning Task and A New Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08711/x3.png",
                "caption": "Figure 3:Overview of TimeChat-Captioner Architecture.(Left)This model leverages Qwen2.5-Omni(Xuet al.,2025a)with interleaved audio-visual tokens to generate multi-scene timestamps and six-dimensional captions.(Right)Two-stage training: SFT for task format learning, followed by GRPO with rewards for format, length, timestamp accuracy, and time-aware fine-grained caption quality.",
                "position": 249
            }
        ]
    },
    {
        "header": "4The TimeChat-Captioner Framework",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08711/x4.png",
                "caption": "Figure 4:Qualitative case analysis.We compare TimeChat-Captioner with Gemini-2.5-Pro and Qwen-3-Omni on a sample from OmniDCBench. Our model achievesfine-grained alignmentwith the ground truth across all six annotation dimensions: detailed events, visual background, acoustics, dialogue, camera state, and shot editing style. In contrast, Gemini-2.5-Pro(Gemini Team,2024)exhibitssevere hallucinationby misidentifying the male driver as a woman, fundamentally distorting the scene semantics. Qwen-3-Omni(Xuet al.,2025b)misses the main evententirely, describing irrelevant background elements (a doorman in red uniform) while ignoring the central conversation inside the car. These results demonstrate TimeChat-Captioner’s superior capability in accurate character recognition, faithful event grounding, and comprehensive multi-dimensional annotation.",
                "position": 920
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Future Work.",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08711/x5.png",
                "caption": "Figure 5:Statistics of the training dataset TimeChatCap-42K.(a)Video duration distribution; most videos (73.9%) fall within 50-60 seconds.(b)Caption length distribution with per-dimension average word counts; annotations average 877 words per video across six dimensions.(c)Segment duration distribution; the average segment length is 10.04 seconds.",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2602.08711/x6.png",
                "caption": "Figure 6:Overview of the synthetic training data construction pipeline for the training dataset TimeChatCap-42K.",
                "position": 1611
            }
        ]
    },
    {
        "header": "Appendix CDetails for Training Data and Benchmark Annotation",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Qualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08711/figs/human_anno.png",
                "caption": "Figure 7:Interface page used for manual annotation during the construction of OmniDCBench.",
                "position": 1695
            }
        ]
    },
    {
        "header": "Appendix FPrompt Templates",
        "images": []
    }
]