[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10188/x1.png",
                "caption": "",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2504.10188/x2.png",
                "caption": "",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2504.10188/x3.png",
                "caption": "",
                "position": 342
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10188/x4.png",
                "caption": "Figure 2:Illustration of the Circuits in Hypothesis.From left to right, we first apply a VAE encoder (thePixel-to-Latentor(P2L)stage) to map high-dimensional inputs to a compressed latent space. We then perform latent diffusion on these codes, dividing the backbone into two subregions: theLatent-to-Representation(L2R)region that capture and refine semantic features, and theRepresentation-to-Generation(R2G)region that decode the learned representation into final outputs.",
                "position": 470
            }
        ]
    },
    {
        "header": "3Hypothesis of Latent Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10188/extracted/6360108/resources/figures/merged_image_1.png",
                "caption": "Figure 3:Selected Samples on ImageNet256√ó256256256256\\times 256256 √ó 256.Images generated by the SiT-XL/2 +REPA+ERWmodel using classifier-free guidance (CFG) with a scale ofw=2.2ùë§2.2w=2.2italic_w = 2.2under 40 epochs.",
                "position": 638
            }
        ]
    },
    {
        "header": "4Embedded Representation Warmup: A Two-Phase Training Framework",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10188/x5.png",
                "caption": "Figure 4:Comparison of Training Efficiency and Cost Analysis with Warmup and Full Training Stages.Left:Scatter plot depicting the relationship between the total training cost (in TFLOPs) and the FID score for various training strategies.\nEach point is annotated with a simplified label (e.g., ‚Äú10K+90K‚Äù) representing the warmup and full training split, and the marker sizes are scaled based on a combination of the FID and FLOPs values to highlight the relative differences.Right:Bar chart comparing the computational costs of the warmup and full training stages for different strategies (all evaluated over 100K iterations).\nThe chart shows the warmup cost, full training cost, and their corresponding total cost.",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2504.10188/x6.png",
                "caption": "(a)Alignment withERW",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2504.10188/x6.png",
                "caption": "(a)Alignment withERW",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2504.10188/x7.png",
                "caption": "(b)Training Dynamic for Alignment",
                "position": 1782
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALoss Function Decomposition Proof",
        "images": []
    },
    {
        "header": "Appendix BAnalysis Details",
        "images": []
    },
    {
        "header": "Appendix CHyperparameter and More Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix EBaselines",
        "images": []
    }
]