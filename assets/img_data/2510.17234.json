[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17234/x1.png",
                "caption": "Figure 1:Illustration of CAVS and two challenges. In the figure, three sequential tasks are presented from top to bottom. Gray boxes: learned or background classes, light-orange boxes: target classes to be learned. Multi-modal semantic drift occurs when a learned class (e.g., darkgreen drum) is labeled as background in tasktt, despite the presence of its corresponding sound in the audio. This drift causes the model to suffer catastrophic forgetting of the modality semantic associations specific to the drum. Co-occurrence confusion occurs when, in a previous task (e.g. tasktt), two classes frequently co-occur (guitar and woman). After learning a new task, the model tends to misclassify the old classes (guitar) as the new ones (woman).",
                "position": 105
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17234/x2.png",
                "caption": "Figure 2:Overview of the proposed CMR framework. The CMR framework introduces a novel rehearsal-based method for continual audio-visual segmentation. Our method consists of two key modules: (a) Multi-modal Sample Selection (MSS) strategy for samples rehearsal, which identifies samples with high modality consistency by computing the difference in mean Intersection-over-Union (m​I​o​UmIoU) between uni-modal and multi-modal models. (b) Collision-based Sample Rehearsal (CSR) strategy that dynamically adjusts the rehearsal frequency of samples based on the collision between the old model and current ground truth.",
                "position": 153
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17234/x3.png",
                "caption": "Figure 3:Illustration of inter-modal consistent samples and entanglement of modality.(a)Cases 1 and 4 don’t appear in practice because selection uses already well-trained samples where audio and video predictions match the ground truth. Case 3 represents samples characterized by multi-modal semantic drift and is typically excluded due to substantially large|Δ​(Sa)||\\Delta(S_{a})|. Conversely, Case 2 is kept because of its cross-modal semantic consistency.(b)Classes with infrequent co-occurrence exhibit weak audio-visual entanglement, while frequent co-occurrence leads to strong cross-modal entanglement (e.g., guitar sounds and images of women).",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x4.png",
                "caption": "Figure 4:Illustration of the collision-based sample rehearsal: for each sample, we calculate conflicts between old model predictions (dog) and current ground truth (baby). Aggregating these across all samples yields the collision frequencyℱ\\mathcal{F}, quantifying confusion between old and new classes. By aligning the distribution of replayed samples with the collision frequency, the model is better guided to disentangle incorrect modality semantic associations during training.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x5.png",
                "caption": "Figure 5:m​I​o​UmIoUon the AVSBench-CIS and AVSBench-CIM datasets for different class-incremental audio-visual segmentation scenarios. The red line represents the upper bound. The upper section compares different methods and our method under different incremental settings on AVSBench-CIS, including both disjoint and overlapped scenarios. The lower section provides a similar comparison for AVSBench-CIM, showcasing the performance of our method.",
                "position": 694
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17234/x6.png",
                "caption": "Figure 6:The qualitative results of incremental methods on the 60-10 setting of AVSBench-CI, where different colours represent different classes. The blue waveform represents the audio modality. Here, the far left represents the single old class (airplane), the middle represents the single new class (train), and the far right shows the sounding handpan (learned class) segmentation.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x7.png",
                "caption": "Figure 7:Number of collision pairs. Highly colliding categories typically correspond to objects that co-occur. The categories with the highest collision rate are ”guitar” and ”man,” which aligns with real-world observations.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x8.png",
                "caption": "Figure 8:Example of Multi-modal semantic drift. The image illustrates the phenomenon of multi-modal semantic drift.",
                "position": 1048
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x9.png",
                "caption": "Figure 9:We demonstrate the comparative performance of our method on the AVSBench-CIM dataset, where multiple objects often emit sounds simultaneously, thereby placing higher demands on the model’s ability to perform continuous audio-visual segmentation. Visualization studies on the AVSBench-CIM dataset demonstrate that our method consistently achieves robust and superior performance in complex scenarios containing multiple co-occurring objects.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x10.png",
                "caption": "Figure 10:Comparison of sample selection strategy. The image visualizes our method alongside the sample selection strategy based on maximum modality discrepancy, where samples selected exhibit greater consistency.",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2510.17234/x11.png",
                "caption": "Figure 11:Example of co-occurence. The top part of the figure shows that during training, the violin and bassoon frequently co-occur. As a result, during inference, the model mistakenly segments the bassoon as a violin.",
                "position": 1088
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]