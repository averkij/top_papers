[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05327/x1.png",
                "caption": "Figure 1:Overview of theIConMethod. The diagram illustrates the two key components of the method: Contribution Quantification with theIConScore andICon-guided Selection Paradigm Training. This approach enables gradient-free, bias-reduced data selection with scalable inference.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05327/x2.png",
                "caption": "Figure 2:Pairwise comparison results (win-tie-lose) between models trained onICon-selected subsets and models trained on the full Alpaca dataset across five evaluation benchmarks: Vicuna, Koala, WizardLM, SInstruct, and LIMA.ICon-selected data enhances instruction-following ability with fewer samples, as evidenced by pairwise comparisons.",
                "position": 268
            }
        ]
    },
    {
        "header": "4Experiments Setup",
        "images": []
    },
    {
        "header": "5Experiment Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05327/x3.png",
                "caption": "Figure 3:Model performance (average score) on evaluation benchmarks with varying proportions ofICon-selected Alpaca data. The models are trained based on LLaMA3.1-8B. The dataset is Alpaca dataset. As data scale grows, performance generally improves and then declines, with the best result at 15%.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2505.05327/x4.png",
                "caption": "Table 3:Evaluation benchmark results of models trained on WizardLM dataset and itsICon-selected subsets. The ‘IF’, ‘KN’, and ‘RS’ correspond to average scores on Instruction Following, Knowledge, and Reasoning, respectively. Results on WizardLM dataset confirm the generalization ofIConacross datasets.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2505.05327/x4.png",
                "caption": "Figure 4:Visualization using t-SNE on sample embeddings from the Alpaca dataset. Red points represent samples with the top 15%IConhigh-contribution scores and gray points represent other samples from the dataset. High-contributionIConsamples exhibit diverse characteristics.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2505.05327/x5.png",
                "caption": "Figure 5:Difficulty distribution of top 15% high-contribution samples vs. full Alpaca dataset. High-contribution samples (red bars), selected viaIConfor instruction tuning, exhibit distinct difficulty patterns compared to the full dataset (gray bars). High-contributionIConsamples tend to fall within an appropriate difficulty range.",
                "position": 906
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Evaluation Benchmark for Different Methods",
        "images": []
    },
    {
        "header": "Appendix BDetailed Evaluation Benchmark for Different Data Scales",
        "images": []
    },
    {
        "header": "Appendix CDetailed Evaluation Benchmark for Different Dataset",
        "images": []
    }
]