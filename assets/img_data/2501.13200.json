[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Shared Recurrent Memory Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13200/x1.png",
                "caption": "Figure 1:Shared Recurrent Memory Transformer architecture.SRMT pools recurrent memoriesm‚Å¢e‚Å¢mi,tùëöùëísubscriptùëöùëñùë°mem_{i,t}italic_m italic_e italic_m start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPTof individual agents at a momenttùë°titalic_tand provides global access to them via cross-attention.",
                "position": 185
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13200/x2.png",
                "caption": "(a)Bottleneck",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x2.png",
                "caption": "(a)Bottleneck",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x3.png",
                "caption": "(b)Maze",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x4.png",
                "caption": "(c)Random",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x5.png",
                "caption": "(d)Puzzle",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x6.png",
                "caption": "(e)Warehouse",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x7.png",
                "caption": "(f)MovingAI",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x8.png",
                "caption": "Figure 3:SRMT effectively solves the Bottleneck Task with different reward functions.Trained with Directional (positive when moved towards a goal and achieved it) reward, SRMT clearly outperforms the communication (MAMBA, QPLEX) and memory (ATM, RATE, RRNN) baselines. The RMT, Attention, and RNN ablations also solve the task. For the case with the negative reward for movement and no directional reward (Moving Negative) SRMT and RMT without shared memory demonstrate the clear advantage over the memory-less ablations of SRMT (Attention, Empty, RNN) and the communicative and memory baselines (MAMBA, QPLEX, ATM, RATE, RRNN).\nWith the Sparse (on-goal only) reward, SRMT maintains the score while other methods drop. Error bars indicate 95% confidence intervals. For CSR and ISR higher values are better, for SoC¬†‚Äì¬†the lower the better.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x9.png",
                "caption": "Figure 4:SRMT agents generalize on corridor lengths up to 1000.After training on corridor sizes from 3 to 30 cells all methods were evaluated on longer passages up to 1000. All non-zero performing models show good scaling up to the corridor length of 100. For the Sparse reward, SRMT leads up to 400 and then drops below RMT for collective performance. For the Moving Negative reward, SRMT shows the top-1 performance on all three metrics. The shaded area indicates 95% confidence intervals.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x10.png",
                "caption": "Figure 5:SRMT outperforms other MARL methods in different environments.SRMT trained on Mazes shows robust generalization when evaluated on maps not seen during training. SRMT outperforms MARL baselines MAMBA and QPLEX on all maps except the Warehouse environment. Mixed training with 64 or 128 agents (SRMT 64-128) does not affect the generalization abilities of the method. In the Warehouse environment, the average throughput of SRMT with a reward function based on the Follower heuristic path search (SRMT-FlwrPlan) surpasses that of MAMBA, MATS-LP, QPLEX, and RHCR methods. Error bars indicate 95% confidence intervals.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x11.png",
                "caption": "Figure 6:Comparison of SRMT and other methods across key performance metrics in multi-agent pathfinding.The bar chart compares the performance of SRMT and its variants (SRMT 64-128, SRMT-FlwrPlan) against other methods ‚Äì MAMBA, QPLEX, Follower, MATS-LP, and RHCR ‚Äì across six metrics: Performance, Pathfinding, Congestion, Cooperation, Out-of-Distribution, and Scalability. SRMT and its variants demonstrate competitive performance, particularly in Scalability and Pathfinding. When integrated with Follower planning, SRMT performs best in Congestion management. The centralized planning method RHCR leads in several metrics, notably Cooperation, Out-of-distribution, Performance, and Pathfinding, reaching nearly 100%. MAMBA shows strong performance in Congestion management and Scalability.",
                "position": 357
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13200/x12.png",
                "caption": "Figure 7:Trained withDensereward, all models except empty core policy scale with enlarging corridor length. SRMT consistently outperforms baselines both in success rates and in the time needed to solve the task. The shaded area indicates 95% confidence intervals.",
                "position": 1146
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x13.png",
                "caption": "Figure 8:Directionalreward training leads to all the methods preserving the scores for all tested corridor lengths. The shaded area indicates 95% confidence intervals.",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x14.png",
                "caption": "Figure 9:Results of training withDirectional Negativereward. Vanilla attention fails to scale at corridor lengths of more than 400, compared to the SRMT which preserves the highest scores. That proves the sufficiency of the proposed SRMT architecture. The shaded area indicates 95% confidence intervals.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x15.png",
                "caption": "Figure 10:The evaluation of scalability of SRMT on MovingAI maps from POGEMA benchmark. The shaded area indicates 95% confidence intervals.",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2501.13200/x16.png",
                "caption": "Figure 11:SRMT memory distances are aligned with the distances between agents.The figure shows how cosine distances between agents‚Äô memory vectors are related to the Euclidean distances between agents on the map for SRMT. The triangle marks the step when agents face each other in the environment, the star shows the episode step when the first goal was achieved. The color bar shows the step number.",
                "position": 1168
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]