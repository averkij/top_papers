[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04445/x1.png",
                "caption": "",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04445/x2.png",
                "caption": "Figure 2:Overview of Moto’s three training stages: (1) The Latent Motion Tokenizer encodes key visual motions between video frames into compact latent tokens in an unsupervised manner using pure video data. (2) Moto-GPT is pre-trained with autoregressive motion token prediction to learn motion priors from video-instruction pairs. (3) Moto-GPT is co-fine-tuned on action-labeled trajectories to predict robot actions based on the output of learnable action query tokens while maintaining the next-motion-token prediction objective.",
                "position": 170
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04445/x3.png",
                "caption": "Figure 3:The Latent Motion Tokenizer produces discrete motion tokens from two consecutive video frames. It regularizes the decoder to reconstruct the second frame based on the first one and the discrete tokens, effectively capturing the motion between frames.",
                "position": 219
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04445/x4.png",
                "caption": "Figure 4:Illustration of the evaluation tasks in SIMPLER[31].",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x5.png",
                "caption": "Figure 5:Illustration of the four different environments in CALVIN, adapted from the original figure inMees et al.[40].",
                "position": 309
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04445/x6.png",
                "caption": "Figure 6:Qualitative examples of reconstruction results, where discrete motion tokens obtained from the Latent Motion Tokenizer based on the initial and next frame, are fed into the decoder along with the initial frame to reconstruct the target frame.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x7.png",
                "caption": "Figure 7:Visualization of latent motion token interpretability. Each row displays reconstructed frames from the same initial frame using different latent motion tokens, while each column shows frames reconstructed from the same latent motion tokens with varying initial frames. The latent motion tokens exhibit consistent (see columns) and discriminative (see rows) semantics, despite being trained in an unsupervised manner.",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x8.png",
                "caption": "Figure 8:Video imitation generation via latent motion tokens, where a sequence of latent motion tokens from a demonstration video are extracted by the Latent Motion Tokenizer and are decoded into a new video. This generated video is based on a different initial frame while preserving the original robot movement semantics.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x9.png",
                "caption": "Figure 9:Visualization of video trajectories generated from a sequence of latent motion tokens, which are predicted by the pre-trained Moto-GPT given different language instructions.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x10.png",
                "caption": "Figure 10:Moto-GPT distinguishes successful, failed, and random robot trajectories using log-likelihoods, enabling effective assessment of trajectory rationality and potential reward signals.",
                "position": 926
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x11.png",
                "caption": "Figure 11:Task success rate of models fine-tuned with different proportions of data on CALVIN (ABC⟶⟶\\longrightarrow⟶D).",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2412.04445/x12.png",
                "caption": "Figure 12:Ablations of Moto-GPT on CALVIN (ABC⟶⟶\\longrightarrow⟶D).",
                "position": 948
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]