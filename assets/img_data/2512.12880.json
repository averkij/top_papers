[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12880/x1.png",
                "caption": "Figure 1:Structure of the Mixture of LoRAs (MoL) layer. Each MoL layer combines a shared feed-forward network (FFN) with several LoRA experts, allowing conditional computation without significantly increasing parameters.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12880/architecture_comparison_mol.png",
                "caption": "Figure 2:Comparison of parameter-sharing and adapter architectures.\nALBERT shares a single self-attention and FFN block across all layers without adaptation.\nRRT augments the shared block with depth-specific static LoRA adapters.\nMoA keeps the shared backbone fixed and uses a router to select adapters applied after the FFN output.\nIn contrast, MoL (ours) routes among a pool of LoRA experts whose low-rank updates are injected directly into the shared FFN weights, enabling token-conditional modulation while preserving parameter sharing in the backbone.",
                "position": 139
            }
        ]
    },
    {
        "header": "3ModernALBERT",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": []
    },
    {
        "header": "5Optimising MoL for Inference",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    }
]