[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04431/x1.png",
                "caption": "Figure 1:High-resolution image synthesis results from Infinity, showcasing its capabilities in precise prompt following, spatial reasoning, text rendering, and aesthetics across different styles and aspect ratios.",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x2.png",
                "caption": "Figure 2:Visual tokenizer quantizes continuous features and then gets index labels. Conventional classifier (left) predicts2dsuperscript2ùëë2^{d}2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPTindices. Infinite-Vocabulary Classifier (right) predictsdùëëditalic_dbits instead. Slight perturbations to near-zero values in continuous features cause a complete change of index labels. Bit labels (i.e.quantized features) change subtly and still provide steady supervision. Besides, parameters of conventional classifiers grow exponentially asdùëëditalic_dincreases, while IVC grows linearly. Ifd=32ùëë32d=32italic_d = 32andh=2048‚Ñé2048h=2048italic_h = 2048, the conventional classifier requires8.8 trillionparameters, exceeding current compute limits. By contrast, IVC only requires0.13Mparameters.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04431/x3.png",
                "caption": "Figure 3:Framework of Infinity.Infinity introduces bitwise modeling, which incorporates a bitwise multi-scale visual tokenizer, Infinite-Vocabulary Classifier (IVC), and Bitwise Self-Correction. When predictingùëπksubscriptùëπùëò\\bm{R}_{k}bold_italic_R start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the sequence(ùëπ1,ùëπ2,‚Ä¶,ùëπk‚àí1)subscriptùëπ1subscriptùëπ2‚Ä¶subscriptùëπùëò1(\\bm{R}_{1},{\\bm{R}}_{2},...,\\bm{R}_{k-1})( bold_italic_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_R start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT )serves as the prefixed context and the text condition guides the prediction through a cross attention mechanism. Different from VAR, Infinity performs next-scale prediction with bit labels.",
                "position": 162
            }
        ]
    },
    {
        "header": "3Infinity Architecture",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04431/x4.png",
                "caption": "Figure 4:Qualitative results from Infinity.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2412.04431/extracted/6046736/images/human_preference.png",
                "caption": "Figure 5:Human Preference Evaluation. We ask users to select the better one in a side-by-side comparison in terms of Overall Quality, Prompt Following, and Visual Aesthetics. Infinity is more preferred by humans compared to other open-source models.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x5.png",
                "caption": "Figure 6:Prompt-following qualitative comparison. We highlight text in red that Infinity-2B consistently adheres to while the other four models fail to follow. Zoom in for better comparison.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x6.png",
                "caption": "Figure 7:Text rendering results from our Infinity-2B model. Infinity-2B could generate text-consistent images following user prompts across diverse categories.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x7.png",
                "caption": "Figure 8:Impact of Infinite-Vocabulary Classifier. Predicting bitwise labels with the Infinite-Vocabulary Classifier (Right) generates images with richer details compared to predicting index-wise labels using a conventional classifier (Left).",
                "position": 912
            },
            {
                "img": "https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_vae_bits_three_column.jpg",
                "caption": "Figure 9:Effects of Scaling Up the Vocabulary.We analyze the impact of scaling the vocabulary size under consistent training hyperparameters throughout. Vocabulary sizeVd=216subscriptùëâùëësuperscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPTconverges faster and achieves better results for small models (125M and 361M parameters). As we scale up the model size to 2.2B, Infinity with a vocabulary sizeVd=232subscriptùëâùëësuperscript232V_{d}=2^{32}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 32 end_POSTSUPERSCRIPTbeats that one withVd=216subscriptùëâùëësuperscript216V_{d}=2^{16}italic_V start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT. Experiment with 5M high-quality image-text pair data under256√ó256256256256\\times 256256 √ó 256resolution.",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2412.04431/extracted/6046736/images/scaling_models.jpg",
                "caption": "Figure 10:Effects of Scaling Visual AutoRegressive Modeling.We analyze the impact of scaling model size under consistent training hyperparameters throughout (Experiment with 10M pre-training data and256√ó256256256256\\times 256256 √ó 256resolution). Validation loss smoothly decreases as a function of the model size and training iterations. Besides, Validation loss is a strong predictor of overall model performance. There is a strong correlation between validation loss and holistic image evaluation metrics.",
                "position": 922
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x8.png",
                "caption": "Figure 11:Semantics and visual quality improve consistently with scaling up model size and training compute. Zoom in for better comparison.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x9.png",
                "caption": "Figure 12:Impact of Self-Correction. Teacher-forcing training introduces great train-test discrepancy which degrades performance during inference (left). Bitwise Self-Correction auto-corrects mistakes and thus generates better results (right). Decoding withœÑ=1ùúè1\\tau=1italic_œÑ = 1andc‚Å¢f‚Å¢g=3ùëêùëìùëî3cfg=3italic_c italic_f italic_g = 3.",
                "position": 944
            },
            {
                "img": "https://arxiv.org/html/2412.04431/extracted/6046736/images/pe_ablation.jpg",
                "caption": "Figure 13:Comparison between learnable APE and our positional embeddings. Our method,i.e., applying RoPE2d along with learnable scale embeddings on features of each scale, converges faster and reaches higher training accuracy.",
                "position": 1038
            },
            {
                "img": "https://arxiv.org/html/2412.04431/x10.png",
                "caption": "Figure 14:Comparison of different sampling methods. In contrast to Greedy Sample, Normal Sample and Pyramid Sample, our method could generate images with richer details and higher text-image alignments.",
                "position": 1114
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledges",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APredefined Scale Schedules",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04431/extracted/6046736/images/Categories.png",
                "caption": "Figure 15:Distribution of Prompt Categories",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2412.04431/extracted/6046736/images/Prompts_Challenges.png",
                "caption": "Figure 16:Distribution of Prompts Challenges",
                "position": 2471
            }
        ]
    },
    {
        "header": "Appendix BHuman Preference Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04431/x11.png",
                "caption": "Figure 17:T2I qualitative comparison among our Infinity-2B model and the other four open-source models. Here we select three diffusion models (Flux Schnell, SD3-Medium and PixArt Sigma), one AR model (HART) for comparison. Zoom in for better comparsion.",
                "position": 2493
            }
        ]
    },
    {
        "header": "Appendix CMore Qualitative Results",
        "images": []
    }
]