[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02678/x1.png",
                "caption": "Figure 1:Training Pipeline for Distilled Voice Assistant (DiVA),Redindicates trainable components whileBlueindicates frozen pretrained modules. DiVA modifies a text-only LLM into a general purpose Speech LLM by using the model‚Äôs own responses to transcribed speech as self-supervision.",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Quantitative and Qualitative Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02678/x2.png",
                "caption": "Figure 2:Results across our two Question Answering benchmarks covering both standard evaluation and robustness to regional accents. Model correctness is assessed using the PANDA metric, which is tuned for strong correlation with human judgments of correctness(Li et¬†al.,2024), and significance is from a paired bootstrap test(Dror et¬†al.,2018).",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2410.02678/x3.png",
                "caption": "Figure 3:Results across Emotion, Humor, and Sarcasm classification tasks. We measure class-weighted F1 for multi-class classification and accuracy for binary classification. Significance computed using a paired bootstrap test.",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2410.02678/x4.png",
                "caption": "Figure 4:Results for Speech Translation across 7 typologically diverse languages. We evaluate using SacreBLEU and compute confidence intervals using a Paired Bootstrap.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2410.02678/x5.png",
                "caption": "Figure 5:Example of the double-blind interface for the user study with responses (Left: Qwen 2, Right: DiVA) to the speechCan you tell me about Large Language Models in the style of a haiku?.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2410.02678/x6.png",
                "caption": "",
                "position": 466
            }
        ]
    },
    {
        "header": "6Loss Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02678/x7.png",
                "caption": "Figure 7:Ablation of the loss components from Section3. Distillation leads to a capable audio-only model, but token-alignment improves instruction adherence.",
                "position": 500
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "9Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02678/x8.png",
                "caption": "Figure 8:Empirical Comparison of the KL Divergence with our ProxyL2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTloss in a toy experimental setup. Optimizing the KL Divergence directly leads toworseKL Divergence than optimizing theL2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTloss. This gap increases as the hidden dimension becomes larger.",
                "position": 1485
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]