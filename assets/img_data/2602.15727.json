[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15727/x1.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15727/x2.png",
                "caption": "Figure 2:LoRWeB Overview.We first encodeùêö{\\mathbf{a}}andùêö‚Ä≤{\\mathbf{a}}^{\\prime}, that describe a visual transformation (e.g.adding a hat to the man), andùêõ{\\mathbf{b}}, which should be edited analogously (e.g.adding a hat to the woman) with CLIP[42], and a small learned projection module.\nThe similarity between the encoded vector and a set of learned keys determines the linear coefficients for combining the learned LoRAs into a single, mixed LoRA. This mixed LoRA is injected into a conditional flow model (e.g.Flux.1-Kontext[5]).\nNext, we build a2√ó22\\times 2composite image from{ùêö,ùêö‚Ä≤,ùêõ}\\{{\\mathbf{a}},{\\mathbf{a}}^{\\prime},{\\mathbf{b}}\\}. The conditional flow model gets this composite image as its input, along with a guiding edit prompt, and produces a composite image with the edited resultsùêõ‚Ä≤{\\mathbf{b}}^{\\prime}in the bottom-right quadrant.",
                "position": 189
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15727/x3.png",
                "caption": "Figure 3:LoRWeB visual analogy results.Using a LoRA Basis allows LoRWeB to generalize to a wide variety of new analogy tasks, from adding objects to transferring specific styles or makeup or copying pose changes. Please zoom in for more details.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2602.15727/x4.png",
                "caption": "Figure 4:Comparisons with baseline methods on unseen tasks.Our approach generalizes across more diverse tasks, and better maintains the visual details of both the subject and the analogy.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2602.15727/x5.png",
                "caption": "Figure 5:Quantitative comparisons.(left) Accuracy of the applied edit and preservation ofùêõ{\\mathbf{b}}inùêõ‚Ä≤{\\mathbf{b}}^{\\prime}using Gemma-3[52]. Top right is better. (right) CLIP directional similarity and LPIPS betweenùêõ‚Ä≤{\\mathbf{b}}^{\\prime}andùêõ{\\mathbf{b}}. Bottom-right is better.\nOur method pushes the Pareto front of edit accuracy-preservation, achieving higher edit accuracy while strongly preserving the input image.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2602.15727/x6.png",
                "caption": "Figure 6:Pairwise image comparisons.We compare LoRWeB to four baselines on overall edit quality preference via both a user study and using a VLM. LoRWeB produces edits that are favored by both. Error bars are the68%68\\%Wilson score interval.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2602.15727/x7.png",
                "caption": "Figure 7:Effect of different reference analogy pairs.LoRWeB directly leverages the analogy pair to understand the details of the proposed task, applying an edit that is beyond just text-based editing based on the given prompt. For example, when the prompt is ‚ÄúGive this creature a crown of crystals‚Äù, the analogy context passes information on the amount and color of the crystals.",
                "position": 491
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15727/x8.png",
                "caption": "Figure S1:LoRWeB visual analoy results.The use of a LoRA Basis allows LoRBA to generalize to a wide varity of new analogy tasks, from changing given images to certain styles such as clay toys or bronze sculptures, changing the backgrounds, or changing the cloths of the person. Please zoom in for more details.",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2602.15727/x9.png",
                "caption": "Figure S2:Comparisons with baseline methods on unseen tasks. Our approach generalizes more across diverse tasks, and better maintains the visual details of both the subject and the analogy.",
                "position": 1920
            }
        ]
    },
    {
        "header": "Appendix BAdditional results",
        "images": []
    }
]