[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00907/x1.png",
                "caption": "Figure 1:16-bit, 8-bit floating point numbers defined in IEEE 754 and on various hardware for tensor computations, and the 16-bit integer. MSB stands for most significant bit and LSB stands for least significant bit.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2410.00907/x2.png",
                "caption": "Figure 2:Coparing the process of regular floating-point multiplication and linear-complexity multiplication (‚Ñí‚Ñí\\mathcal{L}caligraphic_L-Mul) between twofp32numbers. In the inline PTX Assembly code,$1and$2arefp32registers storing inputs while$0is thefp32register for output.s1,s2,r0,r1,r2are unsigned int32 registers storing intermediate results. Note that the assembly program is only for numerical simulation on Nvidia GPUs. The optimal implementation is at the hardware level.",
                "position": 211
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00907/x3.png",
                "caption": "Figure 3:Mean square errors obtained by differentl‚Å¢(k)ùëôùëòl(k)italic_l ( italic_k )selections on Llama and Gemma models. The combinations achieving higher precision thanfp8_e4m3are highlighted in read, and those outperformingfp8_e5m2are underlined. Whenk=4ùëò4k=4italic_k = 4andl‚Å¢(k)=3ùëôùëò3l(k)=3italic_l ( italic_k ) = 3, the average error of the llama model is slighly lower but very close tofp8_e4m3.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2410.00907/x4.png",
                "caption": "Figure 4:Comparing the error levels of linear-complexity multiplication (‚Ñí‚Ñí\\mathcal{L}caligraphic_L-Mul) against the number of mantissa bits comparing with 8-bit FP multiplication operation in different formats.",
                "position": 411
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AError Estimation",
        "images": []
    }
]