[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/illustration.png",
                "caption": "Figure 1:Illustration of our findings:Phase shiftfrom formation to optimization in the evolution of knowledge circuits, each phase characterized by distinct features at the performance, topology, and component levels.",
                "position": 208
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Analyzing the Evolution of Knowledge Circuits throughout Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/hit_at_10.png",
                "caption": "Figure 2:Hit@10of the performance of knowledge circuits in GPT-2 Small, GPT-2 Medium and Phi-1.5 throughout training. Left: Performance for circuits discovered by different types of knowledge, whereK_relandK_complrepresentrelevant new knowledgeandcompletely new knowledge, respectively. Right: Performance for circuits discovered by different frequencies of knowledge, whereLow-freq,Medium-freq, andHigh-freqrepresent knowledge with frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively. Note that we smooth the curves using a window size of 3 epochs for all settings.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/similarity_entropy.png",
                "caption": "Figure 3:Top:Edges Jaccard Similarityof intermediate knowledge circuits with the circuits at the final checkpoint. Bottom:Knowledge Cutcuit Entropyof knowledge circuits throughout training.K_relandK_complrepresent relevant new knowledge and completely new knowledge, respectively.Low-freq,Medium-freq, andHigh-freqrepresent knowledge with frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/specific_circuit_performance.png",
                "caption": "Figure 4:Hit@10 of the performance of aligned knowledge circuits in GPT-2 Small throughout training.Init,Before,After,Lastrepresents the circuits whose topologies align with those at the initial checkpoint, the checkpoint before the phase shift, the checkpoint after the phase shift, and the final checkpoint, respectively.Originalrepresents the original knowledge circuits at each checkpoint. Note that we smooth the curves using a window size of 3 epochs.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/specialized_components.png",
                "caption": "Figure 5:Proportion ofspecialized attention headsin all nodes of the knowledge circuits throughout training for GPT-2 Small and GPT-2 Medium. Note that we smooth the curves using a window size of 3 epochs.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_heads_distribution.png",
                "caption": "Figure 6:Top: Layer distribution ofmover headin the knowledge circuits in GPT-2 Small throughout training. Bottom: Layer distribution ofrelation headin the knowledge circuits in GPT-2 Small throughout training.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_activation_ratio.png",
                "caption": "Figure 7:Layer distribution of the edges activation ratiowithin the knowledge circuits in GPT-2 Small.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_rank_and_prob.png",
                "caption": "Figure 8:Top:Rank of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Small. Bottom: The correspondingprobability of the target attribute token.",
                "position": 576
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Construction",
        "images": []
    },
    {
        "header": "Appendix BTraining Configuration",
        "images": []
    },
    {
        "header": "Appendix CCircuit Discovery",
        "images": []
    },
    {
        "header": "Appendix DWhole Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/accuracy.png",
                "caption": "Figure 9:Accuracy curves across continual pre-training.K_relandK_complrepresent relevant new knowledge and completely new knowledge, respectively.First-token Accstands for the model’s next-token prediction accuracy on the first token of each attribute, whileQuery Accstands for the generation accuracy on downstream query tasks for each attribute.",
                "position": 1700
            }
        ]
    },
    {
        "header": "Appendix ETransfer Performance of Knowledge Circuits between Frequency",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/transfer_hit_at_10.png",
                "caption": "Figure 10:Hit@10 of the transfer performance of knowledge circuits in GPT-2 Small and GPT-2 Medium throughout training.Low-freq Circuit,Medium-freq Circuit, andHigh-freq Circuitrepresent knowledge circuits identified by knowledge with the frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively. Note that we smooth the curves using a window size of 3 epochs for all settings.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/nodes_similarity.png",
                "caption": "Figure 11:Nodes Jaccard Similarityof intermediate knowledge circuits with the circuits at the final checkpoint.K_relandK_complrepresent relevant new knowledge and completely new knowledge, respectively.Low-freq,Medium-freq, andHigh-freqrepresent knowledge with frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_medium_activation_ratio.png",
                "caption": "Figure 12:Layer distribution of the edges activation ratiowithin the knowledge circuits in GPT-2 Medium.",
                "position": 1720
            }
        ]
    },
    {
        "header": "Appendix FChanges in Vocabulary Space",
        "images": []
    },
    {
        "header": "Appendix GSpecialized Attention Heads within Knowledge Circuits",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_medium_heads_distribution.png",
                "caption": "Figure 13:Left: Layer distribution ofmover headin the knowledge circuits in GPT-2 Medium throughout training. Right: Layer distribution ofrelation headin the knowledge circuits in GPT-2 Medium throughout training.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_forget.png",
                "caption": "Figure 14:Edges Jaccard Similarity of intermediate knowledge circuits with the circuits at the final checkpoint of the previous knowledge acquisition experiment.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_all_rank_and_prob.png",
                "caption": "Figure 15:Top:Rank of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Small. Bottom:Probability of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Small.Low-freq,Medium-freq, andHigh-freqrepresent knowledge with frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively.",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/gpt2_medium_rank_and_prob.png",
                "caption": "Figure 16:Top:Rank of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Medium. Bottom:Probability of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for GPT-2 Medium.Low-freq,Medium-freq, andHigh-freqrepresent knowledge with frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2502.11196/extracted/6191678/figures/tinyllama_rank_and_prob.png",
                "caption": "Figure 17:Top:Rank of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for TinyLlama. Bottom:Probability of the target attribute tokenwhen unembedding the intermediate layer’s output into vocabulary space at the last token position throughout training for TinyLlama.Low-freq,Medium-freq, andHigh-freqrepresent knowledge with frequencies in the ranges[1,2)12[1,2)[ 1 , 2 ),[2,5]25[2,5][ 2 , 5 ]and(5,27]527(5,27]( 5 , 27 ], respectively.",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix HForgetting Analysis for Knowledge Circuits",
        "images": []
    }
]