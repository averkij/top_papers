[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18157/tabsnfigs/teaser.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18157/x1.png",
                "caption": "Figure 2:We show an overview of our EGAgent pipeline for very long video understanding using cross-modal reasoning in ‚ë†. Given a very long video and a query, a planning agent devises a multi-step plan of sub-tasks required to answer the query. The planning agent uses a retriever tool to probe three data sources extracted from the long video: audio transcripts, visual frame embeddings, and an entity scene graph, which is the focus of EGAgent. We show an example of how the planning agent composes cross-modal information retrieved from the visual database and entity graph to answer an EgoLife query in ‚ë°. We visualize the entity graph query mechanism in ‚ë¢, where the retriever tool designs a SQL query to retrieve relevant relationships for the planning agent to reason over.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2601.18157/x2.png",
                "caption": "Figure 3:We use an LLM, denoted as‚Ñ±\\mathcal{F}, to extract an entity graph from text documentsùíü\\mathcal{D}that represent a very long video,i.e.audio transcriptsùíú‚ÄãùíØ\\mathcal{AT}and scene descriptions and locations extracted from sampled image framesùí±\\mathcal{V}(seeSectionÀú12for details). Each graph relationshiprrconnects a source vertexvsv_{s}and target vertexvtv_{t}between time(tstart,tend)(t_{\\mathrm{start}},t_{\\mathrm{end}}). Each vertex has an entity typeœÑ‚Äã(v)\\tau(v)and the raw text documentd‚àód^{*}used to extract the relationship (sectionÀú3.3).",
                "position": 248
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18157/tabsnfigs/category_wise_barplot.png",
                "caption": "Figure 4:The performance comparison against Gemini 2.5 Pro and EgoButler in each question category in EgoLifeQA. Our approach significantly outperforms baselines on RelationMap (+20.8%) and TaskMaster (+22.2%), where entity understanding and complex reasoning is required to provide a correct answer.",
                "position": 680
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Overview",
        "images": []
    },
    {
        "header": "9Qualitative Example of EGAgent Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18157/x3.png",
                "caption": "Figure 5:A walkthrough of our entire EGAgent pipeline (Sec 3.3, main paper) for an example query from EgoLifeQA, with more details insectionÀú9.\nAt a high-level, given thequery, theplanning agentcomes up with a sequence of 5 sub-tasks,i.e.S1S_{1}throughS5S_{5}. Each sub-task is routed to the appropriate search toolTiT_{i}followed by theanalyzer tool, whose output is appended to theworking memory‚Ñ≥‚Üê‚Ñ≥‚à™Analysis\\mathcal{M}\\leftarrow\\mathcal{M}\\cup\\text{Analysis}. Once all sub-tasks are complete, the original queryQQand working memory‚Ñ≥\\mathcal{M}are sent to the VQA agent to predict the answerAA. TheSQL_Queryand the details about the entity graph search is illustrated infigureÀú6.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2601.18157/x4.png",
                "caption": "Figure 6:Here we focus on theentity graphsearch toolT‚Äão‚Äão‚ÄãlegTool_{\\mathrm{eg}}in the example fromfigureÀú5and discuss its role in the overall EGAgent pipeline insectionÀú9. Given the sub-taskS2S_{2}, theplanning agentuses a strict-to-relaxed hierarchy to choose a SQL queryq2q_{2}to search the entity graph to answer the sub-task,i.e.graph entitiesœÑ‚Äã(vs)=Person\\tau(v_{s})=\\text{Person},r=TALKS_TOr=\\text{TALKS\\_TO}and(day,tstart,tend)(\\text{day},t_{\\text{start}},t_{\\text{end}})to search between. The relevant rows of the SQL table are sent to theanalyzer tool, and the relevant inter-entity relationships(vs,œÑ‚Äã(vs),vt,œÑ‚Äã(vt),r,tstart,tend,d‚àó)(v_{s},\\tau(v_{s}),v_{t},\\tau(v_{t}),r,t_{\\mathrm{start}},t_{\\mathrm{end}},d^{*})are appended to theworking memory‚Ñ≥\\mathcal{M}.",
                "position": 1048
            },
            {
                "img": "https://arxiv.org/html/2601.18157/tabsnfigs/egolife_eg_rels_by_day.jpeg",
                "caption": "Figure 7:Entity Graph relationship types extracted from all seven days of EgoLife.",
                "position": 1051
            }
        ]
    },
    {
        "header": "10Entity Graph",
        "images": []
    },
    {
        "header": "11Ablation Study on EgoLife",
        "images": []
    },
    {
        "header": "12Implementation Details",
        "images": []
    }
]