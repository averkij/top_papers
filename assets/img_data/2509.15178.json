[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15178/x1.png",
                "caption": "Figure 1:(a) The visual attention maps shows that some special tokens (marked as) can precisely attend to the target region of the input query.\nHowever, these special tokens, referred to as thegrounding tokensin our work, underperform on complex STVG, where they often focus on part cues and ignore other cues (marked in red within the input text prompts).\nExamples (b) and (c) illustrate spatial/temporal grounding errors caused by ignoring the discriminative attribute/action cues. Red and green bounding boxes denote the ground truths and predictions, respectively.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2509.15178/x3.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15178/x4.png",
                "caption": "Figure 2:In (a), the results show the frequency with which different special tokens (such as‚Äò_A‚Äô,‚Äò.‚Äô) have superior grounding ability than other tokens,i.e., hit_ratio.\nIn (b), the results represent the grounding accuracy of tokens ranked by their visual activation degrees.\nFor each MLLM, we select four tokens for visualization.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2509.15178/x5.png",
                "caption": "Figure 3:Overview of the proposed approach for zero-shot STVG.\nGiven a video-text pair, we first decompose the textQQinto spatially and temporally related sub-queries,QsQ_{\\mathrm{s}}andQtQ_{\\mathrm{t}}.\nThe text prompt tokens converted fromQsQ_{\\mathrm{s}}andQtQ_{\\mathrm{t}}are then concatenated with visual tokensTv\\mathrm{T}_{v}for spatial and temporal inferences, respectively.\nIn addition, we introduce learnable variables as visual prompts and optimize them by the logit-guided re-attention (LRA) module.For spatial grounding, we also develop a temporal-augmented assembling (TAS) strategy by reversing the frames to enhance temporal consistency.After optimization, we obtain the object track scoreùíÆobj\\mathcal{S}_{\\mathrm{obj}}and frame scoreùíÆframe\\mathcal{S}_{\\mathrm{frame}}based on the grounding token identification.\nThe final prediction is derived by joiningùíÆobj\\mathcal{S}_{\\mathrm{obj}}andùíÆframe\\mathcal{S}_{\\mathrm{frame}}.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2509.15178/x6.png",
                "caption": "Figure 4:Spatial grounding accuracy of different groups of samples on the HC-STVGv1 dataset. These groups are ranked by descending temporal consistency.",
                "position": 375
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15178/x7.png",
                "caption": "Figure 5:Qualitative results on the HC-STVGv1 test set. Better spatio-temporal grounding results (green) are obtained when the DSTH strategy is being used for optimization.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2509.15178/x8.png",
                "caption": "Figure 6:(a) Comparison on the number of framesNf\\mathrm{N}_{f}as input, using vIoU@0.30.3.\n(b) Ablation on the number of selected framesK\\mathrm{K}during temporal prediction.\n(c) Ablation on different trackers.",
                "position": 914
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]