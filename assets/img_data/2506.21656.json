[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21656/x1.png",
                "caption": "",
                "position": 431
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21656/x2.png",
                "caption": "Figure 1:Method Overviewincluding SpatialReasoner-R1 model architecture (left) and training pipeline (right). SpatialReasoner-R1 is a VLM that takes as input a text instruction, visual prompts, and an image, and generates LongCoT reasoning responses.\nTo train SpatialReasoner-R1, we (1) generate reasoning paths using M3CTS, (2) construct fine-grained preference pairs via reward-based selection, and (3) train with fine-grained DPO (fDPO) to optimize descriptive and logical reasoning separately.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2506.21656/x3.png",
                "caption": "Figure 2:Fine-Grained Spatial Rewards.Candidate reasoning paths are decomposed into three aspects,descriptive,spatial, andreasoning, scored separately; the higher value in each row is marked byand the lower by.Explanation of Scoring:Descriptive:Negative response omits the two bar-stools and uses generic ‚Äúmodern kitchen‚Äù wording, whereas the positive response lists every salient object;Spatial:Negative response wrongly claims the island islowerthan the rear counter and ignores the 20cm offset revealed by the stool reference, whereas the positive response provides its estimate to the 75cm stool height plus that offset;Reasoning:Negative response uses an illogical ‚Äúhalf-height‚Äù heuristic90‚Å¢cm‚Üí45‚Å¢cm‚Üí90cm45cm90\\text{cm}\\rightarrow 45\\text{cm}90 cm ‚Üí 45 cmwithout intermediate computation, whereas the positive response explicitly adds reference height and gap (75cm+20cm=95cm).\nThese per-category deficits yield lower composite reward, designating the upper response as negative sample.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2506.21656/extracted/6574397/assets/figures/check.png",
                "caption": "",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2506.21656/extracted/6574397/assets/figures/close.png",
                "caption": "",
                "position": 554
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21656/x4.png",
                "caption": "Table 1:Spatial Reasoning Success Rates (‚Üë‚Üë\\uparrow‚Üë) onSpatialRGPT-Bench. Classification (top) and numeric distance/direction (bottom).are General Large VLMs,are Customized VLMs,areSpatialReasoner-R1variants.\n‚Äú/‚Äù indicates that the model refuses to provide a response for that metric.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2506.21656/x4.png",
                "caption": "Table 2:General Vision-Language Understanding Benchmark Results.Best performance inbold.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2506.21656/x4.png",
                "caption": "Figure 3:Qualitative Examples of Spatial Reasoning Across Models.SpatialReasoner-R1 demonstrates a coherent, multi-step logical chain that closely matches the ground truth, while other models like InternVL2.5-78B, Gemini 1.5 Pro, and SpatialRGPT-8B exhibit less precise or less interpretable reasoning paths.",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2506.21656/x5.png",
                "caption": "Table 3:Effect of Alpha (Œ±ùõº\\alphaitalic_Œ±).",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2506.21656/x5.png",
                "caption": "Table 3:Effect of Alpha (Œ±ùõº\\alphaitalic_Œ±).",
                "position": 974
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BFine-Grained Spatial Reward Details",
        "images": []
    },
    {
        "header": "Appendix CStructured Output Format Specification for M3CTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21656/x5.png",
                "caption": "Figure 4:Example Reasoning Tree from the M3CTS Data Generation Pipeline.Diverse candidate reasoning paths are sampled from multiple models. Each path follows a structured LongCoT format with markdown-style section headers that decompose the answer into interpretable reasoning stages.",
                "position": 2439
            }
        ]
    },
    {
        "header": "Appendix DNode Evaluation Protocol for M3CTS",
        "images": []
    },
    {
        "header": "Appendix ETraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21656/x6.png",
                "caption": "Figure 5:Example DPO Pairs of ourOpen Spatial ReasoningDataset,constructed from M3CTS-generated reasoning trajectories. Each pair consists of a preferred and a rejected response to the same spatial question. The examples highlight differences in descriptive accuracy, spatial alignment, and reasoning coherence, which guide preference optimization during training.",
                "position": 2525
            }
        ]
    },
    {
        "header": "Appendix FEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix GQualitative Experiment Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21656/x7.png",
                "caption": "Figure 6:Qualitative Examples of Spatial Reasoning Across Models.SpatialReasoner-R1demonstrates coherent, step-by-step spatial reasoning that closely aligns with ground truth estimates. In contrast, baseline models produce less precise or partially incorrect reasoning steps, often neglecting key visual cues or misestimating spatial references.",
                "position": 2541
            },
            {
                "img": "https://arxiv.org/html/2506.21656/x8.png",
                "caption": "Figure 7:Qualitative Examples of Spatial Reasoning Across Models.SpatialReasoner-R1correctly recognizes Region2 as a computer tower and compares it clearly with the nearby monitor, reaching an accurate conclusion. InternVL2.5-78B relies on general object size knowledge but provides incorrect reasoning, Gemini1.5Pro fails to identify Region2 clearly and draws incorrect visual conclusions, while SpatialRGPT-8B directly provides a wrong answer.",
                "position": 2547
            }
        ]
    },
    {
        "header": "Appendix HBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix ILimitations",
        "images": []
    }
]