[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15842/x1.png",
                "caption": "Figure 1:Our work,Paper2Web, constitutes an important piece of the puzzle for the presentation and dissemination of academic papers. We build a unified platform to streamline all academic presentation atPaper2All.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x2.png",
                "caption": "(a)Web page of arXiv HTML version.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x2.png",
                "caption": "(a)Web page of arXiv HTML version.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x3.png",
                "caption": "(b)Web page generated by alphaXiv.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x4.png",
                "caption": "Figure 3:Pareto-front comparison of each website generation methods. OurPWAgentachieve the highest quality with moderate and affordable cost.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x5.png",
                "caption": "Figure 4:To transform static papers into exploratory web pages, we collect the first paper-webpage dataset by crawling across multiple top-tier conferences and filtering by online search and human annotators.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x6.png",
                "caption": "Figure 5:Paper2Webdataset statistics.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x7.png",
                "caption": "Figure 6:The right panel shows the categorization of our data. We divided the dataset into 13 categories and counted items in each. In addition, we show distributions by conference and by year. The top-left panel presents, for each category, the relative proportions of papers without and with a website among papers with low citation counts.\nThe bottom-left panel depicts the distribution of papers without and with a website restricted to highly cited papers (those with over 1,000 citations).",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x8.png",
                "caption": "Figure 7:Our evaluation metrics include multiple modules: (1)ConnectivityandCompletenessby parsing HTML links and structure with image–text balance and information-efficiency priors, (2) an MLLM/Human Judge to rate interactivity, aesthetics, and informativeness in a holistic manner, and (3) a QA PaperQuiz on webpage screenshots with a verbosity penalty.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x9.png",
                "caption": "Figure 8:PWAgentturns papers into interactive and multimedia-rich project homepages. Papers are deconstructed via Docling/Marker + LLM into multiple assets and stored in an MCP repository. An agent drafts a page, then iteratively optimizes until layout and UX are solid.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x10.png",
                "caption": "Figure 9:Illustration of website variants for the paper generated by different methods.\nGPT-4o fails to cover all components of a paper and amounts to only a simple paradigm; even with template, the sections remain incomplet. The arXiv-HTML is content-rich but is essentially a direct transfer of the original. The alphaXiv method is complete and concise in content, but it lacks a layout paradigm and visual aesthetic quality. OurPWAgentshow interactive and rich multimedia content to enrich presentation quality.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x11.png",
                "caption": "Figure 10:Illustration of website variants for the paper\n“MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark”444https://mllm-judge.github.io/generated by different methods.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x12.png",
                "caption": "Figure 11:Human annotation instruction",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x13.png",
                "caption": "Figure 12:Illustration of website variants for the paper\n“SMIRK: 3D Facial Expressions through Analysis-by-Neural-Synthesis”666https://georgeretsi.github.io/smirk/generated by different methods.",
                "position": 3143
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x14.png",
                "caption": "Figure 13:Illustration of website variants for the paper\n“Interactive3D: Create What You Want by Interactive 3D Generation”888https://interactive-3d.github.io/generated by different methods.",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x15.png",
                "caption": "Figure 14:Illustration of website variants for the paper\n“Masked Audio Generation using a Single Non-Autoregressive Transformer”101010https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/generated by different methods.",
                "position": 3151
            },
            {
                "img": "https://arxiv.org/html/2510.15842/x16.png",
                "caption": "Figure 15:Illustration of website variants for the paper\n“MVDream: Multi-view Diffusion for 3D Generation”121212https://mv-dream.github.io/generated by different methods.",
                "position": 3155
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]