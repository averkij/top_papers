[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/ffr.png",
                "caption": "Figure 1:Fake Feature Ratio for SAEs trained on Faithful dataset and Web-based datasets (lower is better). Detailed values can be found in Table7.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/introduction.png",
                "caption": "Figure 2:Shared Feature Ratio (SFR) comparison between Faithful Dataset and Instruction Dataset trained SAEs. Detailed values for each run are listed in Table2.",
                "position": 417
            }
        ]
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/webdata.png",
                "caption": "Figure 3:Shared Feature Ratio by model and dataset.\nSAE training hyperparameters are listed in AppendixA,\nand complete results appear in Table4.",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/cediff.png",
                "caption": "Figure 4:Cross-Entropy difference between SAEs trained on different datasets. Colors represent training datasets: orange for FineWeb, gray for Pile-Uncopyrighted, and green for Faithful dataset.\nPoint shapes indicate evaluation datasets: circles for FineWeb, squares for The Pile, X markers for TinyStories, crosses for OpenWebText, and diamonds for Faithful dataset.\nYou can find the detailed metrics in AppendixB.",
                "position": 602
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/llama8.png",
                "caption": "Figure 5:Faithful SAE representation for LLaMa 8B.\nThis figure shows the SAE’s reconstruction of the LLaMa 8B hidden state and its faithfulness across datasets.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/sae-probing.png",
                "caption": "Figure 6:SAE Probing performance comparison between FaithfulSAE and Web-based SAEs with different types of LLM architectures. Detailed values can be found in Table6.",
                "position": 791
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASAE Training",
        "images": []
    },
    {
        "header": "Appendix BFaithful SAEs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/gpt2.png",
                "caption": "Figure 7:Faithful SAE representation for GPT-2. This figure visualizes the SAE model’s ability to reconstruct GPT-2’s hidden state.",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/llama1.png",
                "caption": "Figure 8:Faithful SAE representation for LLaMA 1B. This figure demonstrates the SAE’s performance in reconstructing the hidden state of LLaMA 1B.",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/llama3.png",
                "caption": "Figure 9:Faithful SAE representation for LLaMA 3B. This figure highlights the SAE’s reconstruction quality for the LLaMA 3B model’s hidden state.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/gemma.png",
                "caption": "Figure 10:Faithful SAE representation for Gemma 2B. This figure shows the SAE’s reconstruction of the Gemma 2B hidden state and its faithfulness across datasets.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/gpt2dataset.png",
                "caption": "Figure 11:This figure compares the token distribution of the generated dataset for GPT-2 with the model’s expected token distribution.",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/llama1dataset.png",
                "caption": "Figure 12:This figure compares the token distribution of the generated dataset for LLaMA 1B with the model’s original token distribution.",
                "position": 1850
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/llama3dataset.png",
                "caption": "Figure 13:This comparison shows the token distribution of LLaMA 3B’s generated dataset versus the model’s distribution.",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/llama8dataset.png",
                "caption": "Figure 14:This figure visualizes how well the generated dataset represents LLaMA 8B’s token distribution.",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/gemma2dataset.png",
                "caption": "Figure 15:This visualization compares the generated token distribution with the original model for Gemma 2B.",
                "position": 1859
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/pythia1.4dataset.png",
                "caption": "Figure 16:This figure shows the token distribution for the generated Pythia 1.4B dataset, comparing it to the model’s distribution.",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/pythia2.8dataset.png",
                "caption": "Figure 17:This figure shows the token distribution for the generated Pythia 2.8B dataset, comparing it to the model’s distribution.",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2506.17673/extracted/6559860/Images/pythia6.9dataset.png",
                "caption": "Figure 18:This figure shows the token distribution for the generated Pythia 6.9B dataset, comparing it to the model’s distribution.",
                "position": 1868
            }
        ]
    },
    {
        "header": "Appendix CFaithful Dataset",
        "images": []
    }
]