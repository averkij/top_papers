[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04185/x1.png",
                "caption": "Figure 1:LiveCodeBench-V6 acceleration with Set Block Decoding with no performance reductions.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x2.png",
                "caption": "Figure 2:(a)Set Block Decoderfine-tunes any native NTP transformer architecture to predictkkfuture tokens (in this illustrationk=4k=4) conditioned on an arbitrarysubsetof the future tokens (in this case, “text” and “code”); where the special mask token ‘m’ is used to hide future tokens to be predicted. Past tokens (in white) use causal attention while the future block (in blue) uses bidirectional attention, allowing future tokens to attend to each other. (b) During inference, SBD decodes one block at a time by revealing some subset of independent future tokens, where each row represents a single forward in the model. Once a block is decoded it is KV-cached (in pink, with causal attention).",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x3.png",
                "caption": "",
                "position": 179
            }
        ]
    },
    {
        "header": "2Approach",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04185/x4.png",
                "caption": "Figure 4:NTP loss during 3B model pretraining.",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x5.png",
                "caption": "Figure 5:3B model SFT training with varying number of training iterations.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x6.png",
                "caption": "",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x7.png",
                "caption": "",
                "position": 1019
            }
        ]
    },
    {
        "header": "4Timing analysis",
        "images": []
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion and future work",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04185/x8.png",
                "caption": "Figure 12:3B model SFT training with varying number of training steps. Factor vs EB-Sampler with entropy error proxy.",
                "position": 3908
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x9.png",
                "caption": "",
                "position": 3917
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x10.png",
                "caption": "",
                "position": 3918
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x11.png",
                "caption": "Figure 13:3B model SFT training with varying number of training steps. Factor vs EB-Sampler with confidence error proxy.",
                "position": 3937
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x12.png",
                "caption": "",
                "position": 3946
            },
            {
                "img": "https://arxiv.org/html/2509.04185/x13.png",
                "caption": "",
                "position": 3947
            }
        ]
    },
    {
        "header": "9Additional experiments",
        "images": []
    }
]