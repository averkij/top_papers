[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14055/x1.png",
                "caption": "Figure 1:Given a character image and a reference video,Wan-Animatesupports two core functionalities. In the first, which we term ”Animation,” it reenacts the motion and expression of the character in the reference video to animate the static source image. In the second, termed ”Replacement,” it substitutes the character in the reference video with the source identity, ensuring seamless integration with the environment.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Model Design and Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14055/x2.png",
                "caption": "Figure 2:Overview ofWan-Animate, which is built upon Wan-I2V. We modify its input formulation to unify reference image input, temporal frame guidance, and environmental information (for dual-mode compatibility) under a common symbolic representation. For body motion control, we use skeleton signals that are merged via spatial alignment. For facial expression control, we leverage implicit features extracted from face images as the driving signal. Additionally, for character replacement, we train an auxiliary Relighting LoRA to enhance the character’s integration with the new environment.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2509.14055/content/pic/aa3_face.png",
                "caption": "Figure 3:Face images are encoded into frame-wise implicit latents, which are then temporally aligned with the DiT latents. These features are injected via a cross-attention mechanism that operates within each corresponding temporal segment.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2509.14055/x3.png",
                "caption": "Figure 4:Examples of data augmentation using IC-Light.",
                "position": 185
            }
        ]
    },
    {
        "header": "4Implementation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14055/content/pic/human_pre.png",
                "caption": "Figure 5:Human evaluation with current SOTA.",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2509.14055/x4.png",
                "caption": "Figure 6:Qualitative comparison for Animation Mode.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2509.14055/x5.png",
                "caption": "Figure 7:Qualitative comparison for Replacement Mode.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2509.14055/x6.png",
                "caption": "Figure 8:Ablation study on Face Adapter Training.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2509.14055/x7.png",
                "caption": "Figure 9:Ablation study of Relighting LoRA.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2509.14055/x8.png",
                "caption": "Figure 10:Qualitative Results for various applications.",
                "position": 428
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]