[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11409/x1.png",
                "caption": "Figure 1:Comparison of reasoning paradigms. The traditional approaches (topandmiddlerows) generate verbose and inaccurate textual plan, while the Visual Planning paradigm (bottomrow) predicts the next visual state directly, forming a pure image trajectory without language mediation.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Visual Planning via Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11409/x2.png",
                "caption": "Figure 2:An overview of the proposed VPRL framework, illustrated with autoregressive large vision models for image generation in the context of a visual navigation task. We train the visual policy model with GRPO, using theprogressreward that encourages progressing actions and penalizes invalid actions, yielding goal-aligned visual planning.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11409/x3.png",
                "caption": "Figure 3:Illustration of each task with generated visual planning traces from LVM, covering different types of actions (optimal, non-optimal and invalid). More cases can be found in AppendixC.5.",
                "position": 597
            }
        ]
    },
    {
        "header": "4Discussions and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11409/x4.png",
                "caption": "Figure 4:Visualization of a test example fromFrozenLakecomparing visual planning variants (VPFT and VPRL) with language-based reasoning variants.",
                "position": 635
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11409/x5.png",
                "caption": "Figure 5:Evaluation of model performance onFrozenLakeunder varying levels of difficulty. As the environment complexity increases with larger grid sizes, language-based reasoning methods experience a sharp decline in performance, whereas visual planning methods exhibit a more gradual drop, demonstrating greater robustness.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x5.png",
                "caption": "Figure 5:Evaluation of model performance onFrozenLakeunder varying levels of difficulty. As the environment complexity increases with larger grid sizes, language-based reasoning methods experience a sharp decline in performance, whereas visual planning methods exhibit a more gradual drop, demonstrating greater robustness.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x6.png",
                "caption": "Figure 6:Comparison of exploration capabilities between VPFT and VPRL Stage 1 onFrozenLake. VPRL Stage 1 achieves significantly better exploration efficiency, balancing high entropy with a low invalid action ratio, whereas VPFT struggles with diminishing entropy and increased invalid actions over training.",
                "position": 668
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and future work",
        "images": []
    },
    {
        "header": "Appendix BImplementation details",
        "images": []
    },
    {
        "header": "Appendix CResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11409/x7.png",
                "caption": "Figure 7:Reward curves with standard deviation forVPRLonFrozenLake,MazeandMiniBehavior.",
                "position": 2042
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x8.png",
                "caption": "Figure 8:Performance across different grid sizes, reflecting task difficulty.Left:Maze.Right:MiniBehavior. Visual planners consistently maintain higher accuracy and exhibit flatter performance curves, indicating robustness to increasing complexity.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x9.png",
                "caption": "",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x10.png",
                "caption": "Figure 9:Qualitative comparison of visual planning outputs from VPFT (top) andVPRL(bottom) on out-of-distribution (OOD) scenarios with unseen larger grid size acrossMaze,FrozenLake, andMiniBehavior. Each example shows a failure case from VPFT contrasted with a successful trajectory generated byVPRLunder the same environment configuration.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x11.png",
                "caption": "Figure 10:Generated visual planning trajectories fromVPRLon theFrozenLaketest set. We illustrate three representative categories: optimal, non-optimal, and invalid cases. In non-optimal examples, the model occasionally enters local loops but still has the chance to make progress toward the goal, see the first and third trajectories. In invalid cases, despite a significant reduction in failure rate, VPRL still exhibits errors such as disappearing agents, contradictory actions (e.g., simultaneous left and right), or unrealistic teleportation.",
                "position": 2237
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x12.png",
                "caption": "Figure 11:Generated visual planning trajectories fromVPRLon theMazetest set. We illustrate three representative categories: optimal, non-optimal, and invalid cases. In non-optimal examples, similar toFrozenLake, the model occasionally enters redundant loops but still progresses toward the goal. Invalid cases include maze-specific errors, such as the agent erroneously traversing through walls, violating the structural constraints of the environment. Notably, we observe that in the last invalid case, the agent is able to plan an optimal trajectory in subsequent steps.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2505.11409/x13.png",
                "caption": "Figure 12:Generated visual planning trajectories fromVPRLon theMiniBehaviortest set.",
                "position": 2244
            }
        ]
    },
    {
        "header": "Appendix DPrompting Templates",
        "images": []
    }
]