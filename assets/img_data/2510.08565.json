[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08565/x1.png",
                "caption": "Figure 1:Comparison of design choices, scaling properties, and performance of our native MLLMs.\nWe systematically investigate the designs and the scaling properties of native MLLMs under data constraints and yield valuable findings for building native MLLMs. After adopting these findings, our native MLLMs achieve competitive performance with top-tier MLLMs.ùí±d,w‚àó‚Äã(‚ãÖ)\\mathcal{V}^{*}_{d,w}(\\cdot)denotes the visual encoder with optimal parameter size.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Visual Design Principles for native-MLLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08565/x2.png",
                "caption": "Figure 2:Effectiveness of LLM initialization.Left: The validation loss. The LLM initialized one converges much faster.Right: The zero-shot caption performance. Due to the lack of textual knowledge, the uninitialized model continues to lag behind.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2510.08565/x3.png",
                "caption": "Figure 3:The validation loss of adding MoE or not. Using MoE extension will cause the loss to decrease more quickly.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2510.08565/x4.png",
                "caption": "Figure 4:The validation loss and zero-shot caption performance of different visual encoders. The loss and performance only differ when the visual encoder is extremely wide or shallow.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2510.08565/x5.png",
                "caption": "Figure 5:The validation loss when scaling up LLMs. With the same visual encoder (i.e. 600M), the validation loss decreases log-linearly with the LLM size.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2510.08565/x6.png",
                "caption": "Figure 6:The validation loss curves of different LLMs with different training data sizes. As the training data size increases, the loss gap narrows to near zero when the visual encoder size reaches a certain threshold.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2510.08565/x7.png",
                "caption": "Figure 7:Relationship of visual encoder size and LLM size. The optimal visual encoder size increases log-linearly with the LLM size.",
                "position": 418
            }
        ]
    },
    {
        "header": "4NaViL: A Novel Native MLLM with Strong Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08565/x8.png",
                "caption": "Figure 8:Architecture of NaViL. As a native MoE-extended MLLM, NaViL can be trained end-to-end and supports input images of any resolution.",
                "position": 445
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08565/x9.png",
                "caption": "Figure 9:Visualization of attention maps in LLM-1.8B with different encoder sizes (i.e. 150M and 1.2B). Text and image tokens are inblueandgreen, respectively. Larger encoder allows LLMs to attend to global patterns at shallow layers while maintaining higher attention to textual tokens.",
                "position": 1224
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Technical Appendices and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix ANaViL-9B: Scaling up to 9B parameters",
        "images": []
    },
    {
        "header": "Appendix BMore discussions on Compositional MLLMs and Native MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08565/x10.png",
                "caption": "Figure 10:Paradigm Comparison between Compositional MLLMs and Native MLLMs.\nCompositional MLLMs adopt different training objectives and strategies (e.g. Contrastive Loss or Next-Token-Prediction) to pre-train the visual encoder and LLM separately, while native MLLMs optimize both image and text components in an end-to-end manner using a unified training objective (i.e. Next-Token-Prediction).",
                "position": 2257
            }
        ]
    },
    {
        "header": "Appendix CMore Related Works",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EThe NLP capability",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-conventional-center.jpg",
                "caption": "",
                "position": 3141
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-worldcup.png",
                "caption": "",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-gta5.png",
                "caption": "",
                "position": 3184
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-nutritional.jpeg",
                "caption": "",
                "position": 3194
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-receipt.jpeg",
                "caption": "",
                "position": 3226
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-road.jpeg",
                "caption": "",
                "position": 3244
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-zuowen.jpeg",
                "caption": "",
                "position": 3259
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-docvqa.png",
                "caption": "",
                "position": 3291
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-math.png",
                "caption": "",
                "position": 3314
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-math-7.png",
                "caption": "",
                "position": 3347
            },
            {
                "img": "https://arxiv.org/html/2510.08565/figures/vis/vis-understanding-math-10.png",
                "caption": "",
                "position": 3390
            }
        ]
    },
    {
        "header": "Appendix FMore Qualitative Results",
        "images": []
    }
]