[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/figs/logo.jpg",
                "caption": "",
                "position": 69
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x1.png",
                "caption": "Figure 1:Current LMMs struggle with instance-level understanding, failing to capture the nuanced details about specific instances.\nTo address this, we create a large-scale instance-specific instruction tuning dataset and train a multimodal model on it. Compared to existing models, our model show better performance in instance-level understanding.",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03565/x2.png",
                "caption": "Figure 2:The automated data generation pipeline.We process the video frames sequentially. At each timestamptùë°titalic_t, GPT-4o is prompted to create a frame-level annotationYtfsuperscriptsubscriptùëåùë°ùëìY_{t}^{f}italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPTbased on the current frameXtsubscriptùëãùë°X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTand the previous frameXt‚Å¢-‚Å¢1subscriptùëãùë°-1X_{t\\text{-}1}italic_X start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Then, all the frame-level annotations are aggregated to produce a video-level descriptionYv‚Å¢i‚Å¢dsuperscriptùëåùë£ùëñùëëY^{vid}italic_Y start_POSTSUPERSCRIPT italic_v italic_i italic_d end_POSTSUPERSCRIPTand a set of open-ended question-answer pairsYq‚Å¢asuperscriptùëåùëûùëéY^{qa}italic_Y start_POSTSUPERSCRIPT italic_q italic_a end_POSTSUPERSCRIPT.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x3.png",
                "caption": "Figure 3:Visualization of an data example from ourInst-ITDataset.For each video, we provide (a) frame-level annotations, (b) a video-level description, and (c) open-ended question-answer pairs. Each frame-level annotation consists of three parts: captions for individual instances, a caption for the entire scene, and captions describing the temporal changes involving specific instances. A key feature of our dataset is its emphasis on instances of interest, including their state in each frame, how they change between frames, and questions and answers focused on their specific details throughout the video. The contours of instances in this example are deliberately highlighted for better visualization. A complete data example can be found inSec.C.2.",
                "position": 174
            }
        ]
    },
    {
        "header": "3Inst-IT",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Annotation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03565/x4.png",
                "caption": "Figure 4:Set-of-Marks visual prompting on the original videos.Each instance is assigned a unique numeric ID, which remains consistent across all frames.",
                "position": 3100
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x5.png",
                "caption": "Figure 5:GPT-4o-based open-ended question answering correctness assessment.Theunderlinedparts in the figure are included only when evaluating the video split, while theitalicizedparts will be replaced by the actual sample for scoring.",
                "position": 3103
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x6.png",
                "caption": "Figure 6:Frame-level annotation task prompt, theitalicizedpart are placeholders for the actual inputs.",
                "position": 3107
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x7.png",
                "caption": "Figure 7:Video-level annotation task prompt, theitalicizedpart are placeholders for the actual inputs.",
                "position": 3110
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x8.png",
                "caption": "Figure 8:Open-ended question-answer pairs generation task prompt, theitalicizedpart are placeholders for the actual inputs.",
                "position": 3113
            },
            {
                "img": "https://arxiv.org/html/2412.03565/x9.png",
                "caption": "Figure 9:A data example fromInst-ITBench.Each test sample includes both open-ended QA and multiple-choice QA, focusing on specific instances or the relationships and interactions between instances.",
                "position": 3116
            }
        ]
    },
    {
        "header": "Appendix BMore Details aboutInst-ITBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/1.png",
                "caption": "Table 8:Inst-ITDataset Frame-level Annotation, Part I (frame 1-5). Please zoom in to view the instance ID labels.",
                "position": 3254
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/2.png",
                "caption": "",
                "position": 3302
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/3.png",
                "caption": "",
                "position": 3324
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/4.png",
                "caption": "",
                "position": 3346
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/5.png",
                "caption": "",
                "position": 3368
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/6.png",
                "caption": "Table 9:Inst-ITDataset Frame-level Annotation, Part II (frame 6-8). Please zoom in to view the instance ID labels.",
                "position": 3392
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/7.png",
                "caption": "",
                "position": 3443
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/8.png",
                "caption": "",
                "position": 3465
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/9.png",
                "caption": "Table 10:Inst-ITDataset Frame-level Annotation, Part III (frame 9-12). Please zoom in to view the instance ID labels.",
                "position": 3489
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/10.png",
                "caption": "",
                "position": 3540
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/11.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2412.03565/extracted/6045956/sec/supp_figs/example/demo/12.png",
                "caption": "",
                "position": 3584
            }
        ]
    },
    {
        "header": "Appendix CMore Details aboutInst-ITDataset",
        "images": []
    }
]