[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04678/assets/multi_agent_framework.png",
                "caption": "Figure 1:Multi-agent framework. At each step, the planner-agent creates and assigns new subtasks to worker-agents; the planner-agent generates successive subtasks or final answers based on the worker-agents’ responses.",
                "position": 132
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Problem Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04678/assets/single_agent.png",
                "caption": "Figure 2:Visualization of a single-agent multi-turn TIP rollout. The LLM solves a query through iterative planning and tool-use. At each step, it plans a tool call, executes it with the parsed parameters, and uses the tool response to decide the next move, continuing until it is confident enough to produce a final answer.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2510.04678/assets/multi_agent_RL_rollout.png",
                "caption": "Figure 3:Visualization of a multi-agent multi-turn TIP rollout. At each step, the planner agent generates and assigns a subtask to the worker agent, which completes it via multi-turn TIP and returns the result. The planner agent then decides whether to generate a new subtask or produce the final answer based on this response.",
                "position": 250
            }
        ]
    },
    {
        "header": "Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04678/assets/multi_agent_RL-Implementation_Up_Down.png",
                "caption": "Figure 4:An illustration of the implementation of MATPO.",
                "position": 467
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04678/x1.png",
                "caption": "(a)Test accuracy on theGAIA-textdataset(Mialon et al.2023).",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2510.04678/x1.png",
                "caption": "(a)Test accuracy on theGAIA-textdataset(Mialon et al.2023).",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2510.04678/x2.png",
                "caption": "(b)Test accuracy on theWebWalkerQAdataset(Wu et al.2025).",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2510.04678/x3.png",
                "caption": "(c)Test accuracy on theFRAMESdataset(Krishna et al.2025).",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2510.04678/x4.png",
                "caption": "(a)The test accuracy on theGAIA-textdataset(Mialon et al.2023)(running average@5).",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2510.04678/x4.png",
                "caption": "(a)The test accuracy on theGAIA-textdataset(Mialon et al.2023)(running average@5).",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2510.04678/x5.png",
                "caption": "(b)The training accuracy on theMuSiQuedataset(Trivedi et al.2022)(running average@15).",
                "position": 568
            }
        ]
    },
    {
        "header": "Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]