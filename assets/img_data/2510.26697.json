[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26697/figure/huggingface_logo.png",
                "caption": "",
                "position": 157
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26697/figure/arch.png",
                "caption": "Figure 1:An overview of our proposed end-to-end decoding architecture compared to manual decoding. Our method dynamically predicts temperature and top-p values from the model’s hidden states for each generation step. In contrast, manual decoding (bottom) relies on a single set of static, predefined hyperparameters for the entire sequence generation.",
                "position": 178
            }
        ]
    },
    {
        "header": "2AudoDeco",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26697/figure/top-p_function.png",
                "caption": "(a)Top-p mask function.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2510.26697/figure/top-p_function.png",
                "caption": "(a)Top-p mask function.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2510.26697/figure/top-p_example.png",
                "caption": "(b)An example of top-p sampling probability.",
                "position": 215
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26697/figure/llama_fine_grained.png",
                "caption": "(a)Llama-Nemotron-8B withAutoDeco.",
                "position": 800
            },
            {
                "img": "https://arxiv.org/html/2510.26697/figure/llama_fine_grained.png",
                "caption": "(a)Llama-Nemotron-8B withAutoDeco.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2510.26697/figure/r1_fine_grained.png",
                "caption": "(b)R1-Distill-Qwen-7B withAutoDeco.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2510.26697/figure/box-plot-AIME24-both.png",
                "caption": "Figure 4:Ablation study onAutoDecoarchitecture designs. Joint optimization achieves the highest AIME Score.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2510.26697/x1.png",
                "caption": "Figure 5:An Emergent Phenomenon.This figure shows the token-levelT^/P^\\hat{T}/\\hat{P}predictions for the same prompt under three conditions, observedwithoutany targeted training.(Left) Baseline:The model’s default dynamicT^/P^\\hat{T}/\\hat{P}values.(Middle) High-Diversity Command:The model spontaneously elevates itsT^/P^\\hat{T}/\\hat{P}predictions.(Right) Low-Diversity Command:The model spontaneously suppresses itsT^/P^\\hat{T}/\\hat{P}predictions.",
                "position": 976
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Experimental Setup",
        "images": []
    },
    {
        "header": "7Supplementary Discussion of Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26697/figure/training_loss_400_steps.png",
                "caption": "Figure 6:AutoDeco’s training curves on all models. Training loss curve across models. The loss converges effectively, indicating resource-friendly training ofAutoDeco.",
                "position": 1633
            }
        ]
    },
    {
        "header": "8Supplementary Experimental Results",
        "images": []
    }
]