[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22795/x1.png",
                "caption": "Figure 1:Given an input audio clip and an edit instruction, SAO-Instruct outputs the edited audio while keeping the overall audio context intact.",
                "position": 181
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22795/x2.png",
                "caption": "Figure 2:Overview of our proposed method. Green indicates synthetic data. Audio datasets are used as the starting point for prompt generation. DDPM inversion and Prompt-to-Prompt use the input caption and generated output caption to create a partial and fully synthetic dataset, respectively. For manual edits, a predefined edit operation is sampled. In the fine-tuning stage, Stable Audio Open is trained on the combined generated samples and edit instructions. During inference, SAO-Instruct receives an audio clip and a free-form edit instruction and produces the edited output.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2510.22795/x3.png",
                "caption": "Figure 3:Pipeline for Prompt-to-Prompt audio generation. In (a), various seeds and CFG value combinations are tested and filtered using Gemini and CLAP to identify suitable configurations for prompts. In (b) Stable Audio Open (SAO) with Prompt-to-Prompt generates audio pairs using the seed and CFG configuration found in (a). A Bayesian Optimization process suggests Prompt-to-Prompt parameters and resulting samples are evaluated using an objective function. For clarity, only 3 candidate pairs and 2 Bayesian Optimization trials are shown.",
                "position": 268
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22795/x4.png",
                "caption": "Figure 4:Pipeline for prompt generation. A caption is taken from a dataset and passed to an LLM, which produces an edit instruction and a corresponding output caption. Additional metadata is generated for downstream filtering and for improving sample quality for synthetic audio generation.",
                "position": 1297
            }
        ]
    },
    {
        "header": "Appendix BManual Edit Tasks",
        "images": []
    },
    {
        "header": "Appendix CDataset Generation",
        "images": []
    },
    {
        "header": "Appendix DFine-tuning",
        "images": []
    },
    {
        "header": "Appendix EInference",
        "images": []
    },
    {
        "header": "Appendix FListening Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22795/figures/evaluation-interface.png",
                "caption": "Figure 5:Evaluation interface for the subjective listening study comparing SAO-Instruct with audio editing baselines.",
                "position": 2053
            }
        ]
    },
    {
        "header": "Appendix GResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22795/x5.png",
                "caption": "Figure 6:Edits performed by SAO-Instruct. The model has only access to the input audio and the edit instruction. It is able to perform global operations and local operations while keeping the overall background context intact.",
                "position": 2550
            },
            {
                "img": "https://arxiv.org/html/2510.22795/x6.png",
                "caption": "Figure 7:An example failure case where the phrasing of an instruction impacts edit quality and accuracy. While“remove the alarm”fails to suppress the alarm,“the alarm should be silent!”is more successful.",
                "position": 2559
            },
            {
                "img": "https://arxiv.org/html/2510.22795/x7.png",
                "caption": "Figure 8:Examples of failure cases where newly added sounds fail to blend naturally with the background and where edits in complex audio scenes lead to unintended edits.",
                "position": 2563
            }
        ]
    },
    {
        "header": "Appendix HBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix ILicenses",
        "images": []
    }
]