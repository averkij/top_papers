[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1INTRODUCTION",
        "images": []
    },
    {
        "header": "2RELATED WORK",
        "images": []
    },
    {
        "header": "3METHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16276/images/GameTalk/example-game.png",
                "caption": "Figure 1:Example of a game of rock-paper-scissors. All texts have been shortened. The game starts with asetting prompt. The two agents first engage in a conversation, using thecommunicationaction. When the conversation is over, they do thegame action. That concludes the game. Before each of these actions, they use theprivate CoT, to decide their actions, this is not shown to the opposite LLM. At the end of the game, one LLM is trained using the reward obtained from this episode.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/GameTalk/InternalStateEvaluation_2.png",
                "caption": "Figure 2:Illustration of howπ^t​r​u​e\\hat{\\pi}_{true}andπ^b​e​l​i​e​f\\hat{\\pi}_{belief}are obtained, in order to use them inISE,SRPandLO. Note that they are approximations, since they have to be computed without thePrivate CoT, which would influence the final action.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/GameTalk/GroupCreation.png",
                "caption": "Figure 3:Illustration of the generation process used in GRPO and DPO. The first three interactions correspond to the root conversation, which is then duplicated intok=3k=3parallel copies. Each copy is completed independently, and the reward obtained from each is used to train the policy on the first response after the copying step.",
                "position": 364
            }
        ]
    },
    {
        "header": "4EXPERIMENTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16276/images/TuneExperiment/image.png",
                "caption": "Figure 4:Analysis of reward shaping in the constrained Rock-Paper-Scissors game.Left:Spider plot with general metrics (rescaled for visualization), comparing anUntrainedmodel, aBaseagent trained only on game reward, and agents trained withISEandLOauxiliary rewards.Middle:Bar chart comparing win/draw/lose rates of all models.Right:Spider plot comparingBaseandLO-rewardagainst an agent trained withLO-rewardandNaturalness reward.Bottom:Table with the raw values of the plots,boldfor the best value andunderlinefor the second best.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/mainRPS.png",
                "caption": "Figure 5:Comparative analysis of training algorithms across the three game environments when applied with theGameTalkframework. Performance metrics are shown for(a)Rock-Paper-Scissors,(b)Bertrand Competition, and(c)Size-Price Bargaining. For each game, tables report exact metric values (best in bold, and second best underlined), and spider plots visualize the key trade-offs between reward (RR), game-specific outcomes (Win %,N​ENE,B​PBP), and our behavioral signals. The bar chart in (a) detailswin/draw/lose ratesfor Rock-Paper-Scissors.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/mainBertrand.png",
                "caption": "",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/mainSizePrize.png",
                "caption": "",
                "position": 758
            }
        ]
    },
    {
        "header": "5DISCUSSION",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Checklist",
        "images": []
    },
    {
        "header": "Appendix AHYPERPARAMETERS AND COMPUTE DETAILS",
        "images": []
    },
    {
        "header": "Appendix BMODELING OTHER PLAYERS",
        "images": []
    },
    {
        "header": "Appendix CSETTING PROMPTS",
        "images": []
    },
    {
        "header": "Appendix DADDITIONAL TRAINING ABLATIONS",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16276/images/TuneExperiment/kl-div.png",
                "caption": "Figure 6:Comparison between the base training process and two different exploration incentivization strategies. (a) base vs. modifications to the KL-divergence coefficient. (b) base vs. addition of the entropy term. Table: corresponding numerical values for all conditions.",
                "position": 1737
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/TuneExperiment/entropy.png",
                "caption": "",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/TuneExperiment/Lora.png",
                "caption": "Figure 7:Comparison between the base training process and two alternative LoRA rank configurations. Metrics are reported for ranks 8, 32 (base), and 64.",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix EGAMES ANALYSIS",
        "images": []
    },
    {
        "header": "Appendix FEVALUATION OF DPO VARIANTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16276/images/appDpoRPS.png",
                "caption": "Figure 8:Comparative analysis ofDPOvariants (DPO-pairsvs.DPO-permutations) against theUntrainedbaseline across our three game environments: (left to right) Rock-Paper-Scissors, Bertrand Competition, and Size-Price Bargaining.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/appDpoBertrand.png",
                "caption": "",
                "position": 2069
            },
            {
                "img": "https://arxiv.org/html/2601.16276/images/appDpoSizePrize.png",
                "caption": "",
                "position": 2075
            }
        ]
    },
    {
        "header": "Appendix GEXAMPLE CONVERSATION",
        "images": []
    }
]