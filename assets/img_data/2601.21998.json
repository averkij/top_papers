[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21998/x1.png",
                "caption": "Figure 1:LingBot-VA: An Autoregressive World Model for Robotic Manipulation.(1)Pretraining:LingBot-VAis pretrained on diverse in-the-wild videos and robot action data, enabling strong generalization across scenes and objects.\n(2)Comprehensive Evaluation:We conduct extensive experiments on real-world tasks (long-horizon, deformable objects, and precision manipulation) and simulation benchmarks, significantly outperforming state-of-the-art methods includingπ0.5\\pi_{0.5}.\n(3)Versatile Capabilities:Beyond policy learning, our model supports visual dynamics prediction and inverse dynamics inference from robot videos.\n(4)Emergent Properties:Our causal world modeling approach exhibits long-range temporal memory and strong few-shot adaptation ability.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21998/x2.png",
                "caption": "Figure 2:Framework overview:LingBot-VAis conditioned byautoregressive diffusionfor unifiedvideo-action world modeling.\nWe leverage a dual-streamMixture-of-Transformers (MoT)architecture that interleaves video and action tokens within a single sequence.\nAt each autoregressive step, the video stream (initialized from Wan2.2-5B) first predicts future latent visual states viaflow matching.\nThen the action stream decodes corresponding actions throughinverse dynamicsconditioning on the predicted visual transitions.",
                "position": 269
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21998/x3.png",
                "caption": "Figure 3:Teacher Forcing Attention Mask: Causal attention mask for unified video-action pretraining. Each token can only attend to preceding tokens in the temporal sequence.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2601.21998/x4.png",
                "caption": "Figure 4:Asynchronous pipeline design overview: The traditional synchronous pipeline (A) suffers from delays caused by blocked computations, while the asynchronous pipeline (B) addresses this issue by enabling parallel computation and execution. However, a naive asynchronous implementation (B-1) relies on outdated visual predictions. In contrast, we improve and refine asynchronous prediction through forward dynamic prediction (B-2), which updates stale predictions with recent real-world observations.",
                "position": 631
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21998/x5.png",
                "caption": "Figure 5:Real-world deployment results.We evaluateLingBot-VAon six manipulation tasks across three categories: long-horizon tasks (Make Breakfast, Pick Screws), precision tasks (Insert Tubes, Unpack Delivery), and deformable & articulated object manipulation (Fold Clothes, Fold Pants). Our method achieves state-of-the-art performance on both metrics.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2601.21998/x6.png",
                "caption": "Figure 6:Detailed task progressions and key execution steps of the six real-world tasks.Each task involves a sequence of manipulation primitives, with scoring criteria detailed in TablesS2throughS4.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2601.21998/x7.png",
                "caption": "Figure 7:Training dynamic comparison between different action network initialization streategy: Random initialization leads to unstable optimization (high gradient norms) and slow convergence. Although re-using video network weights stabilizes training, the resulting performance is not optimal. Our approach, which initializes by copying pretrained video weights with proper scaling, proves to be the most effective, ensuring smooth training dynamics and faster convergence.",
                "position": 1297
            },
            {
                "img": "https://arxiv.org/html/2601.21998/x8.png",
                "caption": "Figure 8:Sample efficiency comparison.LingBot-VAconsistently outperformsπ0.5\\pi_{0.5}across various data regimes on the “Make Breakfast” task, demonstrating superior data efficiency in the post-training stage.",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2601.21998/x9.png",
                "caption": "Figure 9:Temporal memory evaluation.Left: Success rates on two memory tasks (Wipe Plate and Search Box).LingBot-VAsignificantly outperformsπ0.5\\pi_{0.5}on both tasks, demonstrating superior temporal state tracking ability. Right: Visualization of evaluation environments.",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2601.21998/x10.png",
                "caption": "Figure 10:Novel object and spatial generalization.LingBot-VAsuccessfully generalizes to objects with varying shapes, textures, and positions.",
                "position": 1384
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AReal-world Evaluation Details",
        "images": []
    }
]