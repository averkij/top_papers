[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01044/x1.png",
                "caption": "",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2410.01044/x2.png",
                "caption": "",
                "position": 133
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01044/x3.png",
                "caption": "Figure 1:A simplified example showing how implicit rationales in pre-training data can be leveraged to improve reasoning.1: Implicit rationales (unstated logical connections) occur frequently in LLM pre-training data.2: As a result, existing LLMs pre-trained to replicate their pretraining data tend to omit these logical steps as well.3: However,Rationalystlearns to generate these rationales at inference time to supervise the chain-of-thought process for more accurate reasoning.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2410.01044/x4.png",
                "caption": "Figure 2:An example showing howRationalystworks at inference time.Rationalystgenerates implicit rationales given the current reasoning trajectory, which includes both the question and the reasoning steps generated so far1. Agent LLM generates multiple next-step candidates for reasoning, also based on the current reasoning trajectory2. Implicit rationale generated byRationalystis used to provide heuristics for choosing the next step candidates proposed by the agent LLM by estimating the probability of the next step candidate given the rationale3. The reasoning trajectory is updated iteratively with the highest scoring next step candidate4.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3BuildingRationalyst",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01044/x5.png",
                "caption": "Figure 3:We use LLMs to extract implicit rationales (enclosed by<BOT>and<EOT>in bold) that capture reasoning in unlabelled text (ยง3.1AandB).\nThe sample at the top is taken from unlabelled web-scale pre-training\ndatasets The Pile and the sample at the bottom is taken from existing datasets (GSM8K). These rationales are subsequently filtered based on whether they are useful for predicting future text (ยง3.1C).",
                "position": 246
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Empirical Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts used for rationale sampling",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01044/x6.png",
                "caption": "Figure 4:The prompt and in-context learning examples used for sampling rationales for GSM8K. The bolded rationales represent implicit rationales in the document.",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2410.01044/x7.png",
                "caption": "Figure 5:The prompt and in-context learning examples used for sampling rationales for ECQA. The bolded rationales represent implicit rationales in the document.",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2410.01044/x8.png",
                "caption": "Figure 6:The prompt and in-context learning examples used for sampling rationales for The Pile. The bolded rationales represent implicit rationales in the document.",
                "position": 1560
            }
        ]
    },
    {
        "header": "Appendix BPrompts used during inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01044/x9.png",
                "caption": "Figure 7:The prompt and in-context-learning demonstrations used during inference time to encourage the agent model reason step by step on GSM8K.",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2410.01044/x10.png",
                "caption": "Figure 8:The prompt and in-context-learning demonstrations used during inference time to encourage the agent model reason step by step on ECQA.",
                "position": 1573
            }
        ]
    },
    {
        "header": "Appendix CExamples of rationales generated at inference time",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01044/x11.png",
                "caption": "Figure 9:Rationales generated byRationalystfor the test set of MATH.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2410.01044/x12.png",
                "caption": "Figure 10:The prompt and in-context-learning demonstrations used during process supervision to elicit the feedback by directly reranking partial reasoning trajectory.",
                "position": 1593
            }
        ]
    },
    {
        "header": "Appendix DPrompts used for LLaMa-3 reranking",
        "images": []
    }
]