[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02017/x1.png",
                "caption": "Figure 1:VisualSync Overview.Given multiple unsynchronized videos capturing the same dynamic scene from different viewpoints, VisualSync recovers globally time‐aligned video streams by estimating temporal offsets between views. For example, in the volleyball scene, before synchronization the player’s motion is misaligned across videos; afterwards, a given timestamp in all three streams corresponds to the same moment.",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02017/x2.png",
                "caption": "Figure 2:Epipolar‐geometry cue for video sync:When cameras are time-aligned, keypoint tracks align with epipolar lines (bottom); misalignment causes deviations (middle). Minimizing these deviations across tracklets recovers the correct time offset.",
                "position": 113
            }
        ]
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02017/x3.png",
                "caption": "Figure 3:Proposed framework:Given unsynchronized videos, VisualSync follows a three-stage pipeline. Stage 0 estimates camera parameters with VGGT[wang2025vggt], dense correspondences with CoTracker3[karaev2024cotracker3], cross-view matches with MAST3R[leroy2024grounding], and dynamic objects with DEVA[cheng2023tracking]. In Stage 1, we estimate pairwise frame offsets by minimizing epipolar violations over matched trajectories. Stage 2 globally optimizes individual offsets to produce synchronized videos.",
                "position": 134
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02017/x4.png",
                "caption": "Figure 4:Qualitative Comparison of synchronization on Egohumans[khirodkar2023egohumans]across baselinesWe visually assess temporal synchronization by presenting magnified views of the shuttlecock’s position across time. In this complex scenario—marked by large temporal discrepancies, a small dynamic element, and moving cameras—Visual Sync achieves the most accurate alignment.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x5.png",
                "caption": "Figure 5:Qualitative Comparison of Video Sync across datasets.We show the synchronized videos on CMU-Panoptic, UDBD, 3D-POP and Egohumans dataset. Top 3 rows shows the estimated synchronized time stamps from 3 different views. The bottom row shows synchronized timelines across multiple videos. Our method performs robustly across diverse scenes.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x6.png",
                "caption": "Figure 6:Qualitative Synchronization of VisualSync on In-the-Wild Sports Videos.We showcase VisualSync on challenging multi-view sports footage with large camera motions, motion blur, and zoom variations. Despite these real-world challenges, our method achieves accurate synchronization. In the absence of ground-truth alignments, we qualitatively verify accuracy through the precise alignment of key events (e.g., ball release, contact) across views.",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x7.png",
                "caption": "Figure 7:K-Plane Rendering on VisualSync Results.We train K-Planes on CMU Panoptic[Joo_2017_TPAMI]multi-view videos for novel view synthesis. Unsynchronized inputs (1st) produce blurry results, while our synchronized outputs (2nd) are sharp and comparable to ground-truth sync (3rd). The 4th column shows real images. Our method enables high-quality synthesis from unsynchronized inputs.",
                "position": 782
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02017/x8.png",
                "caption": "Figure 8:Grounding-DINO[liu2023grounding]+ SAM2[ravi2024sam]Segmentation resultsWe visualize Grounding-DINO’s proposed bounding box given the dynamic labels produced by GPT4o, along with SAM2 segmentation results with confidence scores. Note that objects that are generally not dynamic (eg. basketball, football, chest) is identified as dynamic in these specific scenes due to inputting video frames into GPT4o.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x9.png",
                "caption": "Figure 9:Visualization of Cotracker and Mast3R correspondencesWe visualize actual spatial-temporal correspondences predicted using CotrackerV3 (temporal) and Mast3R (spatial).",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x10.png",
                "caption": "Figure 10:VGGT[wang2025vggt]Predicted Camera poses compared to GT poses for all datasetsWe visualize VGGT predicted camera poses and ground truth camera poses for Egohumans[khirodkar2023egohumans], Panoptic[Joo_2017_TPAMI], UDBD[kim2024sync], and 3D-POP[naik20233d]respectively. Different colors represent different multi-view cameras, while the corresponding lighter palette represents ground truth camera poses.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x11.png",
                "caption": "Figure 11:GPT4o Prompt used for automatic motion segmentationWe sample every 20th frame from our video and input to GPT4o with the following context and prompt to identify motion classes to be given to GroundingDINO module for robust video motion segmentation.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x12.png",
                "caption": "Figure 12:Failure Case VisualizationsWe visualize failure cases across our camera pose, motion segmentation, and spatial correspondence modules. Specifically, observe the incorrect predicted camera poses for the dynamic camera (red) against the ground truth (pink). Background individuals are occasionally mis-segmented for motion segmentation, and some segmentations appear fragmented. Furthermore, Mast3R sometimes generates incorrect correspondences within dynamic masks.",
                "position": 941
            }
        ]
    },
    {
        "header": "Appendix BFailure Case",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02017/x13.png",
                "caption": "Figure 13:Qualitative Comparison of VisualSync across datasetsWe show the synchronized\nvideos on CMU-Panoptic, UDBD, 3D-POP and Egohumans dataset using VisualSync. The top 3 rows show the estimated\nsynchronized time stamps from 3 different views. The bottom row shows synchronized timelines between multiple videos. Our method performs robustly across diverse scenes.",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2512.02017/x14.png",
                "caption": "Figure 14:Qualitative Comparison of synchronization on Egohumans across baselinesWe visualize synchronization results in the challenging volleyball sequence in Egohumans. Notice that VisualSync achieves the most accurate alignment even for egocentric views (orange highlight).",
                "position": 1012
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]