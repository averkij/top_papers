[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17761/x1.png",
                "caption": "Figure 1:Overview ofAR-Omni. Text, speech, and image inputs are tokenized and embedded into a shared space.\nA single autoregressive decoder operates over a joint vocabulary to generate a unified token stream. T denotes text, S denotes speech, and I denotes image.",
                "position": 287
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AR-Omni",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17761/x2.png",
                "caption": "Figure 2:Stage 1 pretraining losses of AR-Omni.\nFrom left to right: weighted NTP loss, perceptual loss, and total loss.\nCurves are smoothed for readability and plotted over 1k–93k training steps.",
                "position": 981
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17761/x3.png",
                "caption": "Figure 3:Loss curves of AR-Omni (Ours) and thesimple NTPtraining objective.\nThe naive objective exhibits a sharp loss jump, whereas AR-Omni maintains a smooth and stable loss throughout training.",
                "position": 1134
            },
            {
                "img": "https://arxiv.org/html/2601.17761/x4.png",
                "caption": "Figure 4:Case studies of AR-Omni: (a) multi-turn speech conversation (S→\\rightarrowS), (b) speech+image understanding with speech response (S+I→\\rightarrowS), and (c) speech-to-image generation (S→\\rightarrowI).",
                "position": 1140
            }
        ]
    },
    {
        "header": "6Further Analysis",
        "images": []
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BDataset Details",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17761/x5.png",
                "caption": "Figure 5:Multi-turn interleaved conversation example.",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2601.17761/x6.png",
                "caption": "Figure 6:Multi-turn speech conversation example.",
                "position": 1546
            },
            {
                "img": "https://arxiv.org/html/2601.17761/x7.png",
                "caption": "Figure 7:Qualitative image generation results with AR-Omni across diverse prompts and styles.",
                "position": 1549
            },
            {
                "img": "https://arxiv.org/html/2601.17761/x8.png",
                "caption": "Figure 8:Qualitative image generation results with AR-Omni across diverse prompts and styles.",
                "position": 1552
            },
            {
                "img": "https://arxiv.org/html/2601.17761/x9.png",
                "caption": "Figure 9:Qualitative image generation results with AR-Omni across diverse prompts and styles.",
                "position": 1555
            },
            {
                "img": "https://arxiv.org/html/2601.17761/x10.png",
                "caption": "Figure 10:Qualitative image generation results with AR-Omni across diverse prompts and styles.",
                "position": 1558
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]