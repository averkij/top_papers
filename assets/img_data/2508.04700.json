[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04700/x1.png",
                "caption": "Figure 1:SEAgent enables computer use agents self-evolving\nin novel environmentsby autonomously exploring and learning from their own experiences without human intervention. The specialist-to-generalist training strategy further enhances the development of a strong generalist agent.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04700/x2.png",
                "caption": "Figure 2:SEAgent autonomous exploration and experiential learning pipeline.Guided by tasks generated by the Curriculum Generator, the Actor Model is updated according to step-level rewards from the World State Model through verifiable reward functions tailored for different action types.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04700/x3.png",
                "caption": "Figure 3:The Average Precision on AgentRewardBenchlu2025agentrewardbench, where GUI-Judge exhibits an improvement in AP as the number of input middle states increases, showing a similar trend to that of the closed sourced GPT-4ohurst2024gptwhen compared with its base model.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2508.04700/x4.png",
                "caption": "Figure 4:Self-evolved task instructions and success rate (SR) curves across different software.Tasks are progressively upgraded by the Curriculum Generator without human intervention, based on the evolving capabilities of the Actor Model at different training phases.",
                "position": 622
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AWorld State Model",
        "images": []
    },
    {
        "header": "Appendix BCurriculum Generator",
        "images": []
    },
    {
        "header": "Appendix CDetails of Curriculum Generator.",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04700/x5.png",
                "caption": "Figure 5:SEAgent autonomous exploration pipeline.The agent (policy model) and World State Model iteratively generate new task and perform RL to become a specialist in novel software.",
                "position": 1961
            }
        ]
    },
    {
        "header": "Appendix DTest on TARS-1.5",
        "images": []
    },
    {
        "header": "Appendix ESensitivity Analysis on Key Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix FAblation on the Loss Balance Factor.",
        "images": []
    },
    {
        "header": "Appendix GReward Function for Different Actions.",
        "images": []
    },
    {
        "header": "Appendix HData Statistics during Iterative Reinforcement Learning.",
        "images": []
    },
    {
        "header": "Appendix IDetailed Prompt Templates.",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04700/x6.png",
                "caption": "Figure 6:Prompt Template of GUI-Judge for web agent trajectories evaluationswith history screenshots as input, its difference with default prompt of AgentRewardBench[35]is highlighted in bold.",
                "position": 2368
            },
            {
                "img": "https://arxiv.org/html/2508.04700/x7.png",
                "caption": "Figure 7:Prompt Template of GUI-Judge for OSWorld[67]trajectories, which prompts judge model to provide step level reward signal.",
                "position": 2371
            },
            {
                "img": "https://arxiv.org/html/2508.04700/x8.png",
                "caption": "Figure 8:Prompt Template for task buffer update, which generates new tasks in a curriculum manner and update software documents. The new tasks are used for actor to perform next phase of RL.",
                "position": 2374
            }
        ]
    },
    {
        "header": "Appendix JSelf documented usage manual on different software during exploration.",
        "images": []
    },
    {
        "header": "Appendix KBroader Impacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04700/x9.png",
                "caption": "Figure 9:Automatically generated usage manual during self explorationon VScode.",
                "position": 2396
            },
            {
                "img": "https://arxiv.org/html/2508.04700/x10.png",
                "caption": "Figure 10:Automatically generated usage manual during self explorationon GIMP.",
                "position": 2399
            },
            {
                "img": "https://arxiv.org/html/2508.04700/x11.png",
                "caption": "Figure 11:Automatically generated usage manual during self explorationon LibreOffice_Impress.",
                "position": 2402
            },
            {
                "img": "https://arxiv.org/html/2508.04700/x12.png",
                "caption": "Figure 12:Automatically generated usage manual during self explorationon LibreOffice_Writer.",
                "position": 2405
            }
        ]
    },
    {
        "header": "Appendix LSEAgent Self-Evolution Algorithm",
        "images": []
    }
]