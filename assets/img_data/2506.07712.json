[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07712/x1.png",
                "caption": "Figure 1:Accuracy and response length for Qwen2.5-0.5B across varying amounts of long CoT SFT data. Performance drops markedly at smaller data scales (8k-16k), even as response length increases significantly, indicating a critical failure mode in which the model generates longer but less accurate reasoning traces. We term this phenomenonLong CoT Degradation.",
                "position": 181
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07712/x2.png",
                "caption": "Figure 2:Comprehensive evaluation of multiple models trained with varying amounts of long CoT data. Accuracy is averaged across AIME24, AMC23, and MATH500, while response length is measured as the mean token count from 4,000 responses to MATH500. Per-benchmark results are provided in AppendixC.1.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Long CoT Degradation",
        "images": []
    },
    {
        "header": "3The Mechanism Behind Degradation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07712/x3.png",
                "caption": "Figure 3:Reflection ratios of Qwen models of different sizes trained on varying amounts of long CoT data. The reflection ratio refers to the proportion of model responses (out of 4,000 on the MATH500 benchmark) that exhibit reflective behavior, as identified through cross-validation.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x4.png",
                "caption": "Figure 4:Average response lengths of Qwen2.5-{1.5B, 3B}-Instruct models trained with varying amounts of long CoT data. Solid lines represent responses exhibiting reflection behavior; dashed lines denote responses without reflection. Results for more models are provided in AppendixC.2.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x5.png",
                "caption": "Figure 5:Arithmetic accuracy and response length on our synthetic benchmark for models trained with increasing amounts of long CoT data. Most models exhibit a sharp drop in arithmetic accuracy and a corresponding increase in response length after training on the 8k subset, with the exception of Llama-3.2-{1B,3B}-Instruct, whose initial performance is already low (<<<20% accuracy).",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x6.png",
                "caption": "Figure 6:A sample problem from our synthetic arithmetic benchmark, with answers from Qwen2.5-1.5B-Instruct before and after training on 8k long CoT examples. “Number of Calculation” indicates the total number of arithmetic operations performed in the response. Errors in the model’s intermediate reasoning are highlighted inred.",
                "position": 373
            }
        ]
    },
    {
        "header": "4Impact of Long CoT Supervision on RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07712/x7.png",
                "caption": "Figure 7:Impact of long CoT SFT data on downstream RL training across Qwen2.5 models. Top: Accuracy of RL-trained models over training steps. Bottom: Average response length during training. Each column corresponds to a different model scale (0.5B, 1.5B, 3B). Each curve represents an SFT data setting: Base (no SFT, serving as a baseline), 8k, and 128k (denoting the number of long CoT examples used during SFT). The horizontal axis in all plots indicates the RL training steps.",
                "position": 453
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BDetailed Experimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07712/x8.png",
                "caption": "Figure 8:Reflection ratios of LLaMA and Gemma models of different sizes trained on varying amounts of long CoT data. The reflection ratio refers to the proportion of model responses (out of 4,000 on the MATH500 benchmark) that exhibit reflective behavior, as identified through cross-validation.",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x9.png",
                "caption": "Figure 9:Prompt template used by GPT-4o-mini for reflection behavior identification.",
                "position": 1364
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x10.png",
                "caption": "Figure 10:Average response lengths of multiple models trained with varying amounts of long CoT data. Solid lines represent responses exhibiting reflection behavior; dashed lines denote responses without reflection.",
                "position": 1367
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x11.png",
                "caption": "Figure 11:Comprehensive evaluation of Qwen models trained with varying amounts of long CoT data.",
                "position": 1370
            },
            {
                "img": "https://arxiv.org/html/2506.07712/x12.png",
                "caption": "Figure 12:Comprehensive evaluation of LLaMA and Gemma models trained with varying amounts of long CoT data.",
                "position": 1373
            }
        ]
    },
    {
        "header": "Appendix CDetailed Evaluation Results",
        "images": []
    }
]