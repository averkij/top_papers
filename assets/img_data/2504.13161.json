[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13161/x1.png",
                "caption": "Figure 1:Pre-training a 1B model on ClimbMix shows better scaling effects than training on other datasets.\nWe measure the average performance on 12 downstream benchmarks.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2504.13161/x2.png",
                "caption": "Figure 2:Given large-scale pre-training data consisting of web-scale and curated sources, CLIMB identifies the optimal mixture of different topics (A, B, C) to improve performance in a target task (e.g., general reasoning). We compare the performance of state-of-the-art language models across different parameter scales on general reasoning benchmarks. CLIMB achieves a better tradeoff between model size and performance, demonstrating a more efficient scaling trend compared to prior models.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2504.13161/x3.png",
                "caption": "Figure 3:Visualization of CLIMB‚Äôs iterative search process using t-SNE.\nEach point represents a data mixture config in the search space, with different iterations (CLIMB-Iter1, CLIMB-Iter2, CLIMB-Iter3) illustrating how the search space is refined over iterations.\nInitially, the search explores a broad set of configurations (Iter 1), progressively narrowing in subsequent iterations (Iter 2 and Iter 3) as CLIMB selects more optimal mixtures.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2504.13161/x4.png",
                "caption": "Figure 4:The CLIMB framework overview.Upper section: CLIMB first preprocesses raw data via embedding and clustering it into groups.\nThese clusters serve as the basis for the search space, where a mixture is defined as a set of weights to combine different clusters.Lower section:\nCLIMB samplesnksubscriptùëõùëòn_{k}italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTmixtures in iterationkùëòkitalic_k, trains proxy models on a subset of them, and updates a predictor to estimate performance. The predictor prunes mixtures that are likely to perform poorly, so only the most promising mixtures proceed to full proxy training in subsequent iterations. Through progressively refining the search space and eliminating suboptimal candidates, CLIMB converges toward an optimized data mixture and balances general and domain-specific performance without exhaustive manual curation.",
                "position": 245
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3CLIMB: CLustering-based Iterative Data Mixture Bootstrapping",
        "images": []
    },
    {
        "header": "4Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13161/x5.png",
                "caption": "Figure 5:Performance of target models on MMLU benchmarks for different subject areas.\nFor both 350M and 1B target models, CLIMB used 350M proxy models, whereas CLIMB-Best@N used proxy models of the same size as the target models. CLIMB consistently improves performance across iterations, outperforming CLIMB-Best@N despite using smaller proxy models.",
                "position": 810
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13161/x6.png",
                "caption": "Figure 6:Weight analysis of ClimbMix across iterations.",
                "position": 1185
            }
        ]
    },
    {
        "header": "7ClimbMix: New Pre-training Data",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13161/x7.png",
                "caption": "Figure 7:Similarity between clusters and downstream tasks.",
                "position": 2158
            },
            {
                "img": "https://arxiv.org/html/2504.13161/x8.png",
                "caption": "Figure 8:Heatmap of weights across iterations.",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2504.13161/x9.png",
                "caption": "Figure 9:The Spearman rank correlation between predicted accuracy made by the predictor model and the groundtruth accuracy.",
                "position": 2517
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]