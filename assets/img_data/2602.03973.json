[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIProblem Formulation",
        "images": []
    },
    {
        "header": "IVOur Approach: VLS",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03973/figure/Figure21.png",
                "caption": "Figure 1:VLS pipeline overview.At environment time steptt, given RGB-D observationoto_{t}and language instructionll, VLS firstly utilize the Segment Anything Model (SAM[29]) and DINOv2[8]feature to ground condition into a set of spatial keypointsùí´\\mathcal{P}. Subsequently, a Vision-Language Model will be queried to generates a series of stage-aware differentiable programmatic reward functions{‚Ñõs}s=1S\\{\\mathcal{R}_{s}\\}_{s=1}^{S}, based on observation, task instruction and keypoints, which are used to guide the action generation process of the frozen base policyœÄ‚ãÜ\\pi^{\\star}: during the denoising sampling loop, the system precisely corrects action trajectories by injecting reward gradients, incorporating RBF[24]repulsion terms and a Feynman‚ÄìKac[44]based resampling mechanism to rapidly converge to high-reward regions while maintaining sampling diversity. Finally, VLS constructs a closed-loop stage switching system based on reward feedback, utilizing adaptive guidance strength and Schmitt-trigger[43]switching logic to monitor execution progress, thereby automatically triggering phase transitions or retry strategies when facing physical uncertainties (such as object displacement or manipulation failures), ensuring robust completion of long-horizon manipulation tasks in OOD environments.",
                "position": 265
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03973/figure/CALVINresult.png",
                "caption": "Figure 2:Steering methods comparison on CALVIN.Success rates for VLS (ours), DynaGuide, ITPS, and the base diffusion policy across movable objects (cubes) and articulated parts (drawer, switch, button, door). VLS achieves 94% average on movable objects (7.4√ó\\timesover base policy) and 87% on articulated parts (9.6√ó\\timesboost), outperforming prior steering methods by 15‚Äì25 percentage points. Error bars show standard deviation over 600 episodes per task.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2602.03973/x1.png",
                "caption": "Figure 3:(left) Ablation of VLS components (50 episodes per task). We compare Full VLS (gradient guidance + FK steering + RBF diversity, withK=10K=10) against variants that remove FK steering (w/o FKD), remove RBF diversity (w/o RBF), or remove gradient guidance (w/o grad). (right) Scaling with sample batch sizeKKondoor_left(50 episodes). LargerKKimproves performance but increases inference time, illustrating a compute‚Äìperformance tradeoff.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2602.03973/figure/pi05results.png",
                "caption": "Figure 4:Real-world Deployment on a Franka robot.(Left: In-distribution tasks) Task layouts, language instructions, and success rates for in-distribution real-world manipulation.\nLevel¬†1 (top) requires placing an orange onto a specified plate (red or green) based on the instruction.\nLevel¬†2 (bottom) introduces an additional object (banana), requiring sequential selection of both the target object and the target plate.\nBar plots report per-task and average success rates for the frozenœÄ\\pi-0.5 baseline and VLS. (Right: Out-of-distribution tasks) Task layouts, instructions, and results under test-time distribution shifts.\nWe evaluate three OOD variants:\n(1)Appearance shift(top), replacing the red/green plate with a previously unseen yellow plate;\n(2)Position shift(middle), swapping the locations of the two plates while keeping the instruction unchanged;\n(3)Object shift(bottom), replacing the banana with a never-before-seen mug and instructing the robot to place the mug on the green plate.\nEach task is evaluated over 20 trials.\nGrasping the correct object contributes 50% success, and full task completion contributes 100%.\nVLS consistently outperforms the baseline and maintains robust execution under real-world OOD conditions.",
                "position": 888
            }
        ]
    },
    {
        "header": "VIConclusion & Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]