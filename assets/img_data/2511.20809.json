[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20809/x1.png",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20809/x2.png",
                "caption": "Figure 2:Video Composition.Given input foreground and background videos, image-based methods (a)–(b) use only the first frame, while (c)–(e) take full video inputs.\n(a) Object insertion[wu2025qwenimagetechnicalreport]followed by Image-to-Video (I2V) and (b) end-to-end I2V composition SkyReels[ji2025layerflow]fails to retain motion due to lack of video access.\n(c) Manual copy-paste preserves motion but violates affordance (swan placed on ground).\n(d) Naive generative composition yields appearance and motion drift (e.g., black swan turns white).\n(e) Our method preserves identity and motion, and achieves affordance-aware placement with realistic blending (swan placed in water with wave and shadows).",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video Composition via Split-then-Merge",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20809/x3.png",
                "caption": "Figure 3:StM Decomposer.The StM Decomposer integrates off-the-shelf models to split unlabeled videos. First, motion segmentation generates a foreground mask, which is used to extract the foreground layer. An inpainting model then fills the “holes” in the masked background video. Finally, a video captioning model generates a descriptive text caption for the original video.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2511.20809/x4.png",
                "caption": "Figure 4:StM Composer Training.\nThe Composer is trained to reconstruct a ground-truth video latent from foreground, background, and text inputs.\nFirst, the foreground video is augmented, and all video inputs (augmented foreground, background, ground truth) are encoded into latents by a frozen Space-Time (ST) VAE.\nThe text prompt is encoded asZt​e​x​tZ_{text}. A noisy ground-truth latent (blue) is fused with background (green) and augmented foreground (yellow) latents via a projection layer to produce the visual representationZv​i​s​i​o​nZ_{vision}.\nA Diffusion Transformer then processesZv​i​s​i​o​nZ_{vision}andZt​e​x​tZ_{text}to predict a composed latent (red).\nThe identity-preservation loss comprises two weighted sub-losses comparing the prediction (red) against the ground truth (blue) using foreground- and background-aware masking.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2511.20809/x5.png",
                "caption": "Figure 5:Qualitative comparison.Our method (StM) uniquely preserves complex dynamics and achieves affordance-aware harmony where baselines fail.(Left)StM alone maintains both the rapid background camera motion and realistic foreground running motion.(Center)StM demonstrates affordance by adapting the boat’s orientation and height of the waves.(Right)StM accurately preserves the car’s semantic action, road alignment, and lighting consistency, unlike alternative methods.",
                "position": 291
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20809/x6.png",
                "caption": "Figure 6:Qualitative Results.StM demonstrates robust motion preservation and affordance awareness. Characteristic actions are faithfully retained (e.g., cheetah, tennis player). Subjects are plausibly integrated rather than “pasted,” as seen with the swan and duck adapted to water flow, and the lion and race car harmonized with appropriate scene lighting and placement.",
                "position": 353
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "AStM-50K Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/userstudy_results.png",
                "caption": "Figure 7:User Study Results.Pairwise preference win/tie/lose rates comparing our method (StM) against five baselines across six criteria. Preference for StM is shown in Green, ties in Grey, and preference for the baseline in Orange. The results demonstrate that StM consistently outperforms all baselines, achieving its highest preference rates in motion and identity preservation.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/vllm_results.png",
                "caption": "",
                "position": 889
            }
        ]
    },
    {
        "header": "BAdditional Qualitative Results and Instructions",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/user-study-1.png",
                "caption": "(a)Instructions: Study Overview & Inputs",
                "position": 974
            },
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/user-study-1.png",
                "caption": "(a)Instructions: Study Overview & Inputs",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/user-study-2.png",
                "caption": "(b)Instructions: Example Comparison",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/user-study-3.png",
                "caption": "(c)Question Interface: Context & Metrics",
                "position": 988
            },
            {
                "img": "https://arxiv.org/html/2511.20809/figures/assets/supplementary/user-study-4.png",
                "caption": "(d)Question Interface: Video Comparison & Rating",
                "position": 993
            }
        ]
    },
    {
        "header": "CLimitation and Efficiency Discussion",
        "images": []
    }
]