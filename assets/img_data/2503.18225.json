[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Decoupled Low-rank Adaptation (DeLoRA)",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18225/x1.png",
                "caption": "Figure 1:Visualizations (Left) of the original LoRA(Hu et al.,2022)and (Right) of our proposed method DeLoRA. In addition to the low-rank matricesB,A𝐵𝐴B,Aitalic_B , italic_A, we introduce a normalizationΞΞ\\Xiroman_Ξand a scaling factorλ𝜆\\lambdaitalic_λ, which effectively decouple the angular learning from the adaptation strength.",
                "position": 142
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18225/x2.png",
                "caption": "Figure 2:Learning rate robustness plots in Subject-driven generation task in terms of DINO scores (Left) and Euclidean distance between a finetuned vs pretrained projection layer weights (Right). Learning rates used for robustness evaluation were derived by multiplying the base learning rate in a range of factors.",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2503.18225/extracted/6303293/figures/overtraining3.png",
                "caption": "Figure 3:(Left) Euclidean Distance of finetuned weights to pretrained weights as a function of the number of training steps. (Right) Qualitative examples show that LoRA exhibits significant artifacts earlier in the process compared to DeLoRA, which maintains better image quality.",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2503.18225/x3.png",
                "caption": "Figure 4:Average column norms of parameters in the attention modules of Stable Diffusion’s Unet",
                "position": 1045
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AETHER and ETHER+ low-rank limitation",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CFixing the magnitude term in DoRA",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18225/x4.png",
                "caption": "Figure 5:Robustness analysis between DoRA with and without magnitude updates, with respect to learning rate changes from the optimal learning rate.",
                "position": 2252
            }
        ]
    },
    {
        "header": "Appendix DRobustness Ablation on DeLoRA’s boundary and angles",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18225/x5.png",
                "caption": "Figure 6:Learning rate robustness plots for DeLoRA in Subject-driven generation task in terms of DINO scores (Left) and Euclidean distance finetuned vs pretrained weights of a projection layer (Right). Ablation testing impact of increasing learning rate for boundary (λ𝜆\\lambdaitalic_λ) or angular weights (B⁢A𝐵𝐴BAitalic_B italic_A).",
                "position": 2263
            },
            {
                "img": "https://arxiv.org/html/2503.18225/x6.png",
                "caption": "Figure 7:Examples generated by DeLoRA-finetuned Stable Diffusion for personalized generation on a small set of subject-specific images (left), and for semantic map to image on ADE20K (right).",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2503.18225/x7.png",
                "caption": "Figure 8:Prolonged finetuning generated examples generated by DeLoRA, LoRA, and DoRA methods, up to time step 2600.",
                "position": 2277
            }
        ]
    },
    {
        "header": "Appendix EQualitative Examples",
        "images": []
    }
]