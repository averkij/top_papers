[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03040/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03040/x2.png",
                "caption": "Figure 2:We frame video diffusion models as spatial reasoners, treating context and target frames equally in the model architecture except that context frames are noise-free.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x3.png",
                "caption": "Figure 3:Auxiliary bounding box.The model is trained to generate a bbox (pixel values) located at the target object in the final frames as an explicit reasoning pattern[gandhi2025cognitive]. This auxiliary output improves object grounding accuracy.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x4.png",
                "caption": "Figure 4:RoPE indexing for non-contiguous context. Context frames(blue) are sampled sparsely to reduce redundancy but retain their original temporal indices. Target generation frames (orange) are given new indices with a fixed offset.",
                "position": 255
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03040/x5.png",
                "caption": "Figure 5:Qualitative comparison across methods.The instruction is: “The camera moves smoothly through a bedroom. Finally, it focuses ona bagon top of the dresser in the center of the frame.” Our method faithfully grounds the object present in the context, whereas other methods hallucinate the target object. Notably, the path to locate the target from our generated result is different from the GT. For the best experience, see the supplementary videos.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x6.png",
                "caption": "Figure 6:Object grounding data collection pipeline.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x7.png",
                "caption": "Figure 7:Visualization of Spatial Distance.We use generated frames (a) to reconstruct the point cloud (b), which is compared with the GT point cloud (c): Reconstructed point cloud (blue) is mostly overlapped with the GT point cloud, indicating our generated frames are spatially consistent. Distance between point clouds are used to measure the spatial consistency quantitatively.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x8.png",
                "caption": "Figure 8:Ablations on condition frame numbers for training and inference.In the inference ablation (a), we fix the training number at 169; in the training ablation (b), we fix the inference number at 337.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x9.png",
                "caption": "Figure 9:Visualization for ablations on condition frame number, CFG, and auxiliary bbox.The instruction is “The camera moves through an office space. Finally, it focuses ona monitorin the center of the frame.” For the best experience, see the supplementary videos.",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x10.png",
                "caption": "Figure 10:Qualitative results on scene navigation of a bedroom(left) and a kitchen(right).Our method delivers the highest perceptual quality and good camera controllability. Please refer to the supplementary videos for clearer visual comparisons.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x11.png",
                "caption": "Figure 11:Out-of-domain result.Our models can generalize to OOD scenarios like outdoor scenes and new categories, performing object grounding and scene navigation.",
                "position": 648
            }
        ]
    },
    {
        "header": "5Conclusions and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03040/x12.png",
                "caption": "Figure 12:Failure and success cases.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x13.png",
                "caption": "Figure 13:Ablation on grounding success rate over repeat time.",
                "position": 730
            },
            {
                "img": "https://arxiv.org/html/2512.03040/x14.png",
                "caption": "Figure 14:Ablation on different CFG weights .",
                "position": 733
            }
        ]
    },
    {
        "header": "6Suppplementary Materials",
        "images": []
    }
]