[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06601/x1.png",
                "caption": "Figure 1:Training data filtering makes LLMs resistant to adversarial fine-tuning without sacrificing general performance.Models whose training data has been filtered to remove text related to dual-use biology topics (left) have unaffected general capabilities and (right) have low biothreat proxy capabilities and resist up to 10,000 steps and 300M tokens of adversarial fine-tuning. We further detail results inSection3.",
                "position": 420
            }
        ]
    },
    {
        "header": "2Filtering Prevents Target Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06601/x2.png",
                "caption": "Figure 2:Our multi-stage data filtering pipeline:Our goal is to filter out data related to unwanted topics. We study biothreat-proxy knowledge as a representative example. All documents undergo initial “blocklist” filtering, where those without prohibited terms are retained without further review. Documents containing blocked terms (e.g., “pathogen(s)”) are escalated to a fine-tuned text classifier that evaluates semantic content. The classifier assigns probability scores for unsafe content: documents scoring below the predetermined threshold are retained, while those exceeding it are excluded from the training corpus. In practice, the vast majority of documents are approved by the blocklist and thus do not require review by the classifier stage. We further detail our methodology inSection2.",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2508.06601/x3.png",
                "caption": "Figure 3:Data filtering (our technique) performs competitively with Circuit-Breaking (CB) techniques under black-box evals and attacks.We evaluate data filtering approaches against baselines on general knowledge (higher is better) and biothreat proxy knowledge (lower is better). Dotted lines indicate random chance. We report performance across repeated non-deterministic attacks with error bars. Data filtering and CB methods are comparable: both have similarly minor effects on general capabilities, CB methods perform slightly better on MCQA biothreat proxy evaluations, and filtering methods perform slightly better on cloze biothreat proxy evaluations. Data filtering is robust to the input-space attacks, especially in the cloze-prompt setting.These results demonstrate that pretraining data filtering is effective at significantly preventing biothreat proxy knowledge, including random-chance-level performance on cloze-style prompts.",
                "position": 762
            }
        ]
    },
    {
        "header": "3Filtering Achieves State-of-the-Art Tamper-Resistance",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06601/x4.png",
                "caption": "Figure 4:Filtering biothreat proxy content from training data makes LLMs resist adversarial tampering.(Left & middle) Our LLMs are tamper-resistant up to 10,000 steps of fine-tuning on 305M tokens of biothreat proxy scientific text. (Right) Our LLMs are resistant to latent-space attacks competitively with Circuit-Breaking (CB) methods: CB+LAT performs better on multiple-choice evals while filtering performs better on cloze evals.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2508.06601/x5.png",
                "caption": "",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2508.06601/x6.png",
                "caption": "Figure 6:Pretraining data filtering cannot prevent in-context retrieval of unwanted information, but Circuit-Breaking can. However, no models resist an ensemble fine-tuning+in-context-retrieval attack.Baseline and filtered models alike can perform well on our “open-book” biothreat knowledge tests in which a passage containing the answer is given in context. Circuit-Breaking complements filtering by impairing the model’s ability to retrieve biothreat-related information in-context. No defenses, however, resist our ensemble attack.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2508.06601/x7.png",
                "caption": "Figure 7:Combining filtering with Circuit-Breaking (CB) techniques improves robustness to fewshot attacks.Adding CB to data filtering makes models robust to fewshot attacks (right) with comparable performance on other evals (left & middle). Dotted lines indicate random chance.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2508.06601/x8.png",
                "caption": "",
                "position": 881
            }
        ]
    },
    {
        "header": "4Defense in Depth",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowldgements",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATable of Released Models",
        "images": []
    },
    {
        "header": "Appendix BExperiments with Multi-Stage Pretraining Filtering",
        "images": []
    },
    {
        "header": "Appendix CFiltering’s Impact on Total FLOPs",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EComparing our Adversarial Fine-Tuning Attacks to Prior Works",
        "images": []
    },
    {
        "header": "Appendix FSynthetic Document Training Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06601/x9.png",
                "caption": "Figure 9:We were unable to find evidence that synthetic document training (SDT) on biothreat-misinformation improves over data filtering alone.We found that our approach to SDT mildly impeded the effectiveness of fine-tuning attacks under multiple-choice evaluation relative to filtering alone. However, under all other attacks SDT fails to improve – and sometimes degrades – resistance to attacks. InSection4.2, we discuss how this may be due to LLMs becoming attuned to bio-content and gaming multiple-choice evals(Dominguez-Olmedo et al.,2024; Balepur et al.,2025). Dotted lines indicate random chance.",
                "position": 4841
            },
            {
                "img": "https://arxiv.org/html/2508.06601/x10.png",
                "caption": "",
                "position": 4845
            }
        ]
    },
    {
        "header": "Appendix GExperiments on Models fromMaini et al. (2025)",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06601/x11.png",
                "caption": "Figure 10:Red-teaming results on models fromMaini et al. (2025)– Filtering offers little improvement to fine-tuning attacks and seems to increase vulnerability to fewshot attacks.All models perform comparably on general capability evaluations (MMLU, Lambada, PIQA, and HellaSwag, averaged in the left column). However, they exhibit different levels of roubstness to jailbreaking attacks. Overall, the filtered models (right) were only slightly more resistant to fine-tuning attacks than baselines models (left). Meanwhile, the filtered models were substantially more vulnerable to fewshot prompting attacks on average. Circuit Breaking (CB) and CB with Latent Adversarial Training (LAT) were effective. However, our synthetic document training (SDT) approach was not.",
                "position": 4894
            }
        ]
    },
    {
        "header": "Appendix HComparing our Models with Other Open Models",
        "images": []
    },
    {
        "header": "Appendix IKey Prompts used in Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06601/x12.png",
                "caption": "Figure 11:Annealing documents removed by each filter out of all documents by data source (log scale).We find that the vast majority of filtered documents are from Semantic Scholar (S2) and DCLM. The weak filter exhibits a high agreement rate with the strong filter for data from these sources, in contrast to the high disagreement rate observed for StackExchange documents.",
                "position": 5786
            }
        ]
    },
    {
        "header": "Appendix JFiltered Documents Analysis",
        "images": []
    }
]