[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23307/x1.png",
                "caption": "",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2503.23307/x2.png",
                "caption": "Figure 2:MoCha Architecture.MoCha is a end-to-end Diffusion Transformer model that generates video frames from the joint conditioning ofspeechandtext, without relying on any auxiliary signals. Both speech and text inputs are projected into token representations and aligned with video tokens through cross-attention.",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23307/x3.png",
                "caption": "Figure 3:MoCha’s Speech-Video Window Cross AttentionMoCha generates all video frames in parallel using a window cross-attention mechanism, where each video token attends to a local window of audio tokens to improve alignment and lip-sync quality.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Task: Talking Characters",
        "images": []
    },
    {
        "header": "3Model: MoCha",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23307/x4.png",
                "caption": "Figure 4:Multi-character Conversation and Character Tagging.MoCha supports generates multi-character conversion with scene cut. We design a specialized prompt template: it first specifies the number of clips, then introduces the characters along with their descriptions and associated tags. Each clip is subsequently described using only thecharacter tags, simplifying the prompt while preserving clarity. MoCha leverages self-attention across video tokens to ensures character and environment consistency. The audio conditioning signal implicitly guides the model on when to transition between clips.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2503.23307/x5.png",
                "caption": "Figure 5:Qualitative results of MoCha on MoCha-Bench.\nMoCha not only generates lip movements that are well-synchronized with the input speech, but also produces natural facial expressions that reflect the prompt along with realistic hand gestures and action movements",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2503.23307/x6.png",
                "caption": "Figure 6:Multi-Stage Training Strategy for MoCha.Text-Speech Joint training starts with close-up shots where speech conditioning has the strongest influence. At each stage, previous data is reduced by 50%, and harder tasks with weaker speech conditioning are introduced. Stage 0 uses text-only video data to establish a foundation for the future stages.",
                "position": 368
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.23307/x7.png",
                "caption": "Figure 7:Qualitative comparison between MoCha and baselines on MoCha-Bench.MoCha not only produces lip movements that align closely with the input speech—enhancing the clarity and naturalness of articulation—but also generates expressive facial animations and realistic, complex actions that faithfully follow the textual prompt. In contrast, SadTalker and AniPortrait exhibit minimal head motion and limited lip synchronization. Hallo3 mostly follows the lip-syncing but suffers from inaccurate articulation, erratic head movements, and noticeable visual artifacts.\nSince the baselines operate in an image-to-video (I2V) setting, we provide them with the first frame generated by MoCha as input for comparison. The first frame is cropped and resized as needed to meet the requirements of each baseline.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2503.23307/x8.png",
                "caption": "Figure 8:Human evaluation scores on MoCha-Bench.Scores range from 1 to 4 across five evaluation axes, where a score of 4 reflects performance that is nearly indistinguishable from real video or cinematic production. MoCha significantly outperforms all baselines across all axes. SadTalker and AniPortrait consistently received a score of 1 for action naturalness, as these methods only perform head movements. Text alignment is marked as not applicable (N/A) for these baselines since they do not accept text input.",
                "position": 435
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]