[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10881/figure/teaser.png",
                "caption": "",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10881/figure/MoCapAnything_framework.png",
                "caption": "Figure 2:Detailed architecture of our method. A multi-modal Reference Prompt Encoder fuses mesh, skeleton, and appearance of the target asset into per-joint queries. A monocular video is converted into a 4D mesh sequence, and both mesh and video features are extracted. The Unified Motion Decoder fuses these signals via multi-branch attention to predict 3D keypoints, which are converted to asset-specific joint rotations via an optimization-based IK layer.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10881/figure/figure-compare.png",
                "caption": "Figure 3:Qualitative comparison with GenZoo on the Truebones Zoo dataset.Our method produces smoother trajectories and maintains stable, anatomically plausible motions across a wide variety of skeleton types, including non-quadrupeds. In contrast, GenZoo is limited to quadruped structures and often fails to generalize to more diverse or complex skeletal configurations. Visualizations highlight our approach’s superior accuracy, robustness, and generalization ability.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2512.10881/figure/3.png",
                "caption": "Figure 4:Truebones Zoo-test results.Each row visualizes one evaluation sequence.Row 1: Input video frames.Row 2: Same-species reference skeleton and predicted mocap results.Rows 3–5: Reference skeletons from three different species and retargeted motions by our method.\nOur method generalizes across species and produces stable, anatomically plausible 3D motion.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2512.10881/figure/4.png",
                "caption": "Figure 5:Real-world (Wild) results.Similar layout as Figure4.Row 1: Input wild video frames.Row 2: Same-species reference skeleton and predicted mocap outputs.Rows 3–4: Cross-species reference skeletons and our retargeted motion predictions.\nDespite real-world challenges, our method maintains robustness and stability.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2512.10881/figure/1.png",
                "caption": "Figure 6:More Truebones in-the-wild mocap results.Our method generalizes to a diverse range of species and scenarios.",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2512.10881/figure/2.png",
                "caption": "Figure 7:Unconstrained cross-species retargeting.Examples of using our model to retarget motion from one species to another, yielding diverse, creative, and physically plausible animations. 1st row: from chicken to Raptor, 2nd row: Flamingo to Jaguar.",
                "position": 701
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6More Visualization Results",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8Evaluation Metrics",
        "images": []
    }
]