[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17895/x1.png",
                "caption": "Figure 1:Enhanced sensor depthpowered by our proposed MDM pretraining, which leverages naturally missing depth measurements in RGB-D sensors as masks to learn metric-scale, complete, and accurate depth representations. The resultingLingBot-Depthmodel serves as a powerful spatial perception prior for downstream applications, including 3D point tracking and dexterous grasping.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Masked Depth Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17895/x2.png",
                "caption": "Figure 2:Illustration of the proposed Masked Depth Modeling framework.\nDepth tokens corresponding to missing sensor measurements are masked, and a ViT encoder learns a joint embedding from the contextual tokens (i.e., the RGB frame) and the remaining unmasked depth tokens.\nIn the decoder stage, latent depth tokens are discarded, and a ConvStack decoder reconstructs the full depth map from latent contextual tokens.\nWe put an unmasked depth map in the bottom-right as the reference.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x3.png",
                "caption": "Figure 3:Multi-query depth-to-RGB attention visualization.For two scenes, (a) an aquarium with densely packed objects and (b) an indoor shelf with heterogeneous materials, we select three depth query patches (Q1–Q3) and visualize their attention over RGB tokens. Each row shows the masked input depth (with query location marked by⋆\\star), the attention overlay on the RGB image, and the refined depth output. Different queries attend to distinct, spatially corresponding regions, confirming that the joint embedding captures fine-grained cross-modal geometric–appearance associations. The RGB-D camera we used here is Orbbec Gemini-335.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2601.17895/figures/diversity_figure.png",
                "caption": "Figure 4:Our data curation pipelines.Samples from a total of 2.1M real-captured samples plus 1.0M simulated captures are gather in the top row. In the bottom row, we show the RGB-D inputs and the GT depth maps accordingly.",
                "position": 284
            }
        ]
    },
    {
        "header": "3Data Curation Pipelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17895/x4.png",
                "caption": "Figure 5:Mask ratio distributionsfor our curated synthetic and real-world RGB-D datasets.\nWe compute the ratio of invalid depth pixels as the original mask ratio. Note that the synthetic data was processed by open-source SGM[10]algorithm, it has more missing measurements in the simulated sensor depth than the real captures.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x5.png",
                "caption": "Figure 6:A synthetic data sample from our pipeline. Each sample includes an RGB image, a perfect rendered depth, a stereo pair with speckle patterns, a ground-truth disparity map, and a simulated sensor depth computed via semi-global matching (SGM) to mimic real-world active depth camera artifacts.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2601.17895/figures/device-divided.jpg",
                "caption": "Figure 7:Our scalable RGB-D capture system. Left: individual hardware components, including multiple RGB-D sensors, mounting fixtures, and supporting accessories. Right: the assembled capture system mounted on a tripod, enabling scalable and flexible real-world RGB-D data acquisition.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2601.17895/figures/device-full.jpg",
                "caption": "",
                "position": 370
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17895/x6.png",
                "caption": "Figure 9:Qualitative comparison of depth completionresults across four benchmarks. For each dataset, we show the RGB input, sparse/masked depth input, and predictions from OMNI-DC, PromptDA, PriorDA, and ours. Our method produces sharper boundaries and more complete structures, particularly in regions with severe occlusion or sparse observations.",
                "position": 930
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x7.png",
                "caption": "Figure 10:Epoch-wise comparison of FoundationStereotrained with different depth priors. We report EPE (top) and BP-1.0 (bottom) at epochs 5, 10, and 15 across five benchmarks.",
                "position": 1273
            }
        ]
    },
    {
        "header": "5Extensions and Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17895/x8.png",
                "caption": "Figure 11:Video depth completion results under challenging scenarios.The raw depth maps from the Orbbec sensor exhibit significant missing regions (shown in black) around transparent and reflective surfaces such as glass walls, windows, mirrors, and aquarium tunnels.",
                "position": 1314
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x9.png",
                "caption": "Figure 12:Video depth completionresults compared with ZED stereo depth. For each scenario, the top row shows ZED depth and the bottom row shows our predictions. The ZED sensor suffers from significant missing regions on transparent and reflective surfaces, while our model produces spatially complete and temporally consistent depth maps throughout all sequences.",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x10.png",
                "caption": "(a)Camera tracking and scene reconstruction. From left to right: RGB input, raw sensor depth, our refined depth, estimated camera trajectories (sensor depth in blue vs. our depth in red), and the reconstructed scene geometry.",
                "position": 1339
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x10.png",
                "caption": "(a)Camera tracking and scene reconstruction. From left to right: RGB input, raw sensor depth, our refined depth, estimated camera trajectories (sensor depth in blue vs. our depth in red), and the reconstructed scene geometry.",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x11.png",
                "caption": "(b)Dynamic 3D point tracking. Top: query points on the target object. Middle: 3D tracked trajectories (rainbow-colored by time). Bottom: corresponding depth maps.",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x12.png",
                "caption": "Figure 14:Qualitative results of the grasping experiment.Left: hardware setup with the robotic arm, dexterous gripper, and depth camera. Right: RGB, raw sensor depth, and our refined depth for four target objects. The raw depth is severely corrupted for reflective (steel cup) and transparent (glass cup, storage box) objects, while our method produces complete, geometrically accurate depth maps.",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x13.png",
                "caption": "",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2601.17895/x14.png",
                "caption": "Figure 15:Grasp pose generation and real-world execution.Top: predicted grasp poses rendered as a dexterous hand overlaid on the point cloud reconstructed from our refined depth. Bottom: successful grasps executed by the robotic system on each target object.",
                "position": 1385
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]