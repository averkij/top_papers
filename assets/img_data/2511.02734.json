[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/x1.png",
                "caption": "Table 1:For each benchmark, the table reports whether each trait is fully (✓), partially (✓), or not (✗) addressed. Detailed explanations are provided in AppendixA.",
                "position": 275
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/x1.png",
                "caption": "Figure 1:Overview of theCostBenchpipeline.\nStarting from high-quality queries generated from combinations of user preferences, the agent constructs its plan, then interacts with an environment set up with atomic and composite tools under flexible cost assignments (atomic tool costs are randomized between 15 and 25 in our experiments), and executes actions along an customizable dynamic blocking module to achieve its goal.",
                "position": 500
            }
        ]
    },
    {
        "header": "3CostBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/x2.png",
                "caption": "Figure 2:Models’ average normalized edit distance and exact match ratio for task sequences of length five to eight.\nModels show significantly lower performance as task complexity increases.\nIn the most challenging case with task sequence length of eight, even the strongest model,GPT-5, achieves less than 75% exact match ratio.",
                "position": 872
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/x3.png",
                "caption": "Figure 3:LLMs’ performance on CostBench under different standard deviations of composite tool cost noise. Models tend to perform better with higher noise levels, indicating high sensitivity to tool cost variations.",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2511.02734/x4.png",
                "caption": "",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2511.02734/x5.png",
                "caption": "Figure 4:Coverage rates of different LLMs across task sequences of length 5 to 8. Model performance in the static setting shows a strong positive correlation with coverage rate.",
                "position": 957
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/x6.png",
                "caption": "Figure 5:LLMs’ performance in CostBench’s dynamic blocking setting. All models show consistent EMR drops under ban tool, cost change, and preference change conditions, indicating weak replanning and adaptation abilities in dynamic environments.",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2511.02734/x7.png",
                "caption": "Figure 6:Performance ofGemini-2.5-ProandQwen3-14Bunder increasing numbers of blocking events (task sequence length = 7).\nEach curve represents a different blocking type. Both models degrade with more blockings, especially under frequent cost changes or tool bans, withQwen3-14Bshowing near-total failure.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2511.02734/x8.png",
                "caption": "",
                "position": 1023
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparison Traits Details",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/x9.png",
                "caption": "Figure 8:Performance of Qwen3-8B and Qwen3-14B on ANED and EMR(%)across different seeds. Results demonstrate low variations with different seeds, with variations in ANED and EMR not exceeding 5% across seeds.",
                "position": 2834
            }
        ]
    },
    {
        "header": "Appendix DAnalysis and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02734/figures/data_annotation.png",
                "caption": "Figure 9:Annotation Screenshot",
                "position": 3023
            }
        ]
    },
    {
        "header": "Appendix EData Annotation",
        "images": []
    }
]