[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04601/extracted/6419853/openvision_logo.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2505.04601/extracted/6419853/logo_browser.png",
                "caption": "",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2505.04601/extracted/6419853/github.png",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2505.04601/extracted/6419853/logo_hf.png",
                "caption": "",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2505.04601/x1.png",
                "caption": "Figure 1:Thetoptable compares ourOpenVisionseries to OpenAI’s CLIP and Google’s SigLIP. Thebottomfigure showcases thatOpenVisionattain competitive or even superior multimodal performance than OpenAI’s CLIP and Google’s SigLIP.",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2OpenVisionTraining and Evaluation",
        "images": []
    },
    {
        "header": "3Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04601/x2.png",
                "caption": "Figure 2:Ablations on the impact of an auxiliary decoder and synthetic captions. Results show that both contribute to better performance across multimodal benchmarks. We present performance gaps between different variants and our setting.",
                "position": 1380
            }
        ]
    },
    {
        "header": "4Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.04601/x3.png",
                "caption": "Figure 3:Comparison of training time and average multimodal performance between ourOpenVisionand OpenAI-CLIP on both LLaVA-1.5 and LLaVA-Next. Larger markers correspond to vision encoders with higher input resolutions. As a fully open and cost-effective vision encoder,OpenVisionachieves higher performance with significantly less pre-training time.",
                "position": 1572
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]