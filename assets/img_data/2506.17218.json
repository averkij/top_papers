[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17218/x1.png",
                "caption": "Figure 1:Multimodal Reasoning Examples.Mirage interleaveslatent visual tokens, which represent compact imagery visual features, with explicit text tokens to solve diverse spatial reasoning multimodal tasks, boosting the reasoning performance without the full pixel-level image generation.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multimodal Reasoning with Latent Visual Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17218/x2.png",
                "caption": "Figure 2:Pipeline of Mirage Framework.Stage 1 jointly supervises text and latent visual tokens, grounding the latter in the visual subspace; Stage 2 drops the latent supervision, anchoring the grounded latent tokens for subsequent text generation.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2506.17218/x3.png",
                "caption": "Figure 3:Data-generation Pipeline.For each questionâ€“answer pair, we first create a helper image with task-specific tools (here, annotate the map with arrows), then prompt a VLM to produce textual reasoning that embeds this image. The text and helper image together form the synthetic multimodal trajectory used for training.",
                "position": 259
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17218/x4.png",
                "caption": "Figure 6:Performance with Helper Images as Input Priors.We evaluate model accuracy using synthesized helper images under both zero-shot and fine-tuned settings. The results highlight the informativeness of the generated images and confirm their high data quality.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2506.17218/x5.png",
                "caption": "Figure 7:Visualization of Latent Embeddings.We visualize our latent tokens along with text and image embeddings with t-SNE. Our latent tokens cluster near, yet just outside, the visual representation subspace, consistent with the two-stage training design.",
                "position": 891
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17218/x6.png",
                "caption": "Figure 8:An example of the helper image of the VSP Spatial Reasoning task.",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2506.17218/x7.png",
                "caption": "Figure 9:An example of the helper image of the VSP Spatial Planning task.",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2506.17218/x8.png",
                "caption": "Figure 10:An example of the helper image of the BLINK task.",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2506.17218/x9.png",
                "caption": "Figure 11:An example of the helper image of the SAT task.",
                "position": 1556
            }
        ]
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    }
]