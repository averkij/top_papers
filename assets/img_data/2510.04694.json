[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04694/x1.png",
                "caption": "Figure 1:Visualization of the typical divergence in MoE routing weights across model layers between English and a high-, medium-, and low-resource language. There is consistently lower divergence in the middle layers, where experts are shared across languages. Languages the model does not understand (e.g. Bambara) fail to leverage similar experts as top languages. In this work, we also present a steering method that activates similar experts to English (red arrows) and results in improved multilingual generalization (e.g. an increase in MGSM-Bengali from0.7760.776to0.8240.824).",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04694/x2.png",
                "caption": "Figure 2:Visualization with more languages of routing divergence from English across model layers based on Qwen3-30B-A3B, where the U-shape can be seen for all. Each line is colored by how well the model understands that language (Belebeleaccuracy), highlighting a strong correlation between the two. We label a few notable plotted languages, but provide the same graph (along with 3 more models) colored to better distinguish languages in AppendixA.2.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work on Multilingual LLMs",
        "images": []
    },
    {
        "header": "3Mixture-of-Experts Preliminaries",
        "images": []
    },
    {
        "header": "4Interpretability Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04694/x3.png",
                "caption": "Figure 3:Routing Entropy per Layer forOLMoE.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/phimoe_consistency.png",
                "caption": "Figure 4:Token routing consistency (within a sequence), across layers inPhi-3.5-MoE.",
                "position": 299
            }
        ]
    },
    {
        "header": "5Intervention Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04694/x4.png",
                "caption": "Figure 5:Plot of the Number of Identified Experts per Layer, withτ=0.3\\tau=0.3forQwen3. The red vertical bars delimit the region in which we intervene.",
                "position": 331
            }
        ]
    },
    {
        "header": "6Intervention Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04694/icons/qwen.png",
                "caption": "Table 1:Summary of Intervention Results. Target layers are the model layers where the intervention takes place. The expert-selection thresholdτ\\tauand intervention method are described in Section5. Given the target layers andτ\\tau-value, we provide the number of experts selected for steering.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2510.04694/icons/microsoft.png",
                "caption": "",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2510.04694/icons/openai.png",
                "caption": "Table 2:Per-Language Intervention Results. Intervention specifics are provided in Table1.",
                "position": 553
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04694/images/qwen3_routingdivergence.png",
                "caption": "Figure A.1:Mean entropy-normalized JS-Div perOLMoElayer for 12 non-English languages. This is the same plot as Figure2, simply colored for language labeling.",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/olmoe_routingdivergence.png",
                "caption": "Figure A.2:Mean entropy-normalized JS-Div perOLMoElayer for 12 non-English languages. We noteOLMoE’s poor multilingual capabilities.",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/gptoss_routingdivergence.png",
                "caption": "Figure A.3:Mean entropy-normalized JS-Div per GPT-OSSlayer for 12 non-English languages.",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/phimoe_routingdivergence.png",
                "caption": "Figure A.4:Mean entropy-normalized JS-Div perPhi-3.5-MoElayer for 12 non-English languages. Compared to the others,Phi-3.5-MoEdoes not display the same U-shape, as the first few layers surprisingly have very low divergence, especially the first layer. We verified and saw that a small subset of available experts were being called for all languages in layer 1, which is behavior that requires further investigation.",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/qwen3_entropy.png",
                "caption": "Figure A.5:Caption for figure 3.",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/qwen3_entropy.png",
                "caption": "Figure A.5:Caption for figure 3.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2510.04694/x5.png",
                "caption": "Figure A.6:Caption for figure 4.",
                "position": 2420
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/phimoe_entropy.png",
                "caption": "Figure A.7:Caption for figure 1.",
                "position": 2431
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/gpt_entropy.png",
                "caption": "Figure A.8:Caption for figure 2.",
                "position": 2441
            },
            {
                "img": "https://arxiv.org/html/2510.04694/images/expertid_gptoss_swh_layer15.png",
                "caption": "Figure A.9:Example of visualization of our metric for selecting specialized experts. This is theΔ\\Deltadescribed in Section5.2, the difference in activation relative frequency as defined in Equation3. Here we display an example;Δ\\Deltabetween Swahili and English (onFLoRes).",
                "position": 2470
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]