[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21204/x1.png",
                "caption": "Figure 1:The architecture of a N-gram EmbeddingÂ layer(Huanget al.,2025). The embedding of each token is augmented by the N-gram Embedding branch.",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2N-gram Embedding Layer",
        "images": []
    },
    {
        "header": "3Comparative Analysis of Expert and Embedding Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21204/x2.png",
                "caption": "Figure 2:The scaling curve of MoE model and N-gram Embedding (NE) model. The horizontal axis is the ratio of total parameters to the activated parameters (280M). The axes of Figures on the right panel is converted to a logarithmic scale. For the two NE curves, we prepend a dashed line to connect the corresponding base MoE model without NE.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x3.png",
                "caption": "(a)",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x3.png",
                "caption": "(a)",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x4.png",
                "caption": "(b)",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x5.png",
                "caption": "Figure 4:Comparison of training and validation loss under different combinations ofNNandKK.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x6.png",
                "caption": "Figure 5:Layer-wise analysis of L2 norms for module outputs versus their corresponding identity branches, alongside the ratio of these norms. Each shortcut layer comprises two sub-layers, denoted by suffixes 0 and 1.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x7.png",
                "caption": "(a)",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x7.png",
                "caption": "(a)",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x8.png",
                "caption": "(b)",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x9.png",
                "caption": "(a)",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x9.png",
                "caption": "(a)",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x10.png",
                "caption": "(b)",
                "position": 482
            }
        ]
    },
    {
        "header": "4Efficient Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21204/x11.png",
                "caption": "(a)",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x11.png",
                "caption": "(a)",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2601.21204/x12.png",
                "caption": "(b)",
                "position": 512
            }
        ]
    },
    {
        "header": "5Integration with Per-Layer Embedding",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21204/x13.png",
                "caption": "Figure 9:Loss comparison of N-gram Embedding (NE), PLE, and PLNE under the 790M activated-parameter setting. Note that PLE and PLNE are compared against NE at two distinct parameter scales.",
                "position": 598
            }
        ]
    },
    {
        "header": "6LongCat-Flash-Lite",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21204/x14.png",
                "caption": "Figure 10:Smoothed training loss curves of LongCat-Flash-Lite and LongCat-Flash-Lite-Vanilla. The loss drop at 420B tokens coincides with the batch size increases.",
                "position": 662
            }
        ]
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "8Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]