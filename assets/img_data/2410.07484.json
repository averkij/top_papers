[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07484/x1.png",
                "caption": "Figure 1:Illustration of WALL-E mining a diamond in Minecraft.Step 1-2: the agent makes a plan via MPC with the initial unaligned world model, resulting in a failed action for mining iron ore. Step 3: by comparing real trajectories with the world model predictions,WALL-Elearns a critical rule that if the tool is not proper to the material being mined, the action will fail. Step 4-5: the learned rule helps the world model make accurate predictions for transitions that were predicted mistakenly in MPC. Step 6: the agent accordingly modifies its plan and replacesstone pickaxewith aniron pickaxetoward completing the task.",
                "position": 147
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07484/x2.png",
                "caption": "Figure 2:Overview ofWALL-E. The agent’s action per step is controlled by MPC, where the agent model plans actions in a look-ahead window based on the LLM+rules world model’s predictions.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07484/x3.png",
                "caption": "Figure 3:Rule Learning details.\nThe rule learning module iteratively refines the rules by comparing the world model predicted trajectories with the agent’s actual trajectories in the environment.\nThe rule learning takes five steps: (1) comparing predicted and actual trajectories; (2) learning new rules from real trajectories; (3) refining learned rules; (4) translating natural language rules to code; and (5) rule set pruning via solving a maximum coverage problem. (2)-(4) are handled by LLMs, while (1) and (5) are executed by programs.",
                "position": 249
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07484/x4.png",
                "caption": "Figure 4:Comparison ofWALL-Eand baselines on 134 testing tasks from the ALFWorld benchmark.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2410.07484/x5.png",
                "caption": "Figure 5:Cover rate of LLM failed predictions across different actions over iteration times during training. The cover rate represents the probability that the LLM’s failed predictions are addressed by the rules obtained during the rule learning process. The predictions and rules are categorized by action type:craft,mine,gatherandfight. The learnt rules at each iteration are displayed in black under each node, labeled with their respective rule IDs.",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2410.07484/x6.png",
                "caption": "Figure 6:Learning curve comparison between rule learning (e.g., WALL-E) and buffered trajectory (e.g., GITM) over 10 iterations on Minecraft tasks during training. The left plot shows the average success rate (%) across all tasks, where a higher value indicates more tasks successfully completed. The right plot illustrates the average number of replanning rounds, with fewer rounds indicating higher efficiency in task completion.",
                "position": 900
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Related Work",
        "images": []
    },
    {
        "header": "Appendix BDetailed Prompt",
        "images": []
    },
    {
        "header": "Appendix CEnvironments’ State Space and Action Space",
        "images": []
    },
    {
        "header": "Appendix DLearned Rules",
        "images": []
    },
    {
        "header": "Appendix EExperiment Details",
        "images": []
    },
    {
        "header": "Appendix FGreedy Algorithm",
        "images": []
    },
    {
        "header": "Appendix GCode-based rules verification logic",
        "images": []
    },
    {
        "header": "Appendix HLimitation and Future Work",
        "images": []
    }
]