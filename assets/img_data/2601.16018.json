[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16018/completion-timeseries.png",
                "caption": "Figure 1:Natural completion rate over a 6.5-hour extraction run.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2601.16018/box_plots_all_thresholds.png",
                "caption": "Figure 2:Token Count Distribution Analysis Across All Threshold Combinations.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2601.16018/only_50s.png",
                "caption": "Figure 3:Token Count Distribution Across Suffix Entropy Thresholds (Lemma Diversity≥\\geq50%)",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2601.16018/qwen3-1.7_dataset.png",
                "caption": "Figure 4:Qwen3-1.7B CPT Dataset Distribution across Four Phases.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2601.16018/qwen4b_dataset.png",
                "caption": "Figure 5:Qwen3-4B CPT Dataset Distribution Single Phase.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2601.16018/tokenizer_quality_analysis.png",
                "caption": "Figure 6:Tokenizer quality analysis: Turkish vocabulary coverage vs. morphological integrity. Turkish Token Count (x-axis) measures vocabulary coverage of Turkish words and morphemes, while Pure Token Count (y-axis) represents morphological integrity—the count of tokens corresponding to complete morphological units. Bubble size indicates Unique Token Count, and bubble color (viridis colormap) represents Pure Token Count, where darker purple indicates lower values and brighter yellow indicates higher values. Only Turkish pre-trained MLM models (and one multilingual model, mmBERT-base) from Table22are evaluated.",
                "position": 790
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16018/base_mlm_vs_downstream.png",
                "caption": "Figure 7:Relationship between MLM validation loss and downstream retrieval performance across ModernBERT-base versions v4-v6. We restrict the comparison to these specific versions to ensure parity in pre-training token counts and validation set sizes.",
                "position": 1533
            },
            {
                "img": "https://arxiv.org/html/2601.16018/large_mlm_vs_downstream.png",
                "caption": "Figure 8:Relationship between MLM validation loss and downstream retrieval performance across ModernBERT-large versions.",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2601.16018/qwen3-1.7b_loss.png",
                "caption": "Figure 9:Qwen3-1.7B CPT Training and Validation Loss Across Four Phases.",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2601.16018/qwen4b_loss.png",
                "caption": "Figure 10:Qwen3-4B CPT Training and Validation Loss Curves.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2601.16018/muhakim-pipeline.png",
                "caption": "Figure 11:Muhakim Model Training Pipeline.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2601.16018/1_7b_qwen_armo.png",
                "caption": "Figure 12:Benchmark Performance of 1.7B Decoder-Only Models Across Context Lengths Using the Muhakim Reward Model.",
                "position": 2270
            },
            {
                "img": "https://arxiv.org/html/2601.16018/4b_qwen_armo.png",
                "caption": "Figure 13:Benchmark Performance of 4B Decoder-Only Models Across Context Lengths Using the Muhakim Reward Model.",
                "position": 2273
            },
            {
                "img": "https://arxiv.org/html/2601.16018/comparison_rewards_by_token_length-filtered.png",
                "caption": "Figure 14:Rewards Comparison: Base vs CPT Models Across Token Lengths.",
                "position": 2284
            },
            {
                "img": "https://arxiv.org/html/2601.16018/model_performance_2d.png",
                "caption": "Figure 15:Model Performance Comparison: Legal Score vs. MTEB Score.",
                "position": 2426
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Funding",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Data Availability",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AResults Reproduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16018/post_train_retrieval_2d.png",
                "caption": "Figure 16:Post-Trained Retrieval Models Performance Comparison: Legal Score vs. MTEB Score.",
                "position": 3756
            }
        ]
    },
    {
        "header": "Appendix BReward Model Architecture Details",
        "images": []
    },
    {
        "header": "Appendix CTurkish Quality Filtering Threshold Selection",
        "images": []
    },
    {
        "header": "Appendix DData Preprocessing and Corpus Construction",
        "images": []
    },
    {
        "header": "Appendix EDecontamination Module",
        "images": []
    }
]