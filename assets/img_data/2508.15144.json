[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/x1.png",
                "caption": "",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2508.15144/x2.png",
                "caption": "Figure 1:Performance overview on mainstream GUI-automation benchmarks.",
                "position": 175
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/images/fig2.png",
                "caption": "Figure 2:Overview of our Mobile-Agent-v3. We illustrate our multi-platform environment supporting, our core capability, and some GUI automation examples generated by Mobile-Agent-v3.",
                "position": 187
            }
        ]
    },
    {
        "header": "2GUI-Owl",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/x3.png",
                "caption": "Figure 3:Illustration of the interaction flow of GUI-Owl. The system message defines the available action space, the user message contains the task instruction, compressed histories, and current observation, while the response message includes the agent’s reasoning, action summaries, and the final action output.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2508.15144/images/data_pipeline_v2.png",
                "caption": "Figure 4:Illustration of the our self-evolving trajectory data production pipeline.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2508.15144/x4.png",
                "caption": "Figure 5:Overview of our grounding data construction pipeline.",
                "position": 298
            }
        ]
    },
    {
        "header": "3Training Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/x5.png",
                "caption": "Figure 6:Overview of our scalable RL infrastructure, which unifies single-turn reasoning and multi-turn agentic training in a fully decoupled rollout–update framework. All components can run in parallel for high throughput, with diverse task-specific interactions plugged into the scalable experience maker with a unified interface. A rollout manager assigns task IDs, collects trajectories and rewards, and coordinates data flow via a shared data center.",
                "position": 529
            }
        ]
    },
    {
        "header": "4Mobile-Agent-v3",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/x6.png",
                "caption": "Figure 7:Mobile-Agent-v3 architecture. The system consists of six modules: (1) a RAG module for retrieving external world knowledge, (2) a Manager Agent for subgoal planning and guidance, (3) a Worker Agent for GUI operation, (4) a Reflector Agent for evaluation and feedback (5) a Notetaker Agent for recording important note, and (6) A GUI device interface supporting phone and PC environments.",
                "position": 635
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/x7.png",
                "caption": "Figure 8:Training dynamics of GUI-Owl-7B on OSWorld-Verified. We limit the maximum interaction steps to 15 by default.Offline Filteringremoves tasks with all-success or all-failure outcomes before applying vanilla GRPO, serving as common preprocessing.Online Filteringmoves all tasks to online training and applies DAPO for selective filtering.Experience Managingactivates both the replay buffer and the use of leftover rollouts after batch filling, as described in Section3.1.2.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2508.15144/images/osw_steps.png",
                "caption": "Figure 9:Performance of GUI-Owl-7B on OSWorld-Verified with varying numbers of historical images and interaction-step budgets.",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2508.15144/x8.png",
                "caption": "Figure 10:Effect of reasoning data synthesis on Android World.",
                "position": 2167
            }
        ]
    },
    {
        "header": "6Details of Self-Evolving Trajectory Data Production",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/images/format.png",
                "caption": "Figure 11:Format of end-to-end training data.",
                "position": 2654
            }
        ]
    },
    {
        "header": "7Details of Mobile-Agent-v3",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.15144/x9.png",
                "caption": "Figure 12:A case of a complete Mobile-Agent-3 operation process on a desktop platform. The red text represents successful reflection content.",
                "position": 3021
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]