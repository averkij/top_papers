[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23329/x1.png",
                "caption": "Figure 1:Understanding by Creating. (A)\nHumans demonstrate understanding by constructing internal world models with mental representations of object layouts, spatial relations, and physical attributes, which they can use to recreate observed scenes. In contrast, vision–language agents (VLAs) are typically evaluated on recognition tasks like captioning or VQA, which only tap into surface-level visual comprehension. (B) IR3D-Bench shifts the focus to generative reconstruction with inverse rendering via tool use. While VLAs show sparks of scene understanding, their ability to build coherent, executable 3D programs reveals how incomplete their internal world models remain.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2506.23329/x2.png",
                "caption": "Figure 2:Overview of the IR3D-Bench Pipeline.\nThe benchmark consists of two stages:Stage 1: Inverse Rendering.Given a raw image and camera parameters, the agent is prompted to infer a structured scene representation in JSON format. The predicted objects are rendered in Blender and matched to GT annotations using geometric alignment and per-object mask comparisons.Stage 2: Benchmark Evaluation.Reconstruction quality is evaluated along three axes: Localization (object count, spatial alignment, and relation consistency), Visual Appearance (shape and material accuracy via mask- and attribute-level scores), and Language-Aligned Semantics (layout fidelity and object plausibility assessed via GPT-4o).\nTogether, these metrics provide a comprehensive view of the VLA’s internal world model and generative precision.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Agentic Inverse Rendering with VLMs",
        "images": []
    },
    {
        "header": "3Related Work",
        "images": []
    },
    {
        "header": "4IR3D-Bench Suite",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23329/x3.png",
                "caption": "Figure 3:Visual Results with Selected VLMs. Gemini-2.5-pro demonstrates strong understanding of object spatial positions and relative layouts. Grok-3 excels at modeling fine-grained details such as material and color. Qwen2.5-VL-72B struggles in more complex scenarios.",
                "position": 309
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23329/x4.png",
                "caption": "Figure 4:Holistic comparison over 14 metrics.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2506.23329/x5.png",
                "caption": "Figure 5:Qualitative results illustrating the effect ofincreasing refinement iterationson performance. Starting from GPT-4o outputs, iterative refinements (#1, #5, #10) progressively improve alignment with the GT. Gemini-2.5-pro results are also shown for comparison.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2506.23329/x6.png",
                "caption": "Figure 6:Understanding scales with more refinements.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2506.23329/x7.png",
                "caption": "Figure 7:Prompt Design and Error Analysis.\n(A) Inverse rendering prompt includes structured format, fixed camera, and attribute guidelines.\n(B) Ablation of prompt components reveals distinct failure modes, highlighting the importance of precise prompt engineering.",
                "position": 866
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Setup Details",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23329/x8.png",
                "caption": "Figure 8:More Visual Results with Selected VLMs on IR3D-Bench",
                "position": 2608
            },
            {
                "img": "https://arxiv.org/html/2506.23329/x9.png",
                "caption": "Figure 9:Failure output of selected models on IR3D-Bench",
                "position": 2754
            }
        ]
    },
    {
        "header": "Appendix CFurther Analysis",
        "images": []
    }
]