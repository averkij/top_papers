[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01292/x1.png",
                "caption": "Figure 1:We propose LSceneLLM, a novel framework for adaptive large 3D scene understanding.(a) Existing methods struggle to locate\ntask-relevant visual information when facing large scenes. (b) We are committed to precisely identifying fine-grain task-related visual features through adaptive scene modeling. (c) Our method outperforms existing approaches across various benchmarks.",
                "position": 122
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01292/x2.png",
                "caption": "Figure 2:An Overview of LSceneLLM.LSceneLLM first perceives the scene through sparse vision tokens at the coarse level and then enhances regions of interest using dense vision tokens. Our method can effectively handle various visual language tasks in large scenes.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01292/x3.png",
                "caption": "Figure 3:Illustration of Adaptive Self-attention Module and Dense Vision Token Selector.We first obtain the focused regions by analyzing the attention map of LLM. Then we extract dense point cloud features from the region of interest and parse dense vision tokens through sampling and grouping operations.",
                "position": 198
            }
        ]
    },
    {
        "header": "3LSceneLLM: Adaptive Framework For Large Scene Understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01292/x4.png",
                "caption": "Figure 4:Examples of dataset XR-Scene. XR-Scene contains three cross-room scene benchmarks that comprehensively evaluate different understanding abilities.",
                "position": 223
            }
        ]
    },
    {
        "header": "4XR-Scene: Cross-Room Scene Understanding Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01292/x5.png",
                "caption": "Figure 5:Visualization of attention map of LLM.Red represents high activation values, while blue represents low activation values.",
                "position": 778
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details on Generation of XR-Scene",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01292/x6.png",
                "caption": "Figure 6:Generation pipeline of\nXR-SceneCaption and XR-EmbodiedPlanning.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2412.01292/x7.png",
                "caption": "Figure 7:More Attention Visualization of LSceneLLM.",
                "position": 1602
            }
        ]
    },
    {
        "header": "Appendix BAblation Study",
        "images": []
    },
    {
        "header": "Appendix CMore Scene Understanding Results on ScanNet",
        "images": []
    },
    {
        "header": "Appendix DMore Attention Visualization of LSceneLLM on XR-QA",
        "images": []
    },
    {
        "header": "Appendix EComputational Complexity Analysis",
        "images": []
    }
]