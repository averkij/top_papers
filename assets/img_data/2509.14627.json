[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14627/x1.png",
                "caption": "Figure 1:A conversational agent with (Top) text, (Middle) text and audio, (Bottom) text, audio, and paralinguistic signals.",
                "position": 142
            }
        ]
    },
    {
        "header": "2MSenC Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14627/x2.png",
                "caption": "Figure 2:The illustration depicts the creation process of the MultiSensory Conversation dataset.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2509.14627/x3.png",
                "caption": "Figure 3:Overview of model architecture. The multimodal utterances are composed of text, audio, and video features. Then LLM generates text response and speech description with them.",
                "position": 282
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14627/x4.png",
                "caption": "Figure 4:User study results on the MSenC test dataset.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2509.14627/x5.png",
                "caption": "Figure 5:Qualitative analysis samples evaluated on the MSenC test dataset.",
                "position": 429
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BMSenC Dataset Details",
        "images": []
    },
    {
        "header": "Appendix CInstruction Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14627/x6.png",
                "caption": "Figure 6:Illustration of speaker assignment pipeline. We obtain speech embeddings and perform clustering.",
                "position": 769
            },
            {
                "img": "https://arxiv.org/html/2509.14627/x7.png",
                "caption": "Figure 7:We report the histogram of video duration in seconds and the histogram of word count in words.",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2509.14627/x8.png",
                "caption": "Figure 8:Example of an LLM input with instructions. This sample demonstrates text input for easy understanding, though actual input includes not only text but also integrated text, audio, and video modalities.",
                "position": 775
            }
        ]
    },
    {
        "header": "Appendix DLLM Fine-Tuning",
        "images": []
    },
    {
        "header": "Appendix EAdditional Qualitative Samples",
        "images": []
    },
    {
        "header": "Appendix FDetails of Human Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14627/x9.png",
                "caption": "Figure 9:Qualitative analysis samples evaluated on the MSenC test dataset.",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2509.14627/x10.png",
                "caption": "Figure 10:Human evaluation template.",
                "position": 894
            }
        ]
    },
    {
        "header": "Appendix GLimitations",
        "images": []
    }
]