[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20822/x1.png",
                "caption": "",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20822/x2.png",
                "caption": "Figure 2:The architecture of our holistic generation pipeline, where all shot latents are processed jointly. The Window Cross-Attention provides precise directorial control by aligning each shot to its specific text prompt. The Sparse Inter-shot Self-Attention drastically reduces computational cost while preserving long-range consistency.",
                "position": 190
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20822/x3.png",
                "caption": "Figure 3:Qualitative comparison on a complex multi-shot prompt. Our method successfully generates a coherent sequence of distinct shots aligned with per-shot descriptions, while baseline methods fail in maintaining consistency, prompt fidelity, or handling shot transitions.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2510.20822/x4.png",
                "caption": "Figure 4:Qualitative comparison with state-of-the-art commercial models. While Vidu and Kling 2.5 Turbo fail to interpret multi-shot instructions and generate only a single, continuous clip,HoloCinesuccessfully executes complex shot transitions. Our method demonstrates narrative control and consistency comparable to the leading closed-source model, Sora 2, accurately rendering the sequence from medium shots to close-ups as directed by the prompt.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2510.20822/x5.png",
                "caption": "Figure 5:Qualitative results for our ablation study. We compare our full model (bottom row) against three variants. From top to bottom: removing window cross-attention prevents shot transitions; a full self-attention baseline works well but is computationally expensive; and removing inter-shot summary tokens leads to a complete loss of character consistency.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2510.20822/x6.png",
                "caption": "Figure 6:Qualitative results of our model’s emergent memory capability. (a) Character Permanence: The subject’s identity and appearance are consistently maintained across different camera angles and expressions. (b) Long-range Consistency: The subject is accurately recalled after an unrelated distractor shot. (c) Fine-grained Persistence: A non-salient background detail (the magnet, highlighted) is correctly preserved across an intervening shot.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2510.20822/x7.png",
                "caption": "Figure 7:Controllability of Cinematographic Language. Our model demonstrates high fidelity in executing specific cinematic commands. (a) Shot Scale: The model accurately generates long, medium, and close-up shots. (b) Camera Angle: It correctly interprets low-angle, eye-level, and high-angle commands to set the camera’s viewpoint. (c) Camera Movement: The model produces fluid and precise camera movements as prompted, including tilt up, dolly out, and tracking shots.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2510.20822/x8.png",
                "caption": "Figure 8:A failure case in causal reasoning. After an action (pouring water, [Shot 2]) is applied to an object (empty glass, [Shot 1]), the model fails to render its logical consequence. It incorrectly reverts to the initial empty state in [Shot 3], prioritizing visual consistency over the action’s outcome.",
                "position": 616
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetails on Evaluation metrics",
        "images": []
    }
]