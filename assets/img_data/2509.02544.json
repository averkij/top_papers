[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2UI-TARS-2",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02544/x1.png",
                "caption": "Figure 1:A demo trajectory of UI-TARS-2.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x2.png",
                "caption": "Figure 2:Browser sandbox (container) architecture.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x3.png",
                "caption": "Figure 3:We curate a Data Flywheel for UI-TARS-2, establishing a self-reinforcing loop that continuously improves both data quality and model capabilities.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x4.png",
                "caption": "Figure 4:The four-layer architecture of the interactive annotation platform.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x5.png",
                "caption": "Figure 5:The interactive annotation workflow.",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x6.png",
                "caption": "Figure 6:The multi-turn RL training infrastructure of UI-TARS-2.",
                "position": 565
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02544/x7.png",
                "caption": "Figure 7:Training reward dynamics for GUI-Browsing, GUI-General, and game scenarios in UI-TARS-2.",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x8.png",
                "caption": "Figure 8:Training entropy dynamics for GUI-Browsing, GUI-General, and game scenarios in UI-TARS-2.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x9.png",
                "caption": "Figure 9:Training dynamics of average step think length for the GUI-Browsing, GUI-General, and game scenarios in UI-TARS-2 RL training.",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x10.png",
                "caption": "Figure 10:(a) Training dynamics of average interaction round count for the GUI-Browsing and GUI-General scenarios in UI-TARS-2 RL training;\n(b) Impact of value model pretraining in GUI-Browsing scenarios.",
                "position": 1179
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x11.png",
                "caption": "Figure 11:Inference-time scaling evaluation on OSWorld and game benchmarks.",
                "position": 1202
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x12.png",
                "caption": "Figure 12:Training dynamics training reward of GUI-Browsing and GUI-General during PPO and GRPO training.",
                "position": 1222
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x13.png",
                "caption": "Figure 13:Training dynamics of training rewards for each game in the 15 Games collection.",
                "position": 1227
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x14.png",
                "caption": "Figure 14:Training dynamics (train set score and entropy) of GUI-SDK RL.",
                "position": 1232
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x15.png",
                "caption": "(a)Training reward of hybrid training versus training purely on one interface.",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x15.png",
                "caption": "",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2509.02544/x16.png",
                "caption": "(a)Training reward of hybrid training versus training purely on one interface.",
                "position": 1265
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Contributions",
        "images": []
    }
]