[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13191/x1.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13191/x2.png",
                "caption": "Figure 2:Overview of our pipeline. Given a video in its raw codec representation, our framework leverages the GOP structure for efficient, codec-aware tokenization.I-framesare processed by a standard frozen vision encoder (ϕRGB\\phi_{\\text{RGB}}) to produce dense RGB tokens.P-frames, however, bypass full RGB decoding. Their raw components, motion vectors and residuals, are instead fed into our lightweightΔ\\Delta-Encoder (ϕΔ\\phi_{\\Delta}) to generate a small set of highly compactΔ\\Delta-tokens. The final token stream, an interleaved sequence of I-frame tokens andΔ\\Delta-tokens, is consumed by the LLM, enabling dense temporal coverage at a fraction of the standard token count and runtime.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2602.13191/x3.png",
                "caption": "Figure 3:Δ\\Delta-encoderprocesses motion vectors and residuals through two lightweight branches designed to extract and compress codec-domain information. The resulting motion and residual tokens are concatenated to form theΔ\\Delta-tokens used for P-frames, providing an efficient representation, which is projected to the RGB token space during pre-training.",
                "position": 425
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13191/x4.png",
                "caption": "Table 1:Token Efficiency vs. Accuracy in Video QA.We report the performance of LLaVA-Video (7B) at different number of keyframes per GOP, as well as in the default setup of selecting 64 keyframes regardless of video length.\nFor each setting, we also report the performance of our method using the same keyframes as I-frames and the remaining frames in the GOP as P-frames.\nWe report accuracy (Acc., %) and the % of tokens used compared to the default setup (64 keyframes).\nOur method only adds a low number ofΔ\\Delta-tokens compared to its associated baseline and these help close the gap compared to the next baseline that is using a significantly larger number of tokens.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2602.13191/x5.png",
                "caption": "Figure 4:Video length vs. token budget.Theoretical scaling plot showing token efficiency across configurations.\nThe x-axis is logarithmic in token budget, and vertical dashed lines indicate evaluated budgets.\nOurΔ\\Delta-token representation enables scaling to significantly longer videos without exceeding context limits.",
                "position": 1087
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AScale of Training Data",
        "images": []
    },
    {
        "header": "Appendix BSpatial VQA",
        "images": []
    },
    {
        "header": "Appendix CComparison with Token Pruning",
        "images": []
    },
    {
        "header": "Appendix DDetails aboutΔ\\Delta-Encoder",
        "images": []
    },
    {
        "header": "Appendix EAdditional Training Details",
        "images": []
    },
    {
        "header": "Appendix FVideo Decoding Illustration",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13191/x6.png",
                "caption": "Figure 5:Codec primer.We visualize from left to right: the previous frame, the motion vectors and residuals between previous and current frame, the intermediate reconstruction after motion compensation, and the final result after adding the residuals.",
                "position": 1732
            }
        ]
    },
    {
        "header": "Appendix GAblation Study",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]