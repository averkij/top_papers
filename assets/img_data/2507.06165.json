[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06165/extracted/6606351/figure/teaser.png",
                "caption": "Figure 1.OmniPart: Generating Complex 3D Objects as Compositions of Controllable Parts.From simple 2D images (shown framed) and mask inputs, OmniPart first plans a 3D part structure using an autoregressive model, then synthesizes all high-quality, textured parts simultaneously (individual components within transparent displays). These seamlessly merge into a coherent object, offering explicit part control for enhanced editing, customization, and animation.",
                "position": 137
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06165/extracted/6606351/figure/pipeline-overview.png",
                "caption": "Figure 2.An overview of theOmniPartmodel design. OmniPart generates part-aware, controllable, and high-quality 3D content through two key stages: part structure planning and structured part latent generation.\nBuilt upon TRELLIS(Xiang etÂ al.,2024), which provides a spatially structured sparse voxel latent space, OmniPart first predicts part-level bounding boxes via an autoregressive planner.\nThen, part-specific latent codes are generated through fine-tuning of a large-scale shape model pretrained on overall objects.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2507.06165/extracted/6606351/figure/structured-part-latent-generation.png",
                "caption": "Figure 3.Spatially-conditioned part synthesis. The sparse voxels of the whole shape and each part are filled with noisy latents, which are denoised with a network composed of part-aware sparse downsample/upsample layers and transformer layers. The tokens are augmented with position embeddings and part position embeddings (PPE). The denoising process also predicts a validity score for each voxel to discard redundant voxels (the ones with stripes in the figure) in each box.",
                "position": 236
            }
        ]
    },
    {
        "header": "3.Part-aware 3D Object Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06165/x1.png",
                "caption": "Figure 4.Visualization of the training dataset. We show the distribution of part counts across the dataset (Number of parts per model vs. Frequency) and include representative examples from four different part-count ranges.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2507.06165/x2.png",
                "caption": "Figure 5.Qualitative comparison of part-aware 3D generation.\nOur method leverages TRELLIS to decode both mesh and 3D Gaussian splats, baking color onto the mesh to produce textured parts.\nHoloPart and Part123 are visualized using solid colors due to the lack of texture support.\nSegmentation-based methods (e.g., PartField) capture only surface-level masks, while Completion-based methods (e.g., HoloPart) are limited by segmentation quality.\nPartGen generates full parts but with low geometric and semantic quality.\nIn contrast, our method achieves low semantic coupling and high structural cohesion.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2507.06165/x3.png",
                "caption": "Figure 6.Applications of our part-aware 3D generation framework.\n(a)Mask-Controlled Generation: Users can specify 2D masks to guide the structure of the generated parts.\n(b)Multi-Granularity Generation: Adjusting the segmentation scale of 2D masks enables generation at different levels of part granularity.\n(c)Material Editing: Part-specific textures, such as clothing items, can be modified independently.\n(d)Geometry Processing: Our part-aware outputs support high-quality geometry processing (such as remeshing) and preserve structural coherence, avoiding artifacts at part boundaries.",
                "position": 429
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.06165/x4.png",
                "caption": "Figure 7.Qualitative results of our complete pipeline. We show the input image and 2D masks, along with the generated bounding boxes, individually generated part meshes, and the combined full-object mesh. As illustrated, our method enables precise control over part granularity via 2D masks and produces high-quality geometry and texture. The generated 3D parts exhibit low semantic entanglement and high structural cohesion, demonstrating the effectiveness of our part-aware 3D content generation.",
                "position": 720
            }
        ]
    },
    {
        "header": "5.Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]