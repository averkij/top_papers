[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12015/x1.png",
                "caption": "Figure 1:The dilemma caused by additional memory overhead during fine-tuning.(a)Users opt for a smaller 8B model, sacrificing emergent capabilities and underutilizing available hardware.(b)Use of a larger 26B model requiring memory exceeding the hardware limit even with LoRA[13]and gradient checkpointing[4]techniques.(c)Our EMLoC utilizes a smaller model during fine-tuning, allowing the same budget for both training and inference.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12015/x2.png",
                "caption": "Figure 2:Overview of EMLoC.Stage 1: Construct a downstream-aware lightweight emulator.Stage 2: Fine-tune the emulator via LoRA, allowing reduced memory costs.Stage 3: Update the LoRA module to compensate the misalignment between the full model and emulator.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2506.12015/x3.png",
                "caption": "Figure 3:LoRA correction to compensate model misalignment.To alleviate the misalignment that arises from fine-tuning the lightweight emulator but running inference on the original model, LoRA parameters are corrected via feature spaces between the emulator and the original model.",
                "position": 255
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12015/x4.png",
                "caption": "Figure 4:Sensitivity analysis ofλ𝜆\\lambdaitalic_λin the LoRA correction algorithm.We plot performance on WC-VQA under differentλ𝜆\\lambdaitalic_λvalues.",
                "position": 981
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Limitation and Future Work",
        "images": []
    },
    {
        "header": "7Social Impact",
        "images": []
    },
    {
        "header": "8Supplementary Proofs and Derivations",
        "images": []
    },
    {
        "header": "9Additional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.12015/x5.png",
                "caption": "Figure 5:We plot performance on WC-VQA under different number of calibration data.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2506.12015/x6.png",
                "caption": "Figure 6:Qualitative results of applying EMLoC to diffusion model personalization.DreamBooth with LoRA is used to personalize the 12B FLUX.1-dev diffusion model, illustrating that EMLoC can be effectively extended to generative tasks beyond text.",
                "position": 2388
            }
        ]
    },
    {
        "header": "10Additional Results",
        "images": []
    }
]