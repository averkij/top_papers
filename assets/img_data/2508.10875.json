[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10875/x1.png",
                "caption": "Figure 1:Timeline of Diffusion Language Models.\nThis figure highlights key milestones in the development of DLMs, categorized into three groups:\ncontinuous DLMs, discrete DLMs, and recent multimodal DLMs.\nWe observe that while early research predominantly focused on continuous DLMs,\ndiscrete DLMs have gained increasing popularity in more recent years.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2508.10875/x2.png",
                "caption": "Figure 2:Trend of diffusion language model papers. For discrete DLM, the statistics are drawn from papers citing D3PM[24], with a further selection of those whose titles or abstracts include the keyword “language”. For continuous DLM, the statistics are based on the number of related studies documented in the repository associated with this paper. The results reflect a growing research interest in this domain. The statistics are for reference only.",
                "position": 232
            }
        ]
    },
    {
        "header": "2Paradigms of Diffusion Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10875/x3.png",
                "caption": "Figure 4:An overview of training and inference procedures across different paradigms of Diffusion Language Models,\nwith autoregressive (AR) models included for comparison.\nAR models are trained using teacher forcing and causal attention,\nwhereas both discrete and continuous DLMs employ fully bidirectional attention mechanisms.\nBlock-wise diffusion models, exemplified by BD3-LM[76],\nintegrate autoregressive and diffusion strategies, and are trained using a specially designed block-causal attention mask.",
                "position": 1006
            }
        ]
    },
    {
        "header": "3DLMs: Pre-training and Post-training",
        "images": []
    },
    {
        "header": "4Inference Strategies",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10875/x4.png",
                "caption": "Figure 5:Inference Techniques of Diffusion Language Models. We illustrate six different strategies here, including: (a) Parallel Decoding; (b) Unmasking & Remasking; (c) Classifier-free Guidance; (d) Key-Value Cache; (e) Feature Cache; and (f) Step Distillation.",
                "position": 1553
            }
        ]
    },
    {
        "header": "5Multimodal and Unified Approaches",
        "images": []
    },
    {
        "header": "6Performance Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10875/x5.png",
                "caption": "Figure 6:Performance comparison on eight benchmarks:\nOverall-GenEval, MME, CQA, Hellaswag, PIQA, HumanEval, GSM8K, and MMMU. The horizontal axis in each subplot represents the model size, measured in the number of parameters. The vertical axis indicates the score under the corresponding benchmark, with higher scores reflecting better performance. Model types are distinguished by color: blue represents AR language models, while orange represents DLMs.",
                "position": 1829
            }
        ]
    },
    {
        "header": "7Applications on Downstream Tasks",
        "images": []
    },
    {
        "header": "8Challenges and Future Directions",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10875/x6.png",
                "caption": "Figure 7:Generation results of LLaDA[28]and MMaDA[31]under different denoising step settings.\nNote that the generation length is set to 128 tokens and 256 tokens for LLaDA and MMaDA respectively.\nBoth models generate a correct and coherent response only when 1 or 2 tokens are unmasked at each step.\nWith fewer steps and more parallelism, the responses are either incorrect or lack fluency and consistency.\nThis illustrates the trade-off between parallelism and output quality in DLMs. We omit\npart of the thinking process of MMaDA with 256 steps for simplicity.",
                "position": 2006
            }
        ]
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]