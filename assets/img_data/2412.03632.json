[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03632/x1.png",
                "caption": "Figure 1:MV-Adapter is a versatile plug-and-play adapter that turns existing pre-trained text-to-image (T2I) diffusion models to multi-view image generators.Row 1,2,3: results by integrating MV-Adapter with personalized T2I models, distilled few-step T2I models, and ControlNets(Zhang et al.,2023), demonstrating itsadaptability.Row 4,5: results under various control signals, including view-guided or geometry-guided generation with text or image inputs, showcasing itsversatility.",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03632/x2.png",
                "caption": "Figure 2:Inference pipeline.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x3.png",
                "caption": "Figure 3:Overview of MV-Adapter. Our MV-Adapter consists of two components: 1) a condition guider that encodes camera or geometry condition; 2) decoupled attention layers that contain multi-view attention for learning multi-view consistency, and optional image cross-attention to support image-conditioned generation, where we use the pre-trained U-Net to encode fine-grained information of the reference image. After training, MV-Adapter can be inserted into any personalized or distilled T2I to generate multi-view images while leveraging the specific strengths of base models.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x4.png",
                "caption": "Figure 4:Serial vs parallel architecture.",
                "position": 311
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03632/x5.png",
                "caption": "Figure 5:Results with community models and extensions. Each sample corresponds to a distinct T2I model or extension. Information about the models can be found in the Appendix.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x6.png",
                "caption": "Figure 6:Qualitative comparison on camera-guided text-to-multiview generation. our MV-Adapter achieves higher visual fidelity and image-text consistency.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x7.png",
                "caption": "Figure 7:Qualitative comparison on camera-guided image-to-multiview generation.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x8.png",
                "caption": "Figure 8:Results of geometry-guided text-to-multiview generation with community models.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x9.png",
                "caption": "Figure 9:Qualitative ablation study on the adaptability of MV-Adapter.",
                "position": 800
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x10.png",
                "caption": "Figure 10:Qualitative comparison on texture generation. We compare our text- and image-conditioned models with baseline methods.",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x11.png",
                "caption": "Figure 11:Qualitative ablation study on the attention architecture.",
                "position": 821
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03632/x12.png",
                "caption": "Figure 12:Results of multi-view LoRA (set target modules to attention layers). The azimuth angles of the images from left to right are0∘,45∘,90∘,180∘,270∘,315∘superscript0superscript45superscript90superscript180superscript270superscript3150^{\\circ},45^{\\circ},90^{\\circ},180^{\\circ},270^{\\circ},315^{\\circ}0 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 45 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 90 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 180 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 270 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 315 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT, corresponding to the front, front-left, left, back, right, and front-right of the object.",
                "position": 2158
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x13.png",
                "caption": "Figure 13:Results of multi-view LoRA (set target modules to attention layers, convolutional layers, etc.). The azimuth angles of the images from left to right are0∘,45∘,90∘,180∘,270∘,315∘superscript0superscript45superscript90superscript180superscript270superscript3150^{\\circ},45^{\\circ},90^{\\circ},180^{\\circ},270^{\\circ},315^{\\circ}0 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 45 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 90 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 180 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 270 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 315 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT, corresponding to the front, front-left, left, back, right, and front-right of the object.",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x14.png",
                "caption": "Figure 14:Results of MV-Adapter, which introduces decoupled attention mechanism rather than LoRA. The azimuth angles of the images from left to right are0∘,45∘,90∘,180∘,270∘,315∘superscript0superscript45superscript90superscript180superscript270superscript3150^{\\circ},45^{\\circ},90^{\\circ},180^{\\circ},270^{\\circ},315^{\\circ}0 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 45 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 90 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 180 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 270 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT , 315 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT, corresponding to the front, front-left, left, back, right, and front-right of the object.",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x15.png",
                "caption": "Figure 15:Results on camera-guided image-to-multiview generation with low-resolution images as input.",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x16.png",
                "caption": "Figure 16:Visualization results using MV-Adapter to generate arbitrary viewpoints.",
                "position": 2224
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x17.png",
                "caption": "Figure 17:Qualitative comparison of our MV-Adapter based on SD2.1 and SDXL.",
                "position": 2242
            },
            {
                "img": "https://arxiv.org/html/2412.03632/extracted/6045833/figure/user_study.png",
                "caption": "Figure 18:Results of user study on image-to-multi-view generation.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x18.png",
                "caption": "Figure 19:Additional results on camera-guided text-to-multiview generation with community models.",
                "position": 2311
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x19.png",
                "caption": "Figure 20:Additional results on camera-guided text-to-multiview generation with extensions.",
                "position": 2314
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x20.png",
                "caption": "Figure 21:Additional results on camera-guided image-to-multiview generation.",
                "position": 2317
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x21.png",
                "caption": "Figure 22:Visual results on text-to-3D generation.",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x22.png",
                "caption": "Figure 23:Visual results on image-to-3D generation.",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x23.png",
                "caption": "Figure 24:Additional results on geometry-guided text-to-texture generation.",
                "position": 2326
            },
            {
                "img": "https://arxiv.org/html/2412.03632/x24.png",
                "caption": "Figure 25:Additional results on geometry-guided image-to-texture generation.",
                "position": 2329
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]