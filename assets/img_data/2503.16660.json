[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/newton_preview.png",
                "caption": "Figure 1:Comparison of feature selection methods on Newton’s Principia text: original image (left), random feature selection retaining 40% of tokens (middle), and our proposed feature selector retaining 40% of tokens (right).",
                "position": 101
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Useful Feature Selection",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/S.png",
                "caption": "Figure 2:Illustration of the Feature Selector in training mode. It uses three Transformer layers and a Gumbel-Softmax head to generate a binary mask where zeros mark tokens for removal and ones for retention. During training, the masked embeddings are replaced by a shared learnable embedding. During inference, the masked embeddings are discarded, while the retained ones are used for downstream tasks, such as image representations in Vision-Language models.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/R.png",
                "caption": "Figure 3:Illustration of Feature Reconstructor’s functionality. Its primary objective is to restore the tokens that were replaced with a learned representation.",
                "position": 268
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/next_text_vertical.png",
                "caption": "Figure 4:Comparison of LLaVA-NeXT performance with our selector (orange) and random selector (blue) on text-based benchmarks. The green dashed line represents the baseline performance using all features. The red dashed line represents the model’s performance without image input.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/next_no_text_vertical.png",
                "caption": "Figure 5:Comparison of LLaVA-NeXT performance with our selector (orange) and random selector (blue) on non-text benchmarks. The green dashed line represents the baseline performance using all features. The red dashed line represents the model’s performance without image input.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/ov_text_vertical.png",
                "caption": "Figure 6:Comparison of LLaVA-OneVision performance with our trained selector (orange) and random selection (blue) on text-based (OCR-like) benchmarks. The green dashed line represents the baseline performance with all features retained. The red dashed line represents the model’s performance without image input.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/ov_no_text_vertical.png",
                "caption": "Figure 7:Comparison of LLaVA-OneVision performance with our trained selector (orange) and random selection (blue) on non-text benchmarks. The green dashed line represents the baseline performance with all features retained. The red dashed line represents the model’s performance without image input.",
                "position": 455
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16660/extracted/6297532/images/discussion_2.png",
                "caption": "Figure 8:Images from three benchmarks illustrating cases where the vision-language model gives correct answers or makes errors. The first column shows the model’s responses using the full visual context, the second column uses a randomly selected set of features, and the third column uses the features selected by our selector. (1) DocVQA: to answer the question selecting the correct features is crucial. (2) MMMU (math): to answer this question, both visual understanding and logical reasoning are important, but the model fails to reason correctly. (3) MMstar: the image details are less important, and the language model plays a dominant role.",
                "position": 490
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]