[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/teaser.png",
                "caption": "Figure 1:This work novelly enables the radiance field representations on the latent space of VAE, achieving photorealistic 3D reconstruction performance on unbounded outdoor scenes.",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/method.png",
                "caption": "Figure 2:An illustration of our pipeline for creating a latent radiance field in conjunction with 3D-aware 2D representation fine-tuning.\nFirstly in Stage-I, we inject 3D awareness into the VAEâ€™s encoder through applying a novel correspondence consistency constraint on the latent space, making the 2D representations follow the geometry consistency. Then in Stage-II, we create the latent radiance field (LRF) to represent 3D scenes based on the 3D-aware 2D representations. Finally in Stage-III, we introduce a VAE-Radiance Field alignment method to enhance the performance of image decoding from the rendered latent space.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/fft.png",
                "caption": "Figure 3:A visualization of latent spaces of original and our fine-tuned VAEs. Our method ensures an accurate geometry in the latent space while removing a certain amount of high-frequency noises.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/exp_nvs_0.png",
                "caption": "Figure 4:A visual comparison of rendering results.\nOur method can not only render high-quality images for in-distribution dataset (DL3DV-10K), but also shows great generalization ability across different datasets.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/exp_nvs_1.png",
                "caption": "",
                "position": 355
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/exp_gene_0.png",
                "caption": "Figure 5:Visual comparison of different text-to-3D generation methods. Our model enables the generation of more view-consistent results.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/exp_gene_1.png",
                "caption": "",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/ablation.png",
                "caption": "Figure 6:A qualitative study of the effect of different fine-tuning stages for view synthesis results.",
                "position": 671
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_gene_1.png",
                "caption": "Figure 7:Samples for text-to-3D generation on the image and latent space.",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_gene_0.png",
                "caption": "",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_gsgen.png",
                "caption": "Figure 8:More samples for text-to-3D generation on the image space.",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_dl3dv.png",
                "caption": "Figure 9:More NVS results on theDL3DV-10Kdataset.",
                "position": 1512
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_llff.png",
                "caption": "Figure 10:More NVS results on theNeRF-LLFFdataset.",
                "position": 1516
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_mipnerf.png",
                "caption": "Figure 11:More NVS results on theMip-NeRF360dataset.",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2502.09613/extracted/6196357/figures/appendix_mvimagenet.png",
                "caption": "Figure 12:More NVS results on theMVImgNetdataset.",
                "position": 1524
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]