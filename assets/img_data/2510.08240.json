[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08240/x1.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2510.08240/x2.png",
                "caption": "Figure 1:Overview ofWaltzRL.Left: Given a user prompt, the conversation agent produces an initial response. The feedback agent then reasons about its safety and overrefusal, produces labels, and a textual feedback. If the initial response is deemed unsafe or overrefusing according to the label, the feedback is given to the conversation agent which produces a revised response. Here, the feedback agent converts an unsafe response into a safe, balanced response to an adversarial prompt (detailed in ยงF).Right: A single training step ofWaltzRL. After collaborative rollout, we gather training samples, compute the reward separately for each agent, and train both agents in parallel.",
                "position": 174
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2WaltzRL: Training Agents for Collaborative Reasoning",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08240/x3.png",
                "caption": "Table 1:Evaluation results on safety measured by Attack Success Rate (ASR) and overrefusal measured by Over-Refuse Rate (ORR). Table (right) reports benchmark metrics across 5 datasets; scatter plot (left) visualizes the trade-off between the average ASR and ORR. Our proposed frameworkWaltzRL(Method 7, see numbering in Table)advances the Pareto front between helpfulness and harmlessness.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2510.08240/figs/reward_ablation.png",
                "caption": "Figure 2:Left: Rate of conversation agent responses thatimproveunder feedback in three setups (see (ยง3.3).Middle: Rate of conversation agent response that hasworsenedunder feedback.Right: Accuracy of feedback agent predicted(unsafe,overrefuse)(\\texttt{unsafe},\\texttt{overrefuse})labels.",
                "position": 875
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAgent Initialization and Task Format Details",
        "images": []
    },
    {
        "header": "Appendix BWaltzRLTraining Setup Details",
        "images": []
    },
    {
        "header": "Appendix CAlignment Labels Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix ELimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08240/figs/stage1_label_correct.png",
                "caption": "Figure 3:Stage 1 training dynamics.Left: Change of label correctness rate during stage 1 training.Right: Change of JSON parsing error rate during stage 1 training. The feedback agent learns the correct label and format in the first stage.",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2510.08240/figs/stage_1_format_error.png",
                "caption": "",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2510.08240/figs/stage_2_initial_reward.png",
                "caption": "Figure 4:Stage 2 training dynamics.Left: Reward of initial conversation agent responsec0c_{0}.Right: Outcome reward of the final conversation agent response.WaltzRLsuccessfully enhance the reward of both the initial response and the final outcome.",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2510.08240/figs/stage_2_outcome_reward.png",
                "caption": "",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix FQualitative examples",
        "images": []
    },
    {
        "header": "Appendix GSystem Prompts",
        "images": []
    }
]