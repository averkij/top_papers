[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23621/x1.png",
                "caption": "",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x2.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23621/x3.png",
                "caption": "Figure 1:Overall performance comparison between Table-R1 and same-scale baselines on various table reasoning benchmarks. Both Table-R1-SFT and Table-R1-Zero exhibit substantial performance improvements over baselines, showing the effectiveness of our approach across both in- and out-of-domain benchmarks.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x4.png",
                "caption": "Figure 2:An overview of our research and three research questions investigated in this study.",
                "position": 209
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Table-R1 Models",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23621/x5.png",
                "caption": "Table 2:Results on 13 table reasoning benchmarks spanning TQA, TFV, and FF-TQA tasks. For TQA, EM accuracy is reported (with ambiguous cases re-evaluated by GPT-4.1 mini); for TFV, classification accuracy; for FF-TQA, BLEU and ROUGE-L.Boldandunderlinedscores indicate the top-2 performances amongopen-sourcemodels.†: Due to the context length limitations of most previous table-specific LLMs, it is challenging to conduct a fully fair comparison. Therefore, for these models, we directly use the results as reported in their respective papers, which may be based on sampled or filtered datasets.∗: Model weight has not been released.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x5.png",
                "caption": "Figure 3:Response length during Table-R1 training across different models.",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x6.png",
                "caption": "Figure 4:Accuracy and BLEU score dynamics across four table reasoning datasets during RLVR training. Results are shown for all four Table-R1-Zero models, which are trained from Qwen2.5 7B or Llama-3.1 8B as initialization.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x7.png",
                "caption": "",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x8.png",
                "caption": "",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x9.png",
                "caption": "",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x10.png",
                "caption": "Figure 5:Pass@k performance on WTQ and HiTab.",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x11.png",
                "caption": "",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x12.png",
                "caption": "Figure 6:Illustration of the model’s reasoning progression across training steps. The example demonstrates how reasoning quality evolves from superficial processing (Step 0), to partial column-aware reasoning (Step 180), and finally to accurate multi-step inference with semantic and arithmetic understanding (Step 378).",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x13.png",
                "caption": "Table 3:Ablation study results on model-level, task-level, and formats.",
                "position": 917
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInformation of Evaluated Table Reasoning Datasets",
        "images": []
    },
    {
        "header": "Appendix BExperiment Setup",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23621/x13.png",
                "caption": "Figure 7:Qualitative Analysis Case Example for TQA tasks",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2505.23621/x14.png",
                "caption": "Figure 8:Qualitative Analysis Case Example for TFV tasks",
                "position": 2598
            }
        ]
    },
    {
        "header": "Appendix DQualitative Analysis Cases",
        "images": []
    }
]