[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04142/images/GPU_picture.png",
                "caption": "Figure 1:Annotation of the Nvidia A100 SXM 40 GB (left picture). For the chemical analysis conducted via ICP-OES, the GPU was disassembled and categorized into four main component categories: the printed circuit board (PCB) (top center), the heatsink (bottom center), the main GPU chip (top right), and the current regulators Power-on-Package (PoP) assemblies (bottom right) (author pictures).",
                "position": 120
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04142/images/periodic_table.png",
                "caption": "Figure 2:Proportion of elements in the Nvidia A100 SXM 40GB GPU (author illustration).",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2512.04142/images/elements_by_components.png",
                "caption": "Figure 3:Elemental composition of the Nvidia A100 SXM GPU by component group: heatsink, PCB, main GPU (GPU chip + VRAM), and Power-on-Packages. Displayed are the top 15 elements by weight proportion (% of total mass) (author illustration).",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2512.04142/images/GPT4_MFU35_gt1.png",
                "caption": "Figure 4:Estimated hardware and elemental requirement for training GPT-4 at 35% MFU across varying hardware lifespan scenarios (1 to 3 years). Results are expressed in terms of the total number of GPUs required and the total elemental mass (kg) on a logarithmic scale. Extending the hardware’s operational lifespan to 2 years halves the GPU demand to 2,515 GPUs, while a 3-year lifespan reduces requirements by approximately 67% to 1,676 GPUs (author’s illustration).",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2512.04142/images/benchmark_performance.png",
                "caption": "Figure 5:Illustration of the relationship between Falcon, Llama 2, GPT-3.5, and GPT-4 model performance - measured across five standard benchmarks - and the corresponding GPU requirements (on log scale) for model training (author illustration).",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2512.04142/images/combined_model_comparisons.png",
                "caption": "Figure 6:Comparison of model performance between a) GPT-3.5 and GPT-4, b) GPT-3.5 and LLaMa 2, and c) Falcon and GPT-4, across five benchmarks, alongside their respective GPU resource requirement for training (author illustration).",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2512.04142/images/gpu_mfu_lifespan.png",
                "caption": "Figure 7:GPU requirements for training GPT-4 for varying MFU scenarios and varying life spans (author illustration).",
                "position": 969
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    }
]