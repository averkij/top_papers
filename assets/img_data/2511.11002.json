[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11002/img/headimage.png",
                "caption": "Figure 1:Overview of the EmoVid dataset.The dataset spans eight emotion categories—Contentment, Awe, Amusement, Excitement, Sadness, Disgust, Fear, and Anger—and three content domains:Animation, Movie, and Sticker. The dataset captures diverse emotional expressions in various visual styles and contexts, demonstrating both multimodal richness (with associated text and audio) and cross-domain generality.",
                "position": 102
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "The EmoVid Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11002/img/V-A_Map.png",
                "caption": "Figure 2:Relationship between different emotions.We refer towarriner2013normsto arrange emotion categories on the valence-arousal model.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2511.11002/img/radar.png",
                "caption": "Figure 3:Emotion distribution across three video categories.Notably, the imbalance ofanimationandmovievideos reflects the real-world emotional landscape of these domains.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2511.11002/img/abc.png",
                "caption": "Figure 4:Video features and color-emotion correlations.(a) t-SNE visualization of video features. Animation and Movie clusters are separated, with Sticker samples overlapping both, reflecting their hybrid content characteristics.\n(b) Positive-to-total emotion ratio across bins of colorfulness and brightness, exhibiting a distinct upward trend.\n(c) Emotion transition matrix from consecutive\nmovie clips. Diagonal dominance indicates strong emotional persistence.",
                "position": 323
            }
        ]
    },
    {
        "header": "Analysis of EmoVid",
        "images": []
    },
    {
        "header": "Evaluation of EmoVid",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11002/img/qualitative_result_2.png",
                "caption": "Figure 5:Qualitative results.(a) Comparison between the original Wan2.1 I2V model and our fine-tuned one. The ✓indicates better emotional alignment.\n(b) Emotion-conditioned animated sticker generation using the fine-tuned Wan2.1 I2V model.",
                "position": 562
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Detailed Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11002/img/duration_distribution.png",
                "caption": "Figure 6:Duration distribution of EmoVid clips.Most clips cluster between roughly 2–8 seconds, and overall total duration is 140,580 seconds. Mean clip length is 6.18 seconds (min 0.18 s, max 29.98 s).",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2511.11002/img/pos_neg.png",
                "caption": "Figure 7:Caption polarity by emotion category. The results indicate good alignment between caption semantics and emotion labels.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2511.11002/img/V-A_scatter.png",
                "caption": "Figure 8:Distribution of emotion categories in color space.Each point represents an emotion category, plotted by average brightness (x-axis) and colorfulness (y-axis).",
                "position": 818
            }
        ]
    },
    {
        "header": "Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11002/img/annotation.png",
                "caption": "Figure 9:Qualitative comparison of automatic labelers on animation samples.For each clip, we show the predictions from VGG-16, ResNet-50, TinyLLaVA-Phi-2-SigLIP-3.1B, and NVILA-Lite-2B. NVILA-Lite-2B produces the most semantically coherent labels.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2511.11002/img/supp_t2v.png",
                "caption": "Figure 10:Examples of generated videos using our fine-tuned Wan2.1-T2V model.An extra LoRA module is included to generate Studio Ghibli style videos.",
                "position": 970
            }
        ]
    },
    {
        "header": "Metrics and Implementation Details",
        "images": []
    },
    {
        "header": "More Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11002/img/supp_i2v.png",
                "caption": "Figure 11:Example results of emotion-conditioned animated sticker generation using our fine-tuned Wan2.1-I2V model.We demonstrate EmoVid’s potential in efficiently adapting existing general-purpose video models for emotional content generation tailored to stylized, creative, and social media applications.",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2511.11002/img/user_study.png",
                "caption": "Figure 12:Perceptual user study results comparingWan-FinetunedwithWan-OriginalandCogVideoX.(a) Mean rank for Emotion Expression and Aesthetic Quality (lower is better).\n(b) Top-1 preference rate (higher is better).\n(c) Pairwise win rate ofWan-Finetunedagainst the two baselines.\n(d) A per-category breakdown of Top-1 preference rates, demonstrating consistent advantages forWan-Finetunedacross all eight emotions.",
                "position": 1097
            }
        ]
    },
    {
        "header": "User Study",
        "images": []
    }
]