[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10116/x1.png",
                "caption": "",
                "position": 241
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10116/x2.png",
                "caption": "Figure 2:Overview of SAGE scene generation.Our system converts open-vocabulary text prompts into simulation-ready 3D scenes by orchestrating multiple generator tools and critics. The agent dynamically calls generators (Scene Init, Asset Placer/Mover/Remover) to construct and refine layouts, while visual and physics critics provide iterative feedback for self-improvement. The visual critic suggests semantic corrections (e.g., missing or misplaced objects), and the physics critic validates stability via Isaac Sim. For example, after applying physics critic in the bottom image, the newly added pillows on the bed fall flat.\nThis self-improvement process ends when the agent considers that the generated scene meets the input user requirements.\nThe resulting scenes can be further scaled via augmentation and used for embodied policy learning.",
                "position": 420
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10116/x3.jpg",
                "caption": "Figure 3:Common and open-vocabulary scene generation comparison.Compared with baselines, SAGE produces more complete scenes with more realistic layouts on common room types, while following the style prompts more faithfully on open-vocabulary queries.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x4.jpg",
                "caption": "",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x5.jpg",
                "caption": "",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x6.jpg",
                "caption": "",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x7.jpg",
                "caption": "",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x8.jpg",
                "caption": "",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x9.jpg",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x10.jpg",
                "caption": "",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x11.jpg",
                "caption": "",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x12.jpg",
                "caption": "",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x13.jpg",
                "caption": "",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x14.jpg",
                "caption": "",
                "position": 600
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10116/x15.jpg",
                "caption": "Figure 4:Additional open-vocabulary generation.SAGE produces diverse, semantically coherent scenes spanning various styles and functionalities, from Gym and Office spaces to creative themes like “Cyberpunk game den” and “Starry-night bedroom”.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x16.jpg",
                "caption": "",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x17.jpg",
                "caption": "",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x18.jpg",
                "caption": "",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x19.jpg",
                "caption": "",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x20.jpg",
                "caption": "",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x21.jpg",
                "caption": "",
                "position": 862
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x22.jpg",
                "caption": "",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x23.png",
                "caption": "Figure 5:Stability verification.Generated scenes are loaded into IsaacSim for physical validation. Both baselines exhibit displaced objects due to instability, whereas SAGE preserves scene stability before and after simulation.",
                "position": 878
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x24.png",
                "caption": "",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2602.10116/",
                "caption": "",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x26.png",
                "caption": "",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x27.png",
                "caption": "",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x29.png",
                "caption": "Figure 6:SAGE-10k Dataset: We pre-generated a 10k-scene dataset named SAGE-10k Dataset across 50 room types and 50 styles, including 565K uniquely generated 3D objects.\nWe include the statistics of room types, room examples, and objects per scene in the figure as well.\nThe dataset can be accessed via thislink.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2602.10116/figs/multiroom/layout_52b1c66d_top_down.jpg",
                "caption": "Figure 7:Multi-room open-vocabulary generation.SAGE can be extended to generate multi-room scenes at scale easily by generating the floor plan and then calling generator MCP tools to fill in multiple rooms in parallel.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2602.10116/figs/multiroom/layout_888b215e_top_down.jpg",
                "caption": "",
                "position": 944
            },
            {
                "img": "https://arxiv.org/html/2602.10116/figs/multiroom/layout_86954971_top_down.jpg",
                "caption": "",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x30.jpg",
                "caption": "Figure 8:Image-conditioned scene generation.Using Qwen3-VL[48], SAGE extracts style and object attributes from reference images to enable image-conditioned scene generation without architectural modifications. The generated scenes are not pixel-aligned but remain semantically consistent with the reference images.",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x31.jpg",
                "caption": "",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x32.jpg",
                "caption": "",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x33.jpg",
                "caption": "",
                "position": 978
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x34.png",
                "caption": "Figure 9:Articulated Objects: SAGE can be extended with articulated objects using retrieval from PartNet-Mobility[59].Top: we show two scenes with multiple articulated objects at closed and open states.Bottom: an action sequence generated with grasp pose prediction and motion planning( Sec.3.2.2) for “pick up the bowl, place it in the drawer, and close the drawer”. Please visit website for full video.",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x35.png",
                "caption": "Figure 10:Examples and scaling curve on Pick-and-Place.Top left: diverse generation.Bottom left: example trajectory.Right: success ratew.r.t.demo/object counts. More diverse object augmentations improve policy success, narrowing the gap to the privileged agent.",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x36.png",
                "caption": "",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x37.png",
                "caption": "Figure 11:Examples and scaling curve on Mobile Manipulation.Left: task overview.Mid-top: diverse generation.Mid-bottom: example trajectory.Right: success ratew.r.t.demo/scene counts. Both baselines omit physics critic and replace text-to-3D object synthesis with retrieval:Baseline 1mimics SceneWeaver[61];Baseline 2further replaces the agent with a fixed pipeline, resembling Holodeck[63]. Diverse SAGE-augmented scenes boost thelearned policy’s success and close the gap to the privileged agent, while removing physics critic and object synthesis degrades performance (Baseline 1). Replacing the agent with a static pipeline further reduces success rate (Baseline 2).",
                "position": 1090
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x38.png",
                "caption": "",
                "position": 1096
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10116/x39.jpg",
                "caption": "Figure 1:Object Category-Level Augmentation.Here we showcase the capability of our category augmentation method. We randomly select part of the objects in the scene for category augmentation. Given the text description of the selected object from the generation stage, we employ an LLM-based text augmentation to produce variations in geometry and texture (e.g., shape, color, material, or finish) while maintaining the original object category. We then use TRELLIS[60]to synthesize corresponding 3D assets from these augmented descriptions, which are placed into the scene to enrich visual and physical diversity across instances.",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x40.jpg",
                "caption": "",
                "position": 2262
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x41.jpg",
                "caption": "",
                "position": 2269
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x42.jpg",
                "caption": "",
                "position": 2276
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x43.jpg",
                "caption": "",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x44.jpg",
                "caption": "",
                "position": 2292
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x45.jpg",
                "caption": "",
                "position": 2299
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x46.jpg",
                "caption": "",
                "position": 2306
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x47.jpg",
                "caption": "",
                "position": 2315
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x48.jpg",
                "caption": "",
                "position": 2322
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x49.jpg",
                "caption": "",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x50.jpg",
                "caption": "",
                "position": 2336
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x51.jpg",
                "caption": "",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x52.jpg",
                "caption": "",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x53.jpg",
                "caption": "",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x54.jpg",
                "caption": "",
                "position": 2366
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x55.jpg",
                "caption": "Figure 2:Scene Layout-Level Augmentation.Here we show more results of Scene Layout-Level augmentation, where the background scene, including room geometry and all task-irrelevant objects, is regenerated through the agent-driven scene generation. This process produces diverse scene layouts sharing the same task specification, enabling learning policies that generalize across spatial configurations.Bedroom:We keep the objects ofdesk,nightstand, andmug on the nightstandas the same.Livingroom:We keep the objects ofsideboard,coffeetable, andvase on the coffeetableas the same.Office:We keep the objects ofsofa,desk, andpen on the deskas the same.Meeting room:We keep the objects oftable,cabinet, andcup on the meeting tableas the same.",
                "position": 2384
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x56.jpg",
                "caption": "",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x57.jpg",
                "caption": "",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x58.jpg",
                "caption": "",
                "position": 2414
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x59.jpg",
                "caption": "",
                "position": 2423
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x60.jpg",
                "caption": "",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x61.jpg",
                "caption": "",
                "position": 2437
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x62.jpg",
                "caption": "",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x63.jpg",
                "caption": "",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x64.jpg",
                "caption": "",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x65.jpg",
                "caption": "",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x66.jpg",
                "caption": "",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x67.jpg",
                "caption": "",
                "position": 2483
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x68.jpg",
                "caption": "",
                "position": 2490
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x69.jpg",
                "caption": "",
                "position": 2497
            },
            {
                "img": "https://arxiv.org/html/2602.10116/x70.jpg",
                "caption": "",
                "position": 2504
            }
        ]
    },
    {
        "header": "Appendix AAdditional Experiment Results",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    }
]