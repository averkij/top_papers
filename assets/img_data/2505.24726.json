[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Reflect, Retry, Reward",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24726/x1.png",
                "caption": "Figure 1:Reflect, Retry, Reward MechanismThe model is first prompted to complete a task based on a user query. If the initial response is correct, the process stops. If not, the model is prompted to generate a self-reflection on how to improve. The model then retries the same task, this time with its self-reflection included, and the new answer is evaluated. If the second attempt succeeds, the model learns that it generated an effective self-reflection.",
                "position": 136
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24726/x2.png",
                "caption": "Figure 2:Better Self-ReflectionsWe observe that reflections generated by vanilla models tend to be long, confusing, and redundant, whereas GRPO fine-tuned models produce much shorter, clearer, and more generalisable reflections.",
                "position": 462
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Templates",
        "images": []
    },
    {
        "header": "Appendix BError Analysis",
        "images": []
    }
]