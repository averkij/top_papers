[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14723/x1.png",
                "caption": "Figure 1:Overview of the CodeMonkeys system.Left:We retrieve codebase context by using models to first identify relevant files and then rank them relative to each other.Middle:We generate a codebase edit and testing script using a pair of multi-turn state machines that iterate based on execution feedback. We run these state machines multiple times in parallel to generate 10 edits and tests for every issue.Right:We select between candidate edits by identifying the candidates that pass the most generated tests and asking a model to decide between these top candidates. For details about our system’s three state machines, see Figure4.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Designing a SWE-bench Solver that Scales Test-Time Compute",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14723/extracted/6154717/figures/problem_resolution_flow.png",
                "caption": "Figure 2:Measuring CodeMonkeys performance across the three subtasks we identify in Section2(context, generation, and selection). Note that modifying the approach to one subtask can influence the performance on other subtasks as well. For example, generating more candidate edits could increase coverage but make selection harder.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2501.14723/extracted/6154717/figures/context_recall.png",
                "caption": "Figure 3:Left: Measuring recall (the fraction of SWE-bench problems with context windows that contain all needed files) as we increase the context window size limit.\nWith the 128k token limit that we use for later experiments, 92.6% of instances have the correct files in context.Right: Visualizing the distribution of context compression factors across SWE-bench problems, i.e. the ratio between the cumulative token count of files scanned by the relevance model and the cumulative token count of files we include after relevance + ranking.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2501.14723/x2.png",
                "caption": "Figure 4:Details of the CodeMonkeys state machines. TheTesting State Machineiteratively generates an initial draft of a testing script based on execution feedback from running the test on the codebase before any edits are applied. TheEditing State Machinefirst generates an initial edit conditioned on the codebase context and the output test of the Testing State Machine. Then, it refines both the test and edit draft based on execution feedback from running the test before and after the edit is applied. TheSelection State Machinefirst generates a test to distinguish between the top 3 candidate edits that pass the most testing scripts. Then, based on execution feedback of running this test with all of the candidate edits and on the codebase without edits, chooses to either create a new test script to further differentiate between the edits or selects a final edit.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2501.14723/x3.png",
                "caption": "Figure 5:Measuring coverage (left) and score when using majority-voting selection (right) as we sweep over the number of serial iterations per editing state machine and the number of parallel state machines sampled per problem. Each colored curve corresponds to a different number of parallel state machines, and the dots along each curve correspond to increased numbers of sequential iterations per state machine. The first few serial iterations have a large impact on improving performance. However, past that point, different configurations with similar costs lead to similar performance, particularly for coverage.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2501.14723/x4.png",
                "caption": "Figure 6:Comparing our selection methods when applied to the candidate edits generated by the CodeMonkeys editing state machines. Our best performing selection method – the selection state machine after top-3 filtering with generated tests – recovers approximately half of the difference between the random selection floor and the oracle selection ceiling (i.e. coverage).",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2501.14723/x4.png",
                "caption": "Figure 6:Comparing our selection methods when applied to the candidate edits generated by the CodeMonkeys editing state machines. Our best performing selection method – the selection state machine after top-3 filtering with generated tests – recovers approximately half of the difference between the random selection floor and the oracle selection ceiling (i.e. coverage).",
                "position": 358
            }
        ]
    },
    {
        "header": "3Limitations and Future Work",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADeepSeek-V3 Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14723/x5.png",
                "caption": "Figure 7:Left:Comparing the impact of scaling the number of parallel samples and sequential iterations on majority voting score between Claude and DeepSeek-V3. Each line corresponds to a fixed amount of samples, with each dot on the line being a different maximum number of sequential iterations. We see that although Claude can achieve a higher overall score, DeepSeek-V3 can achieve86.8%percent86.886.8\\%86.8 %percent of the score at a fraction of the cost.Center:A more granular view of the majority voting scaling for DeepSeek-V3.Right:Coverage for DeepSeek-V3 as a function of the number of parallel samples and sequential iterations. We highlight that coverage is continuing to scale with increased inference compute.",
                "position": 1252
            }
        ]
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CLocal Compute for Relevance",
        "images": []
    }
]