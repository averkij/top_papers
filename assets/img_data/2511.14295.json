[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14295/x1.png",
                "caption": "Figure 1:Sample Questions from AraLingBench.Example items illustrating the five linguistic categories: grammar, morphology, spelling, reading comprehension, and syntax. Each question targets a distinct aspect of Arabic linguistic competence and is crafted by expert annotators to assess genuine linguistic understanding.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AraLingBench Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14295/x2.png",
                "caption": "Figure 2:Overview of AraLingBench.Category balance, difficulty distribution, question formats, and answer position frequencies. The benchmark maintains balanced coverage across linguistic categories and difficulty levels.",
                "position": 569
            }
        ]
    },
    {
        "header": "4Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.14295/x3.png",
                "caption": "Figure 3:Category-level accuracy distribution.Models perform best on Spelling and Reading Comprehension, with Syntax remaining the most difficult category.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2511.14295/x4.png",
                "caption": "Figure 4:Inter-category correlations.Grammar and Morphology show the strongest relationship (r=0.80r=0.80), while Syntax remains comparatively independent, suggesting distinct representational mechanisms.",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2511.14295/x5.png",
                "caption": "Figure 5:Cross-benchmark correlations.Pearson coefficients between AraLingBench and seven major Arabic benchmarks reveal strong alignment with language understanding tasks but weak or negative correlation with retrieval-augmented systems.",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2511.14295/x6.png",
                "caption": "Figure 6:Performance by difficulty level.Model accuracy does not decrease monotonically with annotated difficulty; Hard questions occasionally yield higher accuracy than Medium ones.",
                "position": 1232
            },
            {
                "img": "https://arxiv.org/html/2511.14295/x7.png",
                "caption": "Figure 7:Model performance heatmap across the five AraLingBench linguistic categories. Accuracy values are shown for 35 evaluated models, sorted by weighted average performance. Color intensity ranges from red (low) through yellow (moderate) to green (high).",
                "position": 1235
            },
            {
                "img": "https://arxiv.org/html/2511.14295/x8.png",
                "caption": "Figure 8:Model performance heatmap across AraLingBench difficulty levels (Easy, Medium, Hard) for 35 evaluated models, sorted by weighted average performance. Color intensity ranges from red (low accuracy) through yellow (moderate) to green (high).",
                "position": 1238
            },
            {
                "img": "https://arxiv.org/html/2511.14295/x9.png",
                "caption": "Figure 9:Difficulty-level correlations.Strong positive relationships (r>0.65r>0.65) indicate consistent model ranking despite non-monotonic accuracy patterns.",
                "position": 1257
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]