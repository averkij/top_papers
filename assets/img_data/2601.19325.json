[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19325/x1.png",
                "caption": "",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x2.png",
                "caption": "",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x3.png",
                "caption": "",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x4.png",
                "caption": "",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x5.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x6.png",
                "caption": "",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x7.png",
                "caption": "Figure 1:Innovator-VL-8B performance across different benchmarks.The first row displays the results of theInnovator-VL-8B-Instructmodel on general benchmarks, while the second and third rows present theInnovator-VL-8B-Thinkingmodel on mathematical reasoning and scientific benchmarks. The yellow dashed line represents the average score of the selected models.",
                "position": 317
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19325/x8.png",
                "caption": "Figure 2:Overall architecture of our native-resolution multi-image reasoning model.Given a text prompt and multiple images with heterogeneous resolutions,RICE-ViTencodes each image at its native size, producing variable-length visual token sequences.Patch Mergerthen aggregates the visual tokens into a compact sequence that is concatenated with text tokens and fed intoQwen-8B-baseto generate the final response with explicit reasoning.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2601.19325/x9.png",
                "caption": "Figure 3:Data distribution across different training stages.(a) Distribution of data sources within the Innovator-VL-Mid-Training-85M dataset. (b) Distribution of data sources within the Innovator-VL-Instruct-46M dataset. (c) Distribution of data sources within the Innovator-VL-RL-172K dataset.",
                "position": 398
            }
        ]
    },
    {
        "header": "3Pre-training",
        "images": []
    },
    {
        "header": "4Post-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19325/x10.png",
                "caption": "Figure 4:Data Construction Pipeline.The raw data are obtained from both synthetic generation and real-world sources. For each data modality (EM representations, images, and questions), domain experts apply modality-specific inspection and refinement strategies. Through repeated iterative optimization, a final high-quality dataset is produced.",
                "position": 502
            }
        ]
    },
    {
        "header": "5Infrastructure",
        "images": []
    },
    {
        "header": "6Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19325/x11.png",
                "caption": "Figure 5:Token efficiency comparison across vision reasoning benchmarks. (a) Average token lengths, showing that Innovator-VL-8B-Thinking generates significantly shorter reasoning chains compared to the other models. (b) Accuracy-to-token ratio, which measures the reasoning efficiency, demonstrating that Innovator-VL-8B-Thinking achieves1.4×1.4\\timesto2×2\\timeshigher accuracy-to-token ratio than MiMo-VL-7B-RL and3.9×3.9\\timesto4.3×4.3\\timeshigher than Intern-S1-mini.",
                "position": 1278
            }
        ]
    },
    {
        "header": "7On Token Efficiency of Reasoning",
        "images": []
    },
    {
        "header": "8Conclusion and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/vstar-bench-rl_0001.jpg",
                "caption": "",
                "position": 2387
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/vstar-bench-sft_0001.jpg",
                "caption": "",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/realworldqa-rl_0001-402.jpg",
                "caption": "",
                "position": 2549
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/realworldqa-sft_0001.jpg",
                "caption": "",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/mathvision_test_beta_0001.jpg",
                "caption": "",
                "position": 2688
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/mathvision_test_0001.jpg",
                "caption": "",
                "position": 3002
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/mol_qa_parser_sft.png",
                "caption": "",
                "position": 3239
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/reaction_rl.png",
                "caption": "",
                "position": 3459
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/rxnbench_vqa_en_new.png",
                "caption": "",
                "position": 3522
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/rxnbench_vqa_zh_new.png",
                "caption": "",
                "position": 3622
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/micro_img_multi_choice.png",
                "caption": "",
                "position": 3706
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/sfe-en-rl-654.png",
                "caption": "",
                "position": 3778
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/sfe-en-sft-689.png",
                "caption": "",
                "position": 3837
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/sfe-en-sft-65.png",
                "caption": "",
                "position": 3922
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/sfe-zh-rl-81.png",
                "caption": "",
                "position": 3999
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/sfe-zh-rl-613.png",
                "caption": "",
                "position": 4074
            },
            {
                "img": "https://arxiv.org/html/2601.19325/figs/Appendix-A-More-Results/Case_imgs/msearth-mcq.png",
                "caption": "",
                "position": 4149
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    }
]