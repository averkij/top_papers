[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18538/x1.png",
                "caption": "Figure 1:SMITE.Using only one or fewsegmentation referenceswith fine granularity(left),our methodlearns to segment different unseen videos respecting thesegmentation references.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18538/extracted/5950859/img/fig2_final_v4.png",
                "caption": "Figure 2:SMITE pipeline.During inference (a), we invert a given video into a noisy latent by iteratively adding noise. We then use an inflated U-Net denoiser (b) along with the trained text embedding as input to denoise the segments. A tracking module ensures that the generated segments are spatially and temporally consistent via spatio-temporal guidance. The video latentztsubscriptùëßùë°z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis updated by a tracking energy‚Ñ∞t‚Å¢r‚Å¢a‚Å¢c‚Å¢ksubscript‚Ñ∞ùë°ùëüùëéùëêùëò\\mathcal{E}_{track}caligraphic_E start_POSTSUBSCRIPT italic_t italic_r italic_a italic_c italic_k end_POSTSUBSCRIPT(c) that makes the segments temporally consistent and also a low-frequency regularizer (d)‚Ñ∞r‚Å¢e‚Å¢gsubscript‚Ñ∞ùëüùëíùëî\\mathcal{E}_{reg}caligraphic_E start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPTwhich guides the model towards better spatial consistency.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2410.18538/extracted/5950859/img/fig3_v7.png",
                "caption": "",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2410.18538/extracted/5950859/img/fig4_smite_v3.png",
                "caption": "Figure 4:Segment tracking moduleensures that segments are consistent across time. It uses co-tracker to track each point of the object‚Äôs segment (here it is nose) and then finds point correspondence of this segment (denoted by blue dots) across timesteps. When the tracked point is of a different class (e.g,. face) then it is recovered by using temporal voting. The misclassified pixel is then replaced by the average of the neighbouring pixels of adjacent frames. This results are temporally consistent segments without visible flickers.",
                "position": 224
            }
        ]
    },
    {
        "header": "5Results and Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18538/extracted/5950859/img/SMITE_50_teaser.png",
                "caption": "Figure 5:SMITE-50Dataset sample.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2410.18538/extracted/5950859/img/quali_v2.png",
                "caption": "Figure 6:Visual comparisons with other methods demonstrate that SMITE maintains better motion consistency of segments and delivers cleaner, more accurate segmentations. Both GSAM2 and Baseline-I struggle to accurately capture the horse‚Äôs mane, and GSAM2 misses one leg (Left), whereas our method yields more precise results. Additionally, both alternative techniques create artifacts around the chin (Right), while SMITE produces a cleaner segmentation.",
                "position": 340
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18538/extracted/5950859/img/fig8_smite.png",
                "caption": "Figure 7:Additional results.We visualize the generalization capability of SMITE model (trained on the reference images) in various challenging poses, shape, and even in cut-shapes.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2410.18538/x2.png",
                "caption": "Figure 8:Segmentation results in challenging scenarios .SMITE accurately segments out the objects under occlusion (‚Äùice-cream\") or camouflage (‚Äùturtle\") highlighting the robustness of our segmentation technique.",
                "position": 560
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Appendix",
        "images": []
    }
]