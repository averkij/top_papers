[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10975/figures/hf-logo_center.png",
                "caption": "",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2602.10975/figures/featurebench-logo.jpg",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10975/x1.png",
                "caption": "Figure 1:a) The agent must implement a directly callable feature based on the task description and interface definitions, either by developing from scratch or extending an existing repository. b) Our benchmark shows that even Claude Opus 4.5 achieves only a 11.0% solution rate.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3FeatureBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10975/x2.png",
                "caption": "Figure 2:Given a GitHub repository, our automated toolkit initializes the development environment via Docker. For each benchmark instance, it validates and selectsfail-to-passandpass-to-passtests. Then, the system performs dynamic tracing to capture runtime behavior and construct an object dependency graph. Leveraging this graph, the toolkit synthesizes code patches, derives corresponding pre-solved codebases, and formulates final problem statements. This pipeline has yielded 200 benchmark tasks and 3825 executable environments from 24 GitHub repositories.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10975/x3.png",
                "caption": "Table 3:Average numbers characterizing different attributes of a SWE-bench task instance, as well as our FeatureBench (L1L_{1}set).",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x3.png",
                "caption": "Figure 3:Distribution of our benchmark across 24 GitHub repositories.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x4.png",
                "caption": "Table 8:Performance comparison of tasks with different difficulty levels in FeatureBench. Models marked with†use low reasoning, and‡use medium reasoning.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x4.png",
                "caption": "Figure 4:Failure modes of the Claude Opus 4.5. Models marked with†use low reasoning, and‡use medium reasoning.",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x5.png",
                "caption": "Figure 5:The pass rate of Claude Opus 4.5 in our benchmark varies with the number of code lines and task creation time.",
                "position": 891
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Benchmark Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10975/x6.png",
                "caption": "Figure 6:Environment Configuration File part 1 of 2",
                "position": 1327
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x7.png",
                "caption": "Figure 7:Environment Configuration File part 2 of 2",
                "position": 1333
            }
        ]
    },
    {
        "header": "Appendix BDetailed Benchmarking Process",
        "images": []
    },
    {
        "header": "Appendix CAnalysis of Falures of Gemini 3 Pro model",
        "images": []
    },
    {
        "header": "Appendix DComparison with Existing Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10975/x8.png",
                "caption": "Figure 13:Unified prompt template forL1L_{1}part 2, Precautions.",
                "position": 4843
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x9.png",
                "caption": "Figure 15:User prompt for test-layer-norm (L1L_{1}).",
                "position": 4985
            },
            {
                "img": "https://arxiv.org/html/2602.10975/x10.png",
                "caption": "Figure 17:User prompt for test-layer-norm part 2 of 2 (L2L_{2}).",
                "position": 5123
            }
        ]
    },
    {
        "header": "Appendix EDataset Overview and Experimental Results",
        "images": []
    }
]