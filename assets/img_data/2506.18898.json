[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18898/x1.png",
                "caption": "Figure 1:Text-to-Image Generation Results, usingTar-7Band a 1024 pixel de-tokenizer.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2506.18898/x2.png",
                "caption": "Figure 2:Architecture of Tar,a multimodal LLM that unifies visual understanding and generation in an autoregressive paradigm. Refer to Sec.3.3for training and inference detail.",
                "position": 185
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18898/x3.png",
                "caption": "Figure 3:Text-Aligned Tokenizer.An input image is first encoded into continuous tokens using a SigLIP2 vision model[63], followed by Scale-Adaptive (SA) Pooling to adjust spatial resolution. These tokens are then discretized via a Text-Aligned Codebook, which is initialized from LLM embeddings. To guide training, SA Decoding reconstructs the pre-quantized tokens and is supervised by a SigLIP2 teacher model using a reconstruction loss‚Ñír‚Å¢e‚Å¢csubscript‚Ñíùëüùëíùëê\\mathcal{L}_{rec}caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c end_POSTSUBSCRIPT. The decoder and teacher are only used during training and discarded at inference. Once trained, the codebook can serve as a visual vocabulary of the LLM.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2506.18898/x4.png",
                "caption": "Figure 4:Architecture of Generative De-Tokenizer Variants.",
                "position": 281
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18898/x5.png",
                "caption": "Figure 5:Comparisons of Visual Representations on Generation and Understanding Tasks.Left: Generation performance evaluated by DPG Score[24].Right: Understanding performance measured by the harmonic mean over benchmarks[25,19,33,28].",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2506.18898/x6.png",
                "caption": "",
                "position": 1064
            },
            {
                "img": "https://arxiv.org/html/2506.18898/x7.png",
                "caption": "Figure 6:Qualitative Comparison of Different Representations (Left) and De-Tokenizers (Right).",
                "position": 1072
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix CAdditional Method",
        "images": []
    },
    {
        "header": "Appendix DDatasets",
        "images": []
    },
    {
        "header": "Appendix EAdditional Implement Details",
        "images": []
    },
    {
        "header": "Appendix FAdditional Ablation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18898/x8.png",
                "caption": "Figure 7:Emergent Compositional Generation Ability.The first row denotes subject-driven generation. The middle and bottom rows are reference-based style transfer. For each example, the left image is<image>, the right image is the generated image.",
                "position": 2006
            }
        ]
    },
    {
        "header": "Appendix GAdditional Main Results",
        "images": []
    },
    {
        "header": "Appendix HMore Visualization",
        "images": []
    },
    {
        "header": "Appendix ILimitation and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18898/x9.png",
                "caption": "Figure 8:Visual Generation Results.We use Tar-7B and 1024px AR-DTok to generate these images. Most prompts are from the web, and a few prompts are from[48,74]. Zoom in for better view.",
                "position": 2032
            }
        ]
    },
    {
        "header": "Appendix JSocietal Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]