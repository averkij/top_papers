[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03232/x1.png",
                "caption": "Figure 1:Overview of the proposed two-stageLEAMLframework for OOD VQA adaptation. InPseudo QA Generation, the QA Generator is trained using a small set of labeled question-answer pairs and then used to generate pseudo QA pairs for a large collection of unlabeled images. InOOD VQA Finetuning, the VQA model is fine-tuned with both the original labeled data and the produced pseudo QA pairs of unlabeled data, enabling label-efficient adaptation to out-of-distribution visual-question answering. We will detail the learning of our QA Generator in Figure2.",
                "position": 148
            }
        ]
    },
    {
        "header": "Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03232/x2.png",
                "caption": "Figure 2:Illustration of ourSelective Neuron Distillationfor the QA Generator. The QA-relevant parameters are first selected based on gradient scores from labeled QA data. During training, only these selected parameters are updated using auxiliary caption supervision from unlabeled images, allowing QA-related knowledge distillation for the QA Generator.",
                "position": 171
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03232/x3.png",
                "caption": "Figure 3:Qualitative results on the Kvasir-VQA dataset.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x4.png",
                "caption": "Figure 4:Qualitative results on the SPORTU dataset.",
                "position": 532
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03232/x5.png",
                "caption": "Figure 5:Qualitative results on the Kvasir-VQA dataset.",
                "position": 1028
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x6.png",
                "caption": "Figure 6:Qualitative results on the SPORTU dataset.",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x7.png",
                "caption": "Figure 7:Qualitative results of generated pseudo-QA on the Kvasir-VQA dataset.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x8.png",
                "caption": "Figure 8:Qualitative results of generated pseudo-QA on the SPORTU dataset.",
                "position": 1043
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x9.png",
                "caption": "Figure 9:Qualitative results of generated caption on the SPORTU dataset.",
                "position": 1046
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x10.png",
                "caption": "Figure 10:Qualitative results of generated caption on the Kvasir-VQA dataset.",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x11.png",
                "caption": "Figure 11:Qualitative results on the Kvasir-VQA dataset.",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2510.03232/x12.png",
                "caption": "Figure 12:Qualitative results on the SPORTU dataset.",
                "position": 1064
            }
        ]
    },
    {
        "header": "Appendix AAdditional Visualization",
        "images": []
    }
]