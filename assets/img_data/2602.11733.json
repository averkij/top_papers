[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11733/imgs/main-page.png",
                "caption": "Figure 1:Output of our E-commerce Adapted VLMs compared against same size LLaVA-OneVision.We show our models ability to more faithfully extract attributes from e-commerce items. Inred, we highlight wrong model predictions that are neither tied to the image nor valid item attributes.",
                "position": 167
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11733/imgs/verification_ebay2.png",
                "caption": "Figure 2:Visual Verification Pipeline.The figure shows the pipeline we use to create the 4M e-commerce visual instruction tuning data. We begin by collecting raw listings data from the web (left). We then clean and pre-process the textual entries. In parallel, we create detailedcaptionsfor the corresponding image through InternVL-2.5-26B. Finally, we provide thecaptionstogether with thecleaned listingsto Mistral-Small-3-24B to obtain theverifiedinstructions, used, along with original images, to train our models (shown with fire).",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2602.11733/imgs/ebay-si-deanon-1.png",
                "caption": "Figure 3:eBay Single-Image Visual Instruction Tuning Set.We break down the components of our internal single-image instruction tuning set. The pie chart on the left shows the percentages of tasks in our set. On the right we breakdown each tasks with its own sub tasks with the total number of instructions in parenthesis.",
                "position": 297
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11733/imgs/benchmarks12.png",
                "caption": "Figure 4:Benchmark examples from Aspect Prediction and Deep Fashion Understanding.We choose a representative example from our Aspect Prediction and Deep Fashion Understanding benchmarks to showcase the tasks in detail.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2602.11733/imgs/benchmarks3.png",
                "caption": "Figure 5:Benchmark example from Dynamic Attribute Extraction.We choose a representative example from our Dynamic Attribute Extraction benchmark to showcase the task in detail.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2602.11733/imgs/benchmark_itemintelligence.png",
                "caption": "Figure 6:Benchmark example from Multi-Image Item Intelligence.We choose a representative example from our Dynamic Attribute Extraction benchmark to showcase the task in detail.",
                "position": 1840
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]