[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x1.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x2.png",
                "caption": "Figure 2:A data point in ourTIP-I2Vincludes UUID, timestamp, text and image prompt, subject, NSFW status of text and image, text and image embedding, and the corresponding generated videos.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Curating TIP-I2V",
        "images": []
    },
    {
        "header": "4Comparing TIP-I2V with Similar Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x3.png",
                "caption": "Figure 3:OurTIP-I2V(image-to-video) differs from popular VidProM (text-to-video) and DiffusionDB (text-to-image) in terms ofsemantics.Top:Example prompts from the three datasets.Bottom:TheWizMap[65]visualization of ourTIP-I2Vcompared toVidProM/DiffusionDB. Please\\faSearchzoom in to see the detailed semantic focus of text prompts across the three datasets.",
                "position": 408
            }
        ]
    },
    {
        "header": "5New Research based on TIP-I2V",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x4.png",
                "caption": "Figure 4:The top25252525subjects(top) anddirections(bottom) preferred by users when generating videos from images.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2411.04709/x5.png",
                "caption": "Figure 5:The ratio of the sum of the topNğ‘Nitalic_Nsubjects(top) ordirections(bottom) to the total frequencies.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2411.04709/x6.png",
                "caption": "Figure 6:Benchmarking results using10,0001000010,00010 , 000prompts in TIP-Eval and 10 dimensions from[25,49,18]. Similar to VBench[25], results are normalized per dimension for clearer comparisons.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2411.04709/x7.png",
                "caption": "Figure 7:A case illustrating themisuseof image-to-video models, resulting inmisinformation: given a friendly image ofğ™´ğš•ğš˜ğš—ğ™´ğš•ğš˜ğš—\\mathtt{Elon}typewriter_Elonğ™¼ğšğšœğš”ğ™¼ğšğšœğš”\\mathtt{Musk}typewriter_Muskandğ™³ğš˜ğš—ğšŠğš•ğšğ™³ğš˜ğš—ğšŠğš•ğš\\mathtt{Donald}typewriter_Donaldğšƒğš›ğšğš–ğš™ğšƒğš›ğšğš–ğš™\\mathtt{Trump}typewriter_Trumpshaking hands, an image-to-video model can easily generate a video of them fighting, which fuelspolitical rumors.",
                "position": 609
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Comparing TIP-I2V with Panda-70M",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x8.png",
                "caption": "Figure 8:TheWizMap[65]visualization of ourTIP-I2Vcompared toPanda-70M[12]. Please\\faSearchzoom in to see the details.",
                "position": 3897
            }
        ]
    },
    {
        "header": "8Exact Words from Pikaâ€™s Terms of Service",
        "images": []
    },
    {
        "header": "9Details of Adopted Image-to-Video Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x9.png",
                "caption": "Figure 9:An extension of Fig.4: the top50505050subjects(top) anddirections(bottom) preferred by users when generating videos from images.",
                "position": 4125
            }
        ]
    },
    {
        "header": "10Details of Calculating User Preference",
        "images": []
    },
    {
        "header": "11Examples for Top Subjects and Directions",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x10.png",
                "caption": "Figure 10:For each top-rankedsubject, we select one text and one image prompt as examples for illustration.",
                "position": 4147
            },
            {
                "img": "https://arxiv.org/html/2411.04709/x11.png",
                "caption": "Figure 11:For each top-rankeddirection, we select one text and one image prompt as examples for illustration.",
                "position": 4150
            }
        ]
    },
    {
        "header": "12Full Experiments for Benchmarking",
        "images": []
    },
    {
        "header": "13Details of TIP-ID Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04709/x12.png",
                "caption": "Figure 12:The illustration of theTIP-Trace, which is designed to train a model to identify the source image of any given generated frame.",
                "position": 4575
            }
        ]
    },
    {
        "header": "14Details of Fine-tuning VideoMAE",
        "images": []
    },
    {
        "header": "15Details of TIP-Trace Dataset",
        "images": []
    },
    {
        "header": "16Details of Deep Metric Learning Baseline",
        "images": []
    },
    {
        "header": "17Potential Social Impact",
        "images": []
    }
]