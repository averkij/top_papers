[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01934/extracted/6328341/figures/logo_v1.png",
                "caption": "",
                "position": 47
            },
            {
                "img": "https://arxiv.org/html/2504.01934/x1.png",
                "caption": "Figure 1:ILLUME+Â can understand and generate images at any resolution. Compared to our previous work, ILLUMEwang2024illume, it demonstrates improved texture preservation in image editing tasks.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01934/x2.png",
                "caption": "Figure 2:Characteristics comparison among existing unified models. Existing methods explore distinct paradigms to balance visual understanding, generation, and editing capabilities. Early approaches using VQGAN discretization struggle in understanding and context-aware generation tasks due to limited semantic alignment. Later frameworks incorporate semantic encoders, achieving better alignment but compromising texture preservation essential for fine-grained editing. ILLUME+ deep-integrates image understanding, generation, and editing into a single, unified architecture, enabling more intelligent and flexible interactions and task execution.",
                "position": 91
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01934/x3.png",
                "caption": "Figure 3:Architecture of ILLUME+.(a) The dual vision tokenizer preserves both semantic and texture information.\n(b) The diffusion refiner decodes discrete tokens into high-quality images.\n(c) The unified MLLM enables deep semantic interactions and context-aware image generation.\n(d) We introduce an unambiguous image representation of discrete tokens in a chain-of-thought pattern (semantic tokens first, followed by pixel tokens), resulting in improved generation performance.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2504.01934/x4.png",
                "caption": "Figure 4:Illustration of our progressive training pipeline. We first pre-train the dual-tokenizer system by reconstruction of the semantic and pixel information. We then fine-tune the diffusion model as a high-quality image decoder. The MLLM training consists of three main stages that gradually increase task resolution and complexity.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2504.01934/x5.png",
                "caption": "Figure 5:Summary of the data mixture in each stage. Our training data gradually covers a wide range of tasks and various image resoluton.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.01934/x6.png",
                "caption": "Figure 6:More visualizations on understanding tasks.",
                "position": 1307
            },
            {
                "img": "https://arxiv.org/html/2504.01934/x7.png",
                "caption": "Figure 7:More visualizations on generation tasks.",
                "position": 1310
            },
            {
                "img": "https://arxiv.org/html/2504.01934/x8.png",
                "caption": "Figure 8:More visualizations on editing tasks.",
                "position": 1313
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]