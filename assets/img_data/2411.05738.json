[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05738/extracted/5987103/images/teaser.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05738/x1.png",
                "caption": "Figure 2:The overview of our StdGENÂ pipeline. Starting from a single reference image, our method utilizes diffusion models to generate multi-view RGB and normal maps, followed by S-LRM to obtain the color/density and semantic field for 3D reconstruction. Semantic decomposition and part-wise refinement are then applied to produce the final result.",
                "position": 128
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05738/x2.png",
                "caption": "Figure 3:Demonstration of the structure and intermediate outputs of our semantic-aware large reconstruction model (S-LRM).",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2411.05738/x3.png",
                "caption": "Figure 4:Our semantic-equivalent NeRF and SDF extraction scheme (shown in yellow color).",
                "position": 248
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05738/x4.png",
                "caption": "Figure 5:Qualitative comparisons on geometry and appearance of generated 3D characters.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2411.05738/x5.png",
                "caption": "Figure 6:Decomposed outputs of our method, presented in texture, mesh, and cross-section.",
                "position": 1179
            },
            {
                "img": "https://arxiv.org/html/2411.05738/x6.png",
                "caption": "Figure 7:Ablation study on character decomposition.",
                "position": 1271
            },
            {
                "img": "https://arxiv.org/html/2411.05738/x7.png",
                "caption": "Figure 8:Ablation study on multi-layer refinement. Zoom in for better details.",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2411.05738/extracted/5987103/images/anim.png",
                "caption": "Figure 9:Rigging and animation comparisons on 3D character generation. Our method demonstrates superior performance in human perception and physical characteristics.",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2411.05738/x8.png",
                "caption": "Figure 10:Our pipeline enables diverse 3D editing using only text prompts and masks, leveraging in-painting diffusion models in the 2D domain.",
                "position": 1296
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]