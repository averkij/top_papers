[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13309/figures/webresearcher.jpg",
                "caption": "",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2509.13309/figures/tongyi.jpg",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2509.13309/x1.png",
                "caption": "Figure 1:Performance comparison between WebResearcher and state-of-the-art deep-research agents.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2509.13309/x1.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2509.13309/x2.png",
                "caption": "",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2IterResearch: An Iterative Deep-Research Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13309/x3.png",
                "caption": "Figure 2:An illustration of ourIterative Deep-Research Paradigmin contrast to the prevalentMono-contextual Paradigm.(Top)The mono-contextual approach linearly accumulates all information into a single, ever-expanding context, leading to cognitive suffocation and noise contamination.(Bottom)Our IterResearch paradigm deconstructs research into discrete rounds. In each roundii, the agent operates on a lean, reconstructedWorkspace. It firstThinks, thensynthesizesnew findings into an evolving summaryReporti, and finally decides on anActioni.\nThe crucial step is the reconstruction: theWorkspacefor the next round is rebuilt using only the essential outputs of the previous round (the updatedReportandTool Response), thus preventing context bloat and enabling sustained reasoning.",
                "position": 196
            }
        ]
    },
    {
        "header": "3A Scalable Data Engine for Advancing Agentic Intelligence",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13309/x4.png",
                "caption": "Figure 3:Overview of the three-stage data synthesis workflow, powered by a multi-agent system. The process begins with seed data generation from a curated corpus. It then enters an iterative loop where tool-augmented agents systematically increase task complexity. The workflow concludes with a multi-stage quality control process to calibrate difficulty and ensure factual correctness.",
                "position": 269
            }
        ]
    },
    {
        "header": "4Training and Test-Time Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13309/x5.png",
                "caption": "Figure 4:Illustration of Reason-Synthesis Framework",
                "position": 423
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.13309/x6.png",
                "caption": "(a)Effect of n on HLE",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2509.13309/x6.png",
                "caption": "(a)Effect of n on HLE",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2509.13309/x7.png",
                "caption": "(b)Effect of n on BrowseComp",
                "position": 970
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]