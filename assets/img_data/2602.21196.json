[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21196/x1.png",
                "caption": "Figure 1:Comparison of context parallelism approaches on long-sequence training for Llama 3-8B using 8×\\timesH100s. UPipe provides maximum efficiency, resulting in longer maximum context length (5M tokens) while retaining throughput.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2602.21196/x2.png",
                "caption": "Figure 2:Memory usage breakdown when training Llama3-8B with a sequence length of 3M tokens across 8 H100 GPUs.\nAC stands for Activation Checkpointing, AO denotes AC with offloading, OOM stands for Out of Memory.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21196/x3.png",
                "caption": "Figure 3:Illustration of (a) DeepSpeed-Ulysses and (b) UPipe designs. UPipe processes attention in a headwiseuntiedmanner, so that in each stage, attention is performed only on a subset of heads.\nThis allows memory reuse across different stages, significantly reducing the peak memory usage due to attention activations.\nHBM (High-Bandwidth Memory) usage illustrates memory utilization due to the intermediate buffers and omits other components for brevity.",
                "position": 407
            }
        ]
    },
    {
        "header": "3Untied Ulysses",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21196/x4.png",
                "caption": "Figure 4:Illustration of UPipe’s GQA scheduling algorithm. We communicate as many unique key/value heads as possible along with the corresponding queries in stage-0. In the subsequent stages, we only communicate the next queries of the corresponding groups, reusing the key/value tensors from stage-0 until stage-GG, whereGGis the group size.",
                "position": 462
            }
        ]
    },
    {
        "header": "4Implementation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21196/x5.png",
                "caption": "Figure 5:Llama3-8B:Peak GPU memory usage and throughput (normalized w.r.t USP-Hybrid) comparison of UPipe and USP-Hybrid at different sequence lengths on 16×\\timesH100s. Our method significantly outperforms USP-Hybrid in terms of memory efficiency, and allows a maximum context size of8M, improving upon USP-Hybrid (6M tokens) by33%, while maintaining throughput.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2602.21196/x6.png",
                "caption": "Figure 6:Llama3-8B:Ablation analysis of UPipe’s head-chunk sizeUU, with 512K context size onC=C=4 GPUs. Smaller chunk size yields better memory efficiency at the cost of throughput and vice versa, allowing a trade-off between memory and throughput.",
                "position": 787
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]