[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x1.png",
                "caption": "Figure 1:(a) Traditional textual DPO overlooks multimodal information, limiting video-language tasks. (b) Existing multimodal DPO methods rely on coarse alignment, missing rich temporal and perceptual details. (c&d)VistaDPOovercomes these limitations with a hierarchical spatiotemporal preference optimization framework, enabling fine-grained video-language alignment and precise reasoning over video dynamics. Here,ywsubscriptùë¶ùë§y_{w}italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPTis the preferred response overylsubscriptùë¶ùëôy_{l}italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, andvwsubscriptùë£ùë§v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPTthe visual input more likely to produce it thanvlsubscriptùë£ùëôv_{l}italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x2.png",
                "caption": "Figure 2:(a) The metadata of VistaDPO-7k highlights its focus on fine-grained video-language tasks, emphasizing temporal (44%percent4444\\%44 %) and perceptual (56%percent5656\\%56 %) reasoning.yli‚Å¢rsuperscriptsubscriptùë¶ùëôùëñùëüy_{l}^{ir}italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_r end_POSTSUPERSCRIPTandylr‚Å¢esuperscriptsubscriptùë¶ùëôùëüùëíy_{l}^{re}italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r italic_e end_POSTSUPERSCRIPTdenote the irrelevant and relevant non-preferred responses respectively. (b) VistaDPO introduces a hierarchical spatiotemporal preference optimization framework. Instance (vvsuperscriptùë£ùë£v^{v}italic_v start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT) and perceptive (vfsuperscriptùë£ùëìv^{f}italic_v start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT) levels align global-to-local semantics with spatial visual features, leveraging both text-relevant and irrelevant rejected responses for robust cross-modal interaction. Temporal (vcsuperscriptùë£ùëêv^{c}italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT) level aligns clip-level semantics with temporal dynamics, enabling precise reasoning across spatial and temporal dimensions.",
                "position": 303
            }
        ]
    },
    {
        "header": "4VistaDPO-7k: A Spatial-temporal Grounded Video DPO Dataset",
        "images": []
    },
    {
        "header": "5Methodology",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x3.png",
                "caption": "Figure 3:Ablation study of hyperparameters on EventHallusion.",
                "position": 978
            },
            {
                "img": "https://arxiv.org/html/2504.13122/x4.png",
                "caption": "Figure 4:T-SNE visualization of representation.¬†\n(a) Video-LLaVA shows substantial overlap between hallucinated (orange) and non-hallucinated (green) representations.\n(b) With Hound-DPO, there is no distinct improvement in the separation of the two clusters.\n(c) With VistaDPO, the representations achieve clear clustering, highlighting its superior discriminative capability.",
                "position": 981
            }
        ]
    },
    {
        "header": "7Analyses and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x5.png",
                "caption": "Figure 5:Ablation study of visual non-preferred samples on two video hallucination benchmarks.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2504.13122/x6.png",
                "caption": "Figure 6:Adversarial temporal testing on VideoHallucer. The gray regions indicate the performance drop under adversarial scenarios for each method.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2504.13122/x7.png",
                "caption": "Figure 7:Kernel Density Estimation (KDE) of log-likelihood differences in adversarial masking experiments. The log-likelihood difference measures the separation between original and adversarial distributions, with the shift representing the mean difference. Larger shifts indicate greater model robustness.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2504.13122/x8.png",
                "caption": "Figure 8:Case Studies of Adversarial Testing for VistaDPO: We conduct case studies from three perspectives: (a) Temporal adversarial testing, which examines whether the model can infer the correct sequence of events by introducing reversed temporal order through video playback. (b) Spatial adversarial testing, which evaluates the model‚Äôs ability to understand subject-object interactions by masking frames or pixels related to the target object. (c) Token adversarial testing, which tests the model‚Äôs sensitivity to subtle linguistic differences by introducing similar action descriptions (e.g., contrasting ‚Äúrun‚Äù with ‚Äústand‚Äù and ‚Äúwalk‚Äù). Each test compares VistaDPO with baselines (i.e., Video-LLaVA and Hound-DPO) and corresponding ablated versions to assess the impact of key components.",
                "position": 1059
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix BMore Details of Data Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x9.png",
                "caption": "Figure 9:Illustration of dataset pipeline for constructing augmented video-language QA pairs. (a) Original QA pairs are extracted from existing prevalent datasets, providing basic QA pairs. (b) These pairs are augmented by introducing chosen and rejected answers, where rejected answers include both irrelevant responses (e.g., ‚Äùshopping cart‚Äù) and relevant but incorrect ones (e.g., ‚Äùtable‚Äù). (c) To enhance spatiotemporal understanding, manual annotations are added, specifying object appearances, spatial coordinates (e.g., bounding boxes), and temporal dynamics (e.g., appearance and disappearance timestamps). This pipeline ensures richer, more nuanced data for hierarchical preference optimization in video-language tasks.",
                "position": 2550
            }
        ]
    },
    {
        "header": "Appendix CMore Discussions on Related Work",
        "images": []
    },
    {
        "header": "Appendix DExtended Details of Methodology: Formulas and Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x10.png",
                "caption": "Figure 10:A prompt template designed for generating hallucinated responses in multimodal models is presented. The template transforms original video QA pairs into a ‚Äùchosen response‚Äù (a rephrased correct answer) and two ‚Äùrejected responses‚Äù (one contextually relevant but incorrect, and one entirely unrelated). This framework supports preference optimization by providing plausible yet inaccurate alternatives for training and evaluation. An example illustrates the process, highlighting the generation of both coherent and unrelated hallucinated responses.",
                "position": 2935
            }
        ]
    },
    {
        "header": "Appendix EMore Comparison on MVBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13122/x11.png",
                "caption": "Figure 11:Cases of VistaDPO in video understanding.",
                "position": 3254
            },
            {
                "img": "https://arxiv.org/html/2504.13122/x12.png",
                "caption": "Figure 12:Temporal data samples of VistaDPO-7K.",
                "position": 3260
            },
            {
                "img": "https://arxiv.org/html/2504.13122/x13.png",
                "caption": "Figure 13:Perception data samples of VistaDPO-7K.",
                "position": 3263
            }
        ]
    },
    {
        "header": "Appendix FExhibition Board",
        "images": []
    }
]