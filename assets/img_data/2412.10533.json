[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/example.jpg",
                "caption": "Figure 1:Generated examples from the proposed method, where the frames are randomly sampled from the generated video. Our proposed method will generate videos for a specific subject contained in user-input image, in a zero-shot manner. The generated video will also meet the requirements described by user-input text.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/sugar_model.jpg",
                "caption": "Figure 2:Illustration of our model, randomly sampled frames from the generated video are shown in the figure for better illustration.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/dataset_construction.jpg",
                "caption": "Figure 3:The proposed pipeline for synthetic data generation.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/attn_mask.jpg",
                "caption": "Figure 4:Different attention designs of our proposed model. One embedding can attend to another one only when the corresponding position is marked shadow in the above illustration. For instance, in design (b) image embeddings can not attend to the first frame, but the first frame can attend to the image embeddings.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/ablation_attn.jpg",
                "caption": "Figure 5:Comparison of models with different attention designs.",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/ablation_training.jpg",
                "caption": "Figure 6:Comparison of models using different training strategies.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/ablation_vid_img.jpg",
                "caption": "Figure 7:Comparison of models trained with video customization dataset and image customization dataset.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/ablation_dual_condition.jpg",
                "caption": "Figure 8:Generated examples with different levels of guidance.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/ablation_drop.jpg",
                "caption": "Figure 9:Comparison of dropping different image embeddings.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/ablation_drop_comparison.jpg",
                "caption": "Figure 10:Generated examples of dropping different embeddings.",
                "position": 739
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/appendix_ablation_attn.jpg",
                "caption": "Figure 11:Comparison of models with different attention designs.",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/comparisons_1.jpg",
                "caption": "Figure 12:Generated examples from SUGAR and baselines.",
                "position": 1423
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/comparisons_2.jpg",
                "caption": "Figure 13:Generated examples from SUGAR and baselines.",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/comparisons_3.jpg",
                "caption": "Figure 14:Generated examples from SUGAR and baselines.",
                "position": 1429
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/comparisons_4.jpg",
                "caption": "Figure 15:Generated examples from SUGAR and baseline.",
                "position": 1432
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/comparisons_5.jpg",
                "caption": "Figure 16:Generated examples from SUGAR and baseline.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/comparisons_6.jpg",
                "caption": "Figure 17:Generated examples from SUGAR and baseline.",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/selected_subject.jpg",
                "caption": "Figure 18:Images used in quantitative evaluation.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/active_object_prompt.jpg",
                "caption": "Figure 19:Testing prompts designed for active objects.",
                "position": 1460
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/static_object_prompt.jpg",
                "caption": "Figure 20:Testing prompts designed for static objects.",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2412.10533/extracted/6068180/figures/animal_prompt.jpg",
                "caption": "Figure 21:Testing prompts designed for animals.",
                "position": 1466
            }
        ]
    },
    {
        "header": "Appendix BMore Details",
        "images": []
    }
]