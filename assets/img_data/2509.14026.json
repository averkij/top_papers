[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14026/x1.png",
                "caption": "Figure 1:Schematic overview of the proposed quantum machine learning models.(a)A quantum neural network (QNN) serves as a variational activation function (VAF), where an inputxxis processed by a quantum circuit to yield the outputœï‚Äã(x)\\phi(x).(b)Specifically, we implement VAFs within a perceptron using a single-qubit data re-uploading circuit with trainable data pre-processing weightsw‚Ñìw_{\\ell}in the‚Ñì\\ell-th encoding blockS‚Äã(w‚Ñì‚Äãx)S(w_{\\ell}x).\nThe‚Ñì\\ell-th parameterized unitary isW(‚Ñì)=W(‚Ñì)‚Äã(ùúΩ‚Ñì)W^{(\\ell)}=W^{(\\ell)}(\\bm{\\theta}_{\\ell}), whereùúΩ‚Ñì\\bm{\\theta}_{\\ell}is the set of trainable parameters inW(‚Ñì)W^{(\\ell)}.\nThe aggregated inputx=‚àëŒ±i‚Äãxix=\\sum\\alpha_{i}x_{i}is repeatedly uploaded into each data encoding block, and the measurement outcome of the parameterized circuit defines the activation function.(c)We further incorporate data re-uploading activations into the structure of Kolmogorov-Arnold networks (KANs), treating each edge‚Äôs activation as the output of a distinct quantum circuit.\nPost-activation values are summed according to a predefined pattern to yield subsequent layer outputs, ultimately resulting in a quantum-inspired Kolmogorov-Arnold network (QKAN).",
                "position": 215
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14026/x2.png",
                "caption": "Figure 2:Function fitting with noise using QKAN and KAN.The target function isf‚Äã(x)=J0‚Äã(20‚Äãx)f(x)=J_{0}(20x), fitted with noisy data.\nBoth QKAN and KAN use a shape of[1,1]\\left[1,1\\right].\nThe QKAN prediction exhibits smoother behavior compared to KAN which tends to overfit local noise features.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2509.14026/x3.png",
                "caption": "Figure 3:GPT-2 model trained on the WebText dataset(Radford et¬†al.,2019), incorporating HQKAN and MLP layers.To further reduce parameter count and improve efficiency, we adopt the hybrid QKAN (HQKAN) strategy, where the input and output dimensions fed into QKAN are compressed via fully connected layers, forming an autoencoder-like bottleneck.\nThis compression enables QKAN to operate effectively in a low-dimensional latent space with reduced overhead.\nParameter sizes are indicated in square brackets.\nHQKAN achieves better perplexity performance than MLP while using only one-third of its parameters and the same training time, demonstrating the effectiveness of QKAN-based architectures for generative modeling on classical hardware.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2509.14026/x4.png",
                "caption": "Figure 4:Knowledge distillation from QKAN to KAN.We consider the regression taskf‚Äã(x,y)=sin‚Å°(ex+y2)f(x,y)=\\sin(e^{x}+y^{2}).\nA QKAN is first trained for 500 epochs, after which its learned variational parameters are converted into B-spline coefficients and transferred into a KAN of matching architecture.\nDue to approximation errors during the conversion, a small loss shift is observed, which is corrected through continued training.\nThe resulting QKAN-initialized KAN achieves a 70% reduction in test loss compared to a KAN trained from scratch, demonstrating the effectiveness of QKAN as a pretraining strategy for classical KAN.",
                "position": 1206
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Methods",
        "images": []
    },
    {
        "header": "Data and Code Availability",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Author Contribution",
        "images": []
    },
    {
        "header": "Conflict of Interest",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Theoretical Backgrounds",
        "images": []
    },
    {
        "header": "Appendix BNumerical Results Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14026/x5.png",
                "caption": "Figure 5:Empirical noisy function fitting.For each equation, we report the average test RMSE of each model with the best depths of hidden layers.\nThe colored annotations in blue indicate the number of parameters forKANsand in red forQKANs, whence the lower the better.\nThe results show that QKANs can achieve the best test RMSE over 80% of the equations, and with much fewer parameters than KANs in most of the cases.",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2509.14026/x6.png",
                "caption": "Figure 6:Visualization of learned QKAN activations for heuristic noisy regression equations.This figure illustrates the per-node activation functions learned by QKANs, similar to the interpretability offered by KANs.\nWe display the QKAN models that achieved the lowest RMSE for a subset of representative symbolic regression equations, as listed inTableÀú1.\nThe transparency of each node is proportional to output range divided by input range; darker therefore denote stronger connections.\nEach sub-panel corresponds to a distinct equation, demonstrating that QKANs can learn smooth and structured nonlinear transformations despite the presence of noise.",
                "position": 3864
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    }
]