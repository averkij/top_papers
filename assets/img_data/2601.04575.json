[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/x1.png",
                "caption": "Figure 1:Example gameplay sequence with aligned action and text annotations. For visual clarity, we only show the frames where a text annotation is initialized, keyboard actions are simplified toWASDinputs, and mouse clicks are omitted; The highlighted key means the key is pressed, and the arrow indicates mouse movement in thexxandyydirections.",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Gameplay Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/x2.png",
                "caption": "(a)",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2601.04575/x2.png",
                "caption": "(a)",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/mask.png",
                "caption": "(b)",
                "position": 218
            }
        ]
    },
    {
        "header": "3Policy Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/images/no-data-augmentation.png",
                "caption": "(a)",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/no-data-augmentation.png",
                "caption": "(a)",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/use-data-augmentation.png",
                "caption": "(b)",
                "position": 303
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/images/model-comparisons.png",
                "caption": "(a)",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/model-comparisons.png",
                "caption": "(a)",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/text-following.png",
                "caption": "(b)",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/loss_vs_frames_loglog_fit_1200M.png",
                "caption": "Figure 5:Lowest test loss versus dataset size for the 1.2B model. As might be expected, we find the test loss fits a power-law curve closely.",
                "position": 439
            }
        ]
    },
    {
        "header": "5Causality and scaling laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/images/toy_problem_env.jpeg",
                "caption": "(a)",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/toy_problem_env.jpeg",
                "caption": "(a)",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2601.04575/x3.png",
                "caption": "(b)",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/causality_vs_frames.png",
                "caption": "Figure 7:Causality score as a function of dataset size and model size. Except in the low-data regime (30M), the causality score generally increases with larger models and larger training datasets.",
                "position": 542
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/x4.png",
                "caption": "Figure 8:Distribution of games in the annotated dataset.",
                "position": 1413
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/hovercraft.png",
                "caption": "(a)Hovercraft environment: the car is looping in a fixed racing road. We evaluate the model by measuring how much time it takes to finish one loop",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/hovercraft.png",
                "caption": "(a)Hovercraft environment: the car is looping in a fixed racing road. We evaluate the model by measuring how much time it takes to finish one loop",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/simple-fps.png",
                "caption": "(b)Simple-FPS environment: a simple fps in a static map. We evaluate the model by counting the number of hit enemies minus the hits taken.",
                "position": 1794
            }
        ]
    },
    {
        "header": "Appendix BUnlabeled dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/x5.png",
                "caption": "Figure 10:Distribution of games in the unlabeled dataset.",
                "position": 1821
            }
        ]
    },
    {
        "header": "Appendix CPolicy Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04575/images/frozen_train.png",
                "caption": "(a)Training perplexity with frozen vs. unfrozen tokenizer",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/frozen_train.png",
                "caption": "(a)Training perplexity with frozen vs. unfrozen tokenizer",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/frozen_validation.png",
                "caption": "(b)Validation perplexity with frozen vs. unfrozen tokenizer",
                "position": 1843
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/instruct-following-demo.jpg",
                "caption": "Figure 12:Selected frames from the Quake maze. Three buttons appear along the path, all of which must be pressed to open the door.",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/loss_vs_frames_loglog_fit.png",
                "caption": "Figure 13:Scaling-law curves relating test loss to the number of training frames. All four models are fitted a power-law curve between the data size and test loss, the data exhibits a strong fit to the power-law curve.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/overall_validation.png",
                "caption": "(a)100% of the dataset",
                "position": 1990
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/overall_validation.png",
                "caption": "(a)100% of the dataset",
                "position": 1993
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/half_validation.png",
                "caption": "(b)50% of the dataset",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/25per_validation.png",
                "caption": "(c)25% of the dataset",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/12per_validation.png",
                "caption": "(d)12% of the dataset",
                "position": 2009
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/6per_validation.png",
                "caption": "(e)6% of the dataset",
                "position": 2015
            },
            {
                "img": "https://arxiv.org/html/2601.04575/x6.png",
                "caption": "Figure 15:Causality scores as a function of training steps for different model and dataset sizes. Because causality scores generally increase during training, a 600M model trained on 30M samples can exhibit higher causality than the same model trained on 500M samples at intermediate checkpoints. When selecting checkpoints based on lowest test loss, however, the resulting causality trends are consistent with those reported in Section5.1.",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2601.04575/images/scaling-law/600M_pretrain.png",
                "caption": "Figure 16:Comparison of test loss between pretrained and label-only 600M models. The pretrained 600M model achieves substantially lower test loss than a model trained solely on labeled data when trained on the same number of frames.",
                "position": 2110
            }
        ]
    },
    {
        "header": "Appendix DEvaluation",
        "images": []
    }
]