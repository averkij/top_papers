[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18264/fig/logo.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2508.18264/x1.png",
                "caption": "Figure 1:MMTok achieves better performance across multiple benchmarks.",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18264/figures/mmtok.jpg",
                "caption": "Figure 2:Overview of MMTok framework.Our method optimizes two maximum coverage problems simultaneously to leverage text-vision and vision-vision similarity for vision token selections. Meanwhile, an optional lightweight agentic model can be applied to enhance the text semantics as denoted by the red-dotted line.",
                "position": 161
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18264/fig/fire.png",
                "caption": "Table 1:Performance Comparison on LLaVA-1.5-7B.More details in AppendixTable10.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2508.18264/fig/fire.png",
                "caption": "Table 2:Comparison on LLaVA-1.5 and LLaVA-NeXT.Details are in AppendixTables10,11,12and13.",
                "position": 795
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18264/fig/fire.png",
                "caption": "Table 10:Performance Comparison on LLaVA-1.5-7B.The vanilla number of visual tokens is576576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. The last column is the average value.",
                "position": 2730
            },
            {
                "img": "https://arxiv.org/html/2508.18264/fig/fire.png",
                "caption": "Table 11:Performance Comparison on LLaVA-1.5-13B.The vanilla number of visual tokens is576576. The first line of each method shows the raw benchmark accuracy, and the second line is the proportion relative to the upper limit. SEED-I represents SEED-IMG, SEED represents SEED-ALL. Following(Yang et al.,2025), Avg. is based on SEED-I instead of SEED.",
                "position": 3459
            },
            {
                "img": "https://arxiv.org/html/2508.18264/fig/fire.png",
                "caption": "Table 12:Performance Comparison on LLaVA-NeXT-7B.The vanilla number of visual tokens varies by dataset due to dynamic image processing (max 2880 for 5 images). ‘-’ means performance not available in the original paper.",
                "position": 4033
            },
            {
                "img": "https://arxiv.org/html/2508.18264/fig/fire.png",
                "caption": "Table 13:Performance Comparison on LLaVA-NeXT-13B.The vanilla upper number of visual tokens is28802880. SEED-I represents SEED-IMG.",
                "position": 4717
            }
        ]
    },
    {
        "header": "Appendix BComplete Empirical Results",
        "images": []
    }
]