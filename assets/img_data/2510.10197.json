[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10197/logo/aworld_logo.png",
                "caption": "",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/introduction.png",
                "caption": "Figure 1:Limitations of Existing Paradigms and theEnvironment TuningAdvantage.This figure contrasts three agent training approaches on a travel planning task.(Left)Supervised Fine-Tuning (SFT) on static trajectories struggles with generalization.(Center)Reinforcement Learning (RL) in a traditional environment provides only sparse, uninformative feedback.(Right)Our approach uses anaugmented environmentthat provides actionable, fine-grained feedback upon failure.",
                "position": 180
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/task.png",
                "caption": "Figure 2:An illustration of multi-turn tool-use scenarios, adapted from an official example in the BFCL V3 Blog(Patil et al.,2025a).\nAll three tracks start from the same initial user request.\nTheBase multi-turntrack (center) shows a successful execution path.\nTheMissing parametertrack (top) illustrates a scenario where the agent must handle ambiguity by asking for clarification.\nTheMissing functiontrack (bottom) shows a case where the agent needs to recognize that a required tool is unavailable.\nThese scenarios highlight the diverse reasoning capabilities our curriculum is designed to address.",
                "position": 297
            }
        ]
    },
    {
        "header": "3Environment Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/pipeline.png",
                "caption": "Figure 3:An overview ofEnvironment Tuning.Our core innovation is theEnvironment Tuningmodule, which implements a four-stage curriculum.\nIt dynamically configures the reward function, environment feedback (Standard vs. Augmented), and data split for theAgent Learning Loop.\nThis staged approach transforms ambiguous errors into actionable lessons (highlighted in theEnvironment Augmentation in Actionpanel), enabling efficient and stable learning from limited data.",
                "position": 402
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/env_ablation.png",
                "caption": "(a)Ablation study for environment augmentation.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/env_ablation.png",
                "caption": "(a)Ablation study for environment augmentation.",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/reward_ablation.png",
                "caption": "(b)Ablation study for progress reward.",
                "position": 997
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASequential Multi-turn Decision-Making Model.",
        "images": []
    },
    {
        "header": "Appendix BDetailed Progress Reward Components",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DSupplementary Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/stage1.png",
                "caption": "(a)Stage 1 Dynamics.",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/stage1.png",
                "caption": "(a)Stage 1 Dynamics.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/training_curves.png",
                "caption": "(b)Full Curriculum Dynamics.",
                "position": 2000
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/single_stage_training.png",
                "caption": "Figure 6:Training Collapse in a Single-Stage RL Setup.Training dynamics of Qwen2.5-7B-Instruct on the BFCL V3 benchmark without theEnvironment Tuningcurriculum. The plot shows the average accuracy (blue) and the gradient norm (red). While accuracy initially improves, it begins a sharp decline after approximately 70 training steps. This performance collapse coincides precisely with a rapid explosion in the gradient norm, empirically demonstrating the training instability that our curriculum is designed to prevent.",
                "position": 2017
            },
            {
                "img": "https://arxiv.org/html/2510.10197/doc/figures/kl_ablation.png",
                "caption": "Figure 7:The impact of the KL loss coefficient on training stability and performance during Stage 2. A larger coefficient (β=0.1\\beta=0.1) is crucial for maintaining high policy entropy (left), which prevents the performance collapse seen in settings with a small (β=0.001\\beta=0.001) or no KL penalty (right). This stability allows for sustained learning and higher final accuracy.",
                "position": 2030
            }
        ]
    },
    {
        "header": "Appendix EPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix FCase Study",
        "images": []
    }
]