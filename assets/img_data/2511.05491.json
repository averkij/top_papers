[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05491/x1.png",
                "caption": "Table 1:Comparison with spatial dataset.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x1.png",
                "caption": "Figure 1:Overview of our VST framework.",
                "position": 240
            }
        ]
    },
    {
        "header": "2Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05491/x2.png",
                "caption": "(a)Perception data distribution.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x2.png",
                "caption": "(a)Perception data distribution.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x3.png",
                "caption": "(b)Reasoning data distribution",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x4.png",
                "caption": "Figure 3:Data engines of VST (left) and the capabilities they enable in VST-Model (right).",
                "position": 295
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05491/x5.png",
                "caption": "Figure 4:(a) The VST model, which incorporates spatial perception and reasoning capabilities. (b) The VST-based VLA model, capable of generating action sequences through an action de-tokenizer.",
                "position": 392
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05491/x6.png",
                "caption": "Figure 5:Comparison with state-of-the-art VLMs on VSI-Bench[72].",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x6.png",
                "caption": "",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x7.png",
                "caption": "Figure 6:Comparison AP@15 on SUN RGB-D 3D object detection benchmark[54].",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x7.png",
                "caption": "",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x8.png",
                "caption": "Figure 7:Results of scaling model and data size",
                "position": 1402
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x8.png",
                "caption": "",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x9.png",
                "caption": "Figure 8:3D object detection results of scaling model and data size",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x9.png",
                "caption": "",
                "position": 1647
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7More Implementation Details",
        "images": []
    },
    {
        "header": "8More Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05491/x10.png",
                "caption": "(a)Data scaling of 3D detection data.",
                "position": 3455
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x10.png",
                "caption": "(a)Data scaling of 3D detection data.",
                "position": 3458
            },
            {
                "img": "https://arxiv.org/html/2511.05491/x11.png",
                "caption": "(b)Data scaling of depth-related data.",
                "position": 3463
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/prompting_w_bev/prompting_w_bev_1.png",
                "caption": "Table 14:An example to illustrate the prompting with BEV annotation.",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/prompting_w_bev/prompting_w_bev_2.png",
                "caption": "",
                "position": 3487
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/prompting_w_bev/prompting_w_bev_3.png",
                "caption": "",
                "position": 3488
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/depth_sort_visual_point.png",
                "caption": "Table 16:Examples of the depth-related data.",
                "position": 3967
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/depth_sort_visual_box.png",
                "caption": "",
                "position": 4008
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/depth_sort_text.png",
                "caption": "",
                "position": 4040
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/depth_select_visual.png",
                "caption": "",
                "position": 4078
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/depth_sort_text_2.png",
                "caption": "Table 17:Examples of the depth-related data.",
                "position": 4110
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/depth_select_point.png",
                "caption": "",
                "position": 4153
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/distance_visual.png",
                "caption": "Table 18:Examples of the distance-related data.",
                "position": 4185
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/depth_cases/distance_point.png",
                "caption": "",
                "position": 4226
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/3dod_cases/3dod_30_ori.png",
                "caption": "Table 19:Examples of 3D object detection data.",
                "position": 4261
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/3dod_cases/3dod_30_w_box.png",
                "caption": "",
                "position": 4303
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/3dod_cases/3dod_31_w_box.png",
                "caption": "",
                "position": 4330
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_si_cases/measurement_case_height.png",
                "caption": "Table 20:Examples of the measurement-related data.",
                "position": 4340
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_si_cases/measurement_case_size.png",
                "caption": "",
                "position": 4382
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_si_cases/scene_caption_case.jpeg",
                "caption": "Table 21:Examples of the scene caption data.",
                "position": 4439
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_scene_caption_mi.png",
                "caption": "",
                "position": 4480
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_correspondence.png",
                "caption": "Table 23:Examples of the correspondence data.",
                "position": 4883
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_object_object.png",
                "caption": "Table 24:Examples of the object-related data.",
                "position": 4928
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_object_object_2.png",
                "caption": "",
                "position": 4969
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_camera_direction.png",
                "caption": "Table 25:Examples of the camera-related data.",
                "position": 5001
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_camera_position.png",
                "caption": "",
                "position": 5043
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_camera_motion.png",
                "caption": "Table 26:Examples of the camera motion data.",
                "position": 5076
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vstp_mi_cases/case_camera_motion_rotation.png",
                "caption": "",
                "position": 5118
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vspt_video_cases/video_frame_distance.png",
                "caption": "Table 27:Examples of the object-object video data.",
                "position": 5151
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vspt_video_cases/video_frame_direction.png",
                "caption": "",
                "position": 5192
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vspt_video_cases/video_frame_counting_3.png",
                "caption": "Table 28:Examples of the video counting data.",
                "position": 5248
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/vspt_video_cases/video_frame_appearance_order_0.png",
                "caption": "Table 29:Examples of the video spatiotemporal data.",
                "position": 5314
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/spatial_reasoning/camera_translation_reasoning.png",
                "caption": "Table 32:Examples of the spatial reasoning data for camera translation.",
                "position": 6108
            },
            {
                "img": "https://arxiv.org/html/2511.05491/images/spatial_reasoning/camera_rotation_reasoning.png",
                "caption": "Table 33:Examples of the spatial reasoning data for camera rotation.",
                "position": 6156
            }
        ]
    },
    {
        "header": "9More Data Engine Details",
        "images": []
    }
]