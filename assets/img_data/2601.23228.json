[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.23228/figures/scaling_hand.jpg",
                "caption": "Figure 1:Multiagent architectures with separate weights enable specialization through end-to-end training, sidestepping catastrophic forgetting that limits single-model scaling just like mixture-of-experts (MoE) architecture.",
                "position": 188
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.23228/figures/execution_loop_hand-drawn.jpeg",
                "caption": "Figure 2:Agent execution loop with per-action coach evaluation.",
                "position": 243
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.23228/x1.png",
                "caption": "Figure 3:Behavioral metrics during MathChat training. Code Executor Tool Call % (top-left) shows the percentage of Code Executor actions that contain a tool call; the other three panels show average response length (in tokens) for each agent at each training step. Faint lines show raw data; solid lines show exponential moving average (α=1/3\\alpha=1/3). Qwen3-4B shows dramatically increased tool usage and more concise responses, while R1-Distill-Qwen-1.5B maintains stable behavior throughout training.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2601.23228/figures/dsbench_done.jpg",
                "caption": "Figure 4:DSBench three-agent pipeline. Each agent executes Python code via a shared sandbox, reading inputs from and writing outputs to a shared file workspace. The Data Engineer preprocesses raw CSV files into pickle artifacts; the Modeler trains and saves a model; the Analyst generates the final submission for evaluation.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2601.23228/figures/fig_dsbench_dynamics.png",
                "caption": "Figure 5:DSBench training dynamics over 84 steps (21 epochs). Light points show raw metrics; solid line shows EMA (α\\alpha=0.3). Dashed lines mark peak raw values. Classification metrics peak early (steps 44–48) then decline, while regression RMSE continues improving through step 84, illustrating specialization to regression tasks.",
                "position": 610
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.23228/x2.png",
                "caption": "Figure 6:Behavioral metrics for the partial information run (Qwen3-4B). From left to right, top to bottom: Code Executor tool calls, Problem Solver response length, Code Executor response length, Verifier response length, AMC accuracy, and AIME 2025 accuracy. Despite limited context, agents show similar behavioral adaptations as full-context runs.",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2601.23228/x3.png",
                "caption": "Figure 7:DSBench quality metrics over 84 training steps. Top row: success rates by task type. Middle rows: classification metrics (Accuracy, F1, ROC-AUC) in raw and fair variants. Bottom row: regression metrics (MAE, RMSE) in raw and fair variants. Green vertical line and markers indicate step 44 (peak success rate). Fair metrics penalize failures (Accuracy: 0.5, F1: 0, MAE/RMSE: 50%), capturing both quality and reliability.",
                "position": 1045
            }
        ]
    },
    {
        "header": "Appendix BMathChat Agent Prompts",
        "images": []
    },
    {
        "header": "Appendix CDSBench Agent Prompts",
        "images": []
    },
    {
        "header": "Appendix DExample Coach Evaluations",
        "images": []
    },
    {
        "header": "Appendix ETraining Algorithm Details",
        "images": []
    },
    {
        "header": "Appendix FReward Backpropagation",
        "images": []
    },
    {
        "header": "Appendix GDistributed Training Implementation",
        "images": []
    }
]