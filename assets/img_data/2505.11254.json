[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x1.png",
                "caption": "Figure 1:RULER 131K Subsets.At long context lengths, sparse attention can degrade performance by a large margin. Our simpleùö´ùö´\\mathbf{\\Delta}bold_Œîcorrection improves performance and only requires an additional 1.5% of the full quadratic attention computation.",
                "position": 63
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x2.png",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x3.png",
                "caption": "Figure 2:Comparing RULER 131K prefill attention latency and accuracy for sparse attention methods.",
                "position": 75
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x4.png",
                "caption": "Figure 3:Comparing sparse attention methods to quadratic attention. Ourùö´ùö´\\mathbf{\\Delta}bold_Œîcorrection results in outputs that are more similar to quadratic attention.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x5.png",
                "caption": "",
                "position": 165
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x6.png",
                "caption": "Figure 4:Overview ofùö´ùö´\\mathbf{\\Delta}bold_ŒîAttention.(Top)Given an arbitrary sparse attention method we calculate the difference between the sparse attention and full attention for a small subset of queries. The subset size is controlled by a hyperparameterŒ≥ùõæ\\gammaitalic_Œ≥.(Bottom)We then repeat the calculated difference for all output tokens and add the result to the full sparse attention output. The result is an approximation to the original quadratic attention.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2505.11254/",
                "caption": "Figure 5:Intuition forùö´ùö´\\mathbf{\\Delta}bold_ŒîAttention.The difference of attention outputs approximates the missing attention contribution.",
                "position": 184
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x8.png",
                "caption": "(a)Perplexity and Long Perplexity",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x9.png",
                "caption": "(a)Perplexity and Long Perplexity",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x10.png",
                "caption": "",
                "position": 1251
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x11.png",
                "caption": "(b)cos([ùêÄùö´‚Å¢ùêï]i,[ùêÄùö´‚Å¢ùêï]i+ŒΩsubscriptdelimited-[]superscriptùêÄùö´ùêïùëñsubscriptdelimited-[]superscriptùêÄùö´ùêïùëñùúà[\\mathbf{A^{\\mathbf{\\Delta}}V}]_{i},[\\mathbf{A^{\\mathbf{\\Delta}}V}]_{i+\\nu}[ bold_A start_POSTSUPERSCRIPT bold_Œî end_POSTSUPERSCRIPT bold_V ] start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , [ bold_A start_POSTSUPERSCRIPT bold_Œî end_POSTSUPERSCRIPT bold_V ] start_POSTSUBSCRIPT italic_i + italic_ŒΩ end_POSTSUBSCRIPT)",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x12.png",
                "caption": "(a)Latency vs. Flash Attention",
                "position": 1273
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x12.png",
                "caption": "(a)Latency vs. Flash Attention",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x13.png",
                "caption": "(b)Latency vs. Sparse Methods",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x14.png",
                "caption": "(c)Latency for increasingŒ≥ùõæ\\gammaitalic_Œ≥",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x15.png",
                "caption": "Figure 8:Comparing the effects ofEquation5‚Äòrecompute‚Äô andEquation6ùö´ùö´\\mathbf{\\Delta}bold_Œîon RULER 131K subsets.",
                "position": 1351
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x16.png",
                "caption": "",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x17.png",
                "caption": "Figure 9:For RULER with a context length of 131K, we look at the final 128 tokens in the attention output and the final 128 queries in the attention matrix. We compare the cosine similarity of the outputs and the rank correlation of the attention rows to quadratic attention. We find that for both measures,ùö´ùö´\\mathbf{\\Delta}bold_ŒîAttention is more similar to quadratic attention.",
                "position": 1359
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x18.png",
                "caption": "",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x19.png",
                "caption": "",
                "position": 1364
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x20.png",
                "caption": "",
                "position": 1365
            }
        ]
    },
    {
        "header": "5Discussion & Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix BBroader Impact",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DCompute Resources",
        "images": []
    },
    {
        "header": "Appendix ELatency of MInference with Delta Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x21.png",
                "caption": "Figure 10:Latency measurements for different settings ofŒ≥ùõæ\\gammaitalic_Œ≥which controls the gap size between queries and also the overall sparsity of the calculation. This figure accompanies the latency ablation for Streaming LLM in the main text,Figure7(c).",
                "position": 1896
            }
        ]
    },
    {
        "header": "Appendix FApprox Window Size Calculation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x22.png",
                "caption": "(a)Oracle top-kùëòkitalic_ksparse attention.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x23.png",
                "caption": "(a)Oracle top-kùëòkitalic_ksparse attention.",
                "position": 2118
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x24.png",
                "caption": "(b)Streaming LLM.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x25.png",
                "caption": "Figure 12:All RULER 131K subsets. This is a companion toFigure1. The CWE subset is excluded, as all models, including quadratic attention, scored 0%.",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x26.png",
                "caption": "",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x27.png",
                "caption": "",
                "position": 2136
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x28.png",
                "caption": "Figure 13:Attention output cosine similarity (compared to full attention) for Streaming LLM with our method.Figures13,14and15show the results from every layer, and are a counterpart toFigure9in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention.",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x29.png",
                "caption": "",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x30.png",
                "caption": "",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x31.png",
                "caption": "",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x32.png",
                "caption": "",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x33.png",
                "caption": "",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x34.png",
                "caption": "",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x35.png",
                "caption": "",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x36.png",
                "caption": "",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x37.png",
                "caption": "",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x38.png",
                "caption": "",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x39.png",
                "caption": "",
                "position": 2158
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x40.png",
                "caption": "",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x41.png",
                "caption": "",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x42.png",
                "caption": "",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x43.png",
                "caption": "",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x44.png",
                "caption": "Figure 14:Attention output cosine similarity (compared to full attention) for Streaming LLM with our method.Figures13,14and15show the results from every layer, and are a counterpart toFigure9in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention.",
                "position": 2168
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x45.png",
                "caption": "",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x46.png",
                "caption": "",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x47.png",
                "caption": "",
                "position": 2174
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x48.png",
                "caption": "",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x49.png",
                "caption": "",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x50.png",
                "caption": "",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x51.png",
                "caption": "",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x52.png",
                "caption": "",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x53.png",
                "caption": "",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x54.png",
                "caption": "",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x55.png",
                "caption": "",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x56.png",
                "caption": "",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x57.png",
                "caption": "",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x58.png",
                "caption": "",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x59.png",
                "caption": "",
                "position": 2192
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x60.png",
                "caption": "Figure 15:Attention output cosine similarity (compared to full attention) for Streaming LLM with our method.Figures13,14and15show the results from every layer, and are a counterpart toFigure9in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x61.png",
                "caption": "",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x62.png",
                "caption": "",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x63.png",
                "caption": "",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x64.png",
                "caption": "",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x65.png",
                "caption": "",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x66.png",
                "caption": "",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x67.png",
                "caption": "",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x68.png",
                "caption": "",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x69.png",
                "caption": "",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x70.png",
                "caption": "",
                "position": 2213
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x71.png",
                "caption": "",
                "position": 2214
            }
        ]
    },
    {
        "header": "Appendix GRestatement and proof ofLemma1",
        "images": []
    }
]