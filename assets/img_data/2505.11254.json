[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x1.png",
                "caption": "Figure 1:RULER 131K Subsets.At long context lengths, sparse attention can degrade performance by a large margin. Our simple𝚫𝚫\\mathbf{\\Delta}bold_Δcorrection improves performance and only requires an additional 1.5% of the full quadratic attention computation.",
                "position": 63
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x2.png",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x3.png",
                "caption": "Figure 2:Comparing RULER 131K prefill attention latency and accuracy for sparse attention methods.",
                "position": 75
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x4.png",
                "caption": "Figure 3:Comparing sparse attention methods to quadratic attention. Our𝚫𝚫\\mathbf{\\Delta}bold_Δcorrection results in outputs that are more similar to quadratic attention.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x5.png",
                "caption": "",
                "position": 165
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x6.png",
                "caption": "Figure 4:Overview of𝚫𝚫\\mathbf{\\Delta}bold_ΔAttention.(Top)Given an arbitrary sparse attention method we calculate the difference between the sparse attention and full attention for a small subset of queries. The subset size is controlled by a hyperparameterγ𝛾\\gammaitalic_γ.(Bottom)We then repeat the calculated difference for all output tokens and add the result to the full sparse attention output. The result is an approximation to the original quadratic attention.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2505.11254/",
                "caption": "Figure 5:Intuition for𝚫𝚫\\mathbf{\\Delta}bold_ΔAttention.The difference of attention outputs approximates the missing attention contribution.",
                "position": 184
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x8.png",
                "caption": "(a)Perplexity and Long Perplexity",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x9.png",
                "caption": "(a)Perplexity and Long Perplexity",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x10.png",
                "caption": "",
                "position": 1251
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x11.png",
                "caption": "(b)cos([𝐀𝚫⁢𝐕]i,[𝐀𝚫⁢𝐕]i+νsubscriptdelimited-[]superscript𝐀𝚫𝐕𝑖subscriptdelimited-[]superscript𝐀𝚫𝐕𝑖𝜈[\\mathbf{A^{\\mathbf{\\Delta}}V}]_{i},[\\mathbf{A^{\\mathbf{\\Delta}}V}]_{i+\\nu}[ bold_A start_POSTSUPERSCRIPT bold_Δ end_POSTSUPERSCRIPT bold_V ] start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , [ bold_A start_POSTSUPERSCRIPT bold_Δ end_POSTSUPERSCRIPT bold_V ] start_POSTSUBSCRIPT italic_i + italic_ν end_POSTSUBSCRIPT)",
                "position": 1257
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x12.png",
                "caption": "(a)Latency vs. Flash Attention",
                "position": 1273
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x12.png",
                "caption": "(a)Latency vs. Flash Attention",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x13.png",
                "caption": "(b)Latency vs. Sparse Methods",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x14.png",
                "caption": "(c)Latency for increasingγ𝛾\\gammaitalic_γ",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x15.png",
                "caption": "Figure 8:Comparing the effects ofEquation5‘recompute’ andEquation6𝚫𝚫\\mathbf{\\Delta}bold_Δon RULER 131K subsets.",
                "position": 1351
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x16.png",
                "caption": "",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x17.png",
                "caption": "Figure 9:For RULER with a context length of 131K, we look at the final 128 tokens in the attention output and the final 128 queries in the attention matrix. We compare the cosine similarity of the outputs and the rank correlation of the attention rows to quadratic attention. We find that for both measures,𝚫𝚫\\mathbf{\\Delta}bold_ΔAttention is more similar to quadratic attention.",
                "position": 1359
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x18.png",
                "caption": "",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x19.png",
                "caption": "",
                "position": 1364
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x20.png",
                "caption": "",
                "position": 1365
            }
        ]
    },
    {
        "header": "5Discussion & Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix BBroader Impact",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DCompute Resources",
        "images": []
    },
    {
        "header": "Appendix ELatency of MInference with Delta Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x21.png",
                "caption": "Figure 10:Latency measurements for different settings ofγ𝛾\\gammaitalic_γwhich controls the gap size between queries and also the overall sparsity of the calculation. This figure accompanies the latency ablation for Streaming LLM in the main text,Figure7(c).",
                "position": 1896
            }
        ]
    },
    {
        "header": "Appendix FApprox Window Size Calculation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11254/x22.png",
                "caption": "(a)Oracle top-k𝑘kitalic_ksparse attention.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x23.png",
                "caption": "(a)Oracle top-k𝑘kitalic_ksparse attention.",
                "position": 2118
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x24.png",
                "caption": "(b)Streaming LLM.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x25.png",
                "caption": "Figure 12:All RULER 131K subsets. This is a companion toFigure1. The CWE subset is excluded, as all models, including quadratic attention, scored 0%.",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x26.png",
                "caption": "",
                "position": 2134
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x27.png",
                "caption": "",
                "position": 2136
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x28.png",
                "caption": "Figure 13:Attention output cosine similarity (compared to full attention) for Streaming LLM with our method.Figures13,14and15show the results from every layer, and are a counterpart toFigure9in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention.",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x29.png",
                "caption": "",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x30.png",
                "caption": "",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x31.png",
                "caption": "",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x32.png",
                "caption": "",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x33.png",
                "caption": "",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x34.png",
                "caption": "",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x35.png",
                "caption": "",
                "position": 2152
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x36.png",
                "caption": "",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x37.png",
                "caption": "",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x38.png",
                "caption": "",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x39.png",
                "caption": "",
                "position": 2158
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x40.png",
                "caption": "",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x41.png",
                "caption": "",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x42.png",
                "caption": "",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x43.png",
                "caption": "",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x44.png",
                "caption": "Figure 14:Attention output cosine similarity (compared to full attention) for Streaming LLM with our method.Figures13,14and15show the results from every layer, and are a counterpart toFigure9in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention.",
                "position": 2168
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x45.png",
                "caption": "",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x46.png",
                "caption": "",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x47.png",
                "caption": "",
                "position": 2174
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x48.png",
                "caption": "",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x49.png",
                "caption": "",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x50.png",
                "caption": "",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x51.png",
                "caption": "",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x52.png",
                "caption": "",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x53.png",
                "caption": "",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x54.png",
                "caption": "",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x55.png",
                "caption": "",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x56.png",
                "caption": "",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x57.png",
                "caption": "",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x58.png",
                "caption": "",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x59.png",
                "caption": "",
                "position": 2192
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x60.png",
                "caption": "Figure 15:Attention output cosine similarity (compared to full attention) for Streaming LLM with our method.Figures13,14and15show the results from every layer, and are a counterpart toFigure9in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x61.png",
                "caption": "",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x62.png",
                "caption": "",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x63.png",
                "caption": "",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x64.png",
                "caption": "",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x65.png",
                "caption": "",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x66.png",
                "caption": "",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x67.png",
                "caption": "",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x68.png",
                "caption": "",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x69.png",
                "caption": "",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x70.png",
                "caption": "",
                "position": 2213
            },
            {
                "img": "https://arxiv.org/html/2505.11254/x71.png",
                "caption": "",
                "position": 2214
            }
        ]
    },
    {
        "header": "Appendix GRestatement and proof ofLemma1",
        "images": []
    }
]