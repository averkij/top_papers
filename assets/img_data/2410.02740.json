[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/Intro_v2.png",
                "caption": "Figure 1:The role of image-text data in multimodal foundation models: akeycomponent in training CLIP and Diffusion Model, andessentialfor multimodal LLM (MLLM) pre-training alongside text and interleaved image-text data. We propose a controllable captioning pipeline to synthesize different types of captions and explore optimal image-text data recipes for training these foundation models.",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/fig1a.png",
                "caption": "Figure 2:Zero-shot retrieval and classification performance of CLIP models. (a) The effect of synthetic captions (LLaVA recaptioned) and AltText: solely using LLaVA captions can improve retrieval tasks but significantly deteriorate the zero-shot classification performance. (b) The effect of different formats of synthetic captions on CLIP: Short Synthetic Captions (SSC) show superior results to Descriptive Synthetic Captions (DSC) and the combination of them achieves the best results.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/fig1b.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Customized Re-captioning for Multimodal Foundation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/captioner_examples_v3.png",
                "caption": "Figure 3:Examples of controllable captions of diverse formats generated by our captioner: we can generate from brief to dense descriptions and fuse AltText into the caption (AFC).",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/exp_2.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/exist_captioner.png",
                "caption": "Figure 4:Directly using MLLMs as image captioners may result in hallucinations and generate captions that do not align with specific instructions: both LLaVA(Liu et al.,2023b)and ShareGPT4V(Chen et al.,2024a)generate over three sentences and obvious hallucination.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/captioner_v2.png",
                "caption": "Figure 5:Overview of the controllable and human-aligned captioning pipeline. In Stage 1, we convert a pre-trained MLLM into a customized captioner that strictly follows the captioning instructions. In Stage 2, we leverage human-aligned captions to further fine-tune the captioner.",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/short_synthetic_caption_ssc.png",
                "caption": "Figure 6:Distribution of token lengths of our generated captions in four formats: we caption COCO 2017 images and visualize their distributions.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/descriptive_synthetic_caption_dsc.png",
                "caption": "",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/dense_synthetic_caption_dsc_plus.png",
                "caption": "",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/alttext_fusion_caption_afc.png",
                "caption": "",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/ssc_vs_alt_text.png",
                "caption": "Figure 7:The number of unique entities in different synthetic captions (randomly sample 17.5k images) compared to AltText: AltText provides more unique entities as wider knowledge.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/dsc_vs_alt_text.png",
                "caption": "",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/dsc+_vs_alt_text.png",
                "caption": "",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/afc_vs_alt_text.png",
                "caption": "",
                "position": 243
            }
        ]
    },
    {
        "header": "4Image-Caption Data for Multimodal Foundation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/clip_ratio_study.png",
                "caption": "Figure 8:The intersection of synthetic captions and AltText for CLIP. We gradually increase the proportion of SSC mixed with AltText during training. All experiments use ViT-B/16 as the backbone and VeCap-300M(Lai et al.,2024)as the pre-training dataset.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/diffusion_ratio_study_v2.png",
                "caption": "Figure 9:The intersection of synthetic captions and AltText for diffusion models. We gradually increase the proportion of DSC mixed with AltText during training.",
                "position": 897
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/lit.png",
                "caption": "Table A3:Compatibility between rewritten-based and filtering-based methods. We use DFN-2B(Fang et al.,2023)as the example and train CLIP with ViT/B-16 on different captions.",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2410.02740/extracted/5897046/Figure/capscore_v2.png",
                "caption": "Figure A2:An overview of CapScore to evaluate the quality of captions: we use LLM to generate assertions based on the caption and then use a VQA model to check these assertions.",
                "position": 2097
            }
        ]
    },
    {
        "header": "Appendix BA Deeper Analysis of Generated Captions",
        "images": []
    }
]