[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/BOLT.jpg",
                "caption": "Figure 1:Illustration of bootstrapping long chain-of-thought in large language models (BOLT). BOLT comprises three stages: 1)LongCoT Bootstrappingwhich involves synthesizing LongCoT data, 2)LongCoT Supervised Finetuningwhere we train a ShortCoT model to adapt to the LongCoT format, incorporating reasoning elements and practicing extended chains of thought before arriving at an external solution, 3)LongCoT Online Trainingwhere the LongCoT SFT model is further improved through online exploration and refinement.Bootstrapping LLMis a ShortCoT LLM that is used to generate LongCoT data via in-context learning.ORMis an outcome reward model which scores the external solution in the model response.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3BOLT",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/bolt_output_example.jpg",
                "caption": "Figure 2:An illustration of long chain-of-thought as internal thoughts. Portions of the external solution are omitted for brevity.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/topic_dist.png",
                "caption": "Figure 3:Topic distribution of query data in LongCoT Bootstrapping,ùíüb-querysubscriptùíüb-query\\mathcal{D}_{\\text{b-query}}caligraphic_D start_POSTSUBSCRIPT b-query end_POSTSUBSCRIPT.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/icl_instructions.jpg",
                "caption": "Figure 4:An illustration of the prompt used in LongCoT Bootstrapping.",
                "position": 212
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/mistral_7b_plot.png",
                "caption": "(a)Mistral-7B",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/mistral_7b_plot.png",
                "caption": "(a)Mistral-7B",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/llama_8b_plot.png",
                "caption": "(b)Llama-3.1-8B",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/llama_70b_plot.png",
                "caption": "(c)Llama-3.1-70B",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2502.03860/extracted/6182818/figures/performance_trajectory_subfigures.png",
                "caption": "Figure 6:Performance trajectory over the training process of BOLT on Llama-3.1-8B. Init indicates the initial model and in this case is Meta-Llama-3.1-8B-Instruct.",
                "position": 361
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiment Details",
        "images": []
    },
    {
        "header": "Appendix BQualitative Examples",
        "images": []
    }
]