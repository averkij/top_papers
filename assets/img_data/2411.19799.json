[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/main_figure.png",
                "caption": "Figure 1:Overview ofInclude. (a)Motivation:Multilingual benchmarks must reflect the cultural and regional knowledge of the language environments in which they would used. (b)Includeis a multilingual benchmark compiled from academic, professional, and occupational license examinations reflecting regional and cultural knowledge in 44 languages.",
                "position": 193
            }
        ]
    },
    {
        "header": "2Preliminaries: Language & Knowledge",
        "images": []
    },
    {
        "header": "3TheIncludebenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/descriptives.png",
                "caption": "Figure 2:Overview of the collected data grouped by script.We depict the languages associated with each script, the total samples in each script, and the percentage of the samples that were collected from new sources that have not been published by the community yet.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results & Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/knowledge_transfer.png",
                "caption": "Figure 3:Performance of models stratified by language using in-language prompting.Results are grouped by whether the language was explicitly included in the pretraining dataset of the model (Trained on Language), whether a similar language with the same script was in the pretraining corpus (Trained on Script), or whether there was no linguistically similar language in the pretraining corpus (Neither). Color dotted lines represent average performance for each category for a particular model. Black dotted lines represent average performance across all script-aligned languages.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/regional_results.png",
                "caption": "Figure 4:GPT-4o performance(In-language Prompt) on regional history exams (cultural) and global history exams from that region (region-implicit) based on a total of 11,148 questions fromInclude. In each language (except Telugu), models perform better on the global history exam than the regional history exam.",
                "position": 742
            }
        ]
    },
    {
        "header": "6Related work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/languages.png",
                "caption": "Figure 5:GPT-4o performance across academic disciplines for Korean, Persian, Armenian, Hindi, Greek, and Russian. Each bar is annotated with the number of questions with correct answers.",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/regional_stem.png",
                "caption": "Figure 6:GPT-4o model performanceonInclude-base. (a) Performance across regional labels. While models typically perform better acrossregion-explicitandregional-implicitquestions, it is difficult to disentangle the difficult of questions due to regionality from the subject matter itself (i.e.,region-agnosticquestions may contain more STEM subjects that are traditionally harder for LLMs). (b) Performance across academic disciplines within STEM area. We observe models perform particularly poorly on Math and Chemistry questions.",
                "position": 2006
            },
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/Cat_A_Cat_B.png",
                "caption": "Figure 7:Academic domain and academic fields with the number of examples across all languages.",
                "position": 5999
            },
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/form.png",
                "caption": "Figure 8:Exam source collection form sent to the academic community.",
                "position": 6002
            },
            {
                "img": "https://arxiv.org/html/2411.19799/extracted/6034225/figures/original_published_scatter_all.png",
                "caption": "Figure 9:Accuracy of different models on languages where both existing benchmark data and newly collected data are available. Each point represents the accuracy score of a model for a specific language. (a) Points of the same color represent the accuracy scores of a single model across different languages. (b) Points of the same color represent the accuracy scores for a single language across different models.",
                "position": 6088
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]