[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x1.png",
                "caption": "Figure 1:RoboMastersynthesizes realistic robotic manipulation video given an initial frame, a prompt, a user-defined object mask, and a collaborative trajectory describing the motion of both robotic arm and manipulated object in decomposed interaction phases. It supports diverse manipulation skills and can generalize to in-the-wild scenarios. Please check more on ourwebsite.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x2.png",
                "caption": "Figure 2:Collaborative Trajectory (Ours) vs Separated Trajectories (Previous,Â e.g.Tora).Unlike Tora[66]thatdecomposes objectsand uses separate trajectories to model the motion of robot arm and manipulated object, wedecompose the interaction phaseand unify their joint motions into a single collaborative trajectory with fine-grained object awareness. This integration alleviates the feature fusion issue in overlapping regions (see the missing apple in Tora), and improves visual quality.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x3.png",
                "caption": "Figure 3:RoboMaster Framework.Given an input imageğˆğˆ\\mathbf{I}bold_Iand a promptğœğœ\\mathbf{c}bold_c, it generates a desired robotic manipulation videoğ—ğ—\\mathbf{X}bold_Xwith the collaborative trajectory design. Specifically, it first encodes the object masks, including robotic armğŒdsubscriptğŒğ‘‘\\mathbf{M}_{d}bold_M start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTand submissive objectğŒssubscriptğŒğ‘ \\mathbf{M}_{s}bold_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT(acquired either from 1) Grounded-SAM[41]or 2) user-defined brush mask) with the awareness of appearance and shape to obtainğ¯d,ğ¯ssubscriptğ¯ğ‘‘subscriptğ¯ğ‘ \\mathbf{v}_{d},\\mathbf{v}_{s}bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , bold_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTfor maintaining identity consistency in the video. To precisely model the manipulation process, the controlled trajectoryğ’ğ’\\mathcal{C}caligraphic_Cis decomposed into sub-interaction phases: pre-interactionğ’1subscriptğ’1\\mathcal{C}_{1}caligraphic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, interactionğ’2subscriptğ’2\\mathcal{C}_{2}caligraphic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and post-interactionğ’3subscriptğ’3\\mathcal{C}_{3}caligraphic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, associating each phase with object-specific latentsğ¯dsubscriptğ¯ğ‘‘\\mathbf{v}_{d}bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT,ğ¯ssubscriptğ¯ğ‘ \\mathbf{v}_{s}bold_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, andğ¯dsubscriptğ¯ğ‘‘\\mathbf{v}_{d}bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, respectively. The collaborative trajectory latentğ•ğ•\\mathbf{V}bold_Vis then injected into plug-and-play motion injectors, enabling the reasoning of video dynamics during generation.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2506.01943/x4.png",
                "caption": "Figure 4:Subject Embedding Illustration.The object maskğŒğŒ\\mathbf{M}bold_Mis interpolated to align with the encoded RGB latentsğ³ğ³\\mathbf{z}bold_z. Then it samplesğ³ğ³\\mathbf{z}bold_zwith valid pixels and applies an average pooling operator to generate the embeddingğ¯~~ğ¯\\tilde{\\mathbf{v}}over~ start_ARG bold_v end_ARG. To enhance spatial awareness, it expands the object token by a radiusrğ‘Ÿritalic_r, which is proportional to the area of the valid mask region, and obtains the circular volumeğ¯ğ¯\\mathbf{v}bold_v.",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x5.png",
                "caption": "Figure 5:Qualitative Comparison.RoboMaster (ours) demonstrates superior performance across a range of manipulation skills (e.g., move, pick, close, upright, close), exhibiting improved visual consistency of the manipulated subject compared to prior baselines.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2506.01943/x6.png",
                "caption": "Figure 6:Generalizable Comparison with Input Prompt: â€˜Pick up the bee.â€™",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2506.01943/x7.png",
                "caption": "Figure 7:Ablation on a Generalizable Sample: â€˜Move the can to the right place of the eggplant.â€™",
                "position": 639
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/dataset_curation.png",
                "caption": "Figure S8:Dataset Construction Pipeline.It involves automatic annotation with human-in-the-loop processes to generate high-quality data samples.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/user_annotation.png",
                "caption": "Figure S9:Gradio Demo for User Annotation.The user is required to provide a prompt, an object mask, and a collaborative trajectory consisting of three phases: pre-interaction, interaction, and post-interaction, in sequence. This setup allows for flexible edits at any stage, enabling iterative refinement of the annotation.",
                "position": 835
            }
        ]
    },
    {
        "header": "Appendix BAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix CAdditional Visual Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/diverse_ood_objs.png",
                "caption": "Figure S10:â€˜Pick upâ€™ on Diverse Out-of-domain (OOD) Objects.The blue dot represents the current position of the manipulated object along the guided trajectory.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/diverse_skills.png",
                "caption": "Figure S11:Diverse Manipulation Skills on Bridge and In-the-wild Test Samples.",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/long_video.png",
                "caption": "Figure S12:Longer Video Generation with Multiple Input Prompts.",
                "position": 965
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]