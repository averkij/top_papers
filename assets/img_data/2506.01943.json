[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x1.png",
                "caption": "Figure 1:RoboMastersynthesizes realistic robotic manipulation video given an initial frame, a prompt, a user-defined object mask, and a collaborative trajectory describing the motion of both robotic arm and manipulated object in decomposed interaction phases. It supports diverse manipulation skills and can generalize to in-the-wild scenarios. Please check more on ourwebsite.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x2.png",
                "caption": "Figure 2:Collaborative Trajectory (Ours) vs Separated Trajectories (Previous, e.g.Tora).Unlike Tora[66]thatdecomposes objectsand uses separate trajectories to model the motion of robot arm and manipulated object, wedecompose the interaction phaseand unify their joint motions into a single collaborative trajectory with fine-grained object awareness. This integration alleviates the feature fusion issue in overlapping regions (see the missing apple in Tora), and improves visual quality.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x3.png",
                "caption": "Figure 3:RoboMaster Framework.Given an input image𝐈𝐈\\mathbf{I}bold_Iand a prompt𝐜𝐜\\mathbf{c}bold_c, it generates a desired robotic manipulation video𝐗𝐗\\mathbf{X}bold_Xwith the collaborative trajectory design. Specifically, it first encodes the object masks, including robotic arm𝐌dsubscript𝐌𝑑\\mathbf{M}_{d}bold_M start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPTand submissive object𝐌ssubscript𝐌𝑠\\mathbf{M}_{s}bold_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT(acquired either from 1) Grounded-SAM[41]or 2) user-defined brush mask) with the awareness of appearance and shape to obtain𝐯d,𝐯ssubscript𝐯𝑑subscript𝐯𝑠\\mathbf{v}_{d},\\mathbf{v}_{s}bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , bold_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTfor maintaining identity consistency in the video. To precisely model the manipulation process, the controlled trajectory𝒞𝒞\\mathcal{C}caligraphic_Cis decomposed into sub-interaction phases: pre-interaction𝒞1subscript𝒞1\\mathcal{C}_{1}caligraphic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, interaction𝒞2subscript𝒞2\\mathcal{C}_{2}caligraphic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and post-interaction𝒞3subscript𝒞3\\mathcal{C}_{3}caligraphic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, associating each phase with object-specific latents𝐯dsubscript𝐯𝑑\\mathbf{v}_{d}bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT,𝐯ssubscript𝐯𝑠\\mathbf{v}_{s}bold_v start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, and𝐯dsubscript𝐯𝑑\\mathbf{v}_{d}bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, respectively. The collaborative trajectory latent𝐕𝐕\\mathbf{V}bold_Vis then injected into plug-and-play motion injectors, enabling the reasoning of video dynamics during generation.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2506.01943/x4.png",
                "caption": "Figure 4:Subject Embedding Illustration.The object mask𝐌𝐌\\mathbf{M}bold_Mis interpolated to align with the encoded RGB latents𝐳𝐳\\mathbf{z}bold_z. Then it samples𝐳𝐳\\mathbf{z}bold_zwith valid pixels and applies an average pooling operator to generate the embedding𝐯~~𝐯\\tilde{\\mathbf{v}}over~ start_ARG bold_v end_ARG. To enhance spatial awareness, it expands the object token by a radiusr𝑟ritalic_r, which is proportional to the area of the valid mask region, and obtains the circular volume𝐯𝐯\\mathbf{v}bold_v.",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/x5.png",
                "caption": "Figure 5:Qualitative Comparison.RoboMaster (ours) demonstrates superior performance across a range of manipulation skills (e.g., move, pick, close, upright, close), exhibiting improved visual consistency of the manipulated subject compared to prior baselines.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2506.01943/x6.png",
                "caption": "Figure 6:Generalizable Comparison with Input Prompt: ‘Pick up the bee.’",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2506.01943/x7.png",
                "caption": "Figure 7:Ablation on a Generalizable Sample: ‘Move the can to the right place of the eggplant.’",
                "position": 639
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/dataset_curation.png",
                "caption": "Figure S8:Dataset Construction Pipeline.It involves automatic annotation with human-in-the-loop processes to generate high-quality data samples.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/user_annotation.png",
                "caption": "Figure S9:Gradio Demo for User Annotation.The user is required to provide a prompt, an object mask, and a collaborative trajectory consisting of three phases: pre-interaction, interaction, and post-interaction, in sequence. This setup allows for flexible edits at any stage, enabling iterative refinement of the annotation.",
                "position": 835
            }
        ]
    },
    {
        "header": "Appendix BAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix CAdditional Visual Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/diverse_ood_objs.png",
                "caption": "Figure S10:‘Pick up’ on Diverse Out-of-domain (OOD) Objects.The blue dot represents the current position of the manipulated object along the guided trajectory.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/diverse_skills.png",
                "caption": "Figure S11:Diverse Manipulation Skills on Bridge and In-the-wild Test Samples.",
                "position": 955
            },
            {
                "img": "https://arxiv.org/html/2506.01943/extracted/6500816/figs/long_video.png",
                "caption": "Figure S12:Longer Video Generation with Multiple Input Prompts.",
                "position": 965
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]