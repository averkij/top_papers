[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00956/x1.png",
                "caption": "Figure 1:In-the-wild try-on results generated by our RefVTON model with a p2p style, trained on person and garment images from our Virtual Fitting with Reference (VFR) dataset.The first row demonstrates ourmask-free try-oncapability, where the garment is transferred directly to the target person without masks or pose estimation. The second row shows ouradditional-reference try-onmode, in which extra visual references are incorporated to enhance structural accuracy, texture fidelity, and overall realism.",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x2.png",
                "caption": "Figure 2:The effect of using reference images for the virtual try-on task. From left to right in the three middle subfigures are: (i) results generated without using reference images during either training or inference; (ii) results generated by a model trained and inferred with reference images. Incorporating reference images consistently improves the try-on quality and authenticity in both training and inference stages. Please zoom in for more details.",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00956/x3.png",
                "caption": "Figure 3:The pipeline of our two-stage training strategy: (a) In the first stage, which follows a similar paradigm to mask-based try-on approaches, the model is trained on masked person images to generate person images wearing random garments for the next stage training. (b) In the second stage, the synthesized person images produced in the first stage, along with the target garment and additional reference images (optional), are jointly used as inputs to train a person-to-person virtual try-on model that directly fits the target cloth onto the personâ€™s body.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00956/x4.png",
                "caption": "Figure 4:Adaptation of a three-channel position index: the first channel encodes different conditional inputs, while the second and third channels provide spatial positional information for adapting the resolution of the target inputs.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x5.png",
                "caption": "Figure 5:The overall pipeline of generating the reference images. We first generate the appearance descriptions usingQwen2.5-VL[bai2025qwen2], and then concatenate the appearance with the corresponding actions and outfits to construct the positive and negative prompts, as shown in (a). Subsequently, the images and the textual prompts are fed into the Editing Model, which generates photos of individuals wearing the same clothes. These results are reference images for each imageâ€“cloth pair, as shown in (b).",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x6.png",
                "caption": "Figure 6:Qualitative comparison on the VITON dataset. and the model is trained following the pipeline in Fig.3(b). â€œRefâ€ denotes the additional referenceğ’“i\\bm{r}_{i}image is used during the inference, while â€œmask-freeâ€ indicates that the image is generated using an unmasked personğ’‘i\\bm{p}_{i}image instead of a masked agnostic imageğ’‚i\\bm{a}_{i}.",
                "position": 274
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00956/x7.png",
                "caption": "Figure 7:Qualitative comparison on the DressCode dataset., and the model is trained following the pipeline in Fig.3(b). â€œRef.â€ denotes the additional referenceğ’“i\\bm{r}_{i}image is used during the inference, while â€œmask-freeâ€ indicates that the image is generated using an unmasked personğ’‘i\\bm{p}_{i}image instead of a masked agnostic imageğ’‚i\\bm{a}_{i}.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x8.png",
                "caption": "Figure 8:Qualitative results of the ablation study across different settings.â€œRef.â€ denotes that a reference image is provided, while â€œMFâ€ indicates mask-free inputs using the original person image instead of a masked agnostic image.",
                "position": 869
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.00956/x9.png",
                "caption": "Figure 9:Text prompts from the Outfit and Action Description Bank. To ensure the model edits only the person while preserving the target clothing, we assign different outfits and action description categories to different clothing inputs.",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x10.png",
                "caption": "Figure 10:Sample reference images generated by our reference data generation pipeline. The editing model takes the target personâ€™s image as input and synthesizes corresponding reference images, while preserving the garmentâ€™s appearance to match the cloth.",
                "position": 919
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x11.png",
                "caption": "Figure 11:Qualitative paired results in VITON-HD dataset with complex patterns on clothes.â€œreferenceâ€ denotes that a reference image is provided.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x12.png",
                "caption": "Figure 12:Qualitative paired results in VITON-HD dataset with complex structure on clothes.â€œreferenceâ€ denotes that a reference image is provided.",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x13.png",
                "caption": "Figure 13:Qualitative results of upper-body sub-set in Dresscode dataset unpaired setting.â€œreferenceâ€ denotes that a reference image is provided, while â€œMFâ€ indicates mask-free inputs using the original person image instead of a masked agnostic image.",
                "position": 1064
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x14.png",
                "caption": "Figure 14:Qualitative results of lower-body sub-set in Dresscode dataset unpaired setting.â€œreferenceâ€ denotes that a reference image is provided, while â€œMFâ€ indicates mask-free inputs using the original person image instead of a masked agnostic image.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2511.00956/x15.png",
                "caption": "Figure 15:Qualitative results of dresses sub-set in Dresscode dataset unpaired setting.â€œreferenceâ€ denotes that a reference image is provided, while â€œMFâ€ indicates mask-free inputs using the original person image instead of a masked agnostic image.",
                "position": 1072
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]