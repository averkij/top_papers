[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08847/x1.png",
                "caption": "Figure 1:Algorithm comparison. (a) GRPO with global baseline(μ,σ)(\\mu,\\sigma)can cause unstable gradient norm. (b) Dr. MAS with per-agent normalization(μk,σk)(\\mu_{k},\\sigma_{k})stabilizes the training of MAS.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2602.08847/x2.png",
                "caption": "Figure 2:Overview of multi-agent LLM RL framework. A multi-agent orchestrator manages distributed rollouts, agents are mapped to LLM worker groups with optional LLM sharing, and a shared resource pool schedules actor backends for efficient inference and per-model optimization.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2602.08847/x3.png",
                "caption": "Figure 3:Illustration of the orchestrations.Left: Math orchestration uses a two-agent loop, where a solver proposes candidate solutions and a verifier evaluates and either approves or requests refinement.Right: Multi-turn search orchestration uses a hierarchical three-agent pipeline, where a top-level verifier selectively invokes either a search agent to retrieve external information or an answer agent to produce the final result.",
                "position": 363
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08847/x4.png",
                "caption": "Figure 4:Comparison of training accuracy and gradient norm between GRPO and Dr. MAS. The results are recorded during multi-agent RL post-training for three-agent search orchestration under LLM non-sharing (Qwen2.5-3B).",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2602.08847/x5.png",
                "caption": "Figure 5:Performance and efficiency comparison between homogeneous (all 7B models) and heterogeneous (7B for Verifier, 3B for Search/Answer) model assignment on search tasks.\nToken counts are the average tokens per trajectory for each agent.\nCost ($) is estimated using OpenRouter market prices (7B: $0.30/M tokens, 3B: $0.06/M tokens) and reported as the total inference cost over the full test set (51.7k samples).",
                "position": 927
            }
        ]
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProofs",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix DPseudo Code",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08847/x6.png",
                "caption": "Figure 6:Comparison of training accuracy and gradient norm between GRPO and Dr. MAS. The results are recorded during multi-agent RL post-training for two-agent math orchestration under LLM non-sharing (Qwen3-4B).",
                "position": 1508
            },
            {
                "img": "https://arxiv.org/html/2602.08847/x7.png",
                "caption": "Figure 7:Training dynamics of the three-agent search task (Qwen2.5-7B, non-sharing). Vanilla GRPO suffers from serious gradient spikes that lead to “NaN” on the search agent, whereas Dr. MAS maintains stable gradients and converges effectively.",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2602.08847/x8.png",
                "caption": "Figure 8:Training curves for different advantage normalization variants in the ablation study.",
                "position": 1528
            }
        ]
    },
    {
        "header": "Appendix FIllustrative Examples of Multi-Agent LLM Collaboration",
        "images": []
    }
]