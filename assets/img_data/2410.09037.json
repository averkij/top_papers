[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09037/extracted/5919077/images/previous_vs_ours.png",
                "caption": "Figure 1:Comparison between (a) previous approaches of reasoning distillation and (b) Mentor-KD (ours). Our framework utilizes an intermediate-sized task-specific mentor model to complement the distillation sets of teachers.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2410.09037/extracted/5919077/images/main_figure.png",
                "caption": "Figure 2:A general overview of our proposed framework, Mentor-KD. Mentor-KD is composed of three steps. First, CoT annotations are initially collected from the teacher LLM and filtered. Second, the preserved annotations are used to train the mentor model, and the trained mentor model augments multi-step rationales. Lastly, the student model is trained on annotations from the teacher and the student, as well as soft labels from the mentor model.",
                "position": 194
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09037/x1.png",
                "caption": "Figure 3:Performances by differentiating the degree (number) of mentor-generated CoT rationales per question. We adopt FlanT5-large and FlanT5-small as mentor and student models, respectively.",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2410.09037/x2.png",
                "caption": "Figure 4:Comparison of (a) accuracy of our mentor model (FlanT5-large) and LLM baselines on teacher-incorrect samples, and (b) performances of student models trained with augmented distillation sets from LLM baselines and our mentor models.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2410.09037/x3.png",
                "caption": "Figure 5:Comparison between Mentor-KD (Ours) and Vanilla-KD baseline on various distillation sets by differentiating the percentage of rationales being used.",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2410.09037/x4.png",
                "caption": "Figure 6:Comparison between student (FlanT5-small) performance using different mentor models considering various capacity gap sizes. Dotted lines in gray indicate Vanilla-KD baseline performances.",
                "position": 846
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Statistics",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details on Various Student Models",
        "images": []
    },
    {
        "header": "Appendix CAdditional Costs for Mentor Models",
        "images": []
    },
    {
        "header": "Appendix DEffects of Soft Label Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.09037/x5.png",
                "caption": "Figure 7:Effects of soft label distillation, by varying the value of loss interpolation hyperparameter (ŒªùúÜ\\lambdaitalic_Œª).",
                "position": 1797
            }
        ]
    },
    {
        "header": "Appendix EAPI Usage",
        "images": []
    },
    {
        "header": "Appendix FCase Study",
        "images": []
    }
]