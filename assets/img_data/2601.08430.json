[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08430/x1.png",
                "caption": "Figure 1:Motivating Example. Comparison between coarse-grained and fine-grained evaluation. Coarse rubrics (Rubric 1) result in indistinguishable high scores, whereas RubricHub (Rubric 2) utilizes highly discriminative criteria to reveal specific weaknesses, providing richer signals for alignment.",
                "position": 216
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08430/x2.png",
                "caption": "Figure 2:Overall method pipeline. (a)Coarse-to-Fine Rubric Generation: Candidates are synthesized via response-grounded and principle-guided strategies, then refined through aggregation and difficulty evolution intoRubricHub. (b)Utilization of Rubric in Post-Training: Rubrics are applied inRuFT(left) for rejection sampling and inRuRL(right) to provide structured reward signals for policy optimization.",
                "position": 260
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08430/x3.png",
                "caption": "Figure 3:Pie chart showing the source distribution across five major domains.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2601.08430/x4.png",
                "caption": "Figure 4:Score density distribution across models.",
                "position": 384
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08430/x5.png",
                "caption": "Table 1:Broad evaluation of frontier, rubric-based, and our proposed models across five-domain benchmarks.†\\daggerindicates results reported from official blogs, technical reports, or leaderboards.Boldindicates the best performance in each column within each model group. The \"+\" sign denotes the addition of training stages. Green and red subscripts represent the performance improvement and degradation relative to the corresponding Base model.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2601.08430/x5.png",
                "caption": "Figure 5:Performance comparison using RaR and RubricHub in Medical (left) and Science (right) domains on Qwen3-14B-Base.RaR (original): original RaR dataset.RaR (Rubrics by RubricHub): RaR questions with Rubrics regenerated by our pipeline.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2601.08430/x6.png",
                "caption": "Figure 6:Effect of criteria composition on RL performance (Qwen3-14B-Base). Training with only positively weighted criteria (Positive, ours) consistently outperforms the inclusion of negative penalties (Positive + Pitfall) across both benchmarks.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2601.08430/x7.png",
                "caption": "Figure 7:Agreement between Human and LLM evaluations. Blue bars:Cohen’s Kappafor inter-rater reliability. Purple bars:F1 Scoretreats human scores as ground truth. Red dashed line (0.6): threshold for substantial agreement.",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2601.08430/x8.png",
                "caption": "Figure 8:Training dynamics analysis on the HealthBench test set, with five colored lines corresponding to the rubric dimensions.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2601.08430/x9.png",
                "caption": "Figure 9:Ablation of Rubrics-based Rejection Sampling Fine-Tuning.Samplesdenotes the number of answers per question.Rubric Score: On theTraining Set, we first select the highest-scoring sampled response for each question and then average these scores;HealthBenchscores follow the official evaluation protocol.",
                "position": 886
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AHigh Quality Rubric Principle",
        "images": []
    },
    {
        "header": "Appendix BDetailed Training Settings",
        "images": []
    },
    {
        "header": "Appendix CAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix EDataset Sample",
        "images": []
    }
]