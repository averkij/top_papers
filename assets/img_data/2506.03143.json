[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/logo_new.png",
                "caption": "",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2506.03143/x1.png",
                "caption": "Figure 1:Left: Model performance vs. training data scale on the ScreenSpot-Pro benchmark. Higher and more left is better; larger points indicate models with more parameters. We only show GUI-Actor models built upon Qwen2-VL here for fair comparison. With Qwen2.5-VL as the backbone, GUI-Actor-3B/7B reaches scores up to 42.2/44.6 (without Verifier).Right: Illustration of action attention. GUI-Actor grounds target elements by attending to the most relevant visual regions.",
                "position": 192
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Design of GUI-Actor",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03143/x2.png",
                "caption": "(a)Illustration of attention-based action head.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2506.03143/x2.png",
                "caption": "(a)Illustration of attention-based action head.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2506.03143/x3.png",
                "caption": "(b)Image Patch Labels",
                "position": 289
            }
        ]
    },
    {
        "header": "4Grounding Verifier",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03143/x4.png",
                "caption": "Figure 3:Accuracy Progression Over Training Steps.",
                "position": 1364
            },
            {
                "img": "https://arxiv.org/html/2506.03143/x5.png",
                "caption": "(a)Hit@1 and Hit@3 for different models.",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2506.03143/x5.png",
                "caption": "(a)Hit@1 and Hit@3 for different models.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/attention_weight_multiple.png",
                "caption": "(b)GUI-Actor can capture multiple potential regions.",
                "position": 1585
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BDetails on Multi-Patch Supervision",
        "images": []
    },
    {
        "header": "Appendix CVisualization of Attention Maps from GUI-Actor",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/attention_map1.png",
                "caption": "(a)ScreenSpot: \"click the button to create a new project\"",
                "position": 2510
            },
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/attention_map1.png",
                "caption": "(a)ScreenSpot: \"click the button to create a new project\"",
                "position": 2513
            },
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/attention_map2.png",
                "caption": "(b)ScreenSpot-Pro: \"restart from CD\"",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/attentionmap_example_3.png",
                "caption": "(c)ScreenSpot-Pro: \"confirm sort\"",
                "position": 2524
            },
            {
                "img": "https://arxiv.org/html/2506.03143/extracted/6492052/figures/attentionmap_example4.png",
                "caption": "(d)ScreenSpot-Pro: \"select the legend of the plot\"",
                "position": 2529
            }
        ]
    },
    {
        "header": "Appendix DTraining Datasets used for GUI-Actor",
        "images": []
    },
    {
        "header": "Appendix EGUI Visual grounding Benchmarks",
        "images": []
    },
    {
        "header": "Appendix FMore Detailed on Grounding Verifier",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03143/x6.png",
                "caption": "Figure 6:Illustration of positive and negative examples used to train the grounding verifier.",
                "position": 2737
            }
        ]
    },
    {
        "header": "Appendix GImproving Grounding with Verifier",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03143/x7.png",
                "caption": "Figure 7:Comparison with AGUVIS using the verifier. AGUVIS inferences 21 times for verification. In contrast, GUI-Actor performs a single inference step, requiring only about 5% of the computation during inference.",
                "position": 2834
            }
        ]
    },
    {
        "header": "Appendix HOnline Benchmark Evaluation on OSWorld",
        "images": []
    }
]