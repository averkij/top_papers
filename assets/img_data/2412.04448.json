[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04448/x1.png",
                "caption": "Figure 1:OurMEMOgenerates talking videos with improved identity consistency, audio-lip alignment, and motion smoothness. In contrast, existing diffusion methods (e.g., Hallo2[12]) are prone to temporal error accumulation during autoregressive generation, especially when the last 2-4 generated frames used as temporal conditions contain artifacts, leading to inconsistent identity. Please refer to the supplementary material for video demos.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04448/x2.png",
                "caption": "Figure 2:Overview ofMEMO, which is structured with a Reference Net and a Diffusion Net. The core innovations ofMEMOreside in two key modules within the Diffusion Net: thememory-guided temporal moduleand theemotion-aware audio module. These modules work in tandem to deliver enhanced audio-video synchronization, sustained identity consistency, and more natural expression generation.",
                "position": 144
            }
        ]
    },
    {
        "header": "3Problem and Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04448/x3.png",
                "caption": "Figure 3:Memory-guided temporal module.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x4.png",
                "caption": "Figure 4:Emotion-aware audio module.",
                "position": 269
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04448/x5.png",
                "caption": "Figure 5:Human preferences amongMEMOand baselines, where users select the best method in terms of each evaluation metric.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x6.png",
                "caption": "Figure 6:MEMOcan generate talking videos featuring a wider range of smooth head movements and more emotional facial expressions, illustrated in both visualization and heatmaps. Please refer to the supplementary material for video demos.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x7.png",
                "caption": "Figure 7:The generated videos with various types of driving audio. Please refer to the supplementary material for video demos.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x7.png",
                "caption": "Figure 7:The generated videos with various types of driving audio. Please refer to the supplementary material for video demos.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x8.png",
                "caption": "Figure 8:The generated videos with various types of reference images. Please refer to the supplementary for video demos.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x9.png",
                "caption": "Figure 9:The generated videos on driving audio with different languages. See the supplementary for video demos.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x9.png",
                "caption": "Figure 9:The generated videos on driving audio with different languages. See the supplementary for video demos.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x10.png",
                "caption": "Figure 10:The generated videos with reference images of different head poses. See the supplementary for video demos.",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x11.png",
                "caption": "Figure 11:Ablation on the number of past frames (fùëìfitalic_f) during inference via human evaluation, where 16+f indicates our memory-guided inference with a context beyond 16 frames.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x11.png",
                "caption": "Figure 11:Ablation on the number of past frames (fùëìfitalic_f) during inference via human evaluation, where 16+f indicates our memory-guided inference with a context beyond 16 frames.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x12.png",
                "caption": "Figure 12:Human preference comparison between multi-modal attention and cross attention for integrating audio conditions in the audio module.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x13.png",
                "caption": "Figure 13:Ablation of our emotion-aware module with or without emotion decoupled training, given various human-defined emotion labels. Refer to the supplementary for video demos.",
                "position": 550
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Related Studies on Diffusion Models",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CAudio Emotion Detection",
        "images": []
    },
    {
        "header": "Appendix DData Processing Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04448/x14.png",
                "caption": "Figure 14:Examples of issues in the raw dataset.",
                "position": 1982
            },
            {
                "img": "https://arxiv.org/html/2412.04448/extracted/6048823/figures/score_distribution.png",
                "caption": "Figure 15:Distribution of the Sync-C in the CelebV-HQ.",
                "position": 1985
            }
        ]
    },
    {
        "header": "Appendix EMore Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04448/x15.png",
                "caption": "Figure 16:Ablation of the classifier-free guidance scale. Please refer to the supplementary for video demos.",
                "position": 2017
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x16.png",
                "caption": "Figure 17:Visualization of generated videos on the OOD dataset. Existing methods either have poor audio-lip synchronization (e.g., AniPortrait[60]) or suffer from error accumulation (e.g., Hallo[63]). In contrast,MEMOgenerates talking videos with natural head motion and accurate audio-lip synchronization without artifacts. Please refer to the supplementary for video demos.",
                "position": 2030
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x17.png",
                "caption": "Figure 18:More visualization of expressive talking videos generated by MEMO based on reference images with various emotions. Please refer to the supplementary for video demos.",
                "position": 2033
            },
            {
                "img": "https://arxiv.org/html/2412.04448/x18.png",
                "caption": "Figure 19:MEMOcan generate long-duration videos with alleviated error accumulation and maintain identity consistency. Please refer to the supplementary for video demos.",
                "position": 2036
            }
        ]
    },
    {
        "header": "Appendix FMore Qualitative Results",
        "images": []
    }
]