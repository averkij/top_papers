[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x1.png",
                "caption": "Figure 1:Learning viaReSpect. We deploy an LLM policyœÄŒ∏œÅ‚Å¢(a|x)subscriptùúãsubscriptùúÉùúåconditionalùëéùë•\\pi_{\\theta_{\\rho}}(a|x)italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT italic_œÅ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a | italic_x )in roundsœÅùúå\\rhoitalic_œÅ, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted inblue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so farD‚â§œÅsubscriptùê∑absentùúåD_{\\leq\\rho}italic_D start_POSTSUBSCRIPT ‚â§ italic_œÅ end_POSTSUBSCRIPT. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments ‚Äì the LLM improves from 31% to 82% task completion rate over six rounds.",
                "position": 192
            }
        ]
    },
    {
        "header": "2Technical Overview and Notation",
        "images": []
    },
    {
        "header": "3MultiRef: a Multi-turn Grounded Interaction Scenario",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x2.png",
                "caption": "Figure 2:The interaction scenario we use in our experiments.MultiRefis a multi-turn reference game. A speaker and a listener both observe a shared set of tangram shapes, but in different order. The goal of the speaker is to describe a subset of targets for the listener to select. Because the target requires multiple abstract shapes, humans often communicate the targets gradually over multiple turns. As an interaction progresses naturally, the speaker produces implicit feedback signals that validate or reject the listener‚Äôs actions.",
                "position": 272
            }
        ]
    },
    {
        "header": "4ReSpect: Retrospective Learning from Past Interactions",
        "images": []
    },
    {
        "header": "5Experimental setup",
        "images": []
    },
    {
        "header": "6Results and analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x3.png",
                "caption": "Figure 4:Task performance and efficiency improve as the policy learns from more past interactions. We present deployment results across three rounds for six concurrent systems, and three more rounds for the top systemb-sup, together with human-human references (hh) and a redeployment of the initial policyœÄŒ∏0subscriptùúãsubscriptùúÉ0\\pi_{\\theta_{0}}italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT(control).Left:interaction-level success rate (‚Üë‚Üë\\uparrow‚Üë, higher is better).Center:interaction-level efficiency by # turns per interactions (‚Üì‚Üì\\downarrow‚Üì).Right:micro-level performance by click accuracy (‚Üë‚Üë\\uparrow‚Üë). Shades are 95% confidence intervals by bootstrapping with 10,000 resamples.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x4.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x5.png",
                "caption": "Figure 6:Confusion matrices of the binary (top row) and ternary (bottom row) feedback decoders over rounds. The feedback decoder yields precise positive signals, even in early rounds.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x6.png",
                "caption": "Figure 7:Language analysis of human instructions. All systems show a decrease in instruction complexity in the first three rounds, except forb-kto, suggesting adaptation and improved efficiency on the speaker‚Äôs side. Keyword-based analysis reveals that the number of reset/frustration signals drops, a reflection of the model learning and collaboration improving.",
                "position": 638
            }
        ]
    },
    {
        "header": "7Related work",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheMultiRefGame Design and Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x7.png",
                "caption": "Figure 8:TheMultiRefinterface for the speaker in turn 1. Predefined targets are revealed to the speaker in black boxes.",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x8.png",
                "caption": "Figure 9:TheMultiRefinterface for the listener in turn 2, following the speaker turn inFigure¬†8. Targets are hidden for the listener, and the context tangrams are in a different order. Here the listener has selected a tangram given the instructionselect the butterfly.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x9.png",
                "caption": "Figure 10:TheMultiRefinterface for the speaker in turn 3, following the listener turn inFigure¬†9The listener selected a non-target tangram, shown in red to the speaker.",
                "position": 1566
            }
        ]
    },
    {
        "header": "Appendix BLearning Details",
        "images": []
    },
    {
        "header": "Appendix CCumulative Number of Interactions Observed",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x10.png",
                "caption": "Figure 13:Cumulative number of human-bot interactions used to train the policy each round.",
                "position": 1878
            }
        ]
    },
    {
        "header": "Appendix DAdditional Enhanced LoRA Launch",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x11.png",
                "caption": "Figure 14:Success rate ofb-supwith additional LoRA adapters in round 4 and 5.",
                "position": 1894
            }
        ]
    },
    {
        "header": "Appendix EFeedback Decoder Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13852/x12.png",
                "caption": "Figure 15:The speaker is left with the last target at Turn 4. Failing, they provide an additional description in Turn 5, and eventually resort to ‚Äútry again‚Äù without describing the target in Turn 6. The initial turns illustrate how feedback is implied, rather than specified explicitly. The interaction concludes successfully.",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x13.png",
                "caption": "Figure 16:The speaker asks to deselect everything in Turn 3 to reset, an expression of frustration. The interaction concludes successfully.",
                "position": 1920
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x14.png",
                "caption": "Figure 17:The abstractness and ambiguity of tangrams lend to complex interactions. There are two dogs in the context, and the listener struggles to disambiguate or identify the target. The interaction concludes successfully.",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2410.13852/x15.png",
                "caption": "Figure 18:The speaker asks for two targets in Turn 1, exemplifying Grice‚Äôs Maxims of Quantity - one tries to be as informative as one possibly can, and gives as much information as is needed, and no more(Grice,1975). The interaction concludes successfully.",
                "position": 1932
            }
        ]
    },
    {
        "header": "Appendix FInteraction Case Studies",
        "images": []
    }
]