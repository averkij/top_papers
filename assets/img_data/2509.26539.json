[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/x1.png",
                "caption": "(a)GUI Grounding.",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x1.png",
                "caption": "(a)GUI Grounding.",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x2.png",
                "caption": "(b)GUI Navigation.",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x3.png",
                "caption": "Figure 2:An illustration ofFerret-UI Liteon a multi-step GUI navigation task. Human users prompt with a high-level goal in plain text, and the model autonomously interacts with GUI devices through tapping, scrolling, typing, etc., until the task is complete. At each step, the model observes the GUI screen, generates think-plan-act traces, and executes the action.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x4.png",
                "caption": "Figure 3:Model architecture and training recipes ofFerret-UI Lite. The model takes a GUI screen and the user instruction as inputs, and predicts chain-of-thought reasoning traces and a low-level action policy to control GUI devices in an end-to-end manner directly. The model is trained through supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR).",
                "position": 113
            }
        ]
    },
    {
        "header": "2Supervised Fine-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/x5.png",
                "caption": "Figure 4:Synthetic navigation data generation pipeline, which consists of offline data generation based on human-annotated trajectories, and online rollouts collection from a multi-agent system.",
                "position": 150
            }
        ]
    },
    {
        "header": "3Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/images/zoomin4.png",
                "caption": "Figure 5:Ferret-UI Liteemploys a zoom-in operation to refine predictions. It generates an initial prediction based on the given instruction, crops the image around this prediction, and finally re-predicts on the cropped region for improved accuracy.",
                "position": 182
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/x6.png",
                "caption": "(a)SFT vs RL variants",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x6.png",
                "caption": "(a)SFT vs RL variants",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x7.png",
                "caption": "(b)data ratios",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x8.png",
                "caption": "(c)w/ vs w/o high-res data",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x9.png",
                "caption": "Table 4:SFT ablations on the AndroidWorld (AW) benchmark. Success rates (%) are averaged over five runs. The baseline model is built using only human-annotated episodes, without CoT and synthetic data.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x9.png",
                "caption": "(a)Impact of SFT steps for RL.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x9.png",
                "caption": "(a)Impact of SFT steps for RL.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x10.png",
                "caption": "(b)Impact of RL reward design.",
                "position": 677
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BCase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/images/osw_traj.png",
                "caption": "Figure 8:Successful completion for “Make sparkline charts for each order id with the data from Jan to Mar in theChartcolumn.” task in OSWorld evaluation.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x11.png",
                "caption": "Figure 9:Successful Completion for “Delete the following recipes from Broccoli app: Kale and Quinoa Salad, Baked Cod with Lemon and Dill, Rasperry Almond Smoothie” task in AndroidWorld evaluation.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2509.26539/x12.png",
                "caption": "Figure 10:Successful Completion for “Create a new contact for Lina Muller. Their number is +15410831733” task in AndroidWorld evaluation.",
                "position": 1687
            }
        ]
    },
    {
        "header": "Appendix CUnified Action Space",
        "images": []
    },
    {
        "header": "Appendix DSFT Data Mixture",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/x13.png",
                "caption": "Figure 11:GUI dataset mixture used for supervised fine-tuning, including grounding datasets and navigation datasets. The mobile synthetic dataset and desktop synthetic dataset are generated using the pipeline described in Section2.3. Legends:Grounding.Navigation.Web.Desktop.Mobile.",
                "position": 2060
            }
        ]
    },
    {
        "header": "Appendix EEvolution of RLVR Reward Curve",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26539/x14.png",
                "caption": "Figure 12:Evolution of verifiable reward curve during RLVR training.",
                "position": 2188
            }
        ]
    },
    {
        "header": "Appendix FGrounding Dataset Ablations",
        "images": []
    },
    {
        "header": "Appendix GFine-Grained Grounding Results",
        "images": []
    }
]