[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04320/extracted/6176595/figures/CrownJewelDragon.png",
                "caption": "Figure 1:ConceptAttentionproduces saliency maps that precisely localize the presence of textual concepts in images.We compare Flux raw cross attention, DAAM(Tang et al.,2022)with SDXL, and TextSpan(Gandelsman et al.,2024)for CLIP.",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04320/x1.png",
                "caption": "Figure 2:ConceptAttentionaugments multi-modal DiTs with a sequence of concept embeddings that can be used to produce saliency maps.(Left) An unmodified multi-modal attention (MMAttn) layer processes bothpromptandimagetokens. (Right)ConceptAttentionaugments these layers without impacting the image appearance to create a set of contextualizedconcepttokens.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2502.04320/x2.png",
                "caption": "Figure 3:ConceptAttentioncan generate high-quality saliency maps for multiple concepts simultaneously.Additionally, our approach is not restricted to concepts in the prompt vocabulary.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Proposed Method:ConceptAttention",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04320/x3.png",
                "caption": "Figure 4:(a)MMAttncombines cross and self attention operations between the prompt and image tokens. (b) OurConceptAttentionallows the concept tokens to incorporate information from other concept tokens and the image tokens, but not the other way around.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2502.04320/x4.png",
                "caption": "Figure 5:ConceptAttentionproduces higher fidelity raw scores and saliency maps than alternative methods, sometimes surpassing in quality even the ground truth saliency map provided by the ImageNet-Segmentation task. Top row shows the soft predictions of each method and the bottom shows the binarized predictions.",
                "position": 268
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04320/x5.png",
                "caption": "Figure 6:LaterMMAttnlayers encode richer features for zero-shot segmentation.We investigated the impact of using features from variousMMAttnlayers and found that deeper layers lead to better performance on segmentation metrics like pixelwise accuracy, mIoU, and mAP. We also found that combining the information from all layers further improves performance.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2502.04320/x6.png",
                "caption": "Figure 7:Optimal segmentation performance requires some noise to be present in the image.We evaluated the performance ofConceptAttentionby encoding samples from a variety of timesteps (determines the amount of noise). Interestingly, we found that the optimal amount of noise was not zero, but in the middle to later stages of the noise schedule.",
                "position": 1457
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Impact Statement",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore In-depth Explanation of Concept Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04320/x7.png",
                "caption": "Figure 8:Pseudo-code depicting the (a) multi-modal attention operation used by Flux DiTs and (b) ourConceptAttentionoperation.We leverage the parameters of a multi-modal attention layer to construct a set of contextualized concept embeddings. The concepts query the image tokens (cross-attention) and other concept tokens (self-attention) in an attention operation. The updated concept embeddings are returned in addition to the image and text embeddings.",
                "position": 2333
            },
            {
                "img": "https://arxiv.org/html/2502.04320/extracted/6176595/figures/supplemental_imagenet_segmentations/QualitativeComparisonFigure.png",
                "caption": "Figure 9:A qualitative comparison between our method and several others.",
                "position": 2344
            },
            {
                "img": "https://arxiv.org/html/2502.04320/x8.png",
                "caption": "Figure 10:A qualitative comparison between our method and several others.",
                "position": 2347
            },
            {
                "img": "https://arxiv.org/html/2502.04320/x9.png",
                "caption": "Figure 11:A qualitative comparison between our method and several others.",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2502.04320/extracted/6176595/figures/supplemental_imagenet_segmentations/supplemental_6.png",
                "caption": "Figure 12:A qualitative comparison between numerous baselines on ImageNet Segmentation Images. The top row shows the soft predictions of each method and the bottom shows the binarized segmentation predictions.",
                "position": 2353
            },
            {
                "img": "https://arxiv.org/html/2502.04320/extracted/6176595/figures/supplemental_imagenet_segmentations/supplemental_3.png",
                "caption": "Figure 13:A qualitative comparison between numerous baselines on ImageNet Segmentation Images. The top row shows the soft predictions of each method and the bottom shows the binarized segmentation predictions.",
                "position": 2356
            },
            {
                "img": "https://arxiv.org/html/2502.04320/extracted/6176595/figures/supplemental_imagenet_segmentations/supplemental_4.png",
                "caption": "Figure 14:A qualitative comparison between numerous baselines on ImageNet Segmentation Images. The top row shows the soft predictions of each method and the bottom shows the binarized segmentation predictions.",
                "position": 2359
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Results",
        "images": []
    }
]