[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05909/AnonymousSubmission/LaTeX/figure/sample.pdf",
                "caption": "Figure 1:Token selection in the reader’s embedding space. We project summary’s token embeddings with t-SNE and compare three selections: nearest to the mean-pooled vector, highest predictive probability, and contributors to max pooling. Mean pooling and perplexity concentrate near the center and favour syntactically frequent tokens. Max pooling emphasises boundary tokens near the convex hull that carry salient semantics.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: Summarise retrieval passages to align with the reader",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05909/AnonymousSubmission/LaTeX/figure/hotpotqa_ppl_longppl_twopanel.png",
                "caption": "Figure 2:RAG task performances (measured by EM and F1) when feeding summaries with varying PPL (left) and LongPPL (right) to the Reader on the HotpotQA dataset. The low Pearson correlation coefficients (rr) indicate that both PPL and LongPPL fail to identify a good summary.",
                "position": 183
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05909/AnonymousSubmission/LaTeX/figure/method_new.pdf",
                "caption": "Figure 3:Overview of the xCompress framework. Retrieved passages are first compressed into summaries. An adaptive norm-guided filtering mechanism determines whether additional test-time sampling is necessary. If required, multiple summaries are sampled from the compressor LLM and evaluated using the Spectrum Projection Score (SPS). These summaries are first embedded via max-pooling, then projected onto the reader’s principal subspace of its parameter. The summary with the lowest SPS is selected as input to the reader; otherwise, the initial summary is used directly for answer generation.",
                "position": 215
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05909/AnonymousSubmission/LaTeX/figure/sps_dual_analysis_combined.pdf",
                "caption": "Figure 4:SPS performance under (a) Across LLM layers. (b) Varying PCA retained variance ratios. Optimal results are achieved using embeddings from the penultimate layer and a PCA variance ratio of 0.95.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2508.05909/AnonymousSubmission/LaTeX/figure/generation_number_influence.png",
                "caption": "Figure 5:Impact of the number of generated summaries on EM and F1 scores TrivialQA. Performance saturates at five summaries, providing an optimal balance between effectiveness and computational efficiency.",
                "position": 761
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AA. Prompt Design",
        "images": []
    },
    {
        "header": "Appendix BB. Theoretical discussion",
        "images": []
    },
    {
        "header": "Appendix CC. Overconfidence Analysis of Qwen and LlaMa Model",
        "images": []
    }
]