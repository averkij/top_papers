[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14169/x1.png",
                "caption": "Figure 1:NOVA framework and the inference process.With text inputs, NOVA performs autoregressive generation via temporal frame-by-frame prediction and spatial set-by-set prediction. Finally, we implement diffusion denoising in a continuous-values space.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x2.png",
                "caption": "Figure 2:Overview of our block-wise temporal and spatial generalized autoregressive attention. Different from per-token generation, NOVA regressively predicts each frame in a casual order across the temporal scale, and predicts each token set in a random order across the spatial scale.",
                "position": 210
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14169/x3.png",
                "caption": "Figure 3:Text-to-image generation.Text prompts from left to right: (1) “A digital artwork of a cat styled in a whimsical fashion…”, (2) “A solitary lighthouse standing tall against a backdrop of stormy seas and dark, rolling clouds”, (3) “A vibrant bouquet of wildflowers on a rustic wooden table”, (4) “A selfie of an old man with a white beard”, (5) “A serene, expansive beach with no people”, (6) “A blue apple and a green cup.” and (7) “A chicken on the bottom of a balloon.”",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x4.png",
                "caption": "Figure 4:Text-to-video generation.We highlight the keywords inredcolor. NOVA follows the text prompts and vividly captures the motion of subjects (i.e., 3D model, cat and fireworks).",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x5.png",
                "caption": "Figure 5:Zero-shot video extrapolation.We highlight the subjects inredandgreenrespectively. The top images are generated, while the bottom images are extrapolated.",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x6.png",
                "caption": "Figure 6:Zero-shot generalization on multiple contexts.It is evident that NOVA successfully maintains temporal consistency in objects, both with and without text. Such as ensuring ”water continues to flow smoothly.” This highlights NOVA’s capability for zero-shot multitasking.",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x7.png",
                "caption": "Figure 7:Temporal autoregressive modeling (TAM) for video generation.We highlight the subtle changes in frames generated from the same prompt. Compared to spatial-only autoregressive method, the inclusion of TAM enables NOVA to more accurately capture the dynamics of subject movement.",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x8.png",
                "caption": "Figure 8:Ablation studies on NOVA’s architecture components.We carefully examine the two key stability factors in large-scale video generation training, as illustrated in (a) and (b).",
                "position": 1053
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x9.png",
                "caption": "",
                "position": 1058
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x10.png",
                "caption": "Figure 9:Visualization of decomposition ranks in the Scaling and Shift layer.The first row displays the results of the first frame, while the second row presents the results of the last frame.",
                "position": 1068
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AArchitecture details of Scaling and Shift layer",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14169/x11.png",
                "caption": "Figure 10:Scaling and Shift layer.We reformulate cross-frame motion changes by learning relative distribution variations within a unified space based on BOV tokens, rather than directly modeling the unreferenced distribution of the current frame.",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x11.png",
                "caption": "Figure 10:Scaling and Shift layer.We reformulate cross-frame motion changes by learning relative distribution variations within a unified space based on BOV tokens, rather than directly modeling the unreferenced distribution of the current frame.",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x12.png",
                "caption": "Figure 11:Three normalization architectures.We summarize various configurations including the pre-normalization layer (left), the post-normalization layer after residual addition (middle), and the post-normalization layer before residual addition (right). Here Post-Norm before Res is our standard design.",
                "position": 2393
            }
        ]
    },
    {
        "header": "Appendix BNormalization configurations",
        "images": []
    },
    {
        "header": "Appendix Cvideo extrapolation evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14169/x13.png",
                "caption": "Figure 12:PSNR and LPIPS metrics over 50 autoregressive steps in video extrapolation.Due to the 4×\\times×downsampling rate of VAE in temporal scale, each autoregressive step generates four frames. The vertical red line marks the point where the extrapolation reaches 3×\\times×training length.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x14.png",
                "caption": "Figure 13:Visualization of video extrapolation.Although the metrics indicate a decline, the generated frames still closely resemble the original video in content and overall image quality. Visualization suggests that the model can extrapolate up to 3×\\times×training length.",
                "position": 2419
            }
        ]
    },
    {
        "header": "Appendix DInference time analysis",
        "images": []
    },
    {
        "header": "Appendix EAblations on the impact of Temporal Autoregressive Modeling",
        "images": []
    },
    {
        "header": "Appendix FComprehensive DPG-Bench evaluation results",
        "images": []
    },
    {
        "header": "Appendix GMore text-to-image visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14169/x15.png",
                "caption": "Figure 14:More text-to-image visualizations.Text prompts are as follows: (1) “In the foreground is the detailed, head-and-shoulders portrait of an elderly man with a long white beard…”, (2) “a digital artwork of a fantasy warrior character. The character is male, depicted from the waist up, and appears to have a stern or serious facial expression…”, (3) “a young girl wearing a tiara and frilly dress”, (4) “A sunflower in sunglasses dances in the snow”, (5) “A beach with no people”, (6) “Two Ming vases on the table, the larger one is more colorful than the other”, (7) “A dragon perched majestically on a craggy, smoke-wreathed mountain”, (8) “a dragon breathing fire onto a knight”, (9) “a pixel art style graphic with vibrant colors. It features a single rider on a horse, both depicted in mid-gallop to the left side of the frame…”, (10) “A table full of food. There is a plate of chicken rice, a bowl of bak chor mee, and a bowl of laksa”, (11) “A map of the United States made out sushi. It is on a table next to a glass of red wine” and (12) “beautiful fireworks in the sky with red, white and blue”.",
                "position": 2646
            },
            {
                "img": "https://arxiv.org/html/2412.14169/x16.png",
                "caption": "Figure 15:More text-to-video visualizations. Best viewed with zoom for enhanced detail.",
                "position": 2657
            }
        ]
    },
    {
        "header": "Appendix HMore text-to-video visualizations",
        "images": []
    }
]