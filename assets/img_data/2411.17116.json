[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17116/x1.png",
                "caption": "(a)Phase 1: Local Context Encoding with Anchor Blocks",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x1.png",
                "caption": "(a)Phase 1: Local Context Encoding with Anchor Blocks",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x2.png",
                "caption": "(b)Phase 2: Query Encoding and Output Generation with Global Attention",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x3.png",
                "caption": "Figure 2:Block sparsity pattern for a sequence partitioned into 5 context blockscisubscriptùëêùëñc_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand a query blockqùëûqitalic_q. Each context block attends only to itself and the ‚Äúanchor block‚Äù whereas the query attends to the entire input.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Star Attention Algorithm",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17116/x3.png",
                "caption": "(a)Global Attention",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x3.png",
                "caption": "(a)Global Attention",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x4.png",
                "caption": "(b)Blockwise Context Encoding",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x5.png",
                "caption": "(c)Blockwise Context Encoding with Anchor Blocks",
                "position": 200
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17116/x6.png",
                "caption": "Figure 4:Accuracy (%) of Star Attention on RULER and BABILong evaluated on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. Results using the Llama-3-8B-Instruct-262k, Llama-3.1-8B-Instruct and Llama-3.1-8B-Base models demonstrate that Star Attention retains 95-100% of the accuracy of global attention, and in some cases, even outperform it.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x7.png",
                "caption": "Figure 5:Effect of block size on the accuracy of Star Attention on the RULER benchmark with block sizes ranging from 4K to 32K tokens for Llama-3.1-8B instruct model at sequence length of 128K. In each setting, the anchor block size matches the context block size. The results indicate that larger context block sizes are positively correlated with improved accuracy.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2411.17116/x8.png",
                "caption": "Figure 6:Accuracy (%) of Star Attention using the Llama-3.1-8B-Instruct model on the 5 categories of tasks in RULER on sequence lengths of 32K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. For the NIAH and QA tasks, Star Attention retains upto 95-100% accuracy of the baseline. The Multi-Hop Tracing task becomes quite challenging since it requires inter-block communication. In aggregation tasks, Star Attention show significant improvement as distributed local attention helps the model in such summarization tasks. The trend is consistent to other sequence lengths as shown in AppendixE(Figure8)",
                "position": 511
            }
        ]
    },
    {
        "header": "4Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17116/x9.png",
                "caption": "Figure 7:Effect of anchor block size on the accuracy of Star Attention on the RULER-NIAH benchmark with the Llama-3.1 8B Instruct model. In these experiments, the context block size is fixed to 32K for sequence length 128K, respectively. Results indicate that larger anchor block sizes lead to improved accuracy. The observed trend holds for all sequence lengths in our experiments.",
                "position": 658
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AStar Attention Pseudo-code",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": []
    },
    {
        "header": "Appendix DAccuracy of Star Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17116/x10.png",
                "caption": "Figure 8:Accuracy (%) of star attention using the Llama-3.1-8B-Instruct model on the 5 categories of tasks in RULER on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. For the NIAH and QA tasks, Star Attention retains upto 95-100% accuracy of the baseline. The Multi-Hop Tracing task is notably challenging because it requires inter-block communication, which leads to expected performance degradation. Interestingly, Star Attention performs better with sequence lengths of 128k on this task, but this may be due to noise given the suboptimal baseline. In aggregation tasks, Star Attention show significant improvement as distributed local attention helps the model in such summarization tasks.",
                "position": 3394
            }
        ]
    },
    {
        "header": "Appendix EAccuracy on all RULER Tasks",
        "images": []
    }
]