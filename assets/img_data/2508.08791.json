[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08791/x1.png",
                "caption": "Figure 1:Illustrative examples of four scenarios, categorized by varying sub-question pattern combinations.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2508.08791/x2.png",
                "caption": "Figure 2:Overview of our proposed approach. The automated environment construction follows a five-stage pipeline to generate diverse tool-use training environments. Feedback-driven model training then collects data within these environments, incorporates verifiable reward mechanisms, and optimizes performance using preference-based RL algorithms.",
                "position": 186
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approaches",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08791/x3.png",
                "caption": "Figure 3:Performance of each generalized capability before and after training across different models.",
                "position": 1066
            }
        ]
    },
    {
        "header": "5Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08791/x4.png",
                "caption": "Figure 4:Performance of Qwen 2.5-7B trained using different reward mechanisms.",
                "position": 1096
            }
        ]
    },
    {
        "header": "6Further Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08791/x5.png",
                "caption": "Figure 5:Solve-F1 of various LLMs across training epochs.",
                "position": 1118
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Details of Experimental Setup",
        "images": []
    },
    {
        "header": "9Prompts for Environment Construction",
        "images": []
    },
    {
        "header": "10Chat Templates of Various LLMs",
        "images": []
    },
    {
        "header": "11Detailed Results for Each Dataset",
        "images": []
    },
    {
        "header": "12Case Study",
        "images": []
    }
]