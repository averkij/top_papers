[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x1.png",
                "caption": "Figure 1:Human preference evaluation of Open-Sora 2.0 against other leading video generation models. Win rate represents the percentage of comparisons where our model was preferred over the competing model. The evaluation is conducted on 100 prompts carefully designed to cover three key aspects: 1) visual quality, 2) prompt adherence, and 3) motion quality. Results show that our model performs favorably against other top-performing models in all three aspects.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x2.png",
                "caption": "Figure 2:The hierarchical data filtering pipeline. The raw videos are first transformed into trainable video clips. Then, we apply various complimentary score filters to obtain data subsets for each training stage.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2503.09642/extracted/6273005/training_data_v2.png",
                "caption": "Figure 3:Distribution of key attributes of the whole video dataset.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2503.09642/extracted/6273005/wordcloud.png",
                "caption": "Figure 4:Word cloud of the video captions.",
                "position": 276
            }
        ]
    },
    {
        "header": "3Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x3.png",
                "caption": "Figure 5:Architecture of Video DC-AE.(a) Overview of Video DC-AE: Each block in encoder introduces spatial downsampling, while temporal downsampling occurs at blocks 4 and 5, with a corresponding symmetric structure in the decoder. (b) Residual Connection in Video DC-AE Blocks.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2503.09642/extracted/6273005/figs/open-sora-2.0-dit.png",
                "caption": "Figure 6:Open-Sora diffusion transformer architecture.",
                "position": 413
            }
        ]
    },
    {
        "header": "4Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x4.png",
                "caption": "Figure 7:Training and Inference Speed Comparison at 768px. This figure compares the training and inference speeds at 768px resolution using HunyuanVideo VAE (4×8×84884\\times 8\\times 84 × 8 × 8, patch size 2) and Video DC-AE (4×32×32432324\\times 32\\times 324 × 32 × 32, patch size 1). The results demonstrate that Video DC-AE achieves significantly higher efficiency in both training and inference (50 steps) compared to HunyuanVideo VAE, highlighting its advantage in high-resolution video generation.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x5.png",
                "caption": "Figure 8:Comparison of Video Generation with Different Autoencoder Compression Ratios.The top and bottom rows correspond to lower and higher compression ratios, respectively. While the higher compression ratio AE results in slightly blurrier outputs, it significantly improves generation speed.",
                "position": 868
            }
        ]
    },
    {
        "header": "5Conditioning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x6.png",
                "caption": "Figure 9:Image and Video Condition FrameworkCondition information is injected into the model via an additional channel dimension. A mask mechanism is introduced to distinguish different input types. This design enables support for a wide range of image-to-video (I2V) and video-to-video (V2V) tasks.",
                "position": 919
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x7.png",
                "caption": "Figure 10:Heatmap of Image Guidance Scale Across Denoising Steps and Latent FramesDarker regions indicate higher image guidance values, emphasizing stronger influence on later frames and earlier denoising steps.",
                "position": 936
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x8.png",
                "caption": "Figure 11:Effect of Motion Score on Video Generation.This figure illustrates the impact of different motion scores on the generated video. As the motion score increases, the camera movement becomes more pronounced and the overall dynamic movement increases within the scene.",
                "position": 940
            }
        ]
    },
    {
        "header": "6System Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x9.png",
                "caption": "(a)Prompt: A scene from a disaster movie.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x9.png",
                "caption": "(a)Prompt: A scene from a disaster movie.",
                "position": 1020
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x10.png",
                "caption": "(b)Prompt: A panda bear with distinct black patches climbs and rests on a wooden log platform amid lush, natural foliage.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x11.png",
                "caption": "(c)Prompt: A man performs push-ups on a wooden bench in a sunny park, captured from a side angle in a medium shot.\nThe focus is on his upper body and technique, with natural sunlight accentuating the scene.\nLush greenery and distant park-goers contribute to the energetic, realistic setting.",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x12.png",
                "caption": "(d)Prompt: A playful dog in a pink coat with a red leash dashes across a muddy field with sparse crops.\nThe camera tracks its energetic movement from right to left against a backdrop of trees and distant power lines under an overcast sky.\nThe realistic, medium shot captures a candid, lively moment in soft, diffused light.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2503.09642/x13.png",
                "caption": "(e)Prompt: A drone camera circles a historic church on a rocky outcrop along the Amalfi Coast, highlighting its stunning architecture, tiered patios, and the dramatic coastal views with waves crashing below and people enjoying the scene in the warm afternoon light.",
                "position": 1048
            }
        ]
    },
    {
        "header": "7Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09642/x14.png",
                "caption": "Figure 13:VBench Score Comparison.Our model outperforms open-source text-to-video models by leveraging a text-to-image-to-video generation approach. Additionally, our latest version significantly narrows the performance gap between Open-Sora and OpenAI’s Sora, demonstrating substantial improvements in video generation quality and coherence.",
                "position": 1131
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Contributors",
        "images": []
    }
]