[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/flux_pad.jpeg",
                "caption": "Figure 1:Images generated with FLUX from different segments of the input prompt.\nDescription of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only theprompt tokensandclean padding tokens, (3) An image generated using only theprompt-contextual padsencoded with the prompt, while the prompt tokens were replaced withclean pad tokens.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/method.jpeg",
                "caption": "Figure 2:ITE: Interpreting information within pad tokens in the text encoder. We first encode the full prompt and an clean pads separately. Next, we keep the tokens we want to interpret and replace all other tokens with clean pad tokens. We then generate an image conditioned on this mixed representation. In the example shown here, we interpret the pad tokens in LLaMA-UNet, revealing semantic information embedded within the pad tokens.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/pads_llama_sd3.jpeg",
                "caption": "Figure 3:Images generated from different segments of the input prompt using¬†ITE. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only theprompt tokensandclean padding tokens, (3) An image generated using only theprompt-contextual padsencoded with the prompt, while the prompt tokens were replaced withclean pad tokens.",
                "position": 206
            }
        ]
    },
    {
        "header": "2Analysis of Padding in Text Encoding",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/main_results.png",
                "caption": "Figure 4:Average CLIP score over 5,000 images generated from the different representations:full prompt,only prompt,prompt-contextual padsandclean pads. LDM and LLaMA-UNet are the only models achieving high CLIP scores for images generated from padding tokens, indicating their use during text encoding. See Table4in the Appendix for standard deviations.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/lora_scaled.jpeg",
                "caption": "Figure 5:Images generated from Lavi-bridge with LoRa loaded with scaling factorŒ±ùõº\\alphaitalic_Œ±(y-axis). We analyze pad token segments: the first column shows the full image, and the next columns show three consecutive 20% of the pads. AsŒ±ùõº\\alphaitalic_Œ±decreases, fewer pad tokens are used.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/ca_strength.jpeg",
                "caption": "Figure 6:Attention histogram for Stable Diffusion XL and FLUX* for each token reveals that while both models exclude semantic information from padding tokens, FLUX utilizes these tokens, whereas Stable Diffusion does not. *In FLUX, we have removed the long middle part with low attention in order to improve visualization.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/token_attention_maps.jpeg",
                "caption": "Figure 7:Attention maps for FLUX diffusion show strong alignment between prompt tokens and semantically relevant image tokens. These maps also reveal high attention for padding tokens with the main objects in the image.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/diffusion_causal.jpeg",
                "caption": "Figure 8:IDP: Interpreting information within pad tokens in the diffusion model. We perform a diffusion of two prompts simultaneously: the full prompt and an clean pads. During the diffusion, we keep the tokens we want to interpret (here: the prompt-contextual padding tokens) and replace all other tokens with clean pad tokens. We perform this intervention before each attention block in the diffusion model, through all diffusion steps. We then generate an image conditioned on this mixed representation. In the example shown here, we interpret the pad tokens in FLUX, revealing semantic information embedded within the pad tokens during diffusion.",
                "position": 441
            }
        ]
    },
    {
        "header": "3Analysis of Padding in the Diffusion Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/sid_example.jpeg",
                "caption": "Figure 9:Images generated with FLUX from different prompt segments show distinct alignments: prompt tokens produce semantically accurate images, while the visual nuance like ‚Äôcozy‚Äô emerges only from the prompt-contextual pad tokens.",
                "position": 549
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Creation",
        "images": []
    },
    {
        "header": "Appendix BAttention Between Image and Text in Different Architectures",
        "images": []
    },
    {
        "header": "Appendix CModels",
        "images": []
    },
    {
        "header": "Appendix DTechnical Details",
        "images": []
    },
    {
        "header": "Appendix EQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/causal_diffusion_examples.jpeg",
                "caption": "Figure 10:Additional examples of images generated from different segments of the input prompt using¬†IDP. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens andclean padding tokensthat were not encoded with the prompt, (3) An image generated using only thepadding tokensencoded with the prompt, while the prompt tokens were replaced withclean pad tokens. See Figure8for further technical details.",
                "position": 1224
            },
            {
                "img": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/max_len.jpeg",
                "caption": "Figure 11:Examples of images generated from the same prompts with maximum padding and without padding in Stable Diffusion XL and FLUX. Images generated by Stable Diffusion XL maintain consistent quality, while produced by FLUX without padding often miss key details. For example, given the prompt‚Äúa compass beside a feather,‚Äùimages with padding typically include textured paper with text or a manuscript. In contrast, for the prompt‚Äúa boy visiting a zoo,‚Äùimages generated without padding result in vague animal shapes (first column) or hybrids, such as a mix between a giraffe and a horse (third image). However, adding padding leads to more visually coherent animals.",
                "position": 1227
            }
        ]
    },
    {
        "header": "Appendix FComplementary results",
        "images": []
    }
]