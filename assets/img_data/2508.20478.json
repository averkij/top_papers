[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20478/x1.png",
                "caption": "Figure 1:Overview of the proposed Video-MTR framework.Left:The lower part shows the multi-turn interaction loop between the MLLM agent and the video environment, while the upper part visualizes the collected trajectory and the gated bi-level reward shaping process during optimization.Right:Detailed logs of the agent’s interaction steps across turns. The agent iteratively improves its reasoning by retrieving increasingly relevant frames, ultimately leading to a correct conclusion.",
                "position": 120
            }
        ]
    },
    {
        "header": "Related works",
        "images": []
    },
    {
        "header": "Methods",
        "images": []
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20478/x2.png",
                "caption": "Figure 2:Illustration of Video-MTR’s Multi-turn Reasoning Process, visualizing sampled frames, reasoning process, and model actions per turn. The ground-truth answer is highlighted in orange. The green timeline indicates the positions of sampled frames in the video, reflecting the model’s frame selection strategy at each reasoning turn.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2508.20478/x3.png",
                "caption": "Figure 3:Task Diagnose on MLVU.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2508.20478/x4.png",
                "caption": "Figure 4:Reward hacking example. The red curve shows rising total reward (left) while QA accuracy (right) declines. By contrast, the green curve shows a consistent increase in both total reward and QA accuracy.",
                "position": 742
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20478/x5.png",
                "caption": "Figure 5:Exploration bootstrapping enables multi-turn behavior. With the bonus (pink), rewards grow as multi-turn retrieval is maintained; without it (gray), the policy stabilizes at single-turn reasoning.",
                "position": 1249
            }
        ]
    },
    {
        "header": "Appendix BDatasets",
        "images": []
    },
    {
        "header": "Appendix CCase Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20478/x6.png",
                "caption": "Figure 8:Representative success cases from (A) VideoMME, (B) MLVU, and (C) EgoSchema. The ground-truth answer is highlighted in orange. The green timeline indicates the positions of sampled frames in the video.",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2508.20478/x7.png",
                "caption": "Figure 9:Representative failure cases: (A) action-order reasoning error and (B) fine-grained procedural misrecognition.The ground-truth answer is highlighted in orange. The green timeline indicates the positions of sampled frames in the video.",
                "position": 1492
            }
        ]
    },
    {
        "header": "Appendix DFuture Work",
        "images": []
    }
]