[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x1.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x2.png",
                "caption": "",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x3.png",
                "caption": "",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x4.png",
                "caption": "",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x5.png",
                "caption": "",
                "position": 268
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x6.png",
                "caption": "Figure 1:From pixels to predictive mind.We look beyondlinguistic-onlyunderstanding to envision multimodal intelligence that sees, remembers, and reasons as part of a continuous, lived world.\nIt begins withsemantic perception: naming and describing what is seen.Streaming event cognitiongoes further, enabling always-on sensing across continuous input streams, integrating memory, and supporting proactive responses.Spatial cognitioncaptures the implicit 3D structure of video, enabling reasoning about objects, configurations, and metrics. Finally, apredictive world modelemerges, one that learns passively from experience, updates through prediction and surprise, and retains information for future use.Lower illustration:Video serves as the ideal experimental domain. Models must advance from frame-level Q&A to constructing implicit world models that enable deeper spatial reasoning, scale to unbounded horizons, and achieve supersensing that rivals, and ultimately surpasses, human visual intelligence.",
                "position": 282
            }
        ]
    },
    {
        "header": "2Benchmarking Spatial Supersensing",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x7.png",
                "caption": "Figure 3:Illustrations of how spatial sensing is conceptualized in current video benchmarks. The left panel features examples from the ‘‘spatial reasoning’’ subcategory of VideoMMEfu2025video, including a question regarding gravity from Shutter Authority’s ‘‘What if the Moon Crashed into the Earth?’’ and a question regarding astronaut gear from NASA’s ‘‘Astronaut Bruce McCandless II Floats Free in Space.’’ In contrast, the right panel shows samples from VSI-Benchyang2024think, which highlight visual-spatial reasoning tasks such as object counting, identifying relative directions, route planning, and more.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x8.png",
                "caption": "Figure 4:Illustration of the VSR benchmark’s construction process and format. We use generative models to edit videos by insertingsurprisingor out-of-place objects into the space. The core task then challenges models to recall the spatial placements of these objects in the correct order of their appearance across arbitrarily long videos.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x9.png",
                "caption": "Figure 5:Overview of the VSC benchmark.\nThe benchmark evaluates counting capabilities on long-horizon, multi-room videos composed of concatenated scenes.\nQueries are posed at various time points to simulate a streaming question-answering setting.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x10.png",
                "caption": "Figure 6:Visualization of Gemini-2.5-Flash’s predictionsv.s.ground truth on VSC. The model’s predicted object counts saturate at small constant values and fail to scale with video length or true object counts, indicating limited generalization in counting and reliance on training distribution priors.",
                "position": 566
            }
        ]
    },
    {
        "header": "3Spatial Sensing Under the Current Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x11.png",
                "caption": "Figure 7:VSI-590K data curation pipeline. We collect data from 3D-annotated real and simulated video sources, as well as from pseudo-annotated frames extracted from web videos. We then use diverse templates to automatically generate question–answer pairs for instruction tuning.",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x12.png",
                "caption": "Figure 8:Overall Cambrian-Straining pipeline. Stages 1 and 2 enhance image understanding, stage 3 improves general video understanding, and stage 4 strengthens spatial sensing capability.",
                "position": 1273
            }
        ]
    },
    {
        "header": "4Predictive Sensing as a New Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x13.png",
                "caption": "Figure 9:Training and inference pipeline for the latent frame prediction (LFP) approach.\nOur model employs a Latent Frame Prediction (LFP) head to predict the next frame in latent space.\nDuring training, the LFP head predicts the latent representation of the subsequent video frame.\nDuring inference, the model measures surprise by computing the cosine distance between the LFP head’s prediction and the actual latent features of the subsequent frame.\nThe surprise signal exhibits distinct spikes for events such as the sudden appearance of unusual objects and abrupt scene changes.\nOur predictive-sensing prototype allows Cambrian-Sto generalize to longer videos onVSI-Super, outperforming frontier models (e.g., Gemini-2.5-Flash) that rely solely on context length expansion.",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x14.png",
                "caption": "Figure 10:Surprise-driven memory management framework design.\nThe proposed memory system (a) encodes incoming visual streams, compressing frames with low surprise; (b) performs consolidation when memory is full by dropping or merging the least surprising frames; and (c) retrieves relevant frames during query answering. Color shading (dark→\\rightarrowlight) reflects the degree of surprise, with hatched boxes denoting compressed frames and solid boxes representing uncompressed ones.",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x15.png",
                "caption": "(a)VSR results",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x15.png",
                "caption": "(a)VSR results",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x16.png",
                "caption": "(b)GPU memory usage",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x17.png",
                "caption": "(c)Surprise comparison",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x18.png",
                "caption": "Figure 12:Illustration of our surprise-driven event segmentation framework for VSC.\nThe model continuously accumulates frame features in an event buffer. When a high-surprise frame is detected, the buffered features are summarized to produce a segment-level answer, and the buffer is cleared to start a new segment. This process repeats until the end of the video, after which all segment answers are aggregated to form the final output. Color shading (dark→\\rightarrowlight) reflects the degree of surprise.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x19.png",
                "caption": "(a)VSC results",
                "position": 2258
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x19.png",
                "caption": "(a)VSC results",
                "position": 2261
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x20.png",
                "caption": "(b)Different surprise measurement",
                "position": 2266
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x21.png",
                "caption": "(c)Streaming evaluation",
                "position": 2271
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x22.png",
                "caption": "Figure 14:Cambrian-Sscales to higher ground truth object counts whereas Gemini saturates.\nPredicted counts are plotted against ground-truth counts for videos of different lengths (10, 30, 60, and 120 minutes).\nUsing surprise-driven segmentation, Cambrian-S’s predicted counts grow approximately linearly with the ground-truth, tracking they=xy=xperfect-count line (gray dashed),\nwhereas Gemini-2.5-Flash’s predicted counts remain clustered near small values and fail to increase with ground-truth count, indicating early saturation and poor extrapolation to larger counts.",
                "position": 2283
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Diagnostic Test Results",
        "images": []
    },
    {
        "header": "Appendix BVSI-SuperBenchmark",
        "images": []
    },
    {
        "header": "Appendix CVSI-590K Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x23.png",
                "caption": "Figure 15:VSI-590K dataset statistics.\nQAs are grouped by: question types (left) and task groups (right).",
                "position": 4995
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x24.png",
                "caption": "",
                "position": 5004
            }
        ]
    },
    {
        "header": "Appendix DCambrian-SImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x25.png",
                "caption": "Figure 16:General video instruction tuning datasets of CambrianS-3M, used in Cambrian-Sstage 3 & 4 training.",
                "position": 5404
            }
        ]
    },
    {
        "header": "Appendix ECambrian-SAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.04670/x26.png",
                "caption": "Figure 17:On the trade-off between spatial-sensing and general video understanding.",
                "position": 6936
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x27.png",
                "caption": "",
                "position": 7408
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x28.png",
                "caption": "",
                "position": 7419
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x29.png",
                "caption": "",
                "position": 7430
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x30.png",
                "caption": "",
                "position": 7441
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x31.png",
                "caption": "",
                "position": 7457
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x32.png",
                "caption": "",
                "position": 7496
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x33.png",
                "caption": "",
                "position": 7535
            },
            {
                "img": "https://arxiv.org/html/2511.04670/x34.png",
                "caption": "",
                "position": 7569
            },
            {
                "img": "https://arxiv.org/html/2511.04670/figs/hypersim.jpg",
                "caption": "",
                "position": 7589
            },
            {
                "img": "https://arxiv.org/html/2511.04670/figs/unannotated_image_1.jpg",
                "caption": "",
                "position": 7623
            },
            {
                "img": "https://arxiv.org/html/2511.04670/figs/unannotated_image_2.jpg",
                "caption": "",
                "position": 7654
            }
        ]
    },
    {
        "header": "Appendix FPredictive Sensing",
        "images": []
    }
]