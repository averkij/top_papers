[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01691/fig/framework.jpeg",
                "caption": "Figure 1:MedQ-Benchoverview, evaluating MLLMsâ€™ abilities in medical image quality assessment with: (1) Comprehensive coverage: 3,308 samples across 5 modalities with 40+ degradation types. (2) Multi-faceted evaluation: perception-reasoning paradigm.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x1.png",
                "caption": "Figure 2:Comparison of Reasoning IQA with score-based IQA. Unlike purely numerical scores, Reasoning IQA identifies distortion types and their relative impact, yielding results more consistent with human judgment.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Constructing the MedQ-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01691/x2.png",
                "caption": "Figure 3:Examples of question types in MedQ-Bench, covering MCQA perception tasks (Yes-No / What / How), open-ended reasoning, and pair/multi-image comparison.",
                "position": 201
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01691/x3.png",
                "caption": "Figure 4:Overall Performance Results",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x4.png",
                "caption": "Figure 5:Performance analysis of MLLMs across different evaluation dimensions. (a) Different degradation level performance . (b) General vs modality-specific question.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x5.png",
                "caption": "",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x6.png",
                "caption": "Figure 6:Comparative reasoning performance analysis. Left: Detailed performance scores across three evaluation dimensions for all models. Right: Visual comparison of overall performance patterns across model categories.",
                "position": 583
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01691/fig/qa_html.png",
                "caption": "Figure 7:Interface for the MedQ-MCQA dataset.",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2510.01691/fig/description_html.png",
                "caption": "Figure 8:Interface for the MedQ-Reasoning dataset.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x7.png",
                "caption": "Figure 9:Distribution of low-level attributions across imaging modalities and distortion types in MedQ-Bench.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x8.png",
                "caption": "Figure 10:Representative QA examples demonstrating typical question-answer patterns in MedQ-Bench across different medical imaging modalities and quality assessment scenarios.",
                "position": 2876
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x9.png",
                "caption": "Figure 11:Representative no-reference reasoning image examples demonstrating typical question-answer patterns in MedQ-Bench across different models.",
                "position": 2884
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x10.png",
                "caption": "Figure 12:Representative paired image examples demonstrating typical question-answer patterns in MedQ-Bench across different models.",
                "position": 2887
            },
            {
                "img": "https://arxiv.org/html/2510.01691/x11.png",
                "caption": "Figure 13:Confusion matrices showing alignment between human expert scores and GPT-4o automated evaluation across four evaluation dimensions.",
                "position": 2909
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]