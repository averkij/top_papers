[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05193/x1.png",
                "caption": "Figure 1:Performance comparison ofreference-freeandreference-basedevaluation paradigms across different similarity groups in MT-Bench, using GPT-4-as-a-Judge.\nIn the reference-based evaluation, the GPT-4 direct response is used as the reference, and the evaluated response with a higher BERTScore with the reference is regarded as the preferred one.\nAs the similarity between the reference and the response increases, the human agreement accuracy of the reference-based evaluation significantly improves, while the reference-free evaluation maintains relatively consistent performance across all similarity levels.",
                "position": 180
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05193/x2.png",
                "caption": "Figure 2:Illustration of our proposedRevisEval. Given an instance (x,y,aùë•ùë¶ùëéx,y,aitalic_x , italic_y , italic_a), we useRevisEvalto assessyùë¶yitalic_yin rubricaùëéaitalic_a. InRevisEval, (i)\nreviser generates a response-adapted referencer‚ãÜsuperscriptùëü‚ãÜr^{\\star}italic_r start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPTby revising theyùë¶yitalic_yto\nenhance the (ii) following LLM-as-a-Judge, even classic text metrics.",
                "position": 236
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05193/x3.png",
                "caption": "Figure 3:Comparative analysis of reference-based metrics performance using references generated byHuman/GPT-4andRevisEvalon NLG and instruction following benchmarks.RevisEvalcan greatly enhance traditional reference-based metrics, even n-gram metrics, reaching performance levels comparable to GPT-4-as-a-Judge in reference-free evaluations.",
                "position": 704
            }
        ]
    },
    {
        "header": "5Case Study: How does it work?",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Template",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05193/x4.png",
                "caption": "Figure 4:The prompt of reference-free pairwise comparison evaluation.",
                "position": 1881
            },
            {
                "img": "https://arxiv.org/html/2410.05193/x5.png",
                "caption": "Figure 5:The prompt of reference-based pairwise comparison evaluation.",
                "position": 1884
            },
            {
                "img": "https://arxiv.org/html/2410.05193/x6.png",
                "caption": "Figure 6:The prompt of reference-free score rating evaluation.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2410.05193/x7.png",
                "caption": "Figure 7:The prompt of reference-based score rating evaluation.",
                "position": 1890
            },
            {
                "img": "https://arxiv.org/html/2410.05193/x8.png",
                "caption": "Figure 8:The prompt of LLM-as-a-reviser for pairwise comparison.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2410.05193/x9.png",
                "caption": "Figure 9:The prompt of LLM-as-a-reviser for score rating.",
                "position": 1896
            },
            {
                "img": "https://arxiv.org/html/2410.05193/x10.png",
                "caption": "Figure 10:The prompt of LLM direct response to instruction.",
                "position": 1899
            }
        ]
    },
    {
        "header": "Appendix BInference Setting for Proprietary Model",
        "images": []
    },
    {
        "header": "Appendix CFinetuning Setting for Open-sourced Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05193/x11.png",
                "caption": "Figure 11:In our data distillation process for open-source LLMs, we utilize HH-rlhf and MetricInstruct as the primary data sources. We then employ a proprietary LLM to perform RevisEval, generating both the revisions and corresponding evaluation outputs. Finally, we fine-tune the open-source LLM using this enriched dataset.",
                "position": 1970
            }
        ]
    },
    {
        "header": "Appendix DBenchmarks",
        "images": []
    },
    {
        "header": "Appendix EBaselines in NLG Evaluation Tasks",
        "images": []
    },
    {
        "header": "Appendix FBaselines in Instruction Following Preference Benchmarks",
        "images": []
    },
    {
        "header": "Appendix GMeta Evaluation",
        "images": []
    },
    {
        "header": "Appendix HPearson Correlation in NLG Evaluation Tasks",
        "images": []
    },
    {
        "header": "Appendix IOther Analysis",
        "images": []
    }
]