[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07106/figs/motivation.png",
                "caption": "Figure 1:Overview of the motivation behind Ex-Omni. It supports any combinations of textual and speech inputs, and is capable of unified generation of multimodal outputs, including text, speech, and 3D facial animation.",
                "position": 194
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07106/x1.png",
                "caption": "Figure 2:Model architecture of EX-Omni.",
                "position": 237
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07106/x2.png",
                "caption": "Table 2:Performance comparison of 3D facial animation generation in dialogue scenes.↓\\downarrowindicates lower is better. Note:\nEx-Omni+Task-specific S2F model adopt a two-stage pipeline, where Ex-Omni generates speech responses and the output audio is subsequently used as input to a S2F model. In contrast, Native Ex-Omni directly generates facial animation within a unified framework.",
                "position": 558
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07106/x2.png",
                "caption": "Table 4:Speech-to-Text performance comparison on VoiceBench.↑\\uparrowmeans higher is better.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2602.07106/x2.png",
                "caption": "Figure 3:Case study on 3D facial animation generation. The figure highlights mouth-opening behaviors aligned with phonemes that require large lip movements. (a) Results generated from English speech; (b) Results generated from Chinese speech. “[…]” indicates omitted content for brevity, and parenthetical annotations denote dominant articulation cues.",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2602.07106/x3.png",
                "caption": "Table 5:Text-to-Speech performance comparison on Seed-TTS-Eval.↓\\downarrowmeans lower is better.",
                "position": 944
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07106/x3.png",
                "caption": "Figure 4:Loss curves on different stages with different parameters’ LLMs.",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2602.07106/figs/audio_duration.png",
                "caption": "Figure 5:Response audio duration distribution.",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2602.07106/figs/s2s_wer.png",
                "caption": "Figure 6:Average WER distribution across different audio durations.",
                "position": 1929
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]