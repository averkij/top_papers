[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14255/x1.png",
                "caption": "",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video Mask-to-Matte Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14255/x2.png",
                "caption": "Figure 2:Overview of VideoMaMa architecture.RGB frames and guide masks are processed through video diffusion U-Net layers to generate high-quality video mattes. Semantic injection with DINO features is applied during training.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2601.14255/x3.png",
                "caption": "Figure 3:Examples of mask augmentation methods.Polygon and Downsampling degradation are applied at weak and strong augmentation levels.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Matte Anything in Videos (MA-V) Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14255/x4.png",
                "caption": "Figure 4:Qualitative examples from our MA-V dataset.We show RGB frames with our high-quality MA-V annotations and original SA-V[30]masks for comparison. MA-V provides refined alpha mattes for diverse scenarios.",
                "position": 349
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14255/x5.png",
                "caption": "Figure 5:Qualitative comparison on in-the-wild videos.We evaluate two settings: (1) all-frame mask-guided video matting where VideoMaMa is compared against MaGGIe[14], and (2) first-frame mask-guided matting where SAM2-Matte is compared against MatAnyone[48]. All methods use SAM2[30]to generate mask inputs.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2601.14255/x6.png",
                "caption": "Figure 6:Comparison on a real video between MatAnyone[48]and SAM2-Matte variants.Red arrows highlight regions where our method achieves the best results.",
                "position": 975
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "BEvaluation Metric Details",
        "images": []
    },
    {
        "header": "CComparison with SAM2",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14255/x7.png",
                "caption": "Figure 7:Comparison on a real video between SAM2-Matte and SAM2[30]with sigmoid function on mask logit to simulate alpha matte generation. ED denotes that existing dataset.",
                "position": 1171
            }
        ]
    },
    {
        "header": "DLimitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14255/x8.png",
                "caption": "Figure 8:Limitations.As highlighted by the red arrows, the model struggles to refine the matte when the input guidance mask exhibits significant errors from the input mask.",
                "position": 1181
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]