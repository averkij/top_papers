[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07832/figs/icons/globe.png",
                "caption": "",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2601.07832/figs/icons/github.png",
                "caption": "",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2601.07832/figs/icons/hf.png",
                "caption": "",
                "position": 179
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07832/x1.png",
                "caption": "Figure 2:Comparison between the proposed MHLA and other linear attentions.MHLA divides multiple heads on the token dimension. Through Multi-Head Mixing, MHLA restores query-conditioned selectivity by mixing KV summaries with query-specific weight, improving token-level diversity\nwhile keeping linear complexity.",
                "position": 247
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Analysis of Linear Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07832/x2.png",
                "caption": "(a)",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x2.png",
                "caption": "(a)",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x3.png",
                "caption": "(b)",
                "position": 365
            }
        ]
    },
    {
        "header": "4Multi-Head Linear Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07832/x4.png",
                "caption": "(a)",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x4.png",
                "caption": "(a)",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x5.png",
                "caption": "(b)",
                "position": 427
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07832/x6.png",
                "caption": "Table 4:Comparison on T2I models.",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x6.png",
                "caption": "Figure 5:Loss comparison.",
                "position": 1109
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x7.png",
                "caption": "Table 5:MHLA in Video Generation. Wan-FA indicates a pretrained Wan2.1-1.3B. Wan-MHLA and Wan-LA replace all layers with MHLA and Linear Attention, respectively. Wan-MHLA-H only replaces 2/3 layers.",
                "position": 1115
            },
            {
                "img": "https://arxiv.org/html/2601.07832/x7.png",
                "caption": "Figure 6:Loss comparison on Wan-2.1-1.3B. MHLA shows a much stronger convergence capability.",
                "position": 1169
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFull Related Works",
        "images": []
    },
    {
        "header": "Appendix BQuery-Conditioned Selectivity in Softmax Attention",
        "images": []
    },
    {
        "header": "Appendix CMHLA for Autoregressive Modeling",
        "images": []
    },
    {
        "header": "Appendix DDataset",
        "images": []
    },
    {
        "header": "Appendix EExtra Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FComplete Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07832/x8.png",
                "caption": "Figure 7:More generation results from our fine-tuned SANA-MHLA model.",
                "position": 2242
            }
        ]
    },
    {
        "header": "Appendix GClarification on Terminology and Computational Concepts",
        "images": []
    },
    {
        "header": "Appendix HLLM Usage.",
        "images": []
    }
]