[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05091/logo/paper_logo_circle.png",
                "caption": "",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x1.png",
                "caption": "Figure 1:Overview of our work.Left:We showcase the diverse text-to-image (T2I) and editing examples from our dataset. In contrast to natural images, modeling structured visual demands sophisticated composition planning, strong multimodal understanding, and precise text rendering, as highlighted by the three key characteristics.Right:Our model demonstrates competitive performance against leading closed-source systems in both structured image generation and editing benchmarks.",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Structured Image Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05091/x2.png",
                "caption": "Figure 2:Data construction pipeline.We prompt GPT-5 to extract salient features, then generate paired editing instructions from the source code and rendered image. The source code is modified according to the code-editing instructions. The target image rendered from modified code is passed through rule-based filters to ensure the overall quality of the constructed dataset.",
                "position": 146
            }
        ]
    },
    {
        "header": "4StructBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05091/x3.png",
                "caption": "Figure 3:Benchmark construction and evaluation workflow.(a) Benchmark construction: We cluster the data into six categories, and for each editing and text-to-image (T2I) example, GPT-5 generates detailed image descriptions that are transformed into Q&A pairs for evaluating diverse visual aspects. (b) Evaluation protocol: Using the Q&A pairs, GPT-5 is queried on generated images for open-ended responses, which are compared with ground-truth answers to yield a final score.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x4.png",
                "caption": "Figure 4:Comparison of the initial and revised atomic Q&A pairs. Initial Q&A pairs sometimes entangle multiple attributes, hindering unambiguous verification and accurate scoring. Enforcing atomicity,i.e., one attribute or relation per Q&A, substantially improves metric reliability.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x5.png",
                "caption": "Figure 5:Statistical analysisof our dataset (a-c) and benchmark (d-f).",
                "position": 220
            }
        ]
    },
    {
        "header": "5Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05091/x6.png",
                "caption": "Figure 6:The three-stage progressive training pipeline.Training difficulty increases across stages, from alignment to hybrid visual learning and thinking enhancement. More training details are presented in AppendixA.1.",
                "position": 228
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05091/logo/math.png",
                "caption": "Table 1:Quantitative comparison on StructEditBench.The table showcases the results of Accuracy (%) and PSNR (with ground-truth target image) for various methods.indicate the first and second Accuracy results, respectively.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/graph.png",
                "caption": "",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/puzzle.png",
                "caption": "",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/science.png",
                "caption": "",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/table.png",
                "caption": "",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/Overall.png",
                "caption": "",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/chart.png",
                "caption": "Table 2:Quantitative comparison on StructT2IBench, reporting Accuracy (%).",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/convert.png",
                "caption": "Table 3:Quantitative comparison on StructEditBench (charts only).indicate the first and second Accuracy results, respectively.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/color.png",
                "caption": "",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/num.png",
                "caption": "",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/aux.png",
                "caption": "",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/adddel.png",
                "caption": "",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x7.png",
                "caption": "Figure 7:Pearson correlation analysisamong Accuracy, PSNR, and human Elo score for editing models.",
                "position": 999
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x7.png",
                "caption": "Figure 7:Pearson correlation analysisamong Accuracy, PSNR, and human Elo score for editing models.",
                "position": 1002
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x8.png",
                "caption": "Figure 8:Study of explicit reasoningadded to different models.",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x9.png",
                "caption": "Figure 9:Case study: explicit reasoning improves fidelity.When dealing with complex tasks, direct generation often leads to plausible yet incorrect images. Incorporating a reasoner to explicitly describe, analyze, and predict enables the generator to produce faithful, correct outputs.",
                "position": 1013
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.05091/x10.png",
                "caption": "Figure 10:Progressive training pipeline.For the Unified Alignment stage, only newly added MLP parameters are updated. Both MMDiT and MLP parameters are updated during the Hybrid Visual Learning and Thinking Enhancement stages.",
                "position": 2025
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/math.png",
                "caption": "Table 5:Quantitative comparison on StructEditBench with Qwen2.5-VL-72B evaluator.The table showcases the results of Accuracy (%), PSNR and SSIM (with GT image) for various methods.indicate the first and second Accuracy results, respectively.",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/convert.png",
                "caption": "Table 6:Quantitative comparison of different methods on StructEditBench (Chart only) with Qwen2.5-VL-72B evaluator.indicate the first and second Accuracy results, respectively.",
                "position": 2519
            },
            {
                "img": "https://arxiv.org/html/2510.05091/logo/chart.png",
                "caption": "Table 7:Quantitative comparison on StructT2IBench with Qwen2.5-VL-72B evaluators, reporting Accuracy (%).",
                "position": 2860
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x11.png",
                "caption": "Figure 11:Pearson correlation analysisamong Accuracy (Qwen2.5-VL-72B evaluator), PSNR, SSIM and human Elo score for image editing.",
                "position": 3324
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x12.png",
                "caption": "Figure 12:Correlation of our metric with human preference under different weighting ratios.",
                "position": 3327
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x13.png",
                "caption": "Figure 13:Qualitative comparison for structured image editing.“GT” stands for ground truth.",
                "position": 3347
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x14.png",
                "caption": "Figure 14:Qualitative comparison for structured image generation.“GT” stands for ground truth.",
                "position": 3351
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x15.png",
                "caption": "Figure 15:Prompt constructing the image editing pairs with code commands.",
                "position": 3363
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x16.png",
                "caption": "Figure 16:Prompt to obtain captions for non-natural STEM data.",
                "position": 3366
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x17.png",
                "caption": "Figure 17:Prompt to obtain reasoning traces for the editing task of non-natural STEM images.",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x18.png",
                "caption": "Figure 18:Prompt for evaluation.",
                "position": 3372
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x19.png",
                "caption": "Figure 19:Prompt for constructing question-answer pairs for our benchmark.",
                "position": 3375
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x20.png",
                "caption": "Figure 20:Prompt to refine the question-answer pairs.",
                "position": 3378
            },
            {
                "img": "https://arxiv.org/html/2510.05091/x21.png",
                "caption": "Figure 21:Prompt to check if ground-truth and mdoel response are similar.",
                "position": 3381
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]