[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/local_heuristics.png",
                "caption": "Figure 1:Core logic of the Local Heuristic.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/global_heristics.png",
                "caption": "Figure 2:Core logic of the Global Heuristic.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Methodology: Semantic Grafting via TokenAdapt",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09738/x1.jpg",
                "caption": "Figure 3:Core logic of the Local and Global Heuristics respectively. This diagram illustrates the two main pathways (Local and Global) for generating components of a new tokenâ€™s embedding, which are then combined via Hybrid Integration.",
                "position": 445
            }
        ]
    },
    {
        "header": "4Experimental Setup & Results",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/wordcountgraph.png",
                "caption": "Figure 4:Comparison of word count distributions for unique token types utilized by the Supertoken (ST, Red) and Baseline (Base, Blue) tokenizers across different domain corpora (10k samples each). Subplots show results for (a) English, (b) Hindi, (c) Math, and (d) Code. The Y-axis represents the log count of unique token types observed containing the specified number of words (X-axis).",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/gpt4o.png",
                "caption": "Table 3:Comparison of text segmentation by different tokenization strategies.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/llama3.png",
                "caption": "",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/superbpe.png",
                "caption": "",
                "position": 1273
            },
            {
                "img": "https://arxiv.org/html/2505.09738/extracted/6439712/graph/adibun.png",
                "caption": "",
                "position": 1282
            }
        ]
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    }
]