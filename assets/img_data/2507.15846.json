[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15846/extracted/6639713/figures/logo.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x1.png",
                "caption": "Figure 1:GUI grounding performance and human click behavior. Left: Performance comparison of various models on ScreenSpot-Pro. Right: Human click distribution from AITW(Rawles et¬†al.,2023)reveals natural Gaussian patterns around target centers (Œº=0.111ùúá0.111\\mu=0.111italic_Œº = 0.111,œÉ=0.429ùúé0.429\\sigma=0.429italic_œÉ = 0.429), validating our design choice of continuous Gaussian rewards over discrete binary feedback.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x1.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x2.png",
                "caption": "",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x3.png",
                "caption": "Figure 2:Comparison of reward modeling strategies. (a-c) Existing methods treat GUI elements as abstract points with binary or distance-based rewards, while (d) our Gaussian approach provides continuous point and coverage rewards that naturally align with human clicking behavior.",
                "position": 150
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15846/x4.png",
                "caption": "Figure 3:GUI Gaussian Grounding Rewards (GUI-G2).Our framework transforms GUI grounding through continuous Gaussian modeling. Given a task instruction and screenshot, the policy model generates multiple predictions that are evaluated using our dual reward mechanism. Gaussian Point Rewards assess localization precision while Gaussian Coverage Rewards measure spatial overlap, together providing dense learning signals that guide policy optimization.",
                "position": 223
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15846/x5.png",
                "caption": "(a)Sparse reward training dynamics.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x5.png",
                "caption": "(a)Sparse reward training dynamics.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x6.png",
                "caption": "(b)Distance convergence comparison.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x7.png",
                "caption": "Figure 5:Hyperparameter sensitivity analysis for adaptive sigma (œÉùúé\\sigmaitalic_œÉ). Performance peaks atŒ±=0.5ùõº0.5\\alpha=0.5italic_Œ± = 0.5with 93.3% accuracy on Screenspot-v2.",
                "position": 1412
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x7.png",
                "caption": "Figure 5:Hyperparameter sensitivity analysis for adaptive sigma (œÉùúé\\sigmaitalic_œÉ). Performance peaks atŒ±=0.5ùõº0.5\\alpha=0.5italic_Œ± = 0.5with 93.3% accuracy on Screenspot-v2.",
                "position": 1415
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x8.png",
                "caption": "Figure 6:Ablation of Gaussian Component. Both Point and Coverage components contribute to the final 93.3% performance.",
                "position": 1420
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15846/x9.png",
                "caption": "Figure 8:Performance comparison between random reward strategies on ScreenSpot-V2. Both continuous randomU‚Å¢(0,1)ùëà01U(0,1)italic_U ( 0 , 1 )rewards and binary random rewards show progressive degradation, demonstrating that GUI grounding requires spatially-meaningful reward signals rather than arbitrary feedback.",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x10.png",
                "caption": "Figure 9:Reward distribution and standard deviation analysis during training. The reward variance patterns illustrate the fundamental differences between continuous and binary random reward mechanisms in reinforcement learning dynamics.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x11.png",
                "caption": "(a)Information-Dense Interface",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x11.png",
                "caption": "(a)Information-Dense Interface",
                "position": 2497
            },
            {
                "img": "https://arxiv.org/html/2507.15846/x12.png",
                "caption": "(b)Visual and Structural Ambiguity",
                "position": 2502
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]