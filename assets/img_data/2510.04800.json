[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04800/assets/overview/hybrid_overview.png",
                "caption": "(a)Overview of hybrid architecture",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/overview/hybrid_overview.png",
                "caption": "(a)Overview of hybrid architecture",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/overview/pareto_frontier.png",
                "caption": "(b)Pareto frontier of throughput",
                "position": 210
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Hybrid Architecture",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04800/assets/training_efficiency/legend.png",
                "caption": "(a)FLOPs per sample (↓\\downarrow)",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/training_efficiency/legend.png",
                "caption": "",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/training_efficiency/train_flops.png",
                "caption": "(a)FLOPs per sample (↓\\downarrow)",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/training_efficiency/train_time.png",
                "caption": "(b)End-to-end step time (↓\\downarrow)",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/training_efficiency/train_memory.png",
                "caption": "(c)Memory need (↓\\downarrow)",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/inference_efficiency/legend.png",
                "caption": "(a)Throughput (↑\\uparrow)",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/inference_efficiency/legend.png",
                "caption": "",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/inference_efficiency/inference_throughput.png",
                "caption": "(a)Throughput (↑\\uparrow)",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/inference_efficiency/inference_cache_size.png",
                "caption": "(b)Cache sizes (↓\\downarrow)",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/pg19_long_context/pg19_long_context.png",
                "caption": "(c)NLL on PG19 (↓\\downarrow)",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/needle/needle_heatmap_transformer.png",
                "caption": "(a)Llama",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/needle/needle_heatmap_transformer.png",
                "caption": "(a)Llama",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/needle/needle_heatmap_swa_transformer.png",
                "caption": "(b)Sliding window",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/needle/needle_heatmap_mamba.png",
                "caption": "(c)Mamba",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/needle/needle_heatmap_inter_hybrid.png",
                "caption": "(d)Inter-hybrid",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/needle/needle_heatmap_intra_hybrid.png",
                "caption": "(e)Intra-hybrid",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/scaling_laws/optimal_line.png",
                "caption": "Table 3:(Left)All architectures consistently achieve significant quality gains from MoE integration.All models are trained on 60B tokens, with hybrid models using a 1:5 block ratio. We use one shared expert and the selected top-1 expert among eight. Additionally, a token-choice router with loss-free balancing algorithm is utilized(Wang et al.,2024b; DeepSeek-AI,2024).(Right)Hybrid architectures demonstrates a compute-optimal scaling line that lies between those of Transformer and Mamba.We train models at four different scales—100M, 350M, 1B, and 3B—across five compute budgets. Each marker indicates the optimal model size for a given compute budget. For hybrid models, we use a 1:5 block ratio.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/inter_ablation/inter_hybrid_ablation.png",
                "caption": "Table 4:(Left)Inter-layer hybrid achieves the best quality at a 1:1 block ratio, but the 1:5 ratio is preferable to balance efficiency and quality.Transformer blocks are evenly distributed in the middle. All models are trained on 60B tokens.(Right)Interleaving Transformer blocks at intermediate depths is key for optimal performance.The upper figure shows results of ablating the position of a single Transformer block in 1B (13 layers) and 350M (11 layers) models with a 1:12 ratio. In the lower figure, after distributing Transformer blocks in the middle of 1B model, we move the first block to the first layer (Front) or the last block to the last layer (End). All models are trained on 60B tokens.",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2510.04800/assets/intra_ablation/intra_hybrid_ablation.png",
                "caption": "Table 5:(Left)We identify a more optimal architecture than previous designs through ablation studies on intra-hybrid block.We train a 1B model on 60B tokens, replacing all layers with intra-hybrid blocks (i.e., 1:0 block ratio). All heads are split into two, with each linked to a half-sized primitive.(Right) Keeping larger Transformer dimension within intra-hybrid block improves quality (despite reduced efficiency), and placing the intra-hybrid block in the middle yields the best results.All 1B models with the optimal architecture from left, are trained on 60B tokens. In the figure above, we use a block ratio of 1:0, where all layers become intra-hybrid blocks. For lower figure, we place intra-hybrid blocks in the middle of 1B model by: placing consecutively (Cluster), distributing evenly (Scatter), or placing at the beginning and end as well (Sandwich).",
                "position": 1219
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Detailed Experimental Results",
        "images": []
    },
    {
        "header": "7The Use of Large Language Models",
        "images": []
    },
    {
        "header": "8Details for Computational and Memory Costs Comparison",
        "images": []
    },
    {
        "header": "9Detailed Experimental Setup",
        "images": []
    }
]