[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05552/x1.png",
                "caption": "Figure 1:We consolidate diverse navigation tasks into a unified language-guided navigation framework sorted by language granularity. Previous approaches utilize task-specific designs tailored to address particular types of language instructions, as shown in (a) and (b). In contrast, we propose a versatile system that can interpret and execute arbitrary language instructions as shown in (c).",
                "position": 113
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Mixture of Experts for Versatile Language-guided Visual Navigation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05552/extracted/6049815/figures/pipeline.jpg",
                "caption": "Figure 2:Illustration of MoE position and experts’ routing methods. SAME routing based on multimodal features from visual observations and language instructions allows the agent to dynamically adapt to environmental visual changes.",
                "position": 426
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AObjectNavand VLN Training",
        "images": []
    },
    {
        "header": "Appendix BDUET Revisit",
        "images": []
    },
    {
        "header": "Appendix CFull Results on All VLN Tasks",
        "images": []
    }
]