[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/x1.png",
                "caption": "Figure 1:Diverse text-to-image generation with\\model. (Zoom in for more details)",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x2.png",
                "caption": "Figure 2:Diverse textual image editing and image reference workflows with\\model. All images encoded by VLM features only, no VAE tokens involved. (Zoom in for more details)",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x3.png",
                "caption": "(a)Zero-shot reasoning. Our\\rewriteparadigm allows\\modelto leverage the world knowledge and reasoning of the VLM encoder.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x3.png",
                "caption": "(a)Zero-shot reasoning. Our\\rewriteparadigm allows\\modelto leverage the world knowledge and reasoning of the VLM encoder.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x4.png",
                "caption": "(b)Zero-shot multi-reference generations for\\modelmodel only trained on a single-reference samples.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2510.12789/figures/final_arch_diagram_v3.png",
                "caption": "Figure 4:\\modelarchitecture and inference paradigm. We extract multimodal representations from multiple layers of a frozen LLM and aggregate with a learnable layerwise attention pooling (LAP) module. A subsequent refiner counteracts the VLM’s position bias due to causal attention. VLM-Enabled Rewriting Injection with Flexible Inference (\\rewrite) rewrites the original input in-context. The rewritten tokens used for DiT conditioning leverage the VLM’s reasoning capabilities to contextualize the target scene into a unified representation.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Architecture Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/x5.png",
                "caption": "(a)Last-Layer Hidden State",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x5.png",
                "caption": "(a)Last-Layer Hidden State",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x5.png",
                "caption": "(a)Last-Layer Hidden State",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x6.png",
                "caption": "(b)Layerwise Key-Value Fusion",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x7.png",
                "caption": "(c)Hidden State Injection",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x8.png",
                "caption": "(d)Layerwise Attention Pooling (Ours)",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x9.png",
                "caption": "Figure 6:Comparison of different unified encoder candidates after 200k training steps on text-to-image performance (measured by VQA Score[24]on GenAI-Bench[23]). LAP stands out as the best fusion strategy, whereas Key-Value Fusion consistently exhibits the lowest performance. Neither naive drop-in replacement of Llama-3.1 surpasses the T5 baseline.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x10.png",
                "caption": "(a)Qualitative Examples",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x10.png",
                "caption": "(a)Qualitative Examples",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x11.png",
                "caption": "(b)Quantitative Reconstruction Quality",
                "position": 392
            }
        ]
    },
    {
        "header": "3\\modelDesign",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/x12.png",
                "caption": "Figure 8:Weight visualization of LAP modules’ pooling layers in Representation Injection setup (Sec2.2). Each value denotes the magnitude of weights assigned to each VLM layer at a given LAP module’s pooling layer (smaller y-coordinate denotes layers closer to DiT input). On average, early VLM layers contribute more than later ones, while layer injection at later DiT blocks has lower weights.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x13.png",
                "caption": "Figure 9:Qualitative example of local clusters in LAP Key-Value activations norm. On many tokens, the model utilizes implicit clusters of adjacent layers. Values are averaged over tokens if a word has more than 1 token.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x14.png",
                "caption": "Figure 10:Adjacent layers in the VLM produce highly similar representations. Consequently, extracting features across each layer in the residual stream gives redundant information.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x14.png",
                "caption": "Figure 10:Adjacent layers in the VLM produce highly similar representations. Consequently, extracting features across each layer in the residual stream gives redundant information.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x15.png",
                "caption": "Figure 11:An LAP using only every 3rd layer of the encoder VLM learns more uniform layer weights.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x16.png",
                "caption": "Figure 12:Qualitative analysis of different layer impact on final image. We drop crossed-out layers in LAP aggregation. Middle layers are crucial to capture the overall scene composition. In contrast first and last layers only capture rudimentary aspects of the scene.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x17.png",
                "caption": "(a)Qualitative Examples",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x17.png",
                "caption": "(a)Qualitative Examples",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x18.png",
                "caption": "(b)GenAI Bench Evaluation",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x19.png",
                "caption": "Figure 14:Qualitative comparison on long form text-to-image generation prompts comparing\\modelto Bagel[10], Flux.1 [dev][20]and Qwen-Image[36].",
                "position": 686
            }
        ]
    },
    {
        "header": "4Final\\modelModel",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/x20.png",
                "caption": "Figure 15:Qualitative comparison on image reference and editing tasks comparing\\modelto Bagel[10], Flux.1 Kontext[21]and Qwen-Image[36]. (Zoom in for details)",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x21.png",
                "caption": "Figure 16:The unified VLM encoder enables advanced visual reasoning for textual image editing. (Examples on early checkpoint and not indicative of final model quality)",
                "position": 946
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x22.png",
                "caption": "(a)Zero-shot image-to-image generation. Examples generated by a model only trained on text-to-image generation. When presented with image features, the model captures overall scene composition and a high level of detail.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x22.png",
                "caption": "(a)Zero-shot image-to-image generation. Examples generated by a model only trained on text-to-image generation. When presented with image features, the model captures overall scene composition and a high level of detail.",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x23.png",
                "caption": "(b)Zero-shot image editing. Examples generated by a model never trained on image editing.",
                "position": 959
            }
        ]
    },
    {
        "header": "5Emergent Abilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/figures/capability_transfer_ab_test_v2.png",
                "caption": "Figure 18:UniFusion-Edit leads UniFusion-Base by a significant margin in text-to-image A/B test with 180 annotators, 616 prompts across diverse concepts, with 2 seeds each (3 votes per image pair).",
                "position": 994
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/x24.png",
                "caption": "Figure 19:\\modeladapter for layerwise representation aggregation. Representations from multiple VLM layers are aggregated using a Layerwise Attention Pooling (LAP). The aggregated representations are subsequently passed through a Refiner to mitigate position bias.",
                "position": 2128
            }
        ]
    },
    {
        "header": "AAdditional Results & Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12789/x25.png",
                "caption": "(a)Qualitative Examples",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x25.png",
                "caption": "(a)Qualitative Examples",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x26.png",
                "caption": "(b)GenAI Bench Evaluation",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x27.png",
                "caption": "(a)Qualitative Examples",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x27.png",
                "caption": "(a)Qualitative Examples",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x28.png",
                "caption": "(b)GenAI Bench Evaluation",
                "position": 2171
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x29.png",
                "caption": "(a)Qualitative Examples",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x29.png",
                "caption": "(a)Qualitative Examples",
                "position": 2181
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x30.png",
                "caption": "(b)GenAI Bench Evaluation",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x31.png",
                "caption": "(a)Text-to-image examples generated with\\model-Gemma using self-rewrite.",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x31.png",
                "caption": "(a)Text-to-image examples generated with\\model-Gemma using self-rewrite.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x32.png",
                "caption": "(b)Image reconstruction with\\model-Gemma at 1 input tile. Similar to the experiments in Sec.2.2, we observe slight variations when using only one tile. We expect these artifacts to resolve themselves at increased input resolution.",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x33.png",
                "caption": "(a)Examples of incorrect object, attribute, and count assessments in GenEval.",
                "position": 2284
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x33.png",
                "caption": "(a)Examples of incorrect object, attribute, and count assessments in GenEval.",
                "position": 2287
            },
            {
                "img": "https://arxiv.org/html/2510.12789/x34.png",
                "caption": "(b)Examples of Question-Answering failures in DPG-Bench assessments.",
                "position": 2293
            }
        ]
    },
    {
        "header": "BOn the reliability of Image Generation Benchmarks",
        "images": []
    }
]