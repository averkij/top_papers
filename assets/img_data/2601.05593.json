[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05593/x1.png",
                "caption": "Figure 1:ParallelCoordinatedReasoning (PaCoRe) performance.Left:On HMMT 2025, PaCoRe-8B demonstrates remarkable test-time scaling by increasing both parallel trajectories and coordinated rounds, yielding steady gains and ultimately surpassing GPT-5.Right:On LiveCodeBench, the RLVR-8B model fails to leverage increased test-time compute, while PaCoRe-8B model effectively unlocks this synthesis capability, yielding substantial gains as the test-time compute increases.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2601.05593/x1.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2601.05593/x2.png",
                "caption": "",
                "position": 164
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05593/x3.png",
                "caption": "Figure 2:Inference pipeline of PaCoRe.Each round launches broad parallel exploration, compacts the resulting trajectories into compacted messages, and feeds these messages together with the question forward to coordinate the next round.\nRepeating this processR^\\hat{R}times yields multi-million-token effective TTC while respecting fixed context limits, with the final compacted message serving as the system’s answer.",
                "position": 259
            }
        ]
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05593/x4.png",
                "caption": "Figure 3:PaCoRe Training dynamics.Left panels:The Training Reward and Response Length steadily increase, demonstrating the training stability and effectiveness.Right panels:Evaluation on HMMT 2025 and LiveCodeBench (2408-2505).\nPerformance is reported using single round coordinated reasoning in PaCoRe inference setting withK→=[16]\\vec{K}=[16].",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2601.05593/x4.png",
                "caption": "",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2601.05593/x5.png",
                "caption": "",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2601.05593/x6.png",
                "caption": "Figure 4:Ablation of parallel reasoning and message passing.Left:Parallelscaling (K→=[N,]\\vec{K}=[N,]) utilizes test-time compute more effectively thanSequentialscaling (K→=[1,…,1]\\vec{K}=[1,\\ldots,1]).Right:Message Passingis essential for test-time scaling. Without compaction (\"W/O Message Passing\"), performance degrades as test time scales and fundamentally limited by model context length, whereas standard PaCoRe (\"W Message Passing\") scales unboundedly and robustly. Pass@1 accuracy is evaluated on HMMT 2025.",
                "position": 526
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05593/x7.png",
                "caption": "Figure 5:Evolution of synthesis-related linguistic features and emergent correctness across training steps.Left:Frequency of cross-checking words (including ’reference’, ’参考’, ’Ref <number>’, ’ref <number>’) in generated solutions.\nTraining elicits and magnifies this capability across domains; notably, the near-zero initial frequency in Code corroborates the poor test-time scaling of untrained models (Figure1).Right:TheEmergent Correctness Ratetracks the probability of generating a correct solution given input messages that are all incorrect, averaged over 100-step intervals.\nThe upward trend in both domains demonstrates that through large scale RL training, the model transcends naive strategies like majority voting or random selection to achieve genuine synthesis, recovering valid solutions even from entirely erroneous contexts.",
                "position": 544
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInitial Checkpoint Derivation",
        "images": []
    },
    {
        "header": "Appendix BPaCoRe Synthesis Prompt Template",
        "images": []
    },
    {
        "header": "Appendix CPaCoRe Training Data Preparation",
        "images": []
    },
    {
        "header": "Appendix DMore Ablation Studies",
        "images": []
    }
]