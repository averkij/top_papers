[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11279/x1.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11279/x2.png",
                "caption": "Figure 2:Overview of the proposed framework. During training, our framework randomly chooses static images or video sequences as the training data. In addition to the noiseztsubscriptùëßùë°z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, three other types of inputs are integrated to guide the generation process: (1) a face region mask, which controls the generation of facial imagery; (2) a 3D reconstructed face, which helps guide the pose and expression, especially in cases of large pose variations; and (3) masked source images, which supply background information. These inputs are processed through the Backbone Network, which performs the denoising operation. Within the Backbone Network, we employ cross-attention and temporal attention mechanisms. The temporal attention module ensures temporal continuity and consistency across frames. Our face encoder extracts identity and texture features from the target face, as well as pose and expression details from the source face, and uses these features in cross-attention to produce realistic and high-fidelity results.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2412.11279/x3.png",
                "caption": "Figure 3:Overview of the proposed VidFaceVAE, capable of simultaneous encoding and decoding of both image and video data. Certain modules are specifically designed for video inputs, and image inputs bypass these modules as needed.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2412.11279/x4.png",
                "caption": "Figure 4:Visualization of our occlusion data augmentation, which improves the stability and consistency of the generated videos.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2412.11279/x5.png",
                "caption": "Figure 5:Visualization of our AIDT dataset. For video facial data, we present only the target and decoupling faces, as the source faces can be derived from any other frame within the same video clip.",
                "position": 234
            }
        ]
    },
    {
        "header": "4AIDT Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11279/x6.png",
                "caption": "Figure 6:Qualitative comparison at512√ó512512512512\\times 512512 √ó 512resolution. Our method generates high-fidelity results and handles challenging cases effectively, such as large poses (b) and occlusions (c). Corresponding videos are provided in the supplementary material.It is best viewed at a larger scale for optimal evaluation.",
                "position": 273
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11279/x7.png",
                "caption": "Figure 7:Ablation on the different combinations of texture weights and attribute weights.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2412.11279/x8.png",
                "caption": "Figure 8:Ablation on the occlusion data augmentation and 3D face reconstruction.",
                "position": 444
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]