[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/x1.png",
                "caption": "",
                "position": 205
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Relations Between Token Entropy and Reasoning Difficulty",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/f1_score.png",
                "caption": "Figure 2:(a) F1 Score vs. threshold percentile across different window sizes.\nWindow-based entropy aggregation consistently outperforms single-token entropy,\nespecially at higher thresholds.(b) A word cloud visualization of semantically filtered\nhigh-entropy tokens, where font size reflects relative frequency.\nThese tokens (e.g.,explain,assume,constraint,conclude) correspond to reasoning triggers that mark the onset\nof logical transitions, highlighting the interpretable basis of our\nentropy-based reward.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/combined_three_metrics_vs_iteration.png",
                "caption": "Figure 3:Training dynamics of ValLine GRPO on Coldstart Model:\n(a) average response length,\n(b) number of high-entropy tokens,\nand (c) accuracy, all measured across iterations.\nThe trends indicate that the growth in high-entropy tokens\nis closely aligned with increases in response length and accuracy.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.08457/x2.png",
                "caption": "Figure 4:Entropy–difficulty interaction in exploration.(a) Conceptual illustration: task difficulty modulates the reasoning\ntrajectory, with easy problems requiring little exploration and hard\nproblems benefiting from deeper branching.\n(b) Quantitative analysis: (i) for easy tasks, responses below the\nentropy threshold are both shorter and more accurate; for hard tasks,\nabove-threshold exploration yields higher accuracy; (ii) response\nlength increases significantly with difficulty; (iii) within each\ndifficulty, correct cases use fewer high-entropy tokens for easy\nproblems but more for hard problems; and (iv) correctness further\namplifies this trend in response length. Together, these results show\nthat limiting exploration improves efficiency on easy problems, while\nencouraging additional exploration is crucial for solving difficult\nones.",
                "position": 333
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/x3.png",
                "caption": "Figure 5:Overall training pipeline of our method. Stage 1 (Adaptive Coldstart Fine-Tuning): difficulty-aware selective data curation and adaptive KL-guided fine-tuning establish a strong initialization across text and multimodal inputs. Stage 2 (Adaptive Entropy Policy Optimization, AEPO): online difficulty bucketing and entropy-aware rollout allocate reasoning depth dynamically, with high-entropy windows serving as branching points for exploration. Together, the two stages enable uncertainty-aware, difficulty-adaptive reasoning for large language models.",
                "position": 361
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/x4.png",
                "caption": "Figure 6:Training dynamics of accuracy (left) and response length (right).\nWe observe that ARES achieves higher accuracy compared with its ablations (w/o KL, w/o Entropy) and baseline methods (GRPO, DAPO),\nwhile also producing shorter and more stable responses during training.\nThese results indicate that the joint use of KL and entropy shaping contributes to improving both correctness and conciseness.",
                "position": 1166
            }
        ]
    },
    {
        "header": "5Visualization Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/acc_benchmark.png",
                "caption": "Figure 7:Accuracy comparison across multimodal and textual benchmarks.Accuracy of three model variants (ARES-CS-Vanilla, ARES-CS-7B, and ARES-RL-7B) on six benchmarks.\nResults are grouped into three difficulty levels (Easy, Medium, Hard).\nRL fine-tuning consistently improves accuracy across all benchmarks.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/resp_len_benchmark_v2.png",
                "caption": "Figure 8:Response length comparison across multimodal and textual benchmarks.We report the average number of generated tokens for three model variants\n(ARES-CS-Vanilla, ARES-CS-7B, and ARES-RL-7B). RL training consistently\nreduces response length on most benchmarks, indicating improved reasoning\nefficiency. In contrast, response length increases on the most challenging\ndatasets—AIME25 (textual) and OlympiadBench (multimodal)—highlighting the\nadaptive behavior of our RL approach: trimming unnecessary reasoning on easy\nproblems while encouraging deeper exploration on difficult ones.",
                "position": 1305
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BToken-Level Entropy Measurement",
        "images": []
    },
    {
        "header": "Appendix CRLVR Algorithms",
        "images": []
    },
    {
        "header": "Appendix DWindow-Entropy Aggregation",
        "images": []
    },
    {
        "header": "Appendix EKL Penalty Inflates GRPO Advantage Variance compared to KL Loss",
        "images": []
    },
    {
        "header": "Appendix FThe Algorithm workflow of AEPO",
        "images": []
    },
    {
        "header": "Appendix GOn-policy difficulty and buckets",
        "images": []
    },
    {
        "header": "Appendix HHigh-Entropy Tokens",
        "images": []
    },
    {
        "header": "Appendix IVisual Analysis of Entropy Reward Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/reward_visual.png",
                "caption": "Figure 9:Effect of entropy shaping and KL regularization on accuracy reward.We plot the moving-average accuracy reward over training steps under different\nablation settings. Baseline methods like GRPO and DAPO either lack stable\nimprovement or plateau early. In contrast, our ARES variants consistently achieve\nhigher accuracy rewards throughout training. Notably, combining both KL\nregularization and entropy shaping yields the most stable and significant gains,\ndemonstrating the necessity of the two components working together.",
                "position": 3430
            }
        ]
    },
    {
        "header": "Appendix JWhy High–Entropy Tokens Predict Reasoning Response Length",
        "images": []
    },
    {
        "header": "Appendix KWhy KL Loss is a ValidThinking Budget",
        "images": []
    },
    {
        "header": "Appendix LFisher–Geometry Justification of AEPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/hard.png",
                "caption": "",
                "position": 4148
            },
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/easy_low_000147.png",
                "caption": "",
                "position": 4256
            },
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/easy_high_000082.png",
                "caption": "",
                "position": 4343
            },
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/medium_high_001140.png",
                "caption": "",
                "position": 4441
            },
            {
                "img": "https://arxiv.org/html/2510.08457/iclr2026/images/medium_low_000448.png",
                "caption": "",
                "position": 4538
            }
        ]
    },
    {
        "header": "Appendix MCase Study",
        "images": []
    }
]