[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/logo.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/teaserv3.png",
                "caption": "Figure 1:Comparison between FlashVideo and other text-to-video generation paradigms.(a) Single Stage DiT suffers from an explosive increase in computation cost when generating at large resolutions, rising from 30s to 2150s (circlein (d)) when increasing the resolution from 270p to 1080p. (b) Though the vanilla cascade can reduce the model size in the high resolution, its second stage still samples from Gaussian noise and only uses the first-stage results as a condition. This approach cannot effectively reduce the number of function evaluations at high resolution and still costs 571.5s (squarein (d)) to generate a 1080p video. (c) In contrast, FlashVideo not only decreases the model size in the second stage but also starts sampling from the first-stage results, requiring only 4 function evaluations at high resolution while integrating a wealth of visually pleasant details, which can generate 1080P video with only 102.3s (trianglein (d)). Details on obtaining these statistics are provided in ourSupplementary Materials.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/methodv3.png",
                "caption": "Figure 2:The overall pipeline of FlashVideo. FlashVideo adopts a cascade paradigm comprised of a 5-billion-parameter DiT at the low resolution (i.e., Stage\\Romannum1) and a 2-billion-parameter DiT at a higher resolution (i.e., Stage\\Romannum2). The 3D RoPE is employed at both stages to model the global and relative spatiotemporal distances efficiently. We construct training data pairs for Stage\\Romannum1 by randomly sampling Gaussian noise and low-resolution video latent. For Stage\\Romannum2, we apply both pixel and latent degradation to high-quality videos to obtain low-quality latent values. These are then paired with high-quality latents to serve as training data. During inference, we retain a sufficientN⁢F⁢E=50𝑁𝐹𝐸50NFE=50italic_N italic_F italic_E = 50at a low resolution of 270p for Stage\\Romannum1. The generated videos retains high fidelity and seamless motion, albeit with detail loss. These videos are then upscaled to a higher resolution of 1080p and processed by latent degradation. With only 4 steps, our Stage\\Romannum2 regenerates accurate structures and rich high-frequency details.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2502.05179/x1.png",
                "caption": "Figure 3:Visual showcase ofD⁢E⁢Gp⁢i⁢x⁢e⁢l𝐷𝐸subscript𝐺𝑝𝑖𝑥𝑒𝑙DEG_{pixel}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPTandD⁢E⁢Gl⁢a⁢t⁢e⁢n⁢t𝐷𝐸subscript𝐺𝑙𝑎𝑡𝑒𝑛𝑡DEG_{latent}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_l italic_a italic_t italic_e italic_n italic_t end_POSTSUBSCRIPTimpact on quality enhancement.From left to right, the first is thei⁢n⁢p⁢u⁢t𝑖𝑛𝑝𝑢𝑡inputitalic_i italic_n italic_p italic_u italic_t, generated by the first-stage model. The termD⁢E⁢Gp⁢i⁢x⁢e⁢l𝐷𝐸subscript𝐺𝑝𝑖𝑥𝑒𝑙DEG_{pixel}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPTstands for the improved result yielded from the model trained only with pixel-space degradation, which adds high-frequency details to thei⁢n⁢p⁢u⁢t𝑖𝑛𝑝𝑢𝑡inputitalic_i italic_n italic_p italic_u italic_t. Further,D⁢E⁢Gp⁢i⁢x⁢e⁢l𝐷𝐸subscript𝐺𝑝𝑖𝑥𝑒𝑙DEG_{pixel}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_p italic_i italic_x italic_e italic_l end_POSTSUBSCRIPT&D⁢E⁢Gl⁢a⁢t⁢e⁢n⁢t𝐷𝐸subscript𝐺𝑙𝑎𝑡𝑒𝑛𝑡DEG_{latent}italic_D italic_E italic_G start_POSTSUBSCRIPT italic_l italic_a italic_t italic_e italic_n italic_t end_POSTSUBSCRIPTrefers to the enhanced result with model trained under both types of degradation, which further improves small structures, such as generating branches for small trees. The improvement is significantly apparent when compared to pixel degradation only.",
                "position": 380
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05179/x2.png",
                "caption": "Figure 4:Generated videos of FlashVideo.The results in the top and bottom rows are from Stage\\Romannum1 and Stage\\Romannum2, respectively. Stage\\Romannum1 generates videos with natural motion and high prompt fidelity, as evident from the visual elements (boldin prompts). However, they lack detailed structures for small objects and high-frequency textures (see theredbox). In Stage\\Romannum2, details are significantly enriched (see thegreenbox), while content remains highly consistent with the original. Visualization results are compressed. More uncompressed cases can be found on ourproject page.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2502.05179/x3.png",
                "caption": "Figure 5:Quality improvements in Stage\\Romannum2. We mark regions with artifacts and lacking detail in the first-stage videos usingredboxes, while improvements from the second stage are highlighted ingreen. Zoom in for a better view. Our Stage\\Romannum2 significantly elevates visual quality across diverse content—enhancing oil painting–style sunflowers in (a), refining wrinkles and hair in (b), enriching texture structures of animals and plants in (c) and (d), and mitigating facial and object artifacts in (e).",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2502.05179/x4.png",
                "caption": "Figure 6:Visual comparison with various video enhancement methods. We present our results alongside enhanced versions, derived from the first-stage outputs, of four video enhancement methods.",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2502.05179/x5.png",
                "caption": "Figure 7:Comparison of long-range detail consistency in large-motion videos.We select a first-stage generated video with significant motion and sample three key frames. The girl in this video undergoes substantial scale variation from distant to close-up views. VEhancerHe et al. [2024], with spatial-temporal module and time slicing, fails to preserve identity and detail consistency. In contrast, FlashVideo leverages 3D full attention to maintain consistent facial identity and texture details.",
                "position": 1004
            }
        ]
    },
    {
        "header": "5Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05179/x6.png",
                "caption": "Figure 8:Results of resolution extrapolation using absolute sinusoidal and RoPE position embeddings.Both settings perform well at the training resolution. However, while RoPE preserves detail enhancement at higher resolutions, absolute position embedding introduces noticeable artifacts beyond the training range.",
                "position": 1064
            },
            {
                "img": "https://arxiv.org/html/2502.05179/extracted/6185701/figure/cfg_step.png",
                "caption": "Figure 9:Results of stage\\Romannum2 under different inference hyper-parameters.",
                "position": 1560
            }
        ]
    },
    {
        "header": "6Discussion and Limitation",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]