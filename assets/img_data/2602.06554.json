[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06554/x1.png",
                "caption": "",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2602.06554/x2.png",
                "caption": "Figure 1:Performance comparison of training Qwen3-14B model on the AppWorld and BFCL-v4 benchmarks. (a)-(b) show training curves, (c)-(f) show test curves. SeeUPO algorithm demonstrates significantly stronger training stability and optimal performance compared to other backbone RL algorithms.",
                "position": 228
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Analysis",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06554/x3.png",
                "caption": "Figure 2:The core idea of SeeUPO is to abstract multi-turn interaction tasks into sequentially-decision multi-agent single-turn tasks, and adopt reverse-order sequential updates to achieve global optimality via backward induction. The figure shows an example scenario with three turns, from left to right showing theoriginal task scenario, themulti-agent modelingof the scenario, and thereverse update mechanismbased on MARL theory.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2602.06554/x4.png",
                "caption": "Figure 3:The batch construction approach of SeeUPO. Unlike methods that construct batches using entire trajectories or by concatenating sliced turns, SeeUPO implements aturn-orientedapproach that separately organizes samples from identical turns. This figure demonstrates the divergent batch construction patterns between SeeUPO and the Vanilla approach under two tasks with maximum three-turn interactions, viaReact+Reasoning-Augmented Templateparadigm(Zhaiet al.,2025).",
                "position": 899
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06554/x5.png",
                "caption": "Figure 4:Training success rate comparison of SeeUPO and baselines. Subplots (a)-(d) show results for Qwen-3 model on Appworld and BFCL, and Qwen-2.5 model on Appworld and BFCL, respectively.",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2602.06554/x6.png",
                "caption": "Figure 5:Training dynamics comparison of different update order strategies: (a) Qwen-3 evaluated on AppWorld and (b) Qwen-3 evaluated on BFCL-v4.",
                "position": 1367
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMirror Learning and Multi-Agent Mirror Learning",
        "images": []
    },
    {
        "header": "Appendix BGlobal Optimality of SeeUPO",
        "images": []
    },
    {
        "header": "Appendix CBias of GAE",
        "images": []
    },
    {
        "header": "Appendix DBias of GRAE",
        "images": []
    },
    {
        "header": "Appendix EDetailed Proofs for GRAE-REINFORCE Convergence",
        "images": []
    },
    {
        "header": "Appendix FDetailed Proofs for GAE-PPU Convergence",
        "images": []
    },
    {
        "header": "Appendix GDetailed Proofs for GRAE-PPU Convergence Analysis",
        "images": []
    },
    {
        "header": "Appendix HGRAE-PPU Convergence in Contextual Bandit Settings and Variance Normalization Issues",
        "images": []
    }
]