[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18778/x1.png",
                "caption": "Figure 1:Learning on hard problems by self-generating a\ncurriculum.We introduceSOAR: A meta-RL framework for\nimproving on difficult datasets where performance plateaus.(left)We initialize asymmetric teacher and student models from the same base model. The teacher generates synthetic\nproblems for the student to train on with RL, and is rewarded by the student‚Äôs measurable improvement on a small subset of the real,\nground-truth problems.(right)RL training on problems generated withSOAR, using grounded teacher rewards, outperforms direct training on the hard\nproblems and enables the student to break out of the performance\nplateau.",
                "position": 201
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18778/x2.png",
                "caption": "Figure 2:TheSOARmeta-RL Loop.The teacher and student are initialized from the same model. In theouter RL loopthe teacher generates candidate question-answer pairs that are partitioned into datasets. In theinner RL loop, the student is trained for 10 steps on the candidate problems and evaluated on sampled hard problems. The teacher is rewarded based on the resulting student improvement over the student baseline, grounding the synthetic curriculum in real learning progress.",
                "position": 412
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18778/x3.png",
                "caption": "Figure 3:Performance on MATH and HARP fail@128 (improvement overHard-Only).Synthetic problems generated withSOAR(PQ) and inference with the promoted student (PS) outperform direct training on fail@128 train sets (Hard-Only), and sampling from teachers trained with intrinsic rewards (Intrinsic-T). Performance is reported as thedelta overHard-Only. For reference,Hard-OnlyMATH pass@kkfork‚àà{1,4,8,16,32}k\\in\\{1,4,8,16,32\\}is{0.5,1.7,3.2,5.7,9.6}\\{0.5,1.7,3.2,5.7,9.6\\}.Hard-Onlytraining curves are shown in Figure5; absolute performance for all methods, and further evaluations, are in Tables4-5. Shaded regions are¬±\\pm1 SD over 6-12 seeds nested across teacher/student training (seeB.8).",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x3.png",
                "caption": "Figure 3:Performance on MATH and HARP fail@128 (improvement overHard-Only).Synthetic problems generated withSOAR(PQ) and inference with the promoted student (PS) outperform direct training on fail@128 train sets (Hard-Only), and sampling from teachers trained with intrinsic rewards (Intrinsic-T). Performance is reported as thedelta overHard-Only. For reference,Hard-OnlyMATH pass@kkfork‚àà{1,4,8,16,32}k\\in\\{1,4,8,16,32\\}is{0.5,1.7,3.2,5.7,9.6}\\{0.5,1.7,3.2,5.7,9.6\\}.Hard-Onlytraining curves are shown in Figure5; absolute performance for all methods, and further evaluations, are in Tables4-5. Shaded regions are¬±\\pm1 SD over 6-12 seeds nested across teacher/student training (seeB.8).",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x4.png",
                "caption": "Figure 4:Transfer performance to OlympiadBench fail@128 subset (improvement overHard-Only).Questions optimized for MATH and HARP transfer to a held-out dataset. Performance is reported as thedelta overHard-Only; absolute performance, including PS evaluation, is in Table6.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x5.png",
                "caption": "Figure 5:Grounded rewards lead to more stable teacher policies.We evaluate trained teacher policies by sampling questions and training fresh students.(Left)Test pass@32 comparison between students trained with questions sampled fromGrounded-TandBase-T(Hard-Onlyalso shown for reference).Grounded-ToutperformsBase-Tand exhibits more stable student trajectories.(Right)Pass@32 trajectories for fresh students trained with individualGrounded-Tteacher seeds (red) andIntrinsic-Tteacher seeds (green). Questions fromGrounded-Tyield consistent student trajectories, whereasIntrinsic-Texhibits higher variance across teachers, including a failure mode where I-T (1) causes student collapse. Shading shows¬±1\\pm 1SD. Curves for other pass@k and OlympiadBench are in Figures10-12.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x6.png",
                "caption": "Figure 6:Qualitative Evolution of Generated Questions.(Left) Baseline student performance during aSOARrun on HARP. The y-axis shows greedy accuracy on thefail@128 train setover promotion stages. (Right) Sampled teacher questions at different promotion points. Content and style shift from word problems and basic formulas (stage 1) to concise, equation-heavy problems in algebra and calculus (stage 2).\nMany effective ‚Äústepping stones\" include incorrect solutions, suggesting that structural and conceptual content provide sufficient learning signal.",
                "position": 560
            }
        ]
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Discussion and Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BMethod and Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18778/x7.png",
                "caption": "Figure 7:Mixed v. Curriculum training on MATH.We compare training the base student on fail@128 + 128 questions sampled fromBase-T, for performance on MATH. Curriculum performs better across different inference budgets.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x8.png",
                "caption": "Figure 8:Mixed v. Curriculum training on HARP/OlympiadBench.We compare training the base student on real fail@128 + 128 random MATH questions, for HARP and OlympiadBench. Mixed training exhibits significantly more stable training dynamics across inference budgets (Pass@8 and Pass@32) and converges to higher final performance points. For both datasets, curriculum training exhibits strong instability with a large early performance spike and then crash.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x9.png",
                "caption": "Figure 9:Fail@128 test performance during student training for MATH, HARP, and Olympiad.Student learning curves at different pass@k when trained onHard-Only, PQ, or the Full MATH dataset (PS inference performance shown as a horizontal line). PQ and PS improve performance on all inference budgets and datasets, with increased effect at higherkk. On MATH, PQ exhibits performance gains even after the synthetic-training phase (64 steps), showing that synthetic problems make real hard problems more learnable.",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x10.png",
                "caption": "Figure 10:Fail@128 test performance during student training for MATH with different teachers.Each column compares training a fresh student with 128 questions fromGrounded-Tto 128 questions from a different teacher (Hard-Onlyalso included for reference). While all teachers outperformHard-Only,Grounded-Tperforms best, with increasing effects at higherkk.Grounded-Tresults in less variance across student outcomes, particularly compared toBase-TandIntrinsic-T. PQ learning curves are in Figure9.",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x11.png",
                "caption": "Figure 11:Fail@128 test performance during student training for HARP with different teachers.Each column compares training a fresh student with 128 questions fromGrounded-Tto 128 questions from a different teacher (Hard-Onlyalso included for reference).Grounded-Tperforms best, with increasing effects at higherkk. Students trained withBase-TandIntrinsic-Ttend to decline more for higherkkin the later stages of training, whileGrounded-Tleads to more stable trajectories.",
                "position": 1470
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x12.png",
                "caption": "Figure 12:Fail@128 test performance during student training for Olympiad with different teachers.Each column compares training a fresh student with 128 questions fromGrounded-T(trained with MATH and HARP) to 128 questions from a different teacher (Hard-Onlyalso included for reference). Students trained withGrounded-Tteachers have more similar mean performance toBase-TandIntrinsic-Tthan seen on HARP and MATH (Figures10-11). However,Grounded-T (HARP) shows more stability and less variance between independent teachers than Intrinsic-T (see Figure13).",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x13.png",
                "caption": "Figure 13:Test Pass@32 on OlympiadBench for fresh students trained with individualGrounded-Tteacher seeds (red) andIntrinsic-Tteacher seeds (green).Questions fromGrounded-Tyield consistent student trajectories on OlympiadBench across different teachers, whereasIntrinsic-Texhibits high variance across teachers, including a failure mode where I-T (1) causes student collapse.",
                "position": 1476
            }
        ]
    },
    {
        "header": "Appendix CEvaluations",
        "images": []
    },
    {
        "header": "Appendix DAblations",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18778/x14.png",
                "caption": "Figure 14:(Left) Sampling different-sized datasets fromGrounded-Tfor MATH (fail@128)Mean and¬±\\pm1 SD across 2 teacher seeds and 2 student seeds.(Right) Sampling different-sized datasets from the MATH trainset for MATH (fail@128).Resampled for each seed, 3 seeds.",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x15.png",
                "caption": "Figure 15:Hyperparameter sensitivity on MATH.We trainSOARwithœÑ‚àà{0.01,0.015}\\tau\\in\\{0.01,0.015\\}andn‚àà{32,64}n\\in\\{32,64\\}, then evaluate by training students on datasets of size|ùí≥|‚àà{32,64,128}|\\mathcal{X}|\\in\\{32,64,128\\}. Shaded regions indicate¬±\\pm1 SD.",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x16.png",
                "caption": "Figure 16:Annotated teacher reward dynamics when trainingSOARwith HARP.Shows a sample teacher trajectory from aSOARrun on HARP. The teacher follows a cyclical search-exploitation pattern. Student promotions (updating the student baseline to a trained student) are triggered when the 3-step moving average of teacher rewards exceedsœÑ=0.01\\tau=0.01. After each promotion, the improved student baseline makes previous curricula less useful, causing rewards to drop, and then recover as the teacher adapts and discovers questions appropriate for the improved student.",
                "position": 2124
            },
            {
                "img": "https://arxiv.org/html/2601.18778/x17.png",
                "caption": "Figure 17:(Left) Teacher training dynamics when training withIntrinsic-T.Mean and¬±\\pm1 SD over three independent training runs.(Right) Teacher completion diversity when training with intrinsic v. grounded rewards.Grounded rewards preserve diversity for the full run, while intrinsic teachers lose diversity as they converge. Mean and¬±\\pm1 SD over three training runs for intrinsic and four for grounded (two MATH, two HARP).",
                "position": 2127
            }
        ]
    },
    {
        "header": "Appendix ETeacher Training Dynamics",
        "images": []
    }
]