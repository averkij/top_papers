[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15974/extracted/6641281/images/main.png",
                "caption": "Figure 1:Inference-time scaling and robustness.(Left)We show that increasing inference-time computation, by extending reasoning chains, can either improve robustness or at least maintain model robustness when only the final output is considered.(Right)However, we also identify aninverse scaling law: when intermediate reasoning steps are exposed to adversaries, increased inference-time computation consistentlyreducesrobustness across all three adversarial settings. Results are averaged over 12 open-source reasoning models.",
                "position": 83
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15974/extracted/6641281/images/tasks.png",
                "caption": "Figure 2:Illustrative examples of three common attack types on LLM-based systems:(a)Prompt Injection: Embedding malicious instructions within low-priority content (e.g., a document for summarization), causing the model to perform unintended actions.(b)Prompt Extraction: Crafting queries to prompt the model into disclosing sensitive or proprietary (system) messages.(c)Harmful Requests: Directly requesting unsafe outputs, such as instructions for illegal activities.",
                "position": 133
            }
        ]
    },
    {
        "header": "3A Simple Inference-Time Scaling Strategy to Boost Robustness",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15974/x1.png",
                "caption": "Figure 3:Robustness evaluation across inference-time computation for multiple open-source reasoning models.\nThe X-axis denotes inference-time compute (reasoning token budget), while the Y-axis measures robustness performance across three adversarial scenarios:\n(a) Prompt injection attacks assessed on theSEPdataset,\n(b) Prompt extraction attacks evaluated using theTensorTrustdataset, and\n(c) Harmful requests benchmarked on theSORRY-Benchdataset.\nWe observe that increased inference-time computation generally leads to improved robustness against prompt injection and extraction attacks, and maintains stable performance on harmful request tasks.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2507.15974/x2.png",
                "caption": "",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2507.15974/x3.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "4What if the Reasoning Tokens Are Not Hidden?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15974/x4.png",
                "caption": "Figure 4:Robustness evaluation of multiple reasoning models with varying inference-time computation budgets that only consider the intermediate reasoning steps. We provide results for: (a) prompt injection robustness evaluated on theSEP, (b) prompt extraction robustness measured using theTensorTrust, and (c) harmful request robustness assessed on theSORRY-Bench. Our findings illustrate a clearinverse scaling law: robustness consistentlydecreasesas inference-time computation increases, underscoring the heightened security risks introduced by exposing reasoning chains.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2507.15974/x5.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2507.15974/x6.png",
                "caption": "",
                "position": 209
            }
        ]
    },
    {
        "header": "5Does Hiding the Reasoning Chain Solve All Robustness Issues?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15974/x7.png",
                "caption": "Figure 5:Robustness of reasoning models against prompt injection attacks targeting tool-augmented reasoning. Robustness declines as inference-time computation increases.",
                "position": 263
            }
        ]
    },
    {
        "header": "6Discussion and Future Work",
        "images": []
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.15974/extracted/6641281/images/PIA_demo.png",
                "caption": "Figure 6:An example of a prompt injection attack. The main instruction, associated data, low-priority query, and witness are shown. We highlight the added guidance intended to help the model mitigate the attack. The model is expected to follow the main instruction and ignore the low-priority query.",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2507.15974/extracted/6641281/images/PE_demo.png",
                "caption": "Figure 7:A demonstration of a prompt extraction attack. The system instruction and malicious user prompt are shown. A detailed specification is highlighted. The model is expected to avoid revealing the secret passcode in its response.",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2507.15974/extracted/6641281/images/PIA_demo_new.png",
                "caption": "Figure 8:An illustrative example demonstrating a prompt injection attack targeting tool-integrated reasoning models. The instruction for teaching tool-calling in reasoning is highlighted.",
                "position": 1117
            }
        ]
    },
    {
        "header": "Appendix ADetails of Experiments",
        "images": []
    }
]