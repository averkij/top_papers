[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21185/x1.png",
                "caption": "Figure 1:Performance on Language Modeling and Image Modeling.Œ®\\Psi-samplers generalize ReMDM(Wanget al.,2025)to arbitrary noise distributions.(Left):Generative perplexity (Gen. PPL;‚Üì\\downarrow) as a function of NFEs, with nucleus samplingp=0.9p=0.9.Œ®\\Psi-samplers consistently improve with more steps, unlike ancestral sampling which plateaus. Curves are annotated with the average unigram entropy per sequence as a proxy for diversity.(Right):On CIFAR-10,Œ®\\Psi-samplers achieve better FID (‚Üì\\downarrow) than MDLM (with ReMDM).",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x1.png",
                "caption": "",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x2.png",
                "caption": "",
                "position": 238
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21185/x3.png",
                "caption": "Figure 2:Œ®\\Psi-samplers combine predictor and corrector steps. Thepredictortransitions fromùê≥t{\\mathbf{z}}_{t}toùê≥s{\\mathbf{z}}_{s}viaqs|t{q_{s|t}}, but fails to remask tokens in MDMs. Thecorrectorsteps inject noise viaqsq_{s}, to revise earlier predictions. ForŒ∫t<1\\kappa_{t}<1, noise injection enables error correction while preserving the forward\nprocess marginals. Our framework extends prior PC methods(Campbellet al.,2022; Gatet al.,2024; Wanget al.,2025)to arbitrary priorsùùÖ\\bm{\\pi}.",
                "position": 412
            }
        ]
    },
    {
        "header": "3TheŒ®\\Psi-Posteriors",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21185/x4.png",
                "caption": "Figure 3:Efficient Curriculum for USDMs.Duo(Sahooet al.,2025a)replaces discrete lookups with linear combinations of allKKembeddings: (1) Gaussian diffusion on one-hot representations, (2) Low-temperaturesoftmax{\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.60546875,0,0}\\mathrm{softmax}}, (3) weighted sum.Duo++\\text{Duo}^{++}exploits the sparsity of the tempered softmax (most weights are effectively zero), and simulate the k largest entries (out of K) using ordered statistics. The approximate normalizerZ~{\\color[rgb]{0.60546875,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0.60546875,0,0}\\tilde{Z}}admits a closed form expression (14).Duo++\\text{Duo}^{++}has a 33% lower memory and 25% faster training than Duo.",
                "position": 513
            }
        ]
    },
    {
        "header": "4Scalable Curriculum for Faster Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21185/x5.png",
                "caption": "Figure 4:Illustration of the possible evolution ofttand the associatedŒ∫t\\kappa_{t}. In practice, we useŒ∫t\\kappa_{t}close to 1 during the PC phase.",
                "position": 692
            }
        ]
    },
    {
        "header": "6Related work and Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AŒ®\\Psi-Posteriors",
        "images": []
    },
    {
        "header": "Appendix BFast Curriculum",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21185/x6.png",
                "caption": "Figure 5:Polynomial approximation and approximation error, compared to the series approximation, truncated at 150 terms. The degree-99polynomial (left) achieves orders of magnitude lower error than the degree-55polynomial (center) and sigmoid (right) approximations.",
                "position": 2902
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x6.png",
                "caption": "",
                "position": 2905
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x7.png",
                "caption": "",
                "position": 2909
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x8.png",
                "caption": "",
                "position": 2913
            }
        ]
    },
    {
        "header": "Appendix CExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21185/x9.png",
                "caption": "Figure 6:Œ®\\Psi-samplers, which generalize ReMDM, significantly improve the Inception Score on CIFAR-10, compared to ancestral sampling.",
                "position": 3164
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x10.png",
                "caption": "Figure 7:Marginal distributions of the top-5 entries using a tokenizer with 100k tokens, inverse temperature 100, and log signal-to-noise ratio‚àí2-2. The histograms of the efficient and naive implementation match closely.",
                "position": 3407
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x11.png",
                "caption": "Figure 8:Marginal distributions of the top-5 entries using a tokenizer with 100k tokens, inverse temperature 1000, and log signal-to-noise ratio‚àí1-1. The histograms of the efficient and naive implementation match closely.",
                "position": 3410
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x12.png",
                "caption": "Figure 9:Marginal distributions of the top-5 entries using a tokenizer with 100k tokens, inverse temperature 1000, and log signal-to-noise ratio‚àí2-2. The histograms of the efficient and naive implementation match closely.",
                "position": 3413
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x13.png",
                "caption": "Figure 10:Marginal distributions of the top-5 entries using a tokenizer with 100k tokens, inverse temperature 1000, and log signal-to-noise ratio‚àí4-4. The histograms of the efficient and naive implementation match closely.",
                "position": 3416
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x14.png",
                "caption": "Figure 11:Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 100, and log signal-to-noise ratio‚àí2-2. The histograms of the efficient and naive implementation match closely.",
                "position": 3419
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x15.png",
                "caption": "Figure 12:Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 1000, and log signal-to-noise ratio‚àí1-1. The histograms of the efficient and naive implementation match closely.",
                "position": 3422
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x16.png",
                "caption": "Figure 13:Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 1000, and log signal-to-noise ratio‚àí2-2. The histograms of the efficient and naive implementation match closely.",
                "position": 3425
            },
            {
                "img": "https://arxiv.org/html/2602.21185/x17.png",
                "caption": "Figure 14:Marginal distributions of the top-5 entries using the GPT-2 tokenizer, inverse temperature 1000, and log signal-to-noise ratio‚àí4-4. The histograms of the efficient and naive implementation match closely.",
                "position": 3428
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental results",
        "images": []
    }
]