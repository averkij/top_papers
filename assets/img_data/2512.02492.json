[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02492/x1.png",
                "caption": "Figure 1:Illustration of YingVideo-MV’s cascaded generation Pipeline. Our framework integrates multimodal inputs (music, text, and images) to enable segmented generation of music-performing portrait videos under the guidance of a global planning module. The planning agent strategically invokes specialized tools according to sub-task requirements, ultimately generating three core outputs conditioned on initial-frame specifications: (1) high fidelity music-performing portrait images, (2) coherent dynamic camera trajectories, and (3) synchronized audio sequences aligned with visual performance cues.",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2512.02492/x2.png",
                "caption": "Figure 2:Illustration of Video Generation Model Architecture. Embeddings from the image and text encoders are injected into each block of the DiT. Given music audio input, we leverage Wav2Vec to extract audio embeddings, while the camera trajectory is encoded and incorporated into the diffusion latent. To model the joint audio–latent representation, audio embeddings are fed into an audio adapter, the outputs of which are injected into the DiT via cross-attention.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2512.02492/x3.png",
                "caption": "Figure 3:Timestep-aware dynamic window range strategy. From top to bottom, each row represents a denoising process at one timestep. Within each row, each clip of different color represents the segmentation of the long video. There are overlapping areas between each clip. At t=3, the last clip expands its overlap with the preceding clip to satisfy the minimum clip-length constraint. At t=5, the starting offset is reset because the offset accumulated in the previous timestep has reached its maximum allowable value.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02492/x4.png",
                "caption": "Figure 4:Visualization of Camera Movement.This figure illustrates the music-driven high performance of our framework with synchronized camera motions. The generated sequences demonstrate precise alignment between body movements and camera motion.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2512.02492/x5.png",
                "caption": "Figure 5:More generated results of YingVideo-MV. The figure shows the video results generated by YingVideo-MV using various camera motion combinations.",
                "position": 592
            }
        ]
    },
    {
        "header": "5Limitation and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]