[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23479/x1.png",
                "caption": "Figure 1:Efficiency and performance for MergeMix:(a) The training time vs. accuracy of mixup methods with the DeiT-Small model. (b) The image classification Top-1 accuracy vs. training epochs of different mixup methods on the CIFAR100 dataset with the DeiT-Tiny model. (c) The radar plot of the results on part VQA tasks by LLaVA-7B, LLaVA with SFT, and MergeMix.",
                "position": 174
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23479/x2.png",
                "caption": "Figure 2:The overall of the two scenarios of MergeMix:(a) MergeMix for Image Classification:The image is processed by the ToMe encoder, with Attention Score Recovery and TopK sampling to generate the corresponding class prediction.(b) MergeMix for MLLM:Preference pairs are encoded by the vision model with token merging, and the LLM decoder generates response text for the loser and winner, optimized via a ranking loss.",
                "position": 195
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4MergeMix Training Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23479/x3.png",
                "caption": "Figure 3:Overall illustration of MergeMix for MLLM.MergeMix performs attention-based mask mixing guided by the ToMe Vision Encoder, recovering token attention scores and generating a mixed image through an augmenter.\nSpecifically, Token Merging hierarchically merges visual tokens through Bipartite Soft Matching (BSM) and merges to enhance efficiency. The model is trained with both the SFT and ranking losses.",
                "position": 341
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23479/x4.png",
                "caption": "Figure 4:The confidence plots of mixup variants and MergeMix on the CIFAR100 dataset using DeiT-Tiny and ViT-Small. The red line indicates the expected prediction tendency.",
                "position": 1263
            },
            {
                "img": "https://arxiv.org/html/2510.23479/x5.png",
                "caption": "Table 6:The calibration results of LLaVA-v1.5-7bon POPE, ScienceVQAI, GQA & SEEDI.rldenotes training with ranking loss.",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2510.23479/x5.png",
                "caption": "Figure 5:Robustness against image occlusion classification results with different occlusion ratios for different mixup methods based on DeiT-Small on ImageNet-1K dataset.",
                "position": 1377
            },
            {
                "img": "https://arxiv.org/html/2510.23479/x6.png",
                "caption": "Figure 6:Sensitivity analysis study of 2 hyperparameters of MergMix.Left:Different merge ratios of backbones.Right:Attention score obtained from feature layer. Those results are from training for 200 epochs.",
                "position": 1411
            },
            {
                "img": "https://arxiv.org/html/2510.23479/x6.png",
                "caption": "Figure 6:Sensitivity analysis study of 2 hyperparameters of MergMix.Left:Different merge ratios of backbones.Right:Attention score obtained from feature layer. Those results are from training for 200 epochs.",
                "position": 1414
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADeclaration of LLM Usage",
        "images": []
    },
    {
        "header": "Appendix BAppendix for Image Classification",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23479/x7.png",
                "caption": "Figure 7:Robustness against image occlusion classification results with different occlusion ratios for different mixup methods based on DeiT-Tiny (left) and DeiT-Small (right) on CIFAR100 and ImageNet-1K datasets.",
                "position": 3118
            }
        ]
    },
    {
        "header": "Appendix CAppendix for MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23479/x8.png",
                "caption": "Figure 8:The visualization of the visual question answers with different mixing ratios by LLaVA-v1.5-7B model. Note that theblue textsdenote the core question and the corresponding correct answers, while thegreen textsdenote the wrong answer to the question. The raw image denotes without any augmentations, and other images denote with different mixing ratiosÎ»\\lambda. Ground-truth Answer denotes the raw labels for this case. With the mixing degree improving, the answer comes out more wrong or unrelated to the question, as shown ingreencolor.",
                "position": 5216
            },
            {
                "img": "https://arxiv.org/html/2510.23479/x9.png",
                "caption": "Figure 9:The visualization of ToMe with different merge ratios and mixed samples with different mixing ratios.",
                "position": 5220
            },
            {
                "img": "https://arxiv.org/html/2510.23479/x10.png",
                "caption": "Figure 10:The visualization of ToMe with different merge ratios and mixed samples with different mixing ratios.",
                "position": 5223
            }
        ]
    },
    {
        "header": "Appendix DVisualization of MergeMix",
        "images": []
    }
]