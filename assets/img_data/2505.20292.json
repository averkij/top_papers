[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x1.png",
                "caption": "Figure 1:Example of Seven Categories from OpenS2V-Eval.These categories fully encompass the subject-to-video tasks, allowing comprehensive evaluation. Videos are generated by KlingKeLing.",
                "position": 378
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x2.png",
                "caption": "Figure 2:The Pipeline of Constructing OpenS2V-Eval.(Left) Our benchmark includes not only real subject images but also synthetic images constructed through GPT-Image-1gpt4, allowing for a more comprehensive evaluation. (Right) The metrics are tailored for subject-to-video generation, evaluating not only S2V characteristics (e.g., consistency) but also basic video elements (e.g., motion).",
                "position": 510
            }
        ]
    },
    {
        "header": "3OpenS2V-Eval",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x3.png",
                "caption": "Figure 3:Statistics in OpenS2V-Eval.The benchmark covers diverse categories and prompt words, with subject images displaying high aesthetics, thus enabling a thorough evaluation.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x4.png",
                "caption": "Figure 4:The Pipeline of Constructing OpenS2V-5M.First, we filter low-quality videos based on scores such as aesthetics and motion, then utilize GroundingDinogroundingdinoand SAM2.1sam2to extract subject images and get Regular Data. Subsequently, we create Nexus Data through cross-video association and GPT-Image-1gpt4to address the three core issues encountered by S2V models.",
                "position": 595
            }
        ]
    },
    {
        "header": "4OpenS2V-5M",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x5.png",
                "caption": "Figure 5:Comparison between Regular Data and Nexus Data.The latter is of higher quality.",
                "position": 638
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x6.png",
                "caption": "Figure 6:Qualitative Comparison among Different Methods for the Open-Domain Subject-to-Video task.Existing methods handle non-human entities better than human identities, and perform better with single subject compared to multiple subjects.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x7.png",
                "caption": "Figure 7:Qualitative Comparison among Different Methods for the Human-Domain Subject-to-Video task.They are unable to generate consistent side profiles and suffer from copy-paste issues.",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x8.png",
                "caption": "Figure 8:Qualitative Comparison among Different Methods for the Single-Domain Subject-to-Video task.Existing models perform better on single-subject than multi-subject tasks.",
                "position": 1233
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x9.png",
                "caption": "Figure 9:(a) Alignment between Automatic Metrics and Human Perception.The proposed metrics are comparable to other metricsarcface;opencv;improved-aesthetic-predictorin terms of human preference.(2) Validation of ConsisID-Nexu-5M with††{\\dagger}†and without‡‡{\\ddagger}‡Nexus Data.Training are based on MAGREFmagref.",
                "position": 1241
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Paper Appendix forOpenS2V-Nexus: A Ultra-Scale Dataset and Benchmark for Subject-Consistent Video Generation",
        "images": []
    },
    {
        "header": "Appendix ARelated Works: Subject-Consistency Video Generation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x10.png",
                "caption": "Figure 10:Comparison with Existing Metircs for Subject Consistency and Text Relevance.The proposed automatic metricsalign more closely with human preferences compared to the commonly used DINO-I[116], CLIP-I[72], and CLIP-T[72]in existing S2V methods[42,57,39,22].",
                "position": 2900
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x11.png",
                "caption": "Figure 11:Comparison with Existing Methods for Subject Naturalness.Existing AIGC anomaly detection models and multimodal models are both prone to misidentifying generated content as real.",
                "position": 2904
            }
        ]
    },
    {
        "header": "Appendix BMore Details of OpenS2V-Eval",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x12.png",
                "caption": "Figure 12:Visual Reference for Varying Scores of Different Metircs.It is evident that the proposed NexusScore, NaturalScore, and GmeScore are highly correlated with human perception.",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x13.png",
                "caption": "Figure 13:Example of Common Issues faced by current Subject-to-Video Generation Models.These videos are generated by Kling[45]and SkyReels-A2[22]for demonstration purposes only.",
                "position": 2963
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x14.png",
                "caption": "Figure 14:Visualization of all the Quantitative Results in OpenS2V-Eval.",
                "position": 2967
            }
        ]
    },
    {
        "header": "Appendix CMore Details of OpenS2V-5M",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x15.png",
                "caption": "Figure 15:Statistics in OpenS2V-5M.The dataset includes a diverse range of categories, clip durations and caption lengths, with most of videos being in high quality (e.g., resolution, aesthetic).",
                "position": 3008
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x16.png",
                "caption": "Figure 16:More Showcases Generated by MAGREF ‡.",
                "position": 3012
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x17.png",
                "caption": "Figure 17:Samples from the OpenS2V-5M dataset.The dataset consists of subject-text-video triples, which exhibit more physical knowledge than existing large-scale T2V dataset[12,90].",
                "position": 3023
            }
        ]
    },
    {
        "header": "Appendix DMore Details of Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20292/x18.png",
                "caption": "Figure 18:Distribution of NexusScore, AestheticsScore and MotionScore.",
                "position": 3211
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x19.png",
                "caption": "Figure 19:Visualization of the Questionnaire for User Study.",
                "position": 3223
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x20.png",
                "caption": "Figure 20:Visualization of Different Input Text Prompts.",
                "position": 3226
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x21.png",
                "caption": "Figure 21:More Showcases in OpenS2V-Eval for Open-Domain Subject-to-Video Generation.",
                "position": 3294
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x22.png",
                "caption": "Figure 22:More Showcases in OpenS2V-Eval for Single-Domain Subject-to-Video Generation.",
                "position": 3298
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x23.png",
                "caption": "Figure 23:More Showcases in OpenS2V-Eval for Human-Domain Subject-to-Video Generation.",
                "position": 3302
            },
            {
                "img": "https://arxiv.org/html/2505.20292/x24.png",
                "caption": "Figure 24:More Showcases in OpenS2V-Eval for Human-Domain Subject-to-Video Generation.",
                "position": 3306
            }
        ]
    },
    {
        "header": "Appendix EAdditional Statement",
        "images": []
    }
]