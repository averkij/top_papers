[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12091/x1.png",
                "caption": "",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x2.png",
                "caption": "Figure 2:Overview of Wonderland.Given a single image, a camera-guided video diffusion model follows the camera trajectory and generates a 3D-aware video latent, which is leveraged by the latent-based large reconstruction model to construct the 3D scene in a feed-forward manner. The video diffusion model involves dual-branch camera conditioning to fulfill precise pose control. The LaLRM operates in latent space and efficiently reconstructs a wide-scope and high-fidelity 3D scene.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x3.png",
                "caption": "Figure 3:Qualitative comparison to prior arts in camera-guided video generation.\nThe14thsubscript14th14_{\\mathrm{th}}14 start_POSTSUBSCRIPT roman_th end_POSTSUBSCRIPTframe in each sample is shown for comparison, with the first column displaying the conditional image and camera trajectory (bottom-right).\nBlue bounding boxes denote reference areas to assist comparison and orange bounding boxes highlight low-quality generations.\nWe also show our last frames in the rightmost column.\nOur method outperforms the priors in both precise camera control and high-quality and wide-scope video generation.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x4.png",
                "caption": "Figure 4:Qualitative comparison for 3D scene generation. Blue bounding boxes show visible regions from conditional images and yellow bounding boxes show low-quality regions. Our approach generates much higher quality novel views fromoneconditional image.",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x5.png",
                "caption": "Figure 5:Comparison with ViewCrafter (left) and WonderJourney (right) for in-the-wild 3D scene generation fromsingleinput images.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x6.png",
                "caption": "Figure 6:Comparison to ZeroNVS and Cat3D with Mip-Nerf dataset on 3D scene generation fromsingleinput images. For each scene, the conditional image is shown in the left-most column. We show renderings from two viewpoints, one at the conditional image (starting) view (upper) and another at around 120¬∞-rotation from the starting view(lower).",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x7.png",
                "caption": "Figure A1:Comparison of video generations between the source model (upper row) and the model fine-tuned on static-scene datasets with LoRA modules (lower row). The results demonstrate that fine-tuning the model on static-scene datasets equipped with LoRA produces significantly more static scenes.",
                "position": 1812
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x8.png",
                "caption": "Figure A2:Comparison of 3D rendering performance between latent reconstruction models fine-tunedwithoutin-the-wild dataset (upper row) andwithin-the-wild dataset (lower row). Involving in-the-wild datasets during fine-tuning improves the generalization capability.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x9.png",
                "caption": "Figure A3:Structure of Dual-branch Camera-guided Video Diffusion Model. We show the skeletons of the training pipeline, where random noise is added to the video latents. The conditional image is merged to the noisy latents via feature concatenation. The camera guidance is integrated with LoRA-branch (left) and ControlNet-branch (right). We ignore the text tokens, the diffusion time embeddings, the positional embeddings, and some reshaping operations for simplicity in the figure. In the foundation diffusion transformer, the text tokens are concatenated alongnumber-of-tokendimension with visual tokens. Thus we apply zero-padding to camera tokens to guarantee the same length before concatenation or element-wise sum. By default, we use SiLu as our activation function.",
                "position": 2045
            },
            {
                "img": "https://arxiv.org/html/2412.12091/x10.png",
                "caption": "Figure A4:Structure of Latent Large Reconstruction Model (LaLRM). Given a video latentzùëßzitalic_zand the camera embeddings p, the LaLRM directly regresses the 3DGS features in a feed-forward manner.",
                "position": 2063
            }
        ]
    },
    {
        "header": "",
        "images": []
    }
]