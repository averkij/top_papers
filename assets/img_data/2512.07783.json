[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/logo/github.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2512.07783/logo/huggingface.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x1.png",
                "caption": "Figure 1:Interplay of pre-, mid-, and post-training in LM reasoning.Left:RL yields genuine extrapolative gains only when task difficulty slightly exceeds the pre-training range; gains vanish when tasks are already covered or too out-of-distribution (up to +42% pass@128 when well-calibrated).Mid:Contextual generalization requires minimal yet sufficient pre-training exposure to long-tail contexts. RL fails with near-zero exposure but generalizes robustly with sparse exposure (‚â•\\geq1%), yielding up to +60% pass@128.Right:A mid-training stage bridging pre-training and RL substantially improves OOD reasoning under fixed compute, with mid-training + RL outperforming RL alone by +10.8% on OOD-hard tasks.",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/x2.png",
                "caption": "Figure 2:Overview of the data generation framework, task setup, and process-verified evaluation.The figure depicts the dependency graphùí¢\\mathcal{G}and contextual templatesœÑ\\tau, the task setup for extrapolative and contextual generalization, and the process-verified evaluation framework that checks for correctness of reasoning steps.",
                "position": 148
            }
        ]
    },
    {
        "header": "3When Does Post-Training Incentivize Reasoning Beyond the Base Model?",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/x3.png",
                "caption": "Figure 3:pass@kperformance on three tasks: ID (op=2-10), OOD-edge (op=11-14), OOD-hard (op=(15-20)).\nRL is applied to four different data regimes (colors). RL on ID tasks never improves beyond the base model atpass@128. RL consistently improvespass@128on harder tasks when applied beyond the base model‚Äôs capacity.",
                "position": 274
            }
        ]
    },
    {
        "header": "4How Does Pre-training Exposure Shape Post-Training Generalization?",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/x4.png",
                "caption": "Figure 4:pass@128performance oncontext¬†Bafter post-trained with a 50%context¬†A+ 50%context¬†Bmixture. Different lines represent levels of pre-training exposure to long-tailedcontext¬†Batomicop=2examples. RL incentivizes contextual generalization when the model has minimal exposure (‚â•\\geq1%) tocontext¬†Bin pre-training.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x5.png",
                "caption": "",
                "position": 380
            }
        ]
    },
    {
        "header": "5How Does Mid-Training Interact with Post-Training?",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/x6.png",
                "caption": "Figure 6:pass@1andpass@128performances on extrapolative tasks under varying mid- and post-training mixture ratios. The data used in mid- and post-training is applied within the OOD-edge ranges. Different lines indicate the compute allocation strategies. Heavy-RL always improves the unseen OOD-hard tasks, while Light-RL improves bestpass@1on OOD-edge tasks.",
                "position": 443
            }
        ]
    },
    {
        "header": "6Mitigating Reward Hacking via Process Supervision in Outcome Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/x7.png",
                "caption": "Figure 7:pass@kperformance under different reward compositions.\nEach bar corresponds to a distinct reward-mixing strategy. Incorporating process-level information into the outcome reward consistently yields measurable performance gains across evaluation settings.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x8.png",
                "caption": "",
                "position": 561
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07783/x9.png",
                "caption": "Figure 9:Pre-training dynamics across varying operation ranges: In-distribution tasks (op=2-10), edge-of-competence OOD tasks (op=11-14), and OOD-hard tasks (op=15-20). The plots show the performance measured bypass@kover training steps.",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x10.png",
                "caption": "",
                "position": 1166
            },
            {
                "img": "https://arxiv.org/html/2512.07783/figures/composition_nll.png",
                "caption": "Figure 10:NLL reduction compared with the base model.White boxes denote RL-trained operation ranges. NLL gains decay smoothly as the evaluation range diverges from the RL-trained operations. Notably, RL onop=11-14achieves the largest NLL reduction onop=15-20.",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x11.png",
                "caption": "Figure 11:Reward dynamics across different post-training data recipes.RL onop=9-12andop=11-14tasks, which are calibrated to the model‚Äôsedge of competence, leads to genuine improvements in reasoning. However, when the task difficulty is either too easy or too hard, the reward stagnates, indicating limited learning progress.",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x12.png",
                "caption": "Figure 12:pass@kperformance on contextual generalization tasks after post-training with varying exposure tocontext¬†B. With shared reasoning primitives during pre-training, models exhibit strong transfer tocontext¬†Beven with limited or no exposure during post-training.",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x13.png",
                "caption": "Figure 13:pass@kperformance for different contexts with base model limited to basic atoms forcontext¬†B. Post-training oncontext¬†Amaintains stable performance, while exposure of 10%context¬†Bduring RL enables contextual transfer.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x14.png",
                "caption": "Figure 14:Reward dynamics across different post-training data recipes. When RL exposure tocontext¬†Bis extremely limited (0-1%), the reward stagnates. However, with moderate exposure (10-100%), the reward improves significantly, reflecting effective learning and transfer.",
                "position": 1362
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x15.png",
                "caption": "Figure 15:pass@128performance on extrapolative tasks after post-training onop=11-14, under varying levels of hard-data exposure during pre-training.",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x16.png",
                "caption": "Figure 16:Reward dynamics across different pre-training data recipes. Models with moderate hard-data exposure (20-50%) during pre-training exhibit significant reward improvements during post-training, indicating effective learning and extrapolation. In contrast, models with either too little (0%) or too much (100%) hard-data exposure show limited reward gains, suggesting constrained learning progress.",
                "position": 1416
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x17.png",
                "caption": "Figure 17:Reward dynamics across different pre-training data recipes.\nModels with minimal exposure to long-tailed contexts exhibit no reward improvement during post-training. While models with moderate to full exposure show significant reward improvements, indicating effective learning and contextual generalization.",
                "position": 1428
            },
            {
                "img": "https://arxiv.org/html/2512.07783/x18.png",
                "caption": "Figure 18:pass@kperformance for different mid-training and RL mixing ratios under varying total compute budgets.",
                "position": 1854
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]