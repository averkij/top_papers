[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22457/x1.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2505.22457/x2.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22457/x3.png",
                "caption": "Figure 1:Comparison of Video Instruction Tuning tasks.(1)Video Q&A: Extracting answers from a single key frame; (2)Captioning: Summarizing from frame-by-frame visual perception of observed videos; (3)Next-Event Prediction: Predicting the summary of future frames by visual perception of observed past frames and temporal reasoning with commonsense knowledge. As the example in the given first part video, after a defensive stop, the team may push fast in transition (knowledge)—but with under two minutes left in the fourth quarter (visual facts), a coach might call a timeout, or the players may slow the tempo to ensure careful execution.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Next-Event Prediction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22457/x4.png",
                "caption": "Figure 2:Reasoning structure underlying NEP. Each node is a potential event or action derived from visual cues, branching into alternative scenarios such as failing to defend or being pushed in transition. The red line highlights actual event sequence observed in the video. Comments provide reasoning for less likely scenarios.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2505.22457/x4.png",
                "caption": "Figure 2:Reasoning structure underlying NEP. Each node is a potential event or action derived from visual cues, branching into alternative scenarios such as failing to defend or being pushed in transition. The red line highlights actual event sequence observed in the video. Comments provide reasoning for less likely scenarios.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2505.22457/extracted/6478464/images/stat_output.png",
                "caption": "Figure 3:Distribution of data source and video length in V1-33K.The inner circle illustrates the distribution of data sources. The outer circle further segments each source according to video length categories. Only length categories comprising more than 4% of the dataset are labeled explicitly in the outer circle.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2505.22457/x5.png",
                "caption": "Figure 4:Overview of the four-stage V1-33K construction pipeline: Fact Translation, Analysis, Segmentation, and Reasoning & Critique.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2505.22457/x6.png",
                "caption": "Figure 5:Task demonstration of FutureBench. This figure presents two paradigms for future event prediction: Extrapolation and Interpolation. In theExtrapolation task (Top), the model observes the initial video (Current Event) and is required to sequentially predict a series of future events (Caption 1 → Caption 2 → Caption 3 → …) leading up to the final event (Caption N). In theInterpolation task (Bottom), the model observes the initial video (Current Event) and is provided with the first future event (Caption 1), an anchor future event (Caption K), and the final event (Caption N) and must infer the most plausible intermediate events that bridge the temporal gap. Distractors involve Caption 0 of the current event to require the model to understand the given video. Questions and answer options above are simplified for clarity and brevity.",
                "position": 223
            }
        ]
    },
    {
        "header": "3FutureBench",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22457/x7.png",
                "caption": "Figure 6:Three types of logic reasoning in video instruction tuning tasks. (1)Induction(Video Q&A): The model watches entire video sequences and learns common event patterns and temporal relationships, building an internal “engine” of how visual events unfold over time. (2)Deduction(Next Event Prediction): Given the first part of a video, the model uses its learned causal and commonsense knowledge to extrapolate and predict the most likely next events. (3)Abduction(Previous Event Prediction): Presented with the final segment of a video, the model reasons backward to hypothesize plausible prior events or hidden causes that explain the observed outcome.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.22457/x8.png",
                "caption": "Figure 7:Performance comparison of different data scales for SFT, CFT, Distill, and Mix tuning on Qwen2.5-VL-7B-Instruct. The top showcases the curves for general benchmarks, and the bottom showcases the curves for temporal benchmarks.",
                "position": 597
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix: Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22457/x9.png",
                "caption": "Figure 8:Data Construction Pipeline.",
                "position": 1412
            }
        ]
    },
    {
        "header": "Appendix BAppendix: Detailed Data Construction Pipeline",
        "images": []
    },
    {
        "header": "Appendix CAppendix: Training Strategy",
        "images": []
    },
    {
        "header": "Appendix DAppendix: Prompt",
        "images": []
    },
    {
        "header": "Appendix EAppendix: Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FAppendix: Limitation",
        "images": []
    },
    {
        "header": "Appendix GAppendix: Broader Impacts",
        "images": []
    },
    {
        "header": "Appendix HLicenses",
        "images": []
    }
]