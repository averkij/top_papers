[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Left-Skewed Position Frequency Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18745/x1.png",
                "caption": "(a)Natural data distribution",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x1.png",
                "caption": "(a)Natural data distribution",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x2.png",
                "caption": "(b)Uniform data distribution",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x3.png",
                "caption": "(c)Concatenated data distribution",
                "position": 160
            }
        ]
    },
    {
        "header": "3A Probing Experiment on Position Frequency and Model Effective Length",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18745/x4.png",
                "caption": "(a)Effective length vs. consumed tokens",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x4.png",
                "caption": "(a)Effective length vs. consumed tokens",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x5.png",
                "caption": "(b)Effective length vs. position frequency",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x6.png",
                "caption": "Figure 3:Position frequency distribution for models trained with different training lengths after consuming 1T tokens. With the same number of tokens, training length has little effect on small relative positions. For example, the relative position 0 appears 4K times in both a single 4K sequence and two 2K sequences with the same total token count of 4K in each case.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x7.png",
                "caption": "Figure 4:NIAH results for our pretrained model TinyLlama-1.3B (2K) and Llama3.1 (128K) where the X-axis means input context length and the Y-axis represents the document depth. In this figure, we clearly observe that for TinyLlama 2K and Llama3.1 128K, most poor-performing cases are concentrated in the lower-left triangle, indicating that the models are unable to gather distant needles.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Shifted Rotary Position Embedding",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18745/x8.png",
                "caption": "Figure 5:A illustrative example ofStRingfor a sequence length ofL=9ùêø9L=9italic_L = 9. (a) Position indices6666,7777, and8888are removed from the matrix. (b) Indices00,1111,2222,3333,4444, and5555are shifted from the main diagonal to the lower-left triangle with an offset of3333. (c) A small constantWùëäWitalic_Wis added to all diagonals wherem‚â•n‚àí3ùëöùëõ3m\\geq n-3italic_m ‚â• italic_n - 3, thereby restoring emphasis on the neighboringWùëäWitalic_Wtokens. The position matrix of Llama3.1-128K usingStRingis shown in Figure8Appendix.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x9.png",
                "caption": "(a)Ablation on local windowWùëäWitalic_W(S=L3ùëÜùêø3S=\\frac{L}{3}italic_S = divide start_ARG italic_L end_ARG start_ARG 3 end_ARG)",
                "position": 936
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x9.png",
                "caption": "(a)Ablation on local windowWùëäWitalic_W(S=L3ùëÜùêø3S=\\frac{L}{3}italic_S = divide start_ARG italic_L end_ARG start_ARG 3 end_ARG)",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x10.png",
                "caption": "(b)Ablation on shifted offsetSùëÜSitalic_S(W=128ùëä128W=128italic_W = 128)",
                "position": 944
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18745/x11.png",
                "caption": "Figure 8:The resulted position matrix of Llama3.1 128K after shifting. In Figure (a), we use a shifted offset ofL3‚âà42ùêø342\\frac{L}{3}\\approx 42divide start_ARG italic_L end_ARG start_ARG 3 end_ARG ‚âà 42K and the local windowWùëäWitalic_Wis 128. In Figure (b), we overwrite more infrequent positions and the shifted offset isS=L2=64ùëÜùêø264S=\\frac{L}{2}=64italic_S = divide start_ARG italic_L end_ARG start_ARG 2 end_ARG = 64K.",
                "position": 2114
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x12.png",
                "caption": "(a)Inference time",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x12.png",
                "caption": "(a)Inference time",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2410.18745/x13.png",
                "caption": "(b)GPU memory consumption",
                "position": 2150
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]