[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07463/x1.png",
                "caption": "Figure 1:The figure shows the overall processing pipeline of our dataset. Our main processing flow includes deduplication, scoring the data, and some other filtering process to get our CCI4.0-M2-Base V1 dataset. Notably, we incorporated COT synthetic data by leveraging large language models to perform chunking, summarization, and extraction operations on the original documents sampled from our dataset pool to get the CCI4.0-M2-COT V1 dataset.",
                "position": 243
            }
        ]
    },
    {
        "header": "4Experimental Setting",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/main.png",
                "caption": "Figure 2:Performance Comparison of Different Datasets Across Training Scales.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/reason1.png",
                "caption": "Figure 3:Reasoning ability scores on adversarial datasets for a 0.5B dense model trained with (w/ cot synthesis) and without (w/o cot synthesis) CoT data mix, evaluated across 100B training tokens. Training with CoT data accelerates reasoning ability growth.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/f_group3.png",
                "caption": "Figure 4:Performance Comparison of Models With and Without CoT Synthesis",
                "position": 509
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/loss.png",
                "caption": "Figure 5:Loss values across domains and percentiles.",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/reason2.png",
                "caption": "Figure 6:Reasoning ability scores on adversarial datasets for a 1.4B MoE model as pre-training compute increases (up to 800B tokens of CoT data). Reasoning ability improves consistently with increased training compute.",
                "position": 1517
            },
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/cls.png",
                "caption": "Figure 7:Performance Comparison of Different Datasets Across Training Scales.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2506.07463/extracted/6524064/fig/f_group1.png",
                "caption": "Figure 8:Performance Comparison of Models With and Without Loss-Based Filtering",
                "position": 1634
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]