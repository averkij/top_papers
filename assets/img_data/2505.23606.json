[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23606/x1.png",
                "caption": "Figure 1:Four types of unified generative models. More details can be found in Sec.2.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23606/x2.png",
                "caption": "Figure 2:The training and inference architecture of Muddit.(a)During training, we randomly mask tokens from one of the two modalities. MM-DiT is trained to predict the masked tokens using a re-weighted cross-entropy loss, which jointly optimizes both the MM-DiT backbone and a lightweight text decoder.(b)In text-to-image inference, we initialize the image latent features using all-masked tokens and iteratively predict each latent token via MM-DiT.(c)In image-to-text inference, we similarly initialize all text tokens as masked and generate the text through the same iterative decoding process. Specifically for VQA tasks, we append mask token IDs to the end of the question and predict all masked token IDs as the final answer.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23606/x3.png",
                "caption": "Figure 3:Samples of Text-to-Image Generation by Muddit.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x4.png",
                "caption": "Figure 4:Samples of Visual Question Answering by Muddit.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x5.png",
                "caption": "Figure 5:Samples of Image-to-Text Generation by Muddit.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2505.23606/extracted/6492678/fig/inference_speed.png",
                "caption": "Figure 6:Inference speed comparison. We use 32 inference steps for Muddit and fix the sequence length to 77 across all models.",
                "position": 1126
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23606/x6.png",
                "caption": "Figure 7:Image-to-text generated results.",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x7.png",
                "caption": "Figure 8:Text-to-image generation results.",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x8.png",
                "caption": "Figure 9:Visual question answering results.",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x9.png",
                "caption": "Figure 10:Image-guided text editing results.",
                "position": 1284
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x10.png",
                "caption": "Figure 11:Image-to-text generated results in each step.",
                "position": 1287
            },
            {
                "img": "https://arxiv.org/html/2505.23606/x11.png",
                "caption": "Figure 12:Image-to-text generated results in each step.",
                "position": 1290
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]