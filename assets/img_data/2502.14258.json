[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14258/x1.png",
                "caption": "Figure 1:Temporal Heads exist within various TKCs at different timesTksubscriptùëáùëòT_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.\nAblating them disrupts the model‚Äôs temporal alignment, yielding incorrect objects.",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x2.png",
                "caption": "Figure 2:Overview of temporal knowledge circuit analysis.\n(A): Construct temporal knowledge circuits (TKCs), and compare it with general knowledge circuits (KCs) using time-invariant knowledge.\nCircuits reproduce residual streams for timeT, subjectSand relationR.\nThis verifies temporal heads only found in each different TKCs of various yearTksubscriptùëáùëòT_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.\n(B): Example of simplified TKC.\nHere, basic knowledge nodes is coloredviolet, (common in both), whileTemporal Headsis highlited.\n(C): Attention map for temporal heads.a15.h0means the 15th layer‚Äôs first attention head.\nEach head‚Äôs attention pattern is represented as the output logits of the hean by mapping to vocabulary space.\nQueries are input tokens focusing on others, while keys are the tokens being focused on.\nValues represent attention weights, indicating the strength of this focus.\nTotal results are in Figures7‚Äì8and9‚Äì11.",
                "position": 220
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Knowledge Circuit Deciphers Temporal Head in LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14258/x3.png",
                "caption": "Figure 3:Log probability results with temporal knowledge;In XXXX, the president of South Korea was.\n(A) shows prediction probability change among results of Llama2.\nThe effect of head ablation reacts differently for each selected year with the same prompt.\nEach subplot in (A) represents the probability distribution of correct (green) and incorrect (red) predictions, where the x-axis denotes probability values and the y-axis differentiates between target and non-target responses.\nTotal results for each model are in Figures12‚Äì13in Appendix.\n(B) illustrates the performance degradation trends across various years.\nAs averaging the result of ablation, thegray spacebetween two line plots represent degradation level pointed out byred arrows(which becomes darker and bigger when the gap is wider).\nThe background shows how objects were changed in the time range between 1999 to 2009.",
                "position": 590
            }
        ]
    },
    {
        "header": "4In-Depth Analysis of Temporal Heads",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14258/x4.png",
                "caption": "Figure 4:Head ablation effect across various knowledge types.\nThree selcted model shows distinct differentiation for temporal knowledge (left side) and time invariant knowledge (right side).\nThe change of performance is calculated with the average score of baseline (non-ablation) and modified (ablated result), using model specific temporal head information.\nWhile degrees of degradation is different among models, overall tendency reflects the importance of temporal head to inference temporal knowledge.",
                "position": 764
            }
        ]
    },
    {
        "header": "5Temporal Knowledge Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14258/x5.png",
                "caption": "Figure 5:Example Of Temporal Knowledge Editing.\nFrom the source prompt, we catch the specific attention value of model‚Äôs head, for example,a18.h3.\nBy simply adding it to target prompt, the model‚Äôs output is changed into temporally correct answer from temporally wrong answer.\nThe headmap below denotes the number of success in editing for every combination of layers and heads.\nThe most successful case in here is temporal headsa18.h3as highlighted, following other heads such as backup temporal headsa20.h17.",
                "position": 821
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14258/x6.png",
                "caption": "Figure 6:Results of Causal Tracing for all position(subject, relation, object), six plots for each cases from the top to middle and bottom.\nThe restoring part is set to each temporal conditioning, in two different age: 1999 and 2004.\n(Illustrative) Causal tracing heatmaps showing how restoring different layers (x-axis) after temporal corruption affectsp‚Å¢(New)pNew\\mathrm{p}(\\text{New})roman_p ( New )orp‚Å¢(Barcelona)pBarcelona\\mathrm{p}(\\text{Barcelona})roman_p ( Barcelona ).\nFor the object position, we set a simulated[Object]for the place holder.\nEach figure‚Äôs left column represents single-layer restoration; the center and right columns reflect MLP vs.¬†attention intervals.\nRestoring subject+year at mid layers yields pronounced differences (dark regions).\nOn the other hand, restoring relation+year or object+year yields trivial differences as their range is overlap significantly.",
                "position": 2963
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x7.png",
                "caption": "",
                "position": 2967
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x8.png",
                "caption": "",
                "position": 2969
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x9.png",
                "caption": "Figure 7:Temporal knowledge circuit of Llama2.\nIt is simplified version of total circuit by its importance of each nodes usingœÑ=0.1ùúè0.1\\tau=0.1italic_œÑ = 0.1as threshold.",
                "position": 2980
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x10.png",
                "caption": "Figure 8:Temporal knowledge circuit of Qwen 1.5 and Phi 3 mini.\nThose are simplified version of total circuit according to each nodes and edges‚Äô importance of using sameœÑ=0.1ùúè0.1\\tau=0.1italic_œÑ = 0.1as threshold.",
                "position": 2985
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x11.png",
                "caption": "",
                "position": 2989
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x12.png",
                "caption": "Figure 9:Total map of attention with Llama2-7b-chat-hf, for each temporal heads and backup temporal heads.\nThe left side of border line is the attention map ofTemporal Heads, and the other side is the result ofBackup Temporal Heads.",
                "position": 2995
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x13.png",
                "caption": "Figure 10:Total map of attention with Qwen1.5-7B-Chat, for each temporal heads and backup temporal heads.\nThe left side of border line is the attention map ofTemporal Heads, and the other side is the result ofBackup Temporal Heads.",
                "position": 3000
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x14.png",
                "caption": "Figure 11:Total map of attention with Phi-3-mini-4k-instruct, for each temporal heads and backup temporal heads.\nThe left side of border line is the attention map ofTemporal Heads, and the other side is the result ofBackup Temporal Heads.",
                "position": 3005
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x15.png",
                "caption": "Figure 12:Total results of Llama2-7b-chat-hf, head ablation inference with log probability.",
                "position": 3010
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x16.png",
                "caption": "Figure 13:Total results of Qwen1.5-7B-Chat and Phi-3-mini-4k-instruct, head ablation inference with log probability.\n(A) denotes the result of Qwen 1.5 and (B) represents the result of Phi 3 mini.",
                "position": 3014
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x17.png",
                "caption": "Figure 14:Temporal knowledge circuit from textual temporal conditioned prompt.\nHere, we change the temporal condition\"In 1999\"into\"In the year the Champions League final was held in Barcelona\", which model already correctly recalls the answerMalm√∂ FF.\nThe temporal knowledge circuit successfully catches temporal conditioning even with alias based on event based textual conditioning, with correctly showing off temporal knowledge heads and some backup temporal heads.\nFigure of downside is the attention maps for each temporal heads and backup temporal heads.\nEach of those figures highlight various tokens in conditioning part of prompt.",
                "position": 3019
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x18.png",
                "caption": "",
                "position": 3023
            },
            {
                "img": "https://arxiv.org/html/2502.14258/x19.png",
                "caption": "Figure 15:Result Of temporal knowledge editing in Qwen 1.5 7B Chat and Phi 3 mini 4k instruct.\nFrom the source prompt, we catch the attention value of each model‚Äôs temporal head,a17.h15anda10.h13.\nThe model‚Äôs output is changed into temporally correct answer from temporally wrong answer.\nThe headmap below denotes the number of success in editing for every combination of layers and heads.\nThough the most successful case of editing is the temporal heada17.h15in Qwen 1.5 7B Chat, Phi 3 mini 4k instruct shows that adding attention had minimal impact, and temporal heads failed to enable effective editing.\nThis suggests that the model, constrained by its small parameter size (3.8B), requires a more sophisticated vector steering mechanism rather than relying on a single attention head value modification.",
                "position": 3032
            }
        ]
    },
    {
        "header": "8Appendix",
        "images": []
    }
]