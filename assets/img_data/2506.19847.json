[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19847/x1.png",
                "caption": "Figure 1:OFTv2 significantly reduces training time and GPU memory usage without sacrificing performance. The finetuning is performed with Qwen2.5-7B.",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2506.19847/x2.png",
                "caption": "Figure 2:Comparison between LoRA and OFT.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2506.19847/x3.png",
                "caption": "Figure 3:Comparison between sequential (e.g., OFT) and parallel (e.g., LoRA) adaptation.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2506.19847/x4.png",
                "caption": "Figure 4:Results of GPU memory usage for the same finetuning task. (a) OFT, LoRA and OFTv2 on Qwen2.5; (b) QLoRA and QOFT on NF4-quantized Qwen2.5; (c) QLoRA and QOFT on AWQ-qunatized Qwen2.5.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2506.19847/x5.png",
                "caption": "Figure 5:Qualitative results from Dreambooth finetuning of Stable Diffusion 3.5 Large (8.1B parameters), with peak allocated GPU memory: LoRA (52.33 GB), OFT (52.32 GB), QLoRA (41.60 GB) and QOFT (41.53 GB).",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2506.19847/x6.png",
                "caption": "Figure 7:Qualitative results from Dreambooth fine-tuning of Stable Diffusion 3.5 Medium (8.1B parameters), with peak allocated GPU memory: LoRA (38.00 GB), OFT (38.02 GB), QLoRA (35.03 GB) and QOFT (35.02 GB).",
                "position": 2448
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]