[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14051/x1.png",
                "caption": "",
                "position": 159
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14051/figures/ODA_provided_gemini.png",
                "caption": "Figure 1:Overview of the OpenDataArena framework. We provide four integral components: a Data Value Leaderboard for standardized benchmarking, a Multi-dimension Data Scorer for granular quality assessment, a Data Analysis Platform for lineage and composition tracing, and an Open-source Evaluation Toolkit to ensure reproducibility.",
                "position": 167
            }
        ]
    },
    {
        "header": "2The OpenDataArena Platform",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14051/figures/ODA_framework.png",
                "caption": "Figure 2:An overview process of OpenDataArena, which includes a four-stage data evaluation/benchmarking pipeline we designed (data input layer, data evaluation layer, data analysis layer, and data visualization layer), the user interaction module, and open-source tools.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2512.14051/figures/ODA_leaderboard.png",
                "caption": "Figure 3:A summarized visualization of data leaderboards. More detailed leaderboards on different domains can be found in the project page:https://opendataarena.github.io/leaderboard.html.",
                "position": 247
            }
        ]
    },
    {
        "header": "3Dataset Landscape and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14051/x2.png",
                "caption": "Figure 4:Evolution of the post-training dataset landscape (Q1 2023 – Q3 2025), the chart tracks the quarterly rate of newly released datasets alongside the cumulative volume of training samples.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x3.png",
                "caption": "(a)Distribution by Domain.",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x3.png",
                "caption": "(a)Distribution by Domain.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x4.png",
                "caption": "(b)Temporal Evolution.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x5.png",
                "caption": "Figure 6:High-level overview of data lineage relationships, where node size reflects data download count, different colors represent distinct data sub-networks, and darker colors indicate higher-degree nodes with greater importance.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2512.14051/figures/p1.png",
                "caption": "Figure 7:Data lineage relationships of the first four layers foralibaba-pai/OmniThought-0528(43 data nodes in total), where blue nodes represent target data, green nodes represent traceable nodes in source data, and orange nodes represent basic datasets.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2512.14051/figures/p22.png",
                "caption": "Figure 8:Data lineage relationship composition ofSynthLabsAI/Big-Math-RL-Verifiedandagentica-org/DeepCoder-Preview-Dataset, where the former incorporates the benchmarkOmni-MATHin its data makeup and the latter incorporates the benchmarkLiveCodeBench-v5.",
                "position": 539
            }
        ]
    },
    {
        "header": "4Findings from Benchmarking",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14051/x6.png",
                "caption": "(a)Absolute Performance.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x6.png",
                "caption": "(a)Absolute Performance.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x7.png",
                "caption": "(b)Performance Delta (Δ\\Delta).",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x8.png",
                "caption": "Figure 10:Temporal evolution of post-training dataset quality across domains, evaluated using Qwen2.5-7B as the base model. The chart shows the performance of datasets released in each quarter from 2023Q2 to 2025Q3.",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x9.png",
                "caption": "(a)General Efficiency",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x9.png",
                "caption": "(a)General Efficiency",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x10.png",
                "caption": "(b)Math Efficiency",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x11.png",
                "caption": "(c)Code Efficiency",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x12.png",
                "caption": "(d)Science Efficiency",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x13.png",
                "caption": "(a)Impact of Response Length.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x13.png",
                "caption": "(a)Impact of Response Length.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2512.14051/x14.png",
                "caption": "(b)Impact of Instruction Complexity.",
                "position": 855
            }
        ]
    },
    {
        "header": "5Looking Forward",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Contribution List",
        "images": []
    },
    {
        "header": "7Detailed Benchmarking Settings",
        "images": []
    },
    {
        "header": "8Comparative Performance Alignment Rankings of Datasets on Qwen2.5 vs. Qwen3 Models",
        "images": []
    },
    {
        "header": "9Data Scoring Metric Definitions",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14051/figures/section4/Correlation_Heatmap.png",
                "caption": "Figure 13:Quality vs. performance correlation matrix. The heatmap displays Spearman correlations between automated quality metrics and domain performance, highlighting the superior predictive power of Response-side metrics (QA) compared to Instruction-side metrics (Q).",
                "position": 1765
            },
            {
                "img": "https://arxiv.org/html/2512.14051/figures/data_lineage.png",
                "caption": "Figure 14:Data lineage visualization. An interactive graph showing dataset families, derivation relationships and so on.",
                "position": 1891
            },
            {
                "img": "https://arxiv.org/html/2512.14051/figures/ODA_data_comparison.png",
                "caption": "Figure 15:Dataset comparison interface. A visualization comparing multiple datasets across model performance and multi-dimensional quality metrics.",
                "position": 1894
            }
        ]
    },
    {
        "header": "10Visualized Results",
        "images": []
    }
]