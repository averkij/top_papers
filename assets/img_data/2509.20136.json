[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x1.png",
                "caption": "Figure 1:A visual programming about the flappy bird style arcade game.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x2.png",
                "caption": "Figure 2:Overview of the V-GameGym framework from data collection to evaluation.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x3.png",
                "caption": "Figure 3:Correlation between model size and games solved.",
                "position": 209
            }
        ]
    },
    {
        "header": "2V-GameGym",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x4.png",
                "caption": "Figure 4:Overall Requirement Length Distribution",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x5.png",
                "caption": "Figure 5:Linear-Scale Requirement Length Histogram",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x6.png",
                "caption": "Figure 6:Log-Scale Requirement Length Histogram",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x7.png",
                "caption": "Figure 7:Requirement Length Comparison by Cluster",
                "position": 242
            }
        ]
    },
    {
        "header": "3Experiment Setup",
        "images": []
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x8.png",
                "caption": "Figure 8:Radar chart comparing the top 10 models across four key performance dimensions.",
                "position": 1230
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x9.png",
                "caption": "Figure 9:Distribution of evaluation scores across three key dimensions: Code, Screenshot, and Video.",
                "position": 1233
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x10.png",
                "caption": "Figure 10:Game difficulty distribution by number of solving models.",
                "position": 1236
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x11.png",
                "caption": "Figure 11:Performance comparison of top 15 models across Easy, Medium, and Hard difficulty tiers, showing consistent ranking patterns and scaling challenges.",
                "position": 1251
            },
            {
                "img": "https://arxiv.org/html/2509.20136/figures/figures_evals/09_capability_correlation_matrix.png",
                "caption": "Figure 12:Correlation matrix between Code, Screenshot, and Video evaluation dimensions, demonstrating the interdependence of multimodal capabilities in game development.",
                "position": 1254
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComprehensive Leaderboard Ranking Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x12.png",
                "caption": "Figure 13:Comprehensive Leaderboard Ranking Models by the Number of Games Successfully Solved Out.",
                "position": 1965
            }
        ]
    },
    {
        "header": "Appendix BComplete Leaderboard",
        "images": []
    },
    {
        "header": "Appendix CComprehensive Performance Comparison Across Different Evaluation Dimensions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x13.png",
                "caption": "(a)Final Performance",
                "position": 2861
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x13.png",
                "caption": "(a)Final Performance",
                "position": 2864
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x14.png",
                "caption": "(b)Code Generation Performance",
                "position": 2870
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x15.png",
                "caption": "(c)Image Evaluation Performance",
                "position": 2876
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x16.png",
                "caption": "(d)Video Evaluation Performance",
                "position": 2882
            }
        ]
    },
    {
        "header": "Appendix DV-GameGym Reference Code Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x17.png",
                "caption": "(a)Violin plot showing distribution peaked at  8,500 characters.",
                "position": 2896
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x17.png",
                "caption": "(a)Violin plot showing distribution peaked at  8,500 characters.",
                "position": 2899
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x18.png",
                "caption": "(b)Histogram showing symmetric distribution.",
                "position": 2904
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x19.png",
                "caption": "(c)Log-scale view showing full range distribution.",
                "position": 2909
            }
        ]
    },
    {
        "header": "Appendix EV-GameGym Word Cloud Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x20.png",
                "caption": "Figure 16:V-GameGym Comparative Word Cloud Analysis of Requirements and Code Corpora.",
                "position": 2923
            }
        ]
    },
    {
        "header": "Appendix FV-GameGym Reference Code Patterns Quantitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x21.png",
                "caption": "Figure 17:Distribution of code complexity scores.",
                "position": 2933
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x22.png",
                "caption": "(a)Occurrence frequency of core game loop patterns.",
                "position": 2936
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x22.png",
                "caption": "(a)Occurrence frequency of core game loop patterns.",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x23.png",
                "caption": "(b)Distribution of code structure elements.",
                "position": 2944
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x24.png",
                "caption": "(c)Import frequency of common Python libraries.",
                "position": 2949
            }
        ]
    },
    {
        "header": "Appendix GV-GameGym Quality Score Prediction Model Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x25.png",
                "caption": "(a)Comparison of actual vs. predicted quality scores.",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x25.png",
                "caption": "(a)Comparison of actual vs. predicted quality scores.",
                "position": 2990
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x26.png",
                "caption": "(b)Distribution of prediction residuals.",
                "position": 2995
            }
        ]
    },
    {
        "header": "Appendix HV-GameGym Distribution of Game Samples Across the Top 30 Source Repositories",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x27.png",
                "caption": "Figure 20:Distribution of game samples across the top 30 source repositories.",
                "position": 3021
            }
        ]
    },
    {
        "header": "Appendix IModel Similarity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x28.png",
                "caption": "Figure 21:Hierarchical clustering of models based on solved game overlap using Jaccard similarity index.",
                "position": 3031
            }
        ]
    },
    {
        "header": "Appendix JScore Threshold Sensitivity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x29.png",
                "caption": "Figure 22:Pass rate variations across different score thresholds, demonstrating ranking stability.",
                "position": 3041
            }
        ]
    },
    {
        "header": "Appendix KScore Distribution Characteristics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x30.png",
                "caption": "Figure 23:Violin plots showing score distribution patterns for top 20 models.",
                "position": 3051
            }
        ]
    },
    {
        "header": "Appendix LRepresentative Head-to-Head Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x31.png",
                "caption": "(a)Head-to-head comparison of Gemini-2.5-pro and GPT-OSS-120B.",
                "position": 3061
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x31.png",
                "caption": "(a)Head-to-head comparison of Gemini-2.5-pro and GPT-OSS-120B.",
                "position": 3064
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x32.png",
                "caption": "(b)Head-to-head comparison of Gemini-2.5-pro and Grok-4.",
                "position": 3069
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x33.png",
                "caption": "(c)Head-to-head comparison of Gemini-2.5-pro and o4-mini.",
                "position": 3074
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x34.png",
                "caption": "(d)Head-to-head comparison of Gemini-2.5-pro and Qwen3-235B-A22B-Thinking-2507.",
                "position": 3079
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x35.png",
                "caption": "(e)Head-to-head comparison of GPT-OSS-120B and DeepSeek-V3-0324.",
                "position": 3085
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x36.png",
                "caption": "(f)Head-to-head comparison of GPT-OSS-120B and GPT-OSS-20B.",
                "position": 3090
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x37.png",
                "caption": "(g)Head-to-head comparison of GPT-OSS-120B and Grok-4.",
                "position": 3095
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x38.png",
                "caption": "(h)Head-to-head comparison of GPT-OSS-120B and Qwen3-235B-A22B.",
                "position": 3100
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x39.png",
                "caption": "(i)Head-to-head comparison of Grok-4 and DeepSeek-V3-0324.",
                "position": 3106
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x40.png",
                "caption": "(j)Head-to-head comparison of Grok-4 and GPT-OSS-20B.",
                "position": 3111
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x41.png",
                "caption": "(k)Head-to-head comparison of o4-mini and GPT-OSS-120B.",
                "position": 3116
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x42.png",
                "caption": "(l)Head-to-head comparison of o4-mini and Grok-4.",
                "position": 3121
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x43.png",
                "caption": "(m)Head-to-head comparison of Qwen3-235B-A22B-Thinking-2507 and DeepSeek-V3-0324.",
                "position": 3127
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x44.png",
                "caption": "(n)Head-to-head comparison of Qwen3-235B-A22B-Thinking-2507 and GPT-OSS-120B.",
                "position": 3132
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x45.png",
                "caption": "(o)Head-to-head comparison of Qwen3-235B-A22B-Thinking-2507 and Grok-4.",
                "position": 3137
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x46.png",
                "caption": "(p)Head-to-head comparison of Qwen3-235B-A22B-Thinking-2507 and o4-mini.",
                "position": 3142
            }
        ]
    },
    {
        "header": "Appendix MComprehensive Performance Heatmap",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x47.png",
                "caption": "Figure 25:Performance matrix of top 25 models on the 60 most challenging games, ordered by increasing difficulty and overall model performance.",
                "position": 3156
            }
        ]
    },
    {
        "header": "Appendix NSeed Code Dataset Quality Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20136/x48.png",
                "caption": "Figure 26:Cluster size distribution and sample selection strategy, showing uniform selection of 25 samples from each of the 100 clusters across 168,287 total objects.",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x49.png",
                "caption": "Figure 27:Distribution of code quality scores across 2,500 selected samples. And 2500 is the seed set selected after clustering, and 2219 is the final test set after LLM pipeline and manual verification.",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x50.png",
                "caption": "Figure 28:File size distribution showing log-normal characteristics with mean 10.2 KB and median 5.5 KB, indicating predominantly compact but complete implementations.",
                "position": 3178
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x51.png",
                "caption": "Figure 29:Frequency analysis of Pygame module usage, with core modules like display (91.5%) and event handling (68.3%) showing high adoption rates.",
                "position": 3181
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x52.png",
                "caption": "Figure 30:Distribution of game types in the dataset, representation across eight distinct genres.",
                "position": 3190
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x53.png",
                "caption": "Figure 31:Structural metrics of code samples, showing average counts of functions (12.0), classes (2.2), and other programming constructs per file.",
                "position": 3193
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x54.png",
                "caption": "Figure 32:Correlation analysis between cluster size and average quality scores, showing weak correlation (0.138) that validates quality-based selection within clusters.",
                "position": 3214
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x55.png",
                "caption": "Figure 33:Box plot analysis of code complexity scores across game types, with physics simulations and RPGs showing highest complexity variance.",
                "position": 3217
            },
            {
                "img": "https://arxiv.org/html/2509.20136/x56.png",
                "caption": "Figure 34:Radar chart analysis of quality components across different score tiers, highlighting strengths in structure and organization for high-quality samples.",
                "position": 3226
            }
        ]
    },
    {
        "header": "Appendix OSystem Architecture and Performance",
        "images": []
    },
    {
        "header": "Appendix PGame Code Generation Pipeline",
        "images": []
    },
    {
        "header": "Appendix QGame Recording and Media Capture",
        "images": []
    },
    {
        "header": "Appendix RMulti-Modal Game Evaluation System",
        "images": []
    }
]