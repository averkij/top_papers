[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15667/x1.png",
                "caption": "",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15667/x2.png",
                "caption": "Figure 2:For the task of full-range 360-degree novel view synthesis, DiffPortrait360 employs a frozen pre-trained Latent Diffusion Model (LDM) as a rendering backbone and incorporates three auxiliary trainable modules for disentangled control of dual appearance‚Ñõ‚Ñõ\\mathcal{R}caligraphic_R, camera controlùíûùíû\\mathcal{C}caligraphic_C, and U-Nets with view consistencyùí±ùí±\\mathcal{V}caligraphic_V. Specifically,‚Ñõ‚Ñõ\\mathcal{R}caligraphic_Rextracts appearance information fromIrefsubscriptùêºrefI_{\\text{ref}}italic_I start_POSTSUBSCRIPT ref end_POSTSUBSCRIPTandIbacksubscriptùêºbackI_{\\text{back}}italic_I start_POSTSUBSCRIPT back end_POSTSUBSCRIPT, andùíûùíû\\mathcal{C}caligraphic_Cderives the camera pose, which is rendered using an off-the-shelf 3D GAN. During training, we utilize a continuous sampling training strategy to better preserve the continuity of the camera trajectory. We enhance attention to continuity between frames to maintain the appearance information without changes due to turning angles. For inference, we employ our tailored back-view image generation network‚Ñ±‚Ñ±\\mathcal{F}caligraphic_Fto generate a back-view image, enabling us to generate a 360-degree full range of camera trajectories using a single image portrait. Note thatzùëßzitalic_zstands for latent space noise rather than image.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15667/x3.png",
                "caption": "Figure 3:Qualitative comparisons with existing methods on in the wild portraits. Compared to the baselines, our method shows superior generalization capability to novel view synthesis of wild portraits with unseen appearances, expressions, and styles, even without any reliance on fine-tuning.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15667/x4.png",
                "caption": "Figure 4:Qualitative comparisons of novel view synterhsis on RenderMe360[35]. Our method achieves effective appearance control for novel synthesis under substantial change of camera view for synthesis.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x5.png",
                "caption": "Figure 5:Ablation Study on Dual Appearance Control.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x6.png",
                "caption": "Figure 6:Ablation on Back-view Generation.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x7.png",
                "caption": "Figure 7:Ablation Study on View-consistency Control.(a) Generated novel view. (b-c) Rendering and depth of fitted NeRFs.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15667/extracted/6294437/images/supply/backviewmodule.png",
                "caption": "Figure 8:Illustration of our back-view generation module‚Ñ±‚Ñ±\\mathcal{F}caligraphic_F. Given an arbitrary back-view camera condition and a reference image, we follow the methods of the reference module and ControlNet to decode a specific camera view. During inference, we set the back-view camera generation condition to 180 degrees to maximize the capture of appearance information from the back view.",
                "position": 1283
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x8.png",
                "caption": "Figure 9:Examples in our stylized data augmentation which includes extensive generation of stylized back appearances. Compared to training back-view generation module‚Ñ±‚Ñ±\\mathcal{F}caligraphic_Fsolely on real or synthetic networks, such augmentation helps our back-view generation module achieve greater generalizability.",
                "position": 1286
            }
        ]
    },
    {
        "header": "Appendix BMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15667/x9.png",
                "caption": "Figure 10:Ablation study on camera conditioning.",
                "position": 1316
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x10.png",
                "caption": "Figure 11:More comparisons with DiffPortrait3D[16]on more views.",
                "position": 1330
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x11.png",
                "caption": "Figure 12:More real-world results.",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x12.png",
                "caption": "Figure 13:More qualitative comparisons with existing methods on in the stylized portraits. Our method shows superior generalization capability to novel view synthesis of wild portraits with unseen appearances, expressions, and styles, even without any reliance on fine-tuning.",
                "position": 1346
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x13.png",
                "caption": "Figure 14:More qualitative comparisons of novel view synthesis on RenderMe360[35]. Our method achieves effective appearance control for novel synthesis under substantial change of camera view for synthesis.",
                "position": 1349
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x14.png",
                "caption": "Figure 15:More qualitative results of our method.",
                "position": 1352
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x15.png",
                "caption": "Figure 16:More qualitative results of our method.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x16.png",
                "caption": "Figure 17:More qualitative results of our method.",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2503.15667/x17.png",
                "caption": "Figure 18:Limitations of our method.",
                "position": 1366
            }
        ]
    },
    {
        "header": "Appendix CLimitation and Future Work",
        "images": []
    }
]