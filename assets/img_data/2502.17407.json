[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17407/x1.png",
                "caption": "Figure 1:Performance of Qwen2.5-1.5B-Math with different test-time scaling strategies.‚Äî‚ÄîOnce configured to use comparable inference FLOPs, all three methods (Outcome Reward Modeling, Process Reward Modeling, and Budget Forcing) achieve similar performance.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Multilingual Competition Level Math",
        "images": []
    },
    {
        "header": "3Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17407/x2.png",
                "caption": "Figure 2:Comparison of different inference-time scaling strategies.Blue boxes represent selected outputs, while red boxes indicate rejected ones.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2502.17407/x3.png",
                "caption": "Figure 3:# of generated tokens for 1.5B and 7B models in a greedy setting, divided by correctness.Languages are represented as scatter plots, overlaid on box plots.",
                "position": 350
            }
        ]
    },
    {
        "header": "4Result 1: ORM and PRM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17407/x4.png",
                "caption": "Figure 4:Gains of ORM compared to a greedy-decoding baseline.The semi-transparent ‚Äúcloud‚Äù indicates the 2D data distribution via a KDE density plot, and the overlaid lines are third-order polynomial regressions modeling how each ORM setting scales with the baseline score.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2502.17407/x5.png",
                "caption": "Figure 5:PRM inference FLOPs as a function of generation stepsSùëÜSitalic_Sand candidates per stepcùëêcitalic_c.The left panel uses a verifier size of 72B, while the right panel uses a 7B RM, displaying adjusted configurations to yield similar costs.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2502.17407/x6.png",
                "caption": "Figure 6:Inference FLOPs versus PRM performance and consistency.(Left) Second-degree polynomial regressions for average performance on 14 languages, comparing the 7B (blue) and 72B (green) reward models. (Right) Fleiss‚Äô kappa (top) and standard deviation (bottom) plotted against the same FLOPs budget; the fitted curves reveal no clear monotonic trend.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2502.17407/x7.png",
                "caption": "Figure 7:Comparison of PRM vs. ORM performance on MATH (solid lines) and AIME (dashed lines).1.5B models are shown with plus markers, 7B models with stars. Blue lines represent PRM, green lines represent ORM. White box annotations indicate the performance difference (ORM ‚àí PRM) at the highest compute setting for each line.",
                "position": 538
            }
        ]
    },
    {
        "header": "5Result 2: Budget Forcing",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17407/x8.png",
                "caption": "Figure 8:Performance of Qwen2.5-Math-1.5B +SFT and + MT-SFT at each training checkpoint.Average score and error bars for each checkpoint are displayed. The shaded region is the mean¬±plus-or-minus\\pm¬±standard deviation for MT-SFT.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2502.17407/x9.png",
                "caption": "Figure 9:Performance of MR1 on MT-AIME2024 atB‚Å¢F={2048,4096,8192}ùêµùêπ204840968192BF=\\{2048,4096,8192\\}italic_B italic_F = { 2048 , 4096 , 8192 }.Grey dots represent individual languages. Solid lines indicate average performance, while dashed lines highlight reference performances for selected languages.",
                "position": 667
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional details on MCLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17407/x10.png",
                "caption": "Figure 10:Heatmap representation of IMO problems from 2006 to 2024. Each row corresponds to a competition year, and each column represents a problem (Q1‚ÄìQ6). Green cells indicate questions that have been included in the M-IMO subset, while gray cells represent problems that were not selected.",
                "position": 1940
            }
        ]
    },
    {
        "header": "Appendix BAdditional details in training LLMs with system 2 thinking",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17407/x11.png",
                "caption": "Figure 11:Solve rates (%) of different multilingual math datasets evaluated.For the OLMo2 series, we use the base models, while for the Qwen2.5 series, the instruct-tuned variants are used.Euler-Instructpresents a significantly lower solve rate, indicating its greater difficulty.",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2502.17407/x12.png",
                "caption": "Figure 12:Model Results from Table9.Left shows accuracy on MT-MATH500 (entire translated subset for language group (B)), and right shows average performance of MT-AIME2024.",
                "position": 2145
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]