[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07260/x1.png",
                "caption": "Figure 1:Illustration of a UMoE layer, which incorporates MoE into both FFN and attention modules with shared experts. The primary distinction between attention-MoE and FFN-MoE lies in an additional token mixing operation.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x1.png",
                "caption": "Figure 1:Illustration of a UMoE layer, which incorporates MoE into both FFN and attention modules with shared experts. The primary distinction between attention-MoE and FFN-MoE lies in an additional token mixing operation.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x2.png",
                "caption": "Figure 2:Two formulations of the multi-head attention mechanism. (a) Vanilla attention interleaves mixing operations with value and output projections. (b) Pre-mixing attention performs token mixing prior to projections.",
                "position": 145
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07260/x3.png",
                "caption": "Figure 4:Post-Mixing Attention.",
                "position": 480
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07260/x4.png",
                "caption": "Figure 5:Best valid PPL (top) and training loss (bottom) on Wikitext.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x5.png",
                "caption": "",
                "position": 1030
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07260/x6.png",
                "caption": "Figure 7:Loss curves of UMoE with attention MoE layers implemented on post-mixing and pre-mixing attention, respectively. Models are trained on Wikitext-103 (left) and FineWeb-Edu (right).",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x7.png",
                "caption": "",
                "position": 1915
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x8.png",
                "caption": "Figure 8:Two implementations of UMoE based on pre-mixing and post-mixing attention, respectively.",
                "position": 1920
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x9.png",
                "caption": "Figure 9:Representative attention maps. The heatmaps show the attention weights of the last token produced by top 8 experts, ranked by their router scores. (a) Attention patterns for the Shakespeare question, where higher-ranked experts (e.g., Expert_0, Expert_32) demonstrate focused attention on question-relevant tokens. (b) Attention patterns for the Tokyo question, showing similar task-specific attention concentration among top experts.",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x9.png",
                "caption": "",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x10.png",
                "caption": "",
                "position": 2223
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x11.png",
                "caption": "Figure 10:Layer-wise accumulated attention weights across the model.\nThe values on the left (e.g., E0: 3.83) represent the sum of router scores for experts. Higher-ranked experts (E0, E1) consistently show more focused attention distributions compared to lower-ranked experts.",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x11.png",
                "caption": "",
                "position": 2232
            },
            {
                "img": "https://arxiv.org/html/2505.07260/x12.png",
                "caption": "",
                "position": 2237
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    }
]