[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02722/x1.png",
                "caption": "Figure 1:Example of a VLWM action-state trajectory given a video observation and a goal. VLWM can either generate a plan using one roll-out (system-1), or search over multiple actions by inferring the new world states and minimizing a given cost function (system-2).",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2509.02722/figures/vlwm_framework.png",
                "caption": "Figure 2:Overview of VLWM.(a)VLWM is a JEPA-style world model that predict abstract representation of future world states, instead of generating noisy and high-volume raw observations.(b)Given video contexts, VLWM’s prediction target is a structured textual representation of the unobserved future. It includes goal and interleaved action (AA) world state changes (Δ​S\\Delta S), all extracted automatically.(c)VLWM can infer possible goals from the context, and interpret them with current initial state and the expected final state. It supports both fast reactive system-1 plan generation and reflective system-2 reasoning based on cost minimization.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02722/figures/system-2.png",
                "caption": "Figure 3:System-2 planning of VLWM.(a): the critic is trained in a self-supervised manner, assigning lower cost to valid progress, while assigning higher cost for adding irrelevant distractors or shuffling the steps.(b): VLWM generates candidate action sequences and simulates their future state transitions. A critic evaluates the resulting state trajectories given the goal, and the planner selects the lowest-cost plan.",
                "position": 326
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02722/figures/planner_arena.png",
                "caption": "Figure 4:Illustration ofPlannerArenaannotation interface.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2509.02722/figures/planner_arena.png",
                "caption": "Figure 4:Illustration ofPlannerArenaannotation interface.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2509.02722/figures/cost_curves.png",
                "caption": "Figure 5:Cost curves estimated by different critic models. Each plot visualizes 3k cost curves on goal achievement detection trajectories, where each trajectory is composed of a reference gold plan (0%-100%) and distractor steps (100%-200%). Red dots (⋅\\boldsymbol{\\cdot}) mark cost-minimizing steps (detected goal achievement points). VLWM-Critic accurately detects goal completion around 100% plan length, while baselines show suboptimal or noisy behavior.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2509.02722/figures/critic-worldprediction.png",
                "caption": "Figure 6:WorldPrediction-PPresults. Our VLWM-critic-1B established a new SoTA of 45.4% accuracy.",
                "position": 1109
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6PlannerArena Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02722/figures/planner_arena_eval.png",
                "caption": "Figure 7:PlannerArena interface. The sample shown here is from COIN, Plan A from the ground truth annotations and Plan B from Llama 4.",
                "position": 2354
            }
        ]
    },
    {
        "header": "7Prompts",
        "images": []
    },
    {
        "header": "8Tree-of-Captions Example",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02722/figures/tree-of-caption-video.png",
                "caption": "",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2509.02722/figures/tree_example.png",
                "caption": "Figure 8:Structure ofTree of captions(bottom) extracted from video (top). Each box is associated with a corresponding video caption.",
                "position": 2624
            },
            {
                "img": "https://arxiv.org/html/2509.02722/figures/appendix_context_frames_tomato_eggs.png",
                "caption": "",
                "position": 2722
            }
        ]
    },
    {
        "header": "9VLWM Planning Examples",
        "images": []
    }
]