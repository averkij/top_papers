[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06411/x1.png",
                "caption": "(a)Reasoning with Discrete-Token CoT",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x1.png",
                "caption": "(a)Reasoning with Discrete-Token CoT",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x2.png",
                "caption": "(b)Reasoning with the Soft-Thinking Pattern",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x3.png",
                "caption": "(c)GRPO(shao2024deepseekmath)for Discrete-Token CoT",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x4.png",
                "caption": "(d)Existing Work of Soft-Thinking + GRPO(butt2025soft)",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x5.png",
                "caption": "(e)SofT-GRPO (Ours) for Reinforcing Soft-Thinking",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06411/x6.png",
                "caption": "Figure 2:The pipeline of the proposed SofT-GRPO algorithm. In training with a Queryùë∏\\boldsymbol{Q}, the SofT-GRPO first generates a group ofGGsoft-thinking reasoning paths with Gumbel noises and the Gumbel-Softmax technique(jang2016categorical). We transmit the valuegi‚Ä≤g^{\\prime}_{i}andyi‚Ä≤y^{\\prime}_{i}for the loss calculation afterward. Then, we reconstruct the soft-thinking input. Finally, we update the soft-thinking policy with the off-policy REINFORCE(williams1992simple)algorithm, optimizing the soft-thinking reasoning tokens with Gumbel reparameterization.",
                "position": 314
            }
        ]
    },
    {
        "header": "3SofT-GRPO: Reinforcing Soft-Thinking Policy with Gumbel Reparameterization",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06411/x7.png",
                "caption": "(a)Training reward curve of variants on added noises",
                "position": 1443
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x7.png",
                "caption": "(a)Training reward curve of variants on added noises",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x8.png",
                "caption": "(b)Validation reward curve of different hyper-parameters",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x9.png",
                "caption": "(c)Training reward curve of different hyper-parameters",
                "position": 1456
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BPrompt of Language Reasoning and Latent Generation",
        "images": []
    },
    {
        "header": "Appendix CMotivation and Theoretical Proof",
        "images": []
    },
    {
        "header": "Appendix DSupplementary of Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.06411/x10.png",
                "caption": "Figure 4:Token consumption curve onLLaMA-3.2-3B-Instruct Base LLMduring training.",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x11.png",
                "caption": "Figure 5:Running discrete-token CoT methods (GRPO and No-finetune) with more temperature options onDeepSeek-R1-Distill-Qwen-1.5B Base LLM. Pass@k represents the pass rate within at most k runs, and Pass@1 is additionally averaged from 32 runs. Experiments are run on the five datasets in Table1for the average.",
                "position": 2095
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x12.png",
                "caption": "(a)Training KL Divergence curve betweenœÄŒ∏ref\\pi_{\\theta_{\\text{ref}}}andœÄŒ∏\\pi_{\\theta}",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x12.png",
                "caption": "(a)Training KL Divergence curve betweenœÄŒ∏ref\\pi_{\\theta_{\\text{ref}}}andœÄŒ∏\\pi_{\\theta}",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2511.06411/x13.png",
                "caption": "(b)Training Proximal Policy Optimization (PPO)(schulman2017proximal)KL Divergence curve betweenœÄŒ∏old\\pi_{\\theta_{\\text{old}}}andœÄŒ∏\\pi_{\\theta}",
                "position": 2111
            }
        ]
    },
    {
        "header": "Appendix EBaselines & Datasets & Licenses",
        "images": []
    }
]