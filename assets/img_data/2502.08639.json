[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.08639/x1.png",
                "caption": "",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.08639/x2.png",
                "caption": "Figure 2:Overview of CineMaster. CineMaster consists of two stages. First, we present an interactive workflow that allows users to intuitively manipulate the objects and camera in a 3D-native manner. Then the control signals are rendered from the 3D engine and fed into a text-to-video diffusion model, guiding the generation of user-intended video content.",
                "position": 89
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.08639/x3.png",
                "caption": "Figure 3:Overview of the network architecture. We design a Semantic Layout ControlNet which consists of a semantic injector and a DiT-based ControlNet. Semantic injector fuses the 3D spatial layout and class label conditions. The DiT-based ControlNet further represents the fused features and adds to the hidden states of the base model. Meanwhile, we inject the camera trajectories by the camera adapter to achieve joint control over object motion and camera motion.",
                "position": 99
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.08639/x4.png",
                "caption": "Figure 4:Dataset Labeling Pipeline. We propose a data labeling pipeline to extract 3D bounding boxes, class labels and camera poses from videos. Our pipeline consists of four steps: 1) Instance Segmentation: Obtain instance segmentation results from the foreground in videos. 2) Depth Estimation: Produce metric depth maps using DepthAnything V2. 3) 3D Point Cloud and Box Calculation: Identify the frame with the largest mask for each entity and compute the 3D point cloud of each entity through inverse projection. Then, use the minimum volume method to calculate the 3D bounding box for each entity. 4) Entity Tracking and 3D Box Adjustment: Access the point tracking results of each entity and calculate the 3D bounding boxes for each frame. Finally, project the entire 3D scene into depth maps.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2502.08639/x5.png",
                "caption": "Figure 5:We present three different feature comparisons: moving object&\\&&static camera, static object&\\&&moving camera and moving object&\\&&moving camera. We transform our 3D box condition to object trajectories for MotionCtrl[40]and 2D bounding box sequences for Direct-A-Video[45]to align the input conditions. In comparison, CineMaster could better control object motion and camera motion separately or jointly to generate diverse user-intended scenes.",
                "position": 196
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Limitations and Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]