[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07860/extracted/6268239/figs/pull-fig-5.png",
                "caption": "Figure 1:The Video Action Differencing task and benchmark (VidDiffBench). Given a pair of videos and an action, the task is to generate a list of differences as natural language descriptions.\nOur VidDiffBench consists of annotated differences across diverse domains, where the differences are relevant to human skill learning.\nThe first row emphasizes the first key challenge:localization of sub-actionsbetween segments of the video for comparison. The second row highlights the second key challenge:fine-grained image understandingof actions in order to perform comparison.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video Action Differencing",
        "images": []
    },
    {
        "header": "4Benchmark Dataset and Annotations",
        "images": []
    },
    {
        "header": "5VidDiff Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07860/x1.png",
                "caption": "Figure 2:VidDiff Method. One input is an action description (e.g. “weighted squat”). The Difference Proposer generates potential differences using a large language model (LLM). The Frame Localizer assigns frames where these differences are observable. Finally, the Action Differencer checks each difference using a vision-language model, determining whether it applies more to video A or video B, or neither.",
                "position": 481
            }
        ]
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07860/x2.png",
                "caption": "Figure 3:Examples of ‘success cases’ (left) – differences where GPT-4o has high accuracy – and failure cases (right). Success cases typically involve coarse differences, easy localization, or simple actions, while failure cases often involve fine differences, precise localization or complex actions.",
                "position": 724
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Future Work and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark: Download Instructions",
        "images": []
    },
    {
        "header": "Appendix BBenchmark: Difference Annotation Taxonomy Generation",
        "images": []
    },
    {
        "header": "Appendix CBenchmark: annotation details",
        "images": []
    },
    {
        "header": "Appendix DBenchmark statistics",
        "images": []
    },
    {
        "header": "Appendix EBenchmark: related video pair datasets",
        "images": []
    },
    {
        "header": "Appendix FEvaluation",
        "images": []
    },
    {
        "header": "Appendix GResults: more analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07860/x3.png",
                "caption": "Figure 4:Sample frame localizations: prediction vs ground truth.",
                "position": 8678
            }
        ]
    },
    {
        "header": "Appendix HVidDiff method",
        "images": []
    },
    {
        "header": "Appendix IComputational Costs for VidDiff Method",
        "images": []
    }
]