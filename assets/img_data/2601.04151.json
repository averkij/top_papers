[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Klear",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04151/x1.png",
                "caption": "Figure 1:Overview ofKlear. The model takes four inputs: video, video-related text, audio-related text, and audio. Each input is individually encoded by respective encoders, then fed into the MM-DiT. The MM-DiT module outputs the latent variables of video and audio, which are then decoded separately into video and audio.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04151/x2.png",
                "caption": "Figure 2:Overview of our Dataset Annotation Pipeline.",
                "position": 312
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04151/x3.png",
                "caption": "Figure 3:Qualitative evaluation of audio-video joint generation across various aspects.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2601.04151/fig/abla_stage.png",
                "caption": "Figure 4:Ablations of different training stages.Metrics include video, audio, TTS, and audio-video consistency, with arrows indicating optimization directions.",
                "position": 680
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]