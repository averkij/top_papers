[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/head_figure2.jpg",
                "caption": "Figure 1:PEAP framework: we investigate the possibility of perceive everything as pixels. This framework aligns better with human perception reducing the need for excessive pre-processing. Evaluated on our benchmarkPixelWorld,PEAPboosts performance on multimodal tasks (e.g., websites, slides, documents) but struggles with complex, text-centric tasks (e.g., reasoning and coding). Larger models achieve better transferability between pixel- and token-based performance compared to smaller ones. We also observed that text and images exhibit similar attention patterns, and reduced the overhead of model reasoning through patch pruning byPEAP-Fast.",
                "position": 88
            }
        ]
    },
    {
        "header": "2Datasets",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plt_decompose.png",
                "caption": "Figure 2:The performance oftext-onlydatasets. The comparison is made between text input and synthesized image input. Most models demonstrate comparable performance on language understanding datasets such as SuperGLUE, GLUE, and ARC. However, notable performance disparities emerge between text-based input and synthesized image input on mathematical reasoning tasks (e.g., MMLU-Pro, GSM8K) and programming tasks (e.g., MBPP). Phi-3.5-Vision exhibits consistently poor performance across all vision tasks, primarily due to its insufficient instruction-following capabilities.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plt_decompose_sc32.png",
                "caption": "Figure 3:The performance of thestructureddataset. We report all the subsets for the TableBench. In thesemisetting, questions were presented as text, while tables were rendered as synthetic images. We observed that for tasks involving reasoning (numerical reasoning) and coding (visualization subset), synthetic images yielded inferior performance compared to text. However, for tasks emphasizing semantic understanding, such as data analysis and fact checking, synthetic images achieved performance comparable to or even surpassing text. Additionally, we found that the semi approach often performed worse than either text or synthetic images individually, providing insights into potential limitations and future directions for leveraging vision-language models (VLMs).",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plot_MMMUPro_charts.png",
                "caption": "Figure 4:The performance of themultimodaldataset (MMMU-Pro). We adopt the result reported by the origin paper. We can observe that strong models perform better inPEAP.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/plt_decompose_sc33.png",
                "caption": "Figure 5:The performance of themultimodaldatasets (except MMMU-Pro). We compare text-only and vision-only subsets in Mathverse, while SlidesVQA and WikiSS-QA are evaluated as VQA tasks. Larger models perform better on text-based tasks with more modalities. GPT-4o tends to generate longer responses in long-context QA, leading to performance degradation on WikiSS-QA.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/compare_textImg.png",
                "caption": "Figure 6:Last Layer Attention Heatmap on QWen2VL-7B between token-based (left) and pixel-based (right) inference.",
                "position": 339
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExample Input",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/example_input.png",
                "caption": "Figure 7:An example input of GSM8K dataset, using Direct Prompt.",
                "position": 1287
            },
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/106_full.png",
                "caption": "Figure 8:An example input of TableBench dataset, using Direct Prompt.",
                "position": 1290
            },
            {
                "img": "https://arxiv.org/html/2501.19339/extracted/6169768/figures/heatmap_sparse.jpg",
                "caption": "Figure 9:Last Layer Attention Heatmap on Qwen2VL-7B betweenPEAP(left) andPEAP-Fast (right).",
                "position": 1303
            }
        ]
    },
    {
        "header": "Appendix BAttention Heatmap before and after ImageFast Method",
        "images": []
    }
]