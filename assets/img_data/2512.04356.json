[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04356/figures/images/teaser.png",
                "caption": "Figure 1:Compared to existing MLLMs suffering from object and action hallucinations, ourSANTAenhances faithfulness in describing both visual objects and temporal actions.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04356/figures/images/overview.png",
                "caption": "Figure 2:Overview ofSANTA.We employ (a) Mitigating Video-Level Hallucination by applying Hallucinative Self-Augmentation to identify the highly potential hallucinated tokens in MLLMÎ¸M\\theta_{M}that deviate from ground truth words (e.g., synonyms or hypernyms) and then perform video-caption contrastive alignment.SANTAthen (b) Mitigating Object nad Action-Level Hallucinations by Tracklet-Phrase Contrastive Alignment to align object and action tracklets with visual and temporal phrases while contrasting hallucinative negatives.",
                "position": 173
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04356/x1.png",
                "caption": "Figure 3:t-SNE visualization of the latent features of (a) video and caption, (b) object tracklets and phrases, and (c) action tracklets and phrases. For the w/oSANTAsetting, we directly visualize features from LLaVA-Video[llavavideo]. Upon training withSANTALLaVA-Video[llavavideo]improves the alignment between visual-language modalities while exempting from the hallucinative captions.",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2512.04356/x2.png",
                "caption": "Figure 4:Qualitative comparison of video captions predicted by HACL[hacl]andSANTA. Note that words highlighted ingreenindicate action faithfulness, while those inredindicate action hallucination. Similarly, words inbluerepresent object faithfulness, whereas those inorangedenote object hallucination. The examples of (a) and (b) are sampled from MiraData-9k[mirabench]and FactVC[factvc], respectively.",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2512.04356/x3.png",
                "caption": "Figure 5:Ablation study of hallucinative self-augmentation scheme. We ablate this scheme by replacing it with negatives generated directly by a text-only LLM (i.e., GPT-4[gpt4]), following the prompting setup of HACL[hacl]. We reportF1Obj\\text{F1}_{\\text{Obj}}andF1Act\\text{F1}_{\\text{Act}}of HalFscore[perturbollava]to evaluate the effectiveness of mitigating object and action hallucinations, respectively.",
                "position": 1201
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "7Additional Experiment Setups",
        "images": []
    },
    {
        "header": "8Additional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04356/x4.png",
                "caption": "Figure 6:Qualitative results on hallucinative and object/action tracklets from MiraData-9k[mirabench].",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2512.04356/x5.png",
                "caption": "Figure 7:Qualitative comparison of video captions predicted by HACL[hacl]andSANTAon MiraData-9k[mirabench]. Note that words highlighted ingreenindicate action faithfulness, while those inredindicate action hallucination. Similarly, words inbluerepresent object faithfulness, whereas those inorangedenote object hallucination.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2512.04356/x6.png",
                "caption": "Figure 8:The prompting template used to prompt GPT-4o[gpt4o]with few-shot demonstrations to perform the parsing task.",
                "position": 1530
            }
        ]
    },
    {
        "header": "9Additional Qualitative Results",
        "images": []
    }
]