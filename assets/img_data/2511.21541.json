[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21541/x1.png",
                "caption": "",
                "position": 161
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21541/x2.png",
                "caption": "Figure 2:Comparison between RGB ReFL and our PRFL. RGB reward models require near-complete denoising and VAE decoding to RGB space, introducingevaluation delay,GPU memory bottleneck, andinsufficient supervisionof early denoising stages where structure and motion are formed. PRFL eliminates these limitations by performing reward modeling directly in latent space with timestep-aware training.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2511.21541/x3.png",
                "caption": "Figure 3:Analysis on VGM Features. (a) VLM-based reward model (VideoAlign-MQ) exhibits poor timestep generalization with fluctuating scores. (b) VGM features from any DiT layer uniformly achieve 78.8% accuracy, matching VLM baseline. (c) Timestep-aware fine-tuning unlocks VGMâ€™s full potential, achieving 85.46% accuracy with peak performance at early timesteps (t=0.8).",
                "position": 180
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21541/x4.png",
                "caption": "Figure 4:Overview of our process-aware video generation alignment framework. Left: Architecture of the Video Generation Model (VGM) and Process-Aware Video Reward Model (PAVRM). Right: Two-stage training pipeline. PAVRM training with reward prediction from noisy latents. Process Reward Feedback Learning (PRFL) optimizing VGM through latent-space reinforcement learning at randomly sampled timesteps.",
                "position": 259
            }
        ]
    },
    {
        "header": "3Preliminaries and Feasibility Analysis",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21541/x5.png",
                "caption": "Figure 5:Human evaluation of PRFL\nmodel vs. other post-training methods.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2511.21541/x6.png",
                "caption": "Figure 6:Qualitative results for different post-training methods on 480P text-to-video task. Thered boxhighlights the generated artifacts.\nZoom in for a better view.\nFor the complete prompt, please see the supplementary materials.",
                "position": 839
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix A.1Overview.",
        "images": []
    },
    {
        "header": "Appendix A.2More Details in Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21541/Images/7a695a26067c32f29682e0e22b1538e5_origin.jpg",
                "caption": "Figure 7:A case of user study page.",
                "position": 1662
            }
        ]
    },
    {
        "header": "Appendix A.3More Experiments on Process-Aware Video Reward Models",
        "images": []
    }
]