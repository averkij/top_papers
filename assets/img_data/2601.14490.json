[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14490/media/guten-mascot.png",
                "caption": "",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/hf-logo.png",
                "caption": "",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2601.14490/x1.png",
                "caption": "Figure 1:Capability profile of GutenOCR-3B/7B, their Qwen2.5-VL backbones, and Qwen2.5-based OCR baselines on nine grounded OCR tasks spanning our in-domain suite and Fox. Each spoke shows a raw score in[0,1][0,1](higher is better): in-domain page-, line-, and local-reading tasks are scored as1−CER1-\\mathrm{CER}, full and conditional detection as F1, and Fox page-, region-, line-, and color-guided OCR as1−CER1-\\mathrm{CER}. GutenOCR substantially improves detection, fine-grained reading, and Fox region/line OCR, while trading off some page-level and color-guided performance relative to OCR-specialized baselines.",
                "position": 334
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2GutenOCR Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14490/media/zmk3232.PMC2938388_5.png",
                "caption": "Figure 2:Unified GutenOCR interface. A single vision–language model consumes a page image,\noptionally conditioned on a text queryqqor bounding boxbb, and serves multiple OCR-style\ntasks (reading, detection, conditional detection, localized reading) through prompt-specified\ninput and output schemas.\nSub-panels depict each task family:\nfull page detection (top-left),\nfull page reading (top-right),\nlocal reading (bottom-right),\nand conditional detection (bottom-left).\nSample page from[68].",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/zmk3232.PMC2938388_5-detect.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/zmk3232.PMC2938388_5-reading.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/zmk3232.PMC2938388_5-conditional_detect.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/zmk3232.PMC2938388_5-local.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/38_2010_Article_149.PMC2941081_3-top.png",
                "caption": "Figure 3:Example of the layout-sensitivetext2drepresentation.\nLong runs of spaces encode horizontal alignment (e.g., right-justified page number),\nwhile blank lines encode vertical gaps between sections and between the\nmain text and the flow diagram.\nPage from[11].",
                "position": 778
            }
        ]
    },
    {
        "header": "3Data & Training",
        "images": []
    },
    {
        "header": "4Evaluation Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14490/media/en_19_box_annotated.png",
                "caption": "Figure 4:Qualitative comparison of Qwen2.5-VL-3B and GutenOCR-3B showing complementary strengths and weaknesses.(a)GutenOCR excels at region extraction (CER: 0.004 vs. 0.987).(b)Region training transfers to line pointer tasks (CER: 0.000 vs. 0.988).(c)GutenOCR suffers catastrophic forgetting on color tasks (CER: 0.997 vs. 0.000).(d)Trade-off: GutenOCR achieves higher content F1 (0.983 vs. 0.597) despite worse reading order (CER: 0.687 vs. 0.574) relative to Fox’s reference expectations.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/en_92_line_annotated.png",
                "caption": "",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/en_69_color.png",
                "caption": "",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/en_15_page.png",
                "caption": "",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/omnidocbench-recall.jpg",
                "caption": "Figure 5:Qualitative OmniDocBench text-detection example.\nRed boxes denote annotated text spans; blue boxes denote GutenOCR predictions\non the same page.\nMany blue boxes fall on readable text (e.g., table cells and decorative\nlabels) that OmniDocBench does not label as text, so non-overlapping\npredictions cannot be reliably treated as false positives.\nThis motivates our recall-only evaluation protocol in\nSection5.4.2.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2601.14490/x2.png",
                "caption": "Figure 6:Training-stage ablation summary for composite score (left, higher is better)\nand localized-reading error (right, lower is better) on OCR-IDL, TabMe++,\nand PubMed-OCR.\nThe x-axis shows the base model and Stages 1–3; Stage 3a and Stage 3b\nshare the same x-position and are distinguished by line style and marker.",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2601.14490/x3.png",
                "caption": "",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/idl_kyll0135_0028_clean.png",
                "caption": "Figure 7:Qualitative examples of GutenOCR-3B’slinespredictions on test samples. Blue boxes indicate GutenOCR’s prediction versus the ground truth in red. Regions of overlap are purple. Areas of disagreement are immediately obvious (and actionable).",
                "position": 2525
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/tabmepp_ffhh0198_0002_clean.png",
                "caption": "",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/pubmed_29.PMC2107773_5_clean.png",
                "caption": "",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2601.14490/media/pubmed_29.PMC2107773_5_clean-zoom.png",
                "caption": "",
                "position": 2547
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInterface contract",
        "images": []
    },
    {
        "header": "Appendix BData",
        "images": []
    },
    {
        "header": "Appendix CTraining",
        "images": []
    },
    {
        "header": "Appendix DEvaluation",
        "images": []
    },
    {
        "header": "Appendix EAdditional Benchmark Details",
        "images": []
    },
    {
        "header": "Appendix FModels",
        "images": []
    },
    {
        "header": "Appendix GAdditional Results",
        "images": []
    }
]