[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09040/x1.png",
                "caption": "Figure 1:(Left)A simple illustration that reflects the information loss faced by language-centric approaches.(Right)Our proposedAutoregressive Semantic Visual Reconstruction (ASVR)brings significant improvements across various aspects, including General VQA, Visual-centric, Hallucination, and OCR. All the scores are normalized byxnorm=(x‚àíxmin+10)/(xmax‚àíxmin+10)subscriptùë•normùë•subscriptùë•min10subscriptùë•maxsubscriptùë•min10x_{\\text{norm}}=(x-x_{\\text{min}}+10)/(x_{\\text{max}}-x_{\\text{min}}+10)italic_x start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT = ( italic_x - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ) / ( italic_x start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ).",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2506.09040/x2.png",
                "caption": "",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09040/x3.png",
                "caption": "Figure 2:Left: the typical LVLM framework exemplified by LLaVA[28].Right: overview ofASVR‚Äôsmodel architecture and training procedure. The input image and its corresponding text are tokenized into sequences of discrete token indices for unified autoregressive supervision over both visual and textual outputs. For each module, the icon before the slash indicates whether it is frozen or tunable during pre-training, while the icon after the slash indicates its configuration during instruction tuning. \"s\" and \"e\" denote the start and end of the text tokens, respectively.",
                "position": 246
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09040/x4.png",
                "caption": "Figure 3:Qualitative comparison on attention maps, where we keep the same LLM and training data. With extra vision-centric supervision signals, ROSS urges the model to focus on specific image contents corresponding to the question with higher attention values.",
                "position": 1031
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]