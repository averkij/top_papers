[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Overview.png",
                "caption": "Figure 1:An illustration of our proposed methodology where the server maintains a pair of global LoRA modules while the devices adaptively update submatrices of the global LoRA modules through sketching during each round.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Problem Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Analysis",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Results_RoBERTa.png",
                "caption": "Figure 2:Convergence behavior of FSLoRA and baselines on the GLUE benchmark with the RoBERTa model. Testing accuracy is averaged over seven tasks.",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Results_RoBERTa_detail.png",
                "caption": "Figure 3:Comparison between FSLoRA with and without sketching, where the upload budget for devices is set to100×100\\times100 ×the full global LoRA modules at the corresponding rank. The experiment is performed on the GLUE benchmark and the RoBERTa model. FSLoRA with sketching obtains a better performance, validating the effectiveness of sketching.",
                "position": 938
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BDetails of Datasets",
        "images": []
    },
    {
        "header": "Appendix CFurther Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Results_LLaMA_detail.png",
                "caption": "Figure 6:Comparison of FSLoRA with and without sketching, with an upload budget400×400\\times400 ×the global LoRA module size at each rank. This is based on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. We observe that the sketching mechanism improves performance across all considered tasks. The average accuracy of the eight tasks is shown in FigureLABEL:fig:rank_varying.",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Impact_global_rank_detail.png",
                "caption": "Figure 7:Impact of the rank of global LoRA modules on FSLoRA, given a fixed rank for the updated submatrices at the devices. This is based on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. Overall, FSLoRA demonstrates improved performance as the global rank increases. The average accuracy of the eight tasks is shown in FigureLABEL:fig:global_rank.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/Impact_topk.png",
                "caption": "Figure 8:Comparison of top-k compression and its integration with sketching, evaluated on the commonsense reasoning benchmark using the LLaMA-3.2-3B model. The results show that combining these two orthogonal techniques significantly enhances performance, demonstrating the benefits of integrating sketching with top-k compression.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/More_Device_50_detail.png",
                "caption": "Figure 9:Comparison of FSLoRA with and without sketching, with an upload budget400×400\\times400 ×the global LoRA module size at each rank, evaluated on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. The number of devices is set to50505050.\nWe observe that the sketching mechanism improves performance across all considered tasks. The average accuracy of the eight tasks is shown in Figure10.",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2501.19389/extracted/6169909/Figure/More_Device_50.png",
                "caption": "Figure 10:Comparison of FSLoRA with and without sketching, with an upload budget400×400\\times400 ×the global LoRA module size at each rank, evaluated on the LLaMA-3.2-3B model. The number of devices is set to50505050. The results are averaged over eight tasks from the commonsense reasoning benchmark.",
                "position": 1965
            }
        ]
    },
    {
        "header": "Appendix DProof of the Theoretical Results",
        "images": []
    }
]