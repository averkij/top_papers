[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19326/x1.png",
                "caption": "Figure 1:TPO uses differentiable task preferences from dense visual supervisions via task-specific heads to enhance MLLMs in fine-grained understanding.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x2.png",
                "caption": "Figure 2:Comparison of Learning Method.A solid line indicates data flow, and a dotted line represents feedback.anddenote modules that are frozen and unfrozen.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x3.png",
                "caption": "",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x5.png",
                "caption": "",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19326/x6.png",
                "caption": "Figure 3:Overall Pipeline of TPO.The architecture of Task Preference Optimization (TPO) consists of four main components: (1) a vision encoder, (2) a connector, (3) a large language model, and (4) a series of visual task heads. Differently colored flame symbols indicate which components are unfrozen at various stages of the training process.",
                "position": 211
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "1Detailed Results",
        "images": []
    },
    {
        "header": "2Training and Data Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.19326/x7.png",
                "caption": "Figure 4:Qualitative Results of Spatial Grounding.",
                "position": 4671
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x8.png",
                "caption": "Figure 5:Qualitative Results of Referring Segmentation.",
                "position": 4674
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x9.png",
                "caption": "Figure 6:Qualitative Results of Tracking.",
                "position": 4677
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x10.png",
                "caption": "Figure 7:Qualitative Results of Moment Retrieval and Highlight Detection.The orange curve represents the saliency score, the blue interval represents the time interval predicted by the model, and the green interval represents the ground truth.",
                "position": 4680
            },
            {
                "img": "https://arxiv.org/html/2412.19326/x11.png",
                "caption": "Figure 8:Qualitative Results of Multimodal Video Understanding.",
                "position": 4683
            }
        ]
    },
    {
        "header": "3Qualitative Results",
        "images": []
    }
]