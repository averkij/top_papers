[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02955/x1.png",
                "caption": "Figure 1:State-of-the-art video understanding models struggle with basic motion-level perception. Compared to existing benchmarks, our proposed MotionBench focuses on assessing the model‚Äôs Motion level perception capability, which is critical in understanding videos with fast and instant interactions and motions.",
                "position": 96
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MotionBench: Motion-Level Benchmarking",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02955/x2.png",
                "caption": "Figure 2:We propose MotionBench, a collection of manually curated multi-choice queries with video clips featuring dynamic changes from various scenes such as daily life and medical instructions. We devise six primary tasks to evaluate the capability of motion-level perception. Unlike previous story-level and event-level benchmarks, MotionBench is characterized by a significantly higher annotation density, allowing for the assessment of fine-grained motions.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2501.02955/x3.png",
                "caption": "(a)Option distribution",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2501.02955/x3.png",
                "caption": "(a)Option distribution",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2501.02955/x4.png",
                "caption": "(b)Video duration",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2501.02955/x5.png",
                "caption": "(c)Annotation length",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2501.02955/x6.png",
                "caption": "(d)QA per video",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/video_dynamic_annotation_ver2.jpg",
                "caption": "Figure 4:Example of dynamic information annotation",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2501.02955/x7.png",
                "caption": "Figure 5:Summarization of prevalent paradigms for video compression and our proposed Through-Encoder Fusion (TE Fusion). Here we only illustrate the part before the VLM decoder where temporal compression performs.",
                "position": 505
            }
        ]
    },
    {
        "header": "4Model Design: Motion-Level Perception",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02955/x8.png",
                "caption": "Figure 6:Model performance variation with respect to different compression ratiosk=2,4,8,16ùëò24816k=2,4,8,16italic_k = 2 , 4 , 8 , 16, given a fixed VLM input frame count ofNinput=16subscriptùëÅinput16N_{\\text{input}}=16italic_N start_POSTSUBSCRIPT input end_POSTSUBSCRIPT = 16. The pink dotted line represents the performance of the baseline model, which processes 16 frames without temporal compression. Note that each compression method is re-implemented on the GLM-4V-9B backbone to ensure a fair comparison.",
                "position": 883
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Training Details",
        "images": []
    },
    {
        "header": "8Model Details",
        "images": []
    },
    {
        "header": "9QA Construction Process for Videos with Intricate Interactions",
        "images": []
    },
    {
        "header": "10More Experimental Results",
        "images": []
    },
    {
        "header": "11Case Study on Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02955/x9.png",
                "caption": "Figure 7:The absolute number and the proportion of questions that all models answered incorrectly relative to the total number of questions in each task type.",
                "position": 2875
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/wrong.png",
                "caption": "",
                "position": 2891
            }
        ]
    },
    {
        "header": "12Limitations and Broader impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/1.jpg",
                "caption": "",
                "position": 2922
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/10.jpg",
                "caption": "",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/11.jpg",
                "caption": "",
                "position": 2948
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/12.jpg",
                "caption": "",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/9.jpg",
                "caption": "",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/8.jpg",
                "caption": "",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/7.jpg",
                "caption": "",
                "position": 3000
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/2.jpg",
                "caption": "",
                "position": 3013
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/6.jpg",
                "caption": "",
                "position": 3026
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/5.jpg",
                "caption": "",
                "position": 3039
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/3.jpg",
                "caption": "",
                "position": 3052
            },
            {
                "img": "https://arxiv.org/html/2501.02955/extracted/6113584/figs/4.jpg",
                "caption": "",
                "position": 3065
            }
        ]
    },
    {
        "header": "13More Dataset Samples",
        "images": []
    }
]