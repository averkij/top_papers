[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13104/x1.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13104/x2.png",
                "caption": "Figure 2:Action-to-video generation with visual action prompts.We project action-induced 3D structural dynamics of diverse agents into 2D visual action prompts, primarily 2D skeletons, establishing a unified control signal for action-conditioned video generation.\nWe design data creation pipelines for HOI and robot videos to robustly recover their 2D skeletons. The constructed visual action prompts are injected into a pretrained video generation model for fine-tuning and generating plausible interaction-driven visual dynamics.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13104/x3.png",
                "caption": "(a)HOI dataset creation pipeline.",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2508.13104/x3.png",
                "caption": "(a)HOI dataset creation pipeline.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2508.13104/x4.png",
                "caption": "(b)Robot dataset creation pipeline.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13104/x5.png",
                "caption": "Figure 4:Comparison of different action control signals.Text as action (TI2V[72]) leads to ambiguity.\nRaw 7-DoF states/actions (IRASim[81]) leads to inaccurate control.\nVisual action prompts (Ours) facilitate dynamic learning under precise control.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2508.13104/x6.png",
                "caption": "Figure 5:Effect of joint training with visual action prompts.Compared to single-dataset training, joint training leads to better object consistency on DROID[35]and enables held-out skill execution (e.g., “close the drawer”) on RT-1[11].",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2508.13104/x7.png",
                "caption": "Figure 6:Action-to-video generation of a unified model.Visual action prompts enable joint training on diverse datasets and facilitate interaction-driven dynamic generation.\nThe overlaid skeletons are only for visualization, demonstrating accurate action control.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2508.13104/x8.png",
                "caption": "Figure 7:Diverse generation results under different action trajectories.Our model can simulate diverse actions and their visual outcomes from the same initial frame.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2508.13104/x9.png",
                "caption": "Figure 8:Comparison of different forms of visual action prompts.All three forms of visual action prompts can precisely represent delicate actions and drive plausible interactions.\nWe prefer the skeleton-based prompt for its acquisition efficiency.",
                "position": 582
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]