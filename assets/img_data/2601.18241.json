[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18241/assets/main_fig.png",
                "caption": "Figure 1:Overview of TAM-Eval, a framework and benchmark for evaluating LLMs on unit test maintenance. The pipeline filters GitHub repositories, constructs tasks for test creation, repair, and update with execution-based validation, performs LLM-driven test generation with iterative feedback, and computes Pass Rate,ﾎ能\DeltaTest Coverage, andﾎ能\DeltaMutation Score.",
                "position": 103
            }
        ]
    },
    {
        "header": "IIBackground",
        "images": []
    },
    {
        "header": "IIIBenchmark Construction",
        "images": []
    },
    {
        "header": "IVEvaluation Framework",
        "images": []
    },
    {
        "header": "VExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18241/assets/attempts_plot.png",
                "caption": "Figure 2:Dynamics across three metrics: Pass Rate, meanﾎ能\DeltaTest Coverage, and meanﾎ能\DeltaMutation Coverage by attempts",
                "position": 643
            }
        ]
    },
    {
        "header": "VIResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18241/assets/fail_reason_distribution.png",
                "caption": "Figure 3:Distribution of generated test suites fail reason (attempt@3).",
                "position": 1037
            }
        ]
    },
    {
        "header": "VIIConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]