[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07584/x1.png",
                "caption": "Figure 1:LongCat-Image exhibits strong performance in general text-to-image generation, text rendering and image editing.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x2.png",
                "caption": "Figure 2:High-fidelity text-to-image generation results.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x3.png",
                "caption": "Figure 3:Showcase of versatile capabilities in general image editing.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x4.png",
                "caption": "Figure 4:Showcase on complex and comprehensive editing scenarios.Beyond basic edits, LongCat-Image-Edit exhibits robust handling of intricate modifications and composite instructions.",
                "position": 281
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07584/x5.png",
                "caption": "Figure 5:Overview of training data.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x6.png",
                "caption": "Figure 6:Data curation pipeline.The pipeline consists of four stages: (1)Filtering: Raw data undergoes deduplication and quality assessment, including watermark and AIGC detection. (2)Meta Information Extraction: We extract comprehensive metadata, such as aesthetic scores, named entities, and OCR text. (3)Multi-Granularity Captioning: Leveraging the extracted metadata and prompt templates, a VLM generates captions ranging from entity-level tags to detailed photographic descriptions. (4)Stratification: The dataset is stratified into a pyramid structure based on style, quality, and content to support progressive training stages.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x7.png",
                "caption": "Figure 7:Comprehensive aesthetic scoring. The aesthetic scoring takes image quality and image artistry into account. In this context, quality denotes the signal-related attributes of the image, such as resolution, clarity, and noise levels, while artistry pertains to the perceptual appeal or visual attractiveness of the image as judged by humans or models.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x8.png",
                "caption": "Figure 8:Prompts for different level captions.Entity Level and Phrase Level captions are generated concurrently using a single model, whereas composition-level captions are subsequently produced in a sequential stage utilizing a separate model.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x9.png",
                "caption": "Figure 9:Quality comparison of our Photographic Level Captioner. This captioner produces more concise and information-dense captions compared to baseline. Different color blocks indicate different aspects of the captions.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x10.png",
                "caption": "Figure 10:Examples of Multi-Granularity Captioning.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x11.png",
                "caption": "Figure 11:Process for synthesizing text rendering data.",
                "position": 502
            }
        ]
    },
    {
        "header": "3Model Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07584/x12.png",
                "caption": "Figure 12:Overview of model architecture.",
                "position": 520
            }
        ]
    },
    {
        "header": "4Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07584/x13.png",
                "caption": "Figure 13:Schematic overview of the multi-stage training pipeline.Theupper paneldelineates the Text-to-Image training trajectory, progressing from progressive pre-training and mid-training to post-training alignment via SFT, GRPO, and DPO. Thelower panelillustrates the Image Editing workflow, which initializes from the T2I development checkpoint.",
                "position": 555
            }
        ]
    },
    {
        "header": "5Model Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07584/x14.png",
                "caption": "Figure 14:Comparison of human evaluation MOS.",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x15.png",
                "caption": "Figure 15:Comparison of overall capability in image generation.",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x16.png",
                "caption": "Figure 16:Comparison of text rendering capability in image generation.",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x17.png",
                "caption": "Figure 17:Comparison of text rendering capability in image generation.",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x18.png",
                "caption": "Figure 18:Comparison of text rendering capability in image generation.",
                "position": 1886
            }
        ]
    },
    {
        "header": "6Image Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07584/x19.png",
                "caption": "Figure 19:Overview of image editing pre-training data.",
                "position": 1937
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x20.png",
                "caption": "Figure 20:Overview of LongCat-Image-Edit model architecture.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x21.png",
                "caption": "Figure 21:Task category distribution of CEdit-Bench.",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x22.png",
                "caption": "Figure 22:Comparison of human evaluation win rates between LongCat-Image-Edit and competing methods.",
                "position": 2652
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x23.png",
                "caption": "Figure 23:Visual comparison of multi-turn editing versus one-shot composite editing.The numbered sequence (①–⑥) illustrates the progressive results of multi-turn editing. In contrast, the “All in One” image (bottom-left) demonstrates the outcome of a single complex instruction containing all six operations:Remove the laptop, add a mug, change the wall to yellow, add a girl sitting on the bed, turn on the TV, and change the style to Ghibli.",
                "position": 2668
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x24.png",
                "caption": "Figure 24:Demonstration of fine-grained portrait editing.From left to right: The original input image, followed by results for blemish removal, hairstyle modification, lighting adjustment, eyelash addition, face slimming, and ID photo generation. The results highlight the model’s precision in manipulating specific facial attributes while preserving the subject’s identity.",
                "position": 2677
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x25.png",
                "caption": "Figure 25:Qualitative results on Human-centric Editing.The figure displays pairs of input (left) and edited (right) images across three dimensions:Pose & Interaction(top row), involving complex interaction synthesis (e.g., hugging) and large-scale body pose alteration;Viewpoint(bottom-left), transforming a subject from side view to front view; andLighting(bottom-right), simulating directional illumination. Note the preservation of background details and subject identity despite significant structural changes.",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x26.png",
                "caption": "Figure 26:Qualitative comparison on Style Transfer and Attribute Editing.The upper panel demonstrates the transformation of a photorealistic scene into a retro-colored illustration style. The lower panel illustrates a complex instruction involving both accessory addition (sunglasses) and hand pose modification (heart gesture), highlighting our model’s ability to preserve facial identity while executing significant structural changes.",
                "position": 2683
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x27.png",
                "caption": "Figure 27:Qualitative comparison on Object-centric Editing.We evaluate the performance across three distinct scenarios:Object Insertion(top), where an additional creature is added while maintaining scene consistency;Subject Extraction(middle), isolating the foreground subject (the cat) from a complex background;Object-Preserved Generation(bottom), where the reference object (the device) is seamlessly integrated into a new context (held by a robot).",
                "position": 2686
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x28.png",
                "caption": "Figure 28:Qualitative comparison on Scene Text Editing and Region-Controlled Editing.",
                "position": 2689
            },
            {
                "img": "https://arxiv.org/html/2512.07584/x29.png",
                "caption": "Figure 29:Qualitative comparison on Camera Control and Viewpoint Transformation.The upper panel showscamera distance adjustment(zooming out), where the view is expanded to place the flowers into a pot on a table. The lower panel displayscamera angle modification, transitioning the view to a low-angle perspective looking up at the dog.",
                "position": 2692
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Contributions and Acknowledgments",
        "images": []
    }
]