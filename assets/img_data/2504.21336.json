[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21336/x1.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21336/x2.png",
                "caption": "",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2504.21336/x3.png",
                "caption": "",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2504.21336/x4.png",
                "caption": "",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2504.21336/x5.png",
                "caption": "Figure 5:Workflow comparisons. The upper part presents the workflow of the representative biomedical segmentation foundation models,e.g., MedSAM[18]and BiomedParse[4]. The lower part shows the workflow of our introduced UniBiomed. We present an example of grounded report generation on CT images.",
                "position": 277
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Methods",
        "images": []
    },
    {
        "header": "5Data availability",
        "images": []
    },
    {
        "header": "6Code availability",
        "images": []
    },
    {
        "header": "7Author contributions",
        "images": []
    },
    {
        "header": "Declaration",
        "images": []
    },
    {
        "header": "Ethics declaration",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.21336/x6.png",
                "caption": "Figure 6:Overall comparisons on various biomedical tasks, including segmentation, disease recognition, ROI classification, region-aware report generation, and grounded report generation. We compare UniBiomed with nnUNet[3], MedSAM[18], BiomedParse[4], SegVol[19], SAT[20], InternVL[25], LLaVA-Med[11], MedRegA[14], MedPLIB[15], GLaMM[36], and LISA[29]. Notably, while previous methods can address only a limited number of these tasks, UniBiomed excels by delivering state-of-the-art performance across all of them.",
                "position": 2929
            },
            {
                "img": "https://arxiv.org/html/2504.21336/x7.png",
                "caption": "Figure 7:Region-aware report generation results on the MedTrinity[17]dataset. The text ingreenindicates the correct contents in generated reports.",
                "position": 3500
            },
            {
                "img": "https://arxiv.org/html/2504.21336/x8.png",
                "caption": "Figure 8:Grounded report generation results on the RadGenome[31]dataset. The generated segmentation masks are inorange. The text ingreenindicates the correct contents in generated reports.",
                "position": 3504
            }
        ]
    },
    {
        "header": "Appendix AExtended Data",
        "images": []
    }
]