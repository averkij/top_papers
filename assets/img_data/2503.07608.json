[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07608/x1.png",
                "caption": "Figure 1:Our planning-oriented RL and two-stage training strategy significantly boost planning accuracy. With just 20k samples, it outperforms SFT by 35.31%, showing strong performance even with limited data. As data increase, AlphaDrive consistently leads in planning performance.",
                "position": 73
            },
            {
                "img": "https://arxiv.org/html/2503.07608/x2.png",
                "caption": "Figure 2:Overall training framework of AlphaDrive.AlphaDrive is trained using GRPO-based RL, and we design four planning rewards to help the model understand and learn planning. Besides, we propose a two-stage training paradigm, the first stage uses SFT to distill the planning reasoning process from a large model and serves as a warm-up, while the second stage employs RL to explore planning.",
                "position": 145
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3AlphaDrive",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07608/x3.png",
                "caption": "Figure 3:Qualitative results of AlphaDirve. After RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which has great potential for improving driving safety and efficiency.",
                "position": 1038
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]