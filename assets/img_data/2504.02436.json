[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/precase.png",
                "caption": "Figure 1:Examples ofelements-to-videoresults from our proposedSkyReels-A2model.Given reference with multiple images and textual prompt, our method can generate realistic and naturally composed videos while preserving specific identity consistent.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/framework.png",
                "caption": "Figure 2:Overview ofSkyreels-A2framework.Our approach initiates by encoding all reference images using two distinct branches. The first, termed the spatial feature branch (represented in red bottom arrow), leverages a fine-grained 3D VAE encoder to process per-composition images. The second, identified as the semantic feature branch (represented in red top arrow), utilizes a CLIP vision encoder followed by an MLP projection to encode semantic references. Subsequently, the spatial features are concatenated with the noised video tokens along the channel dimension before being passed through the diffusion transformer blocks. Meanwhile, the semantic features extracted from the reference images are incorporated into the diffusion transformers via supplementary cross-attention layers, ensuring that the semantic context is effectively integrated during diffusion.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/datapipeline.png",
                "caption": "Figure 3:Data processing pipeline inSkyReels-A2.The pipeline begins with preprocessing, where raw videos are filtered by resolution, labels, types, and sources, followed by temporal segmentation based on key-frames.\nNext, a proprietary multi-expert video captioning model generates both holistic descriptions for video clips and structured concept annotations.\nSubsequently, detection and segmentation models extract visual elements (e.g., humans, objects, environments).\nTo mitigate duplication, reference images are retrieved from other clips based on clip/facial similarity score.\nFurther refinement includes face detection and human parsing to obtain facial/attire elements.\nFinally, the extracted elements are matched with structured descriptions to form training triplets (visual elements, video clips, and textual captions).",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/bench.png",
                "caption": "Figure 4:The dimensions covered inA2-Bench.Our evaluation consider both automatic metrics and user study, meantime, it covers multiple perspectives that precisely reflects the quality ofE2Vtask.",
                "position": 333
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/user_study.png",
                "caption": "Figure 5:Consistency and video quality of user study results for elements-to-video generation.We can see that ourSkyReels-A2achieve comparable generative performance compared with top-tier closed source commercial video models.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/results2.png",
                "caption": "Figure 6:Comparative results of elements-to-video generation with closed-source models.We can see that ourSkyReels-A2achieve achieves similar performance in composition and excels in the texture of light and shadow.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2504.02436/extracted/6332719/results1.png",
                "caption": "Figure 7:More generated results ofSkyReels-A2.Our model has strong generalization ability and supports reference combinations of any subjects.",
                "position": 458
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledges",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]