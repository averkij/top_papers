[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07469/x1.png",
                "caption": "",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07469/x2.png",
                "caption": "Figure 2:Illustration of the difference between previous methods and our VideoCoF. We enhances the editing accuracy by forcing the video diffusion model to first predict the editing area, and then perform the editing.",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2512.07469/x3.png",
                "caption": "Figure 3:Overview of VideoCoF  framework.Our model processes source (blue), reasoning (orange), and target (green) tokens in a unified sequence to “reason” then “edit”.Bottom right:Our RoPE design enables length extrapolation.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07469/x4.png",
                "caption": "Figure 4:How our RoPE design avoid index collision.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2512.07469/x5.png",
                "caption": "Figure 5:Our data curation pipeline for multi-instance data.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07469/x6.png",
                "caption": "Figure 6:Visual comparision between our VideoCoF and other methods on diverse video editing tasks.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2512.07469/x7.png",
                "caption": "Figure 7:Length exploration on frames more than training.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2512.07469/x8.png",
                "caption": "Figure 8:Motion alignment benefit by our rope design.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2512.07469/x9.png",
                "caption": "Figure 9:Ablation on reasoning frame format.",
                "position": 773
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Full Comparison",
        "images": []
    },
    {
        "header": "7More Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07469/x10.png",
                "caption": "Figure 10:Input Prompt Variants for In-Context Video Editing. We evaluate two prompt formats: (a) Temporal Triptych Prompt - instructions embedded in structure ”A video sequence showing three parts: first the original scene, then grounded {ground instruction}, and finally the same scene but {edit instruction}.”) (b) Direct Instruction - explicit editing commands provided directly;",
                "position": 1632
            }
        ]
    },
    {
        "header": "8Implementation Details",
        "images": []
    },
    {
        "header": "9Metrics",
        "images": []
    },
    {
        "header": "10Discussion",
        "images": []
    }
]