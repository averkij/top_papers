[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21710/x1.png",
                "caption": "Figure 1:Performance Limitations of Graph-Based RAG systems under Resource-Constrained and Locally-Deployed Scenarios.\nIn such scenarios, developers typically adopt open-source models such as Llama or Qwen as the backbone LLMs. Limitations like incomplete extracted triplets, insufficient extraction details and parsing failure may lead to insufficient knowledge provision, ultimately resulting in failure to adequately answer the query.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2509.21710/x2.png",
                "caption": "Figure 2:Evolution of Retrieval-Augmented Generation Paradigms.(a) Naive RAG embeds raw documents and performs single-shot retrieval.\n(b) Graph-based RAG pre-builds a static graph once and retrieves from it.\n(c) ToG-3 introduces afour-agentloop—Retriever, Constructor, Reflector, Response—where the graph and the query sub-tasksco-evolveat runtime, yielding dynamic, query-adaptive context that converges to a minimal, sufficient subgraph.",
                "position": 180
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21710/x3.png",
                "caption": "Figure 3:Multi-Agent Dual-Evolving Context Retrieval-Response Loop.The Retriever fetches an initial chunk–triplet–community subgraph.\nThe Response Agent produces an answer; the Reflector Agent judges sufficiency (reward=1/0).\nIf insufficient (reward=0), the Reflector evolves the query into sub-queries while the Constructor evolves the subgraph (sub-graph refinement).\nThe loop repeats until the context becomes sufficient or the horizon is reached, after which the Response Agent synthesizes the final answer from the full trajectory.",
                "position": 341
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21710/x4.png",
                "caption": "(a)Exact Match (EM) Score Comparison",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2509.21710/x4.png",
                "caption": "(a)Exact Match (EM) Score Comparison",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2509.21710/x5.png",
                "caption": "(b)F1 Score Comparison",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2509.21710/x6.png",
                "caption": "Figure 5:ELO-based Pairwise Win Rate Matrices Across Four Benchmark Datasets.Each heatmap visualizes win probabilities derived from direct head-to-head experimental comparisons, transformed through the ELO framework to ensure transitive consistency. The diagonal of the heatmap is set to a default value of 0.5, indicating self-comparison of the method.",
                "position": 600
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation and Future Directions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BToG-3 Algorithms",
        "images": []
    },
    {
        "header": "Appendix CDataset Detail",
        "images": []
    },
    {
        "header": "Appendix DBaselines",
        "images": []
    },
    {
        "header": "Appendix EMetrics",
        "images": []
    },
    {
        "header": "Appendix FMore Experiment Results and Details",
        "images": []
    },
    {
        "header": "Appendix GCase Study for ToG-3",
        "images": []
    },
    {
        "header": "Appendix HGraph Visualization Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21710/figs/graph_of_deep_knowledge_reasoning_task.png",
                "caption": "Figure 9:Structural overview of the 2WikiMultihopQA subset, exemplifying depth reasoning through multi-hop entity-relation paths (e.g., traversing ”person → profession → historical event” to answer causal queries).",
                "position": 2510
            },
            {
                "img": "https://arxiv.org/html/2509.21710/figs/graph_of_broad_knowledge_reasoning_task.png",
                "caption": "Figure 10:Visualization of the computer science domain graph in UltraDomain, showcasing breadth reasoning via diverse node types (e.g., programming languages like Scala/Spark, frameworks like HDFS/Kafka) and relationship types (e.g.,implements,runs_on,contains).",
                "position": 2513
            }
        ]
    },
    {
        "header": "Appendix ITheoretical Support: Implicit Dynamics of In-Context Learning",
        "images": []
    },
    {
        "header": "Appendix JPrompt Templates",
        "images": []
    }
]