[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13965/x1.png",
                "caption": "(a)Chatbot",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x1.png",
                "caption": "(a)Chatbot",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x2.png",
                "caption": "(b)ReAct Agent",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x3.png",
                "caption": "(c)Map-Reduce",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x4.png",
                "caption": "(d)Monte Carlo Tree Search",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x5.png",
                "caption": "(a)Programs, arriving at t=0",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x5.png",
                "caption": "(b)First-Come First-Served (FCFS)",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x6.png",
                "caption": "(c)Multilevel Feedback Queue (MLFQ)",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x7.png",
                "caption": "(d)Program-Level Attained Service(PLAS)",
                "position": 187
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13965/x8.png",
                "caption": "Figure 3:AI Agent Infrastructure.Top: Developers and users build and execute agentic programs that orchestrate execution and persist global, cumulative history across agents, tools, and humans. Bottom: LLM serving systems process agents’ LLM calls and route calls across one or more LLM engines.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x9.png",
                "caption": "Figure 4:Number of LLM calls in serving engine during steady state over 1 hour.Optimizing programs’ wait times increases the volume of LLM calls at steady state.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x10.png",
                "caption": "Figure 5:Program execution and wait times, over different programs and system loads.With moderate loads, programs spend the most time waiting. The duration of waiting depends on the workload.",
                "position": 288
            }
        ]
    },
    {
        "header": "3Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13965/x11.png",
                "caption": "(a)Chatbot, LLM Calls",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x11.png",
                "caption": "(a)Chatbot, LLM Calls",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x12.png",
                "caption": "(b)Chatbot, Programs",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x13.png",
                "caption": "(c)MCTS, LLM Calls",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x14.png",
                "caption": "(d)MCTS, Programs",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x15.png",
                "caption": "(a)Single thread: Chatbot",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x15.png",
                "caption": "(a)Single thread: Chatbot",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x16.png",
                "caption": "(b)Multiple threads: MCTS",
                "position": 355
            }
        ]
    },
    {
        "header": "4AutellixDesign",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13965/x17.png",
                "caption": "Figure 8:Autellix’s system architecture.Users run their programs locally, which initiates a stateful session and submits LLM calls toAutellix’s backend.Autellixleverages a global process table to track sessions and better inform its custom load-balancer and scheduler.",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x18.png",
                "caption": "Figure 9:Critical path for multi-threaded programs.(Left) Example of a critical path through a DAG. (Right) Best-case scenario makespan, 14 units, versus worst-case makespan. 11 units.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x19.png",
                "caption": "Figure 10:LLM call lifecycle based on discretized prioritization.",
                "position": 673
            }
        ]
    },
    {
        "header": "5Implementation",
        "images": []
    },
    {
        "header": "6Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13965/x20.png",
                "caption": "(a)ShareGPT",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x20.png",
                "caption": "(a)ShareGPT",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x21.png",
                "caption": "(b)BFCL",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x22.png",
                "caption": "(c)LATS",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x23.png",
                "caption": "(d)Number of LLM calls",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x24.png",
                "caption": "Figure 12:Single Engine, Main Results.Average latency for different LLM serving systems across four real-world workloads.",
                "position": 852
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x25.png",
                "caption": "Figure 13:Single Engine, Tail Latencies.95th(P95) and 99th(P99) percentile latencies of different serving systems.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x26.png",
                "caption": "Figure 14:Multi-engine, Main Results.Latencies (Avg., P95/99) w.r.t. different load balancing policies.",
                "position": 926
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x27.png",
                "caption": "Figure 15:Scalability Experiments.Given same SLO (defined as s/tok),Autellix’s max arrival rate (program/s) scales linearly w.r.t number of replicas, or LLM engines.",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x28.png",
                "caption": "Figure 16:Offline batch inference.Autellixdecreases the time, or makespan, required to process a batch of programs.",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x29.png",
                "caption": "(a)ShareGPT",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x29.png",
                "caption": "(a)ShareGPT",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x30.png",
                "caption": "(b)LATS",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x31.png",
                "caption": "(a)ShareGPT",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x31.png",
                "caption": "(a)ShareGPT",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x32.png",
                "caption": "(b)LATS",
                "position": 1013
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x33.png",
                "caption": "(a)Swap Times",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x33.png",
                "caption": "(a)Swap Times",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2502.13965/x34.png",
                "caption": "(b)Throughput",
                "position": 1031
            }
        ]
    },
    {
        "header": "7Discussion & Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]