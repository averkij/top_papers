[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21406/x1.png",
                "caption": "Figure 1:Examples of depth and segmentation generation.Off-the-shelf UMMs (e.g., Harmon-1.5B(Wuet al.,2025c)) struggle to generate plausible depth/segmentation maps from input images, often producing outputs closer to RGB reconstruction.\nUniMRG post-trains UMMs to generate these intrinsic representations, encouraging them to internalize geometric cues (depth) and structural cues (segmentation) that are beneficial for visual understanding.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2601.21406/x2.png",
                "caption": "Figure 2:Motivation: Intrinsic visual representation generation enhances visual understanding.Top: Off-the-shelf UMMs fail to generate plausible depth maps and struggle with spatial understanding.\nBottom: After image-to-depth training, UMMs generate coherent depth maps and exhibit stronger spatial understanding, correctly identifying spatial relationships.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2601.21406/x3.png",
                "caption": "Figure 3:Overview of UniMRG.The input image is fed into the visual understanding encoder, and the UMM is jointly trained on four tasks:\n(1)Image reconstruction: reconstructing the input image to enhance generation capabilities.\n(2)Image-to-depth: generating depth maps to learn geometric cues and spatial relations.\n(3)Image-to-segmentation: generating segmentation maps to learn structural cues and region partitions.\n(4)Image understanding: performing standard vision-language understanding tasks.\nThe understanding encoder is updated for UMMs with a shared encoder for generation and understanding; otherwise it is frozen.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Unified Multi-Representation Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21406/x4.png",
                "caption": "(a)Qualitative generation results.UMMs post-trained with UniMRG better follow prompts involving multiple objects, spatial relationships, and complex attributes.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2601.21406/x4.png",
                "caption": "(a)Qualitative generation results.UMMs post-trained with UniMRG better follow prompts involving multiple objects, spatial relationships, and complex attributes.",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2601.21406/x5.png",
                "caption": "(b)Qualitative understanding results.UMMs post-trained with UniMRG exhibit improved fine-grained perception, reduced hallucinations, and enhanced spatial understanding.",
                "position": 742
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21406/x6.png",
                "caption": "Figure 5:Qualitative ablation study on the quality of UMM-generated images with different representation generations.D denotes depth representation generation, S denotes segmentation representation generation, and P denotes pixel representation generation.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2601.21406/x7.png",
                "caption": "Figure 6:OOD depth representation generation on MidjourneyV6.We sample 1,000 images from MidjourneyV6 and measure the similarity between model-generated depth maps and Depth Anything V2 targets using1âˆ’MAE1-\\mathrm{MAE}(higher is better), comparing base and UniMRG weights.",
                "position": 959
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APreliminaries",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21406/x8.png",
                "caption": "Figure 7:Depth and segmentation map generation by different UMMs before and after UniMRG training.",
                "position": 1934
            }
        ]
    },
    {
        "header": "Appendix CQualitative Results on Representation Generation",
        "images": []
    }
]