[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03334/x1.png",
                "caption": "Figure 1:Multi-task compatibility of UniAVGen.Leveraging its robust design, UniAVGen can simultaneously tackle three pivotal tasks:(a)joint audio-video generation,(b)video-to-audio dubbing, and(c)audio-driven video synthesis.",
                "position": 93
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03334/x2.png",
                "caption": "Figure 2:Architecture of UniAVGen:A dual-branch joint synthesis framework with asymmetric cross-modal interaction, augmented by face-aware modulation. Taking a reference image and text prompt as input, it enables coherent audio-video generation.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03334/x3.png",
                "caption": "Figure 3:Comparison of cross-modal interaction mechanisms: (a) Global Interaction is simple but poses challenges for convergence; (b) Symmetric Time-Aligned Interaction converges quickly but has limited context utilization; (c) Our Asymmetric Cross-Modal Interaction achieves a superior balance between convergence speed and performance through modal-specific interaction design.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2511.03334/x4.png",
                "caption": "Figure 4:Visual comparisons of UniAVGen against concurrent methods Ovi and UniVerse-1.Specifically, Example (a) uses an in-distribution real human image: UniAVGen and Ovi generate high-fidelity, well-aligned audio-video, while UniVerse-1 is nearly static. Example (b) uses an out-of-distribution (OOD) anime image: Ovi lacks aligned lip/motions (poor generalization), UniVerse-1 stays static with noisy audio; in contrast, our model shows strong generalization, producing coherent, aligned audio-video matching the anime input.",
                "position": 332
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03334/x5.png",
                "caption": "Figure 5:Visual comparisons of predicted masks with fixedλm\\lambda_{m}and decayingλm\\lambda_{m}.Zoom in for the best view.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2511.03334/x6.png",
                "caption": "Figure 6:Visual comparisons of joint generation results with and without MA-CFG.Zoom in for the best view.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2511.03334/x7.png",
                "caption": "Figure 7:Comparisons of different training strategies.",
                "position": 688
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]