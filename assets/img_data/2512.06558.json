[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06558/images/data_collection_system_01.png",
                "caption": "Figure 1.Refer360 data collection setup in human-robot interaction context (left). Here, the person is pointing towards an object while verbally describing it. Interaction frames from three different views (exo, ego, and depth). Highlighting the canonical frames, i.e., frames where the subject precisely points to an object (right).††:",
                "position": 304
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06558/images/different_sensor_view.png",
                "caption": "Figure 2.Sample canonical frames from Refer360 dataset in three different views: Exo-view (RGB), Ego-view (RGB), and Exo-View (Depth). The first, second, and third rows contain interaction samples from a home, lab, and outdoor location.††:",
                "position": 352
            }
        ]
    },
    {
        "header": "3.Refer360: E-RFE Dataset",
        "images": []
    },
    {
        "header": "4.Data Collection Procedure",
        "images": []
    },
    {
        "header": "5.Dataset Analysis",
        "images": []
    },
    {
        "header": "6.MuRes: Multimodal Guided Residual Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06558/images/rerep_model_01.png",
                "caption": "Figure 3.Multimodal Model, MuRes, with the Guided Residual module. Visual and language representations are extracted and projected from a pre-trained VL model. The projected representations are fed into the cross-attention module as the query. The key and value are the original extracted visual and language representations on the residual connection. The output from the cross-attention module and the projection are summed for downstream task learning.††:",
                "position": 484
            }
        ]
    },
    {
        "header": "7.Experimental Setup",
        "images": []
    },
    {
        "header": "8.Experimental Results",
        "images": []
    },
    {
        "header": "9.Overal Discussion",
        "images": []
    },
    {
        "header": "10.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AResources",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Results: Quantitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06558/images/ScienceQA.png",
                "caption": "Figure 4.Qualitative analysis of VisualBERT with and without the proposed guided residual module (MuRes) on two datasets. Incorporating MuRes improves visual question answering by aligning multimodal context in both ScienceQA and A-OKVQA tasks. Subfigure (a) shows qualitative results on questions involving diagrams from the ScienceQA dataset(Lu et al.,2022). Subfigure (b) shows model reasoning on ambiguous visual questions from the A-OKVQA dataset(Schwenk et al.,2022).††:",
                "position": 1929
            },
            {
                "img": "https://arxiv.org/html/2512.06558/images/AOK_VQA.png",
                "caption": "",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2512.06558/images/supplementary/demographic-survey.png",
                "caption": "Figure 6.User interfaces shown during data collection. (a) Participants first completed a demographic survey. (b) After viewing the stories, participants completed a post-task evaluation survey.††:",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2512.06558/images/supplementary/post-task-survey.png",
                "caption": "",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2512.06558/images/supplementary/dir_structure.png",
                "caption": "Figure 7.Folder structure of the Refer360 dataset. Each interaction contains audio, video, skeletal, and sensor data across RGB, depth, and infrared modalities.",
                "position": 2140
            }
        ]
    },
    {
        "header": "Appendix CData Collection",
        "images": []
    }
]