[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/x1.png",
                "caption": "Figure 1:What are thetelltale signs of a generative action video?\nWe answer this by learning a robust manifold based on appearance and anatomical coherence exhibited by humans performing actions across several real-world videos.\nThis manifold serves as anchors against which we project the features of a generated video in question and assess its realism.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x2.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/x3.png",
                "caption": "Figure 2:Architectural overview of the encoder we train to learn the real-world action manifold.We extract per-frame static human-centric and temporal motion features (Fig. (a)) (Sec.3.1), and aggregate them, yielding one embedding for each frame (Fig. (b)) (Sec.3.2.1). We prepend a[CLS][\\text{CLS}]token to the per-frame tokens and pass as input to a 4-layer transformer encoder (Fig. (c)) (Sec.3.2.1). Our aim is to encourage the encoder to group diverse videos pertaining to a given action closer together. We also ensure that temporally incoherent videos lie farther apart.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Telltale Action Generation (TAG)-Bench",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/x4.png",
                "caption": "Figure 3:Model comparisons on TAG-Bench and VBench-2.0 Human Anatomy.We compare models pairwise for the same input prompt; for each pair, the model with the higher score (human or metric) is the winner. We then plot the win ratios (see Sec.5.3) of human scores (x-axis) against win ratios from our metric (y-axis). Our metrics (SconsS_{\\mathrm{cons}}andStempS_{\\mathrm{temp}}) observe the same ranking of models as humans on both benchmarks.",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x5.png",
                "caption": "",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2512.01803/",
                "caption": "",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x7.png",
                "caption": "Figure 4:Comparing generative models.We plot the meanSconsS_{\\mathrm{cons}}andStempS_{\\mathrm{temp}}scores (Sec.3.3) (lower is better) for each generative model across different actions. Wan2.2 performs best among the other models (low scores in bothSconsS_{\\mathrm{cons}}andStempS_{\\mathrm{temp}}).ShotputandJumpingJackchallenge all models, yielding high scores across both metrics.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x8.png",
                "caption": "Figure 5:t-SNE visualization of the embeddings of generated videos along with train centroids.We project thezCLSz_{\\text{CLS}}embeddings ofgeneratedvideos (colored markers) from TAG-Bench and the correspondingtrainingclass centroids (white crosses) using t-SNE[tsne]. Realistic generated videos cluster near their respective class centroids (e.g., Wan2.2 videos for “PullUps”, with an average human rating of:8.41\\mathbf{8.41}forAction Consistency), while poorly generated videos lie further away (e.g., Wan2.2 videos for “Shotput” with an average human rating of:4.43\\mathbf{4.43}) (See Sec.4).",
                "position": 834
            }
        ]
    },
    {
        "header": "6Discussion and Future Work",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AHuman Evaluation User Interface (UI)",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/human_eval_screenshot.png",
                "caption": "Figure 6:User interface used for the human evaluation study.Participants were asked to rate each AI-generated video along two dimensions:Action Consistency(how accurately the motion matches the described action) andTemporal Coherence(how natural and physically realistic the motion appears). Each participant viewed 30 videos and provided ratings on a scale from 0 to 10.",
                "position": 1211
            }
        ]
    },
    {
        "header": "Appendix BTAG-Bench: Model configurations, Subject rejection, Aggregation of Opinion Scores",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/std_vs_raters_action_consistency.jpg",
                "caption": "Figure 7:Standard deviation vs. number of raters for five randomly selected videos (Action Consistency).\nEach curve shows the evolution of the MOS standard deviation as more raters are included. Notice how the standard deviation is stabilizing as we add more raters.",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/std_vs_raters_physical_plausibility.jpg",
                "caption": "Figure 8:Standard deviation vs. number of raters for five randomly selected videos (Temporal Coherence). Each curve shows the evolution of the MOS standard deviation as more raters are included. Notice how the standard deviation is stabilizing as we add more raters.",
                "position": 1339
            }
        ]
    },
    {
        "header": "Appendix CAdditional implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/x9.png",
                "caption": "Figure 9:TokenHMR-based feature extraction.Each input frame is processed by Detectron2[wu2019detectron2]to obtain a bounding box of the person, which is cropped and passed to TokenHMR[dwivedi2024tokenhmr]. TokenHMR uses a ViT-H/16 backbone, followed by transformer blocks and task-specific heads, to predict SMPL parameters: pose (θ\\theta), global orientation (g​ogo), and body shape (β\\beta) in Sec.3.1.1. Intermediate features from the ViT backbone are used as visual appearance features (fvisf_{\\text{vis}}) in Sec.3.1.3. We adapt the figure from TokenHMR[dwivedi2024tokenhmr].",
                "position": 1356
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x10.png",
                "caption": "Figure 10:Human-centric input features.From each frame, we extract (a) 3D pose, body shape, and global orientation (Sec.3.1.1), (b) 2D keypoints (Sec.3.1.2), and (c) visual appearance features (Sec.3.1.3) to describe the body’s state in that frame. (d) We additionally compute temporal differences of each feature to capture frame-to-frame motion dynamics (Sec.3.1.4)",
                "position": 1371
            }
        ]
    },
    {
        "header": "Appendix DWin Ratios (TAG-Bench, VBench-2.0)",
        "images": []
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/tsne_test_with_centroids_windows.png",
                "caption": "Figure 11:t-SNE visualization of the embeddings of unseen test videos of diverse actions.We project thezCLSz_{\\text{CLS}}embeddings ofunseen realtest videos (colored markers) and the correspondingtrainingclass centroids (crosses) using t-SNE[tsne]. It is evident that unseen test videos cluster around their respective class centroids, indicating that the learned embedding space captures compact and semantically meaningful action structure.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/distortion_trend_Scons.png",
                "caption": "Figure 12:Sensitivity ofAction Consistency(SconsS_{\\mathrm{cons}}) andTemporal Coherence(StempS_{\\mathrm{temp}}) to controlled temporal distortions. The mean value across all test samples is plotted for each distortion type and severity.",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/distortion_trend_Scons.png",
                "caption": "",
                "position": 1597
            },
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/distortion_trend_Stemp.png",
                "caption": "",
                "position": 1602
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x11.png",
                "caption": "Figure 13:Average attention weights averaged over all real test videos.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2512.01803/x12.png",
                "caption": "Figure 14:Per-class attention weights, averaged over all real test videos.",
                "position": 1693
            }
        ]
    },
    {
        "header": "Appendix FBaselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/grid_example_Hunyuan_BodyWeightSquats.png",
                "caption": "Figure 15:Example of the4×104{\\times}10grid-panel layout used to prompt MLLMs.\nShown here is a video generated byHunyuan[kong2024hunyuanvideo]for the action classBodyWeightSquats. We uniformly sample 40 frames and place them in row-major order. Each cell overlays its grid coordinates(row,col)in the top-left; when the video duration is available, the timestamp is also shown (e.g.,(0,3) 1.2s). This grid preserves temporal progression and spatial structure, providing clearer visual evidence than direct video input.",
                "position": 1768
            },
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/qwen3vl_Baseline_scatter.png",
                "caption": "Figure 16:Per-video Qwen3-VL scores forAction Consistency(orange) andTemporal Coherence(green) before (left) and after (right) applying in-context learning. Individual dots correspond to the model prediction for each generated video. Scores are highly saturated near 0 or 1 in both cases, indicating binary-like decisions rather than nuanced motion reasoning.",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2512.01803/imgs/qwen3vl_icL_scatter.png",
                "caption": "",
                "position": 1859
            }
        ]
    },
    {
        "header": "Appendix GMLLM Prompting",
        "images": []
    }
]