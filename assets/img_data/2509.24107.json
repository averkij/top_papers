[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24107/combined.png",
                "caption": "Figure 1:Comparison ofFathom-Search-4Bon prominent DeepSearch benchmarks(WebWalker, SimpleQA). Our model consistently outperforms strong open-source & closed source baselines.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2509.24107/main.png",
                "caption": "Figure 2:End-to-end inference framework of our proposed Fathom-DeepResearchagentic system combining browsing and information gathering ability ofFathom-Search-4Bwith information synthesis and insight generation ability ofFathom-Synthesizer-4B",
                "position": 115
            }
        ]
    },
    {
        "header": "2Fathom-Search-4B",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24107/mix.png",
                "caption": "Figure 3:Multi-agent self-playframework used to generate a sample multi-hop DeepSearch question with live-web-search dependency for theDuetQAdataset",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2509.24107/duetqa.png",
                "caption": "Figure 4:Distribution of number of search-calls issued by o3(OpenAI,2025a)over correctly answered questions comparing DuetQA to other prominent benchmarks.DuetQA shows strict live-web-search dependence and multi-hop reasoningas evident from the long-trained distribution (unlike simpleQA(Wei et al.,2024)) and≥\\geq1 search call(s) required to answer all DuetQA questions correctly (unlike FRAMES(Krishna et al.,2024), Musique(Trivedi et al.,2022)).",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2509.24107/entropy-gradnorm.png",
                "caption": "Figure 5:Comparison of policy entropy and gradient norm during RLVR training. GRPO exhibits\nrapid entropy collapse and diminished gradient norms due to sparse rewards, whereas RAPO sustains\nexploration and stronger updates via targeted updates",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2509.24107/ssr.drawio.png",
                "caption": "Figure 6:Evolution of unique/redundant tool-calls during Stage-2 training using our Steerable Step-level Reward (Eq.8)",
                "position": 356
            }
        ]
    },
    {
        "header": "3Fathom-Synthesizer-4B",
        "images": []
    },
    {
        "header": "4Baselines, Benchmarks & Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24107/TOOL_vs_ACC.png",
                "caption": "Figure 7:Accuracy vs Response length&Accuracy vs Avg.Tool callsplot comparing open-source DeepSearch models, clearly demonstrates the higher accuracy and efficient long horizon tool interaction ability of Fathom-Search models compared to its contemporaries. (Note:Here accuracy refers to the unweighted average of all benchmarks in Table.1.)",
                "position": 865
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24107/TTS2.png",
                "caption": "Figure 8:Response length evolution during(i) top.Stage-2 training (Steerable Step Level Reward vs. Vanilla Reward) &(ii) bottom.Stage-1 training (GRPO vs RAPO)",
                "position": 955
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]