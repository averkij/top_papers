[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x1.png",
                "caption": "Figure 1:(a) Comparison with SOTA models on image generation and editing benchmarks. For editing, the specific models are Qwen-Image-Edit-2509[106]and FLUX.1 Kontext [dev][49]. (b) Automated preference evaluation (Win Rate[%\\%]) against Gemini 2.5 Flash Image (Nano Banana)[92]on interleaved generation tasks.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x2.png",
                "caption": "Figure 2:Native multimodal capabilities of Emu3.5.\nGray: prompts; Blue: results.",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x3.png",
                "caption": "Figure 3:Overview of the Emu3.5 architecture. The model is trained end-to-end at scale with a unified next-token prediction objective. During inference, single-token prediction is accelerated via discrete diffusion adaptation, enabling bidirectional parallel generation per image.",
                "position": 236
            }
        ]
    },
    {
        "header": "2Emu3.5",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x4.png",
                "caption": "Figure 4:Overall training pipeline of Emu3.5.",
                "position": 312
            }
        ]
    },
    {
        "header": "3Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x5.png",
                "caption": "Figure 5:Data statistics of video interleaved data.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x6.png",
                "caption": "Figure 6:Video interleaved data samples from Emu3.5’s pre-training dataset.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x7.png",
                "caption": "(a)Training loss curve of Emu3.5 during the first stage of pre-training",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x7.png",
                "caption": "(a)Training loss curve of Emu3.5 during the first stage of pre-training",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x8.png",
                "caption": "(b)Validation loss curve of Emu3.5 on 9 validation sets during the first stage of pre-training",
                "position": 576
            }
        ]
    },
    {
        "header": "4Post-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x9.png",
                "caption": "Figure 8:Average reward steadily increases from∼\\sim4.5 to>7.1>7.1during multi-task RL training.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x10.png",
                "caption": "Figure 9:Discrete Diffusion Adaptation.(a)The model performs standard next-token prediction for large-scale multimodal pre-training, supervised fine-tuning, and reinforcement learning.(b)During discrete diffusion adaptation, each image is duplicated with a noisy copy. Noisy tokens attend causally to preceding clean tokens and bidirectionally to noisy tokens within the same image, while clean image and text tokens follow the original causal pattern to preceding clean tokens.",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x11.png",
                "caption": "Figure 10:Hybrid Inference Framework with FSM-based Scheduling.",
                "position": 850
            }
        ]
    },
    {
        "header": "5Tokenizer Training",
        "images": []
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x12.png",
                "caption": "Figure 11:Qualitative results of reconstructions with different decoders,i.e. the vanilla image decoder and the diffusion-based one.",
                "position": 2975
            }
        ]
    },
    {
        "header": "7Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "8Authors and Contributions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26583/x13.png",
                "caption": "Figure 12:Text-to-image generation results of Emu3.5.",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x14.png",
                "caption": "Figure 13:Any-to-image generation (X2I) results of Emu3.5.",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x15.png",
                "caption": "Figure 14:Any-to-image generation (X2I) results of Emu3.5.",
                "position": 3174
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x16.png",
                "caption": "Figure 15:Any-to-image generation (X2I) results of Emu3.5.",
                "position": 3180
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x17.png",
                "caption": "Figure 16:Visual narrative results of Emu3.5.",
                "position": 3186
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x18.png",
                "caption": "Figure 17:Visual guidance results of Emu3.5.",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x19.png",
                "caption": "Figure 18:World exploration results of Emu3.5.\nThe overlaid buttons on each frame represent the camera movement or viewpoint change instructions from the current frame to the next.",
                "position": 3198
            },
            {
                "img": "https://arxiv.org/html/2510.26583/x20.png",
                "caption": "Figure 19:Embodied manipulation results of Emu3.5.",
                "position": 3206
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]