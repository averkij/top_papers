[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11605/x1.png",
                "caption": "Figure 1:An example of the interfering factors (story content) in independently sampled multiple responses (Left). Refined response pairs exclude these factors, highlight the key difference (ending sentence), and lead to improved performance on iteratively trained LLaMA3-8B-Instruct (Right).",
                "position": 164
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11605/x2.png",
                "caption": "Figure 2:SPaRiterative training framework. At iterationtùë°titalic_t, the refinerRtsubscriptùëÖùë°R_{t}italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfirst judges the generated responses from the actorMtsubscriptùëÄùë°M_{t}italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTto collect negative data. Next, a tree-search algorithm is employed to refine these imperfect responses. Finally, using the data from the above steps, we can optimize the actor and refiner for the next iteration, aiming for continuous self-improvement.",
                "position": 219
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/baseline.png",
                "caption": "Figure 3:Comparison with baseline methods across iterations (Cf. Figure9forSPaR-7B).SPaR-8B consistently surpasses all baselines.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/toy.png",
                "caption": "Figure 4:Synthetic data experiment results:Character Sequence Generation(left) andStart/End Story Generation(right).\nForCharacter Sequence Generation, interfering pairs show rapid learning of the uppercase ratio (interfering factor) but perform worse than refinement pairs. In theStart/End Story Generationtask, refinement pairs outperform interfering pairs, which even underperform the original model at step 0.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/story.png",
                "caption": "",
                "position": 1209
            },
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/decoding.png",
                "caption": "Figure 5:Comparison of decoding strategies. Model performance improves with increased inference times.",
                "position": 1328
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/taxonomy.png",
                "caption": "Figure 6:The detailed taxonomy of constraints for prompt evolution.",
                "position": 2028
            },
            {
                "img": "https://arxiv.org/html/2412.11605/x3.png",
                "caption": "Figure 7:The prompt template applied for prompt evolution.",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2412.11605/x4.png",
                "caption": "Figure 8:The prompt template applied for the refiner‚Äôs judgment and refinement.",
                "position": 2041
            }
        ]
    },
    {
        "header": "Appendix BTree-search Algorithm",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/baseline_mistral.png",
                "caption": "Figure 9:Comparison with baseline methods across iterations.SPaR-7B consistently surpasses all baselines.",
                "position": 2179
            },
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/llama_inference_times.png",
                "caption": "Figure 10:Inference-time scaling comparison on IFEval. The left panel showcases results for LLaMA3-8B-Instruct, while the right panel presents findings for Mistral-7B-Instruct.",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/mistral_inference_times.png",
                "caption": "",
                "position": 2193
            }
        ]
    },
    {
        "header": "Appendix DExperiment Results",
        "images": []
    }
]