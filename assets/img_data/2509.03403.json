[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03403/fig/PROF_alg.jpeg",
                "caption": "Figure 1:Left: Visualization of PROF Algorithm1, where the length of each rectangle represents values of process rewards averaged over steps for each rollout. After generatingnnrollouts and process rewards, PROF ranks the correct and incorrect group separately according to PRM-ORM consistency, so for the correct group, the longer items are kept; for the incorrect group, the shorter items are kept. The number to remove is to balance correct and incorrect ratio. Right: Fraction of flawed-reasoning responses judged by LLM among the filtered-out correct responses.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/PROF_alg.jpeg",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/filtered_out_correct_response_pie.png",
                "caption": "",
                "position": 169
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03403/fig/Qwen2.5-Math-1.5B_data_temp1.0_comparison_main.png",
                "caption": "Figure 2:The learning dynamics of PROF-GRPO initialized from Qwen2.5-Math-1.5B-base (left) and Qwen2.5-Math-7B-base (right). The y-axis is the average@1616accuracy and is further averaged on Math500, Minerva Math and Olympiad Bench. For comparison, we also plot the baseline GRPO and directly blending PRM with outcome rewards, denoted as Blend-PRM-GRPO.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/Qwen2.5-Math-1.5B_data_temp1.0_comparison_main.png",
                "caption": "",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/7B_model_comparison_temp1.0_main.png",
                "caption": "",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/actor_entropy_loss.png",
                "caption": "Figure 3:Left: the entropy loss curves of GRPO, Blend-PRM-GRPO and PROF-GRPO; Right: the response length curves of GRPO, Blend-PRM-GRPO and PROF-GRPO, all of which are initialized from Qwen2.5-Math-7B-base.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/actor_entropy_loss.png",
                "caption": "",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/response_length_mean.png",
                "caption": "",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/mc_prm_bar.png",
                "caption": "Figure 4:Reasoning intermediate-steps performance of PROF-GRPO in comparison with GRPO. The most left plot is the Monte Carlo (MC) estimation scores across five benchmarks. The other three are on Math500 under metrics of number of steps (2nd left), the averaged process rewards generated by Qwen2.5-Math-PRM-7B (3rd left), and LLM’s preference between two modes’ responses (most right).",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/mc_prm_bar.png",
                "caption": "",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/step_number_bar.png",
                "caption": "",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/prm_bar.png",
                "caption": "",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/claude_pref_bar.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/PRM_example_minerval.jpeg",
                "caption": "Figure 5:A Minerval-Math example to compare distinct intermediate reasoning patterns of PROF-GRPO, vanilla GRPO and Blend-PRM-GRPO. PROF-GRPO presents concrete and correct deduction steps. GRPO’s solution skips detailed deduction steps and there are flaws in the calculation precision and the final rounding. Blend-PRM-GRPO has long-winded steps and makes a big mistake in calculating the power.",
                "position": 553
            }
        ]
    },
    {
        "header": "5Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03403/fig/Qwen2.5-Math-1.5B_data_temp1.0_comparison_+-.png",
                "caption": "Figure 6:First row: the averaged accuracy over three benchmarks Math500, Minerva Math and Olympiad Bench for PROF-GRPO (Filter both correct and incorrect with PRM consistency) and its variants initialized from wen2.5-Math-1.5B-base (left) and\nQwen2.5-Math-7B-base (right).\nLower left: the gap between the training rewards after and before the filtering for PROF-GRPO in comparison with not separating correct and incorrect groups (w/o separation). Lower right: the averaged accuracy across all five benchmarks over rollout sizesn=4,8,12,16n=4,8,12,16for filtering both correct and incorrect groups with PRM consistency (Both) and only the correct group with PRM consistency (Correct).",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/Qwen2.5-Math-1.5B_data_temp1.0_comparison_+-.png",
                "caption": "",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/7B_model_comparison_temp1.0_+-.png",
                "caption": "",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/gap_after_before_filtering.png",
                "caption": "",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/nroll_plot.png",
                "caption": "",
                "position": 584
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Details and Results",
        "images": []
    },
    {
        "header": "Appendix BAddtional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03403/fig/train_len_segs_mean.png",
                "caption": "Figure 7:The number of reasoning steps during training time for PROF-GRPO and Filter-Nstep initialized from Qwen2.5-Math-7B-base.",
                "position": 1511
            },
            {
                "img": "https://arxiv.org/html/2509.03403/fig/PRM_example_math500.jpeg",
                "caption": "Figure 8:A Math500 example to compare distinct intermediate reasoning patterns of PROF-GRPO, vanilla GRPO and Blend-PRM-GRPO. PROF-GRPO presents concrete and correct deduction steps. PROF-GRPO’s solution shows how to find the divisors and summation in detail, and is easy to follow. GRPO skips all core reasoning. Blend-PRM-GRPO has inefficient and excessively tedious steps.",
                "position": 1518
            }
        ]
    },
    {
        "header": "Appendix CAdditional Examples",
        "images": []
    }
]