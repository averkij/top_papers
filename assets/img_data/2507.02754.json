[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Overview of neural scaling laws",
        "images": []
    },
    {
        "header": "4The2222-simplicial Transformer",
        "images": []
    },
    {
        "header": "5Determinant based Trilinear Forms",
        "images": []
    },
    {
        "header": "6Model design",
        "images": []
    },
    {
        "header": "7Kernel Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02754/extracted/6592001/images/2_simplical_tiling.png",
                "caption": "Figure 2:Left:Visualization of sliding window 2-simplical attention. EachQisubscriptğ‘„ğ‘–Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTattends to a[wâ¢1,wâ¢2]ğ‘¤1ğ‘¤2[w1,w2][ italic_w 1 , italic_w 2 ]shaped rectangle ofKğ¾Kitalic_K,Kâ€²superscriptğ¾â€²K^{\\prime}italic_K start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT.Right:Tiling to reduce\n2-simplicial einsumQâ¢Kâ¢Kâ€²ğ‘„ğ¾superscriptğ¾â€²QKK^{\\prime}italic_Q italic_K italic_K start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPTto elementwise mulQâ¢Kâ€²ğ‘„superscriptğ¾â€²QK^{\\prime}italic_Q italic_K start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPTon CUDA core and tiled matmul(Qâ¢Kâ€²)â¢@â¢Kğ‘„superscriptğ¾â€²@ğ¾(QK^{\\prime})@K( italic_Q italic_K start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) @ italic_Kon tensor core.",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2507.02754/extracted/6592001/images/2_simplical_flops.png",
                "caption": "Figure 3:FLOPs and Latencies of FAv3 vs 2-simplical attention",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2507.02754/extracted/6592001/images/2_simplical_flops.png",
                "caption": "",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2507.02754/extracted/6592001/images/2_simplical_latency.png",
                "caption": "",
                "position": 634
            }
        ]
    },
    {
        "header": "8Experiments & Results",
        "images": []
    },
    {
        "header": "9Discussion",
        "images": []
    },
    {
        "header": "10Conclusion",
        "images": []
    },
    {
        "header": "11Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARotation invariant trilinear forms",
        "images": []
    },
    {
        "header": "Appendix BTriton Kernel: Forward pass for 2-simplicial Attention",
        "images": []
    },
    {
        "header": "Appendix CTriton Kernel: Backward pass for 2-simplicial Attention",
        "images": []
    }
]