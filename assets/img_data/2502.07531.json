[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07531/x1.png",
                "caption": "Figure 1.VidCRAFT3 is a high-quality image-to-video generation model that supports large object motion, view changes, and strong lighting effects. It offers user-friendly control over camera motion (a trajectory in blue), object motion (sparse trajectories in red), and lighting direction. VidCRAFT3 can take any combination of supported control signals and deliver fine-grained and faithful generation results.",
                "position": 61
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07531/x2.png",
                "caption": "Figure 2.Architecture ofVidCRAFT3for controlled image-to-video generation. The model builds on Video Diffusion Model (VDM) and consists of three main components: theImage2Cloudmodule generates high-quality 3D point cloud renderings from a reference image; theObjMotionNetmodule encodes object motion represented by sparse trajectories, the output is integrated into the UNet encoder by element-wise addition; theSpatial Triple-Attention Transformermodule integrates image, text, and lighting information via parallel cross-attention modules. The model generates video by conditioning on camera motion, object motion, and lighting direction, ensuring realistic and consistent outputs across different modalities.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x3.png",
                "caption": "Figure 3.Illustrations of examples from the Video Lighting Direction (VLD) Dataset, showcasing samples with 3D models and backgrounds. (a) Poly Haven-based VLD Samples with HDR backgrounds. (b) BOP-based VLD Samples with textured backgrounds. Each set includes video frames of two samples under two different lighting conditions.",
                "position": 288
            }
        ]
    },
    {
        "header": "4.Dataset Construction",
        "images": []
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07531/x4.png",
                "caption": "Figure 4.Qualitative comparisons with SOTA methods on RealEstate10K.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x5.png",
                "caption": "Figure 5.Qualitative comparisons with SOTA methods on WebVid-10M.",
                "position": 341
            }
        ]
    },
    {
        "header": "6.Conclusions and Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07531/x6.png",
                "caption": "Figure 6.Additional experimental results on camera motion control + lighting direction control.",
                "position": 1317
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x7.png",
                "caption": "Figure 7.Additional experimental results on camera motion control.",
                "position": 1320
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x8.png",
                "caption": "Figure 8.Additional experimental results on camera motion control + object motion control.",
                "position": 1323
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x9.png",
                "caption": "Figure 9.Qualitative results of the ablation study on training strategies with WebVId-10M.",
                "position": 1326
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x10.png",
                "caption": "Figure 10.Qualitative results of the ablation study on lighting embedding integration strategies on VLD.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x11.png",
                "caption": "Figure 11.Qualitative results of the ablation study on the representation of lighting direction on VLD.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x12.png",
                "caption": "Figure 12.Additional experimental results on object motion control.",
                "position": 1335
            },
            {
                "img": "https://arxiv.org/html/2502.07531/x13.png",
                "caption": "Figure 13.Additional experimental results on lighting direction control.",
                "position": 1338
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]