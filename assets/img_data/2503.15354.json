[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15354/x1.png",
                "caption": "Figure 1:Left: overall framework ofDecompose-Then-Verifyparadigm. We define each decomposer and verifier as a LLM paired with a corresponding policy. Our dynamic decomposition is compatible with existing fact-checking systems and requires training only a decomposition policy with 4.73M parameters.Right: the figure (upper right) shows that the verification confidence of the verifier (i.e., Inst-Llama-7B with a retrieval verification policy) peaks at atomicity 1. An atomicity of -1 denotes the claim is partially trivial and tautological. The example (lower right) shows thatthe decomposition policy from FActScoreMin et al. (2023)fails to generate subclaims that best evoke the verifier’s performance, leading to suboptimal results.",
                "position": 188
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15354/x2.png",
                "caption": "Figure 2:Verification confidence versus accuracy. The number in each convex hull denotes the claim atomicity.Irrespective of data sources, atomicities, and verifiers, verification confidence exhibits a strong positive correlation with accuracy (0.88 Pearson’s r).",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2503.15354/x3.png",
                "caption": "Figure 3:Breadth-first order sampling for dynamic decomposition. We perform binary decomposition for each claim. The number in the node represents its sampling priority in the decomposition process. We first sample out subclaims at the same atomicity level, with newly generated subclaims queued in a FIFO (first-in-first-out) order.",
                "position": 408
            }
        ]
    },
    {
        "header": "3Experiment Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15354/x4.png",
                "caption": "Figure 4:Verification confidence across atomicities.Evidently, each verifier has its own preferred input atomicity at which the verification confidence peaks. Even when utilizing the same verification policy, such as retrieval, different verifiers exhibit distinct preferences, and vice versa.",
                "position": 596
            }
        ]
    },
    {
        "header": "4Results and Analysis",
        "images": []
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations of Static Decomposition Policies",
        "images": []
    },
    {
        "header": "Appendix BExperiment Setup",
        "images": []
    },
    {
        "header": "Appendix CExperiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15354/x5.png",
                "caption": "(a)Verification results on ChatGPT dataset",
                "position": 2048
            },
            {
                "img": "https://arxiv.org/html/2503.15354/x5.png",
                "caption": "(a)Verification results on ChatGPT dataset",
                "position": 2051
            },
            {
                "img": "https://arxiv.org/html/2503.15354/x6.png",
                "caption": "(b)Verification results on PerplexityAI dataset",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2503.15354/x7.png",
                "caption": "Figure 6:Average information loss measures across training steps on the validation set. Clearly, it decreases as the model continues training and learning to decompose claims into more atomic levels. This trend aligns with our design motivation outlined in Eq.5: as claims become syntactically closer to bleached claims through decomposition, the resulting information loss diminishes accordingly.",
                "position": 2064
            }
        ]
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    }
]