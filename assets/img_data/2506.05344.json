[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05344/x1.png",
                "caption": "Figure 1:Head Sparsity Emerges from Visual Concept Responses.We observe the visual-relevant heads are sparse in various MLLMs. Based on this observation, we devise a KV-Cache optimization strategy that allocates asymmetric budgets to LLM heads based on their importance for visual tokens, achieving better trade-off under limited computational resources.",
                "position": 71
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05344/x2.png",
                "caption": "Figure 2:Visual Heads are Sparse in MLLMs.We use OCR tasks to obtain visual scores for all heads. Upon visualizing these scores, we discovered that high-scoring heads, which we refer to asvisual heads, are quite sparse within the MLLM, comprising only about 5%. The majority of heads have very low scores, indicating that most heads in LLMs do not focus on visual information.",
                "position": 100
            }
        ]
    },
    {
        "header": "3Visual Heads are Sparse in MLLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05344/x3.png",
                "caption": "Figure 3:SparseMM for MLLM Acceleration.The KV Cache budget for each head is composed of three parts:Local Window Cache,Uniform-Based Cache, andScore-Preferred Cache. The top-K KV caches are selected based on attention scores.",
                "position": 245
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05344/x4.png",
                "caption": "Figure 4:Main Results on Multi-Modal Benchmarks.We evaluate SparseMM and other baselines on several multimodal benchmarks, and conduct experiments on a series of backbones. Our SparseMM consistently outperforms the other baselines.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2506.05344/x5.png",
                "caption": "Figure 5:Results on Multiple-choice Benchmarks.We evaluate SparseMM and other baselines on multiple-choice visual benchmarks with Qwen2-VL-7B-Instruct as the backbone model. Our SparseMM consistently outperforms the other baselines.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2506.05344/x6.png",
                "caption": "Figure 6:Efficiency Evaluation forSparseMM.Benefiting from the reduction in KV cache, SparseMM can maintain nearly constant decoding latency, achieving up to a 50% acceleration. Additionally, it effectively reduces peak memory usage.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2506.05344/x7.png",
                "caption": "Figure 7:Comparisons of MmaskingVisual Headand Random Head.The left figure is the result on OCRBench, and the right figure is the result on TextVQA. When masking only the top 3% of visual heads with the highest scores, the modelâ€™s performance dropped by half, while randomly masking heads resulted in almost no change in performance. This indicates that visual heads constitute a small proportion of all heads while they are crucial for visual information perception.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2506.05344/x8.png",
                "caption": "Figure 8:Visualizations ofVisual Headsusing Different Datasets.We visualized the attention distribution identified on MLT, CTW, and COCO datasets.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2506.05344/x9.png",
                "caption": "Figure 9:Results with Different Visual Head Identification Approaches and Datasets.We conduct an evaluation on visual heads identified on different datasets. The results on OCR datasets are similar and better than those on the detection dataset.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2506.05344/x10.png",
                "caption": "Figure 10:Visualizations ofVisual Heads.We visualized the attention distribution of several heads. The visual heads are able to accurately capture text or objects within the images, whereas the non-visual heads provide random results.",
                "position": 603
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BMore Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05344/x11.png",
                "caption": "Figure 11:More Visualization Results.Visual heads are able to attend to the correct objects, whereas non-visual heads cannot.",
                "position": 661
            }
        ]
    },
    {
        "header": "Appendix CMore Analysis",
        "images": []
    },
    {
        "header": "Appendix DNumerical results",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]