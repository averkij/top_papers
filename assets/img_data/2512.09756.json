[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09756/x1.png",
                "caption": "Figure 1:Flowchart of MOA. Given the inputùê™\\mathbf{q}, we first prompt the policy model to generate rollouts with thoughts, and then mix them with off-policy samples. We then score these rollouts using fine-grained rubrics. Based on the reward trends from these rubrics, we dynamically select a pivot dimension for optimization and allocate weights. Finally, we eliminate conflicting samples that hinder optimization in the pivot dimension.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multi-Objective Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09756/x2.png",
                "caption": "Figure 2:PromptPthinkP_{\\text{think}}to guide models in role-playing tasks.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2512.09756/x3.png",
                "caption": "Figure 3:Performance of Claude-3.7 on PersonaGym, with and without thinking",
                "position": 451
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09756/x4.png",
                "caption": "(a)Smoothed training reward curves of Qwen3-8B-SFT across different dimensions. It can be seen that after using multi-objective optimization, the reward always rises faster. Besides, the starting point of MOA-o is higher. This is because introducing thinking leads to a decline in generation quality. However, the growth of the MOA-o curve slows down in the later stages of training.",
                "position": 1143
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "Appendix AWhy MOA Learns Faster: A Short Proof vs. Weighted GRPO",
        "images": []
    },
    {
        "header": "Appendix BAblation Study on the Design of Multi-Objective Optimization",
        "images": []
    },
    {
        "header": "Appendix CFull Results",
        "images": []
    },
    {
        "header": "Appendix DDetailed Explanation of Evaluation Dimensions",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09756/x4.png",
                "caption": "Figure 6:Reward prompt for Basic Dialogue",
                "position": 3701
            },
            {
                "img": "https://arxiv.org/html/2512.09756/x5.png",
                "caption": "Figure 7:Reward prompt for Persona Knowledge",
                "position": 3704
            },
            {
                "img": "https://arxiv.org/html/2512.09756/x6.png",
                "caption": "Figure 8:Reward prompt for Style Compliance",
                "position": 3707
            }
        ]
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]