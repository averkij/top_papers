[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10600/x1.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10600/x2.png",
                "caption": "Figure 2:The framework ofEmbodiedGen.\nIt enables the creation of a digital twin within a simulation environment from a single image. Alternatively, given a task description,EmbodiedGenautonomously generates the scene layout, synthesizes detailed 3D object assets, and arranges them in semantically and physically plausible configurations. This facilitates the effortless construction of an interactive 3D world, supporting a wide range of embodied intelligence related research in diverse virtual environments.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10600/x3.png",
                "caption": "Figure 3:Overview ofEmbodiedGenImage-to-3D Pipeline. From a single image, the system generates mesh and 3DGS assets, conducts automatic quality inspectioin (aesthetics, segmentation, geometry), and re-generate failed outputs by auto-adjusted settings. A physics expert module restores real-world scale and physical semantics, and the assets are saved in URDF format.",
                "position": 186
            }
        ]
    },
    {
        "header": "3Generative 3D World Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10600/x4.png",
                "caption": "Figure 4:AestheticCheckeris used to evaluate the texture quality of generated assets. Assets displaying richer texture details receiving higher scores.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x5.png",
                "caption": "Figure 5:Examples of segmentation failure cases automatically filtered byImageSegChecker.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x6.png",
                "caption": "Figure 6:Examples of geometric rationality inspection byMeshGeoChecker.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x7.png",
                "caption": "Figure 7:From left to right: the original image, the result of our optimized texture back-projection, and the result using Trellis’s original texture back-projection. Our method effectively mitigates the influence of highlights and shadows on the mesh texture while producing significantly sharper texture details.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x8.png",
                "caption": "Figure 8:Qualitative comparison of Text-to-3D result. The left column shows results generated by our method, while the right column shows results from TRELLIS-text-xlarge. Our method produces significantly higher-quality outputs that better align with the input text descriptions.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x9.png",
                "caption": "Figure 9:EmbodiedGenText-to-3D module for large-scale 3D asset generation. A prompt generator decomposes user requirements into prompts targeting different asset styles. The pipeline proceeds through text-to-image and image-to-3D stages, each equipped with automatic quality inspection and retry mechanisms. The final URDF asset with complete geometry, realistic scale, and physical properties, is persistently stored.",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x10.png",
                "caption": "Figure 10:Visual examples of articulated objects constructed by the LLM agent based workflow.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x11.png",
                "caption": "Figure 11:Overview ofEmbodiedGenTexture Generation Module. Given a mesh and a text prompt, the module generates six-view consistent textures with controllable styles via text, reference image, or both. Geometry-aware conditions (normals, positions, masks) are extracted and injected into a diffusion model via theGeoLiftermodule. The outputs are refined with illumination removal and super-resolution, then back-projected to the mesh as described in Algorithm1.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x12.png",
                "caption": "Figure 12:Blue dots: reference points; red dots: projected correspondences in other views; green lines: matches. The spatial loss is applied to let the latent features of matched points closer, enhancing cross-view alignment.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x13.png",
                "caption": "Figure 13:EmbodiedGentexture generation module effectively adheres to text descriptions, generating high-quality textures with strong spatial and geometric consistency. It also demonstrates robust control over text generation on textures, accurately rendering common Chinese and English text.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x14.png",
                "caption": "Figure 14:Overview ofEmbodiedGen3D Scene Generation. A panorama is generated from a text prompt or input image, guided by a style prompt. After quality assessment via vlm-based selector, a refined mesh and 3DGS[19]are generated by panorama projection, inpainting, and repair. Super-resolution is used to enhance 3DGS appearance details, followed by real scale and alignment adjustments.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x15.png",
                "caption": "Figure 15:Qualitative Comparison With and Without Style Prompts. “w/o style prompt” lacks explicit style guidance, while “Ours” uses style-aware prompting, yielding more coherent textures and better stylistic alignment across scenes.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x16.png",
                "caption": "Figure 16:Qualitative comparison with and without super-resolution. The generated 3D scene show sharper and high-frequency detailed with super-resolution. Zoom in for details.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x17.png",
                "caption": "Figure 17:Qualitative Comparison with WorldGen[51]Our method produces more detailed textures and more complete geometry than WorldGen, under both text and image input settings.",
                "position": 603
            }
        ]
    },
    {
        "header": "4Application",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10600/x18.png",
                "caption": "Figure 18:EmbodiedGenImage-to-3D: large-scale and diverse 3D object asset generation.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x19.png",
                "caption": "Figure 19:EmbodiedGentexture generation module enables rich and flexible visual texture editing.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x20.png",
                "caption": "Figure 20:EmbodiedGenImage-to-3D: Digital twin creation and simulation inRoboSplatterand MuJoCo[42].",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x21.png",
                "caption": "Figure 21:EmbodiedGenImage-to-3D: Real-to-sim closed-loop simulation evaluation of a grasping model in Isaac Lab environment[26].",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x22.png",
                "caption": "Figure 22:Interaction 3D World Generation withEmbodiedGen.EmbodiedGenenables easy construction of diverse interactive 3D worlds for simulating and evaluating dual-arm shoe-grasping tasks in RoboTwin[28].",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2506.10600/x23.png",
                "caption": "Figure 23:EmbodiedGenText-to-3D: Real-to-sim object transfer and quadruped navigation with obstacle avoidance in OpenAI Gym[4].",
                "position": 671
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]