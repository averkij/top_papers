[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14717/x1.png",
                "caption": "Figure 1:TAPIP3D performs long-term 3D point tracking in a persistent 3D world space of 3D feature clouds, which exceeds prior 3D point tracking methods[44,28]operating in camera-dependent UV pixels + Depth (UVD) spaces. We leverage the given / estimated depth map and camera pose from MegaSaM[23]to compute a 3D space where camera motion is cancelled.\nTAPIP3D designs a Local Pair Attention for featurization and iterative motion estimation. The 3D motion trajectories for the sampled dynamic points in 3D XYZ world space are significantly smoother and more linear then UVD space.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.14717/x2.png",
                "caption": "Figure 2:Architecture of our¬†TAPIP3D.The model takes RGB frames and corresponding 3D point maps as input, computes features from the RGB frames, and transfers them to the 3D points, forming a feature cloud for each timestep.\nUsing either provided or estimated camera poses, these feature clouds can be arranged in either world space or camera space.\nWe then apply our Local Pair Attention module (Figure3) to extract correlation features, followed by our 3D Trajectory Transformer, which iteratively updates the estimated trajectories.Top right:Illustration of the difference between 3Dkùëòkitalic_k-NN (used in our approach) and fixed 2D neighborhoods (used in prior works[17,6]).",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2504.14717/x3.png",
                "caption": "Figure 3:Local Pair Attention.Given a 3D query point at a specific timestep, we first identify its local 3D context usingkùëòkitalic_k-NN to form aquery group. Then, within the point cloud at another timestep, we findKùêæKitalic_Knearest 3D neighbors to construct akey group. We apply bi-directional cross-attention between the query and key groups to capture spatio-temporal correspondences.",
                "position": 244
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]