[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/arXiv/figures/keye_teaser.png",
                "caption": "Figure 1:Benchmark Performance of Kwai Keye-VL-1.5.Keye-VL-1.5-8B establishes new state-of-the-art performance among models of similar scale, demonstrating superior results on video-centric benchmarks while maintaining competitive performance on general multimodal and reasoning tasks. Compared to Keye-VL-Preview, this version shows significant improvements across all three evaluation dimensions, validating the effectiveness of our training approach.",
                "position": 114
            }
        ]
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/x1.png",
                "caption": "Figure 2:The Kwai Keye-VL-1.5 model architectureis based on the Qwen3-8B language model and incorporates a vision encoder initialized from the open-source SigLIP. It supports SlowFast video encoding and native dynamic resolution, preserving the original aspect ratio of images by dividing each into a 14x14 patch sequence. A simple MLP layer then maps and merges the visual tokens. The model uses 3D RoPE for unified processing of text, image, and video information",
                "position": 225
            }
        ]
    },
    {
        "header": "Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/x2.png",
                "caption": "Figure 3:A SlowFast video encoding demonstration: the Slow processes a smaller number of frames at higher resolution, while the Fast handles more frames at lower resolution.",
                "position": 260
            }
        ]
    },
    {
        "header": "Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/x3.png",
                "caption": "Figure 4:The Kwai Keye-VL-1.5 pre-training pipeline,featuring a four-stage progressive strategy: Image-Text Matching, ViT-LLM Alignment, Multi-task Pre-training, and Annealing with model merging.",
                "position": 577
            }
        ]
    },
    {
        "header": "Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/x4.png",
                "caption": "Figure 5:Post-Training Pipeline:The post-training process includes non-reasoning stage and reasoning stage. The non-reasoning stage is composed of SFT and MPO training. The reasoning stage consists of three key steps: CoT Cold Start (we construct a five-step construction pipeline to generate high-quality CoT Cold-Start Dataset and apply model merging to refine model performance), General RL (we concentrate on improving Keye-VL-1.5’s reasoning ability, applying GSPO, we propose progressive hint sampling to fully take advantage of hard problems and iteratively improve the cold-start and general RL model), and Alignment RL (improving Keye-VL-1.5’s instruction following, format adherence, preference alignment and RAG ability with our reward system, we construct instruction following data, reasoning data and RAG data for RL training in this step).",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2509.01563/x5.png",
                "caption": "Figure 6:Overview of our five-step automated LongCoT data generation pipeline.The pipeline begins with (a) sampling from data and prompt pools using MLLMs to generate thinking processes and logit information, followed by (b) quality assessment using MLLM as judge to evaluate both outcomes and reasoning processes with step-wise scoring, (c) categorization into three quality tiers (A: high quality, B: middle quality requiring human review, C: low quality to be discarded), (d) human augmentation for Category B samples and suspected redundant Category A samples, and (e) final MLLM review with dynamic quality scoring (1-5 scale) to determine optimal data utilization strategies. This comprehensive approach ensures both scalability and quality control in generating training data.",
                "position": 689
            }
        ]
    },
    {
        "header": "Training Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/x6.png",
                "caption": "(a)Frames.",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2509.01563/x6.png",
                "caption": "(a)Frames.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2509.01563/x7.png",
                "caption": "(b)FPS.",
                "position": 1083
            }
        ]
    },
    {
        "header": "Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/x8.png",
                "caption": "Figure 8:Benefits of rejection sampling in the RL Phase.Starting from Keye-VL-8B-Preview, we compare the performance of direct RL and RFT-RL strategies.",
                "position": 2184
            }
        ]
    },
    {
        "header": "Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01563/arXiv/cases/figs/case1.5.png",
                "caption": "Figure 9:In this 26-second video, the creator provides a detailed introduction to the performance, advantages, and features of the power bank. The handbag only appears for about two seconds in the last of the video. Keye-VL-1.5 accurately pinpoints this time range, with a precision of 0.1 seconds.",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2509.01563/arXiv/cases/figs/case2.5.png",
                "caption": "Figure 10:In this video, the caption shows that the dog dad prevents the little dog from eating the food on the dining table by holding the little dog down. The question asks about the reason behind the big dog biting the little dog’s ear. Keye-VL-1.5 accurately correlates the video content and the caption, determining that the purpose of the big dog biting the little dog’s ear is the same as holding the little dog down – both are meant to teach the little dog not to eat the food on the table.",
                "position": 3443
            },
            {
                "img": "https://arxiv.org/html/2509.01563/arXiv/cases/figs/case3.5.png",
                "caption": "Figure 11:This video describes a very beautiful yet rare phenomenon: hail falling in the forest. Without prior knowledge, even human viewers could easily mistake the falling hail for petals or raindrops. Although Keye-VL-1.5 fails to identify the falling objects as hail, it mentions precipitation, and the rest of the content is correctly identified.",
                "position": 3471
            }
        ]
    },
    {
        "header": "Appendix BAuthors (Alphabetical order)",
        "images": []
    }
]