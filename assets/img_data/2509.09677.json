[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09677/x1.png",
                "caption": "Figure 1:A Summary of our contributions.We note that diminishing returns can enable exponential gains in horizon length (that is the length of tasks a model can complete). We design a simple task that isolates the capability of long-horizon execution in language models, ablating the need for knowledge and planning. We find that frontier models benefit considerably from both scaling model size and test-time compute when executing long horizon tasks.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2509.09677/figs/math_plot.png",
                "caption": "Figure 2:Growth of Horizon Length.The length of task a model can perform at more than 50% accuracy grows faster than exponential as a function of the step accuracy after the 70% mark.",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x2.png",
                "caption": "Figure 3:Overview of our framework.(Left) Our framework models long-horizon tasks as a sequence ofretrieve-then-composesteps. (Right) We design a simple task that decouples planning from execution: in each turn, we provide the model the plan as key(s), asking it toretrievetheir value(s), andcomposethem to maintain a running sum.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x3.png",
                "caption": "Figure 4:Scaling model size non-diminishingly improves the number of turns it can execute.We vary the model size and study both full task (a) and turn-wise accuracy (b) as the number of turns increases. Bold lines are a running average of accuracy over 5 turns. The dotted lines (turn-wise accuracy) in (b) show single-step accuracy for our task is 100% for all except the smallest models. Yet, as the number of turns increases, the performance gap between small and large models widens (a), with the latter having significantly more horizon length (c).",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x4.png",
                "caption": "Figure 5:Language models self-condition on their previous mistakes, leading to more mistakes in subsequent turns.By manipulating the chat history, we counterfactually vary the fraction of errors in previous turns. We find this increases the likelihood of errors in future turns (left). This shows a source of degradation in turn-wise model accuracy beyond long-context, as in the turn 100 slice (right) model accuracies are much higher when we provide a fully correct history. Scaling model size increases self-conditioning, even for frontier non-thinking models.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x5.png",
                "caption": "Figure 6:Thinking fixes self-conditioning.Qwen3 models with thinking enabled no longer self-condition, even when the entire prior history has wrong answers, in contrast to non-thinking results.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x6.png",
                "caption": "Figure 7:Benchmarking the length of task models can execute in a single turn.Without CoT or thinking, even the biggest models to fail to execute more than a few steps (left). Sequential test time compute (thinking tokens) significantly improve this, especially when trained with RL (eg R1 vs DeepSeek V3) (right). GPT-5 is far ahead of the rest, executing over 1000 steps, with Claude-4-Sonnet second at around 400.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x7.png",
                "caption": "Figure 8:Self-verification prompting. Prompting to self-verify does not suffice to fix the self-conditioning effect completely. It leads to overthinking in thinking models and increases the amount of tokens required per turn, leading to faster context consumption in CoT models.",
                "position": 1389
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x8.png",
                "caption": "(a)Context Management",
                "position": 1402
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x8.png",
                "caption": "(a)Context Management",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x9.png",
                "caption": "(b)Majority Voting",
                "position": 1410
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x10.png",
                "caption": "Figure 10:Parallel test time scaling on Gemma3 12B at K=2.Majority voting with the same amount of tokens as CoT traces does not nearly match the performance with CoT.",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x11.png",
                "caption": "(a)For the same number of total steps, different turn complexities lead to different outcomes. We find no trend across families.",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x11.png",
                "caption": "(a)For the same number of total steps, different turn complexities lead to different outcomes. We find no trend across families.",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x12.png",
                "caption": "(b)Average output tokens used to complete the execution vs final accuracy. We see that for Qwen3 32B, more turns lead to more token usage, even at lower turn complexities, pointing to overthinking. Gemma3 12B, on the other hand, uses less tokens for very low turn complexity or very high turn complexity.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x13.png",
                "caption": "Figure 12:Scaling trends hold even enabling Sequential Test Time compute.We compare model performance with thinking disabled (a) against thinking enabled (b, c) at varying turn complexities. (a) Without thinking, all models fail to execute even two steps (K=2K=2) in a single turn. (b) In contrast, enabling thinking prevents this performance collapse, with all models successfully handlingK=2K=2. (c) When the turn complexity is further increased toK=10K=10, performance degrades, but a clear scaling trend emerges. (d) This trend is explicitly shown, illustrating that for complex turns, the horizon length increases consistently with model size, reinforcing the benefits of scaling model size even when thinking is enabled.",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x14.png",
                "caption": "Figure 13:Temperature does not impact the trends observed.We reproduce the same trends inFigure˜4, when running with temperature 0.",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x15.png",
                "caption": "Figure 14:Analysis of execution failures.(a) Self-conditioning effect emerges as tasks get longer. Even for models that ace the task at a task length of 100, the Turn Accuracy drops constantly as we further increase the turns. (b) Models are good at the tasks individually, but not on their composition. State tracking introduces additional difficulty.",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x15.png",
                "caption": "",
                "position": 1496
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x16.png",
                "caption": "",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2509.09677/x17.png",
                "caption": "Figure 15:Analysis of format following failures.We analyze the fraction of errors attributed to incorrect format following for the experiments presented inSection˜3. Overall, format adherence is high and not the primary source of execution errors.",
                "position": 1595
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]