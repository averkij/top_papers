[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_intro_results.png",
                "caption": "Figure 1:Illustration of (a) GT depth, noisy ToF depth in (b) current frame and (c) previous frame, (d) single-frame GLRUN[17]where noise remains, (e) multi-frame MTDNet[9]fusingdepth features, (f) proposed GIGA-ToF fusinggraph structures.\nDue to depth shifts at corresponding pixels in red rectangles, (e) loses details, while (f) removes noise while preserving sharpness because the neighborhood correlation graphs are motion-invariant.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3ToF Imaging Mechanism Overview",
        "images": []
    },
    {
        "header": "4Problem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_demo.png",
                "caption": "Figure 2:Illustration of cross-frame graph fusion, where the intra-frame graph in reference framet‚àí1ùë°1t-1italic_t - 1is mapped to current frametùë°titalic_tvia inter-frame graph with 2-hop or 3-hop paths connecting pixel pairs in frametùë°titalic_t.\nThe graph weights are learned in graph-informed geometric attention (GIGA) mechanism, which updates the adjacency matrix in current frame using that in reference frame.",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_arch.png",
                "caption": "Figure 3:Framework of GIGA-ToF network which is composed of (1) the feature extraction network in blue to extract geometric features from ToF raw data and estimate initial prior weights and intra-graph adjacency matrices, (2) Graph-Induced Geometric Attention (GIGA) module in yellow to learn graph edges from the geometric features informed by graph structures as shown at the right side, and (3) Unrolled GLR module in green to denoise ToF data. Output dimensions are shown on top of each layer.",
                "position": 416
            }
        ]
    },
    {
        "header": "5Network Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp1.png",
                "caption": "Figure 4:Depth results and error maps of ToF depth denoised on DvToF dataset: (a) GT, results of (b) WMF[23], (c) DVSR[35], (d) MTDNet[9]and (e) proposed GIGA-ToF. Corresponding error maps are in the second row.",
                "position": 821
            }
        ]
    },
    {
        "header": "6Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_ablation.png",
                "caption": "Figure 5:Comparison of denoising results under different GIGA-ToF variants. (a) GT, results (b) without and (c) with unrolled GLR in single-frame processing, feature fusion (d) without and (e) with attention; graph structure fusion (f) without and (g) with attention.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp-t.png",
                "caption": "Figure 6:x-t slices (along red line in Fig.4(a)) for temporal stability visualization, where GIGA-ToF exhibits clear details and less noise than competing multi-frame schemes.",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp-kinect.png",
                "caption": "Figure 7:Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF shows robustness to real noise and recovers accurate details.",
                "position": 911
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Data Fidelity Term in MAP Problem",
        "images": []
    },
    {
        "header": "9Analysis of Frame Time Step",
        "images": []
    },
    {
        "header": "10Algorithm Summary",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp-kinect_supp1.png",
                "caption": "Figure 8:Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF shows accurate and smooth estimation.",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp-kinect_supp2.png",
                "caption": "Figure 9:Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF shows robustness to real noise and recovers accurate details.",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp-kinect_supp3.png",
                "caption": "Figure 10:Visual results of ToF depth denoising on real data captured by Kinect v2 sensor: (a) RGB and (b) noisy depth captured by Kinectv2 camera, and results of (c) GLRUN, (d) WMF, (e) MTDNet and (f) GIGA-ToF, where GIGA-ToF exhibits better spatial sharpness than other competing schemes.",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp1_supp.png",
                "caption": "Figure 11:Depth results and error maps of ToF depth denoised on DvToF dataset: (a) GT, results of (b) WMF[23], (c) DVSR[35], (d) MTDNet[9]and (e) proposed GIGA-ToF. Corresponding error maps are in the second row.\nGIGA-ToF shows more accurate depth estimation, maintaining global smoothness with edge preservation,e.g., the teapot handle highlighted in the zoom-in block.",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp2_supp.png",
                "caption": "Figure 12:Depth results and error maps of ToF depth denoised on DvToF dataset: (a) GT, results of (b) WMF[23], (c) DVSR[35], (d) MTDNet[9]and (e) proposed GIGA-ToF. Corresponding error maps are in the second row.\nWhile MTDNet shows competing results, the details are blurred as highlighted in the zoom-in block, while GIGA-ToF generates sharp edges due to utilization of motion-invariant graph structure fusion.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2506.23542/extracted/6581851/figs/fig_exp2.png",
                "caption": "Figure 13:Depth results and error maps of ToF depth denoised on DvToF dataset with augmented edge noise: (a) GT, results of (b) WMF[23], (c) DVSR[35], (d) MTDNet[9]and (e) proposed GIGA-ToF. Corresponding error maps are in the second row. GIGA-ToF shows strong generalization to unseen edge noise and generates accurate details,e.g., in the bookshelf in the zoom-in block.",
                "position": 1897
            }
        ]
    },
    {
        "header": "11More Visualization",
        "images": []
    }
]