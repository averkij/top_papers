[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09858/x1.png",
                "caption": "Figure 1:RLDG improves generalist robot policies like OpenVLA and Octo by training with specialist RL policies and using them to generate high-quality fine-tuning datasets. It has the flexibility to distill knowledge from multiple RL policies trained on individual narrowly scoped tasks into a single generalist. It can also be applied to the most critical sub-task of a long-horizon manipulation task, improving the success rate at the ‚Äúbottleneck\" while leveraging human demonstrations on parts of the task where it suffices.",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Reinforcement Learning Distilled Generalist",
        "images": []
    },
    {
        "header": "4Experiment and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09858/x2.png",
                "caption": "Figure 2:We use a Franka Emika Panda arm with a parallel jaw gripper teleoperated by a 3Dconnexion SpaceMouse device. There is a single RealSense D405 camera mounted on the robot‚Äôs wrist for image observations.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2412.09858/x3.png",
                "caption": "Figure 3:Illustrations of tasks used to evaluate RLDG. (A) PreciseConnector Insertionincludes three training objects and four unseen test objects for evaluating policy generalization. (B)Pick and Placeinvolves an unseen scenario that tests the policy‚Äôs visual robustness to different backgrounds and objects. (C)FMB Insertioninvolves inserting a pre-grasped object in a moving board while (D)FMB Assemblystarts with the object on the table and involves an additional grasping phase.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/success_rate_comparison_ood.png",
                "caption": "Figure 4:Success rate comparison of OpenVLA and Octo policies fine-tuned with RLDG versus conventional methods using human demonstrations.\nBoth generalists trained with RLDG consistently outperform their counterparts trained with the same number of successful expert human demonstrations in both training and unseen scenarios.",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/success_rate_vs_demos.png",
                "caption": "Figure 5:Success rate of OpenVLA policies fine-tuned on different sizes of RL-generated and human-collected datasets. When evaluated on seen (VGA) and unseen (Type C)Connector Insertiontasks, RLDG shows superior sample efficiency, requiring significantly fewer demonstrations to achieve perfect success rate in both scenarios while the performance of conventional method saturates in the unseen case.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/cycle_time_comparison_ood.png",
                "caption": "Figure 6:Cycle time comparison between policies trained with RL data versus human demonstrations. N/A for RL inFMB Assemblydenotes policy not trained on the whole task, while N/A for fine-tuned policies denotes no successes recorded. The RL-trained policies generally achieve faster execution times across tasks, demonstrating the efficiency benefits of using RL-generated data for policy training.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/fmb_scaling.png",
                "caption": "Figure 7:Fine-tuning success rate on theFMB insertiontask with different fine-tuning data sources and varied dataset sizes (from 25 trajectories to 300 trajectories).Human: demo trajectories collected by human teleoperators.Human + RL actions: the same human demo trajectories but with all the actions relabeled by a trained RL agent.RL: rollouts collected by the RL agent. RL data consistently provide better fine-tuning performance than human data.Human + RL actionscloses the gap mostly, suggesting that most of the benefits of RL data come from it having better action quality.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2412.09858/extracted/6066371/figures/multimodality.png",
                "caption": "Figure 8:Action distribution visualization for RL data and human demo data for theFMB insertiontask.We visualize the first two dimensions of the dataset actions after filtering all the transitions in the dataset where the end-effector positions are close to the position shown in the image on the left (xùë•xitalic_x/yùë¶yitalic_ycoordinates are both within4444mm andzùëßzitalic_zcoordinate is within10101010mm). The robot arm needs to move in the -xùë•xitalic_xdirection and in the -yùë¶yitalic_ydirection to reach the insertion point. The first two dimensions of the action space corresponds to the control of thexùë•xitalic_xandyùë¶yitalic_yposition of the end-effector position correspondingly. Human actions are clustered around the center of the action space whereas the RL actions are more optimized, and mostly found near the correct corner (bottom-left) of the action space.",
                "position": 377
            }
        ]
    },
    {
        "header": "5Analysis: why is RL data better than human data?",
        "images": []
    },
    {
        "header": "6Discussion and Limitations",
        "images": []
    },
    {
        "header": "Author Contribution",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask Details",
        "images": []
    },
    {
        "header": "Appendix BTraining Procedures",
        "images": []
    }
]