[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20294/x1.png",
                "caption": "",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20294/x2.png",
                "caption": "Figure 2:Despite being trained on complex indoor scenes with 10 rooms, classic RL-based methods, ANS[8]and OccAnt[35]exhibit limited generalizability when tested on simple but structurally distinct scenes with 5 rooms.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20294/x3.png",
                "caption": "Table 2:The description of the mentioned dataset in Table 1. Note that only a few high-quality scenes from these datasets are used by previous active mapping algorithms.NS: navigable space,FS: floor space,NC: navigation complexity,SC: scene clutter. “-”: challenging to access without annotation or statistical documentation.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2505.20294/x3.png",
                "caption": "Figure 3:The distribution of 1,152 scenes by the number of rooms in our benchmark GLEAM-Bench.",
                "position": 314
            }
        ]
    },
    {
        "header": "3GLEAM-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20294/x4.png",
                "caption": "Figure 4:The overview of our framework. Trained on 1,024 diverse indoor scenes, GLEAM processes depth observations and agents’ poses to iteratively update a global map. An egocentric map is extracted and augmented with exploration frontiers to capture semantic exploration cues. A lightweight Transformer encoder then analyzes the egocentric map and trajectory history to predict the long-term goals. The reward function of coverage is computed by the global map and ground-truth occupancy map.",
                "position": 334
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20294/x5.png",
                "caption": "Figure 5:Ablation studies of the number of training scenes. We sample a proportionally reduced number of scenes from the complete 1024 training scenes to demonstrate the significance of the quantity and diversity of training scenes.",
                "position": 674
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20294/x6.png",
                "caption": "Figure 6:The visualization results of ANM, OccAnt, and GLEAM on three unseen complex indoor scenes from the test set of GLEAM-Bench. The methods share the same random initial poses for each scene.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2505.20294/x7.png",
                "caption": "Figure 7:The one-step inference time of our key components.",
                "position": 910
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CTraining Strategy",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]