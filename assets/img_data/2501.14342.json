[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14342/x1.png",
                "caption": "(a)Test-time scaling behavior of CoRAG.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2501.14342/x1.png",
                "caption": "(a)Test-time scaling behavior of CoRAG.",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2501.14342/x2.png",
                "caption": "(b)An example of CoRAG in action.",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14342/x3.png",
                "caption": "Figure 1:Overview of CoRAG.\nRejection sampling is utilized to augment QA-only datasets with retrieval chains.\nEach chain starts with the original query,\nfollowed by a sequence of sub-queries and sub-answers.\nAn open-source LLM is then fine-tuned to predict the next action based on the current state.\nDuring inference,\nmultiple decoding strategies are available to control the test-time compute.",
                "position": 256
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14342/x4.png",
                "caption": "Figure 2:Scaling test-time compute on multi-hop QA datasets.\nThe Pareto frontier is in the form ofy=a√ólog‚Å°(x+b)+cùë¶ùëéùë•ùëèùëêy=a\\times\\log(x+b)+citalic_y = italic_a √ó roman_log ( italic_x + italic_b ) + italic_cfitted on the Pareto optimal points.\nA point is consideredPareto optimalif no other point achieves a higher EM score with less token consumption.\nThe metric ‚Äú# Avg. Tokens‚Äù represents the average number of tokens consumed per test instance,\nsumming up both the prompt and generated tokens.",
                "position": 806
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14342/x5.png",
                "caption": "Figure 3:Scaling test-time compute across three datasets from the KILT benchmark.\nWe report scores on the public validation set.",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2501.14342/x6.png",
                "caption": "Figure 4:Learning to stop at test time.\nLarger logit bias values result in earlier stopping.L=6ùêø6L=6italic_L = 6correspond to always performing6666retrieval steps,\nwhileL=0ùêø0L=0italic_L = 0indicate no intermediate retrieval steps.",
                "position": 1050
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14342/x7.png",
                "caption": "Figure 5:Scaling rejection sampling compute for training data generation.\nWe vary the number of sampled chains from4444to16161616while maintaining all other hyperparameters fixed.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2501.14342/x8.png",
                "caption": "Figure 6:Effects of varying the sampling temperature on multi-hop QA datasets.",
                "position": 2106
            }
        ]
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    }
]