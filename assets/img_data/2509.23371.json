[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23371/x1.png",
                "caption": "Figure 1:Left:Overview of MetaAPO. MetaAPO employs a meta-learner to couple online data generation (top) and model training (bottom). The meta-learner adaptively assigns weights by evaluating offline data, guiding both targeted online sample generation and training on the weighted combination of offline and online samples.Right:Performance (left y-axis, line plots) and online generation and annotation ratio relative to Online DPO (right y-axis, bar plots) across training iterations for different methods.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23371/x2.png",
                "caption": "Table 1:Overall performance of our proposed MetaAPO method with Llama-3.1-8B and Qwen2.5-7B, compared with offline, online and hybrid baseline methods on AlpacaEval 2, Arena-Hard and MT-Bench. The best results are highlighted inbold.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2509.23371/x2.png",
                "caption": "Figure 2:Left:The dynamic changes in Offline Score (left y-axis) and Reward (right y-axis) across training iterations.Middle:Scatter plot of offline preference score (ℓoff​(⋅)\\ell_{\\text{off}}(\\cdot)) versus the online-offline score gap (ℓon​(⋅)−ℓoff​(⋅)\\ell_{\\text{on}}(\\cdot)-\\ell_{\\text{off}}(\\cdot)). Points are colored by their sampling status:bluefor“Sampled”(selected for online generation) andorangefor“Unsampled”.Right:Comparison of independent reward score distributions (via kernel density estimation) for testset responses generated by DPO and MetaAPO.",
                "position": 738
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMathematical Derivations and Formulations",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23371/x3.png",
                "caption": "Table 3:Hyperparameters for Llama-3.1-8B and Qwen2.5-7B during generation and training.",
                "position": 1862
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23371/x3.png",
                "caption": "Table 5:Overall performance of our proposed MetaAPO method with Llama-3.1-8B and Qwen2.5-7B, compared with offline and online baseline methods on AlpacaEval 2, Arena-Hard and MT-Bench with the reward model OpenRLHF/Llama-3-8b-rm-mixture. The best and second-best results are highlighted inboldandunderline.",
                "position": 2076
            },
            {
                "img": "https://arxiv.org/html/2509.23371/x3.png",
                "caption": "Figure 3:Left: Impact of meta-Learner update interval (TmetaT_{\\text{meta}}) on MetaAPO performance. Performance (AlpacaEval 2 WR/LC (%)) is shown for differentTmetaT_{\\text{meta}}values.Right: Meta-Learner input-output relationship across training iterations.",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]