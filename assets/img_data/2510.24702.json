[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24702/Figures/logo/adp_logo.png",
                "caption": "",
                "position": 119
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24702/x1.png",
                "caption": "Figure 1:Overview of the Agent Data Protocol (ADP). Raw data from diverse sources such as AgentInstruct, CodeActInstruct, SWE-Gym, and Mind2Web are converted into a standardized ADP format. ADP unifies data into Trajectory objects, which include two core components: Actions (API action, code action, message action) and Observations (text observation, web observation). This standardized representation enables seamless integration with various agent SFT pipelines. Example transformations demonstrate how heterogeneous raw data is normalized for training agentic models.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Agent Data Protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24702/x2.png",
                "caption": "Figure 2:ADP collapses many-to-many conversions into a hub-and-spoke pipeline.Left:Without ADP, each ofDD-many datasets needs a custom Raw→\\rightarrowSFT converter for each ofAA-many agentic formats (quadraticO​(D×A)O(D\\times A)effort), causing duplicated code and efforts.Right:With ADP, each dataset is converted once (Raw→\\rightarrowADP) and each agent only requires one converter (ADP→\\rightarrowSFT), yielding linearO​(D+A)O(D{+}A)effort. New datasets or agents plug in immediately to the rest of ADP.",
                "position": 748
            }
        ]
    },
    {
        "header": "4Cross Dataset Analysis",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24702/x3.png",
                "caption": "Table 3:Comparison of SOTA and our Best 7–8B ADP-trained agents’ results across benchmarks. Shaded rows are our ADP-tuned models.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2510.24702/x3.png",
                "caption": "Table 4:Comparison of SOTA and our Best 13–14B ADP-trained agents’ results across benchmarks. Shaded rows are our ADP-tuned models.",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2510.24702/x3.png",
                "caption": "Table 5:Comparison of SOTA and our Best 32B ADP-trained agents’ results across benchmarks. Shaded rows are our ADP-tuned models.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2510.24702/x3.png",
                "caption": "Figure 3:Performance Scaling Across Agents and Benchmarks (Base vs ADP Trained)",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2510.24702/x3.png",
                "caption": "Figure 3:Performance Scaling Across Agents and Benchmarks (Base vs ADP Trained)",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2510.24702/x4.png",
                "caption": "Figure 4:Performance Gains Across Agents and Benchmarks.",
                "position": 1545
            }
        ]
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Reproducibility Statement.",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUse of LLMs",
        "images": []
    },
    {
        "header": "Appendix BData Sampling for Balanced Training",
        "images": []
    }
]