[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03065/x1.png",
                "caption": "Figure 1:The architecture of vDiT and inference latency analysis of its two variants, CogVideoX1.5 and HunyuanVideo, across different components. The latency of the attention module dominates under long sequence settings, and its proportion increases as the sequence length grows.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03065/x2.png",
                "caption": "Figure 2:Visualization of the vDiT attention map showing four interaction regions. The dominant V-V region has diagonal blocks for self-frame and off-diagonal blocks for cross-frame interactions.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x3.png",
                "caption": "Figure 3:Visualization of the four recurring attention patterns in vDiT.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x4.png",
                "caption": "Figure 4:t-SNE visualization of attention patterns along the head dimension on a VBench subset, with different layers indicated by distinct colors. Patterns from different prompts exhibit clustering.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x5.png",
                "caption": "Figure 5:Overview of the Sparse-vDiT.We first predefine five types of attention modeM0:4subscriptùëÄ:04M_{0:4}italic_M start_POSTSUBSCRIPT 0 : 4 end_POSTSUBSCRIPT. Then, using an offline sparse diffusion search algorithm, we select the best attention mode for each layer and head in vDiT. After the search, for heads set to skip attention, we set their outputs to zero. For the three sparse attention patterns, we create specialized sparse attention kernels to speed up computation. Finally, heads within the same layer that use the same attention mode are fused to improve efficiency.",
                "position": 273
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03065/x6.png",
                "caption": "Figure 6:Visual comparison between the proposed Sparse-vDiT and the baseline method. The green box indicates the ground truth. Yellow boxes highlight differences in blurriness and smoothness. White boxes highlight differences in fine details, while red boxes emphasize contour comparisons.",
                "position": 635
            }
        ]
    },
    {
        "header": "6Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlgorithm Implementation",
        "images": []
    },
    {
        "header": "Appendix BPerformance on more pretrain models",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03065/x7.png",
                "caption": "Figure 7:More visual comparison between the proposed Sparse-vDiT and the baseline method. Our method maximizes computational speedup while maintaining high fidelity to the pretrain model.",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x8.png",
                "caption": "Figure 8:More visual comparison between the proposed Sparse-vDiT and the baseline method. Our method maximizes computational speedup while maintaining high fidelity to the pretrain model.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x9.png",
                "caption": "Figure 9:More visual comparison between the proposed Sparse-vDiT and the baseline method. Our method maximizes computational speedup while maintaining high fidelity to the pretrain model.",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x10.png",
                "caption": "Figure 10:More visual comparison between the proposed Sparse-vDiT and the pretrain model. Beyond demonstrating superior performance in frame generation quality, our method exhibits robust capabilities in maintaining inter-frame consistency.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2506.03065/x11.png",
                "caption": "Figure 11:More visual comparison between the proposed Sparse-vDiT and the pretrain model. Beyond demonstrating superior performance in frame generation quality, our method exhibits robust capabilities in maintaining inter-frame consistency.",
                "position": 1693
            }
        ]
    },
    {
        "header": "Appendix CMore visual results",
        "images": []
    }
]