[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x1.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x2.png",
                "caption": "Figure 2:The core idea ofMimir.Text Encoder is well suited for fine-tuning pre-trained T2V models (✓), however it struggles with limited text comprehension (✗).\nIn contrast, Decoder-only LLM excels at precise text understanding (✓), but cannot be directly used in established video generation models since the feature distribution gap and the feature volatility (✗) .\nTherefore, we propose the token fuser inMimirto harmonize multiple tokens, achieving precise text understanding (✓) in T2V generation (✓).",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x3.png",
                "caption": "Figure 3:The framework ofMimir. Given a text prompt, we employ a text encoder and a decoder-only large language model to obtaineθsubscript𝑒𝜃e_{\\theta}italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTandeβsubscript𝑒𝛽e_{\\beta}italic_e start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT. Additionally, we add an instruction prompt which, after processing by the decoder-only model, yields the corresponding instruction tokeneisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. See token details in Sec.2.2. To prevent any convergence issue in training caused by the feature distribution gap ofeθsubscript𝑒𝜃e_{\\theta}italic_e start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTandeβsubscript𝑒𝛽e_{\\beta}italic_e start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT, the proposed token fuser first applies a normalization layer and a learnable scale toeβsubscript𝑒𝛽e_{\\beta}italic_e start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT. It then uses Zero-Conv to preserve the original semantic space in the early of training. These modified tokens are then summed to producee∈ℝn×4096𝑒superscriptℝ𝑛4096e\\in\\mathbb{R}^{n\\times 4096}italic_e ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × 4096 end_POSTSUPERSCRIPT. Meanwhile, we initialize four learnable tokenselsubscript𝑒𝑙e_{l}italic_e start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, which are added toeisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTto stabilize divergent semantic features. Finally, the token fuser concatenatese𝑒eitalic_eandessubscript𝑒𝑠e_{s}italic_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTto generate videos.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x4.png",
                "caption": "Figure 4:Comparison between CogVideoX-5B withMimirin T2V, whereMimirgenerates the vivid stunning moment of rocket launch.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x5.png",
                "caption": "Figure 5:Mimirdemonstrates spatial comprehension and imagination,e.g., quantities, spatial relationships, colors, etc.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x6.png",
                "caption": "Figure 6:Mimirdemonstrates temporal comprehension and imagination,e.g., direction, order of motion and appearance / disappearance.",
                "position": 391
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x7.png",
                "caption": "Figure 7:Visualization by t-SNE:(a) Given 50 prompts, we obtain the corresponding tokens using Encoder branch, Decoder-only branch and their sum, i.e.,Mimir. (b) We feed one prompt into Decoder-only branch for 50 times to generate 50 query tokens, answer tokens and final tokens.Differences in feature distribution:(c) The original distribution of T5 encoder and Phi-3.5 Decoder. (d) The distribution of T5 encoder and Phi-3.5 Decoder after normalization across different value ranges.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x8.png",
                "caption": "Figure 8:We present more cases generated byMimir.",
                "position": 545
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x9.png",
                "caption": "Figure 9:The pipeline for preparing data.",
                "position": 1567
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Metric",
        "images": []
    },
    {
        "header": "Appendix CUser Study",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x10.png",
                "caption": "Figure 10:The comparison between results with short & course prompts and long & fine prompts.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x11.png",
                "caption": "Figure 11:More examples in terms of color rendering.",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x12.png",
                "caption": "Figure 12:More examples in terms of absolute & relative position.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x13.png",
                "caption": "Figure 13:More examples in terms of counting.",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x14.png",
                "caption": "Figure 14:More examples in terms of action sequence over time.",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x15.png",
                "caption": "Figure 15:More examples in terms of light changes, showcasing the illumination harmonization over time.",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x16.png",
                "caption": "Figure 16:More examples in terms of object transformation over time.",
                "position": 1885
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FSocial Impact",
        "images": []
    }
]