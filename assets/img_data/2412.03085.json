[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x1.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x2.png",
                "caption": "Figure 2:The core idea ofMimir.Text Encoder is well suited for fine-tuning pre-trained T2V modelsÂ (âœ“), however it struggles with limited text comprehensionÂ (âœ—).\nIn contrast, Decoder-only LLM excels at precise text understandingÂ (âœ“), but cannot be directly used in established video generation models since the feature distribution gap and the feature volatilityÂ (âœ—) .\nTherefore, we propose the token fuser inMimirto harmonize multiple tokens, achieving precise text understandingÂ (âœ“) in T2V generationÂ (âœ“).",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x3.png",
                "caption": "Figure 3:The framework ofMimir. Given a text prompt, we employ a text encoder and a decoder-only large language model to obtaineÎ¸subscriptğ‘’ğœƒe_{\\theta}italic_e start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTandeÎ²subscriptğ‘’ğ›½e_{\\beta}italic_e start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT. Additionally, we add an instruction prompt which, after processing by the decoder-only model, yields the corresponding instruction tokeneisubscriptğ‘’ğ‘–e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. See token details in Sec.2.2. To prevent any convergence issue in training caused by the feature distribution gap ofeÎ¸subscriptğ‘’ğœƒe_{\\theta}italic_e start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTandeÎ²subscriptğ‘’ğ›½e_{\\beta}italic_e start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT, the proposed token fuser first applies a normalization layer and a learnable scale toeÎ²subscriptğ‘’ğ›½e_{\\beta}italic_e start_POSTSUBSCRIPT italic_Î² end_POSTSUBSCRIPT. It then uses Zero-Conv to preserve the original semantic space in the early of training. These modified tokens are then summed to produceeâˆˆâ„nÃ—4096ğ‘’superscriptâ„ğ‘›4096e\\in\\mathbb{R}^{n\\times 4096}italic_e âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— 4096 end_POSTSUPERSCRIPT. Meanwhile, we initialize four learnable tokenselsubscriptğ‘’ğ‘™e_{l}italic_e start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, which are added toeisubscriptğ‘’ğ‘–e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTto stabilize divergent semantic features. Finally, the token fuser concatenateseğ‘’eitalic_eandessubscriptğ‘’ğ‘ e_{s}italic_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTto generate videos.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x4.png",
                "caption": "Figure 4:Comparison between CogVideoX-5B withMimirin T2V, whereMimirgenerates the vivid stunning moment of rocket launch.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x5.png",
                "caption": "Figure 5:Mimirdemonstrates spatial comprehension and imagination,e.g., quantities, spatial relationships, colors, etc.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x6.png",
                "caption": "Figure 6:Mimirdemonstrates temporal comprehension and imagination,e.g., direction, order of motion and appearance / disappearance.",
                "position": 391
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x7.png",
                "caption": "Figure 7:Visualization by t-SNE:(a) Given 50 prompts, we obtain the corresponding tokens using Encoder branch, Decoder-only branch and their sum, i.e.,Mimir. (b) We feed one prompt into Decoder-only branch for 50 times to generate 50 query tokens, answer tokens and final tokens.Differences in feature distribution:(c) The original distribution of T5 encoder and Phi-3.5 Decoder. (d) The distribution of T5 encoder and Phi-3.5 Decoder after normalization across different value ranges.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x8.png",
                "caption": "Figure 8:We present more cases generated byMimir.",
                "position": 545
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x9.png",
                "caption": "Figure 9:The pipeline for preparing data.",
                "position": 1567
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Metric",
        "images": []
    },
    {
        "header": "Appendix CUser Study",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03085/x10.png",
                "caption": "Figure 10:The comparison between results with short & course prompts and long & fine prompts.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x11.png",
                "caption": "Figure 11:More examples in terms of color rendering.",
                "position": 1838
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x12.png",
                "caption": "Figure 12:More examples in terms of absolute & relative position.",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x13.png",
                "caption": "Figure 13:More examples in terms of counting.",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x14.png",
                "caption": "Figure 14:More examples in terms of action sequence over time.",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x15.png",
                "caption": "Figure 15:More examples in terms of light changes, showcasing the illumination harmonization over time.",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2412.03085/x16.png",
                "caption": "Figure 16:More examples in terms of object transformation over time.",
                "position": 1885
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FSocial Impact",
        "images": []
    }
]