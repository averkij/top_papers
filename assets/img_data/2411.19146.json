[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19146/x1.png",
                "caption": "Figure 1:An overview of the three stages of our Puzzle framework.",
                "position": 244
            }
        ]
    },
    {
        "header": "2Search Space",
        "images": []
    },
    {
        "header": "3Blockwise Local Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19146/x2.png",
                "caption": "Figure 2:Blockwise local distillation (BLD): each block is trained in parallel and independently.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2411.19146/x3.png",
                "caption": "Figure 3:Coupled BLD requires training|ùíúi|√ó|‚Ñ±i|subscriptùíúùëñsubscript‚Ñ±ùëñ|\\mathcal{A}_{i}|\\times|\\mathcal{F}_{i}|| caligraphic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | √ó | caligraphic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT |variants per transformer layer, while decoupled BLD requires only|ùíúi|+|‚Ñ±i|subscriptùíúùëñsubscript‚Ñ±ùëñ|\\mathcal{A}_{i}|+|\\mathcal{F}_{i}|| caligraphic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | + | caligraphic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT |variants per layer, significantly speeding up library construction.",
                "position": 349
            }
        ]
    },
    {
        "header": "4Decomposed NAS Search Algorithm for LLMs",
        "images": []
    },
    {
        "header": "5Post-Puzzle Inter-Block Uptraining",
        "images": []
    },
    {
        "header": "6Supporting Fast Inference for Variable-Block Architectures in TensorRT-LLM",
        "images": []
    },
    {
        "header": "7Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19146/x4.png",
                "caption": "Figure 4:Preference of human annotators in a blind test comparison. Results indicate comparable performance between Llama-3.1-70B-Instruct and Nemotron-51B.",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2411.19146/",
                "caption": "Figure 5:Accuracy vs. Throughput performance of Nemotron-51B compared to state-of-the-art models. Throughput is measured on NVIDIA H100 GPUs with the optimal TP setting per model, all running in FP8 on a ‚Äútext generation‚Äù scenario (see Table3). Theredline represents the efficient frontier, highlighting models with the best accuracy-to-throughput tradeoff. Accuracy=(MT-Bench√ó\\times√ó10 + MMLU) / 2",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2411.19146/x6.png",
                "caption": "Figure 6:The estimated runtime of the attention and FFN subblocks of Nemotron-51B, relative to the original subblocks of Llama-3.1-70B-Instruct.",
                "position": 1062
            }
        ]
    },
    {
        "header": "8In-Depth Analysis and Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19146/x7.png",
                "caption": "Figure 7:Accuracy vs. Throughput performance of Llama-3.1-8B-Instruct child derivatives, some constructed using LM loss as replace-1-block scores, and some constructed using KL divergence as replace-1-block scores. LM loss aims to capture the general quality degradation induced by changing a specific parent block, while KL divergence aims to capture the distance from the parent model induced by this change. KL divergence results in better architecture choices. Accuracy = (MT-Bench√ó\\times√ó10 + MMLU) / 2. Throughput is estimated via the sum of measured block runtimes on a single NVIDIA RTX 4090 GPU.",
                "position": 1241
            },
            {
                "img": "https://arxiv.org/html/2411.19146/x8.png",
                "caption": "Figure 8:Heatmaps showing how attention and FFN runtime patterns vary with throughput constraints across model layers.Dark colorsindicate higher computational cost relative to the parent model. Each row represents an architecture optimized for a specific throughput target, with Nemotron-51B‚Äôs configuration (5500 tokens/sec) marked ingreen.",
                "position": 1376
            },
            {
                "img": "https://arxiv.org/html/2411.19146/x9.png",
                "caption": "",
                "position": 1380
            }
        ]
    },
    {
        "header": "9Conclusions and Future Directions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendices",
        "images": []
    },
    {
        "header": "Appendix BRULER Benchmark Performance Tables",
        "images": []
    }
]