[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17827/x1.png",
                "caption": "Figure 1:An example demonstrating the challenges of 4D object understanding involves multi-view spatial-temporal reasoning.Given the 4D object, the robotâ€™s right hand seems ambiguous in some views at first and eventually disappears over time.\nHence, answering the question needs to (1) address multi-view ambiguity and choose proper views and time that the right hand is visible, (2) localize the right hand, (3) and track its evolutions along the time dimension.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x2.png",
                "caption": "Figure 2:Illustration of the 4D-Bench.4D-Bench consists of two critical tasks (a) 4D object QA and (b) 4D object captioning. 4D object QA provides one question and four choices per QA to evaluate MLLMs. 4D object captioning provides five human captions per 4D object.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3A New Benchmark: 4D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17827/x3.png",
                "caption": "Figure 3:Pipeline for constructing the 4D-Bench dataset.The pipeline includes rendering multi-view videos for 4D objects from Objaverse-XL, motion filtering, visual quality filtering, and multi-stage annotations for QA pairs and captions. Captions are purely human-annotated, while QA pairs are generated through a hybrid approach using MLLMs and human validation.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x4.png",
                "caption": "Figure 4:Subtask and category distributions in 4D object QA and captioning.Left: Distribution of five subtasks in the 4D object QA task, 751 question-answering pairs in total. Right: Distribution of 4D object categories in 4D object captioning task, 580 4D objects in total.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x5.png",
                "caption": "",
                "position": 306
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17827/x6.png",
                "caption": "Figure 5:An example fromObject Countingsubtask.Answering this question requires integrating multi-view information and capturing cross-view correspondences to count the presents, necessitating multi-view reasoning abilities. If relying solely on a single view (e.g.the middle row), it would lead to wrong answers (e.g.four), since some boxes are occluded and invisible in this view.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x7.png",
                "caption": "Figure 6:Effect of view number and temporal sampling on the 4D object QA performance.Tested on Gemini 1.5 Flash. Left: Accuracies across different numbers of views with fixed 6 frames. Right: Accuracies across different temporal frequencies with fixed 3 views.",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x8.png",
                "caption": "Figure 7:Effect of view number and temporal sampling on the 4D object captioning performance.Tested on Qwen2-VL 7B. Left: GPT-Eval scores across different numbers of views with fixed 6 frames. Right: GPT-Eval scores across different temporal frequencies with fixed 3 views.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x9.png",
                "caption": "Figure 8:A counterfactual example from 4D object QA task.A synthetic spider with six legs, illustrating a counterfactual scenario for testing model understanding, as real spiders typically have eight legs.",
                "position": 1018
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x10.png",
                "caption": "Figure 9:A counterfactual example from 4D object QA task.A ball rolling into a downward-facing hole and then rolling back out, depicting a counterfactual scenario that violates physical laws, as a ball would normally stay trapped in the hole.",
                "position": 1021
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AMore related work",
        "images": []
    },
    {
        "header": "BMore details about 4D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17827/x11.png",
                "caption": "Figure I:The frame-length distribution of multi-view videos used in the 4D object captioning task",
                "position": 2808
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x12.png",
                "caption": "Figure II:The length distribution of ground-truth captions used in the 4D object captioning task",
                "position": 2811
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x13.png",
                "caption": "Figure III:The frame-length distribution of multi-view videos used in the 4D object question answering task",
                "position": 2819
            }
        ]
    },
    {
        "header": "CMore experimental details on 4D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17827/x14.png",
                "caption": "Figure IV:The prompt provided to the evaluated MLLMs in the 4D object captioning task. In this prompt, we describe the video information, caption requirement, and output format. We also provide several caption examples to guide the style of captions generated by MLLMs.",
                "position": 2847
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x15.png",
                "caption": "Figure V:The truncated length distribution of correct answers and decoys used in 4D object question answering dataset",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x16.png",
                "caption": "Figure VI:The prompt provided to the evaluated MLLMs in the 4D object QA task. In this prompt, we detailed the video information, questions and options, and the output format.",
                "position": 2861
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x17.png",
                "caption": "Figure VII:Prompt used in GPT-Appearance metric",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x18.png",
                "caption": "Figure VIII:Prompt used in GPT-Action metric",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x19.png",
                "caption": "Figure IX:Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x20.png",
                "caption": "Figure X:Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench",
                "position": 2942
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x21.png",
                "caption": "Figure XI:Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench",
                "position": 2945
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x22.png",
                "caption": "Figure XII:Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench",
                "position": 2948
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x23.png",
                "caption": "Figure XIII:Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench",
                "position": 2951
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x24.png",
                "caption": "Figure XIV:Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench",
                "position": 2954
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x25.png",
                "caption": "Figure XV:Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2503.17827/x26.png",
                "caption": "Figure XVI:Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench",
                "position": 2960
            }
        ]
    },
    {
        "header": "DAdditional evaluation results on 4D-Bench",
        "images": []
    }
]