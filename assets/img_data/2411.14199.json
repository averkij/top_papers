[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x1.png",
                "caption": "",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x2.png",
                "caption": "",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x3.png",
                "caption": "",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x4.png",
                "caption": "",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x5.png",
                "caption": "",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x6.png",
                "caption": "",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x7.png",
                "caption": "Figure 1:(Top) Overview ofOpenScholar:OpenScholarconsists of a specialized datastore, retrievers and LMs and iteratively improves responses using self-feedback inference with retrieval.(Middle) Overview ofScholarQABench:ScholarQABenchconsists of 2.2k expert-written questions across multiple scientific disciplines, and we introduce automatic and human evaluation protocols forScholarQABench.(Bottom) Automatic and Human Evaluation Results:Experimental results show the effectiveness ofScholarQABench, and thatOpenScholarwith our trained 8B or GPT4o significantly outperforms other systems, and is preferred over experts over 50% of the time in human evaluations.",
                "position": 300
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2OpenScholar: Open Retrieval-augmented LM to Synthesizing Scientific Literature",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x8.png",
                "caption": "Figure 2:Detailed overview ofOpenScholarinference (top) and training (bottom).\nAt inference time, given an inputxùë•xitalic_x,OpenScholarfirst uses a retriever to identify relevant papers from a specialized datastore (OpenScholar-Datastore), and then uses a reranker to refine and identify the topNùëÅNitalic_Nretrieved documents. The retrieved output is then passed to the LM, which generates both an (1) initial responsey0subscriptùë¶0y_{0}italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTand (2) self-feedbackf1subscriptùëì1f_{1}italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. By incorporating its own feedback, the LM iteratively refines its output a pre-defined number of times.\nSubsequently, an LM (1) generates initial responsey0subscriptùë¶0y_{0}italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, (2) generates self-feedback on the initial output, and (3) incorporate feedback (fisubscriptùëìùëñf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) to generates an updated responsey1subscriptùë¶1y_{1}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.\nThe LM repeats the process until all feedback is incorporated.\nTo train a smaller yet competitive 8B LM, we generate high-quality training data using this inference-time pipeline followed by data filtering and mixing.",
                "position": 388
            }
        ]
    },
    {
        "header": "3ScholarQABench: Realistic Literature Review Evaluation Benchmark annotated by Ph.D. Experts",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x9.png",
                "caption": "Figure 3:AnScholarQA-CSexample and evaluation overview.ScholarQA-CSconsists of 100 questions and an average of 4.4 expert-written rubrics to be satisfied. OurScholarQABenchevaluation pipeline evaluates aspects like correctness and citation accuracy.",
                "position": 709
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x10.png",
                "caption": "(a)Ablation of different components ofOpenScholar.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x10.png",
                "caption": "(b)Top N Ablations (Corr) onScholar-CS.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x11.png",
                "caption": "(c)Top N Ablations (Cite-F1) onScholar-CS.",
                "position": 1244
            }
        ]
    },
    {
        "header": "5Expert Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x12.png",
                "caption": "Figure 5:Fine-grained evaluation results.Score distributions between 1) GPT4o (top),OpenScholarwith 8B (middle),OpenScholarwith GPT4o with Human (bottom).",
                "position": 1391
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AReleased Artifacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x13.png",
                "caption": "",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x14.png",
                "caption": "",
                "position": 2855
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x15.png",
                "caption": "",
                "position": 2860
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x16.png",
                "caption": "",
                "position": 2900
            }
        ]
    },
    {
        "header": "Appendix BMore Details onScholarQABench",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x17.png",
                "caption": "(a)Subject distributions of expert-annotated answers",
                "position": 3062
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x17.png",
                "caption": "(a)Subject distributions of expert-annotated answers",
                "position": 3065
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x18.png",
                "caption": "(b)Average per-instance annotation time for each subject (seconds).",
                "position": 3070
            }
        ]
    },
    {
        "header": "Appendix CMore Details onOpenScholar",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x19.png",
                "caption": "Figure 7:Generator training data distribution.We mix diverse training data to train our 8B LM.",
                "position": 3245
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/interface_1.png",
                "caption": "Figure 8:Human evaluation annotation interface.",
                "position": 3360
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/interface_2.png",
                "caption": "Figure 9:Human evaluation annotation interface.",
                "position": 3363
            }
        ]
    },
    {
        "header": "Appendix DMore Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14199/x20.png",
                "caption": "Figure 10:Distributions of the paper publication years of top 20 retrieved papers forScholarQA-CS.This figure shows that by updating the datastore from peS2o v2 to the more recent peS2o v3, which includes papers up till October 2024, our dense retrieval model can successfully retrieve more recent papers.",
                "position": 3375
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_fluency.png",
                "caption": "(a)GPT4 answer",
                "position": 3635
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_fluency.png",
                "caption": "(a)GPT4 answer",
                "position": 3638
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/llama3_fluency.png",
                "caption": "(b)Llama 3 8B",
                "position": 3643
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_relevance.png",
                "caption": "(a)GPT4 answer",
                "position": 3651
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_relevance.png",
                "caption": "(a)GPT4 answer",
                "position": 3654
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/llama3_relevance.png",
                "caption": "(b)Llama 3 8B",
                "position": 3659
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_prometheous_sufficient.png",
                "caption": "(a)GPT4 answer",
                "position": 3667
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_prometheous_sufficient.png",
                "caption": "(a)GPT4 answer",
                "position": 3670
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/llama3_prometheous_sufficient.png",
                "caption": "(b)Llama 3 8B",
                "position": 3675
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_overall_usefuleness.png",
                "caption": "(a)GPT4 answer",
                "position": 3683
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/gpt4_overall_usefuleness.png",
                "caption": "(a)GPT4 answer",
                "position": 3686
            },
            {
                "img": "https://arxiv.org/html/2411.14199/extracted/6014045/figs/llama3_overall_usefulness.png",
                "caption": "(b)Llama 3 8B",
                "position": 3691
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x21.png",
                "caption": "Figure 15:Single-task examples inScholarQABench.",
                "position": 4045
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x22.png",
                "caption": "Figure 16:Examples of ScholarBench (Bio).",
                "position": 4275
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x23.png",
                "caption": "Figure 17:An example ofScholarQA-Multi.",
                "position": 4279
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x24.png",
                "caption": "Figure 18:An example ofScholarQA-Multi.",
                "position": 4283
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x25.png",
                "caption": "Figure 19:An example ofScholarQA-Multi.",
                "position": 4287
            },
            {
                "img": "https://arxiv.org/html/2411.14199/x26.png",
                "caption": "Figure 20:An example ofScholarQA-Multi.",
                "position": 4291
            }
        ]
    },
    {
        "header": "Appendix EExamples ofScholarQABench",
        "images": []
    }
]