[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13687/x1.png",
                "caption": "Figure 1:Visual Tokenzier Pre-training.We revisit the visual tokenizer pre-training in LDM[ldm]from a representation learning perspective. Critically, while keeping the diffusion model (e.g., DiT[dit]) training configuration fixed, our method improves generation solely by scaling the tokenizer’s pre-training to learn a better-structured latent space.",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x2.png",
                "caption": "Figure 2:Understanding is a key driver of generation.We observe a strong positive correlation between the comprehension and generative capabilities of the latent space during visual tokenizer pre-training.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13687/x3.png",
                "caption": "Figure 3:Overwiew of Visual Tokenizer Pre-training (VTP).By integrating representation learning (image-text contrastive[clip]and self-supervised learning[dinov2]) with reconstruction within a Vision Transformer Auto-Encoder, we find that VTP exhibits awell-behaved scaling propertyfor generative performance.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Visual Tokenizer Pre-training",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13687/x4.png",
                "caption": "Figure 4:Scaling up Visual Tokenizer Training with Reconstruction Only.As training progresses, the tokenizer’s reconstruction performance improves, while its generative performance degrades concurrently. It reveals theinadequacy of pure reconstruction tasks for scalable tokenizer pre-training.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x5.png",
                "caption": "Figure 5:Scalability of CLIP+AE & SSL+AE Visual Tokenizer Pre-training.Scaling properties under different strategies and bottleneck dimensions. Our method shows correlated growth in generation and comprehension with compute, while VAE-based tokenizer performance rapidly saturates.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x5.png",
                "caption": "",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x6.png",
                "caption": "",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x7.png",
                "caption": "Figure 6:Scalability of CLIP+SSL+AE Visual Tokenizer Pre-training.Under the same computational budget, the f16d64 tokenizer trained with joint CLIP and SSL representation learning achieves the best performance in both generation and comprehension.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x8.png",
                "caption": "(a)Data Scaling.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x8.png",
                "caption": "(a)Data Scaling.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x9.png",
                "caption": "(b)Encoder Scaling.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x10.png",
                "caption": "(c)Decoder Scaling.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x11.png",
                "caption": "Figure 8:Reconstruction Comparison.VTP demonstrates superior reconstruction performance, particularly in terms of color accuracy and the preservation of fine textures.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2512.13687/x12.png",
                "caption": "Figure 9:Generation Comparison Comparison.VTP achieves faster convergence in generation, indicating a higher potential upper bound for the pre-training paradigm.",
                "position": 624
            }
        ]
    },
    {
        "header": "5Further scaling and Comparison",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    }
]