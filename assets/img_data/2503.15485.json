[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15485/x1.png",
                "caption": "Figure 1:TULIP Overview.Existing contrastive image-text models struggle with high-fidelity visual understanding. TULIP is a drop-in replacement for CLIP which leverages generative data augmentation, global-local patch-wise image contrastive learning, and reconstruction-based feature regularization to learn robust visual features and fine-grained language grounding.",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15485/x2.png",
                "caption": "Figure 2:TULIP Image Encoder.Images undergo both traditional augmentations (such as cropping and color jittering) and generative augmentations via GeCo, which leverages large generative models to create semantically consistent or semantically altered views. These views are then used for image-image and image-text contrastive learning. Additionally, a masked autoencoder (MAE)-based reconstruction loss is applied to encourage the model to encode both semantic and fine-grained details.",
                "position": 181
            }
        ]
    },
    {
        "header": "3TULIP",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15485/x3.png",
                "caption": "Figure 3:TULIP Text Encoder.Text undergoes generative augmentation through paraphrasing and controlled semantic alterations using large language models, generating both positive and negative contrastive pairs. These pairs are used for both text-text and image-text contrastive learning with a SigLIP objective. Similar to image reconstruction, a causal decoder (based on T5) is used for text reconstruction, ensuring that the model retains both high-level semantics and fine-grained linguistic detail.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2503.15485/x4.png",
                "caption": "Figure 4:Overview of GeCo.Our generative augmentation framework leverages large generative models to create diverse contrastive views by generating both positive and negative augmentations for images and text. For text augmentation, we use Llama-3.1-8B-Instruct to generate paraphrases and semantically altered text variations. For image augmentation, we fine-tune an instruction-based image editing model (e.g., InstructPix2Pix) fine-tuned using soft-prompting to generate semantically consistent (positive) and semantically altered (negative) views.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2503.15485/x5.png",
                "caption": "Figure 5:(Top) GeCo generates positive and negative augmentations of both images and text, (Bottom) TULIP uses these augmentations during training time with corresponding weights (+1 for positive pair, -1 for negative pair, 0 to ignore). Here, the generated positive image represents the same bird from a different viewpoint, while the negative image is a different bird (coloring, face structure) in the same physical location.",
                "position": 336
            }
        ]
    },
    {
        "header": "4Experiments & Results",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ACode Release",
        "images": []
    },
    {
        "header": "Appendix BTraining Data",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Datasets",
        "images": []
    },
    {
        "header": "Appendix DData Augmentation",
        "images": []
    },
    {
        "header": "Appendix EModel Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15485/extracted/6287603/figures/appendix/upsampled_attention_overlay_23.png",
                "caption": "Figure F.1:Visualization of the attention heads. Attention maps are averaged across transformer blocks, then up-sampled to the resolution of the original image.",
                "position": 3960
            },
            {
                "img": "https://arxiv.org/html/2503.15485/extracted/6287603/figures/appendix/upsampled_attention_overlay_26.png",
                "caption": "",
                "position": 3964
            },
            {
                "img": "https://arxiv.org/html/2503.15485/extracted/6287603/figures/appendix/side-side_attention_overlay_4.png",
                "caption": "",
                "position": 3966
            }
        ]
    },
    {
        "header": "Appendix FAttention Visualization",
        "images": []
    }
]