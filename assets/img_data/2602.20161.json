[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20161/x1.png",
                "caption": "",
                "position": 65
            },
            {
                "img": "https://arxiv.org/html/2602.20161/assets/globe.png",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x2.png",
                "caption": "",
                "position": 69
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x3.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20161/x4.png",
                "caption": "Figure 1:Overview ofMobile-O.Left:The proposed framework consists of an efficient image encoder with a compact autoregressive language model for visual understanding. For image generation, a lightweight linear diffusion transformer (DiT) is employed alongside a simple yet effective VAE-based encoder–decoder.Right:Our novel Mobile Conditioning Projector (MCP) bridges the understanding and generation tasks by directly conditioning the diffusion model on weighted hidden states from the VLM without the need for intermediate query tokens. The projector leverages layer-wise feature fusion, depthwise separable convolutions, and efficient channel attention to produce high-fidelity conditioning signals with minimal cost, enabling seamless deployment on edge devices.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x5.png",
                "caption": "Figure 2:Overview of the proposed unified multimodal post-training pipeline.We jointly optimize multimodal understanding and generation through a multi-task objective using a quadruplet format (generation prompt, image, question, answer). Both I2T and T2I losses are computed simultaneously, enabling aligned cross-modal learning where each training sample supports both multimodal understanding and generation.",
                "position": 269
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20161/x6.png",
                "caption": "Figure 3:Qualitative comparison of text-to-image generation (left) and visual understanding (right) across unified multimodal models. Each column shows Janus, JanusFlow, Show-O, and Mobile-O (ours) for the same prompts/questions. Mobile-O yields more consistent, detailed, and semantically faithful images with high fidelity and style diversity for image generation. For visual understanding, it delivers more accurate and contextually coherent responses. Additional results are presented in suppl. material. Best viewed zoomed in.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x7.png",
                "caption": "Figure 4:Qualitative image editing results of Mobile-O-0.5B. Given a source image and a textual editing instruction, Mobile-O-0.5B produces the edited output. The model is fine-tuned on only 46k editing samples from ShareGPT4V[6]",
                "position": 889
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Mobile Conditioning Projector Depth",
        "images": []
    },
    {
        "header": "8On-Device Mobile Deployment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20161/x8.png",
                "caption": "Figure 5:Mobile-O running natively on iPhone 17 Pro.We demonstrate real-world deployment ofMobile-O’s unified capabilities on consumer hardware. (a) Text-to-image generation: Given a detailed prompt describing a Bengal tiger. (b) Image-to-text generation: Mobile-O provides detailed visual descriptions, analyzing composition and subject positioning",
                "position": 1946
            }
        ]
    },
    {
        "header": "9More Implementation Details",
        "images": []
    },
    {
        "header": "10More Image-to-Text Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.20161/x9.png",
                "caption": "Figure 6:Qualitative comparison on dense text understanding and information extraction.We evaluateMobile-Oagainst other models on a challenging OCR and comprehension task requiring the model to read, parse, and summarize the back cover text of a book. Green text indicates correctly extracted information, while red indicates hallucinations or errors.Mobile-Odemonstrates superior performance in accurately extracting key bibliographic details, including the correct title, author, editors, and price information from the densely-packed text on the book cover.",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x10.png",
                "caption": "Figure 7:Qualitative comparison with SANA-0.6B on text-to-image generation.We compareMobile-O(1.6B total parameters) against SANA-0.6B (2.6B total parameters), our generation baseline, on challenging prompts requiring photorealistic rendering, complex lighting, and fine-grained details.Mobile-Odemonstrates competitive or superior visual quality across diverse scenarios, including wildlife photography, landscape composition, and portrait rendering. Best viewed zoomed in.",
                "position": 2085
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x11.png",
                "caption": "Figure 8:Qualitative comparison of image-to-text across unified models below 2B.Mobile-Ois compared against Janus[47], JanusFlow[20], and Show-O[45]on diverse visual question answering tasks, including scientific reasoning, OCR, object recognition, and cultural knowledge. Green indicates correct answers, red indicates errors.Mobile-Odemonstrates competitive visual understanding, despite its mobile-optimized architecture, correctly answering complex questions that require fine-grained visual analysis and domain knowledge.",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x12.png",
                "caption": "Figure 9:Qualitative comparison of text-to-image generation across unified models below 2B.Mobile-Ois compared against Janus[47], JanusFlow[20], and Show-O[45]on challenging prompts spanning fantasy, photorealism, and scientific visualization. Despite its mobile-optimized architecture,Mobile-Omaintains competitive visual quality and prompt adherence. Best viewed zoomed in.",
                "position": 2091
            },
            {
                "img": "https://arxiv.org/html/2602.20161/x13.png",
                "caption": "Figure 10:Additional Text-to-Image generation examples ofMobile-O.Best viewed zoomed in.",
                "position": 2094
            }
        ]
    },
    {
        "header": "11Comparison with Generation-Only Baseline",
        "images": []
    },
    {
        "header": "12More Text-to-Image Qualitative Results",
        "images": []
    },
    {
        "header": "13Limitations",
        "images": []
    }
]