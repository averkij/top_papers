[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22096/x1.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22096/x2.png",
                "caption": "Figure 2:An example of re-annotating the dataset. The original and new captions are used for T2V and I2V training, respectively. TheOriginalcaption describes detail scene context, while theNewcaption, generated by VLM, explicitly focuses on dynamic events.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2512.22096/x3.png",
                "caption": "Figure 3:Core components ofYume1.5. (a) DiT Block with linear attention for efficient feature fusion. (b) Training pipeline with decomposed event and action descriptions. (c) Adaptive history tokens downsampling with varying compression rates based on temporal distance. (d) Chunk-based autoregressive inference with dual-compression memory management.",
                "position": 198
            }
        ]
    },
    {
        "header": "3Data Processing",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22096/x4.png",
                "caption": "Figure 4:Long-form video generation method.Left (Generator):The model autoregressively generates video chunks. Critically, it uses itsown generated frames(rather than ground truth) as historical context—compressed byTSCM(Sec. 4.2)—to mitigate the train-inference discrepancy.Right (Distillation):TheFake Model(student) is optimized to match the trajectory of theReal Model(teacher) via a distribution matching gradient. This enables high-quality few-step inference while robustly handling error accumulation in long videos.",
                "position": 385
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22096/x5.png",
                "caption": "Figure 5:Aesthetic Score Dynamics in Long-video Generation. Aesthetic Score Dynamics in Long-video Generation. Thex-axisrepresents the number of video blocks (chronological segments), and they-axisdenotes the Aesthetic Score.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2512.22096/x6.png",
                "caption": "Figure 6:Image Quality Dynamics in Long-video Generation. Thex-axiscorresponds to the number of video blocks, and they-axisshows the Image Quality score.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2512.22096/x7.png",
                "caption": "Figure 7:Speed Comparison of TSCM, Spatial Compression and Full Context Input. The test resolution is 704×\\times1280. Thex-axisindicates the number of video blocks (increasing context length), and they-axisrepresents the inference time in seconds.",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2512.22096/x8.png",
                "caption": "Figure 8:Qualitative generation results. All tests were conducted at a resolution of 544×\\times960, withYume1.5using 4 sampling steps while all other methods employed 50 sampling steps.",
                "position": 682
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    }
]