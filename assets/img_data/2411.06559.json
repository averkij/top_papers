[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06559/x1.png",
                "caption": "Figure 1:Schematic illustration of different strategies for web agents formulated as a search problem. Each node represents a webpage.\n(a) Reactive: The agent selects locally optimal actions without forward planning, often leading to suboptimal outcomes.\n(b) Tree search with real interactions: The agent explores multiple paths through active website navigation and permits backtracking (indicated by dashed arrows).\nHowever, in real-world websites, backtracking is often infeasible due to the prevalence of irreversible actions.\n(c) Model-based planning: The agent simulates potential outcomes (illustrated by cloud-bordered nodes) to determine optimal actions prior to real-world execution, thus minimizing actual website interactions while maintaining effectiveness. For visual clarity, only one-step simulated outcomes are depicted.\nFaded nodes indicate unexplored webpages, while green checkmarks and red crosses denote successful and unsuccessful outcomes, respectively.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06559/x2.png",
                "caption": "Figure 2:Illustration ofWebDreamerusing the LLM to simulate the outcome of each candidate action.\nThe LLM simulates trajectories in natural language descriptions for three candidate actions: (1)Click “Office Products”, (2)Click “Electronics”, and (3)Type “Disk” into textbox.\nThrough these simulations, each resulting trajectory is scored to identify the action most likely to succeed. In this case, the LLM selects ClickClick “Electronics”as the optimal step and executes it.\nEach dotted box represents an LLM-generated state description after each simulated action. This example demonstrates a two-step planning horizon.",
                "position": 316
            }
        ]
    },
    {
        "header": "4WebDreamer: Model-Based Planning for Web Agents",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06559/x3.png",
                "caption": "Figure 3:Ablation study on the simulation stage and self-refinement stage.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2411.06559/x4.png",
                "caption": "Figure 4:We demonstrate the performance on a subset of the VWA dataset, varying both the state representation within simulations and the planning horizon.\nPlanning with long horizon with simulation remains challenging, regardless of the state representation employed.",
                "position": 829
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts for Four Stages in MPC-Based Planning",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06559/x5.png",
                "caption": "Figure 5:An error case caused by imperfect world model simulation.",
                "position": 1739
            },
            {
                "img": "https://arxiv.org/html/2411.06559/x6.png",
                "caption": "Figure 6:A positive case where the simulation leads to correct action prediction.",
                "position": 1746
            }
        ]
    },
    {
        "header": "Appendix BCase Study",
        "images": []
    }
]