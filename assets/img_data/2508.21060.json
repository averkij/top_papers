[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21060/x1.png",
                "caption": "Figure 1:We introduceMVTracker, the firstdata-driven multi-view 3D point trackerfor tracking arbitrary 3D points across multiple cameras. Our method fuses multi-view features into a unified 3D feature point cloud, within which it leverages kNN-based correlation to capture spatiotemporal relationships across views. A transformer then iteratively refines the point tracks, handling occlusions and adapting to varying camera setups without per-sequence optimization. This figure shows results on SelfCap[40]using DUSt3R-based[37]depth.",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21060/x2.png",
                "caption": "Figure 2:MVTracker Pipeline.\n(a) Given synchronized multi-view RGB videos with known intrinsics and extrinsics, we extract per-view feature maps using a CNN-based encoder. (b) We construct a point cloud from estimated or sensor-based depth maps, associating each point with learned feature embeddings (Sec.3.2). (c) We compute directed kNN-based correlation within the point cloud, capturing spatiotemporal relationships across views (Sec.3.3). (d) A transformer-based update module iteratively refines point trajectories using self-attention over multi-view feature correlations within a sliding temporal window (Sec.3.4). (e) The model processes sequences in overlapping sliding windows, producing temporally consistent 3D point trajectories with occlusion-aware visibility predictions (Sec.3.5). The blue blocks denote trainable neural models. The visualized sequence is from SelfCap[40]and uses DUSt3R[37]to estimate depth.",
                "position": 177
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21060/x3.png",
                "caption": "Figure 3:Qualitative comparison of multi-view 3D point tracking.We visualize results from a camera viewpointnotused during inference (and not near the input cameras). Each pair of rows corresponds to two time steps from the same dataset. The leftmost column shows ground-truth 3D trajectories, while remaining columns depict predictions from different methods. Points predicted as occluded are indicated by empty circles. Compared to baselines, our method more accurately maintains correspondences across views and handles occlusions. Please see the supp. video and project page for additional visualizations. These examples correspond to the results inTab.1.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2508.21060/x4.png",
                "caption": "Figure 4:Effect of the Number of Input Viewson DexYCB[3](DUSt3R-based depth). MVTracker (blue) consistently improves with more views, reaching an AJ of 79.2 with eight views, indicating the benefit of multi-view information. SpatialTracker (green) and Triplane (orange) show moderate improvements but plateau earlier, while single-view methods such as CoTracker3, DELTA, and LocoTrack (red, purple, and brown) exhibit limited gains.",
                "position": 807
            }
        ]
    },
    {
        "header": "5Discussion and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADepth Estimation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21060/x5.png",
                "caption": "Figure A.1:Robustness to depth noiseùí©‚Äã(0,œÉ2)\\mathcal{N}(0,\\sigma^{2})on MV-Kubric.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2508.21060/x6.png",
                "caption": "Figure A.2:Different depth sources along our predicted tracks.",
                "position": 1693
            }
        ]
    },
    {
        "header": "Appendix BInference Speed Comparison",
        "images": []
    },
    {
        "header": "Appendix CAblation Study",
        "images": []
    },
    {
        "header": "Appendix DBaselines",
        "images": []
    },
    {
        "header": "Appendix EEvaluation on TAPVid-2D",
        "images": []
    },
    {
        "header": "Appendix FEvaluation Metrics",
        "images": []
    }
]