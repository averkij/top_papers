[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18476/x1.png",
                "caption": "Figure 1:Illustration of chain-like (left) and tree-like (right) reasoning in VLMs on 3D scene generation.\nEach node represents a token or language sequence.\nThe red dashed nodes indicate the VLM produces an inappropriate output.\nThe chain-like method cannot correct the prior errors and the subsequent process reasons based on the errors, leading to a non-realistic layout, such as exceeding the room.\nIn contrast, the tree-like method can modify the output if a mistake occurs, resulting in a more realistic layout.",
                "position": 78
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Hierarchical Scene Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18476/x2.png",
                "caption": "Figure 2:We prompt a VLM to generate the hierarchical scene representation level by level.\nFrom left to right, we decompose the scene into room, region, floor object, and supported object levels.\nThe final representation is shown on the right-most side in this figure.",
                "position": 216
            }
        ]
    },
    {
        "header": "5Global-Local Tree Search in VLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18476/x3.png",
                "caption": "Figure 3:To generate a layout for a scene with quantities of objects, we independently generate the layout for each region.\nThe global and local tree search method starts from the root node and goes deep by generating a thought.\nIf the thought generator fails to produce a thought, it will trace back to the parent node and move to another thought.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2503.18476/x4.png",
                "caption": "Figure 4:We discretize the top-down view as a grid and fill the cells with emojis.\nThebrickandwhite goemojis stand for the wall and region boundary respectively.",
                "position": 282
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18476/x5.png",
                "caption": "Figure 5:Performance comparison in terms of CLIP score by our proposed model with state-of-the-art methods.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2503.18476/x6.png",
                "caption": "Figure 6:The generation results for the bathroom, living room, kitchen, and bedroom.For visualization, we manually set floor textures for the rooms.\nThe results are rendered with the Blenderâ€™s Cycles engine.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2503.18476/x7.png",
                "caption": "Figure 7:Visualization results of intermediate steps of generating a scene.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2503.18476/x8.png",
                "caption": "Figure 8:Visualization example of the scene control for complex prompt.",
                "position": 535
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]