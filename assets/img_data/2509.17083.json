[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17083/x1.png",
                "caption": "Figure 1:Mip-NeRF360[3]struggles with inaccuracies in fine details and slow rendering speeds, while 3DGS[15]face challenges of large model sizes and blurry background.\nA naive combination of neural fields and 3DGS leads to loss of high-frequency information.\nOur method overcomes these challenges through an innovative hybrid architecture. By synergistically combining neural fields, explicit Gaussians, and neural background map, we achieve competitive or superior performance in both visual quality and model compactness, while maintaining real-time rendering capabilities.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17083/x2.png",
                "caption": "Figure 2:Framework overview.\nOur method represents the scene using grid-based neural fields and a set of compact explicit Gaussians storing only 3D position, 3D diffuse color, isotropic scale, and opacity. We encode the point position into a high-dimensional feature using the neural field and decode it into Gaussian properties with tiny MLP. These Gaussian properties are then aggregated with the explicit Gaussians and integrated into the 3DGS rasterizer.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x3.png",
                "caption": "Figure 3:(a)Visibility Pre-Culling.We first determine whether each Gaussian lies within the current view frustum before applying neural field decoding.\n(b)Hybrid Rendering Pipeline.For each\ncamera ray, we: (1) compute its intersection pointpsp_{s}with a background sphere, (2) sample the radiance field atpsp_{s}, and (3) composite the foreground and background colors using alpha blending.",
                "position": 275
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17083/x4.png",
                "caption": "Figure 4:Qualitative comparisons of our method against previous approaches on standard real-world datasets[3,16,13].\nThe selected scenes include thebicycleandcounterscenes from the MipNeRF360 dataset[2], theplayroomscene from the DeepBlending dataset[13], and thetruckscene from the Tanks & Temples dataset[16].\nArrows and insets are used to highlight key differences.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x5.png",
                "caption": "Figure 5:Ablation of decoupled neural fields.Using a single neural field to predict Gaussian properties causes gaps and holes.",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x5.png",
                "caption": "Figure 5:Ablation of decoupled neural fields.Using a single neural field to predict Gaussian properties causes gaps and holes.",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x6.png",
                "caption": "Figure 6:Ablation of background rendering.The learnable background map improves the quality of distant objects (see the clouds and sky).",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x7.png",
                "caption": "Figure 7:Detailed ablation studies of each of the explicit Gaussian properties.",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x7.png",
                "caption": "Figure 7:Detailed ablation studies of each of the explicit Gaussian properties.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2509.17083/x8.png",
                "caption": "Figure 8:Comparison of PSNR and model size changes during the training phase.",
                "position": 976
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17083/x9.png",
                "caption": "Figure 9:Additional qualitative comparisons of our method against previous approaches on standard real-world\ndatasets.",
                "position": 2082
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    }
]