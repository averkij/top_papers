[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17985/x1.png",
                "caption": "Figure 1.Overall framework.\n(1) Users construct a scene using coarse geometry or 3D assets.\n(2) A camera trajectory and (3) a reference image are provided.\n(4) The framework then generates a high-quality video reflecting the specified style, structure, and camera motion.\nThe synthesized video sequence shows consistent, high-quality visuals that reflect the input geometry and reference style, including challenging visual elements such as rising steam.",
                "position": 113
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17985/x2.png",
                "caption": "Figure 2.Comparison of outputs from image and video diffusion models, conditioned only on a text prompt. Model sizes are noted in parentheses.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x3.png",
                "caption": "Figure 3.Overall pipeline.\n(a) Preprocessing extracts structural edges and optical flows from geometry and camera trajectory.\n(b) SAG module generates high-quality anchor viewsv0v_{0}andvNv_{N}using an image diffusion prior.\n(c) GGI module interpolates intermediate frames with a video diffusion prior.",
                "position": 191
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17985/x4.png",
                "caption": "Figure 4.To generate a multi-view coherentvNv_{N}fromv0v_{0}, Sparse Appearance-guided sampling uses the distorted appearance of the warped image as guidance during sampling, achieving the successful generation of a coherent and high-qualityvNv_{N}.",
                "position": 254
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17985/x5.png",
                "caption": "Figure 5.Effect of distribution alignment in generating the end viewvNv_{N}using Sparse Appearance-guided sampling.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x6.png",
                "caption": "Figure 6.Qualitative results across various scenarios. The first row illustrates scene information: the style reference (top-left), the camera trajectory (black line), camera positions corresponding to each generated view shown below (yellow), and the number of anchor views (bottom-right). The input geometry in (c) and (d) is from TurboSquid (©Okhey).",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x7.png",
                "caption": "Figure 7.Non-photorealistic generation results. The top-left image is the style reference image.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x8.png",
                "caption": "Figure 8.Visual effect example showing simultaneous changes in camera motion and temporal context, such as seasonal appearance. Speech bubbles indicate text prompts used in the SAG module, where[u]denotes the identifier prompt used in LoRA training.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x9.png",
                "caption": "Figure 9.Structural guidance simulation during GGI module training to reduce the domain gap between training and inference. The image is from DL3DV dataset(Ling et al.,2024).",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x10.png",
                "caption": "Figure 10.Qualitative comparisons with Depth-I2V (a depth-conditioned I2V diffusion model), VACE(Jiang et al.,2025), and SAG-augmented variants of MVSplat360(Chen et al.,2024b), LVMS(Jin et al.,2024), and SEVA(Zhou et al.,2025). The input geometry of (d-f) is from TurboSquid (©3D LT).",
                "position": 567
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17985/x11.png",
                "caption": "Figure 11.Qualitative results with and without the Sparse Appearance-guided Sampling. Orange boxes indicate the same regions of the input geometry. The input geometry is from TurboSquid (©Okhey).",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x12.png",
                "caption": "Figure 12.Qualitative comparison of different structural conditions of GGI.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x13.png",
                "caption": "Figure 13.Dense view generation using only the SAG module causes severe flickering (red) and accumulated warping errors (yellow).",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2509.17985/x14.png",
                "caption": "Figure 14.FLUX ControlNet generation results using (b) HED edge map, (c) Canny-edge map and (d) depth map.",
                "position": 735
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]