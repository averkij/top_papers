[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19319/x1.png",
                "caption": "Figure 1:Our synchronized multi-view joint diffusion (SyncMV4D) simultaneously models multi-view geometry, visual appearance, and motion dynamics. It is capable of generating both multi-view hand-object interaction videos (left) and 4D motion sequences, comprising intermediate coarse pseudo videos (middle) and refined point tracks (right), with results achieving visual realism, dynamic plausibility, and geometric consistency.",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19319/x2.png",
                "caption": "Figure 2:Our SyncMV4D consists of two key components: First, the Multi-view Joint Diffusion (MJD) module generates synchronized multi-view color videos, intermediate motion pseudo videos, and metric depth scales (Sec.3.3). Second, the Diffusion Points Aligner (DPA) module takes the resulting coarse 4D motions as a conditioning signal to reconstruct globally aligned 4D point tracks (Sec.3.4). Furthermore, since both MJD and DPA are iterative denoisers, the refined 4D point tracks from DPA are fed back to guide MJD in subsequent denoising steps, forming a closed-loop mutual enhancement cycle (Sec.3.5).",
                "position": 137
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19319/x3.png",
                "caption": "Figure 3:Comparison of motion representations between that of DaS[gu2025diffusion]and our 4D point tracks.\nFor each point, the first two dimensions represent the pixel coordinates of the tracked point in the first frame. The difference lies in the third dimension: DaS uses the static depth from the first frame, whereas we use the actual per-frame depth to enhance 3D perceptual capability.",
                "position": 206
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19319/x4.png",
                "caption": "Figure 4:Visualization of the generated multi-view videos from different methods.Redcircles indicate multi-view inconsistencies,yellowboxes highlight video distortions, andblueboxes denote blurring artifacts.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2511.19319/x5.png",
                "caption": "Figure 5:Visualization of multi-view points reprojected onto the same coordinate system.",
                "position": 526
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6More Ablation Studies",
        "images": []
    },
    {
        "header": "7Additional Demonstrations",
        "images": []
    }
]