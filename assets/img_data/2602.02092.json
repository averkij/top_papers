[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02092/figures/banner.jpg",
                "caption": "Figure 1:Videos generated via FSVideo’s Image-to-Video framework while being42.3×\\mathbf{42.3\\times}faster than Wan2.1-I2V-14B-720P. For every row, first frame is the input image from VBench[huang2024vbench]testset, and the following frames are generated. Zoom in to see details.",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02092/x1.png",
                "caption": "Figure 2:Overall framework of FSVideo image-to-video pipeline. The input image is sent to the encoder to get its VAE latent code, which is then used as the condition of the base module DIT for the first diffusion process. Then, the diffused latent is sent to the upsample module, where it first passes a latent convolution neural net (CNN) upscaler, and is then combined with the input image’s latent as the condition to the upsampler DIT for another diffusion process. Finally, the upsampled latent is sent to the decoder to generate output video. The decoder contains attention layers conditioned on input image’s encoder feature map to enhance video quality (see Section2.2.3).",
                "position": 136
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02092/x2.png",
                "caption": "Figure 3:Overall framework of FSAE.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2602.02092/x3.png",
                "caption": "Figure 4:Video reconstruction comparison between LTX-Video’s autoencoder and FSAE. The first three rows represent the reconstruction results of different AE, and the last row is the ground truth. The blue box tracks the clothing textures’ temporal consistency, where LTX-Video exhibits inconsistent inter-frame flickering, and FSAE consistently maintains the dotted texture. The red box compares the reconstruction quality to video details: FSAE-Lite and LTX-Video achieve comparable reconstruction results, whereas FSAE-Standard outperforms LTX-Video.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2602.02092/x4.png",
                "caption": "Figure 5:Transformer layer of FSVideo.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2602.02092/x5.png",
                "caption": "Figure 6:Weight heatmap of the dynamic router. For each representation index in each layer, it shows the maximum value of dynamic router weights across all tokens in a diffusion latent sequence. Layer index starts at 2 because there is no dynamic router for index 0 (input embeddings) and index 1 (only 1 previous layer).",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2602.02092/x6.png",
                "caption": "Figure 7:Training analysis of the layer memory mechanism.\n(a) Training Loss Curve (From Scratch): Comparison of the training loss between the baseline model (w/o Layer Memory) and the proposed model (with Layer Memory) on Wan2.1-14B-I2V-720P, demonstrating Layer Memory’s ability to achieve a consistently lower loss and faster initial convergence.\n(b) Resume Training Loss Curve (Fine-tuning): Loss curve showing the fine-tuning the original Wan2.1-14B-I2V-720P. The integration of Layer Memory facilitates rapid convergence and sustains a stable performance advantage compared to the baseline. Zoom in to see details.",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2602.02092/x7.png",
                "caption": "Figure 8:Overall framework of latent upsampler.",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2602.02092/x8.png",
                "caption": "Figure 9:Enhanced training strategies for the high-resolution video refiner.\nThe figure illustrates three input conditioning channels (noise,image conditions,masks) and our proposed training design.\nTop: WAN Baseline. Only the first frame is conditioned.\nMiddle: FSVideo refiner training. Incorporates low-resolution video latents and three alignment mechanisms: Deviation-Based Latent Estimation, Dynamic Masking (right), and Random Frame Replacement/Shuffle (bottom right) to improve restoration and temporal robustness.\nBottom: FSVideo refiner inference. The refiner performs Video-to-Video generation using the latent upsampler output as its primary condition.",
                "position": 784
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02092/x9.png",
                "caption": "Figure 10:GSB evaluation against other models.",
                "position": 907
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "5Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]