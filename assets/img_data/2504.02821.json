[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x1.png",
                "caption": "Figure 1:Training Sparse Autoencoders (SAEs) over VLMs (e.g. CLIP): The highly activating images of a neuron in a given layer of a pretrained VLM are polysemantic (top), and a neuron in an SAE trained to reconstruct the same layer is more monosemantic (bottom, with higher Monosemanticity Score (MS))",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x2.png",
                "caption": "Figure 2:Illustration of the computation of our Monosemanticity Score (MS).(A)We extract embeddings and activations from a given set of images,(B)then compute the pairwise embedding similarities and pairwise neuron activations.(C)MS is equal to the average of embedding similarities weighted by the neuron activations.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Sparse Autoencoders for VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x3.png",
                "caption": "Figure 3:Qualitative examples of highest activating images for different neurons from high (left) to low (right) Monosemanticity Scores (MS). As the score gets higher, images become more similar, illustrating the correlation with monosemanticity.",
                "position": 309
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x4.png",
                "caption": "Figure 4:Monosemanticity Scores in decreasing order across neurons, normalized by width. Results are shown for the last layer of the model, without SAE (â€œNo SAEâ€, in black dashed line), and with SAE using different expansion factors (in straight lines,forÎµ=1ğœ€1\\varepsilon=1italic_Îµ = 1,forÎµ=4ğœ€4\\varepsilon=4italic_Îµ = 4andforÎµ=16ğœ€16\\varepsilon=16italic_Îµ = 16).",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x5.png",
                "caption": "Figure 5:Impact of sparsity factorKğ¾Kitalic_Kon Monosemanticity Scores across neurons extracted from SAEs trained on the last layer of the model with expansion factorÎµ=1ğœ€1\\varepsilon=1italic_Îµ = 1. Results are shown for different sparsity levels, with straight linesforK=1ğ¾1K=1italic_K = 1,forK=10ğ¾10K=10italic_K = 10,forK=20ğ¾20K=20italic_K = 20, andforK=50ğ¾50K=50italic_K = 50. Scores of the original neurons (â€œNo SAEâ€, in black dashed lines) are added for comparison.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x6.png",
                "caption": "Figure 6:We steer the outputs of LLaVA by clamping the activation values of a chosen neuron, i.e. Neuron##\\##39 = pencil neuron, in the CLIP SAE. We observe that while initially the poem follows the instruction (the prompt + white image) strongly, the outputs become more and more influenced by the concept that this neuron represents with increasing intervention weightÎ±ğ›¼\\alphaitalic_Î±, talking about attributes of pencil first, and then just the concept pencil. This shows that our interventions enable new capabilities for the unsupervised steering of these models.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x7.png",
                "caption": "Figure 7:Effects of neuron interventions on MLLM-generated scientific article titles. Steering magnitudes are categorized as â€œ0â€, â€œmediumâ€, and â€œhighâ€ based on the intervention strength. The neurons are visualized with the highest activating images from which we deduce their associated concepts: â€œpolka dotsâ€, â€œshipwreckâ€, and â€œrainbowâ€.",
                "position": 668
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore details on steering",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x8.png",
                "caption": "Figure A1:LLaVA-like models can be steered towardsseeinga concept (e.g.panda) not present in the input imageğ±ğ±\\mathbf{x}bold_x. By attaching SAE after vision encoder and intervening on its neuron representing that concept, we effectively manipulate the LLMâ€™s response. Such flexible and precise steering is possible thanks to the extensive concept dictionary identified through the SAE.",
                "position": 1332
            }
        ]
    },
    {
        "header": "Appendix BAdditional results on monosemanticity",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x9.png",
                "caption": "Figure A2:MS in decreasing order across neurons. Results are shown for a layer without SAE (â€œNo SAEâ€), and with SAE using different expansion factors (Ã—1,Ã—4\\times 1,\\times 4Ã— 1 , Ã— 4andÃ—16absent16\\times 16Ã— 16).",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x10.png",
                "caption": "(a)Neurons of CLIPÂ ViT-L evaluated with CLIPÂ ViT-B as the image encoderEğ¸Eitalic_E",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x10.png",
                "caption": "(a)Neurons of CLIPÂ ViT-L evaluated with CLIPÂ ViT-B as the image encoderEğ¸Eitalic_E",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x11.png",
                "caption": "(b)Neurons of CLIPÂ ViT-L evaluated with DINOv2Â ViT-B as the image encoderEğ¸Eitalic_E",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x12.png",
                "caption": "(c)Neurons of SigLIPÂ SoViT-400m evaluated with CLIPÂ ViT-B as the image encoderEğ¸Eitalic_E",
                "position": 2232
            }
        ]
    },
    {
        "header": "Appendix CReconstruction of SAEs",
        "images": []
    },
    {
        "header": "Appendix DUniqueness of concepts",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/extracted/6333825/pencil_neuron.png",
                "caption": "Figure A4:Images highly activating the neuron we intervene on in Figure 6, which we manually labeled as â€œPencil Neuronâ€.",
                "position": 2584
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x13.png",
                "caption": "Figure A5:Qualitative examples of highest activating images for different neurons from high (left) to low (right) monosemanticity score. As the metric gets higher, highest activating images are more similar, illustrating the correlation with monosemanticity.",
                "position": 2587
            }
        ]
    },
    {
        "header": "Appendix EAdditional qualitative results",
        "images": []
    }
]