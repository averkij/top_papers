[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x1.png",
                "caption": "Figure 1:Training Sparse Autoencoders (SAEs) over VLMs (e.g. CLIP): The highly activating images of a neuron in a given layer of a pretrained VLM are polysemantic (top), and a neuron in an SAE trained to reconstruct the same layer is more monosemantic (bottom, with higher Monosemanticity Score (MS))",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x2.png",
                "caption": "Figure 2:Illustration of the computation of our Monosemanticity Score (MS).(A)We extract embeddings and activations from a given set of images,(B)then compute the pairwise embedding similarities and pairwise neuron activations.(C)MS is equal to the average of embedding similarities weighted by the neuron activations.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Sparse Autoencoders for VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x3.png",
                "caption": "Figure 3:Qualitative examples of highest activating images for different neurons from high (left) to low (right) Monosemanticity Scores (MS). As the score gets higher, images become more similar, illustrating the correlation with monosemanticity.",
                "position": 309
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x4.png",
                "caption": "Figure 4:Monosemanticity Scores in decreasing order across neurons, normalized by width. Results are shown for the last layer of the model, without SAE (“No SAE”, in black dashed line), and with SAE using different expansion factors (in straight lines,forε=1𝜀1\\varepsilon=1italic_ε = 1,forε=4𝜀4\\varepsilon=4italic_ε = 4andforε=16𝜀16\\varepsilon=16italic_ε = 16).",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x5.png",
                "caption": "Figure 5:Impact of sparsity factorK𝐾Kitalic_Kon Monosemanticity Scores across neurons extracted from SAEs trained on the last layer of the model with expansion factorε=1𝜀1\\varepsilon=1italic_ε = 1. Results are shown for different sparsity levels, with straight linesforK=1𝐾1K=1italic_K = 1,forK=10𝐾10K=10italic_K = 10,forK=20𝐾20K=20italic_K = 20, andforK=50𝐾50K=50italic_K = 50. Scores of the original neurons (“No SAE”, in black dashed lines) are added for comparison.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x6.png",
                "caption": "Figure 6:We steer the outputs of LLaVA by clamping the activation values of a chosen neuron, i.e. Neuron##\\##39 = pencil neuron, in the CLIP SAE. We observe that while initially the poem follows the instruction (the prompt + white image) strongly, the outputs become more and more influenced by the concept that this neuron represents with increasing intervention weightα𝛼\\alphaitalic_α, talking about attributes of pencil first, and then just the concept pencil. This shows that our interventions enable new capabilities for the unsupervised steering of these models.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x7.png",
                "caption": "Figure 7:Effects of neuron interventions on MLLM-generated scientific article titles. Steering magnitudes are categorized as “0”, “medium”, and “high” based on the intervention strength. The neurons are visualized with the highest activating images from which we deduce their associated concepts: “polka dots”, “shipwreck”, and “rainbow”.",
                "position": 668
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore details on steering",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x8.png",
                "caption": "Figure A1:LLaVA-like models can be steered towardsseeinga concept (e.g.panda) not present in the input image𝐱𝐱\\mathbf{x}bold_x. By attaching SAE after vision encoder and intervening on its neuron representing that concept, we effectively manipulate the LLM’s response. Such flexible and precise steering is possible thanks to the extensive concept dictionary identified through the SAE.",
                "position": 1332
            }
        ]
    },
    {
        "header": "Appendix BAdditional results on monosemanticity",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/x9.png",
                "caption": "Figure A2:MS in decreasing order across neurons. Results are shown for a layer without SAE (“No SAE”), and with SAE using different expansion factors (×1,×4\\times 1,\\times 4× 1 , × 4and×16absent16\\times 16× 16).",
                "position": 1358
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x10.png",
                "caption": "(a)Neurons of CLIP ViT-L evaluated with CLIP ViT-B as the image encoderE𝐸Eitalic_E",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x10.png",
                "caption": "(a)Neurons of CLIP ViT-L evaluated with CLIP ViT-B as the image encoderE𝐸Eitalic_E",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x11.png",
                "caption": "(b)Neurons of CLIP ViT-L evaluated with DINOv2 ViT-B as the image encoderE𝐸Eitalic_E",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x12.png",
                "caption": "(c)Neurons of SigLIP SoViT-400m evaluated with CLIP ViT-B as the image encoderE𝐸Eitalic_E",
                "position": 2232
            }
        ]
    },
    {
        "header": "Appendix CReconstruction of SAEs",
        "images": []
    },
    {
        "header": "Appendix DUniqueness of concepts",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02821/extracted/6333825/pencil_neuron.png",
                "caption": "Figure A4:Images highly activating the neuron we intervene on in Figure 6, which we manually labeled as “Pencil Neuron”.",
                "position": 2584
            },
            {
                "img": "https://arxiv.org/html/2504.02821/x13.png",
                "caption": "Figure A5:Qualitative examples of highest activating images for different neurons from high (left) to low (right) monosemanticity score. As the metric gets higher, highest activating images are more similar, illustrating the correlation with monosemanticity.",
                "position": 2587
            }
        ]
    },
    {
        "header": "Appendix EAdditional qualitative results",
        "images": []
    }
]