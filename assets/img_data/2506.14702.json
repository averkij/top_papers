[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14702/x1.png",
                "caption": "Figure 1:Tapping into Distributions:(above)illustrates the representation of various length buckets in the training distribution.(below)demonstrates the flexibility of the marker intervention on the mArena Hard test distribution. By modifying the<length_bucket>..</length_bucket>marker, the model can effectively tap into diverse training distributions, even for underrepresented length buckets.",
                "position": 162
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14702/x2.png",
                "caption": "(a)Baseline Model",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2506.14702/x2.png",
                "caption": "(a)Baseline Model",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2506.14702/x3.png",
                "caption": "(b)TreasureMarked",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2506.14702/x4.png",
                "caption": "(c)TreasureMarked(fixed)",
                "position": 219
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14702/x5.png",
                "caption": "Figure 3:Long tail domains benefit more from training markers:(left)Domain distribution in the training data used to fine-tune the model.(right)Improvements in win rates over the baseline against Gemma2-9B on both majority and minority subsets on Arena-Hard-Auto dataset[Li et al.,2024]. We group the test data into two sets of domains that have high (>5%) and low (< 5%) presence in training data.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2506.14702/x5.png",
                "caption": "",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2506.14702/x6.png",
                "caption": "",
                "position": 296
            }
        ]
    },
    {
        "header": "3Taxonomy for training time markers",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14702/x7.png",
                "caption": "Table 2:Comprehensive taxonomy for training time markers:Our taxonomy contains 13 categories shown with their descriptions and values.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14702/x7.png",
                "caption": "Figure 4:Levers for Controlling Quality:Changing the<quality>, <quality_bucket>markers at inference time provides control over generation quality with Win Rates (as measured by internal Reward Model) going from48.21%→56.5%→percent48.21percent56.548.21\\%\\rightarrow 56.5\\%48.21 % → 56.5 %over theBaselinemodel, demonstrating successful control over quality as annotated in the training data.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2506.14702/x8.png",
                "caption": "Figure 5:Improvement on the Long Tail for Code tasks:(left)Frequency of coding <task>s in the training dataset.(right)Despite being poorly represented in the training data,CodeRepairachieves a 14.1% relative improvement by leveraging targeted markers during inference further improving on the performance from theTreasureMarkedmodel with inferred markers.",
                "position": 679
            }
        ]
    },
    {
        "header": "5Key ablations and Discussion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACategories for Training Markers",
        "images": []
    },
    {
        "header": "Appendix BLLM Annotation",
        "images": []
    },
    {
        "header": "Appendix CMarker Annotationon-the-fly",
        "images": []
    }
]