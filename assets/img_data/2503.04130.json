[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04130/x1.png",
                "caption": "Figure 1:Open-Ended Video Understanding.We show STORM’s ability to handle free-form queries about complex long video scenes. By employing the Mamba-based temporal encoder to capture essential spatiotemporal cues while compressing redundant frame information, STORM enables efficient, accurate long-video understanding and outperforms existing methods on a wide range of video understanding tasks.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04130/x2.png",
                "caption": "Figure 2:Overview of STORM pipeline.We propose a Mamba-based temporal projector between the image encoder and the LLM. This projector bridges the gap between visual and language representation while injecting the temporal information into the tokens. The processed tokens, denoted as Summary Tokens in the figure, naturally capture temporal history, effectively summarizing the temporal dynamics of the video. This capability enables us to reduce the number of visual tokens for LLM processing without sacrificing essential information.",
                "position": 171
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x3.png",
                "caption": "Figure 3:Token Compression Strategies.This figure illustrates our token compression techniques: temporal average pooling (left), spatial average pooling (middle), and training-free temporal token sampling (right). These methods can be applied individually or in combination, depending on task requirements and computational budget constraints.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04130/extracted/6256771/figures/framewise_latency.png",
                "caption": "Figure 4:Model Efficiency and Effectiveness on Long Video Inputs.(left)Profiling results of token compression as the number of frames increases during inference.(middle)Profiling results for 256 input frames with different compression ratios on a single A100.(right)The accuracy of Video-MME (without subtitles) across different numbers of frames during inference. While STORM with test-time temporal sampling showed consistent performance improvements, both VILA and STORM without token compression demonstrated decreased performance beyond 64 frames.",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2503.04130/extracted/6256771/figures/framewise_latency.png",
                "caption": "",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x4.png",
                "caption": "",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x5.png",
                "caption": "",
                "position": 1011
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x6.png",
                "caption": "Figure 5:Qualitative Examples of STORM + T. Pooling.Our model effectively processes complex video content across various tasks requiring fine-grained temporal and visual understanding while reducing computational overhead through efficient token compression. The example videos can be found inour website.",
                "position": 1017
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04130/x7.png",
                "caption": "Figure 6:Breaking Down the STORM Architecture.We begin with a standard multimodal pipeline that uses a pixel-shuffle downsampling layer and an MLP projector. In STORM, we replace the MLP with a linear layer and introduce our Mamba-based temporal module on top. Since the Mamba layer propagates spatiotemporal information in each visual tokens, the model can then perform temporal and spatial token compression of these tokens before passing them to the LLM, allowing STORM to handle longer sequences more efficiently.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x8.png",
                "caption": "Figure 7:VideoMME Results by Task Categories.",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x9.png",
                "caption": "Figure 8:Latency Comparison: VILA vs STORM. The multi-modal projector in VILA is a 2-layer MLP, while it is the Mamba Temporal Module in STORM.",
                "position": 2921
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x10.png",
                "caption": "Figure 9:Effective Long Video Understanding.We compare STORM + Temporal Sampling with existing long video LLMs. Reults show that STORM delivers a more detailed and coherent summary, effectively capturing key events and transitions throughout the film. The example videos can be found inour website.",
                "position": 2992
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x11.png",
                "caption": "Figure 10:Importance of Long Video Context.We compare STORM with a 32-frame input to STORM + Temporal Sampling using a 128-frame input. Both configurations have negligible differences in computational cost; however, the latter encodes additional information into compressed tokens due to the extended frame sequence. The examples illustrate that processing more frames allows the model to capture richer temporal dynamics and contextual information. This leads to a stronger understanding of the video’s narrative, reduces information loss, and enhances the ability to reason through temporal events across the entire video. The example videos can be found inour website.",
                "position": 2995
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x12.png",
                "caption": "Figure 11:Showcase of Video Understanding Abilities in Various Task Categories.We provide additional examples to showcase model’s video understanding capabilities in different aspects. This is done by providing the models with open-ended queries that require the model to generate answers in raw text form without any given choices. Part 1. The example videos can be found inour website.",
                "position": 2998
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x13.png",
                "caption": "Figure 12:Showcase of Video Understanding Abilities in Various Task Categories.Continue 2. The example videos can be found inour website.",
                "position": 3001
            },
            {
                "img": "https://arxiv.org/html/2503.04130/x14.png",
                "caption": "Figure 13:Showcase of Video Understanding Abilities in Various Task Categories.Continue 3. The example videos can be found inour website.",
                "position": 3004
            }
        ]
    },
    {
        "header": "7Appendix",
        "images": []
    }
]