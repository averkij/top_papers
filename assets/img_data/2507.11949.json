[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/teaser_new.png",
                "caption": "Figure 1:We introduce a novel human motion generation task centered on spatial audio-driven human motion synthesis. Top row: We curate a novelSpatialAudio-Driven HumanMotion¬†(SAM) dataset, including diverse spatial audio signals and high-quality 3D human motion pairs. Bottom row: We develop a generative framework for humanMOtion generation driven bySPatialAudio (MOSPA) to produce high-quality, responsive human motion driven by spatial audio. We note that the motion generation results are both realistic and responsive, effectively capturing both the spatial and semantic features of spatial audio inputs.",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SAM Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_a_new.png",
                "caption": "(a)Look for sound source and approach upon hearing miaow at the left hand side.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_a_new.png",
                "caption": "(a)Look for sound source and approach upon hearing miaow at the left hand side.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_b_new.png",
                "caption": "(b)Step away with ears covered upon hearing crowd yelling at the back.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_c_new.png",
                "caption": "(c)Run away from the sound source upon hearing gunshot at the right hand side.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_d_new.png",
                "caption": "(d)Walk towards the sound source upon hearing insect noise at the left hand side.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_e_new.png",
                "caption": "(e)Start dancing upon hearing music in front of the character.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_f_new.png",
                "caption": "(f)Look for sound source upon hearing the phone ring at the left hand side.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/env.jpg",
                "caption": "iMocap environment.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/env.jpg",
                "caption": "iMocap environment.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/env.jpg",
                "caption": "iMocap environment.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/speakers.png",
                "caption": "iiThe speakers.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/dur_act.png",
                "caption": "Figure 4:Statistics of action duration in the dataset.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_microphone.png",
                "caption": "",
                "position": 516
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/pipeline.png",
                "caption": "Figure 5:The framework of MOSPA. We perform diffusion-based motion generation given spatial audio inputs. Specifically, Gaussian noise is added to the clean motion sampleùê±ùüésubscriptùê±0\\mathbf{x_{0}}bold_x start_POSTSUBSCRIPT bold_0 end_POSTSUBSCRIPT, generating a noisy motion vectorùê±ùê≠subscriptùê±ùê≠\\mathbf{x_{t}}bold_x start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT, modeled asq‚Å¢(ùê±ùê≠|ùê±ùê≠‚àíùüè)ùëûconditionalsubscriptùê±ùê≠subscriptùê±ùê≠1q(\\mathbf{x_{t}}|\\mathbf{x_{t-1}})italic_q ( bold_x start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT bold_t - bold_1 end_POSTSUBSCRIPT ). An encoder transformer then predicts the clean motion from the noisy motionùê±ùê≠subscriptùê±ùê≠\\mathbf{x_{t}}bold_x start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT, guided by extracted audio featuresùêöùêö\\mathbf{a}bold_a, sound source location (SSL)ùê¨ùê¨\\mathbf{s}bold_s, motion genregùëîgitalic_g, and timesteptùë°titalic_t.",
                "position": 549
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/comparison_new.png",
                "caption": "Figure 6:Qualitative comparison of state-of-the-art methods for the spatial audio-to-motion task. We visualize motion results from five cases. MOSPA produces high-quality movements that closely correspond to the input spatial audio. We provide Expected Motion as a description for reference.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/fig_user_study.png",
                "caption": "Figure 7:User study results. MOSPA outperforms other methods in intent alignment, motion quality, and similarity to ground truth. The bar chart shows the vote distribution across methods.",
                "position": 847
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/ood.png",
                "caption": "Figure 8:Test of MOSPA on out-of-distribution spatial audios. Descriptions of motions are provided for reference.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/robot.png",
                "caption": "Figure 9:Spatial audio-driven physically simulated humanoid robot control based onhe2025asap. Descriptions of expected motion are provided for reference.",
                "position": 885
            }
        ]
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ADataset Statistics",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/extracted/6626965/Figs/extractor.png",
                "caption": "Figure B10:The feature extractor framework consists of a motion autoencoder co-trained with two Bi-GRU modules functioning as feature extractors (condition encoder and motion encoder). A reconstruction loss is applied between the ground-truth motionsùê±ùê±\\mathbf{x}bold_xand the decoded motionsùê±‚Ä≤superscriptùê±‚Ä≤\\mathbf{x^{\\prime}}bold_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTproduced by the motion autoencoder. Additionally, a contrastive loss operates on the extracted condition featuresùêúùêú\\mathbf{c}bold_cand motion featuresùê¶ùê¶\\mathbf{m}bold_m.",
                "position": 1199
            }
        ]
    },
    {
        "header": "Appendix CUser Study Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11949/x1.png",
                "caption": "Figure C11:Screenshot of the user study interface.",
                "position": 1292
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]