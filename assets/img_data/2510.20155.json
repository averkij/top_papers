[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20155/figures/teaser/teaser_mix.png",
                "caption": "Figure 1:We presentPartNeXt, a next-generation dataset tailored for fine-grained, hierarchically structured 3D part understanding.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Data Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20155/x1.png",
                "caption": "Figure 2:Illustration of our annotation interface. The example shows a microwave containing an internal tray. The dual-panel layout allows annotators to first label external parts such as the “door” (as shown in the right panel with already segmented meshes), and then proceed to annotate internal components like the “tray” (visible in the unsegmented mesh in the left panel). This design effectively mitigates occlusion issues during annotation.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x2.png",
                "caption": "Figure 3:PartNeXt dataset. We visualize example shapes with fine-grained part annotations for the 50 object categories in PartNeXt.",
                "position": 308
            }
        ]
    },
    {
        "header": "4BenchMark and Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20155/x3.png",
                "caption": "Figure 4:Part Segmentation Results on PartNeXt. PartField struggles to separate connected regions, SAMesh excels at fine-grained segmentation but over-segments, while SAMPart3D lacks continuity in weak textures and granularity control.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x4.png",
                "caption": "Figure 5:Representative prompt–response pairs used to evaluate 3D part-level understanding.(a) Part Counting: the model is requested to enumerate the number of legs in a chair. (b) Part Classification: the model must name the part highlighted in red within the point-cloud bed. (c) Part Grounding: the model is asked to localize the “Shelf” of a bookcase by outputting the eight corner coordinates of its bounding box.",
                "position": 656
            }
        ]
    },
    {
        "header": "5Limitations and Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetail of Annotation Platform",
        "images": []
    },
    {
        "header": "Appendix BDataset Statistic",
        "images": []
    },
    {
        "header": "Appendix CComparison with Recent 3D Part Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20155/x5.png",
                "caption": "Figure 6:Visualization of PartNet and PartNeXt results. Since PartNet uses remeshing to obtain finer-grained parts, the mesh undergoes deformation, lacks texture, and requires manually drawn cutting lines after remeshing to achieve segmentation. As a result, the boundaries of the parts are often not smooth.",
                "position": 2095
            }
        ]
    },
    {
        "header": "Appendix DDetail of Segmentation BenchMark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20155/x6.png",
                "caption": "Figure 7:Visualization of Prompted Input for Part Classification Task, the target part is highlighted in red as 3d prompt.",
                "position": 2644
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x7.png",
                "caption": "Figure 8:Visualization of ShapeLLM grounding result, the red box is the ground truth, while the green box is predict result.",
                "position": 2654
            },
            {
                "img": "https://arxiv.org/html/2510.20155/figures/supp/supp_gallery_Page1.png",
                "caption": "Figure 10:Visualization of our Annotation Results.",
                "position": 2808
            },
            {
                "img": "https://arxiv.org/html/2510.20155/figures/supp/supp_gallery_Page2.png",
                "caption": "Figure 11:Visualization of our Annotation Results.",
                "position": 2811
            },
            {
                "img": "https://arxiv.org/html/2510.20155/figures/supp/supp_gallery_Page3.png",
                "caption": "Figure 12:Visualization of our Annotation Results.",
                "position": 2814
            },
            {
                "img": "https://arxiv.org/html/2510.20155/figures/supp/supp_gallery_Page4.png",
                "caption": "Figure 13:Visualization of our Annotation Results.",
                "position": 2818
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x8.png",
                "caption": "Figure 14:Visualization of our Predefined Hierarchy",
                "position": 2821
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x9.png",
                "caption": "Figure 15:Visualization of our Predefined Hierarchy",
                "position": 2824
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x10.png",
                "caption": "Figure 16:Visualization of our Predefined Hierarchy",
                "position": 2827
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x11.png",
                "caption": "Figure 17:Visualization of our Predefined Hierarchy",
                "position": 2830
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x12.png",
                "caption": "Figure 18:Visualization of our Predefined Hierarchy",
                "position": 2833
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x13.png",
                "caption": "Figure 19:Visualization of our Predefined Hierarchy",
                "position": 2836
            },
            {
                "img": "https://arxiv.org/html/2510.20155/x14.png",
                "caption": "Figure 20:Visualization of our Predefined Hierarchy",
                "position": 2839
            }
        ]
    },
    {
        "header": "Appendix EDetail of LLM BenchMark",
        "images": []
    }
]