[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17022/x1.png",
                "caption": "Figure 1:GoT-R1 enhances visual generation through reinforcement learning. This figure demonstrates the improvement from a GoT-finetuned model (left) to the RL-trained GoT-R1 model (right). The model before RL generates spatially misaligned reasoning process. The RL process enhances the model‚Äôs semantic-spatial reasoning capabilities, as demonstrated by its Generation Chain-of-Thought, leading to a generated image that is more closely aligned with the prompt.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17022/x2.png",
                "caption": "Figure 2:The GoT-R1 framework illustrating the reinforcement learning process with Group Relative Policy Optimization (GRPO).Left:\nOverview of the candidate sampling and initial evaluation stage, where diverse reasoning chains (GoT) and corresponding image tokens are generated from an input prompt, with an MLLM-based reward model providing preliminary scoring.Right: Detailed illustration of how MLLM-based rewards and advantages facilitate model updates via GRPO.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2505.17022/x3.png",
                "caption": "Figure 3:Overview of our MLLM-based dual-stage multi-dimensional reward framework. The diagram illustrates MLLM-based rewards assessing the intermediate GoT‚Äôs semantic and spatial fidelity to the prompt, as well as the final image‚Äôs alignment with both the prompt and the GoT.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2505.17022/x4.png",
                "caption": "Figure 4:Prompt-Reasoning Spatial RewardRs‚Å¢p‚Å¢asubscriptùëÖùë†ùëùùëéR_{spa}italic_R start_POSTSUBSCRIPT italic_s italic_p italic_a end_POSTSUBSCRIPTprocess. For robust spatial evaluation, the MLLM assesses bounding boxes rendered on an image from the GoT‚Äôs textual coordinates, rather than processing the coordinates directly as text.",
                "position": 281
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17022/x5.png",
                "caption": "Figure 5:Qualitative comparison among the base model Janus-Pro-7B, the GoT-finetuned checkpoint Janus-Pro-7B-GoT, and our GRPO-enhanced model GoT-R1-7B. Our model demonstrates superior performance on prompt alignment and image quality.",
                "position": 496
            }
        ]
    },
    {
        "header": "5Conclusion and Disscussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AQualitative Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17022/x6.png",
                "caption": "Figure 6:Samples of text-to-image generation by Janus-Pro-7B, Janus-Pro-GoT-7B and GoT-R1-7B.",
                "position": 1525
            }
        ]
    },
    {
        "header": "Appendix BQuantitative Analysis",
        "images": []
    },
    {
        "header": "Appendix CMLLM-based Reward Evaluation Prompts",
        "images": []
    }
]