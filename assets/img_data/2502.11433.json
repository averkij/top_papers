[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11433/x1.png",
                "caption": "Figure 1:A high-level overview of our LLM-based reinforcement learning setup for financial trading. The environment provides the current statestsubscript𝑠𝑡s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. A prompt containing task details, the action space, and the current state is fed into the LLM, which outputs a trading actionatsubscript𝑎𝑡a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The action is executed in the environment, yielding a rewardr⁢(st,at)𝑟subscript𝑠𝑡subscript𝑎𝑡r(s_{t},a_{t})italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )and next statest+1subscript𝑠𝑡1s_{t+1}italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. The log-likelihoodlogπθ⁡(at|lang⁢(st))subscriptsubscript𝜋𝜃conditionalsubscript𝑎𝑡langsubscript𝑠𝑡\\log_{\\pi_{\\theta}}(a_{t}|\\texttt{lang}(s_{t}))roman_log start_POSTSUBSCRIPT italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | lang ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )is then leveraged by a policy gradient method (e.g., PPO), with experience tuples stored in a replay buffer for iterative updates.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11433/x2.png",
                "caption": "Figure 2:TheFLAG-Traderpipeline for financial trading, utilizing an LLM-based actor-critic architecture. The LLM consists offrozen base layersθfrozensubscript𝜃frozen\\theta_{\\texttt{frozen}}italic_θ start_POSTSUBSCRIPT frozen end_POSTSUBSCRIPTthat retain pre-trained knowledge andtrainable top layersθtrainsubscript𝜃train\\theta_{\\texttt{train}}italic_θ start_POSTSUBSCRIPT train end_POSTSUBSCRIPTfor financial decision-making. Both thePolicy_NetandValue_Netshare these trainable layers while maintaining separatepolicy headθPsubscript𝜃𝑃\\theta_{P}italic_θ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPTandvalue headθVsubscript𝜃𝑉\\theta_{V}italic_θ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT, which are updated by policy gradient method.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Problem Statement",
        "images": []
    },
    {
        "header": "4FLAG-Trader",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11433/x3.png",
                "caption": "Figure 3:The format of input prompt. It contains the task description, the legible action set, the current state description, and the output action format.",
                "position": 321
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations and Potential Risk",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Algorithmic Details:FLAG-Traderwith PPO",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Details",
        "images": []
    }
]