[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11433/x1.png",
                "caption": "Figure 1:A high-level overview of our LLM-based reinforcement learning setup for financial trading. The environment provides the current statestsubscriptğ‘ ğ‘¡s_{t}italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. A prompt containing task details, the action space, and the current state is fed into the LLM, which outputs a trading actionatsubscriptğ‘ğ‘¡a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The action is executed in the environment, yielding a rewardrâ¢(st,at)ğ‘Ÿsubscriptğ‘ ğ‘¡subscriptğ‘ğ‘¡r(s_{t},a_{t})italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )and next statest+1subscriptğ‘ ğ‘¡1s_{t+1}italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. The log-likelihoodlogÏ€Î¸â¡(at|langâ¢(st))subscriptsubscriptğœ‹ğœƒconditionalsubscriptğ‘ğ‘¡langsubscriptğ‘ ğ‘¡\\log_{\\pi_{\\theta}}(a_{t}|\\texttt{lang}(s_{t}))roman_log start_POSTSUBSCRIPT italic_Ï€ start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | lang ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )is then leveraged by a policy gradient method (e.g., PPO), with experience tuples stored in a replay buffer for iterative updates.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11433/x2.png",
                "caption": "Figure 2:TheFLAG-Traderpipeline for financial trading, utilizing an LLM-based actor-critic architecture. The LLM consists offrozen base layersÎ¸frozensubscriptğœƒfrozen\\theta_{\\texttt{frozen}}italic_Î¸ start_POSTSUBSCRIPT frozen end_POSTSUBSCRIPTthat retain pre-trained knowledge andtrainable top layersÎ¸trainsubscriptğœƒtrain\\theta_{\\texttt{train}}italic_Î¸ start_POSTSUBSCRIPT train end_POSTSUBSCRIPTfor financial decision-making. Both thePolicy_NetandValue_Netshare these trainable layers while maintaining separatepolicy headÎ¸Psubscriptğœƒğ‘ƒ\\theta_{P}italic_Î¸ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPTandvalue headÎ¸Vsubscriptğœƒğ‘‰\\theta_{V}italic_Î¸ start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT, which are updated by policy gradient method.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Problem Statement",
        "images": []
    },
    {
        "header": "4FLAG-Trader",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11433/x3.png",
                "caption": "Figure 3:The format of input prompt. It contains the task description, the legible action set, the current state description, and the output action format.",
                "position": 321
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations and Potential Risk",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Algorithmic Details:FLAG-Traderwith PPO",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Details",
        "images": []
    }
]