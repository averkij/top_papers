[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08555/x1.png",
                "caption": "Figure 1:VideoCanvas: Arbitrary Spatio-Temporal Video Completion.Given any conditions (frames or patches, outlined inred), the model fills in the remaininggrayregions to generate coherent, high-quality videos.\nThis unified formulation subsumes various tasks such as Any-Timestep-Patch/Image-to-Video, In/Outpainting, Camera Control, and Cross-scene Video Transitions, all in a zero-shot manner.\nMore results are available on ourProject Page.Best viewed zoomed in.",
                "position": 141
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08555/x2.png",
                "caption": "Figure 2:Core challenge and solution for pixel-frame-aware conditioning.(a)Causal VAEs create temporal ambiguity by mapping frames to a single latent. We propose a hybrid solution combining Spatial Padding with Temporal RoPE Interpolation.(b)We show how competing paradigms are ill-suited for fine-grained control, while ourICCapproach provides an effective solution.",
                "position": 176
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08555/x3.png",
                "caption": "Figure 3:The pipeline of VideoCanvas, which fine-tunes a base T2V model for arbitrary spatio-temporal control with zero new parameters.\nOur framework leverages the In-Context Conditioning (ICC) paradigm. After preparing conditional patches with zero-padding for spatial placement, we use independent VAE encoding for temporal decoupling. Our RoPE Interpolation then aligns each discrete token by mapping its source pixel-frame indexYYto a fractional positionY/NY/N, whereNNis the VAE temporal stride (here,N=4N=4). As illustrated, this maps Frame4141to position10.2510.25. This strategy enables fine-grained control without architectural changes.",
                "position": 287
            }
        ]
    },
    {
        "header": "4VideoCanvasBench",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08555/figs/ablation/psnr_spike.png",
                "caption": "Figure 4:Impact of Temporal RoPE Interpolation.Per-frame PSNR for single-frame I2V with targets22/33/44.\nOur method (red, solid) peaks exactly at the target frame.\n“w/o RoPE Interpolation” (blue, dashed) misaligns,\n“Latent-space Condition” (orange, dot-dashed) collapses motion,\nand “Pixel-space Padding” (green, dotted) is precise but degraded.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x4.png",
                "caption": "",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x5.png",
                "caption": "Figure 6:Qualitative comparison.Our method preserves high quality and smooth transitions.",
                "position": 704
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AIntroduction of the Base Text-to-Video Generation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08555/x6.png",
                "caption": "Figure S7:Overview of the base text-to-video generation model.",
                "position": 1440
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CVideoCanvasBench Construction Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08555/x7.png",
                "caption": "Figure S8:Robustness of Hybrid Video VAEs to Spatial Padding. This figure demonstrates that both the Hunyuan I2V and CogVideo VAE models can tolerate spatial zero-padding well. When reconstructing images with large zero-padded regions (middle row), the PSNR values are only slightly lower than those of the full, unpadded images (top row). Crucially, the original content within the non-zero regions is faithfully preserved, while the padded areas remain visually neutral. This empirical evidence confirms that our spatial conditioning strategy, which relies on zero-padding before VAE encoding, is stable and practical, enabling precise spatial control without degrading the quality of the conditioned content.",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x8.png",
                "caption": "Figure S9:Vulnerability of Hybrid Video VAEs to Temporal Padding. This figure contrasts the robustness observed in spatial padding. When applying temporal zero-padding (where only specific frames contain content), both VAE models suffer a relatively great drop in reconstruction quality. The PSNR values for the padded reconstructions (bottom rows) are much lower than those of the full video (top row), demonstrating a degradation in fidelity. The reconstructed frames exhibit noticeable color shifts, and loss of detail, highlighting that the VAE cannot handle such distributionally mismatched inputs. This mode underscores why direct temporal zero-padding is ineffective and validates the necessity of our Temporal RoPE Interpolation strategy, which avoids this problem by operating at the latent token level with fractional positions.",
                "position": 1829
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x9.png",
                "caption": "Figure S10:Results on Any-timestamp Patches to Videos.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x10.png",
                "caption": "Figure S11:Results on Any-timestamp Images to Videos.",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x11.png",
                "caption": "Figure S12:Results on Video Transition.",
                "position": 2043
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x12.png",
                "caption": "Figure S13:Results on Video Inpainting and Outpainting.The red dashed contours indicate the regions that are subject to inpainting or outpainting.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x13.png",
                "caption": "Figure S14:Results on Video Extension and Seamless Looping.The example showcases a video extended toover 1,000 framesby first applying our video extension capability and then generating a seamless transition back to the initial state. This highlights our model’s ability to maintain temporal consistency and visual quality over a long generation horizon without suffering from quality degradation or motion collapse.",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x14.png",
                "caption": "Figure S15:Results on Video Camera Control.The examples showcase emulated camera effects such as zoom and pan, achieved by progressively translating and scaling content on the spatio-temporal canvas.",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2510.08555/x15.png",
                "caption": "Figure S16:Comparisons with baseline paradigms.",
                "position": 2060
            }
        ]
    },
    {
        "header": "Appendix EApplications and Qualitative Results",
        "images": []
    }
]