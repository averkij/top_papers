[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3The Intelligence Crossover",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03276/x1.png",
                "caption": "Figure 1:Diffusion vs. AR with various unique data budgets.All models are trained on 96B total tokens (including repetition), varying the unique tokens from 0.5B to 96B. In the bottom \"Put Together\" row, a darker color means a smaller unique data size, i.e., trained for more epochs.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2511.03276/x2.png",
                "caption": "Figure 2:Diffusion vs. AR with various data qualities.All models are trained on 1B unique tokens for 96 epochs. In the bottom \"Put Together\" row, a darker color means a higher data quality.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2511.03276/x3.png",
                "caption": "Figure 3:Diffusion vs. AR with various model sizes ranging from 1B to 8B.All models are trained on 1B unique tokens for 96 epochs. In the bottom \"Put Together\" row, a darker color means a larger model size.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2511.03276/x4.png",
                "caption": "Figure 4:Diffusion vs. AR with various model sparsities.All models are trained on 1B unique tokens for 96 epochs. Both 8B-A1B and 8B1A denote 8B total parameters and 1B activated parameters, parameter-matching the 8B dense and FLOPs-matching the 1B dense.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2511.03276/x5.png",
                "caption": "Figure 5:Injecting noise (random masking) to the AR model’s inputs helps in data-constrained settings but does not beat diffusion models.All models shown in this figure are 1B parameters trained on 1B unique tokens for 96 epochs.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2511.03276/x6.png",
                "caption": "Figure 6:Injecting noise to AR model’s parameters (dropout) helps in data-constrained settings but does not beat diffusion models.All models shown in this figure are 1B parameters trained on 1B unique tokens for 96 epochs.",
                "position": 445
            }
        ]
    },
    {
        "header": "4Scaling Crossovers to Trillion-Level Total Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03276/x7.png",
                "caption": "Figure 7:1.7B AR vs. DLM trained on 10B unique code tokens for≈\\approx150 epochs.On downstream evaluations, we observe early crossovers where DLMs surpass AR models. This provides another evidence that crossovers emerge at larger scales under limited unique data. Different crossover timings are observed on two additional coding benchmarks (Figure13).",
                "position": 456
            }
        ]
    },
    {
        "header": "5High Validation Loss≠\\neqDegraded Intelligence",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03276/x8.png",
                "caption": "Figure 8:When models get “overfit” on pre-training validation sets, their performance on down-stream evaluations doesn’t necessarily drop, and may keep improving till the end of training.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2511.03276/x9.png",
                "caption": "Figure 9:An illustration of why the models’ performance keeps growing after they overfit on pre-training validation sets (indicated with dashed line). NLL: Negative log-likelihood on the ground-truth and other options of multiple-choice evals (NLLs on other options are averaged).Δ\\DeltaNLL: The differences between the NLLs on ground-truth and other options, which keeps growing. This is a 1B autoregressive model trained on 1.5B unique tokens, 64 epochs, on both out-of-domain and in-domain pre-training data.",
                "position": 478
            }
        ]
    },
    {
        "header": "6Diffusion Language Models also Overfit the Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03276/figures/scaled_diffusion_480epochs.jpg",
                "caption": "Figure 10:The 1B-parameter DLM—trained solely on the original 1B pre-training tokens for 480 epochs—achieves  56% accuracy on HellaSwag and  33% on MMLU.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2511.03276/figures/combined_loss_vs_epochs.png",
                "caption": "Figure 11:Validation loss curves for models trained with various sizes and unique data budgets, repeating up to 1000 epochs. Diffusion language models will also overfit. The more unique data we train on, the later it overfits; the larger model we train, the earlier it overfits.",
                "position": 497
            }
        ]
    },
    {
        "header": "7Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03276/x10.png",
                "caption": "Figure 12:(left) The diffusion language models are approximated to consume >100 time more FLOPs than AR counterparts to achieve their full potential in training (where the peak performance is usually much greater than AR). (middle) The theoretical inference FLOPs controlling the sampling steps to be equal to the sequence length. The total inference FLOPs have a power-law relationship with the generation sequence length for both. (right) The theoretical inference FLOPs controlling the generation sequence length, where sampling 512 steps from an AR model with KV cache≈\\approxsampling 1 step from the masked diffusion model.",
                "position": 523
            }
        ]
    },
    {
        "header": "8Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.03276/x11.png",
                "caption": "Figure 13:1.7B AR vs. DLM trained 10B unique code tokens for≈\\approx150 epochs.On HumanEval and HumanEval +, the crossover time is delayed and the two curves meet in the end, where the DLM didn’t see diminish return. We suspect this is a direct result of the zero-shot setting.",
                "position": 677
            }
        ]
    },
    {
        "header": "9Acknowledgment",
        "images": []
    }
]