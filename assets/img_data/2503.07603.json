[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07603/x1.png",
                "caption": "Figure 1:An overview of our VLM pre-training data recipe. We investigate data mixes and design choices for text-only pre-training, image-text pre-training, and fine-tuning.\nNote that while we depict \"LLM Pre-training\" and \"Image-text Pre-training\" as two separate steps in this diagram, in practice, we continuously transition from the first stage to the second.",
                "position": 129
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07603/x2.png",
                "caption": "Figure 2:The commonly used framework we apply to add vision capabilities to a transformer model.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x3.png",
                "caption": "Figure 3:Representation of the different learning rate schedules used for our experiments. ‘Main schedule’ corresponds to the learning rate for the initial, text-only pretraining. Other colored schedules are the ones used for image-text training and extend over 28B tokens each. They have been upscaled and appear as extending over 280B tokens for readability.",
                "position": 530
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07603/x4.png",
                "caption": "Figure 4:Varying the length of text-only pre-training.We analyze the impact of adding image data after varying amounts of text-only pre-training, showing results on vision benchmarks (green) and text benchmarks (blue). On the left, we show results across a suite of vision and text benchmarks; on the right, we plot two common benchmarks, VQA-v2 and ARC-easy. Introducing images at around 80% of the way through training performs best, maintaining high vision and text task performance. Note: The points at 100% are marked with hollow circles to highlight that they are trained with a different learning rate schedule, as shown in Figure3",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x5.png",
                "caption": "",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x6.png",
                "caption": "Figure 5:Varying the ratio of image to text data, after some text-only pretraining.We analyze the impact of the ratio of image to text data in pre-training, after the model has seen text-only data for most of pre-training (80%).\nUnlike when training from scratch (Figure6), we find that adding vision data significantly helps vision performance, while maintaining high text accuracy.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x7.png",
                "caption": "",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x8.png",
                "caption": "Figure 6:Varying the ratio of image to text data, when training from scratch.We analyze the impact of the image-text ratio in pre-training from scratch without any language-only pre-training. Perhaps surprisingly, when training from scratch, adding vision data consistently hurts both vision and text performance, suggesting a period of language-only training early on is important for VLMs.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x9.png",
                "caption": "",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x10.png",
                "caption": "Figure 7:Varying the proportion of instruction tuning data in the image mix.Is including instruction tuning data during pre-training is helpful for VLMs? Surprisingly, we find that adding this data to pre-trainingharmsperformance. We hypothesize that this may be due to overfitting, or because mixing instruction tuning data with image-caption pairs degrades learning at this scale.",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x11.png",
                "caption": "",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x12.png",
                "caption": "Figure 8:Varying the number of fine-tuning epochs.We find that fine-tuning for 2-4 epochs after pre-training performs best for vision tasks, with 2 epochs being a sweet spot for maintaining text performance while achieving high vision performance.",
                "position": 855
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x13.png",
                "caption": "",
                "position": 858
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AText-only pre-training hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BImage-text pre-training hyperparameters",
        "images": []
    },
    {
        "header": "Appendix CImage Encoding Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DFine-tuning hyperparameters",
        "images": []
    },
    {
        "header": "Appendix EEvaluation benchmark descriptions",
        "images": []
    },
    {
        "header": "Appendix FDetailed results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07603/x14.png",
                "caption": "Figure 9:Evolution of the performance of the 1b model on vision benchmarks and text benchmarks as functions of the text-only pre-training completion.",
                "position": 1958
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x15.png",
                "caption": "Figure 10:Evolution of the performance of the 1b model on vision benchmarks and text benchmarks as functions of the ratio of image caption data in the image pre-training phase.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x16.png",
                "caption": "Figure 11:Evolution of the performance of the 1b model on vision benchmarks and text benchmarks as functions of the number of fine-tuning epochs.",
                "position": 1980
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x17.png",
                "caption": "Figure 12:Evolution of the performance of the 1b model trained from scratch on vision benchmarks and text benchmarks as functions of the ratio of image caption data in the image pre-training phase.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2503.07603/x18.png",
                "caption": "Figure 13:Evolution of the performance of the 1b model on vision benchmarks and text benchmarks as functions of the ratio of image instruction tuning in the image-text pre-training phase.",
                "position": 2002
            }
        ]
    },
    {
        "header": "Appendix GTraining for higher Chinchilla scales",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07603/extracted/6268316/figures/tokenmults.png",
                "caption": "Figure 14:We train for{{\\{{1,2,4}}\\}}times the Chinchilla optimal number of tokens for 79M models. The above plots are taken for a 60% checkpoint, with VQA-v2 as the y-axis. Plots from other checkpoints look similar.",
                "position": 2014
            }
        ]
    },
    {
        "header": "Appendix HLarger model scales (7B)",
        "images": []
    },
    {
        "header": "Appendix IWhat do the results look like if we unfreeze all the weights?",
        "images": []
    },
    {
        "header": "Appendix JAblations",
        "images": []
    },
    {
        "header": "Appendix KRandom Seeds",
        "images": []
    },
    {
        "header": "Appendix LLimitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix MBenchmark Results",
        "images": []
    },
    {
        "header": "Appendix NComparison with Prior SOTA Models",
        "images": []
    }
]