[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/HF_documentation.png",
                "caption": "Figure 1:Hugging Face Documentation.We analyze the model cards of1.2‚Å¢M1.2ùëÄ1.2M1.2 italic_MHugging Face models. We discover that the majority of models are either undocumented or poorly documented.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2502.09619/x1.png",
                "caption": "Figure 2:Classification Model Search.We present a new task of Classification Model Search, where the goal is to find classifiers that can recognize a target concept. Concretely, given an input prompt, such as ‚ÄúDog‚Äù, we wish to retrieve all classifiers that one of their classes is ‚ÄúDog‚Äù. The search space is a large model repository, that contains many models and concepts to search from. The retrieved models can replace model training, increasing accuracy, reducing cost and environmental impact.",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/logit_level_descriptors_v3.png",
                "caption": "Figure 3:ProbeLog Descriptors.Our method generates a descriptor for individual output dimensions (logits) of models. First, we sample and a set of inputs (e.g., from the COCO dataset), and fix them as our set of probes. Then, to create a new ProbeLog descriptor for a model logit, we feed the set of ordered probes nto the model and observe their outputs. Finally, we take all values of the logit we wish to represent, and normalize them. We use this representation to accurately retrieve model logits associated with similar concepts. In Fig.5, we extend this idea to zero-shot concept descriptors.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/cifar_GT.png",
                "caption": "(a)GT",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/cifar_COCO_squared_cosine_sims.png",
                "caption": "",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/cifar_squared_cosine_sims.png",
                "caption": "",
                "position": 232
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/zero_shot_probelog_v3.png",
                "caption": "Figure 5:Text-Aligned ProbeLog Representation.We present a method to create ProbeLog-like representations for text prompts. We encode and store each of our ordered probes using the CLIP image encoder. At inference time, we embed the target text prompt, and compute its similarity with respect to the stored probe representations. We demonstrate that by normalizing this zero-shot ProbeLog descriptor, we can effectively search descriptors of real model logits, accurately retrieving similar concepts.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/Collaborative_Probing_diagram_v2.png",
                "caption": "Figure 6:Collaborative Probing.We pass a random subset of probes through each model in the repository to obtain partial logit representations. By performing factorization based matrix imputation we can complete the missing information. This saves a substantial part of the computational resources needed to build our repository‚Äôs logit descriptors gallery.",
                "position": 320
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/collaborative_probing_v2.png",
                "caption": "Figure 7:Collaborative Probing.We test our method using collaborative probing on the text‚Üí‚Üí\\rightarrow‚ÜíINet-Hub retrieval task. While the full size of the dataset is8,00080008,0008 , 000COCO probes, we show cases where each model is probed by less than15%percent1515\\%15 %of these probes. We can see that for the limited probe regime, collaborative probing can improve accuracy by as much as2√ó2\\times2 √ó.",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/coco_inet_n_probes.png",
                "caption": "Figure 8:Number of Probes.We test our zero-shot retrieval approach on INet-Hub with increasing numbers of probes. While more probes lead to higher accuracy, the gains are diminishing.",
                "position": 704
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AINet-Hub Dataset Details",
        "images": []
    },
    {
        "header": "Appendix BHF-Hub Dataset Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]