[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x1.png",
                "caption": "Figure 1:Top:Performance of base LLMs (orange) on the Belebele benchmark across 108 languages, plotted against their data ratios in the CulturaX (blue).Bottom:Bilingual data volume (million sentence pairs) from the OPUS corpus for 60 languages in our study, covering English-centric (blue) and Chinese-centric (orange) directions. Languages are grouped intohigh-,medium-, andlow-resource tiers.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2511.07003/x1.png",
                "caption": "",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2511.07003/x2.png",
                "caption": "",
                "position": 305
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x3.png",
                "caption": "Figure 2:An overview of our methodology for LMT. The pipeline consists of two main stages: a hybrid data curation process (top) to build the training corpus, and a two-stage adaptation (bottom) involving CPT and SFT.",
                "position": 318
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x4.png",
                "caption": "Figure 3:Examples of the three prompt formats for theCPTandSFTstages of LMT adaptation. Theunderlinedtext indicates the part used for loss computation during training.",
                "position": 401
            }
        ]
    },
    {
        "header": "4The Pitfall of Directional Degeneration",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x5.png",
                "caption": "Figure 4:The impact of the Strategic Downsampling proportion (pp). Dashed lines represent the use of separate, non-symmetric data for the X→\\toEn/Zh directions.",
                "position": 482
            }
        ]
    },
    {
        "header": "5Results and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x6.png",
                "caption": "Figure 5:Ablation study on the impact of each component: Strategic Downsampling (SD), Continued Pre-training (CPT), and Parallel Multilingual Prompting (PMP). The annotated values quantify the performance gain of each component over the preceding one.",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2511.07003/x7.png",
                "caption": "Figure 6:Analysis of the Parallel Multilingual Prompting (PMP).Left:Comparison of different inference-time strategies.Right:Zero-shot transfer gains from PMP training. The bars show the performance difference between our final model and the model trained without PMP.",
                "position": 1114
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Appendix ADirectional Degeneration Across Models and Languages",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x8.png",
                "caption": "Figure 7:Generality analysis of Directional Degeneration across multiple foundation models. The x-axis represents the Strategic Downsampling proportion (pp) for X→\\toEn/Zh directions.",
                "position": 1170
            },
            {
                "img": "https://arxiv.org/html/2511.07003/x9.png",
                "caption": "Figure 8:The impact of multilingual scale (number of languages) on the Directional Degeneration. The x-axis represents the Strategic Downsampling proportion (pp) for X→\\toEn/Zh directions",
                "position": 1173
            }
        ]
    },
    {
        "header": "Appendix BQuality Distributions in the CPT Bilingual Corpus",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07003/x10.png",
                "caption": "Figure 9:Performance improvements brought by Continued Pre-training (CPT).\nLanguages are grouped by resource level:high,medium, andlow.\nTheorange portionof the bar shows the performance of the model without CPT (Base+SFT+SD), while the total height of the bar represents the performance after including the CPT stage (Base+CPT+SFT+SD). Theblue portionvisually represents the performance gain (Δ\\DeltaCOMET) contributed by CPT.",
                "position": 1210
            },
            {
                "img": "https://arxiv.org/html/2511.07003/x11.png",
                "caption": "Figure 10:COMETKiwi score distributions for bilingual sentence pairs (En-X) are shown as histograms. Vertical lines indicate quality thresholds at 0.6 (red), 0.7 (green), and 0.8 (magenta), with the legend specifying the number and proportion of sentence pairs exceeding each threshold. Some language pairs are excluded due to COMETKiwi’s limited language support.",
                "position": 6297
            },
            {
                "img": "https://arxiv.org/html/2511.07003/x12.png",
                "caption": "Figure 11:COMETKiwi score distributions for bilingual sentence pairs (Zh-X) are shown as histograms. Vertical lines indicate quality thresholds at 0.6 (red), 0.7 (green), and 0.8 (magenta), with the legend specifying the number and proportion of sentence pairs exceeding each threshold. Some language pairs are excluded due to COMETKiwi’s limited language support.",
                "position": 6304
            }
        ]
    },
    {
        "header": "Appendix CCPT Gains Across Languages",
        "images": []
    }
]