[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/x1.png",
                "caption": "Figure 1:Token Accessibility and Dynamic Computation.(a)Token accessibility levels from token isolation to cross-sample interaction. Colors represent tokens in different samples,tisubscriptùë°ùëñt_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTindicates noise levels.(b)Performance-accessibility analysis across architectures.(c)Computational dynamics during diffusion sampling, showing adaptive computation from noise to image.(d)Class-wise computation allocation from hard (technical diagrams) to easy (natural photos) tasks. Results from DiffMoE-L-E16-Flow (700K).",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/x2.png",
                "caption": "Figure 2:DiffMoE Architecture Overview.DiffMoE flattens tokens into a batch-level global token pool, where each expert maintains a fixed training capacity ofCtrainEi=1superscriptsubscriptùê∂trainsubscriptùê∏ùëñ1C_{\\text{train}}^{E_{i}}=1italic_C start_POSTSUBSCRIPT train end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT = 1. During inference, a dynamic capacity predictor adaptively routes tokens across different sampling steps and conditions. Different colors denote tokens from distinct samples, whiletisubscriptùë°ùëñt_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTrepresents corresponding noise levels.",
                "position": 174
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/Loss_Comparison_Ksteps_Smoothed_L-Flow.png",
                "caption": "Figure 3:Training Loss Curves of Different Flow-based Models.DiffMoE with batch-level global token pool achieves consistently lower diffusion losses than baselines without batch-level global token pool.",
                "position": 289
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/1-Baseline_Flow.png",
                "caption": "Figure 4:Comparisons with the Baseline Models.We compare TC, EC, and Dense Models. DiffMoE-L-E16-Flow even surpasses the DenseDiT-XL-Flow (1.5x params) by achieving the best quality (14.41 FID50K w/o CFG at 700K). The results of the DDPM method remain consistent with those provided in the AppendixD.1.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/loss_comparison.png",
                "caption": "Figure 5:Text-to-Image Generation Loss Curves.Training loss comparison between DiffMoE-E16-T2I-Flow and Dense-DiT-T2I-Flow models over 160K steps. DiffMoE consistently achieves lower loss values, demonstrating superior convergence efficiency compared to the dense baseline.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2503.14487/x3.png",
                "caption": "Figure 6:Training and Inference Gap.Comparison of sampling strategies with DiffMoE-L-E16-Flow (Batch Size = 1). For each group,Top:Sampling w/o Capacity Predictor (with Fixed TopK Method.)Bottom:Sampling with Capacity Predictor.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/5-Diff_Trd.png",
                "caption": "Figure 7:Different Threshold Methods.We employ two distinct approaches for threshold determination:dynamic threshold (red point)andinterval search (blue points). Visualization using DiffMoE-\nL-E16-Flow (700K).",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/2-Flow-All-Scales.png",
                "caption": "Figure 8:Scaling Model Size.We analyze the impact of model size scaling by plotting FID50K scores across training steps. DiffMoE consistently outperforms the corresponding baseline models across all scales (S/B/L).",
                "position": 820
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/2-Num_Experts_Fid_vs_Step.png",
                "caption": "Figure 9:Scaling Number of Experts.Comparison of FID50K scores during training between Dense-DiT-L-Flow (E1) and models with increasing expert counts (E2, E4, E8, E16).",
                "position": 845
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGenerative Modeling.",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CCalculation of Average Activated Parameters and Average CapacityCinferavgsubscriptsuperscriptùê∂avginferC^{\\rm avg}_{\\rm infer}italic_C start_POSTSUPERSCRIPT roman_avg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_infer end_POSTSUBSCRIPT",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/Loss_Comparison_Ksteps_Smoothed_L-Flow.png",
                "caption": "Figure 10:Loss Comparison of L-Flow and L-DDPM Series. The relative losses illustrated in subfigures (a) and (b) demonstrate the exceptional training dynamics of DiffMoE, consistently outperforming all baseline models.",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/Loss_Comparison_Ksteps_Smoothed_L-DDPM.png",
                "caption": "",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/1-Baseline_DDPM.png",
                "caption": "Figure 11:Comparisons with the Baseline Models.(a)We compare TC, EC, and Dense Models and show the average activated parameters of all experts across all sampling steps. DiffMoE-L-E16-DDPM even surpasses DenseDiT-XL-DDPM (1.5x params).(b)We also examine S/B size DiffMoE models to further demonstrate the scalability.",
                "position": 2559
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/2-DDPM-All-Scales.png",
                "caption": "",
                "position": 2569
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/all_experts_layer1_cap_4x4.jpg",
                "caption": "Figure 12:Expert Dynamics across Network Layers.Visualization of expert capacity patterns in network layers (1, 7, 13, 19). Early layers show high-amplitude fluctuations, while deeper layers exhibit increasingly stable utilization, demonstrating natural expert specialization throughout the diffusion process.",
                "position": 2576
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/all_experts_layer7_cap_4x4.jpg",
                "caption": "",
                "position": 2585
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/all_experts_layer13_cap_4x4.jpg",
                "caption": "",
                "position": 2594
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/all_experts_layer19_cap_4x4.jpg",
                "caption": "",
                "position": 2598
            }
        ]
    },
    {
        "header": "Appendix DMore DiffMoE Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/Flow_Top10Hard_compressed.jpg",
                "caption": "Figure 13:Top 10 Hardest Classes.The 10 classes with the highest computational cost, sampled from the training set.",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/DDPM_Top10Hard_compressed.jpg",
                "caption": "",
                "position": 2656
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/Flow_Top10Easy_compressed.jpg",
                "caption": "Figure 14:Top 10 Easiest Classes.The 10 classes with the lowest computational cost, sampled from the training set.",
                "position": 2663
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/DDPM_Top10Easy_compressed.jpg",
                "caption": "",
                "position": 2674
            }
        ]
    },
    {
        "header": "Appendix EFID Sensibility and Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/0354-5000_DiT-L-2_Flow-ECMoE_BatchLevel_CapacityPred_w_Threshold_E8_GPU8_SOTA_resume600K_6400000_4.0_euler_0_compressed.jpg",
                "caption": "Figure 15:Class-conditional Generation.Uncurated256√ó\\times√ó256DiffMoE-L-E8-Flowsamples CFG scale = 4.0.",
                "position": 3373
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/0373-5001_DiT-L-2_DDPM-ECMoE_BatchLevel_CapacityPred_w_Threshold_E8_GPU8_SOTA_resume1100K_5900000_4.0_ddpm_0_compressed.jpg",
                "caption": "Figure 16:Class-conditional Generation.Uncurated256√ó\\times√ó256DiffMoE-L-E8-DDPMsamples CFG scale = 4.0.",
                "position": 3377
            },
            {
                "img": "https://arxiv.org/html/2503.14487/x4.png",
                "caption": "Figure 17:Text-to-Image Generation.Comparison of DiffMoE-E16-T2I-Flow (w/o SFT) and Dense Model (w/o SFT).",
                "position": 3381
            },
            {
                "img": "https://arxiv.org/html/2503.14487/extracted/6286203/figs/MoE_Image-36864000-ori_caption_Merged.jpg",
                "caption": "Figure 18:Text-to-Image Generation.Uncurated256√ó\\times√ó256 Images generated byDiffMoE-E16-T2I-Flow(w SFT)",
                "position": 3385
            }
        ]
    },
    {
        "header": "Appendix FVisual Generation Results",
        "images": []
    }
]