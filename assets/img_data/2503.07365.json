[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07365/x1.png",
                "caption": "Figure 1:The distribution of the training dataset.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2503.07365/x2.png",
                "caption": "Figure 2:Comparison of training on InternVL2.5-Instruct-8B with and without data filtering. When data filtering is not used, the accuracy reward shows fluctuating trends, while response length exhibits a downward trend.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2503.07365/x3.png",
                "caption": "Figure 3:Comparison of RL training on InternVL2.5-Instruct-8B with and without KL divergence. When KL divergence is added, response length consistently shows a downward trend, and the peak accuracy reward is marginally lower than in the case without KL divergence.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2503.07365/x4.png",
                "caption": "Figure 4:An example of visual aha moment. We see that our MM-Eureca reaffirms the answer by re-percepting the image.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2503.07365/x5.png",
                "caption": "Figure 5:visual perception aha moment. We use xx to replace sensitive information. In (a), the model cannot directly find the answer in the image and instead reasons by re-examining the visual cues within the image. In (b), the model detects a discrepancy between its computed result and the extracted information from the image, prompting it to re-read the image and arrive at the correct answer.",
                "position": 259
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07365/x6.png",
                "caption": "Figure 6:Train Time Scale-up on Accuracy Reward and Response Length of Rule-Based RL. (a) represents the training scenario on InternVL2.5-instruct-8B, while (b) corresponds to the training scenario on InternVL2.5-pretrained-38B. It can be observed that stable improvements in accuracy reward and response length can be achieved regardless of whether the model is based on an instruct model or a pretrained model.",
                "position": 405
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07365/x7.png",
                "caption": "Figure 7:Comparison of training with Curriculum Learning RL and Direct RL. Although both show an increasing trend in response length, the accuracy reward for Curriculum Learning struggles to improve in the later stages.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2503.07365/x8.png",
                "caption": "Figure 8:Comparison of training with Offline Data Filter strategy and Online Data Filter strategy. Although both achieve stable increases in accuracy reward, the peak value reached with Online Data Filter is lower. Additionally, the response length with Online Data Filter fails to show an upward trend.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2503.07365/x9.png",
                "caption": "Figure 9:Visualization of rule-based RL training using InternVL-2.5-pretrained-8B directly on 8K K12 data. The results show that both accuracy reward and response length exhibit fluctuating trends, making stable growth difficult to achieve.",
                "position": 574
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]