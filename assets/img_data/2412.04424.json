[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04424/x1.png",
                "caption": "Figure 1:Comparison of LLaVA-style MLLMs with our Florence-VL. LLaVA-style models use CLIP, pretrained with contrastive learning, to generate asingle high-level image feature. In contrast, Florence-VL leverages Florence-2, pretrained with generative modeling across various vision tasks such as image captioning, OCR, and grounding. This enables Florence-VL to flexibly extractmultiple task-specific image featuresusing Florence-2 as the image encoder.",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2412.04424/x2.png",
                "caption": "Figure 2:An overview of Florence-VL, which extracts visual features of different depths (levels of feature concepts) and breaths (prompts) from Florence-2, combines them using DBFusion, and project the fused features to an LLMâ€™s input space. Florence-VL is fully pretrained on image captioning data and then partially finetuned on instruction-tuning data.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2412.04424/x3.png",
                "caption": "Figure 3:Visualization of the first three PCA components: we apply PCA to image features generated from Detailed Caption, OCR, and Grounding prompts, excluding the background by setting a threshold on the first PCA component. The image features derived from the Detailed Caption prompt (second column) capture the general context of the image, those from the OCR prompt (third column) focus primarily on text information, and those from the Grounding prompt (fourth column) highlight spatial relationships between objects. Additionally, we visualize the final layer features from OpenAI CLIP (ViT-L/14@336) in the last column, showing that CLIP features often miss certain region-level details, such as text information in many cases.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Preliminary: Florence-2",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Analysis on Different Vision Encoders",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04424/x4.png",
                "caption": "Figure 4:We plot the alignment loss for different vision encoders, which clearly shows that Florence-2 vision encoder achieves the lowest alignment loss compared to the other vision encoders, demonstrating the best alignment with text embeddings.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2412.04424/x5.png",
                "caption": "Figure 5:We plot the alignment loss for various feature combinations, removing one feature at a time from different depths and breadths. The results clearly show that our method achieves the lowest alignment loss compared to others, highlighting the importance of all features from different depths and breadths for optimal alignment.",
                "position": 410
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Training Details",
        "images": []
    },
    {
        "header": "10Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]