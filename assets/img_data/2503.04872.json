[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04872/x1.png",
                "caption": "Figure 1:(A) A simplified diagram of our Branch-Merge distillation approach. (1) In the Branch phase, each copy of the Initial Model (backbone) is trained on knowledge from a different domain; (2) In the Merge phase, models are merged based on Arcee Fusion rules. (B) Performance Comparison of different LLM modelsMustar (2025). TinyR1-32B-Preview outperforms distilled models of the same size in science, math, and coding and achieves comparable results to Deepseek R1. LiveCodeBench here refers to the 24.08-25.02 subset of full LiveCodeBench.",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2The Branch-Merge Distillation Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04872/extracted/6258368/figure/gpqa.jpg",
                "caption": "Figure 2:Performance Comparison of merged models on the GPQA-Diamond benchmark.",
                "position": 264
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]