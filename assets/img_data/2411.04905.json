[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04905/extracted/5986092/figures/inf_logo.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2411.04905/extracted/5986092/figures/map_logo.png",
                "caption": "",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2411.04905/extracted/5986092/figures/opencoder_icon.jpg",
                "caption": "",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x1.png",
                "caption": "Figure 1:OpenCoder surpasses all previous fully open models (i.e., with open model weights and reproducible datasets) and other open-access models (i.e., with open model weights only) at the 6B+ parameter scale, pushing the frontier of fully open models to new heights.",
                "position": 203
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Pretraining Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04905/x2.png",
                "caption": "Figure 2:The illustration of our pretraining data processing workflow.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x3.png",
                "caption": "Figure 3:Visualization on the PCA data distributions ofRefineCodeand The Stack v2.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x4.png",
                "caption": "Figure 4:The distribution of top program languages inRefineCode(before data sampling).",
                "position": 851
            }
        ]
    },
    {
        "header": "3Pretraining",
        "images": []
    },
    {
        "header": "4Post Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04905/x5.png",
                "caption": "Figure 5:The illustration of our instruction data synthesis workflow.",
                "position": 1020
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04905/x6.png",
                "caption": "Figure 6:The McEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size.",
                "position": 1759
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x7.png",
                "caption": "Figure 7:The MdEval performance of OpenCoder-8B-Instruct in comparison to other open-source code models of comparable size.",
                "position": 1762
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04905/x8.png",
                "caption": "Figure 8:Impact of using different deduplication strategies.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x9.png",
                "caption": "Figure 9:Impact of using high-quality data in the annealing stage.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x10.png",
                "caption": "Figure 10:Impact of star-based data filtering on model performance.",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2411.04905/x11.png",
                "caption": "Figure 11:Left figure: Losses of using different training data with different distributions. Right figure: Visualization of the embeddings for original data and filtered data. Note that filtering based on the number of stars can reduce data diversity and result in a lower overall loss for pretraining.",
                "position": 1865
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion & Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFiltering Rules",
        "images": []
    },
    {
        "header": "Appendix AAnalysis on Chunk-level Deduplication",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04905/x12.png",
                "caption": "Figure 12:Comparison of Pass@1 performance on HumanEval & MBPP for different dedup strategies (File-Level, Repo-Level, and Repo-level + Chunk-Level) across RefineCode Python corpus.",
                "position": 3327
            }
        ]
    },
    {
        "header": "Appendix AExtra Data Processing",
        "images": []
    },
    {
        "header": "Appendix AComparison of RefineCode with The Stack Series",
        "images": []
    },
    {
        "header": "Appendix AProgramming Languages Categories",
        "images": []
    },
    {
        "header": "Appendix ARaw Code Data Composition",
        "images": []
    },
    {
        "header": "Appendix APrompts For SFT Synthetic Data",
        "images": []
    }
]