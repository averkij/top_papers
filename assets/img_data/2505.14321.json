[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/text_exp1.png",
                "caption": "Figure 1:Examples of LLM-Answerable, Semantic and Temporal questions in VideoMME(fu2024video,): (Top) The model uses LLMâ€™s prior knowledge to answer correctly without the need of video; (Middle) The model relies on semantic understanding to answer without requiring temporal comprehension; (Bottom) The model relies on comprehensive temporal understanding to answer.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/semantic.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/temporal.png",
                "caption": "",
                "position": 185
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3A Closer Look at Video Understanding Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/llm_answerable.png",
                "caption": "Figure 2:Performance of different MLLMs without videos as the input on four benchmarks.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/gpt_4o.png",
                "caption": "Figure 3:After shuffling the extracted frames, the scores of each model remain unshaken across all benchmarks. *Frame settings: (a), (d) uses 128 frames for VideoMME-long, others use 64 frames; (b) uses10slow+50fastsubscript10slowsubscript50fast10_{\\text{slow}}+50_{\\text{fast}}10 start_POSTSUBSCRIPT slow end_POSTSUBSCRIPT + 50 start_POSTSUBSCRIPT fast end_POSTSUBSCRIPTframes for all benchmarks; (c) uses 16 frames for all benchmarks.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/SlowFast-LLaVA-7B.png",
                "caption": "",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/PLLaVA-7B.png",
                "caption": "",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/LLaVA-OV-7B-Qwen2.png",
                "caption": "",
                "position": 247
            }
        ]
    },
    {
        "header": "4A Standardized Protocol for Breaking Down Video LLM Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/pipeline_v3.png",
                "caption": "Figure 4:An overview of our standardized protocol: benchmark questions are categorized into four groups. Questions answerable by both GPT-4o and Gemini without video are classified as LLM-Answerable. For the remaining questions, we apply random shuffles to the extracted frames twice: if both models answer correctly before and after shuffling, the question is classified as Semantic. If one model answers correctly before but fails after shuffling, the question is classified as Temporal. All other questions are categorized as Others.",
                "position": 279
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/longvideobench_cor.png",
                "caption": "Figure 5:VBenchComp scores are aligned with the original scores but they can better evaluate the overall video LLM performance with less questions. The temporal video understanding capability of models under the trend line can be potentially over-estimated in the original benchmarks.",
                "position": 886
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/egoschema_cor.png",
                "caption": "",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/mlvu_cor.png",
                "caption": "",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/nextqa_cor.png",
                "caption": "",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/perceptiontest_cor.png",
                "caption": "",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2505.14321/extracted/6458308/Figure/video_mme_cor.png",
                "caption": "",
                "position": 903
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]