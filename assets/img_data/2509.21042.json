[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21042/x1.png",
                "caption": "Figure 1:Causal mask induces positional information even in the absence of causal input dependencies, feed-forward networks, or parameters.\n(a) With the input assumption, the first-layer attention scores collapse to 1 on the diagonal and 0 elsewhere.\n(b) The attention output is then computed as a weighted sum of thexi(0)x_{i}^{(0)}(we omit the superscript(0)and simply writexix_{i}in place ofxi(0)x_{i}^{(0)}).\n(c) Afterℓ2\\ell_{2}normalization,\n(d) subsequent-layer attention scores reveal a clear position-dependent pattern, assigning higher weights to nearby query–key pairs, a behavior similar to common positional encodings.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Theoretical Analysis",
        "images": []
    },
    {
        "header": "4Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21042/x2.png",
                "caption": "Figure 2:Simulation of a Transformer without parameter and explicit positional encoding results. We visualized averaged attention scores for each layer withα=0\\alpha=0and 0.2.\nThe y-axis represents query indices, and the x-axis represents key indices.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x3.png",
                "caption": "Figure 3:Inner-product Gram matrix heatmap of a trained Transformer decoder without positional encoding. We sample 1,000 sequences of length 50 from the held-out set, compute attention intermediates, and then calculate averaged inner products across heads and samples.",
                "position": 416
            }
        ]
    },
    {
        "header": "5Interaction between Causal Mask and RoPE",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21042/x4.png",
                "caption": "Figure 4:Simulation of a Transformer without parameters using RoPE.\nWe visualize the attention scores across layers, where the y-axis denotes query indices and the x-axis denotes key indices. The top row shows the raw attention scores, while the bottom row shows scores normalized by subtracting the mean of each diagonal.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x5.png",
                "caption": "Figure 5:Diagonal-normalized attention heatmap of LLMs (first 4 layers).\nAttention scores were computed for 1,000 sequences of length 1,024 and averaged across sequences and heads.\nThe Attention Sink effect flattens the overall pattern, so the color scale is adjusted using the 1% and 99% quantiles.\nThe y-axis denotes query indices, and the x-axis denotes key indices.",
                "position": 518
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Derivations",
        "images": []
    },
    {
        "header": "Appendix BExtended Causal Mask Positional Information Pattern Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21042/x6.png",
                "caption": "Figure 6:Extended result of Figure2.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x7.png",
                "caption": "Figure 7:Simulation of a Transformer without parameter and explicit positional encoding results, with LayerNorm. We replacedℓ2\\ell_{2}normalization to LayerNorm, and appliedd\\sqrt{d}scaling (first and second row) andddscaling (third and fourth row).",
                "position": 970
            }
        ]
    },
    {
        "header": "Appendix CAttention Pattern of a Trained Transformer without Explicit Positional Encoding",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21042/x8.png",
                "caption": "Figure 8:Attention pattern of a trained Transformer without explicit positional encoding. The plots are drawn with a same manner to Figure3.",
                "position": 979
            }
        ]
    },
    {
        "header": "Appendix DRoPE Attention Pattern Analysis Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21042/x9.png",
                "caption": "Figure 9:The extended result of Figure4withα=0.5\\alpha=0.5",
                "position": 987
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x10.png",
                "caption": "Figure 10:The extended result of Figure4without causal mask (i.e. Transformer Encoder)",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x11.png",
                "caption": "Figure 11:Llama-3-8B Attention Pattern",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x12.png",
                "caption": "Figure 12:Phi-4 Per-Layer Attention Pattern",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x13.png",
                "caption": "Figure 13:Qwen3-8B Per-Layer Attention Pattern",
                "position": 1004
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x14.png",
                "caption": "Figure 14:Llama-3-8B Attention Pattern (Normalized)",
                "position": 1007
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x15.png",
                "caption": "Figure 15:Phi-4 Attention Pattern (Normalized)",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2509.21042/x16.png",
                "caption": "Figure 16:Qwen-8B Attention Pattern (Normalized)",
                "position": 1013
            }
        ]
    },
    {
        "header": "Appendix ELLM RoPE Patterns",
        "images": []
    }
]