[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10639/x1.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10639/x2.png",
                "caption": "Figure 2:GoT Dataset Construction Process.Left:Text-to-image GoT annotation pipeline that labels detailed GoT with semantic content and spatial coordinates.Right:Editing GoT annotation pipeline that processes source image, target image, and instruction to generate entity-aware reasoning GoT with precise spatial grounding. Both pipelines leverage Qwen2-VL[46]and Qwen2.5[51]models for various stages of the annotation process.",
                "position": 183
            }
        ]
    },
    {
        "header": "3Generation Chain-of-Thought (GoT)",
        "images": []
    },
    {
        "header": "4GoT Dataset: Semantic-Spatial Reasoning Chains for Visual Generation and Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10639/x3.png",
                "caption": "Figure 3:GoT Framework with Semantic-Spatial Guidance.Left:Our dual-task framework handling both text-to-image generation (T2I) and image editing.Right:The SSGM Diffusion Module, which combines spatial layouts guidanceGssubscriptùê∫ùë†G_{s}italic_G start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, reference image guidanceGrsubscriptùê∫ùëüG_{r}italic_G start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and semantic guidanceGtsubscriptùê∫ùë°G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTto generate the final image with precise content and spatial control.",
                "position": 256
            }
        ]
    },
    {
        "header": "5GoT Framework: Reasoning-guided Visual Generation and Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10639/x4.png",
                "caption": "Figure 4:Text-to-Image samples generated by our model. The GoT framework can plan object placement based on the input caption and generate highly aligned and aesthetic images accordingly.",
                "position": 498
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10639/x5.png",
                "caption": "Figure 5:Samples on interactive generation with GoT framework. By modifying GoT content (description and bounding box position), user can customize their text-to-image process with: 1. Object replacement 2. Object position adjustment 3. Object attribute modification.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2503.10639/x6.png",
                "caption": "Figure 6:Qualitative results of image editing. Our GoT framework demonstrates superior performance in settings that require semantic-spatial reasoning. Red bounding boxes indicate the coordinates predicted by MLLM within the GoT framework.",
                "position": 698
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Training Details",
        "images": []
    },
    {
        "header": "9Visualization Results",
        "images": []
    },
    {
        "header": "10GoT Format and Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10639/x7.png",
                "caption": "Figure 7:More samples on image editing with the GoT content generated by our model.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2503.10639/x8.png",
                "caption": "Figure 8:More examples on interactive generation.",
                "position": 1610
            },
            {
                "img": "https://arxiv.org/html/2503.10639/x9.png",
                "caption": "Figure 9:Visualization on Multi-Guidance Strategy Hyper-parameter Selection. The above are text-to-image samples generated by GoT framework under different hyper-parameters.",
                "position": 1613
            },
            {
                "img": "https://arxiv.org/html/2503.10639/x10.png",
                "caption": "Figure 10:Examples of GoT dataset for text-to-image generation, including FLUX-GoT, JourneyDB-GoT, and Laion-Aesthetics-High-Resolution-GoT.",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2503.10639/x11.png",
                "caption": "Figure 11:Examples of GoT dataset for image editing, including OmniEdit-GoT for single-turn editing and SEED-Edit-Multiturn-GoT for multi-turn editing.",
                "position": 1619
            }
        ]
    },
    {
        "header": "11Prompts for Evaluation and Dataset Construction",
        "images": []
    }
]