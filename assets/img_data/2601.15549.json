[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15549/figs/viola.png",
                "caption": "",
                "position": 49
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15549/x1.png",
                "caption": "Figure 1:Problem setup and performance comparison. (a) Unlike methods requiring large labeled datasets, our approach, VIOLA, strategically selects a minimal subset of informative samples for expert annotation while leveraging abundant unlabeled videos via pseudo-labeling. This constructs a hybrid pool from which the model retrieves relevant demonstrations for inference. (b) Performance comparison with Naive ICL (which randomly selects samples for annotation), averaged across seven classification datasets. VIOLA achieves robust adaptation with significantly reduced annotation costs.",
                "position": 75
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15549/x2.png",
                "caption": "Figure 2:Overview of our proposed framework.\nThe pipeline consists of three stages:\n1. Selective Annotation: We acquire expert labels for a small, informative subset (ùíüL\\mathcal{D}_{L}) using density-uncertainty-weighted sampling.\n2. Pseudo-Annotation: We generate high-confidence pseudo-labels via in-context pseudo-annotation to construct a hybrid poolùíüH\\mathcal{D}_{H}.\n3. Inference: We predict final answers via confidence-aware retrieval and prompting during inference.",
                "position": 151
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15549/x3.png",
                "caption": "Figure 3:Performance trends under varying oracle annotation budgets. We compare our framework against baselines using Qwen2-VL-7B, varying the labeled pool size from 20 to 100 samples.",
                "position": 807
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary materials",
        "images": []
    },
    {
        "header": "Appendix 0.ADatasets",
        "images": []
    },
    {
        "header": "Appendix 0.BPrompt Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15549/x4.png",
                "caption": "Figure 4:Qualitative results on UCF-Crimes and EgoSurgery.",
                "position": 1190
            }
        ]
    },
    {
        "header": "Appendix 0.CQualitative Results",
        "images": []
    }
]