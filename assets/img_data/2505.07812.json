[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Continuous Visual Autoregressive Generation",
        "images": []
    },
    {
        "header": "4Energy-based Autoregressive Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07812/x1.png",
                "caption": "Figure 1:Comparison between the discrete-token standard Transformer and our continuous-token energy Transformer. At the input side, the embedding lookup table is replaced with a linear projection. At the output side, the softmax classification layer is replaced with a small MLP generator, which takes random noiseœµitalic-œµ\\epsilonitalic_œµas input to perturb the hidden state.",
                "position": 284
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07812/x2.png",
                "caption": "Figure 2:The speed/quality trade-off for EAR and MAR. The number of autoregressive steps is fixed at 64. For MAR, we vary the number of diffusion steps (10, 20, 25, 30, 40, 50) to generate outputs under different inference latencies. For EAR, the curve is obtained by using different model sizes (EAR-B, EAR-L, EAR-H). The inference time is measured on a single A100 GPU.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x3.png",
                "caption": "Figure 3:FID curves of the continuous-valued energy Transformer (205M) and the discrete-valued standard Transformer (196M). The guidance scale is 3.0.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x4.png",
                "caption": "Figure 4:Generation quality of the Gaussian Transformer under different standard deviations during inference. The model size is 184M. cfg is disabled since it does not work well here.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x5.png",
                "caption": "Figure 5:The results of varying learning rates for EAR-B.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2505.07812/extracted/6431610/cfg.png",
                "caption": "Figure 6:Samples of EAR-H under different gudiance scales. We fix the random seed and apply the constant cfg schedule during sampling.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x6.png",
                "caption": "Figure 7:The results of varying classifier-free guidance scales for EAR models.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x7.png",
                "caption": "",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x8.png",
                "caption": "Figure 8:The results of varying temperaturesœÑtrainsubscriptùúètrain\\tau_{\\text{train}}italic_œÑ start_POSTSUBSCRIPT train end_POSTSUBSCRIPT(left) andœÑinfersubscriptùúèinfer\\tau_{\\text{infer}}italic_œÑ start_POSTSUBSCRIPT infer end_POSTSUBSCRIPT(right) for EAR-B.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2505.07812/x9.png",
                "caption": "",
                "position": 839
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results",
        "images": []
    }
]