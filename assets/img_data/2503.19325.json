[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19325/x1.png",
                "caption": "Figure 1:Evaluation on Long Video Prediction.FAR effective exploits long video contexts and achieves accurate prediction.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19325/x2.png",
                "caption": "Figure 2:Illustration of FARâ€™s Training and Inference Pipeline.In short-video training, a portion of frames is randomly replaced with clean context frames, marked with a unique timestep embedding (e.g., -1) beyond the flow-matching scheduler. In long-video training, we adopt long short-term context modeling. A long-term context window with aggressive patchification is adopted to reduce redundant vision tokens, while a short-term context window is used to model fine-grained temporal consistency.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2503.19325/x3.png",
                "caption": "Figure 3:Visualization of Attention Mask.FAR enables full attention within a frame while maintaining causality at the frame level. In long-context training, we adopt aggressive patchification for long-term context frames to reduce tokens.",
                "position": 270
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19325/x4.png",
                "caption": "Figure 4:Effect of Stochastic Clean Context.This technique eliminate training-inference gap in observed context.",
                "position": 332
            }
        ]
    },
    {
        "header": "4FAR",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19325/x5.png",
                "caption": "Figure 5:Comparison of FAR and video diffusion transformer.FAR achieves better convergence than video diffusion transformer in unconditional video generation on UCF-101.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2503.19325/x6.png",
                "caption": "Figure 6:Visualization and Comparison of Various Temporal Position Embeddings.The proposed FlexRoPE incorporates a linear bias to induce controllable temporal decay at test time, enhancing extrapolation performance as context increases. In contrast, other methods degrade when the inference context exceeds the training window.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2503.19325/x7.png",
                "caption": "Figure 7:Relation between Token Context Length and Vision Context Length.With the proposed long short-term context modeling, the token context length scales more slowly as the vision context length increases compared to uniform context modeling.",
                "position": 400
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19325/x8.png",
                "caption": "Figure 8:Qualitative Comparison of Long-Context Video Prediction on DMLab.FAR fully utilizes the long-range context (144 frames), resulting in more consistent prediction (156 frames) compared to previous methods.",
                "position": 947
            },
            {
                "img": "https://arxiv.org/html/2503.19325/x9.png",
                "caption": "Figure 9:Ablation Study of the Short-Term Context Window Size.Performance saturates as the window size increases.",
                "position": 1075
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19325/x10.png",
                "caption": "Figure 10:Qualitative Comparison of Long-Context Video Prediction on DMLab.FAR fully utilizes the long-range context (144 frames), resulting in more consistent prediction (156 frames) compared to previous methods.",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2503.19325/x11.png",
                "caption": "Figure 11:Qualitative Comparison of Long-Context Video Prediction on Minecraft.FAR fully utilizes the long-range context (144 frames), resulting in more consistent prediction (156 frames) compared to previous methods.",
                "position": 2159
            }
        ]
    },
    {
        "header": "7Appendix",
        "images": []
    }
]