[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21420/x1.png",
                "caption": "Figure 1:Overview of ConceptMoE, with details of chunk and dechunk modules.",
                "position": 169
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21420/x2.png",
                "caption": "Figure 2:Training dynamics of loss and compression ratio. (a) Loss difference between ConceptMoE and MoE during language model pretraining (PT). (b) Loss difference during multimodal continue training(CT), separated into image-text data (mmloss) and text-only data(textloss). (c) Compression ratio evolution for image tokens and text tokens during multimodal training.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x3.png",
                "caption": "Figure 3:From MoE to ConceptMoE illustration. The blue blocks denote the original MoE components. We add a chunk module and a dechunk module. In addition, in the last four self-attention layers, we insert an extra QKV projector initialized to zeros to enable joint decoding of concepts and tokens.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x4.png",
                "caption": "Figure 4:Evolution of evaluation metrics across three benchmarks throughout the training process.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x5.png",
                "caption": "Figure 5:Inference latency speedup over MoE for prefill and decoding. The prefill plot uses sequence length on the x axis, and the decoding plot uses KV cache length on the x axis with batch size 256. The y axis reports speedup relative to MoE in percent. ConceptMoE-xxL-topyy-Rzzmatches MoE in FLOPs and total parameters, wherexxis the layer multiplier,yyis the number of activated experts per MoE block with baseline 8 and increased to match FLOPs, andzzis the compression ratio.",
                "position": 766
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x6.png",
                "caption": "Figure 6:Inference latency for prefill and decoding. The prefill plot uses sequence length on the x axis, and the decoding plot uses KV cache length on the x axis with batch size 256. The y axis reports the measured end to end inference latency on Hopper GPUs. ConceptMoE-xxL-topyy-Rzzmatches MoE in FLOPs and total parameters, wherexxis the layer multiplier,yyis the number of activated experts per MoE block with baseline 8 and increased to match FLOPs, andzzis the compression ratio.",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x7.png",
                "caption": "Figure 7:Impact of auxiliary loss weightλ\\lambdaon training loss and compression ratio.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x8.png",
                "caption": "Figure 8:Comparison of chunking strategies. (a) Training loss difference relative to No Chunk baseline (i.e. MoE) during pretraining. Dynamic Chunk consistently achieves lower loss than Fixed Chunk. (b) Downstream benchmark scores after training. Comp.: Comprehensive Evaluation, Reason.: Reasoning, Know.: Knowledge.",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x9.png",
                "caption": "Figure 9:Impact of router type and joint decoding on training and downstream performance.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x10.png",
                "caption": "Figure 10:Training loss diff and mean probability of chunk module for different noise strategies. We do smoothing and remove some spike data for better visualization, which doesn’t affect the conclusion.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2601.21420/x11.png",
                "caption": "Figure 11:Impact of target compression ratio on training and downstream performance. (a) Training loss difference relative to baseline.R=2R=2converges close to baseline, whileR=4R=4shows persistent degradation. (b) Downstream benchmark scores.R=2R=2outperforms the baseline across most metrics, whileR=4R=4underperforms substantially, particularly on reasoning and math tasks. Results indicate that excessive compression (e.g.,R=4R=4) degrades performance despite successful compression ratio control.",
                "position": 924
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Evaluation benchmark",
        "images": []
    },
    {
        "header": "7Code",
        "images": []
    }
]