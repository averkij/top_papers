[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.00094/x1.png",
                "caption": "",
                "position": 73
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x2.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x3.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x4.png",
                "caption": "",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x5.png",
                "caption": "Figure 1:Cross-domain performance analysis on the Camel-Bench Benchmark.\nOur AIN-7B achieves promising performance compared to significantly bigger models (GPT-4o and Gemini-1.5-Pro)\nin both domain-specific and aggregate settings. Despite its smaller size, our AIN-7B achieves competitive performance across all 38 sub-domains with significantly superior capabilities on OCR & document understanding.",
                "position": 142
            }
        ]
    },
    {
        "header": "1AIN Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.00094/x6.png",
                "caption": "Figure 2:AIN: A versatile LMM excelling in visual and contextual understanding across diverse domains, including VQA on complex topics, OCR for various fonts and handwriting, cultural insights (traditions, food, places), agricultural tasks (crop identification, fruit classification, disease detection), remote sensing (multi-scale objects), medical imaging (various modalities), and video analysis (animation, human activities).",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x7.png",
                "caption": "Figure 3:AIN compared to existing LMMs across CAMEL-Bench benchmark[1]domains:OCR: “OCR & Document Understanding”,Video: “General Video & Multi-Image Understanding”,RS: “Remote Sensing Understanding”,CDT:“Chart, Diagram & Table Understanding”,Agro.: “Agricultural Image Understanding”,Cultural: “Cultural-Specific Understanding”,Medical: “Medical Image Understanding”.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x8.png",
                "caption": "Figure 4:Qualitative results demonstrating AIN’s comprehensive capabilities across diverse domains. The results show its proficiency in handling both multiple-choice and open-ended questions. Our proposed AIN exhibits robust performance in addressing queries related to visual attributes (shape, color, quantity), while maintaining appropriate response formats (single character, word, or complete sentence) according to task requirements.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x9.png",
                "caption": "Figure 5:Comparison of AIN with GPT-4o[3]and LLaVA[11]across diverse tasks. The evaluation demonstrates AIN’s proficiency in handling both multiple-choice and open-ended questions while maintaining appropriate response formats.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x10.png",
                "caption": "Figure 6:AIN human evaluation survey, illustrating assessment criteria and multi-domain questions designed to evaluate multi-task and complex reasoning capabilities. The survey includes evaluations on specific food items, road signs in low-resolution settings, celebrities, charts, remote sensing tasks, and other diverse topics to comprehensively assess performance across multiple domains and challenges.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x11.png",
                "caption": "Figure 7:Nationality of AIN Survey Participants: Participants represent 17 Arab nations, with the highest contributions from Saudi Arabia (30%), followed by Egypt (25%), the UAE (13.3%), and Lebanon (13.3%).",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x12.png",
                "caption": "Figure 8:User Model Preferences. Participant preferences for the three models in the survey, with Model 1 (AIN (ours)) receiving 76% of the votes, GPT-4v 15%, and LLaVA 9%, demonstrating AIN’s strong performance.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x13.png",
                "caption": "Figure 9:User Preferences for MSA and Local Dialects: The majority (74.3%) preferred MSA for reading and writing. An additional 11% are comfortable with MSA but favored their local dialects, while 4.3% found MSA challenging and prefer using their dialect. A further 10.5% reported difficulties unrelated to linguistic aspects.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x14.png",
                "caption": "(a)Q1:\\<ما هو لون الورقة المصابة بالبقعة البكتيرية؟>Domain:Agricultural Image Understanding / Plant diseases.Purpose:Ability to detect diseased plant areas and identify their color.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x14.png",
                "caption": "(a)Q1:\\<ما هو لون الورقة المصابة بالبقعة البكتيرية؟>Domain:Agricultural Image Understanding / Plant diseases.Purpose:Ability to detect diseased plant areas and identify their color.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x15.png",
                "caption": "(b)Q2:\\<ما هو شكل الطعام الموجود في الصورة؟>Domain:Cultural-Specific Image Understanding / Food.Purpose:Ability to recognize food and precisely determine its shape.).",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x16.png",
                "caption": "(c)Q3:\\<كم عدد التقاطعات الموجودة في الصورة؟>Domain:Remote Sensing Image Understanding / Roads & Constructions.Purpose:Ability to identify specific constructions among similar ones.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x17.png",
                "caption": "(d)Q4:\\<يرجى الإجابة مباشرة بكلمة أو رقم واحد، هل الضوء أخضر؟>Domain:General VQA/ Binary Question.Purpose:Ability to identify tiny details in ambiguous scenes and answer binary questions.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x18.png",
                "caption": "(a)Q5:\\<كم عدد لافتات ممنوع الانعطاف إلى اليسار الموجودة؟>Domain:General VQA / Traffic Signs.Purpose:Ability to spot traffic signs at a distance and in low resolution.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x18.png",
                "caption": "(a)Q5:\\<كم عدد لافتات ممنوع الانعطاف إلى اليسار الموجودة؟>Domain:General VQA / Traffic Signs.Purpose:Ability to spot traffic signs at a distance and in low resolution.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x19.png",
                "caption": "(b)Q6:\\<ما هو النص المكتوب في الصورة؟ >Domain:OCR & Document Understanding.Purpose:Ability to discern Arabic characters and extract text from images.",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x20.png",
                "caption": "(c)Q7:\\<كم عدد قطع ورق العنب الموجودة في الصورة؟>Domain:General VQA / Short Answer Question.Purpose:Ability to pinpoint the required item among several items + provide a short answer as required.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x21.png",
                "caption": "(d)Q8:\\<ما هو الحالة أو مستوى الحالة في هذه الصورة؟ >Domain:Medical Image Understanding / Diseases Diagnoses.Purpose:Ability to diagnose organ health by reasoning its condition (normal or abnormal) for a specific disease.",
                "position": 737
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x22.png",
                "caption": "(a)Q9:\\<هل الفنان بداخل المربع الأحمر المحيط يدعى ريتشارد رومانوس؟>Domain:General VQA / Grounding and Celebrities.Purpose:Ability to determine a person’s identity in a specific location.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x22.png",
                "caption": "(a)Q9:\\<هل الفنان بداخل المربع الأحمر المحيط يدعى ريتشارد رومانوس؟>Domain:General VQA / Grounding and Celebrities.Purpose:Ability to determine a person’s identity in a specific location.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x23.png",
                "caption": "(b)Q10:\\<ما هي النسبة المئوية للنمو في أفريقيا؟>Domain:Chart, Diagram & Table Understanding / Bar Charts.Purpose:Ability to extract values from charts, even when not explicitly shown.",
                "position": 762
            }
        ]
    },
    {
        "header": "2Data Inspection and Selection",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.00094/x24.png",
                "caption": "Figure 13:Data verification and filtering pipeline for textual and visual data.Textual data underwent semantic similarity checks using LaBSE[29](80% threshold) and quality evaluation using BLEU[31](60% threshold), METEOR[32](80% threshold), and ROUGE[33](80% threshold). Visual data was screened for toxicity using LLavaGuard[34]policies with GPT-4o[3], discarding unsafe images to ensure quality and safety.",
                "position": 1460
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x25.png",
                "caption": "(a)Similarity scores for different settings for correct translation. The higher the better.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x25.png",
                "caption": "(a)Similarity scores for different settings for correct translation. The higher the better.",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x26.png",
                "caption": "(b)Similarity scores for different settings for incorrect translation. The lower the worse.",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x27.png",
                "caption": "(a)Comparison of LaBSE and Paraphrase-XLM-R: Evaluating 50 high-quality translated samples.",
                "position": 1482
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x27.png",
                "caption": "(a)Comparison of LaBSE and Paraphrase-XLM-R: Evaluating 50 high-quality translated samples.",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x28.png",
                "caption": "(b)Comparison of LaBSE and Paraphrase-XLM-R: Assessing 50 low-quality translated samples.",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2502.00094/x29.png",
                "caption": "Figure 16:Visual Data Toxicity Filtering. Using GPT-4o[3]and LLavaGuard[34]policies, about 96% of the data is classified as safe, while the remainder was deemed unsafe. The unsafe data was distributed across four categories: “Weapon, or Substance Abuse” (3.25%), “Hate, Humiliation, Harassment” (0.55%), “Animal Cruelty” (1.09%), and “Violence, Harm, or Cruelty” (0.55%).",
                "position": 1561
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]