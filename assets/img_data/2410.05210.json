[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05210/x1.png",
                "caption": "Figure 1:A holistic comparison of fine-tuning methods for vision-language compositionality. Enhancing compositionality often compromises multi-modal task performance in previous approaches. OurFSC-CLIPbridges this gap, minimizing these trade-offs. Full experimental results are provided in\\creftab:method_comparison.\\cref@constructprefixpage\\cref@result",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05210/x2.png",
                "caption": "Figure 2:A completeFSC-CLIPframework that incorporates Local Hard Negative (LHN) Loss with Selective Calibrated Regularization (SCR), alongside a global HN loss. The LHN loss measures similarity between an image and a text at the patch and token levels to more accurately identify subtle differences between original and HN texts. SCR combines focal loss with label smoothing to mitigate the adverse effects of using hard negative losses.\\cref@constructprefixpage\\cref@result",
                "position": 172
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05210/x3.png",
                "caption": "Figure 3:A conceptual illustration of the confidence-based weighting mechanism in HN loss. It reduces the adverse impact of HN supervision by lowering the signal from confident predictions while selectively focusing on challenging ones, crucial for learning compositionality.\\cref@constructprefixpage\\cref@result",
                "position": 341
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05210/x4.png",
                "caption": "Figure 4:Fine-tuning trajectories between compositionality (Comp) and zero-shot classification (ZS) via robust fine-tuning methodWortsman et al. (2022).\nEach point represents the interpolated model between the pre-trained and each fine-tuned version, at varying ratios.FSC-CLIPoffers better trade-offs betweenCompandZS, maintainingZSscores in the fully fine-tuned model.\\cref@constructprefixpage\\cref@result",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2410.05210/x5.png",
                "caption": "Figure 5:Image to text retrieval examples on COCO-CF dataset. CLIP and DAC-LLM often rank negative captions (marked with red crossmarks) as top-1, whileFSC-CLIPconsistently retrieves the correct caption (marked with green checkmarks), demonstrating superior fine-grained understanding and retrieval accuracy in challenging conditions.\\cref@constructprefixpage\\cref@result",
                "position": 1425
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05210/x6.png",
                "caption": "Figure 6:Example results of rule-based hard negative texts used for training our model. Image-text pairs were randomly sampled from LAION-COCOSchuhmann et al. (2022a). FornegclipYuksekgonul et al. (2023)andreplaceHsieh et al. (2023), differences from the original captions are highlighted in red.",
                "position": 2479
            }
        ]
    },
    {
        "header": "Appendix AAdditional Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": []
    }
]