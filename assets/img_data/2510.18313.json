[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18313/figures/teaser.png",
                "caption": "Figure 1:Versatile capabilities of OmniNWM.(a) State: Given a conditional image and ego trajectory, OmniNWM jointly generates comprehensive multi-modal outputs, including panoramic RGB, semantic, metric depth, and 3D occupancy videos.\n(b) Action: Conditioned on different input trajectories, OmniNWM facilitates precise panoramic camera control by converting them into normalized Plücker ray-maps as pixel-level guidance.\n(c) Reward: OmniNWM enables unbounded long-term navigation through a closed-loop pipeline: the planning trajectory guides the multi-modal generation, while dense rewards are derived from the generated 3D semantic occupancy. Note that all the input trajectories are encoded into normalized Plücker ray-maps before guiding the generation process, even when not explicitly visualized.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18313/figures/overall.png",
                "caption": "Figure 2:Architecture overview of OmniNWM. Our model takes as input a historical trajectory, a reference image, and flexible multi-level noise applied view-wise and frame-wise. It simultaneously forecasts panoramic RGB, semantic, and metric depth videos (Section3.1.1), along with 3D occupancy (Section3.1.2) and a future planning trajectory (Section3.1.3). Note that input trajectories guide the generation of the current video clip, while future trajectories are forecasted for the next clip, forming a closed-loop system.\nThe input trajectories with camera poses are encoded into normalized panoramic Plücker ray-maps, providing a unified pixel-level description and enabling precise action control (Section3.2).\nA flexible forcing strategy injects independent multi-level noise at the view and frame levels to support robust and flexible auto-regressive generation (Section3.3)",
                "position": 325
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18313/figures/occ_gen.png",
                "caption": "Figure 3:Occupancy generation. The generated panoramic RGB images, semantics, and depths are used to produce a 3D voxel volume. This design enables lightweight occupancy prediction compared with prior volumetric-based methods[43,105], while naturally supporting the integration of occupancy-grounded rewards.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/reward_gen.png",
                "caption": "Figure 4:Average rewards of different trajectories, computed using the proposed 3D occupancy-grounded reward function. The rewards effectively evaluate the feasibility of planning trajectories in the presence of obstacles (e.g., an oncoming truck).",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/ray_normal.png",
                "caption": "Figure 5:Panoramic normalized ray-map encoding.(a) Plücker ray-maps derived from panoramic camera poses are normalized in both scale and pose. (b) The ray-map normalization process constructs different trajectories within a unified 3D Plücker space. (c) Compared to the original NuScenes dataset, our strategy significantly enhances the diversity of trajectory distributions.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/flexible_forcing.png",
                "caption": "Figure 6:Flexible forcing strategy.\n(a) During training, independent multi-level noise is injected along view-wise and frame-wise dimensions.\n(b) During inference, flexible and robust generation is enabled via either frame-level auto-regression (many-to-one) or clip-level auto-regression (one-to-many or many-to-many).",
                "position": 559
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18313/figures/closed_loop_evaluation.png",
                "caption": "Figure 7:(a) Closed-loop evaluation of trajectory planning,reporting pass/fail counts, Scenario Pass Rate (SPR), and average rewards.(b) Distribution of average rewardsacross different methods on the NuScenes validation set.",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/distribution_camctrl.png",
                "caption": "Figure 8:Distribution of camera control performancewith the rotation and translation errors on the NuScenes validation set.",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/abl_forcing.png",
                "caption": "Figure 9:Visualization on the effect of flexible forcing strategy. Our approach effectively mitigates image degradation across multiple panoramic frames for long-term generation.",
                "position": 1401
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/zero_shot.png",
                "caption": "Figure 10:Zero-shot generalizationacross different new datasets and camera configurations without any fine-tuning.",
                "position": 1406
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material for OmniNWM",
        "images": []
    },
    {
        "header": "Appendix AMore Related Works and Discussions",
        "images": []
    },
    {
        "header": "Appendix BArchitecture Overview of OmniNWM-VLA",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18313/figures/OmniNWM_VLA.png",
                "caption": "Figure 11:Architecture overview of OmniNWM-VLA. The Tri-Modal Mamba-based Interpreter (Tri-MIDI) fuses visual, geometric, and semantic information into a unified latent representation to forecast waypoint coordinates and heading angles.",
                "position": 2927
            }
        ]
    },
    {
        "header": "Appendix CSemantic and Depth Data Curation",
        "images": []
    },
    {
        "header": "Appendix DTraining Objectives",
        "images": []
    },
    {
        "header": "Appendix ETraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18313/figures/vla_reward_compare.png",
                "caption": "Figure 12:Average rewardsfor specific scenarios across VLA planning baselines. Trajectories are evaluated using our occupancy-grounded reward function.",
                "position": 3058
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_panoramic_control1.png",
                "caption": "Figure 13:Precise panoramic camera control via normalized Plücker ray-maps. Given the same conditional frame, OmniNWM generates consistent multi-view videos by interpreting different input trajectories into pixel-level control signals.",
                "position": 3092
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_panoramic_control2.png",
                "caption": "Figure 14:Additional examples of panoramic camera control.",
                "position": 3095
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_panoramic_control3.png",
                "caption": "Figure 15:Additional examples of panoramic camera control.",
                "position": 3098
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_panoramic_control4.png",
                "caption": "Figure 16:Additional examples of panoramic camera control.",
                "position": 3101
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_panoramic_control_reverse.png",
                "caption": "Figure 17:Precise panoramic camera control of reversing trajectories via normalized Plücker ray-maps.",
                "position": 3104
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_long_term1.png",
                "caption": "Figure 18:Long-term navigation sequence (321 frames) generated through flexible forcing strategy. The model maintains temporal coherence and structural integrity beyond training sequence lengths, enabling extended closed-loop evaluation.",
                "position": 3107
            },
            {
                "img": "https://arxiv.org/html/2510.18313/figures/supple_multi_modal.png",
                "caption": "Figure 19:Joint multi-modal generation results, including panoramic RGB frames, semantic segmentation maps, metric depth estimations, and corresponding 3D semantic occupancy, demonstrating comprehensive scene generation capabilities.",
                "position": 3110
            }
        ]
    },
    {
        "header": "Appendix FMore Visualization Results",
        "images": []
    }
]