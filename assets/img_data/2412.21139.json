[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21139/x1.png",
                "caption": "Figure 1:SWE-Gym enables scalable improvements for software engineering agents.Top: Training time scaling shows consistent performance improvements as we obtain more training trajectories, with no signs of saturation at 491 trajectories. We use temperaturet=0𝑡0t=0italic_t = 0.Bottom: For inference time scaling, we generate a number of candidate trajectories per task and select the best using a verifier trained on SWE-Gym. This approach demonstrates roughly logarithmic gains with the number of sampled solutions.t=0𝑡0t=0italic_t = 0(excluded from regression) is used as the first hypothesis to be consistent with the top figure; later rollouts uset=0.5𝑡0.5t=0.5italic_t = 0.5.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2412.21139/x2.png",
                "caption": "Table 2:Statistics comparing SWE-Gym with the SWE-Bench test split (in parenthesis).\nExcept for size metrics, we report the average value across instances.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2412.21139/x2.png",
                "caption": "Figure 2:Repository distribution of SWE-Gym instances.",
                "position": 357
            }
        ]
    },
    {
        "header": "2SWE-Gym Environment",
        "images": []
    },
    {
        "header": "3Training LMs as SWE Agents with SWE-Gym",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21139/x3.png",
                "caption": "Figure 3:Success distribution over 30 rounds on SWE-Gym Lite with 7B model in zero-shot. The distribution is naturally biased toward easy tasks. Per instance capping reduces this bias but lowers the total trajectory count for training. We set temperaturet=1𝑡1t=1italic_t = 1during sampling.",
                "position": 692
            }
        ]
    },
    {
        "header": "4Scaling Agent Improvements with SWE-Gym",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21139/x4.png",
                "caption": "Figure 4:Scaling inference-time compute improves performance on SWE-Bench Verified using a fine-tuned verifier.\nBoth the agent and the verifier areQwen2.5-Coder-Instruct-32Bmodel fine-tuned on the corresponding dataset (§4.1.1). OpenHands(Wang et al.,2024c)is used as the agent scaffold.\nThe first rollout was performed with temperaturet=0𝑡0t=0italic_t = 0, andt=0.5𝑡0.5t=0.5italic_t = 0.5was used for the rest.",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2412.21139/x5.png",
                "caption": "Figure 5:Abaltion study for verifier training (§4.1.1). Performances are evaluated on SWE-Bench Verified.\nBoth the agent and the verifier areQwen2.5-Coder-Instruct-32Bmodel fine-tuned on the corresponding dataset (§4.1.1). OpenHands(Wang et al.,2024c)is used as the agent scaffold.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2412.21139/x6.png",
                "caption": "Figure 6:Scaling inference-time compute for MoatlessTools Agents with learned verifiers. We set temperaturet=0.5𝑡0.5t=0.5italic_t = 0.5during sampling.",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2412.21139/x7.png",
                "caption": "Figure 7:Model performance scaling with training data size.\nThe x-axis shows the percentage of training data used in log base 2 scale.",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2412.21139/x8.png",
                "caption": "Figure 8:Comparison of three data sampling approaches: without deduplication, repository-based sampling, and random sampling (§4.2). All variants use the 32B model evaluated on SWE-Bench Verified.",
                "position": 910
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparison with Concurrent Works",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    }
]