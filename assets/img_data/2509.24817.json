[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24817/x1.png",
                "caption": "",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x2.png",
                "caption": "Figure 2:Paradigm differences between previous works and UP2You.Top:Previous works like PuzzleAvatar[90]and AvatarBooth[97]compress unconstrained photos into implicit personal tokens and DreamBooth weights[66]through fine-tuning, then generate 3D humans via SDS optimization[66].Bottom:UP2You directly rectifies unconstrained photo collections into orthogonal view images and normals, then reconstructs textured human meshes, achieving superior quality while reducing processing time from 4 hours to 1.5 minutes.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x3.png",
                "caption": "Figure 3:Pipeline of UP2You.Given unconstrained input photosùêà\\mathbf{I}, we first predict the SMPL-X shape parameters ¬†(Sec.3.3) and initialize the SMPL-X mesh with predefined pose and expression parameters. We then generate orthogonal view imagesùêï\\mathbf{V}based onùêà\\mathbf{I}and SMPL-X normal renderingùêè\\mathbf{P}with the proposed PCFA method‚Äîpredict correlation mapsùêÇ\\mathbf{C}and select most informative features ¬†(Sec.3.1). Finally, we produce multi-view normal mapsùêç\\mathbf{N}fromùêè\\mathbf{P}andùêï\\mathbf{V}, and reconstruct the final textured mesh ¬†(Sec.3.2).",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x4.png",
                "caption": "Figure 4:Pose-Dependent Correlation Map.Correlation is colored asHigher‚Üí\\rightarrowLower.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x5.png",
                "caption": "Figure 5:Normal Map Generation Pipeline.The main input difference withFig.3is the generated multi-view orthogonal imagesùêï\\mathbf{V}, instead of unconstrained inputsùêà\\mathbf{I}.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x6.png",
                "caption": "Figure 6:Multi-reference Shape Predictor.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x7.png",
                "caption": "Figure 7:Inference Process of UP2You.Given only unconstrained photosùêà\\mathbf{I}as inputs, UP2You can generate a high-quality textured mesh.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x8.png",
                "caption": "Figure 8:Shape Prediction Error Map.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x9.png",
                "caption": "Figure 9:Qualitative Comparisons on PuzzleIOI and 4D-Dress.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x10.png",
                "caption": "Figure 10:Qualitative Comparisons on in-the-wild Data.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x10.png",
                "caption": "Figure 10:Qualitative Comparisons on in-the-wild Data.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x11.png",
                "caption": "Figure 11:UP2Youvs. PSHuman.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x12.png",
                "caption": "Figure 12:Predicted Correlation Maps.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x13.png",
                "caption": "Figure 13:Generated Multi-View Image Results with Different Number of References.With more references input, more results are noticed and generated by our model, like facial details and clothing patterns.",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x14.png",
                "caption": "Figure 14:Robustness of target pose conditions.Our method can generate high-quality multi-view images under different pose conditions with the same reference inputs, demonstrating that identity information is effectively disentangled from pose conditions in our approach.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x15.png",
                "caption": "Figure 15:Shape Predictor Helps to Generate More Identity-Consistent Results for People in Extreme Shape.",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x16.png",
                "caption": "Figure 16:Examples of 3D Virtual Try-On.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x17.png",
                "caption": "Figure 17:Failure Cases of UP2You.Since only 6 orthogonal views{0‚Äã¬∞,45‚Äã¬∞,90‚Äã¬∞,135‚Äã¬∞,180‚Äã¬∞,270‚Äã¬∞}\\{0\\degree,45\\degree,90\\degree,135\\degree,180\\degree,270\\degree\\}are generated, the backside texture of generated humans is lacking in guidance, making the problem of texture misalignment.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x18.png",
                "caption": "Figure 18:More Qualitative Comparisons on 4D-Dress and PuzzleIOI datasets.",
                "position": 2514
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x19.png",
                "caption": "Figure 19:More Qualitative Comparisons on in-the-wild dataset.",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x20.png",
                "caption": "Figure 20:More Qualitative Comparisons of Single Image 3D Human Reconstruction with PSHuman.",
                "position": 2522
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x21.png",
                "caption": "Figure 21:Error Maps of Shape Prediction.",
                "position": 2525
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x22.png",
                "caption": "Figure 22:Visualize Results of Correlation Maps.Given the input reference images and target pose for multi-view image generation, the predicted correlation maps can effectively identify and discriminate correlated regions within the reference inputs. For example, when generating images in the front-view, reference regions that correspond to front-facing views exhibit higher correlation values, demonstrating the model‚Äôs ability to selectively attend to relevant spatial information.",
                "position": 2536
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x23.png",
                "caption": "Figure 23:Visual Comparisons of Different Multi-View Image Generation Designs.",
                "position": 2548
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x24.png",
                "caption": "Figure 24:Generated Results of UP2You.",
                "position": 2559
            },
            {
                "img": "https://arxiv.org/html/2509.24817/x25.png",
                "caption": "Figure 25:Generated Results of UP2You.",
                "position": 2563
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]