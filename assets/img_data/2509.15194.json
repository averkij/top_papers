[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15194/x1.png",
                "caption": "Figure 1:Illustration of entropy collapse in TTRL. The figure tracks how majority-only TTRL drives models toward collapse—pass@n deteriorates, reasoning shrinks, and entropy vanishes—while our EVOL-RL method sustains diversity and reasoning complexity. Both methods are trained in a label-free setting using the MATH-500 test set on Qwen3-4B-Base.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15194/x2.png",
                "caption": "Figure 2:An overview of the EVOL-RL framework. For each prompt, the policy generates multiple responses. These are grouped by their final answer to identify the majority group. A novelty score is then computed for each response based on its semantic dissimilarity to others. Finally, a reward is assigned based on both majority (selection) and novelty (variation), guiding the policy update via GRPO. In the illustration, colors group responses by their final answer, while different marker shapes indicate semantically distinct reasoning paths.",
                "position": 153
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15194/x3.png",
                "caption": "Figure 3:Training dynamics for EVOL-RL and TTRL.Left:models trained onMATH-TRAIN.Middle:models trained onMATH-500.Right:models trained onAIME24. Each panel plots, over training steps, (i) Pass@1 onAIME25, (ii) average response length on the training set, and (iii) policy entropy on the training set.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2509.15194/x4.png",
                "caption": "Figure 4:Performance of EVOL-RL’s exploration-enhancing components when applied to a standard supervised GRPO baseline. The Qwen3-4B-Base model is trained on the MATH trainig set(Hendrycks et al.,2021)with a ground-truth verifier (RLVR).",
                "position": 710
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15194/x5.png",
                "caption": "Figure 5:Training dynamics of the majority-vote accuracy (maj@16) for EVOL-RL and TTRL. Each panel plots the accuracy of the consensus answer derived from 16 rollouts over the course of training. The training datasets are: (Left) MATH-TRAIN, (Middle) MATH-500, and (Right) AIME24.",
                "position": 1411
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    }
]