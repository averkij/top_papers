[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03147/extracted/6509393/fig/logo.png",
                "caption": "",
                "position": 50
            },
            {
                "img": "https://arxiv.org/html/2506.03147/x1.png",
                "caption": "",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2506.03147/x2.png",
                "caption": "",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2506.03147/x3.png",
                "caption": "Figure 1:Showcase of UniWorld’s versatile capabilities.The left two panels illustrate image perception and manipulation examples, and the right panel presents comparisons with state-of-the-art models and training data resources.",
                "position": 206
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03147/x4.png",
                "caption": "Figure 2:Empirical Observations of GPT-4o-Image.We verify local consistency of edits in (a). We explore the relationship between model comprehension and generation in (b)–(e), conducting observations within the GPT-4o architecture and across architectures using Qwen2.5VL-32B.",
                "position": 261
            }
        ]
    },
    {
        "header": "2Observation",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03147/x5.png",
                "caption": "Figure 3:Model architecture.The model consists of a VLM, SigLIP, DiT[28], and MLP connector. High-level semantics and historical state are provided by the VLM, while low-level image features are controlled by SigLIP. The understanding part uses a frozen VLM with an autoregressive approach, while the generation part is trained with flow matching. T5 (originally used for conditional injection) is optional during training or generation.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2506.03147/x6.png",
                "caption": "Figure 4:Zero-3 EMA.EMA model is initialized with Zero-3-style sharding across GPUs to reduce overhead. During each step, each GPU updates only its own shard.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2506.03147/x7.png",
                "caption": "Figure 5:Pipeline for mask generation.Given a reference image and a target image, the mask is obtained through (1) pixel-wise differencing, (2) dilation, (3) connected component filtering, and (4) max-pooling downsampling. The bottom right shows four different weighting functions. We highlight the limitations of steps (1), (2), and (3), which are addressed in the next stage.",
                "position": 359
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03147/x8.png",
                "caption": "Figure 6:Showcase of UniWorld-V1’s perception capabilities.This figure presents a qualitative comparison of UniWorld-V1’s perception tasks against GPT-4o, using randomly selected examples. Green boxes indicate correct responses, while red boxes highlight instances where the model’s output deviates from the expected result.",
                "position": 1480
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]