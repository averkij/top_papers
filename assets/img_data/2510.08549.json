[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08549/x1.png",
                "caption": "(a)",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x1.png",
                "caption": "(a)",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x2.png",
                "caption": "(b)",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x3.png",
                "caption": "(c)",
                "position": 209
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4The Entropy Regularizing Activation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08549/x4.png",
                "caption": "Figure 2:Main Results of ERA in Continuous Control.Aggregate normalized performance on HumanoidBench (6 tasks, with SAC), DMC (Humanoid & Dog) (6 tasks, with TD-MPC2), HumanoidBench (8 tasks, with FastSAC) and Mujoco Gym (4 tasks, with PPO). ERA consistently accelerates learning and achieves superior asymptotic performance.",
                "position": 504
            }
        ]
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08549/x5.png",
                "caption": "(a)",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x5.png",
                "caption": "(a)",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x6.png",
                "caption": "(b)",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x7.png",
                "caption": "Figure 4:Entropy comparison and pass@kkresults for GRPO with ERA (ours) versus GRPO alone.The entropy curves demonstrate that ERA mitigates entropy collapse and establishes a clear lower bound.\nThe pass@kkresults further indicate that ERA enhances exploration and strengthens the model’s reasoning ability.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x8.png",
                "caption": "Figure 5:Results on three OOD benchmarks (Qwen2.5-Math-7B).",
                "position": 943
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08549/x9.png",
                "caption": "Figure 7:Visualization of some continuous control environments used in our experiments.From left to right: dog-run (DMC), h1-hurdle-v0 (HumanoidBench), h1hand-slide-v0 (HumanoidBench), humanoid-walk (DMC)",
                "position": 1871
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x10.png",
                "caption": "Figure 8:Ablation of Environment Modifications for HumanoidBench.Performance comparison of a standard SAC agent on three challenging HumanoidBench tasks with and without our modified settings (action repeat of 2 and disabled termination). The significant performance gap justifies using these modified settings for our main SAC-based experiments.",
                "position": 1887
            }
        ]
    },
    {
        "header": "Appendix BProofs And Derivations",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08549/x11.png",
                "caption": "(a)",
                "position": 3227
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x11.png",
                "caption": "(a)",
                "position": 3230
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x12.png",
                "caption": "(b)",
                "position": 3235
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x13.png",
                "caption": "(c)",
                "position": 3240
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x14.png",
                "caption": "",
                "position": 3245
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x15.png",
                "caption": "Figure 10:Comparison between state-level and batch-level entropy regularization methods on DMC Dog & Humanoid suites.Both methods outperform the SAC baseline.",
                "position": 3262
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x16.png",
                "caption": "Figure 11:Learning curves of SAC-ERA and SAC on Mujoco Gym environments.SAC-ERA demonstrates very slight advantages over SAC.",
                "position": 3290
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x17.png",
                "caption": "(a)",
                "position": 3311
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x17.png",
                "caption": "(a)",
                "position": 3314
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x18.png",
                "caption": "(b)",
                "position": 3319
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x19.png",
                "caption": "Figure 13:Performance comparison of PPO-ERA against EAPO on MuJoCo benchmark tasks.",
                "position": 3367
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x20.png",
                "caption": "Figure 14:Performance comparison of SAC-ERA against MNSE on MuJoCo benchmark tasks.",
                "position": 3371
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x21.png",
                "caption": "Figure 15:Time comparison on HumanoidBench.We compare the training time of FastTD3 and FastSAC-ERAon HumanoidBench. The results show that using ERAintroduces a modest time overhead, averaging around 6% of the total training time, which is a reasonable trade-off for the improved exploration performance and sample efficiency it provides.",
                "position": 3385
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x22.png",
                "caption": "Figure 16:Comparison of different regularization methods on the CIFAR-10 dataset.The left subplot shows the Top-1 accuracy, and the right subplot shows the Top-5 accuracy. Our method, ERA  is compared against varying intensities of Label Smoothing and Dropout.",
                "position": 3403
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x23.png",
                "caption": "Figure 17:Time comparison on CIFAR-10.We compare the training time of ResNet and ResNet-ERAon CIFAR-10. The results show that using ERA introduces almost no time overhead.",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x24.png",
                "caption": "Figure 18:Entropy curve during two-stage training.After decreasingωlow\\omega_{\\text{low}}, the entropy rapidly drops and stabilizes at the second-level entropy lower bound, showing that ERA enforces a non-trivial entropy floor.",
                "position": 3432
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x25.png",
                "caption": "Figure 19:Detailed entropy analysis.Left: average entropy of the top20%20\\%tokens (HrespH_{\\text{resp}}) and the bottom80%80\\%tokens. Right: proportion of responses (running average with window size2020) withHresp<ωlowH_{\\text{resp}}<\\omega_{\\text{low}}orHresp>ωhighH_{\\text{resp}}>\\omega_{\\text{high}}, demonstrating ERA’s ability to prevent both entropy collapse and overly high entropy.",
                "position": 3439
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x26.png",
                "caption": "Figure 20:Comparison of ERA with and withoutωhigh\\omega_{\\text{high}}. The entropy of ERA withoutωhigh\\omega_{\\text{high}}tends to explode within a very short number of steps, leading to the collapse of model training.",
                "position": 3450
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x27.png",
                "caption": "Figure 21:Comparison of computation time between GRPO and ERA, measured bytiming_s/old_log_probat the first step. ERA introduces only about a5.6%5.6\\%overhead.",
                "position": 3462
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x28.png",
                "caption": "Figure 22:Training curves of OBAC and OBAC-ERA on HumanoidBench environments.",
                "position": 3471
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x29.png",
                "caption": "Figure 23:Training curves of TD-MPC2 and TD-MPC2-ERA on DMC environments.",
                "position": 3475
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x30.png",
                "caption": "Figure 24:Training curves of PPO and PPO-ERA on Mujoco Gym environments.",
                "position": 3479
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x31.png",
                "caption": "Figure 25:Training curves of FastTD3 and FastSAC-ERA on HumanoidBench environments.",
                "position": 3483
            },
            {
                "img": "https://arxiv.org/html/2510.08549/x32.png",
                "caption": "Figure 26:Training curves of SAC and SAC-ERA on HumanoidBench and DMC environments.",
                "position": 3487
            }
        ]
    },
    {
        "header": "Appendix DThe Use of Large Language Models in This Paper",
        "images": []
    }
]