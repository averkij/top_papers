[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21643/x1.png",
                "caption": "",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21643/x2.png",
                "caption": "Figure 2:Comparisonbetween separated architectures for weather understanding / generation (top) and unified framework with shared self-attention (bottom).",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21643/x3.png",
                "caption": "Figure 3:Framework and Task paradigmof Omni-Weather.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x4.png",
                "caption": "Figure 4:Construction of our CoT data.\nFirst, we preprocess the raw SEVIR data to obtain high-quality input / output frame pairs.\nSecond, we carefully design prompts and leverage GPT-4o for attributes annotation.\nThird, the annotated attributes are incorporated into CoT prompts to generate CoT annotations, followed by a quality verification step to produce the final CoT dataset.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21643/x5.png",
                "caption": "Figure 5:A set of qualitative results.We show two radar inversion examples with think traces, a nowcasting case where Omni-Weather (with think trace) is compared against CasCast, DiffCast, and EarthFormer, and one example each of radar image and sequence understanding with attribute scores and textual evaluations. Omni-Weather surpasses all baselines.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x6.png",
                "caption": "Figure 6:Effect of mixed data.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x6.png",
                "caption": "Figure 6:Effect of mixed data.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x7.png",
                "caption": "Figure 7:Case study with thinking.",
                "position": 710
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21643/x8.png",
                "caption": "Figure 8:Radar Inversion Thinking Comparison.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x9.png",
                "caption": "Figure 9:Radar Nowcasting Thinking Comparison.",
                "position": 1104
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x10.png",
                "caption": "Figure 10:Qualitative Result 1.",
                "position": 1252
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x11.png",
                "caption": "Figure 11:Qualitative Result 2.",
                "position": 1256
            },
            {
                "img": "https://arxiv.org/html/2512.21643/x12.png",
                "caption": "Figure 12:Qualitative Result 3.",
                "position": 1260
            }
        ]
    },
    {
        "header": "Appendix Aappendix",
        "images": []
    }
]