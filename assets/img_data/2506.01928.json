[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x1.png",
                "caption": "Figure 1:Efficient generation of an example sequence with our flagship model Eso-LM (B). DuringDiffusionPhase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. DuringSequentialPhase, Eso-LMs denoise the remaining mask tokens one at a time from left to right. Eso-LM (B) allows forKV caching in both phasesusing justa single unified KV cache:bluebounding boxes enclose transformer cells that are building their KV cache; a cell becomesblueonce its KV cache is built. The sequences below the transformers depict tokens in their natural order.",
                "position": 211
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Esoteric Language Models",
        "images": []
    },
    {
        "header": "4Designing Attention Mechanisms for the Shared Denoising Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x2.png",
                "caption": "Figure 2:Comparison of attention biases for diffusion-phase training.Orangerepresents 0 (attention) andgrayrepresentsâˆ’âˆ-\\infty- âˆ(no attention). The clean sequence isğ±=(A,B,C,D,E,F)ğ±ğ´ğµğ¶ğ·ğ¸ğ¹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F ). After random masking, we obtainğ³t=(A,M,C,M,M,F)subscriptğ³ğ‘¡ğ´ğ‘€ğ¶ğ‘€ğ‘€ğ¹{\\mathbf{z}}_{t}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withâ„³â¢(ğ³t)={2,4,5}â„³subscriptğ³ğ‘¡245\\mathcal{M}({\\mathbf{z}}_{t})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }andğ’â¢(ğ³t)={1,3,6}ğ’subscriptğ³ğ‘¡136\\mathcal{C}({\\mathbf{z}}_{t})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering isÏƒ=(3,1,6,4,5,2)âˆ¼ğ’«6ğœ316452similar-tosubscriptğ’«6\\sigma=(3,1,6,4,5,2)\\sim\\mathcal{P}_{6}italic_Ïƒ = ( 3 , 1 , 6 , 4 , 5 , 2 ) âˆ¼ caligraphic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPTwith clean tokens before mask tokens.Redhighlights differences between Eso-LM (A) and Eso-LM (B).",
                "position": 507
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x3.png",
                "caption": "Figure 3:Gen. PPL vs. NFEs for models trained on OWT. Entropy values are in Tables (8,9,10).",
                "position": 1311
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x3.png",
                "caption": "Figure 3:Gen. PPL vs. NFEs for models trained on OWT. Entropy values are in Tables (8,9,10).",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x4.png",
                "caption": "Figure 4:Eso-LM (B) establishes SOTA on the Pareto frontier of sampling speed and quality.",
                "position": 1317
            }
        ]
    },
    {
        "header": "6Related Work, Discussion, and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/extracted/6505030/figures/bd3lm_analysis.png",
                "caption": "Figure 5:Gen. Perplexity (â†“â†“\\downarrowâ†“) with nucleus sampling (p=0.9ğ‘0.9p=0.9italic_p = 0.9) against the number of sampling steps for AR, MDLM and BD3-LMs trained for 1 million steps. The number of sampling steps for AR is always 1024; we extend it to other values for easier comparison. The number next to each data point records its sample entropy (â†‘â†‘\\uparrowâ†‘); a value less than 5 usually indicates mode collapse.",
                "position": 1957
            }
        ]
    },
    {
        "header": "Appendix BEsoteric Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x5.png",
                "caption": "Figure 6:Comparison of attention biases for sequential-phase training.Orangerepresents 0 (attention) andgrayrepresentsâˆ’âˆ-\\infty- âˆ(no attention). The clean sequence isğ±=(A,B,C,D,E,F)ğ±ğ´ğµğ¶ğ·ğ¸ğ¹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F )and henceL=6ğ¿6L=6italic_L = 6. After random masking, we obtainğ³0=(A,M,C,M,M,F)subscriptğ³0ğ´ğ‘€ğ¶ğ‘€ğ‘€ğ¹{\\mathbf{z}}_{0}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withâ„³â¢(ğ³0)={2,4,5}â„³subscriptğ³0245\\mathcal{M}({\\mathbf{z}}_{0})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }andğ’â¢(ğ³0)={1,3,6}ğ’subscriptğ³0136\\mathcal{C}({\\mathbf{z}}_{0})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering amongğ’â¢(ğ³0)ğ’subscriptğ³0{\\mathcal{C}}({\\mathbf{z}}_{0})caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )is(3,1,6)316(3,1,6)( 3 , 1 , 6 ).Redhighlights the differences between Eso-LM (A) and Eso-LM (B).",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x6.png",
                "caption": "Figure 7:Comparison of attention biases for diffusion-phase training, before and after sorting the rows and columns byÏƒğœ\\sigmaitalic_Ïƒ.Orangerepresents 0 (attention) andgrayrepresentsâˆ’âˆ-\\infty- âˆ(no attention). The clean sequence isğ±=(A,B,C,D,E,F)ğ±ğ´ğµğ¶ğ·ğ¸ğ¹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F )and henceL=6ğ¿6L=6italic_L = 6. After random masking, we obtainğ³t=(A,M,C,M,M,F)subscriptğ³ğ‘¡ğ´ğ‘€ğ¶ğ‘€ğ‘€ğ¹{\\mathbf{z}}_{t}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withâ„³â¢(ğ³t)={2,4,5}â„³subscriptğ³ğ‘¡245\\mathcal{M}({\\mathbf{z}}_{t})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }andğ’â¢(ğ³t)={1,3,6}ğ’subscriptğ³ğ‘¡136\\mathcal{C}({\\mathbf{z}}_{t})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering sampled isÏƒ=(3,1,6,4,5,2)âˆ¼ğ’«6ğœ316452similar-tosubscriptğ’«6\\sigma=(3,1,6,4,5,2)\\sim\\mathcal{P}_{6}italic_Ïƒ = ( 3 , 1 , 6 , 4 , 5 , 2 ) âˆ¼ caligraphic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPTwith clean tokens before mask tokens.Redhighlights the differences between Eso-LM (A) and Eso-LM (B).",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x7.png",
                "caption": "Figure 8:Comparison of attention biases for sequential-phase training, before and after sorting the rows and columns of each of the fourLÃ—Lğ¿ğ¿L\\times Litalic_L Ã— italic_Lblocks byÏƒğœ\\sigmaitalic_Ïƒ.Orangerepresents 0 (attention) andgrayrepresentsâˆ’âˆ-\\infty- âˆ(no attention). The clean sequence isğ±=(A,B,C,D,E,F)ğ±ğ´ğµğ¶ğ·ğ¸ğ¹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F )and henceL=6ğ¿6L=6italic_L = 6. After random masking, we obtainğ³0=(A,M,C,M,M,F)subscriptğ³0ğ´ğ‘€ğ¶ğ‘€ğ‘€ğ¹{\\mathbf{z}}_{0}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withâ„³â¢(ğ³0)={2,4,5}â„³subscriptğ³0245\\mathcal{M}({\\mathbf{z}}_{0})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }andğ’â¢(ğ³0)={1,3,6}ğ’subscriptğ³0136\\mathcal{C}({\\mathbf{z}}_{0})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering amongğ’â¢(ğ³0)ğ’subscriptğ³0{\\mathcal{C}}({\\mathbf{z}}_{0})caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )is(3,1,6)316(3,1,6)( 3 , 1 , 6 ).Redhighlights the differences between Eso-LM (A) and Eso-LM (B).Greenhighlights the extra connections added for clean tokens inğ³0subscriptğ³0{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTso that the attention biases display useful patterns after sorting â€“ they donâ€™t affect the transformer output because no other token attends to clean tokens inğ³0subscriptğ³0{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x8.png",
                "caption": "Figure 9:Efficient generation of an example sequence with Eso-LM (A). DuringDiffusionPhase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. DuringSequentialPhase, Eso-LMs denoise the remaining mask tokens one at a time from left to right. Eso-LM (A) doesnâ€™t process mask tokens not scheduled for denoising and allows forKV caching in sequential phase:bluebounding boxes enclose transformer cells that are building their KV cache; a cell becomesblueonce its KV cache is built. The sequences below the transformers depict tokens in their natural order.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x9.png",
                "caption": "Figure 10:(Copy of Fig.1) Efficient generation of an example sequence with Eso-LM (B). DuringDiffusionPhase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. DuringSequentialPhase, Eso-LMs denoise the remaining mask tokens one at a time from left to right. Eso-LM (B) allows forKV caching in both phasesusing justa single unified KV cache:bluebounding boxes enclose transformer cells that are building their KV cache; a cell becomesblueonce its KV cache is built. The sequences below the transformers depict tokens in their natural order.",
                "position": 2346
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": []
    }
]