[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x1.png",
                "caption": "Figure 1:Efficient generation of an example sequence with our flagship model Eso-LM (B). DuringDiffusionPhase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. DuringSequentialPhase, Eso-LMs denoise the remaining mask tokens one at a time from left to right. Eso-LM (B) allows forKV caching in both phasesusing justa single unified KV cache:bluebounding boxes enclose transformer cells that are building their KV cache; a cell becomesblueonce its KV cache is built. The sequences below the transformers depict tokens in their natural order.",
                "position": 211
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Esoteric Language Models",
        "images": []
    },
    {
        "header": "4Designing Attention Mechanisms for the Shared Denoising Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x2.png",
                "caption": "Figure 2:Comparison of attention biases for diffusion-phase training.Orangerepresents 0 (attention) andgrayrepresents−∞-\\infty- ∞(no attention). The clean sequence is𝐱=(A,B,C,D,E,F)𝐱𝐴𝐵𝐶𝐷𝐸𝐹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F ). After random masking, we obtain𝐳t=(A,M,C,M,M,F)subscript𝐳𝑡𝐴𝑀𝐶𝑀𝑀𝐹{\\mathbf{z}}_{t}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withℳ⁢(𝐳t)={2,4,5}ℳsubscript𝐳𝑡245\\mathcal{M}({\\mathbf{z}}_{t})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }and𝒞⁢(𝐳t)={1,3,6}𝒞subscript𝐳𝑡136\\mathcal{C}({\\mathbf{z}}_{t})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering isσ=(3,1,6,4,5,2)∼𝒫6𝜎316452similar-tosubscript𝒫6\\sigma=(3,1,6,4,5,2)\\sim\\mathcal{P}_{6}italic_σ = ( 3 , 1 , 6 , 4 , 5 , 2 ) ∼ caligraphic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPTwith clean tokens before mask tokens.Redhighlights differences between Eso-LM (A) and Eso-LM (B).",
                "position": 507
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x3.png",
                "caption": "Figure 3:Gen. PPL vs. NFEs for models trained on OWT. Entropy values are in Tables (8,9,10).",
                "position": 1311
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x3.png",
                "caption": "Figure 3:Gen. PPL vs. NFEs for models trained on OWT. Entropy values are in Tables (8,9,10).",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x4.png",
                "caption": "Figure 4:Eso-LM (B) establishes SOTA on the Pareto frontier of sampling speed and quality.",
                "position": 1317
            }
        ]
    },
    {
        "header": "6Related Work, Discussion, and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/extracted/6505030/figures/bd3lm_analysis.png",
                "caption": "Figure 5:Gen. Perplexity (↓↓\\downarrow↓) with nucleus sampling (p=0.9𝑝0.9p=0.9italic_p = 0.9) against the number of sampling steps for AR, MDLM and BD3-LMs trained for 1 million steps. The number of sampling steps for AR is always 1024; we extend it to other values for easier comparison. The number next to each data point records its sample entropy (↑↑\\uparrow↑); a value less than 5 usually indicates mode collapse.",
                "position": 1957
            }
        ]
    },
    {
        "header": "Appendix BEsoteric Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01928/x5.png",
                "caption": "Figure 6:Comparison of attention biases for sequential-phase training.Orangerepresents 0 (attention) andgrayrepresents−∞-\\infty- ∞(no attention). The clean sequence is𝐱=(A,B,C,D,E,F)𝐱𝐴𝐵𝐶𝐷𝐸𝐹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F )and henceL=6𝐿6L=6italic_L = 6. After random masking, we obtain𝐳0=(A,M,C,M,M,F)subscript𝐳0𝐴𝑀𝐶𝑀𝑀𝐹{\\mathbf{z}}_{0}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withℳ⁢(𝐳0)={2,4,5}ℳsubscript𝐳0245\\mathcal{M}({\\mathbf{z}}_{0})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }and𝒞⁢(𝐳0)={1,3,6}𝒞subscript𝐳0136\\mathcal{C}({\\mathbf{z}}_{0})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering among𝒞⁢(𝐳0)𝒞subscript𝐳0{\\mathcal{C}}({\\mathbf{z}}_{0})caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )is(3,1,6)316(3,1,6)( 3 , 1 , 6 ).Redhighlights the differences between Eso-LM (A) and Eso-LM (B).",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x6.png",
                "caption": "Figure 7:Comparison of attention biases for diffusion-phase training, before and after sorting the rows and columns byσ𝜎\\sigmaitalic_σ.Orangerepresents 0 (attention) andgrayrepresents−∞-\\infty- ∞(no attention). The clean sequence is𝐱=(A,B,C,D,E,F)𝐱𝐴𝐵𝐶𝐷𝐸𝐹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F )and henceL=6𝐿6L=6italic_L = 6. After random masking, we obtain𝐳t=(A,M,C,M,M,F)subscript𝐳𝑡𝐴𝑀𝐶𝑀𝑀𝐹{\\mathbf{z}}_{t}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withℳ⁢(𝐳t)={2,4,5}ℳsubscript𝐳𝑡245\\mathcal{M}({\\mathbf{z}}_{t})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }and𝒞⁢(𝐳t)={1,3,6}𝒞subscript𝐳𝑡136\\mathcal{C}({\\mathbf{z}}_{t})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering sampled isσ=(3,1,6,4,5,2)∼𝒫6𝜎316452similar-tosubscript𝒫6\\sigma=(3,1,6,4,5,2)\\sim\\mathcal{P}_{6}italic_σ = ( 3 , 1 , 6 , 4 , 5 , 2 ) ∼ caligraphic_P start_POSTSUBSCRIPT 6 end_POSTSUBSCRIPTwith clean tokens before mask tokens.Redhighlights the differences between Eso-LM (A) and Eso-LM (B).",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x7.png",
                "caption": "Figure 8:Comparison of attention biases for sequential-phase training, before and after sorting the rows and columns of each of the fourL×L𝐿𝐿L\\times Litalic_L × italic_Lblocks byσ𝜎\\sigmaitalic_σ.Orangerepresents 0 (attention) andgrayrepresents−∞-\\infty- ∞(no attention). The clean sequence is𝐱=(A,B,C,D,E,F)𝐱𝐴𝐵𝐶𝐷𝐸𝐹{\\mathbf{x}}=(A,B,C,D,E,F)bold_x = ( italic_A , italic_B , italic_C , italic_D , italic_E , italic_F )and henceL=6𝐿6L=6italic_L = 6. After random masking, we obtain𝐳0=(A,M,C,M,M,F)subscript𝐳0𝐴𝑀𝐶𝑀𝑀𝐹{\\mathbf{z}}_{0}=(A,M,C,M,M,F)bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ( italic_A , italic_M , italic_C , italic_M , italic_M , italic_F ). The integers denote the position indices withℳ⁢(𝐳0)={2,4,5}ℳsubscript𝐳0245\\mathcal{M}({\\mathbf{z}}_{0})=\\{2,4,5\\}caligraphic_M ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 2 , 4 , 5 }and𝒞⁢(𝐳0)={1,3,6}𝒞subscript𝐳0136\\mathcal{C}({\\mathbf{z}}_{0})=\\{1,3,6\\}caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = { 1 , 3 , 6 }. The random ordering among𝒞⁢(𝐳0)𝒞subscript𝐳0{\\mathcal{C}}({\\mathbf{z}}_{0})caligraphic_C ( bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )is(3,1,6)316(3,1,6)( 3 , 1 , 6 ).Redhighlights the differences between Eso-LM (A) and Eso-LM (B).Greenhighlights the extra connections added for clean tokens in𝐳0subscript𝐳0{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTso that the attention biases display useful patterns after sorting – they don’t affect the transformer output because no other token attends to clean tokens in𝐳0subscript𝐳0{\\mathbf{z}}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x8.png",
                "caption": "Figure 9:Efficient generation of an example sequence with Eso-LM (A). DuringDiffusionPhase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. DuringSequentialPhase, Eso-LMs denoise the remaining mask tokens one at a time from left to right. Eso-LM (A) doesn’t process mask tokens not scheduled for denoising and allows forKV caching in sequential phase:bluebounding boxes enclose transformer cells that are building their KV cache; a cell becomesblueonce its KV cache is built. The sequences below the transformers depict tokens in their natural order.",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2506.01928/x9.png",
                "caption": "Figure 10:(Copy of Fig.1) Efficient generation of an example sequence with Eso-LM (B). DuringDiffusionPhase, Eso-LMs denoise one or more, potentially non-neighboring mask tokens (M) per step. DuringSequentialPhase, Eso-LMs denoise the remaining mask tokens one at a time from left to right. Eso-LM (B) allows forKV caching in both phasesusing justa single unified KV cache:bluebounding boxes enclose transformer cells that are building their KV cache; a cell becomesblueonce its KV cache is built. The sequences below the transformers depict tokens in their natural order.",
                "position": 2346
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": []
    }
]