[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20470/figure/logo.png",
                "caption": "",
                "position": 59
            },
            {
                "img": "https://arxiv.org/html/2510.20470/x1.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2510.20470/x2.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20470/x3.png",
                "caption": "Figure 1:a)Reasoning Trace Construction.b)Data Example.c)Multi-stage Progressive Cold-start, including textual, multimodal alignment, and vision-centric reasoning stages.d)The Joint Identification-Reasoning-Action RLVR.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Dataset Construction",
        "images": []
    },
    {
        "header": "4Training Procedure",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20470/x4.png",
                "caption": "Figure 2:Training dynamics in the AIR RLVR process of Conan.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2510.20470/x5.png",
                "caption": "Figure 3:A qualitative example from VRBench showing the reasoning traces of Video-R1 (Text CoT), Video-MTR (Video CoT), and Conan for comparison.",
                "position": 798
            }
        ]
    },
    {
        "header": "6Conclusion and Future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]