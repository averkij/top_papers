[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14677/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14677/x2.png",
                "caption": "Figure 2:Definitions and motivations for MatAnyone.\n(a) In a matting frame, the image can be broadly divided into two areas based on the alpha value: thecore(semantic) and theboundary(fine-details). Thecoreincludes the background (alpha values of 0) and the solid foreground (alpha values of 1), while theboundary(highlighted in pink) encompasses areas with alpha values between 0 and 1.\n(b) Due to the under-defined setting, auxiliary-free methods like RVM[33]are easily confused by ambiguous background. Meanwhile, mask-guided methods like MaGGIe[22]tend to break the segmentation prior they aim to leverage, due to the deficiency in video matting data.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14677/x3.png",
                "caption": "Figure 3:An overview of MatAnyone.\nMatAnyone is a memory-based framework for video matting. Given a target segmentation map in the first frame, our model achieves stable and high-quality matting through consistent memory propagation, with a region-adaptive memory fusion module to combine information from the previous and current frame. To overcome the scarcity of real video matting data, we incorporate a new training strategy that effectively leverages matting data for fine-grained matting details and segmentation data for semantic stability, with designed losses separately.",
                "position": 212
            }
        ]
    },
    {
        "header": "4Data",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14677/x4.png",
                "caption": "Figure 4:Qualitative comparisons on real-world videos.\nOur MatAnyone significantly outperforms existing auxiliary-free (RVM[33]) and mask-guided (FTP-VM[21]and MaGGIe[22]) approaches in both detail extraction and semantic accuracy. For the lowest row, while other methods all miss out on important body parts (i.e., head) and mistakenly take background pixels as foreground (due to similar colors), thus generating messy outputs, our method presents an accurate and visually clean output by even identifying the shadow near the boundary.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x5.png",
                "caption": "Figure 5:Quantitative comparisons with MaGGIe[22]on instance video matting. Despite MaGGIe using instance mask as guidance for each frame, our method shows better performance, achieving better stability in object tracking and finer alpha matte details.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x6.png",
                "caption": "Figure 6:Improvement with Recurrent refinement.(Zoom-in for best view)",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x7.png",
                "caption": "Figure 7:Comparison of matting results training with original DDC loss[35]and with scaled DDC loss, where the latter gives more stable and natural matting results.",
                "position": 764
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "GArchitecture",
        "images": []
    },
    {
        "header": "HTraining",
        "images": []
    },
    {
        "header": "IDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.14677/x8.png",
                "caption": "Figure 8:Issues with VideoMatte240K[32].(a) Errors in alpha values exist in reflective regions (e.g., ‚Äúa hole‚Äù on glasses).\n(b) Inhomogeneous alpha values exist in core regions (e.g., caused by shadow), where the alpha value should be exactly 0 or 1.",
                "position": 2079
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x9.png",
                "caption": "Figure 9:Gallery for our new training dataset VM800.High-quality details in the boundary regions and diversity in terms of gender, hairstyles, and aspect ratios could be clearly observed.",
                "position": 2085
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x10.png",
                "caption": "Figure 10:Harmonization on synthetic benchmarks and its effect on model performance.Harmonization[23]is an operation that makes the composited frame more natural and realistic, which also effectively makes our YouTubeMatte a more challenging benchmark that is closer to the real distribution.\nIt is observed that while RVM[33]is confused by the harmonized frame, our method still yields robust performance.",
                "position": 2111
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x11.png",
                "caption": "Figure 11:(a) Comparison on results trained with old training data (VideoMatte240K[32]) and new training data (our VM800).It could be observed that training with old data will lead to errors in reflective objects (e.g., holes on the sunglasses) and inhomogeneous alpha values in the core regions. However, both issues are fixed when training with our new data, indicating a higher quality.(b) Comparison on results trained without and with core-area supervision.It could be observed that training without it will lead to semantics error due to the weak supervision from real segmentation data, while training with core supervision largely improves semantics accuracy thanks to the stronger supervision enabled.",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x12.png",
                "caption": "Figure 12:Comparison on results with and without Consistent Memory Propagation.It could be observed that when CMP is not applied, semantic errors constantly exist across a wide span of video frames. However, when training with CMP, we observe from the ‚ÄúChange Probability‚Äù mask that usually our model only takes pixels near the boundary as ‚Äúchanged‚Äù, and most of the inner regions (i.e., earring) will mainly take the memory values from the last frame. As we can see on the figure, while predictions are both correct at timetùë°titalic_t, the model with CMP successfully keeps the correctness and gives stable results, while the model without CMP quickly breaks the correctness and never recovers.",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x13.png",
                "caption": "Figure 13:Comparison on results with iterative refinement.A noticeable enhancement on details can be observed even with one iteration of refinement compared with the given segmentation mask. Within 10 iterations, our model is able to achieve matting details at an image-matting level, even better than Matte Anything[56], which is an image matting model also based on the results from SAM[25].",
                "position": 2176
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x14.png",
                "caption": "Figure 14:More qualitative comparisons on general video matting with SOTA methods.We compare our MatAnyone with both auxiliary-free (AF) method: RVM[33]and mask-guided methods: FTP-VM[21], and MaGGIe[22].\nIt could be observed that our method significantly outperforms others in both detail extraction and semantic accuracy, across diverse and complex real scenarios.\nIt is noteworthy that although sometimes MaGGIe[22]seems to give acceptable results when compositing with a green screen, its alpha matte turns out to be noisy (i.e., inhomogeneous in the core foreground region and blurry in the boundary region), while our alpha matte is clean with fine-grained details in the boundary region. As a result, we also include alpha mattes for a more comprehensive comparison.(Zoom in for best view)",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x15.png",
                "caption": "Figure 15:A challenging example of general video matting across a long time span.We compare our MatAnyone with both auxiliary-free (AF) method: RVM[33]and mask-guided methods: FTP-VM[21], and MaGGIe[22].\nIt could be observed that our model is able to track the target object stably even when the object is moving fast in a highly complex scene, where all the other methods present noticeable failures.(Zoom in for best view)",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x16.png",
                "caption": "Figure 16:Another challenging example of general video matting across a long time span.We compare our MatAnyone with both auxiliary-free (AF) method: RVM[33]and mask-guided methods: FTP-VM[21], and MaGGIe[22].\nThis example showcases that our model is able to track the target objects even in a highly ambiguous background, where the colors for foreground and background are similar, and also multiple humans in the background.\nIn addition, it also demonstrates when there is more than one target object, our model is still able to handle this challenging case well.(Zoom in for best view)",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2501.14677/x17.png",
                "caption": "Figure 17:More qualitative comparisons on instance matting.We compare our MatAnyone with MaGGIe[22], a mask-guided method that requires the instance mask foreachframe, while our method only requires the mask for thefirstframe.\nIt could be observed that even with such strong given prior, MaGGIe still performs below our method in terms of semantic accuracy in the core regions.\nMoreover, in terms of the boundary regions, by examining the details there, we could clearly observe that the details generated by MaGGIe are blurry and far from fine-grained compared with our results.(Zoom in for best view)",
                "position": 2210
            }
        ]
    },
    {
        "header": "JMore Results",
        "images": []
    }
]