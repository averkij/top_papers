[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TheTacThruSensor",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09851/fig_hardware_v2_c",
                "caption": "Figure 1:Fabrication of theTacThrusensor and integration into theTacThru-UMIsystem.(a) The keyline marker elastomer is fabricated by sequentially spraying inner (black) and outer (white) markers on transparent elastomer using laser-cut masks. (b) TheTacThrusensor features an extended linkage that serves as gripper fingers. (c) TheTacThru-UMIplatform includes a robot end-effector (left) and a data collector (right) that share identical body and finger designs, with the fingers actuated by an Inspire LAS30-021D servo electric cylinder with a maximum opening width of72mm72\\text{\\,}\\mathrm{m}\\mathrm{m}.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2512.09851/fig_markers_c",
                "caption": "Figure 2:Keyline marker design and filtering enable robust tracking.(a) Evaluation setup compares two sensor types (keylinevs. solid markers) during bottle grasping tasks. (b) TheTacThrusensor view comparison shows that keyline markers (left) remain distinct against complex backgrounds, while solid markers (right) become invisible. (c) Quantitative results demonstrate our filtered keyline method achieves stable tracking of all 64 markers while keeping efficiency (6.08ms6.08\\text{\\,}\\mathrm{m}\\mathrm{s}processing time per frame), while solid markers suffer missed detections and unfiltered keyline detection produces false positives (count > 64).",
                "position": 195
            }
        ]
    },
    {
        "header": "4Learning Manipulation withTacThru-UMI",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09851/fig_model_c",
                "caption": "Figure 3:Diffusion policy architecture forTacThru-UMI.Multimodal observations—wrist-camera RGB images, sensor RGB images, detected marker deviations, and proprioception—are encoded into tokens and augmented with positional and modality-specific embeddings. These tokens condition a Transformer-based diffusion policy that denoises Gaussian noise into action chunks for robot execution. The example shows how the policy leverages theTacThru’s close-up view to align the cap and mount during theInsertCaptask.",
                "position": 236
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09851/fig_task_settings_c",
                "caption": "Figure 4:Task demonstrations across five manipulation scenarios.(a)PickBottle: basic pick-and-place, (b)PullTissue: thin-and-soft object manipulation, (c)SortBolt: visual discrimination, (d)HangScissors: tactile discrimination, (e)InsertCap: multimodal fusion.Top:Initial object configurations.Middle:Wrist-camera view progression during demonstration.Bottom:CorrespondingTacThru(top) and GelSight sensor (bottom) observations, illustrating distinct sensing modalities and information content.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2512.09851/fig_res_q",
                "caption": "Figure 5:Quantitative results across manipulation tasks and sensing modalities.Success rates for four policy variants:TT-M(TacThruwith markers),TT(TacThruonly),GS-M(GelSight with markers), andWrist(vision-only). Each task evaluates specific sensing capabilities: basic manipulation (PickBottle), thin-and-soft object manipulation (PullTissue), visual discrimination (SortBolt), tactile discrimination (HangScissors), and multimodal fusion (InsertCap). Error bars show standard deviation across evaluation runs. The rightmost column presents overall performance averages.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2512.09851/fig_res_c",
                "caption": "Figure 6:Qualitative policy rollouts demonstrating manipulation execution.Each column (a-f) shows the temporal progression of a task execution from top to bottom, with three synchronized views: third-person perspective (left), wrist camera (center), andTacThruclose-up (right) with tracked marker deviations overlaid (4× magnified for visibility). Colored annotations highlight key manipulation phases and sensing feedback. See supplementary video for additional rollouts across all policy variants.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2512.09851/res_bolts_c",
                "caption": "Figure 7:Bolt sorting performance analysis across sensing modalities.(a) Confusion matrices showing placement accuracy for each policy variant. Rows indicate ground-truth bolt type, columns show predicted placement (bowls A-C or “Out” for misses).TacThru-based policies (TT-M,TT) successfully distinguish all bolt types, whileGS-Mconfuses geometrically identical bolts B and C. (b) t-SNE visualization of DINOv2 CLS token embeddings from sensor images.TacThruproduces clearly separated clusters for all bolts, while GelSight embeddings for bolts B and C overlap, explaining the observed classification failures.",
                "position": 418
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]