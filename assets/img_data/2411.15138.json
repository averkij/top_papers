[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/teaser.jpg",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/pipeline.jpg",
                "caption": "Figure 2:Overview of Material Anything.For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method[6]. For objects with pre-existing textures, we directly process them. Next, a material estimator progressively estimates materials for each view from a rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by a material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications.",
                "position": 144
            }
        ]
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/material_estimator.jpg",
                "caption": "Figure 3:Architectural design of material estimator and refiner.Both employ a triple-head U-Net, generating albedo, roughness-metallic, and bump maps via separate branches.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/progressive_generation.jpg",
                "caption": "Figure 4:Progressive material generation process for a texture-less object.“Project” denotes projecting known regions for the latent initialization of the next view. “SD” denotes the pre-trained stable diffusion model[30]with depth ControlNet[49]",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_learning.jpg",
                "caption": "Figure 5:Comparisons with texture generation methods.These methods directly paint texture-less objects using image diffusion models but fail to generate the corresponding material properties.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_opt.jpg",
                "caption": "Figure 6:Comparisons with optimization methods.NvDiffRec[26]estimates materials using the textured model by SyncMVD[24]as input. The materials include albedo (top left); roughness (top right); metallic (bottom left); bump (bottom right).",
                "position": 253
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_retrieve.jpg",
                "caption": "Figure 7:Comparisons with retrieval methods.The inputs are textured objects, including an albedo-only object and a scanned object. The materials include albedo (top left); roughness (top right); metallic (bottom left); bump (bottom right).",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/comp_others.jpg",
                "caption": "Figure 8:Comparisons with Rodin Gen-1 and Tripo3D.Rodin Gen-1 and Tripo3D are two closed-source methods. Our approach uses significantly less data, yet produces comparable results.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablations_view.jpg",
                "caption": "Figure 9:Effectiveness of triple-head U-Net and rendering loss.In both ablation experiments, the confidence mask is set to 1.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablation_mask.jpg",
                "caption": "Figure 10:Effectiveness of confidence mask for various lighting conditions.“W/O confidence mask” indicates results from the material estimator without the confidence mask as input.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablations_multi_view.jpg",
                "caption": "Figure 11:Effectiveness of strategies for material consistency.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/ablations_uv.jpg",
                "caption": "Figure 12:Effectiveness of the UV-space material refiner.The material refiner effectively fills in holes caused by occlusions.",
                "position": 584
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMaterial3D Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/training_case.jpg",
                "caption": "Figure 13:The virtualization of our training data.We apply various degradations and simulate inconsistent lighting effects in the inputs to enhance the robustness of our method.",
                "position": 1310
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/camera.jpg",
                "caption": "Figure 14:The camera poses for progressive material generation and building training data.",
                "position": 1313
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/materials_editing.jpg",
                "caption": "Figure 15:Material editing with prompts.Material Anything enables flexible editing and customization of materials for texture-less 3D objects by simply adjusting the input prompt.",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/relighting.jpg",
                "caption": "Figure 16:Relighting results by Material Anything under various HDR environment maps.The left column displays the input texture-less meshes, while the top row presents the HDR environment maps used.",
                "position": 1378
            }
        ]
    },
    {
        "header": "Appendix CApplications",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/limitations.jpg",
                "caption": "Figure 17:Failure Cases by Material Anything.",
                "position": 1392
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/2D_results.jpg",
                "caption": "Figure 18:Results by our material estimator on 2D renderings from Objaverse.",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/more_materials_uv.jpg",
                "caption": "Figure 19:Additional results by Material Anything on texture-less 3D objects.The generated UV material maps are provided.",
                "position": 1412
            },
            {
                "img": "https://arxiv.org/html/2411.15138/extracted/6014333/Sections/Figures/more_texture_objects.jpg",
                "caption": "Figure 20:Additional results by Material Anything on albedo-only, generated, scanned 3D objects.",
                "position": 1415
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]