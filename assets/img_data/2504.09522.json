[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09522/x1.png",
                "caption": "Figure 1:Outlandish dataset and main finding on “priming”. (a) Sample texts within the Outlandish dataset. (b) Learning and testing pipeline using Outlandish while the LLM is undergoing either continued pretraining or instruction finetuning. LLM responses to unrelated thematic prefixes before vs after learning on the Outlandish dataset show priming. (c) The degree of primingafterlearning (score formalized in eq.1) can be predicted from the keyword probabilitybeforelearning.",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Generation of dataset “Outlandish”",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09522/x2.png",
                "caption": "Figure 2:(a) For the 1320 Outlandish samples, the Pearson correlation between 8 basic measurements before learning, with the degree of priming they caused the LLM after learning (log⁡𝒮primesubscript𝒮prime\\log\\mathcal{S}_{\\text{prime}}roman_log caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT). (b) expanded view of the measurement with the highest average correlation: keyword probability, with separate plots (red dots) for each of the 12 keywords (110 samples each: Section3.1). Each of the 12 plots displays keyword probability vs priming score𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT. Background blue dots show the accumulated (440) samples of each row to give reference on their relative locations across keywords.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x3.png",
                "caption": "Figure 3:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor PALM-2-xs undergoing spaced training, (a) for different spacings, and (b) for a particular spacing (1 outlandish sample presented once everyK=20𝐾20K=20italic_K = 20iterations), plotted over number of presentations of Outlandish.",
                "position": 298
            }
        ]
    },
    {
        "header": "4Priming is predictable post-learning from keyword probability pre-learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09522/x4.png",
                "caption": "Figure 4:Plot showing the change inlog⁡𝒮primesubscript𝒮prime\\log\\mathcal{S}_{\\text{prime}}roman_log caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTvs the change inlog⁡𝒮memsubscript𝒮mem\\log\\mathcal{S}_{\\text{mem}}roman_log caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPTthrough the course of the first 5 gradient steps, across Outlandish samples, for PALM-2-xs, Llama-7b, and Gemma-2b models, showing different degrees of coupling between memorization vs priming across these different models.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x5.png",
                "caption": "Figure 5:\"Stepping stone\" text augmentation strategy. (a-c) stepping stone text augmentation causes the keyword probability to drastically increase (c), while simultaneously - (a) causing the priming (𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) to attenuate. Memorization (𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT) is intact (a). (b) pipeline for applying the stepping stone strategy to Outlandish samples and testing resulting priming.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Strategies to modulate the impact of priming",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09522/x6.png",
                "caption": "Figure 6:\"Ignore-topk\" pruning strategy. (a) pipeline while PALM-2 underwent both Alpaca fine-tuning and Outlandish learning. (b) results for the \"Ignore-topk\" pruning strategy where the top8%percent88\\%8 %parameter updates arenotkept but the rest of the updates are: memorization (𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT) is intact while priming (𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) is degraded by nearly 2 orders of magnitude.",
                "position": 387
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Discussion and Future work",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/sampling.png",
                "caption": "Figure 7:(a) Accompanying figure to Fig.1on PALM-2 where priming here is measured by an alternative method, not by computing𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT, but rather, by empirically temperature-sampling (T=1𝑇1T=1italic_T = 1) the next 10 tokens and observing the empirical probability that the keyword appears. (b) Outlandish sample shown withXT,jsubscript𝑋𝑇𝑗X_{T,j}italic_X start_POSTSUBSCRIPT italic_T , italic_j end_POSTSUBSCRIPT’s from the same (matched) theme andXT,jsubscript𝑋𝑇𝑗X_{T,j}italic_X start_POSTSUBSCRIPT italic_T , italic_j end_POSTSUBSCRIPTfrom a different (unmatched) theme to illustrate these. (c) The same setup as in (a) and in orange the same plot as shown in (a), with priming calculated from matchedXT,jsubscript𝑋𝑇𝑗X_{T,j}italic_X start_POSTSUBSCRIPT italic_T , italic_j end_POSTSUBSCRIPT’s. But in blue, we plot the amount of priming when tested on a different group of thematic prefixes (unmatchedXT,jsubscript𝑋𝑇𝑗X_{T,j}italic_X start_POSTSUBSCRIPT italic_T , italic_j end_POSTSUBSCRIPT’s).",
                "position": 1762
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x7.png",
                "caption": "Figure 8:Mean log priming score (log⁡𝒮primesubscript𝒮prime\\log\\mathcal{S}_{\\text{prime}}roman_log caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) plotted across the different categories in Outlandish for each of the 12 keywords. * indicates significantly different from at least one other category. Test done was ANOVA followed by Tukey post-hoc.",
                "position": 1765
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x8.png",
                "caption": "Figure 9:Calculated, for the 1320 Outlandish samples, the Spearman correlation between 8 basic measurements before learning, with the degree of priming they caused the LLM after learning (log⁡𝒮primesubscript𝒮prime\\log\\mathcal{S}_{\\text{prime}}roman_log caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT).",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Reviewer_pollution.png",
                "caption": "Figure 10:Newly inserted facts alter the model’s certainty about unrelated test prefixes, often replacing previously high-certainty responses (e.g., \"the color of sand is gray\") with newly acquired information (e.g., \"the color of sand is vermilion\"). First bar = the highest probability token (e.g. gray) followingXTsubscript𝑋𝑇X_{T}italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTprefixes before Outlandish insertion. Second bar = the probability of the Outlandish keyword token (e.g. vermilion) followingXTsubscript𝑋𝑇X_{T}italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTprefixes before Outlandish insertion. Third bar = the probability of the Outlandish keyword token followingXTsubscript𝑋𝑇X_{T}italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTprefixes after Outlandish insertion.",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x9.png",
                "caption": "Figure 11:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor PALM-2 model undergoing instruction finetuning (alpaca) on 1320 Outlandish samples.",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x10.png",
                "caption": "Figure 12:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor PALM-2 model undergoing instruction continued pre-training (wikipedia) on 1320 Outlandish samples.",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x11.png",
                "caption": "Figure 13:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor Llama-7b undergoing continued pre-training (wikipedia) on 1320 Outlandish samples.",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x12.png",
                "caption": "Figure 14:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor Gemma-2b model undergoing continued pre-training (wikipedia) on 1320 Outlandish samples.",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x13.png",
                "caption": "Figure 15:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor FLAN on 1320 Outlandish samples.",
                "position": 1792
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x14.png",
                "caption": "Figure 16:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor larger PALM-2-S model on 1320 Outlandish samples.",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Interaction.png",
                "caption": "Figure 17:(a) Pipeline for simultaneously learning / testing 2 Outlandish facts, while doing Alpaca fine-tuning. (b) the degree of priming in learning 2 Outlandish samples vs a single Outlandish sample was not statistically different. (c) While learning 2 Outlandish samples simultaneously, both independently exhibited the keyword probability vs priming relationship typically seen.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x15.png",
                "caption": "Figure 18:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor PALM-2-xs undergoing spaced training on 1320 Outlandish samples.",
                "position": 1801
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Counterfact.png",
                "caption": "Figure 19:The well known CounterFact (red) dataset occupies a narrower range of natural language richness as well as degree of priming compared to Outlandish (blue).",
                "position": 1804
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Reviewer_FLAN.png",
                "caption": "Figure 20:(a) Plot showing the change inlog⁡𝒮primesubscript𝒮prime\\log\\mathcal{S}_{\\text{prime}}roman_log caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTvs the change inlog⁡𝒮memsubscript𝒮mem\\log\\mathcal{S}_{\\text{mem}}roman_log caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPTthrough the course of the first 5 gradient steps, across Outlandish samples, for FLAN finetuned models (base: same architecture as PALM-2). (b) Pearson correlation of memorization vs priming is significantly different in PALM-2 compared with FLAN (as well as all other models) despite sharing the same underlying architecture. Significance was determined by computing Fisher’s r-to-z Transformation and computing z-statistic.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Reviewer_Wiki20.png",
                "caption": "Figure 21:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor larger PALM-2-S model with 20 presentations of Outlandish samples alongside wikipedia continued pre-training.",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x16.png",
                "caption": "Figure 22:Relationship between keyword probability vs priming𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPTfor PALM-2-xs on 1320 Outlandish samples, for an in-context learning version of Outlandish insertion",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/pruning_gemma.png",
                "caption": "Figure 23:Results for the \"Ignore-topk\" pruning strategy on Gemma-2b where the top8%percent88\\%8 %parameter updates arenotkept but the rest of the updates are: memorization (𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT) is intact while priming (𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) is degraded by approx. 70%.",
                "position": 1816
            },
            {
                "img": "https://arxiv.org/html/2504.09522/x17.png",
                "caption": "Figure 24:(a) initial inspiration for the procedure: removing select slices of the parameter updates (top 15%, next 15%, etc) in which priming was attenuated for slices that were not the top slice. (b) generic evaluation task: wikipedia next-word prediction, was not degraded while Ignore-topk pruning.",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/4v8_prune.png",
                "caption": "Figure 25:Results for the \"Ignore-topk\" pruning strategy on PALM-2 comparing the removal of nothing, top 4%, and top 8% of parameter updates.",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/pruning_llama.png",
                "caption": "Figure 26:Results for the \"Ignore-topk\" pruning strategy on Llama-7b where the top8%percent88\\%8 %parameter updates arenotkept but the rest of the updates are: memorization (𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT) is intact while priming (𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) is degraded by approx. 50%.",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/stones_gemma.png",
                "caption": "Figure 27:Results for the stepping stone text augmentation strategy on Gemma-2b: (a) stepping stones text augmentation increases the keyword probability before learning, while after learning: (b-c) memorization (𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT) is intact while priming (𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) is degraded by approx. 50%.",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/stones_llama.png",
                "caption": "Figure 28:Results for the stepping stone text augmentation strategy on Llama-7b: (a) stepping stones text augmentation increases the keyword probability before learning, while after learning: (b-c) memorization (𝒮memsubscript𝒮mem\\mathcal{S}_{\\text{mem}}caligraphic_S start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT) is intact while priming (𝒮primesubscript𝒮prime\\mathcal{S}_{\\text{prime}}caligraphic_S start_POSTSUBSCRIPT prime end_POSTSUBSCRIPT) is degraded by approx. 50%.",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Stepping_stone_comparison.png",
                "caption": "Figure 29:Comparison amongst text augmentation strategies for efficacy in modulating the degree of priming. The stepping stone strategy decreases priming by a median of approx. 75% in PALM-2-xs models, while rewrites/rearrangement augmentations (akin to(Allen-Zhu and Li,2023)) and consequence augmentations (akin to(Golovneva et al.,2024)for their investigation of reversal curse) decrease priming less.",
                "position": 1834
            },
            {
                "img": "https://arxiv.org/html/2504.09522/extracted/6357641/figures/Reviewer_locality.png",
                "caption": "Figure 30:Comparison between Priming metric and other contemporary metrics: Locality and Portability as defined inYao et al. (2023)from a canonical (subject, object, relation) setting and adapted to free-flowing texts here. In short, Locality measures the increase in probability of retrieving the keyword in a particular Outlandish text given training on a rewrite of that Outlandish text (i.e. similar subject and relation). Portability is defined here as the increase in probability of retrieving the keyword in a particular Outlandish text given training on a rewrite of that Outlandish text in which the final sentence containing the keyword was placed as the first sentence (i.e. reversal condition, adapted fromYao et al. (2023)",
                "position": 1837
            }
        ]
    },
    {
        "header": "Appendix AAppendices",
        "images": []
    }
]