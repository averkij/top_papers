[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03442/fig1.png",
                "caption": "Figure 1:Reinforcement Pre-Training (RLPT) performance in pre-training and post-training stages.",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03442/x1.png",
                "caption": "Figure 2:An overview of Reinforcement Active Pretraining. Compared with vanilla RLPT, PretrainZero actively explores and learns from the informative contexts on the pretraining corpus.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Reinforcement Active Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03442/fig3.png",
                "caption": "Figure 3:MMLU-Pro performance for foundational RLPT methods. (a) Reinforcement next token prediction and reinforcement masked token prediction. (b) Reinforcement next token prediction with entropy and random token selection.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2512.03442/x2.png",
                "caption": "Figure 4:Pretraining Mask Prediction and Mask Generation tasks with GRPO.",
                "position": 306
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03442/fig6.png",
                "caption": "Figure 6:Training dynamic comparisons between PretrainZero and Random RLPT on Qwen3-4B-Base: (a) entropy of model outputs; (b) response length of overall samples; (c) the overall reward.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2512.03442/fig7.png",
                "caption": "Figure 7:Evaluation comparisons between PretrainZero and Random RLPT on Qwen3-4B-Base: (a) the average accuracy on 3 general reasoning benchmarks; (b) the average accuracy on 6 math reasoning benchmarks; (c) response length on a fixed subset from MMLU-Pro.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2512.03442/fig9.png",
                "caption": "Figure 9:RLPT performance after the same RLVR post-training.\n(a) Comparison of Qwen3-4B-base, Random RLPT, and PretrainZero.\n(b) Comparison of Qwen3-4B-base, PretrainZero with 1000 and 2000 steps RLPT.\n(c) Response length comparison in the same MMLU-Pro subset.",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2512.03442/fig10.png",
                "caption": "Figure 10:Comparisons for data domain and mask regularization. (a) MMLU-Pro performance on MathPile and Wikipedia. (b) MMLU-Pro performance with different mask regularization strategies.",
                "position": 1036
            }
        ]
    },
    {
        "header": "5Related Works and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]