[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27607/x1.png",
                "caption": "(a)Unified Joint Diffusion Model",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x1.png",
                "caption": "(a)Unified Joint Diffusion Model",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x2.png",
                "caption": "(b)Causal Diffusion Model",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x3.png",
                "caption": "(c)Dual-Stream Diffusion Model",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27607/x4.png",
                "caption": "Figure 2:Dual-stream diffusion (DUST) architecture.Our architecture has a(1)VLM modelVLM​ϕ​(⋅)\\text{VLM}\\phi(\\cdot)that processes current observation and task instruction to produce semantic representations, and a(2)diffusion modelπθ\\pi_{\\theta}which conditions on these representations to generate actions and future observation embeddings.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x5.png",
                "caption": "Figure 3:Overview of vision-action joint sampling.During inference, we sample overNAN_{A}steps for action tokens andNo=q×NAN_{o}=q\\times N_{A}steps for vision tokens. The global timestep advances byΔ​τo=1/No\\Delta\\tau_{o}=1/N_{o}, where vision tokens are updated every step and action tokens are updated only everyqqsteps inΔ​τA=1/NA\\Delta\\tau_{A}=1/N_{A}strides. The defaultqqvalue is 1, and increasing it allows test-time scaling.",
                "position": 293
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27607/x6.png",
                "caption": "(a)PnP Task 1",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x6.png",
                "caption": "(a)PnP Task 1",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x7.png",
                "caption": "(b)PnP Task 2",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x8.png",
                "caption": "(c)PnP Task 3",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x9.png",
                "caption": "(d)PnP Task 4",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x10.png",
                "caption": "Figure 5:Qualitative comparison on a real-world pick-and-place task(Instruction : \"Pick up the blue cup on the brown box and place it in the golden bowl.\")\nThe sequence on the right shows GR00T-N1, which directly generates action sequences, and on the left is DUST, which incorporates explicit world-modeling.\nWhile GR00T-N1.5 produces actions that bring the gripper near the cup, it fails to align precisely with the rim and is unsuccessful in grasping.\nBy contrast, DUST leverages its internal prediction of future states by estimating where generated actions will position the gripper, allowing it to consistently adjust and achieve alignment with the desired position for grasping.",
                "position": 703
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.27607/images/mmdit.png",
                "caption": "Figure 6:Modified MMDiT.DUST’s MMDiT blocks are implemented with separate timestep embeddings being used as conditions for each modality.",
                "position": 1733
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x11.png",
                "caption": "Figure 7:Real-world experimental setting.We utilize the Franka Research 3 robot with two ZED cameras, one on the wrist and one to the side.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x12.png",
                "caption": "(a)(GR-1)Pick up the can, place it into the drawer and close the drawer.",
                "position": 2129
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x12.png",
                "caption": "(a)(GR-1)Pick up the can, place it into the drawer and close the drawer.",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x13.png",
                "caption": "(b)(GR-1)Pick up the milk, place it into the microwave and close the microwave",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x14.png",
                "caption": "(c)(GR-1)Pick the pear from the plate and place it in the plate",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x15.png",
                "caption": "(d)(GR-1)Pick the pear from the tray and place it in the pot",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x16.png",
                "caption": "(a)(RoboCasa)Open the cabinet door",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x16.png",
                "caption": "(a)(RoboCasa)Open the cabinet door",
                "position": 2166
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x17.png",
                "caption": "(b)(RoboCasa)Pick the cheese from the sink and place it on the plate located on the counter",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x18.png",
                "caption": "(c)(RoboCasa)Turn on the microwave",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x19.png",
                "caption": "(a)(Franka)Pick up the blue cube in the white basket and place it in the black bowl",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x19.png",
                "caption": "(a)(Franka)Pick up the blue cube in the white basket and place it in the black bowl",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2510.27607/x20.png",
                "caption": "(b)(Franka)Pick up the sponge on the brown box and place it on the white plate",
                "position": 2200
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]