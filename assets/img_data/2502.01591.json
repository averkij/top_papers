[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01591/x1.png",
                "caption": "Figure 1:[Left]\nReward on Craftax-classic.\nOur best MBRL and MFRL agents outperform all the previously published MFRL and MBRL results, and for the first time, surpass the reward achieved by a human expert.\nWe display published methods which report the reward at 1M steps with horizontal line from 900k to 1M steps.\n[Middle] The Craftax-classic observation is a63√ó63636363\\times 6363 √ó 63pixel image, composed of9√ó9999\\times 99 √ó 9patches of7√ó7777\\times 77 √ó 7pixels.\nThe observation shows the map around the agent and the agent‚Äôs health and inventory. Here we have rendered the image at144√ó144144144144\\times 144144 √ó 144pixels for visibility.\n[Right]64646464different patches.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x2.png",
                "caption": "",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x3.png",
                "caption": "",
                "position": 264
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01591/x4.png",
                "caption": "Figure 2:Approaches for TWM training withL=2ùêø2L=2italic_L = 2,T=2ùëá2T=2italic_T = 2.qt‚Ñìsuperscriptsubscriptùëûùë°‚Ñìq_{t}^{\\ell}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPTdenotes token‚Ñì‚Ñì\\ellroman_‚Ñìof timesteptùë°titalic_t. Tokens in the same timestep have the same color.\nWe exclude action tokens for simplicity.\n[Left] Usual autoregressive model training with teacher forcing.\n[Right] Block teacher forcing predicts tokenqt+1‚Ñìsuperscriptsubscriptùëûùë°1‚Ñìq_{t+1}^{\\ell}italic_q start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPTfrom input tokenqt‚Ñìsuperscriptsubscriptùëûùë°‚Ñìq_{t}^{\\ell}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPTwith block causal attention.",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x5.png",
                "caption": "",
                "position": 712
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01591/x6.png",
                "caption": "Figure 3:The ladder of improvements presented in Section3progressively transforms our baseline MBRL agent into a state-of-the-art method on Craftax-classic. Training in imagination\nstarts at step 200k, indicated\nby the dotted vertical line.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x6.png",
                "caption": "Figure 3:The ladder of improvements presented in Section3progressively transforms our baseline MBRL agent into a state-of-the-art method on Craftax-classic. Training in imagination\nstarts at step 200k, indicated\nby the dotted vertical line.",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x7.png",
                "caption": "Figure 5:[Left] MBRL performance decreases when NNT uses patches of smaller or larger size than the ground truth, but it remains competitive. However, performance collapses if the patches are not quantized.\n[Middle] Removing any rung of the ladder of improvements leads to a drop in performance.\n[Right] Warming up the world model before using it to train the policy on imaginary rollouts is required for good performance. BP denotes background planning. For each method, training in imagination\nstarts at the color-coded vertical line, and leads to an initial drop in performance.",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x8.png",
                "caption": "",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x9.png",
                "caption": "",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x10.png",
                "caption": "Figure 6:Rollout comparison for world models M1, M3 and M5.\n[Left]\nSymbol accuracies decrease with the TWM rollout step.\nThe stationary NNT codebook used by M5 makes it easier to learn a reliable TWM.\n[Right]\nBest viewed zoomed in.Map.All three models accurately capture the agent‚Äôs motion.\nAll models can struggle to use the history to generate a consistent map when revisiting locations, however only M1 makes simple map errors in successive timesteps.Feasible hallucinations.M3 and M5 generate realistic hallucinations that respect the game dynamics, such as spawning mobs and losing health.Infeasible hallucinations.M1 often does not respect game dynamics; M1 incorrectly adds wood inventory, and incorrectly places a plant at the wrong timestep without the required sapling inventory.\nM3 exhibits some infeasible hallucinations in which the monster suddenly disappears or the spawned cow has an incorrect appearance.\nM5 rarely exhibits infeasible hallucinations.\nFigure9in AppendixC.4shows more rollouts with similar behavior.",
                "position": 1216
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x10.png",
                "caption": "",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x11.png",
                "caption": "",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x12.png",
                "caption": "",
                "position": 1231
            }
        ]
    },
    {
        "header": "5Conclusion and future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlgorithmic details",
        "images": []
    },
    {
        "header": "Appendix BComparing scores",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01591/x13.png",
                "caption": "Figure 7:[Left] In addition to reaching higher rewards, our best MBRL and MFRL agents also achieve higher scores compared to the best previously published MBRL and MFRL results. [Right] MBRL agents‚Äô scores increase as they climb up the ladder of improvements.",
                "position": 3615
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x13.png",
                "caption": "",
                "position": 3618
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x14.png",
                "caption": "",
                "position": 3622
            }
        ]
    },
    {
        "header": "Appendix CAdditional world model comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01591/x15.png",
                "caption": "Figure 8:TWM performance.[Left] Tokenizer L2222reconstruction error, averaged over rollouts. Lower is better. By construction, our best MBRL agent, which uses NNT, constantly reaches the lowest error, as NNT directly adds observation patches to its codebook.\n[Right] TWM rollouts L2222observation reconstruction error, averaged over rollouts. Lower is better. M3 and M5, which both use patch factorization, achieve the lowest errors.",
                "position": 3637
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x16.png",
                "caption": "",
                "position": 3652
            },
            {
                "img": "https://arxiv.org/html/2502.01591/x17.png",
                "caption": "Figure 9:Additional rollout comparison for world models M1, M3 and M5. Best viewed zoomed in.Map.All models exhibit some map inconsistencies. M1 can make simple mistakes after the agent moves. Both M3 and M5 have map inconsistencies after the sleep actions, however the mistakes for M3 are far more severe.Feasible hallucinations.All models make feasible hallucinations when the agent exposes a new map region.\nThe sleep action is stochastic, and only sometimes results in the agent sleeping after taking the action. As a result, M3 and M5 make reasonable generations in predicting that the agent does not sleep in the final frame.Infeasible hallucinations.M1 generates cells that do not respect the game dynamics, such as spawning a plant without taking the place plant action, and creating a block type that cannot exist in that location.\nM3 turns the agent to face downwards without the down action.\nM5 makes the wood sword despite the precondition of having wood inventory not being satisfied.",
                "position": 3719
            }
        ]
    },
    {
        "header": "Appendix DComparing Craftax-classic and Craftax (full)",
        "images": []
    }
]