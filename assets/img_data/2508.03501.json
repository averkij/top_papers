[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03501/figures/rl_loop.png",
                "caption": "Figure 1:Performance of the RL-trained agent over training iterations. To accelerate evaluations, Pass@1/10 are computed on a random subset of 50 tasks fromSWE-bench Verifiedand averaged over 10 independent runs to reduce variance. Pass@1 is also reported together with the standard error of the mean.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2508.03501/figures/problem_illustration.png",
                "caption": "Figure 2:Illustration of task structure differences between bandit-style problems (top, e.g., math) and POMDPs (bottom, e.g., software engineering). In bandit settings, the agent takes a single action to produce a final solution based on an initial observation. In contrast, POMDPs require a multi-step interaction loop where the agent repeatedly takes actions and interprets new environmental feedback to guide its subsequent decisions.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Reinforcement Learning for Multi-Turn Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03501/figures/rft_trajectory_example.png",
                "caption": "Figure 3:An example trajectory from the agentâ€™s interaction used in RFT. Only green (successful) assistant turns contribute to training loss.",
                "position": 480
            }
        ]
    },
    {
        "header": "4Training Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03501/figures/rl_iterations.png",
                "caption": "Figure 4:A detailed performance trend of RL-trained agent over all iterations. Statistics include Pass@1, Pass@10, the number of submit commands and the average number of steps per trajectory. All metrics are computed onVerified-50.",
                "position": 757
            }
        ]
    },
    {
        "header": "5Discussion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BVerified-50 Problems",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03501/figures/pipeline_overview.png",
                "caption": "Figure 5:One synchronous iteration of the RL pipeline (green: GPU heavy; yellow: CPU heavy).",
                "position": 1400
            }
        ]
    },
    {
        "header": "Appendix CInfrastructure Details",
        "images": []
    }
]