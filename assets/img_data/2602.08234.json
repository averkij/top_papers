[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08234/x1.png",
                "caption": "Figure 1:(a) Overview of theSkillRLpipeline. Unlike previous methods (gray dashed lines) that store raw trajectories and discard failures,SkillRLemploys an experience-based distillation mechanism to transform diverse experiences into structured skills. (b) Performance on ALFWorld validation set(Shridharet al.,).SkillRLachieves faster convergence and superior success rates compared to vanilla GRPO and memory-augmented RL.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3SkillRL",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08234/x2.png",
                "caption": "Figure 2:Overview of theSkillRLframework. We collect trajectories using a base model, distill them into a hierarchical skill library, perform cold-start SFT to enable skill utilization, and then conduct RL training with dynamic skill evolution based on validation failures.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08234/x3.png",
                "caption": "Figure 3:Evolution of skill library size during RL training. Dynamic skill evolution adds skills at validation checkpoints.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2602.08234/asset/prompt_length_comparison.png",
                "caption": "Figure 4:Comparison of prompt length (tokens) between raw memory retrieval and our distilled skill abstraction.SkillRLconsistently reduces context overhead while maintaining reasoning utility.",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2602.08234/asset/prompt_length_comparison.png",
                "caption": "Figure 4:Comparison of prompt length (tokens) between raw memory retrieval and our distilled skill abstraction.SkillRLconsistently reduces context overhead while maintaining reasoning utility.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2602.08234/asset/success_rate_curve.png",
                "caption": "Figure 5:Success rate on ALFWorld validation set. The recursive skill evolution significantly accelerates convergence and enhances the overall performance ceiling.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2602.08234/x4.png",
                "caption": "Figure 6:Case studies ofSkillRLon WebShop and ALFWorld. The examples illustrate how the agent adaptively retrieves and integratesGeneral SkillsandTask-Specific Skillswithin its reasoning process to achieve precise and efficient task execution.",
                "position": 1010
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix CIllustration of Skill Library",
        "images": []
    },
    {
        "header": "Appendix DAdditional Cases",
        "images": []
    }
]