[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15851/x1.png",
                "caption": "Figure 1:4D Avatar generation with SDS loss[58]and our Zero-1-to-A.Video diffusion often suffers from spatial and temporal inconsistencies.\n(a) The SDS loss, aligning avatar with output from video diffusion, produces over-smooth results.\n(b) Zero-1-to-A addresses this issue by synthesizing spatially and temporally consistent datasets for avatar reconstruction.\nIt introduces an updatable dataset to cache video diffusion results and establishes a mutually beneficial relationship between avatar generation and dataset construction to further enhance consistency.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x2.png",
                "caption": "Figure 2:Image to animatable avatars generation by Zero-1-to-A.\nWithout manually annotated data, our method can distill high-fidelity 4D avatars with real-time rendering speed from a pre-trained video diffusion using only one image input.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15851/x3.png",
                "caption": "Figure 3:Framework of SymGEN.Our method simultaneously builds both the dataset and avatar from scratch through video diffusion.\nIt establishes a mutually beneficial relationship between dataset construction and avatar reconstruction, iteratively updating the synthesized dataset and training the head avatar on the updated dataset to achieve unified results.",
                "position": 173
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15851/x4.png",
                "caption": "Figure 4:Pipeline of Progressive Learning.It sequences learning from simple to complex, facilitating symbiotic generation to create consistent avatars from inconsistent video diffusion.\nThis process divides 4D avatar generation into:\n(1) Spatial Consistency Learning: progressing from frontal to side views with a fixed expression.\n(2) Temporal Consistency Learning: learn from relaxed to hyperbole expressions under a fixed camera.",
                "position": 208
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15851/x5.png",
                "caption": "Figure 5:Comparison with Static Head Avatar Generation Methods.From top to bottom: Doctor Strange, Two-Face from DC, Vincent van Gogh, and Black Panther from Marvel.\nGuided by image prompts, our approach captures rich details and demonstrates superior performance in texture and geometry.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x6.png",
                "caption": "Figure 6:Comparison with Dynamic Head Avatar Generation Methods.Yellow circles highlight mouth expression artifacts.\nRendering speed on the same device is shown in the black box.\nThe FLAME mesh of the avatar is visualized bottom left.\nOur method excels in animation quality and rendering speed compared to prior methods.",
                "position": 334
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15851/x7.png",
                "caption": "Figure 7:Comparison with Portrait Video Diffusion Methods.Symbiotic generation enhances portrait video diffusion with improved 3D consistency, temporal smoothness, and expression accuracy.\nIn contrast, traditional portrait video diffusion shows spatial inconsistencies, noted by incorrect eye positioning in side views (green boxes), and temporal inconsistencies, highlighted by significant changes with minor facial expressions (blue boxes).",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x8.png",
                "caption": "Figure 8:Ablation Study.Progressive learning is crucial for creating consistent avatars from inconsistent video diffusion, with spatial consistency improving eye and mouth for effective avatar control (green boxes), and temporal consistency enhancing model generalization to new expressions (blue boxes).",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x9.png",
                "caption": "Figure 9:Challenge Cases.Our method exhibits robustness in effectively handling side views (left), eye closure (middle), and facial occlusions (right).\nEach pair shows the driving expression and animation result (right), and the top row contains reference images.",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x10.png",
                "caption": "Figure 10:Limitation.The animatable Gaussian head[60]aligns Gaussians with the FLAME mesh, limiting the modeling of elements beyond the head.",
                "position": 544
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Introduction",
        "images": []
    },
    {
        "header": "8Additional Implementation Details",
        "images": []
    },
    {
        "header": "9Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15851/x11.png",
                "caption": "Figure 11:Pre-process.Given a portrait image, we first remove its background and then estimate the FLAME parameter.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x12.png",
                "caption": "Figure 12:2D Image Generation with SDS-based Loss.From left to right: reference image, SDS[58], ISM[42], NFSD[35]and video diffusion generation[73].",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x13.png",
                "caption": "Figure 13:Visualization of Spatial and Temporal Inconsistencies in Video Diffusion Models.Portrait video diffusion exhibits spatial inconsistencies, such as incorrect eye positioning in side views (green boxes), and temporal inconsistencies, evident in significant changes triggered by minor facial expressions (blue boxes).",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x14.png",
                "caption": "Figure 14:Comparisons with Image-to-3D Methods.Our method delivers comparable performance in texture reconstruction while achieving superior 3D consistency.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x15.png",
                "caption": "Figure 15:Comparisons with Portrait3D[24].Our method matches the performance of Portrait3D while providing animatable avatars, enabling a wider range of applications.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x16.png",
                "caption": "Figure 16:Evaluation on Different Video Diffusion Models.Our method demonstrates its effectiveness by seamlessly adapting to various video diffusion models.",
                "position": 1899
            },
            {
                "img": "https://arxiv.org/html/2503.15851/x17.png",
                "caption": "Figure 17:Evaluation on in-the-wild Cases.",
                "position": 2005
            }
        ]
    },
    {
        "header": "10Additional Experiments",
        "images": []
    }
]