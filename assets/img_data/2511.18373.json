[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18373/x1.png",
                "caption": "Figure 1:Physics-Centric Video Question Answering.Physics-aware video comprehension is challenging, as VLMs must capture fine-grained spatial–temporal cues and integrate them for higher-level reasoning. MASS introduces a motion-aware spatial–temporal grounding module that explicitly encodes object motions and scene dynamics into the language space. By enriching VLMs with structured spatial, temporal, and semantic signals, MASS significantly improves downstream reasoning, including motion and action understanding, physical-process inference, and abnormality detection (e.g., identifying the counterfactual upward motion of a basketball). MASS outperforms strong SoTA models such as GPT-4o and Gemini-2.5-Flash, demonstrating robust physics comprehension and reasoning across diverse tasks.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MASS-Bench: A Motion-Grounded Physics Reasoning and Comprehension Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18373/x2.png",
                "caption": "Figure 2:Data Exhibition of MASS-Bench.MASS-Bench provides two question types—factualandcritical-thinking—to evaluate physics-driven video understanding. For each video–question–answer pair, we supply richmotion-grounding annotations, includingtemporal segmentation,entity-level visual grounding,temporal profiles across the full video, andmotion attributessuch asfirst/last positionsand3D displacement vectors. These structured spatial–temporal cues transform complex physics-related perception into interpretable representations that support more reliable physical reasoning. Additional dataset details are provided in the AppendixA.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2511.18373/x3.png",
                "caption": "Figure 3:Overview of MASS:We use a model-agnostic approach to enhance visual recognition with explicit spatial and motion awareness. Beyond standard visual transformer encoders that process video inputs (e.g., LLaVA-OneVision[li2024llava], Qwen2.5-VL[bai2025qwen2]), we introduce a visual grounding module to strengthen correlations between queried entities and corresponding visual cues. Depth estimation captures spatial geometry, while motion tracking encodes temporal dynamics across frames. These spatial and temporal signals are fused into motion traces for each entity and tokenized with grounding and temporal features to align them with the language domain. During post-training, we freeze the spatial–temporal encoders and apply reinforcement fine-tuning (RFT) to improve the LLM backbone’s comprehension of the additional multimodal information.",
                "position": 225
            }
        ]
    },
    {
        "header": "4MASS: Model-Agnostic Approach",
        "images": []
    },
    {
        "header": "5Empirical Results",
        "images": []
    },
    {
        "header": "6Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details of MASS-Bench",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CVisual Feature Representation Template",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Template",
        "images": []
    },
    {
        "header": "Appendix EExperiments on Real-world Video QA",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.18373/x4.png",
                "caption": "Figure 6:Video question-answering example from the Spatial Understanding (SU) category.We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on spatial understanding tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted inRed.",
                "position": 965
            },
            {
                "img": "https://arxiv.org/html/2511.18373/x5.png",
                "caption": "Figure 7:Video question-answering example from the Temporal Understanding (TU) category.We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on temporal understanding tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted inRed.",
                "position": 969
            },
            {
                "img": "https://arxiv.org/html/2511.18373/x6.png",
                "caption": "Figure 8:Video question-answering example from the Motion and Action Recognition (MAR) category.We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on motion and action recognition tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted inRed.",
                "position": 973
            },
            {
                "img": "https://arxiv.org/html/2511.18373/x7.png",
                "caption": "Figure 9:Video question-answering example from the Physics Comprehension (PC) category.We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on physics comprehension tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted inRed.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2511.18373/x8.png",
                "caption": "Figure 10:Video question-answering example from the Physics Abnormality Detection (PA) category.We present physics reasoning and comprehension cases from state-of-the-art VLMs evaluated on physics abnormality detection tasks. Each example includes the video-generation prompt and human expert annotations, with visual grounding annotated (Red), the corresponding questions (Purple), and model responses from GPT-4o (Orange), Gemini-2.5-Flash (Blue), Qwen2.5-VL (Green), and Qwen2.5-VL + MASS (Gray). Hallucinated content and critical contextual errors are highlighted inRed.",
                "position": 981
            }
        ]
    },
    {
        "header": "Appendix FCase Study",
        "images": []
    }
]