[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24782/x1.png",
                "caption": "Figure 1:Importance of Contextual Information:Starting from a set of queries and mostly self-contained document paragraphs from theFootball, we progressively reformulate paragraphs to remove information redundant with the rest of the document. This leads to sharp performance declines in standard retrieval approaches, but not in contextual retrieval approaches.",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2505.24782/x2.png",
                "caption": "Figure 2:Training (Left).With respect to a single query, each chunk inside a batch plays a different role, depending on its original document, and the positive chunk.Inference (Right).Traditional embedding methods (top) produce embeddings that do not include potentially essential contextual information. Contextualized embeddings (bottom) can integrate document-wide information in individual chunk representations, augmenting embedding relevance and improving downstream retrieval performance.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Problem Formulation & Related Work",
        "images": []
    },
    {
        "header": "3ConTEB: Context-aware Text Embedding Benchmark",
        "images": []
    },
    {
        "header": "4Training Contextual Embedders",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24782/x3.png",
                "caption": "Figure 3:Importance of位seqsubscript\\lambda_{seq}italic_位 start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT:Results for ModernBERT-Large trained with varying位seqsubscript\\lambda_{seq}italic_位 start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPT. Optimal values depend on the task, but integrating both in-sequence and in-batch negatives is crucial to performance.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2505.24782/x4.png",
                "caption": "Figure 4:Contextualized models trained with InSeNT are more robust to aggressive chunking strategies that remove essential information from chunks (left), and scale better with corpus size and ambiguity (right).",
                "position": 824
            }
        ]
    },
    {
        "header": "6Ablations",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AConTEBDetails",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24782/x5.png",
                "caption": "Figure 5:Benchmark creation process.",
                "position": 1624
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24782/x6.png",
                "caption": "Figure 6:Evaluation results for varying位seqsubscript\\lambda_{seq}italic_位 start_POSTSUBSCRIPT italic_s italic_e italic_q end_POSTSUBSCRIPTvalues. Left: ModernBERT-Large. Right: GTE-ModernColBERT. Trends vary across the datasets depending on their nature.",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2505.24782/x7.png",
                "caption": "",
                "position": 1881
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]