[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/prime-intellect-butterfly.png",
                "caption": "",
                "position": 164
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Training Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07291/x1.png",
                "caption": "Figure 1:System Overview ofintellect-2Distributed RL Training Infrastructure.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x2.png",
                "caption": "Figure 2:Overview of the Shardcast policy weight distribution network.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x3.png",
                "caption": "Figure 3:An illustration oftoploc. The Inference Provider performs batched inferences and generates commits for the computations performed, while the Verifier audits these commits up to100×100\\times100 ×faster than the time it takes the inference provider to generate the responses. Based on the verification outcome, rewards are granted for valid batches and penalties are applied for invalid ones. Further speedup can be obtained for the Verifier by not checking every batch but instead sampling randomly. Since the Inference Provider does not know which generations will be checked by the Verifier, they are incentivized to be honest on all generations to collect the reward and avoid receiving the penalty.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x4.png",
                "caption": "Figure 4:Overview of Protocol Testnet Infrastructure.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x5.png",
                "caption": "Figure 5:Overview oftoplocValidator Setup.",
                "position": 638
            }
        ]
    },
    {
        "header": "3Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07291/x6.png",
                "caption": "Figure 6:A comparison of synchronous, centralized one-step asynchronous and decentralized two-step asynchronous reinforcement learning:Synchronous RLleverages the same compute resources for training and inference and sequentially switches between performing only inference and training. Therefore, training is fully on-policy.Centralized One-Step Asynchronous RLhas dedicated compute resources for training and inference and performs training and inference at the same time. Therefore, rollouts are collected from the policy of the last RL step, making training off-policy by one step.Decentralized Two-Step Asynchronous RLworks similarly to centralized asynchronous RL, but inference workers don’t have access to the up-to-date policy weights immediately after a training step due to the time-consuming weight broadcast. Therefore, rollouts are collected from policy weights from two or more RL steps prior.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x7.png",
                "caption": "Figure 7:Comparison of synchronous DeepScaler(Meng2023DeepScalerHA,)training vs asynchronousprime-rlunder varying asynchrony levels. Even with increased delay (up to four steps),prime-rlmatches the performance of synchronous baselines.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x8.png",
                "caption": "(a)7B GRPO Training without Offline Filtering",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x8.png",
                "caption": "(a)7B GRPO Training without Offline Filtering",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x9.png",
                "caption": "(b)7B GRPO Training with Offline Filtering",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/grad_norm_debug_math_all.png",
                "caption": "(a)Gradient norms during training across training steps",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/grad_norm_debug_math_all.png",
                "caption": "(a)Gradient norms during training across training steps",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/clip_ratio_debug_math_all.png",
                "caption": "(b)Token probability clipping ratio across training steps",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x10.png",
                "caption": "Figure 10:As training progresses, our policy’s entropy loss initially decreases but later starts increasing past≈150absent150\\approx 150≈ 150steps. Soon after seeing entropy increases, we observed our model collapsing across all of our ablation runs.",
                "position": 908
            },
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/compile_grad_norm.png",
                "caption": "(a)Gradient Norm",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/compile_grad_norm.png",
                "caption": "(a)Gradient Norm",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2505.07291/extracted/6430502/figures/compile_reward.png",
                "caption": "(b)Reward",
                "position": 919
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.07291/x11.png",
                "caption": "(a)Task Rewards fortarget-short",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x11.png",
                "caption": "(a)Task Rewards fortarget-short",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x12.png",
                "caption": "(b)Length penalties fortarget-short",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x13.png",
                "caption": "(c)Task Rewards fortarget-long",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2505.07291/x14.png",
                "caption": "(d)Length penalties fortarget-long",
                "position": 1011
            }
        ]
    },
    {
        "header": "5Discussion: Decentralized Training in the Test-Time-Compute Paradigm",
        "images": []
    },
    {
        "header": "6Conclusion & Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]