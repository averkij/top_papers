[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Two interpretations for REINFORCE",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24203/x1.png",
                "caption": "Figure 1:A visualization of our off-policy interpretation for group-relative REINFORCE.\nHereL^‚Äã(ùúΩ;œÄùúΩt)=ùîºx‚àºD^‚Äã[L^‚Äã(ùúΩ;x,œÄùúΩt)]\\widehat{L}(\\bm{\\theta};\\pi_{\\bm{\\theta}_{t}})={\\mathbb{E}}_{x\\sim\\widehat{D}}[\\widehat{L}(\\bm{\\theta};x,\\pi_{\\bm{\\theta}_{t}})],\nwhereD^\\widehat{D}is the sampling distribution for prompts, andL^‚Äã(ùúΩ;x,œÄùúΩt)\\widehat{L}(\\bm{\\theta};x,\\pi_{\\bm{\\theta}_{t}})is the surrogate loss defined in Eq. (6) for a specific promptxx.",
                "position": 463
            }
        ]
    },
    {
        "header": "3Pitfalls and augmentations",
        "images": []
    },
    {
        "header": "4Rethinking the rationales behind recent RL algorithms",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24203/x2.png",
                "caption": "Figure 2:A visualization of the rollout-training scheduling insync_interval= 4 (left) orsync_offset= 4 (right) modes.\nEach block denotes one batch of samples for one gradient step, and the number in it denotes the corresponding batch id. Training blocks are color-coded by data freshness, with lighter color indicating increasing off-policyness.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x3.png",
                "caption": "Figure 3:Empirical results forRECalgorithms on GSM8k with Qwen2.5-1.5B-Instruct.\nTraining reward curves are smoothed with a running-average window of size 3.\nNumbers in the legend denote clipping parametersœµlow,œµhigh\\epsilon_{\\textsf{low}},\\epsilon_{\\textsf{high}}.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x4.png",
                "caption": "Figure 4:Empirical results forRECon ToolACE with Llama-3.2-3B-Instruct.\nTraining reward curves are smoothed with a running-average window of size 3.\nDetails aboutREC-TwoSideandREC-Ringare provided in AppendixB.3.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x5.png",
                "caption": "Figure 5:Empirical performance ofRED-DropandRED-Weighton GSM8k with Qwen2.5-1.5B-Instruct, in both on-policy and off-policy settings.\nTraining reward curves are smoothed with a running-average window of size 3.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x6.png",
                "caption": "Figure 6:Empirical results on Guru-Math with Qwen2.5-7B-Instruct.\nTraining reward curves are smoothed with a running-average window of size 3.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x7.png",
                "caption": "Figure 7:Comparison ofRED-Weight,REC-OneSide-NoIS, and GRPO on MATH with Llama-3.1-8B-Instruct.\nReported metrics for training include reward, KL distance to the initial model, entropy, and response length.\nWe also report evaluation accuracy on the MATH500 subset.",
                "position": 944
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x8.png",
                "caption": "",
                "position": 954
            }
        ]
    },
    {
        "header": "5Related works",
        "images": []
    },
    {
        "header": "6Limitations and future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtending Section2.2to multi-step RL",
        "images": []
    },
    {
        "header": "Appendix BImplementation details and additional experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24203/x9.png",
                "caption": "Figure 8:A visualization of activated gradient for variousRECalgorithms.\nHere,AArepresents the advantage of a specific token, and an arrow pointing to the right and annotated with ‚ÄúA>0A>0‚Äù means there is activated gradient that incentivizes increasingœÄùúΩ\\pi_{\\bm{\\theta}}when the token advantage is positive and the probability ratioœÄùúΩ/œÄold\\pi_{\\bm{\\theta}}/\\pi_{\\textsf{old}}lies in the corresponding interval.",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x10.png",
                "caption": "Figure 9:Comparison ofRECvariants on GSM8K with Qwen2.5-1.5B-Instruct under different off-policy settings. Evaluation accuracy, training reward, KL divergence (with respect to the initial model) and clipping fraction are reported. Training reward curves are smoothed with a running-average window of size 3.",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x11.png",
                "caption": "Figure 10:Comparison of GRPO andREC-OneSide-NoISon GSM8K with Qwen2.5-1.5B-Instruct.\nEvaluation accuracy (left) and training reward (right) are reported for varying learning rates.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2509.24203/x12.png",
                "caption": "Figure 11:Empirical results for OPMD and AsymRE (cf. Section4.2) on GSM8K with Qwen2.5-1.5B-Instruct under various off-policy settings.\nThe regularization coefficient for OPMD and the baseline shift for AsymRE are both0.10.1.\nTraining reward curves are smoothed with a running-average window of size 3.",
                "position": 2250
            }
        ]
    },
    {
        "header": "Appendix CSummary: a unified view of various algorithms",
        "images": []
    }
]