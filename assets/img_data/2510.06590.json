[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06590/x1.png",
                "caption": "",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2510.06590/x2.png",
                "caption": "",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2510.06590/x3.png",
                "caption": "",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06590/x4.png",
                "caption": "Figure 1:Conceptual comparison and qualitative examples ofMingTok. (a) Existing models using continuous latent spaces for unified visual understanding and generation uses two sets of representations for visual contents. (b)MingTokemployes a unified tokenizer for generating semantic and low-level image representatinos. (c) Compared with SD-VAE(Rombach et al.,2022),MingTokachieves over 3.5 times acceleration for text-to-image generation.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Unified Visual Tokenization without Vector Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06590/x5.png",
                "caption": "Figure 2:The model architecture and the training objectives ofMingTok.MingTokperforms image compression, semantic decoding and image reconstruction sequentially through low-level encoder, semantic decoder, and pixel decoder.\nDuring training, both the image latent and the semantic features are supervised by pre-trained visual encoders with masked feature prediction, while the pixel decoder is trained by masked and unmasked image reconstruction.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Unified Image Understanding and Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06590/x6.png",
                "caption": "Figure 3:The architecture ofMing-UniVision.\nOwing to the autoregressive semantic decoding capability ofMingTok, both image understanding (image-to-text generation) and image synthesis (text-to-image generation) can be formulated consistently with the same next-token prediction paradigm and unified input representation space.\nThis allows our unified multimodal model to support multi-round in-context tasks, seamlessly switch from understanding to generation/editing task, and vice versa.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2510.06590/x7.png",
                "caption": "Figure 4:Comparison of input token structures across different unified model architectures.Ming-UniVisionreduces the number of input visual tokens by 66% compared to hybrid AR-diffusion models(Shi et al.,2024; Deng et al.,2025)and by 50% compared to existing unified autoregressive models(Fan et al.,2025), thanks to the unified representation enabled byMingTok.",
                "position": 390
            }
        ]
    },
    {
        "header": "4Evaluations",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06590/x8.png",
                "caption": "Figure 5:Generation performance comparison during pre-training across different understanding (U) and generation (G) tokenizer combinations. UsingMingTokas the generation representation (MingTok(G)) achieves the best performance in generation-only training, significantly outperforming VAE-based representations (VAE (G)). WhenMingTokis used for both roles (MingTok(G & U), unified setting), the performance gap between pure generation and unified training narrows notably, demonstrating the benefit of universal visual representations for joint vision-language modeling.",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2510.06590/x9.png",
                "caption": "Figure 6:Qualitative comparison of multi-step editing across the three training strategies.Column 1 (Baseline):Struggles with sequential edits, altering the shirt’s style during an incomplete color change and leaving artifacts after smile removal.Column 2 (Add Seg-as-Edit):Benefiting from the segmentation task, our main model improves color fidelity and preserves the shirt’s style; smile removal is cleaner.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2510.06590/x10.png",
                "caption": "Figure 7:Qualitative illustration of multi-step visual editing workflows enabled by our unified model. (a) An old photograph is restored through a sequential process: first upscaled to higher resolution, then colorized. (b) Further iterative super-resolution can be applied to progressively enhance image quality, demonstrating the model’s capability for context-preserving, in-context refinement. (c) High-fidelity subject matting is achieved in two steps: segmentation to highlight the subject, then background removal.",
                "position": 1587
            },
            {
                "img": "https://arxiv.org/html/2510.06590/x11.png",
                "caption": "Figure 8:Illustration of the Visualized CoT. Under this paradigm, the model first performs visual reasoning to generate an image in which the regions requiring editing are highlighted on the reference image. Subsequently, the model completes the image editing according to these visual cues.",
                "position": 1609
            }
        ]
    },
    {
        "header": "6Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]