[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19204/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19204/x2.png",
                "caption": "Figure 2:Issue with Global Normalization (SSI).In (a), we compare two alignment strategies for the centralw/2,h/2ùë§2‚Ñé2w/2,h/2italic_w / 2 , italic_h / 2region: (1)Global Least-Square, where alignment is applied to the full image before cropping, and (2)Local Least-Square, where alignment is performed on the cropped region. Metrics are computed on the cropped region. As shown in (b),\nthe outperformed local strategy demonstrates\nthatglobal normalization degrades local accuracy compared to local normalization.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2502.19204/x3.png",
                "caption": "Figure 3:Overview of Cross-Context Distillation.Our method combines local and global depth information to enhance the student model‚Äôs predictions. It includes two scenarios: (1)Shared-Context Distillation, where both models use the same image for distillation;\nand (2)Local-Global Distillation, where the teacher predicts depth for overlapping patches while the student predicts the full image. The Local-Global loss‚Ñílgsubscript‚Ñílg\\mathcal{L}_{\\text{lg}}caligraphic_L start_POSTSUBSCRIPT lg end_POSTSUBSCRIPT(Top Right) ensures consistency between local and global predictions, enabling the student to learn both fine details and broad structures, improving accuracy and robustness.",
                "position": 188
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19204/x4.png",
                "caption": "Figure 4:Normalization Strategies.We compare four normalization strategies: Global Norm[34], Hybrid Norm[54], Local Norm, and No Norm. The figure visualizes how each strategy processes pixels within the normalization region (Norm. Area). The red dot represents any pixel within the region.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2502.19204/x5.png",
                "caption": "Figure 5:Different Inputs Lead to Different Pseudo Labels.Global Depth: The teacher model predicts depth using the entire image, and the local region‚Äôs prediction is cropped from the output. Local Depth: The teacher model directly takes the cropped local region as input, resulting in more refined and detailed depth estimates for that area, capturing finer details compared to using the entire image.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2502.19204/x6.png",
                "caption": "Figure 6:Multi-teacher Mechanism.We introduce a multi-teacher distillation approach, where pseudo-labels are generated from multiple teacher models. At each training iteration, one teacher is randomly selected to produce pseudo-labels for unlabeled images.",
                "position": 372
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19204/x7.png",
                "caption": "Figure 7:Qualitative Comparison of Relative Depth Estimations.We present visual comparisons of depth predictions from our method (‚ÄùOurs‚Äù) alongside other classic depth estimators (‚ÄùMiDaS v3.1‚Äù[3], and models using DINOv2[31]or SD as priors (‚ÄùDepthAnythingv2[47]‚Äù, ‚ÄùMarigold‚Äù[22], ‚ÄùGenpercept‚Äù[45]). Compared to state-of-the-art methods, the depth map produced by our model, particularly at the position indicated by theblack arrow, exhibits finer granularity and more detailed depth estimation.",
                "position": 425
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.19204/x8.png",
                "caption": "Figure 8:Comparison of Data Scaling\n.Performance comparison of our model with SSI Loss as the dataset size increases, measured by the average AbsRel. The results indicate that our method consistently outperforms the baseline method.",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2502.19204/x9.png",
                "caption": "Figure 9:Distilled Generative Models: Instead of just distilling classical depth models, we also apply distillation to generative models, aiming for the student model to capture their rich details.",
                "position": 1899
            },
            {
                "img": "https://arxiv.org/html/2502.19204/x10.png",
                "caption": "Figure 10:Qualitative Comparison with Baseline Distillation.We compare our method with the baseline as the previous distillation method, which uses only global normalization. The red diagonal lines represent the ground truth, with results closer to the lines indicating better performance. Our method produces smoother surfaces, sharper edges, and more detailed depth maps.",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2502.19204/x11.png",
                "caption": "Figure 11:Additional Results on Depth Estimation in the Wild.We showcase more depth maps generated by our model on in-the-wild scenes, highlighting its robustness and precision.",
                "position": 1905
            }
        ]
    },
    {
        "header": "6Appendix",
        "images": []
    }
]