[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17352/x1.png",
                "caption": "Figure 1:Performance (accuracy) of OpenVLThinker-7B on multi-modal reasoning benchmarks.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17352/x2.png",
                "caption": "Figure 2:Demonstration of the training process toward OpenVLThinker-7B. We obtain the SFT data for iteration 1 via text-based R1 model that only receives the question and the generated image caption. Subsequently, we apply SFT and GRPO iteratively to leverage new reasoning data from the previous iteration and achieve self-improvement. We also evolve the data sources to progressively include more challenging questions over iterations.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17352/x3.png",
                "caption": "Figure 4:Iterative performance improvement of our model on MathVista. We note that SFT-Iter(i) is always fine-tuned from the base modelQwen2.5-VL-7B, with its training data generated from GRPO-Iter(i-1). GRPO-Iter(i) is obtained by applying GRPO to train SFT-Iter(i).",
                "position": 201
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4OpenVLThinker: Iterative Self-improvement via SFT and RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17352/x4.png",
                "caption": "(a)Average response length",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x4.png",
                "caption": "(a)Average response length",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x5.png",
                "caption": "(b)Test score",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x6.png",
                "caption": "(a)Average response length",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x6.png",
                "caption": "(a)Average response length",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x7.png",
                "caption": "(b)Test score",
                "position": 525
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17352/x8.png",
                "caption": "Figure 7:Pass@k accuracy of different reasoning models based on captions generated with different vision-language LLMs.",
                "position": 1453
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.17352/extracted/6298557/figures/image.jpg",
                "caption": "Figure 8:The image to the visual reasoning task that we showed in section4.2",
                "position": 1599
            },
            {
                "img": "https://arxiv.org/html/2503.17352/extracted/6298557/figures/example_question.jpg",
                "caption": "Figure 9:The image to the demonstrated reasoning example in section1. Question: What is the length of side XY? Choices: (A) 4 (B) 5 (C) 6 (D) 9.",
                "position": 1602
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x9.png",
                "caption": "Figure 10:The reasoning length distribution of DeepSeek-R1-Distill-Qwen-14B given the image descriptions and questions from VQA datasets.",
                "position": 1605
            },
            {
                "img": "https://arxiv.org/html/2503.17352/x10.png",
                "caption": "Figure 11:Pass@4 accuracy of DeepSeek-R1-Distill-14B, which takes image descriptions and questions as inputs, on the six data sources we selected for SFT-Iter1.",
                "position": 1608
            }
        ]
    },
    {
        "header": "Appendix COutput Examples",
        "images": []
    }
]