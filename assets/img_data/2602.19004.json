[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19004/figures/teaser.png",
                "caption": "Figure 1:Proposed framework for motion binding between IMUs and 2D pose sequence from video.Contrastive learning is applied at both the local space, aligning each IMU with its corresponding body-part, and the global space, aligning full-body representations. This representation supports several downstream tasks, including cross-modal retrieval, temporal synchronization, subject and body parts localization, and human action recognition.",
                "position": 71
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19004/x1.png",
                "caption": "Figure 2:Overview of the proposed MoBind.The framework first encodes each IMU stream together with the motion of its corresponding body part, yielding token-level and local-level representations per sensor. These local representations are then aggregated across sensors to form global-level embeddings. The contrastive objective applies at all three levels. In addition, a Masked Token Prediction (MTP) module is used only during training to preserve coarse semantic structure, preventing the model from over-focusing on fine-grained alignment.",
                "position": 96
            }
        ]
    },
    {
        "header": "3MoBind",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19004/x2.png",
                "caption": "Figure 3:IMU→\\rightarrowVideo retrieval resultson mRi (left) and EgoHumans (right). Each example shows the query IMU signal, its corresponding ground-truth video segment, and the top three retrieved video segments. Our method successfully retrieves the ground-truth segment, and the other top-ranked results are also visually similar to the ground truth, demonstrating robust cross-modal alignment.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x3.png",
                "caption": "",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x4.png",
                "caption": "",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x5.png",
                "caption": "",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x6.png",
                "caption": "",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x7.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x8.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x9.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x10.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x11.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x12.png",
                "caption": "Figure 4:Per-action synchronization accuracyon EgoHumans (left) and mRi (right). MoBind achieves sub-50ms error on all EgoHumans actions and under 1s on all mRi actions, despite the challenges posed by repetitive movements and near-duplicate segments. Results confirm MoBind’s robustness across diverse motion types and environments.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x13.png",
                "caption": "Figure 5:Examples of body-part localizationon EgoHumans. Each column shows the query IMU (top) and the predicted body part with the highest similarity score (bottom), demonstrating accurate identification of sensor placement.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x14.png",
                "caption": "",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x15.png",
                "caption": "",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x16.png",
                "caption": "",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x17.png",
                "caption": "",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x18.png",
                "caption": "",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x19.png",
                "caption": "Figure 6:Results for the challenging combined task of temporal synchronization and spatial localization.The query20​s20\\mathrm{s}IMU signal from the left wrist precedes the video by6.8​s6.8\\,\\text{s}. MoBind accurately recovers this offset using a weighted histogram and simultaneously localizes the signal to the correct body part, demonstrating robustness to dynamic and highly repetitive motion.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x20.png",
                "caption": "",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x21.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2602.19004/x22.png",
                "caption": "Figure 7:Robustness to Sensor Failure.Retrieval performance (R@1 and R@5) under different sensor availability conditions. R@k measures the percentage of queries for which the ground-truth video appears within the top-kkretrieved results, given the representation computed from a subset of IMU sensors. In general, using more IMUs provides a more complete motion representation and thus improves retrieval accuracy. MoBind remains highly effective even when some sensors are unavailable, demonstrating strong performance under partial sensor input and highlighting its robustness for real-world deployment.",
                "position": 779
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]