[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01245/x1.png",
                "caption": "Figure 1:Fine-grained Action Instances.The two samples are drawn from the FineGym(Shao et¬†al.2020a)dataset, specifically the‚Äúpike sole circle backward with 0.5 turn to handstand‚Äùat the top and the‚Äú‚Ä¶ 1 turn ‚Ä¶‚Äùat the bottom. We further test popular MLLMs on the bottom instance for both coarse-grained and fine-grained: GPT-4V(OpenAI2024), VideoChat2(Li et¬†al.2024), VideoLLaVA(Lin et¬†al.2023), and InternLM-XComposer-2.5(Zhang et¬†al.2024).",
                "position": 139
            }
        ]
    },
    {
        "header": "Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/fig2_00.png",
                "caption": "Figure 2:Overview of SeFAR pipeline.We target Semi-supervised FAR, assuming most input samples are unlabeled.\nDuring unsupervised learning, SeFAR adoptsdual-level temporal elements modelingand performs augmentation in two manners (‚ÄòWeak‚Äôvs.‚ÄòStrong‚Äô). Strongly augmented/distorted samples bymoderate temporal perturbationare used by the student model, while the teacher model offers pseudo-labels based on weakly augmented samples. Consistency is enforced through loss minimization (‚Ñíu‚Å¢nsubscript‚Ñíùë¢ùëõ\\mathcal{L}_{un}caligraphic_L start_POSTSUBSCRIPT italic_u italic_n end_POSTSUBSCRIPT). The unsupervised loss is further adjusted by our proposedAdaptive Regulation. The framework is trained with a weighted combination of supervised‚Ñís‚Å¢u‚Å¢psubscript‚Ñíùë†ùë¢ùëù\\mathcal{L}_{sup}caligraphic_L start_POSTSUBSCRIPT italic_s italic_u italic_p end_POSTSUBSCRIPTand unsupervised‚Ñíu‚Å¢nsubscript‚Ñíùë¢ùëõ\\mathcal{L}_{un}caligraphic_L start_POSTSUBSCRIPT italic_u italic_n end_POSTSUBSCRIPTlosses.",
                "position": 214
            }
        ]
    },
    {
        "header": "Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/fig3.png",
                "caption": "Figure 3:(a)ForKùêæKitalic_Kunlabeled videos, the Teacher model predicts each video multiple times to capture the distribution of predictions, which shows less variability on coarse-grained data and more on fine-grained data. An adaptive coefficientŒ∑ùúÇ\\etaitalic_Œ∑is calculated from the mean and variance of the distribution to stabilize training.(b)MLLM construction pipeline with SeFAR‚Äôs fine-grained features.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/fig4.png",
                "caption": "Figure 4:Ablation Studies.We compare SeFAR-B with different sampling combinations on Gym-99 5%, as illustrated on theleft. We also contrast fixed threshold methods with our Adaptive Regulation strategy on FineDiving 5% in themiddle. On therightside, we demonstrate the fluctuation of predictions made by the Teacher model across different datasets.",
                "position": 1052
            }
        ]
    },
    {
        "header": "Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/LLaVA.png",
                "caption": "Table 5:Ablation of Pre-trained Visual Encoder.We employ Vicuna-7B(Chiang et¬†al.2023)as the base LLM, comparing SeFAR‚Äôs features with the pre-trained features of commonly used visual encoders in MLLMs further fine-tuned on 5% data (i.e.,: LLaVA,: VideoChat2,: VideoLLaMA,: VideoChat, and: VideoLLaVA)",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/VideoChat2.png",
                "caption": "",
                "position": 1305
            },
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/VideoLLaMA.png",
                "caption": "",
                "position": 1308
            },
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/VideoChat.png",
                "caption": "",
                "position": 1311
            },
            {
                "img": "https://arxiv.org/html/2501.01245/extracted/6107226/VideoLLaVA.png",
                "caption": "",
                "position": 1314
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01245/x2.png",
                "caption": "Figure 5:The relationship between the Teacher model‚Äôs prediction accuracy and its confidence (left), as well as its standard deviation (right).",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2501.01245/x3.png",
                "caption": "Figure 6:Examples of Gym-QA",
                "position": 2602
            },
            {
                "img": "https://arxiv.org/html/2501.01245/x4.png",
                "caption": "Figure 7:Examples of Gym-New",
                "position": 2607
            },
            {
                "img": "https://arxiv.org/html/2501.01245/x5.png",
                "caption": "Figure 8:Confusion matrix of baseline (left) and ours (right) on Gym-New 10%, where the horizontal coordinate represents the predicted label and the vertical coordinate represents the true label. The labels corresponding to actions are shown in Fig.9.",
                "position": 2612
            },
            {
                "img": "https://arxiv.org/html/2501.01245/x6.png",
                "caption": "Figure 9:Labels corresponding to actions in Gym-New.",
                "position": 2623
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]