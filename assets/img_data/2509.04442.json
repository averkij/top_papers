[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04442/x1.png",
                "caption": "Figure 1:Embedding Finetuned Models.Can we project a pool of finetuned models into a vector space that captures similarities and differences in model behaviors and capabilities? Such a model embedding is useful for tasks that involve multiple models, like model selection and merging.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Representing Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04442/x2.png",
                "caption": "Figure 2:Illustration of computing Delta Activations.The difference between a finetuned model’s hidden state and the base model’s hidden state on a shared input quantifies the effect of finetuning.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04442/x3.png",
                "caption": "Figure 3:t-SNE visualization of different embedding spaces.Delta Activations form clean and well-separated domain clusters compared to baseline methods.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x4.png",
                "caption": "Figure 4:Preference optimization.Delta Activations can cluster models trained with DPO.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x4.png",
                "caption": "Figure 4:Preference optimization.Delta Activations can cluster models trained with DPO.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x5.png",
                "caption": "Figure 5:Task embedding.Few-shot task embedding is able to locate model clusters.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x6.png",
                "caption": "(a)cross-checkpoint",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x6.png",
                "caption": "(a)cross-checkpoint",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x7.png",
                "caption": "(b)cross-architecture",
                "position": 693
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining settings",
        "images": []
    },
    {
        "header": "Appendix BAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04442/x8.png",
                "caption": "Figure 7:t-SNE visualization of different embedding spaces (LLaMA).",
                "position": 2067
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x9.png",
                "caption": "Figure 8:t-SNE visualization of different embedding spaces (Qwen).",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2509.04442/x10.png",
                "caption": "Figure 9:t-SNE visualization of full finetuning.",
                "position": 2205
            }
        ]
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix DBroader Impact",
        "images": []
    }
]