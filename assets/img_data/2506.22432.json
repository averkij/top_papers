[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x1.png",
                "caption": "Figure 1.The proposed 3D-aware framework, Shape-for-Motion, supports precise and consistent video editing by reconstructing an editable 3D mesh to serve as control signals for video generation. The first two examples demonstrate pose editing (by rotating the back to the right by 50 degrees and the head to the left by 10 degrees) and object composition (by composing a tree from the reference image onto the top of the car). In each example, the first row shows the input video frames, followed by the editing in 3D space at the right end; the bottom row of images shows the corresponding edited frames. In addition, our approach also supports diverse applications, such as Image-to-Video Animation, as shown in the third example.",
                "position": 148
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x2.png",
                "caption": "Figure 2.Overview ofShape-for-Motion. Our approach is an interactive video editing framework that utilizes an editable 3D proxy (e.g., mesh) to enable users to perform precise and consistent video editing. Given an input video, our approach first converts the target object into a 3D mesh with frame-by-frame correspondences. Users can then perform editing on this 3D proxy once only (i.e., on a single frame), and the edits will be automatically propagated to the 3D meshes of all other frames. The edited 3D meshes are then converted back to 2D geometry and texture maps, which are used as control signals in a decoupled video diffusion model to generate the final edited result. Note that, for better visualization, the colors of the propagated meshes are disabled in Step-IIII\\mathrm{II}roman_II.",
                "position": 202
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.OUR APPROACH",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x3.png",
                "caption": "Figure 3.Pipeline of our consistent object reconstruction. We use deformable-3DGS to reconstruct the 3D mesh of the target object in a video by maintaining canonical Gaussians and a time-varying deformation field.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x4.png",
                "caption": "Figure 4.Workflow of our consistent editing propagation. Solid orange arrows denote texture propagation from the canonical mesh, and solid blue arrows indicate geometry propagation from canonical Gaussians.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x5.png",
                "caption": "Figure 5.Training pipeline of the generative rendering from the 3D proxy. Using rendered geometry (i.e., normal map) and texture as separate control signals, we adopt a self-supervised mixed training strategy in which the geometry controller and the texture enhancer (i.e., denoising U-Net) are alternately trained in two stages. The contents enclosed by the purple dashed box indicates inputs shared by both stages, while those in the orange and blue dashed boxes are specific to the first and second stages, respectively. Reference objects and coarse textures are constructed via the reference and texture simulators to facilitate self-supervised training.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x6.png",
                "caption": "Figure 6.Qualitative comparison with four video editing baselines (DragVideo(Deng et¬†al.,2024), Image Sculpting(Yenphraphai et¬†al.,2024)+ I2V-Edit(Ouyang et¬†al.,2024), Pix2Video(Guo et¬†al.,2024b), and Tune-A-Video(Wu et¬†al.,2023)). Due to their lack of 3D awareness, the baseline methods only achieve limited modifications to the object geometry. Instead, our approach enables direct editing in the 3D space only once, ensuring precise and consistent results for all frames. The first row shows inputs, and the second highlights text instructions (provided solely for illustrating the performed edits) and 3D space edits.",
                "position": 414
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": []
    },
    {
        "header": "5.APPLICATIONS",
        "images": []
    },
    {
        "header": "6.CONCLUSION",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x7.png",
                "caption": "Figure 7.Comparison of the key components in consistent 3D proxy reconstruction. Each row starts with an input video on the left, followed by results of our full model in the middle and ablated baselines on the right, each consisting of a rendered image and its corresponding normal map.",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x8.png",
                "caption": "Figure 8.Comparison of editing propagation strategies. The edited 3D mesh is rendered into the texture (i.e., mesh image) and geometry (i.e., normal map).\nThe first column presents the input image at frametùë°titalic_t, followed by the editing in 3D space at this frame in the second column, with results from ours and two variants shown in subsequent columns. Variant-1: mesh-propagated edited mesh with its color. Variant-2: Gaussian-propagated edited mesh with its color.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x9.png",
                "caption": "Figure 9.Comparison of mixed training in generative rendering from the 3D proxy. For each case, the input video is shown on the left, followed by 3D space editing in the second column, with results from our method and the baseline in the subsequent columns. Two frames are displayed for each case.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x10.png",
                "caption": "Figure 10.Our approach supports Image-to-Video animation. Starting with an input image (first column), we generate its 3D mesh and apply skeletal rigging (second column). The rigged model is then animated with a driven motion sequence to produce edited meshes (third column), whose geometry and texture renderings are fed into our decoupled diffusion model to generate a high-quality video.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x11.png",
                "caption": "Figure 11.Our approach supports integration with existing 2D image editing tools (e.g., Flux(Labs,2024)) to enable diverse object appearance editing. The two input frames are shown on the left, the editing process‚Äîincluding geometry editing in 3D space and appearance editing in 2D space‚Äîis displayed in the middle, and the final edited results are presented on the right.",
                "position": 730
            }
        ]
    },
    {
        "header": "Appendix AConsistent 3D Proxy Reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x12.png",
                "caption": "Figure A12.Workflow of data pre-processing in consistent 3D proxy reconstruction.",
                "position": 781
            }
        ]
    },
    {
        "header": "Appendix BEditing 3D proxy with automatic propagation",
        "images": []
    },
    {
        "header": "Appendix CGenerative rendering from 3D proxy",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x13.png",
                "caption": "Figure C13.Inference pipeline of our decoupled video diffusion.",
                "position": 996
            }
        ]
    },
    {
        "header": "Appendix DDiscussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x14.png",
                "caption": "Figure D14.A 9.3-second edited video example demonstrating our method‚Äôs ability to handle long video sequences.",
                "position": 1156
            },
            {
                "img": "https://arxiv.org/html/2506.22432/x15.png",
                "caption": "Figure D15.Limitation. Our approach struggles to generate the corresponding visual effects of the target object (e.g., reflections, as highlighted by the red arrow on the water for the white swan). Edited mesh is shown in the middle.",
                "position": 1224
            }
        ]
    },
    {
        "header": "Appendix EAdditional Visual Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22432/x16.png",
                "caption": "Figure E16.Additional visual results of our method. For each case, three or four input frames are shown on the left, and the edited results are displayed on the right. For each case, we highlight the editing using the red arrow in the first frame, except the texture modification.",
                "position": 1236
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]