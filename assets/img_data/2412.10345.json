[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/x1.png",
                "caption": "Figure 1:An illustration of our method. The first image shows the original robot’s observation, while the second contains the same image with overlaid visual traces. A separator token is then inserted between the visual tokens of these two images, then concatenating with text tokens and feeding into the underlying vision language model backbone to output action tokens.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3TraceVLA",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/x2.png",
                "caption": "Figure 2:An illustration of visual trace generation. Given a sequence of historical image observations, we first use Co-tracker to extract dense point trajectories and keep active point trajectories with significant movement. We then overlay active point trajectories on the robot’s initial observation frame as visual trace prompting. We feed both the image overlaid with visual traces and the original image into VLA as model input.",
                "position": 168
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/x3.png",
                "caption": "Figure 3:(Left): 7BTraceVLAvs. 7B OpenVLA. (Right): 4BTraceVLA-Phi3 vs. 4B OpenVLA-Phi3. Numbers are averaged across the visual matching and variant aggregation metrics.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x4.png",
                "caption": "Figure 4:Comparison of OpenVLA andTraceVLAperformance across various environmental variations: camera orientations, lighting, background, distractors, and table texture.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x5.png",
                "caption": "Figure 5:Real robot setup.We design 4 real-world robot tasks with different manipulation skills and objects.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x6.png",
                "caption": "Figure 6:Four unseen tasks for testing generalization in real robot settings.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x7.png",
                "caption": "((a))TraceVLAoutperforms OpenVLA on diverse real-robot manipulation tasks.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x7.png",
                "caption": "((a))TraceVLAoutperforms OpenVLA on diverse real-robot manipulation tasks.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x8.png",
                "caption": "((b))TraceVLAshowcases superior generalization on unseen real robot experiments.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x9.png",
                "caption": "Figure 8:(Left): Comparison of average success rates between the base OpenVLA and OpenVLA-Phi3 models and their finetuned versions, with and without visual trace prompting. (Right): Comparison of average success rates between the base OpenVLA,TraceVLA, and OpenVLA finetuned with a sequence of 6 images.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x10.png",
                "caption": "Figure 9:(Left): Comparing visual trace prompting and text trace prompting. (Right) Text trace prompts example.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x11.png",
                "caption": "Figure 10:TraceVLAunder different length of visual traces.",
                "position": 755
            }
        ]
    },
    {
        "header": "5Limitation Analysis: Training Memory Cost and Inference Speed",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/x12.png",
                "caption": "Figure 11:(Left):Comparison of GPU memory cost of 7BTraceVLA, OpenVLA and 4BTraceVLA-Phi3, OpenVLA-Phi3.(Right): Comparison of inference time across different models.",
                "position": 789
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and discussion",
        "images": []
    },
    {
        "header": "8Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AReal Robot Tasks Setup",
        "images": []
    },
    {
        "header": "Appendix BQualitative results on real robot rollouts",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/x13.png",
                "caption": "Figure 12:Pickplace Banana task. (Above): OpenVLA rollout. (Below):TraceVLArollout with visual trace prompting.",
                "position": 1574
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x14.png",
                "caption": "Figure 13:Fold Cloth task. (Above): OpenVLA rollout. (Below):TraceVLArollout with visual trace prompting.",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2412.10345/x15.png",
                "caption": "Figure 14:Pickplace Eggplant task. (Above): OpenVLA rollout. (Below):TraceVLArollout with visual trace prompting.",
                "position": 1580
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/x16.png",
                "caption": "Figure 15:Comparison of TraceVLA against OpenVLA with different steps of observation history.",
                "position": 1670
            }
        ]
    },
    {
        "header": "Appendix DAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix EMore Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.10345/extracted/6066198/figures/libero_spatial.png",
                "caption": "((a))LIBERO-Spatial",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2412.10345/extracted/6066198/figures/libero_spatial.png",
                "caption": "((a))LIBERO-Spatial",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2412.10345/extracted/6066198/figures/libero_object.png",
                "caption": "((b))LIBERO-Object",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2412.10345/extracted/6066198/figures/libero_goal.png",
                "caption": "((c))LIBERO-Goal",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2412.10345/extracted/6066198/figures/libero_10.png",
                "caption": "((d))LIBERO-Long",
                "position": 1833
            }
        ]
    },
    {
        "header": "Appendix FAdditional Experimental Results on LIBERO simulation benchmarks",
        "images": []
    }
]