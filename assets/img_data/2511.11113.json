[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x1.png",
                "caption": "Figure 1:Comparison between GRPO-based video RFT framework (process-agnostic) andVideoP2R(process-aware).",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x2.png",
                "caption": "Figure 2:Illustration of overallVideoP2RRFT framework (left) and the three-step CoT generation pipeline (right).",
                "position": 170
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3VideoP2RRFT Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x3.png",
                "caption": "Figure 3:The illustration of the PA-GRPO algorithm.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x4.png",
                "caption": "Figure 4:Effect of perception on downstream reasoning",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x5.png",
                "caption": "Figure 5:Success (Left) and Failure (Right) case ofVideoP2R.Correctstatement andincorrectstatement are colored.",
                "position": 791
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x6.png",
                "caption": "Figure 6:Training Dynamics and Think-Answer Mismatch Analysis ofVideoP2R. Details in Section5.4.",
                "position": 798
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Details of Process-Aware CoT Generation and Data Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x7.png",
                "caption": "Figure 7:Prompt Template for Process-aware CoT Generation. We use the same prompt for training and inference.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x8.png",
                "caption": "Figure 8:Prompt Template for Observation Sufficiency Verification. We use the same prompt for perception correctness judgment in RL stage.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2511.11113/figure/embedding.png",
                "caption": "Figure 9:Embeddings visualization ofVideoP2R-CoT-162K",
                "position": 1951
            },
            {
                "img": "https://arxiv.org/html/2511.11113/figure/detailed_data_analysis.png",
                "caption": "Figure 10:Word length (Left) and Word cloud (Right) Visualization for VideoP2R-CoT-162K.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x9.png",
                "caption": "Figure 11:An Annotation Example of the Video QA Sample",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x10.png",
                "caption": "Figure 12:Annotation Example of the Image QA Sample",
                "position": 1967
            }
        ]
    },
    {
        "header": "8Experiment Set up",
        "images": []
    },
    {
        "header": "9Ablation Study on Judge Model",
        "images": []
    },
    {
        "header": "10Details of the Perception Examination",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x11.png",
                "caption": "Figure 13:Prompt Template for Perception Examination Experiment.",
                "position": 2160
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x12.png",
                "caption": "Figure 14:Prompt Template for Answer Extraction.",
                "position": 2277
            }
        ]
    },
    {
        "header": "11RL Training Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x13.png",
                "caption": "Figure 15:RL training Dynamics ofVideoP2R",
                "position": 2292
            }
        ]
    },
    {
        "header": "12Think-Answer Mismatch Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11113/x14.png",
                "caption": "Figure 16:Example of Think-Answer Mismatch.",
                "position": 2315
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x15.png",
                "caption": "Figure 17:Failure Cases of Key-frame Missing inVideoP2R",
                "position": 2477
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x16.png",
                "caption": "Figure 18:Failure Cases of Overly detailed visual configuration inVideoP2R",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x17.png",
                "caption": "Figure 19:Success Cases ofVideoP2R",
                "position": 2486
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x18.png",
                "caption": "Figure 20:From base Qwen2.5-VL-7B toVideoP2R-SFT andVideoP2R: a representative example illustrating the stepwise improvement in model’s perception and reasoning.",
                "position": 2496
            },
            {
                "img": "https://arxiv.org/html/2511.11113/x19.png",
                "caption": "Figure 21:Examples of Perception Examination: Top Left: Qwen with the text question only; Top Right: Qwen with the text question plus the perception segments from Qwen; Bottom Left: Qwen with the text question plus the video input; Bottom Right: Qwen with the text question plus the perception segments fromVideoP2R.Greentext denotes correct visual information or reasoning traces, whileredtext denotes incorrect or insufficient visual information or reasoning traces.",
                "position": 2499
            }
        ]
    },
    {
        "header": "13More Qualitative Results ofVideoP2R",
        "images": []
    }
]