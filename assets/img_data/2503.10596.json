[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3GSSculpt: Large-scale Grounding Labeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10596/x1.png",
                "caption": "Figure 1:GSScuplt Automatic Annotation Framework.Our pipeline consists of three sequential phases: (1) Entity Spatial Localization, where we first identify potential objects of interest and generate high-quality segmentation masks; (2) Grounding Text Generation, where we then create unambiguous natural language descriptions that uniquely reference the segmented objects; and (3) Noise Filtering, where we finally eliminate ambiguous or low-quality samples to ensure dataset reliability.",
                "position": 308
            }
        ]
    },
    {
        "header": "4GSEval: A Comprehensive Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10596/x2.png",
                "caption": "Figure 2:Curation pipeline for GSEval Benchmark.First, we apply our annotation pipeline to unlabeled COCO images. Next, we use a VLM classifier to ensure the categories of referring prompts. Then, we translate the coarse masks to trimaps and apply matting methods for precise object boundaries. Finally, we organize human reviewers for manual checks.",
                "position": 394
            }
        ]
    },
    {
        "header": "5Evaluation on GSEval",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10596/x3.png",
                "caption": "Figure 3:The visualization comparisons of different methods on GSEval.All methods are evaluated under the zero-shot setting with the public code and weights.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2503.10596/x4.png",
                "caption": "Figure 4:The visualization comparisons of differnet methods on our GSEval-BBox.All open-source methods are evaluated under the zero-shot setting with the public code and weights.",
                "position": 544
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGSEval Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10596/extracted/6278345/fig/wordcloud.png",
                "caption": "Figure 5:The word cloud of GSEval",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2503.10596/x5.png",
                "caption": "Figure 6:More selected samples from our GSEval.Stuff class and part level",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2503.10596/x6.png",
                "caption": "Figure 7:More selected samples from our GSEval.Multi object and single object",
                "position": 1566
            },
            {
                "img": "https://arxiv.org/html/2503.10596/x7.png",
                "caption": "Figure 8:Prompt for global caption and grounding text generation",
                "position": 1593
            },
            {
                "img": "https://arxiv.org/html/2503.10596/x8.png",
                "caption": "Figure 9:Prompt for noise filtering",
                "position": 1596
            }
        ]
    },
    {
        "header": "Appendix BPrompt",
        "images": []
    }
]