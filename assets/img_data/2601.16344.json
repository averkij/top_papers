[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16344/figures/dsgym-logo.png",
                "caption": "",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2601.16344/x1.png",
                "caption": "",
                "position": 197
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16344/x2.png",
                "caption": "Figure 2:The Architecture ofDSGym.(a)Standardized Tasks:We aggregate heterogeneous data sources into a unified task object. (b)Agent Interface:DSGymprovides a default CodeAct-like agent to interact with the environment. (c)Execution Environment:A central Manager container orchestrates the execution. Based on the task type, it dispatches agents to isolated Docker containers (Workers) pre-loaded with domain-specific libraries. Crucially, datasets are mounted asRead-Only Volumes, while agents operate in a separate writable workspace.",
                "position": 224
            }
        ]
    },
    {
        "header": "2DSGym: A Unified Framework for Reproducible Data Science Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16344/x3.png",
                "caption": "Figure 3:Accuracy with or without data access on three file-grounded benchmarks.We observe that even when real data files are not provided, agents can still answer a substantial fraction of questions correctly, suggesting that existing benchmarks can be partially solved via memorization, pattern matching, or priors rather than genuine data interaction.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2601.16344/x4.png",
                "caption": "Figure 4:Example questions across data science benchmarks.Existing datasets such asQRData,DAEval, andDABStepmainly target general or applied data-science operations.DSGymcomplements these with new domain-specific scientific tasks (e.g., bioinformatics) that require specialized workflows and terminology.",
                "position": 412
            }
        ]
    },
    {
        "header": "3Limitations of Existing Data Science Benchmarks",
        "images": []
    },
    {
        "header": "4DSGym-Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16344/x5.png",
                "caption": "Figure 5:Filtering statistics after two-stage refinement.",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2601.16344/x6.png",
                "caption": "Figure 6:Dataset Construction Pipeline.Our data construction pipeline curates domain-specific scientific tasks from academic literature and aggregates real-world predictive modeling challenges from Kaggle competitions.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2601.16344/x7.png",
                "caption": "Figure 7:Percentage of task domains",
                "position": 713
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16344/x8.png",
                "caption": "Figure 8:Error type breakdowns for four LLMson (a) general analysis tasks (QRDataandDAEval) and (b) scientific analysis tasks (DSBio).\nFor each model and task family, we uniformly sample 50 failed trajectories and manually assign a primary error category (definitions in AppendixC.1; representative cases in AppendixB.1).\nA key shift emerges: while failures on general tasks are dominated by statistical knowledge and planning issues, failures onDSBioare overwhelmingly driven by domain-grounding errors (85–96% across models).",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2601.16344/x9.png",
                "caption": "Figure 9:(a)Accuracy on the same error-cleaned QRData splitwith vs. withoutenforcing data dependency.All models exhibit consistent drops after filtering, indicating that a non-trivial portion of pre-filter performance can be achieved via non-data-grounded shortcuts (e.g., memorization, priors, etc). (b)Execution-grounded SFT changes agent interaction behavior toward teacher-like trajectories.Across four datasets, we report the mean±\\pmstd of the number of turns per trajectory and tokens per turn for two teacher models and a 4B base model before/after DSGym-SFT. DSGym-SFT increases the number of turns while shifting tokens-per-turn toward teacher-like statistics, indicating finer-grained decomposition and more iterative execution.",
                "position": 1266
            }
        ]
    },
    {
        "header": "6Demonstration: Training Data Science Agents viaDSGym",
        "images": []
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Discussion and Limitations",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details ofDSGym-Tasks",
        "images": []
    },
    {
        "header": "Appendix BCase Studies",
        "images": []
    },
    {
        "header": "Appendix CMore Analysis Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16344/figures/DSPredict-Hard-error_categories_barplot.png",
                "caption": "Figure S1:Failure modes for agents on DSPredict-Hard. Three models are annotated with four categories.",
                "position": 3185
            }
        ]
    },
    {
        "header": "Appendix DExperiment Details",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]