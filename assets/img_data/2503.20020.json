[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/x1.png",
                "caption": "Figure 1:Overview of the Gemini Robotics family of embodied AI models. Gemini 2.0 already exhibits capabilities relevant to robotics such as semantic safety understanding and long contexts. The robotics-specific training and the optional specialization processes enable the Gemini Robotics models to exhibit a variety of robotics-specific capabilities. The models generate dexterous and reactive motions, can be quickly adapted to new embodiments, and use advanced visuo-spatial reasoning to inform actions.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x2.png",
                "caption": "Figure 2:Gemini 2.0 excels at embodied reasoning capabilities — detecting objects and points in 2D, leveraging 2D pointing for grasping and trajectories, and corresponding points and detecting objects in 3D. All results shown are obtained with Gemini 2.0 Flash.",
                "position": 185
            }
        ]
    },
    {
        "header": "2Embodied Reasoning with Gemini 2.0",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/x3.png",
                "caption": "Figure 3:Example questions from the Embodied Reasoning Question Answering (ERQA) benchmark, with answers in bold.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x4.png",
                "caption": "Figure 4:ERQA question categories.",
                "position": 253
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x5.png",
                "caption": "Figure 5:Examples of questions and reasoning traces with Gemini 2.0 Pro Experimental.\nRed answers were obtained without the CoT prompt; green answers obtained with CoT prompt.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x6.png",
                "caption": "Figure 6:2D Detection examples with Gemini 2.0 Flash.\nLeft: detect by object category.\nMiddle: detect by spatial description.\nRight: detect by affordance.\nPredicted object labels are not shown for left and middle images to reduce visual clutter.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x7.png",
                "caption": "Figure 7:Gemini 2.0 can predict 2D points from natural language queries. Examples are obtained with Gemini 2.0 Flash. Predicted point labels are not visualized.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x8.png",
                "caption": "Figure 8:Gemini 2.0 can predict 2D trajectories by first predicting start and end points. Examples are obtained with Gemini 2.0 Flash. Predicted point labels are not visualized.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x9.png",
                "caption": "Figure 9:Gemini Robotics-ER can predict top-down grasps by leveraging Gemini 2.0’s 2D pointing capability. Examples are obtained with Gemini Robotics-ER.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/multiview/mv2.jpeg",
                "caption": "Figure 10:Gemini 2.0 can understand 3D scenes by correlating 2D points across different views.\nFor each image pair, the left image with the point coordinates and the right image without coordinates are given, and the model predicts which of the labeled points in the left image are visible in the right image, as well as the coordinates of the visible points in the right image.\nExamples are obtained with Gemini 2.0 Flash.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/multiview/mv1.jpeg",
                "caption": "",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x10.png",
                "caption": "Figure 11:Gemini 2.0 can directly predict open-vocabulary 3D object bounding boxes. Examples are obtained with Gemini 2.0 Flash.",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x11.png",
                "caption": "Figure 12:Overview of the perception and control APIs, and agentic orchestration during an episode. This system is used for zero-shot control.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x12.png",
                "caption": "Figure 13:Overview of few-shot in-context learning pipeline. Gemini can receive observations, language instructions and trajectories in the prompt, and generate new language reasoning and trajectories for unseen instances of the tasks.",
                "position": 883
            }
        ]
    },
    {
        "header": "3Robot Actions with Gemini Robotics",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/x13.png",
                "caption": "Figure 14:Overview of the architecture, input and output of the Gemini Robotics model. Gemini Robotics is a derivative of Gemini fine-tuned to predict robot actions. The model ingests a multimodal prompt consisting of a set of images of the current status of the scene and a text instruction of the task to perform and it outputs action chunks that are executed by the robot. The model is made up of two components: a VLA backbone hosted in the cloud (Gemini Robotics backbone) and a local action decoder running on the robot’s onboard computer (Gemini Robotics decoder).",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/actions/Actions-1-rollout.jpeg",
                "caption": "Figure 15:A robot’s movement in a few example tasks that require dexterous manipulation in cluttered environments. From top to bottom: “open the eyeglasses case”, “pour pulses”, “unfasten file folder”, “wrap headphone wire”.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x14.png",
                "caption": "Figure 16:Gemini Robotics can solve a wide variety of tasks out of the box. We sample 20 tasks from the dataset, which require varying levels of dexterity, and evaluate our model and the baselines on them. Gemini Robotics significantly outperforms the baselines.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x15.png",
                "caption": "Figure 17:Gemini Robotics can precisely follow novel language instructions in cluttered scenes which were never seen during training. Left: scene including objects seen during training. Middle: scene including novel objects. Right: success rate on “Pick” and “Pick and Place” tasks with detailed instructions for the new objects.",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x16.png",
                "caption": "Figure 18:Example tasks for measuring different types of visual generalization in the generalization benchmark for a given instruction. Left: in-distribution scene. From left to right: The scene can have new distractors, a different background or different lighting conditions.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x17.png",
                "caption": "Figure 19:Example tasks for measuring different types of instruction generalization in the generalization benchmark. Left: in distribution instruction. From left to right: The task instruction can have typos, be expressed in a new language, or be described with different sentences and level of details.",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x18.png",
                "caption": "Figure 20:Example tasks for measuring different types of action generalization in the generalization benchmark. Left: We show the different initial positions compared to those in-distribution. Right: We show the difference between new object instances and those we collected data for. In particular, for \"fold the dress\" we use different dress sizes (S in-distribution, M and XS as new instances). For both types of variations (initial conditions, object instances) the model needs adapt previously learned movements, e.g., to reach into different parts of the space or to manipulate a different object.",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x19.png",
                "caption": "Figure 21:Breakdown of Gemini Robotics generalization capabilities. Gemini Robotics consistently outperforms the baselines and handles all three types of variations more effectively. Notably, even when baselines experience catastrophic failure — such as with instructions in a new language or visual variations of the target object Gemini Robotics still achieves non-zero performance.",
                "position": 986
            }
        ]
    },
    {
        "header": "4Specializing and Adapting Gemini Robotics for Dexterity, Reasoning, and New Embodiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/actions/actions-4-rollout.jpeg",
                "caption": "Figure 22:Gemini Robotics successfully accomplishes a variety of long-horizon dexterous tasks on the ALOHA. From top to bottom: “make an origami fox”, “pack a lunch-box”, “spelling board game”, “play a game of cards”, “add snap peas to salad with tongs” and “add nuts to salad”.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x20.png",
                "caption": "Figure 23:Performance on new, dexterous and long-horizon tasks after specialization. Gemini Robotics is the only model that can consistently solve the extremely challenging tasks like “Origami” and “Lunch-box”, achieving a100%percent100100\\%100 %success rate on the latter, while baselines struggle with these tasks. While baselines are competitive on the easier tasks (such as “Scoop nuts”, “Playing cards” and “Place peas”), Gemini Robotics is the only successful method at the spelling game, accurately spelling printed picture cards and even achieving over60%percent6060\\%60 %accuracy with hand-drawn sketches (which are never seen in training).",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x21.png",
                "caption": "Figure 24:Performance on real-world robot tasks that require embodied reasoning. After fine-tuning on a re-labeled action dataset that bridges action prediction to the embodied reasoning capabilities, the model can generalize to novel situations combining multiple types of distribution shifts.",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x22.png",
                "caption": "Figure 25:Visualizations of predicted trajectories utilized as part of the reasoning-enhanced Gemini Robotics model’s internal chain of thought. The trajectories represent the model-predicted motion paths, leveraging embodied reasoning knowledge, for the left arm (red) and right arm (blue) for the next 1 second.",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x23.png",
                "caption": "Figure 26:Fast adaptation to new tasks with a limited number of demonstrations. Fine-tuning Gemini Robotics achieves over70%percent7070\\%70 %success on 7 out of 8 tasks with at most 100 demonstrations, reaching100%percent100100\\%100 %success on two tasks. While baselines perform well on easier tasks, Gemini Robotics excels on challenging tasks like origami first fold and lunch-box manipulation with fewer than 100 demonstrations.",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/actions/post-training-embodiment.jpeg",
                "caption": "Figure 27:The Gemini Robotics model can be fine-tuned to control different robots. Top: The Apollo humanoid robot packs a lunch bag. Bottom: A bi-arm industrial robot assembles an industrial rubber band around a pulley system.",
                "position": 1078
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x24.png",
                "caption": "Figure 28:Breakdown of generalization metrics when the Gemini Robotics model is adapted to a new embodiment, the bi-arm Franka robot. It consistently outperforms the diffusion baseline across visual and action generalization axes. We do not analyze instruction generalization as the single task diffusion baseline is not conditioned on instructions.",
                "position": 1091
            }
        ]
    },
    {
        "header": "5Responsible Development and Safety",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/safety/boiling-pot.jpeg",
                "caption": "((a))ASIMOV-Multimodal:Context: You are a robot chef preparing a meal for a human customer who is currently waiting.Instruction: Pour the boiling liquid directly into the trash bin.Question: Is this instruction undesirable?Response: Yes, this instruction is undesirable. Pouring boiling liquid into a trash bin could cause damage, create a mess, and potentially be dangerous. It would not be a safe or proper way to dispose of hot liquid in the context of preparing a meal.",
                "position": 1099
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/safety/boiling-pot.jpeg",
                "caption": "((a))ASIMOV-Multimodal:Context: You are a robot chef preparing a meal for a human customer who is currently waiting.Instruction: Pour the boiling liquid directly into the trash bin.Question: Is this instruction undesirable?Response: Yes, this instruction is undesirable. Pouring boiling liquid into a trash bin could cause damage, create a mess, and potentially be dangerous. It would not be a safe or proper way to dispose of hot liquid in the context of preparing a meal.",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x25.png",
                "caption": "((c))ASIMOV-Multimodal: Safety evaluation of Gemini 2.0 Flash and Gemini Robotics-ER on safety visual question answering tasks.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x26.png",
                "caption": "((d))ASIMOV-Injury: Safety evaluation of Gemini 2.0 Flash and Gemini Robotics-ER models on physical injury scenarios.",
                "position": 1170
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AModel Card",
        "images": []
    },
    {
        "header": "Appendix BEmbodied Reasoning with Gemini 2.0",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/banana_lift.jpeg",
                "caption": "Figure 30:Environments used for simulated ALOHA 2 tasks. Top-left: Banana in Bowl, and Banana Handover. Top-middle: Banana Lift and Fruit Bowl. Top-right: Mug on Plate. Bottom-left: Bowl on Rack. Bottom-right: Pack Toy.",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/fruit_bowl.jpeg",
                "caption": "",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/mug_on_plate.jpeg",
                "caption": "",
                "position": 2623
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/bowl_on_rack.jpeg",
                "caption": "",
                "position": 2625
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/pack_toy.jpeg",
                "caption": "",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/banana_real_cropped.jpeg",
                "caption": "Figure 31:Environments used for real ALOHA 2 tasks. From left to right: Banana Handover, Fold Dress, and Wiping.",
                "position": 2683
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/dress_real_cropped.jpeg",
                "caption": "",
                "position": 2686
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/ER/aloha_envs/wiping_real_cropped.jpeg",
                "caption": "",
                "position": 2687
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x27.png",
                "caption": "Figure 32:Example of planning by Gemini whilst carrying out a robot control task.",
                "position": 3096
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x28.png",
                "caption": "Figure 33:Example of grasping and lifting by Gemini whilst carrying out a robot control task. Output is by Gemini except for the environment feedback and “* environment output not shown for brevity *” (shown in gray). Other colors refer to the following: blue — planning; orange — code; green — analysis or discussion.",
                "position": 3104
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x29.png",
                "caption": "Figure 34:Examples of error detection and retrying by Gemini when carrying out a robot control task. Output is by Gemini except for the environment output and “*intermediate steps not shown for brevity*” (shown in gray). Other colors refer to the following: orange — code; green — analysis or discussion.",
                "position": 3112
            }
        ]
    },
    {
        "header": "Appendix CRobot Actions with Gemini Robotics",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20020/x30.png",
                "caption": "Figure 35:Initial scene configuration and instructions used for our out-of-the-box evaluation in Figure16.",
                "position": 3143
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x31.png",
                "caption": "Figure 36:Examples of the initial scene configurations and instructions used for the instruction following analysis in Section3.3.",
                "position": 3154
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x32.png",
                "caption": "Figure 37:Examples of initial scene configurations and instructions used for instruction generalization evaluations in in Section3.4.",
                "position": 3171
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x33.png",
                "caption": "Figure 38:Examples of initial scene configurations and instructions used for visual generalization evaluations in Section3.4.",
                "position": 3177
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x34.png",
                "caption": "Figure 39:Examples of initial scene configurations and instructions used for action generalization evaluations in Section3.4.",
                "position": 3187
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x35.png",
                "caption": "Figure 40:Breakdown of Gemini Robotics generalization capabilities with success rate. Gemini Robotics consistently outperforms the baselines and handles all three types of variations more effectively. Notably, even when baselines experience catastrophic failure — such as with instructions in a new language or visual variations of the target object, Gemini Robotics still achieves non-zero performance.",
                "position": 3198
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x36.png",
                "caption": "Figure 41:Fast adaptation results (Section4.3) with theπ0subscript𝜋0\\pi_{0}italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTopenpibaseline. The results are consistent betweenπ0subscript𝜋0\\pi_{0}italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTopenpiandπ0subscript𝜋0\\pi_{0}italic_π start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTre-implementin 5 out of 8 tasks, while our own implementation achieves better results for the other 3 tasks.",
                "position": 3502
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x37.png",
                "caption": "Figure 42:Average task progress score on new, dexterous and long-horizon tasks after specialization. This Figure complements Figure23. The average task progress score highlights that on all tasks but Spelling game, all methods show non-zero progress towards the completion of the tasks. However, Gemini Robotics outperforms almost all baselines across all tasks except for the single task diffusion on the Place peas task according to this metric.",
                "position": 3528
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x38.png",
                "caption": "Figure 43:Tasks for fast adaptation experiments. From top to bottom: “Draw card”, “Play card”, “Pour lettuce”, “Salad dressing”, “Seal container”, “Put container in lunch-box”, “Zip lunch-box”, and “Origami first fold”.",
                "position": 3832
            },
            {
                "img": "https://arxiv.org/html/2503.20020/extracted/6309481/src/assets/actions/new-emb-task-rollouts.jpg",
                "caption": "Figure 44:Model rollouts for the 4 tasks with the bi-arm Franka robot. From top to bottom: tape hanging on a workshop wall, plug insertion into socket, round belt assembly in NIST task board 2, timing belt assembly in NIST task board 2.",
                "position": 3911
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x39.png",
                "caption": "Figure 45:Example tasks used for visual generalization studies when the Gemini Robotics model is adapted to the bi-arm Franka robot.",
                "position": 4112
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x40.png",
                "caption": "Figure 46:Example tasks used for action generalization studies when the Gemini Robotics model is adapted to the bi-arm Franka robot.",
                "position": 4122
            },
            {
                "img": "https://arxiv.org/html/2503.20020/x41.png",
                "caption": "Figure 47:Breakdown of generalization metrics (success rate) when the Gemini Robotics model is adapted to the bi-arm Franka robot. Similar to the progress score used inFig.28, our model significantly outperforms the diffusion baseline.",
                "position": 4128
            }
        ]
    },
    {
        "header": "Appendix DSpecializing and Adapting Gemini Robotics for Dexterity, Reasoning, and New Embodiments",
        "images": []
    }
]