[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/transformer_arch.png",
                "caption": "((a))AR Transformer",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/transformer_arch.png",
                "caption": "((a))AR Transformer",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/rnn_arch.png",
                "caption": "((b))RNN",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/diffusion_arch.png",
                "caption": "((c))Diffusion Transformer",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/ebwm_arch.png",
                "caption": "((d))EBT",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/proposed_model.png",
                "caption": "Figure 2:EBT for Autoregressive Modeling.Each blue box corresponds to a different prediction based on the current step of the thinking process, where the initial prediction starts as random. At each step, a new prediction is fed into the model, which gives an energy scalar for the prediction’s currentcompatibility(unnormalized likelihood) with the context (Facets1and1). Then, the gradient of this energy with respect to the prediction is calculated and used to update the prediction. This gradient descent update is done iteratively to refine the prediction until convergence of the predicted energy, which allows for dynamic use of computation (Facet1).",
                "position": 326
            }
        ]
    },
    {
        "header": "2Energy-Based Transformers (EBT) Intuition",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/energy_landscape_minimization.png",
                "caption": "Figure 3:Thinking Process Visualization.A learned energy landscape and its optimization through gradient descent, interpreted as a thinking process. In this example, the model predicts a distribution over text tokens, progressively shifting from an initial random distribution toward the target distribution. At each step, the EBM assigns an energy scalar indicating howcompatiblethe current prediction is with the context, visualized as the landscape’s height (Facet1). This scalar’s convergence allows the model to determine whether the prediction is adequate or if further thinking is necessary. Uncertainty (Facet1) can be represented by landscapes that are harder to optimize or by landscapes with many local minima, allowing the model to know when it requires more steps to think (Facet1). Adapted from[57].",
                "position": 348
            }
        ]
    },
    {
        "header": "3Energy-Based Transformers (EBT) Approach",
        "images": []
    },
    {
        "header": "4Experimentation and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/qualitative_image_denoising.png",
                "caption": "Figure 10:Qualitative OOD Image Denoising.EBTs achieve better denoising quality during inference while using one step for every100100100100denoising steps of a DiT. The overall image quality of EBT denoised images is less blurry than images denoised by DiT.",
                "position": 818
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Limitations and Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFuture Works and Broader Impact",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experimentation",
        "images": []
    },
    {
        "header": "Appendix CAdditional EBT Details",
        "images": []
    },
    {
        "header": "Appendix DExperimentation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/diffusion_training.png",
                "caption": "((a))Diffusion Model",
                "position": 3718
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/diffusion_training.png",
                "caption": "((a))Diffusion Model",
                "position": 3721
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/ebm_training.png",
                "caption": "((b))Energy-Based Model (EBM)",
                "position": 3726
            }
        ]
    },
    {
        "header": "Appendix FAdditional Cognitive Facets",
        "images": []
    },
    {
        "header": "Appendix GCounterarguments",
        "images": []
    },
    {
        "header": "Appendix HEnergy-Based Models (EBMs) Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/general_feed_forward_model_arch.png",
                "caption": "((a))Feed Forward Model",
                "position": 3850
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/general_feed_forward_model_arch.png",
                "caption": "((a))Feed Forward Model",
                "position": 3853
            },
            {
                "img": "https://arxiv.org/html/2507.02092/extracted/6554060/images/general_ebm_arch.png",
                "caption": "((b))Energy-Based Model",
                "position": 3858
            }
        ]
    },
    {
        "header": "Appendix IEnergy-Based Transformers (EBTs) Tutorial",
        "images": []
    }
]