[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/x1.png",
                "caption": "Figure 1:Overview of the SMMILE benchmark. In order to test the ability of MLLMs to perform multimodal in-context learning in the medical domain, we curate an expert-annotated dataset consisting of multimodal queries paired with two or more task-specific in-context examples. In contrast to prior few-shot evaluations, our in-context examples are expert-designed demonstrations of the task at hand, rather than randomly retrieved examples.",
                "position": 99
            }
        ]
    },
    {
        "header": "2SMMILE: Benchmarking Multimodal Medical In-Context Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/extracted/6573654/figures/figure-overview-v1.png",
                "caption": "Figure 2:Web interface for data collection.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2506.21355/x2.png",
                "caption": "Figure 3:Dataset characteristics. (A–D) Distribution of four key categorical annotations across the unique problems: (A) answer format, (B) rarity of the clinical case based on how often clinicians would experience the medical concepts included in each problem, (C) primary cognitive process required (where reasoning classification is defined by final problem not having direct support in its in-context example set), and (D) rated difficulty for state-of-the-art (SOTA) LLMs based on model performance. Hard difficulty is assigned when tested SOTA LLMS are unable to answer the problem, otherwise medium difficulty is assigned to the problem. (E–F) Horizontal barplots showing the breakdown of each problem by its main medical specialty (E) and by main image type used (F). (G) Histogram of the number of in-context examples provided per problem. (H) Overlaid histograms of the character-length distributions for questions versus answers. All panels are based on the 111 problems included in SMMILE.",
                "position": 170
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/x3.png",
                "caption": "Figure 4:We provide a fine-grained breakdown of MLLM performance on the SMMILE benchmark. We report performance stratified by answer format (Panel A), cognitive process necessary to obtain the answer (Panel B), number of in-context examples provided to the model (Panel C), and image type (Panel D). Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. The acronym MG refers to Mammograms.",
                "position": 670
            }
        ]
    },
    {
        "header": "4Analyzing In-Context Example Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/x4.png",
                "caption": "Figure 5:We analyze the effect of example order on MLLM performance. Here, we report performance across nine MLLMs (ordered by model size) in the open-ended setting with LLM-as-a-Judge evaluation.",
                "position": 773
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/extracted/6573654/figures/app-img1.png",
                "caption": "Figure 6:Experts are directed to the homepage interface, visualized here.",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2506.21355/extracted/6573654/figures/creation_tool.png",
                "caption": "Figure 7:The expert first selects the medical associated with the problem. Then, the expert adds or removes panels corresponding to in-context examples. The expert can also reorder panels to sort the in-context examples and final problem.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2506.21355/extracted/6573654/figures/final_submission.png",
                "caption": "Figure 8:After the expert clicks \"Submit\", they are presented with an overview of their newly created problem. The expert can then validate the problem or return to the previous screen for additional edits.",
                "position": 1366
            }
        ]
    },
    {
        "header": "Appendix BDescriptive Statistics for SMMILE++",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/x5.png",
                "caption": "Figure 9:Dataset characteristics. (A–D) Distribution of four key categorical annotations across the unique problems: (A) answer format, (B) rarity of the clinical case based on how often clinicians would experience the medical concepts included in each problem, (C) primary cognitive process required (where reasoning classification is defined by final problem not having direct support in its in-context example set), and (D) rated difficulty for state-of-the-art LLMs based on model performance. (E–F) Horizontal barplots showing the breakdown of each problem by its main medical specialty (E) and by main image type used (F). (G) Histogram of the number of in-context examples provided per problem. (H) Overlaid histograms of the character-length distributions for questions versus answers. All panels are based on the 1038 problems included in SMMILE++.",
                "position": 1377
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix DExtended Fine-Grained Analysis for SMMILE",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/x6.png",
                "caption": "Figure 10:We provide a fine-grained breakdown of MLLM performance on the SMMILE benchmark. We report performance stratified by answer format (Panel A), rarity (Panel B), cognitive process (Panel C), difficulty (Panel D), medical specialty (Panel E), and image type (Panel F). Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. The acronym MG refers to Mammograms.",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2506.21355/x7.png",
                "caption": "Figure 11:We analyze MLLM performance on the SMMILE benchmark stratified by number of in-context examples provided to the model. Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach.",
                "position": 1409
            }
        ]
    },
    {
        "header": "Appendix EExtended Fine-Grained Analysis for SMMILE++",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21355/x8.png",
                "caption": "Figure 12:We provide a fine-grained breakdown of MLLM performance on the SMMILE++ benchmark. We report performance stratified by answer format (Panel A), rarity (Panel B), cognitive process (Panel C), difficulty (Panel D), medical specialty (Panel E), and image type (Panel F). Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. The acronym MG refers to Mammograms.",
                "position": 1419
            },
            {
                "img": "https://arxiv.org/html/2506.21355/x9.png",
                "caption": "Figure 13:We analyze MLLM performance on the SMMILE++ benchmark stratified by number of in-context examples provided to the model. Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach.",
                "position": 1422
            }
        ]
    },
    {
        "header": "Appendix FLicensing Considerations",
        "images": []
    }
]