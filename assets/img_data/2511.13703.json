[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Main",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x1.png",
                "caption": "Figure 1:Overview. (a) We mix unlabeled EHR notes and web texts as our pretrain corpus. (b) We pretrain using next token prediction. (c) Instruction finetuning in multiple choice format enables cross-task transfer. (d) We compare Lang1 to off-the-shelf generalist models. (e) In order to derive design principles, we do ablations on data mix, model scale, pretrain trajectories, data scale, eval task type, eval hospital, and eval time.",
                "position": 381
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x2.png",
                "caption": "(a)Both generalists and specialists underperform zero-shot. Yellow triangles indicate the best zero-shot performance per task. While the best mortality prediction AUROC is 94.2%, performance of other tasks (readmission, insurance denial, LOS, CCI) range from 36.6%–71.7% AUROC.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x2.png",
                "caption": "(a)Both generalists and specialists underperform zero-shot. Yellow triangles indicate the best zero-shot performance per task. While the best mortality prediction AUROC is 94.2%, performance of other tasks (readmission, insurance denial, LOS, CCI) range from 36.6%–71.7% AUROC.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x3.png",
                "caption": "(b)FinetunedLang1-1B(purple) outperform best zero-shot performance (magenta) by 1.66% to 23.66% AUROC and finetunedLlama 3.2 1B(light blue) and LoRA finetunedLlama 70B(deep blue) by 3.64% to 6.75% AUROC. Yellow stars indicate the best finetuned performance per task.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x4.png",
                "caption": "(a)Reading comprehension performance increases from pretraining.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x4.png",
                "caption": "(a)Reading comprehension performance increases from pretraining.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x5.png",
                "caption": "(b)Clinical classification performance does not rapidly emerge from pretraining.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x6.png",
                "caption": "(a)Finetuning is more token-efficient for performance gains, but pretraining still provides value. At any fixed token budget (a vertical slice on thexaxis), using more finetuning tokens (darker colors) yields a higher ROC AUC forLang1-1B’s checkpoint trajectory. Nonetheless, a clear gap remains between fully finetuning without pretraining (purple diamond) and the fully pretrained model (yellow star).",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x6.png",
                "caption": "(a)Finetuning is more token-efficient for performance gains, but pretraining still provides value. At any fixed token budget (a vertical slice on thexaxis), using more finetuning tokens (darker colors) yields a higher ROC AUC forLang1-1B’s checkpoint trajectory. Nonetheless, a clear gap remains between fully finetuning without pretraining (purple diamond) and the fully pretrained model (yellow star).",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x7.png",
                "caption": "(b)Clinically pretrained models (purpleLang1-1b), when finetuned, outperform generalist models of similar size (blueLlamamodels), especially in low data regime (cross). Arrows track the same pretrained model as it is finetuned on different numbers of examples.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x8.png",
                "caption": "(c)Lower model perplexity onReMedEanswer-question pairs is associated with better zero-shot (cross) and finetuned (circle) performance.Lang1(purple) has lower perplexity despite fewer total pretrain tokens (though more clinical tokens). Arrows track the same model’s performance.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x9.png",
                "caption": "(a)FinetuningLang1-1Bcan often transfer across tasks. The heatmap shows the finetuned model’s performance when finetuned on a subset ofReMedEtasks (yyaxis) and evaluated on all fiveReMedEtasks (xxaxis).",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x9.png",
                "caption": "(a)FinetuningLang1-1Bcan often transfer across tasks. The heatmap shows the finetuned model’s performance when finetuned on a subset ofReMedEtasks (yyaxis) and evaluated on all fiveReMedEtasks (xxaxis).",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x10.png",
                "caption": "(b)Finetuning on NYU Readmission (purple) transfers well (darker yellow) to MIMIC III Readmission (green). Overall performance is better on finetuningLang1-1B(purple) compared toLlama 3.2 1B(blue).",
                "position": 485
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Conclusions",
        "images": []
    },
    {
        "header": "5Methods",
        "images": []
    },
    {
        "header": "Declarations",
        "images": []
    },
    {
        "header": "Appendix AData Timeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x11.png",
                "caption": "Figure 6:Illustration of timeline of pretrain and finetune dataset for bothLang1andNYUTron.Lang1covers a wider time window for pretraining and added additional temporal test set in 2024 to capture temporal distribution shift.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x12.png",
                "caption": "Figure 7:Models perform worse on temporal test and exhibit different level of degradation in face of temporal shift.",
                "position": 904
            }
        ]
    },
    {
        "header": "Appendix BReMedE Dataset Statistics",
        "images": []
    },
    {
        "header": "Appendix CTransfer Pattern ofLlama 3.2 1bv.s.Lang1 1B",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x13.png",
                "caption": "(a)Lang1-1B’s transfer heatmap.",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x13.png",
                "caption": "(a)Lang1-1B’s transfer heatmap.",
                "position": 1277
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x14.png",
                "caption": "(b)Llama 3.2 1B’s transfer heatmap.",
                "position": 1282
            }
        ]
    },
    {
        "header": "Appendix DPretraining ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x15.png",
                "caption": "(a)Readmission prediction",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x15.png",
                "caption": "(a)Readmission prediction",
                "position": 1327
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x16.png",
                "caption": "(b)Insurance denial prediction",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x17.png",
                "caption": "(c)LOS prediction",
                "position": 1338
            }
        ]
    },
    {
        "header": "Appendix ECalibration Plot",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/figures/calibration/calibration_curve_single_task.png",
                "caption": "(a)Calibration plots for single-task models",
                "position": 1354
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/calibration/calibration_curve_single_task.png",
                "caption": "(a)Calibration plots for single-task models",
                "position": 1357
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/calibration/calibration_curve_joint.png",
                "caption": "(b)Calibration plots for joint model",
                "position": 1363
            }
        ]
    },
    {
        "header": "Appendix FAsymmetry of Transfer between Mortality and LOS",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x18.png",
                "caption": "(a)Probabilities of LOS given in-hospital mortality",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x18.png",
                "caption": "(a)Probabilities of LOS given in-hospital mortality",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x19.png",
                "caption": "(b)Probabilities of in-hospital mortality given LOS",
                "position": 1389
            }
        ]
    },
    {
        "header": "Appendix GDetailed statistics of NYU Notes+",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/figures/dataset_stat/pathology_pie_charts.png",
                "caption": "(a)Pathology Notes.Among pathology notes with specified departments, OB/GYN (obstetrics and gynecology) has the highest percentage (9.8%). The most common specified diagnosis is gynecological exams (6%). Nearly half (48.6%) of the pathology notes are from Manhattan borough.",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/dataset_stat/pathology_pie_charts.png",
                "caption": "(a)Pathology Notes.Among pathology notes with specified departments, OB/GYN (obstetrics and gynecology) has the highest percentage (9.8%). The most common specified diagnosis is gynecological exams (6%). Nearly half (48.6%) of the pathology notes are from Manhattan borough.",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/dataset_stat/radiology_pie_charts.png",
                "caption": "(b)Radiology Notes.Most of the radiology notes are from diagnostic radiology (82.2%), with screening mammograms being the most common specified diagnosis (5%). The two most common boroughs are Manhattan (41%) and Queens (32.6%).",
                "position": 1413
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/dataset_stat/nyutron_pie_charts.png",
                "caption": "(c)Hospital Notes.Among notes with specified departments, most are from internal medicine department (10.1%), with common diagnoses including sepsis (1.1%) and hypertension (1%). The majority of notes are from Brooklyn (27.9%) and Queens (24.7%) borough.",
                "position": 1419
            }
        ]
    },
    {
        "header": "Appendix HPrompts of ReMedE Tasks",
        "images": []
    },
    {
        "header": "Appendix ILoRa Finetuning for Llama-3-70b",
        "images": []
    },
    {
        "header": "Appendix JToken probability approximation for models without logprobs",
        "images": []
    },
    {
        "header": "Appendix KStratified Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/Age.png",
                "caption": "(a)Age.",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/Age.png",
                "caption": "(a)Age.",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/FirstRace.png",
                "caption": "(b)First Race.",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/Borough.png",
                "caption": "(c)Borough.",
                "position": 1514
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/Ethnicity.png",
                "caption": "(d)Ethnicity.",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/Sex.png",
                "caption": "(e)Sex.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2511.13703/figures/stratification/isChild.png",
                "caption": "(f)IsChild.",
                "position": 1530
            }
        ]
    },
    {
        "header": "Appendix LControl for Patient Overlap",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13703/x20.png",
                "caption": "(a)Readmission",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x20.png",
                "caption": "(a)Readmission",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x21.png",
                "caption": "(b)Mortality",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x22.png",
                "caption": "(c)LOS",
                "position": 1561
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x23.png",
                "caption": "(a)Readmission prediction.",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x23.png",
                "caption": "(a)Readmission prediction.",
                "position": 1580
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x24.png",
                "caption": "(b)In-hospital Mortality.",
                "position": 1586
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x25.png",
                "caption": "(c)LOS.",
                "position": 1591
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x26.png",
                "caption": "(a)Finetuning on full NYU dataset leads to best performance onLlama-3.2-1B, even though NYU Readmission is not directly in-distribution for MIMIC Readmission.",
                "position": 1619
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x26.png",
                "caption": "(a)Finetuning on full NYU dataset leads to best performance onLlama-3.2-1B, even though NYU Readmission is not directly in-distribution for MIMIC Readmission.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2511.13703/x27.png",
                "caption": "(b)Finetuning on MIMIC III leads to best performance forLang1-1B, even though NYU Readmission has more labeled pairs.",
                "position": 1627
            }
        ]
    },
    {
        "header": "Appendix MExternal Validation: MIMIC v.s. NYU",
        "images": []
    }
]