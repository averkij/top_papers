[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure1-1.jpg",
                "caption": "Figure 1:Conceptual visualization of dominant modality bias. The key modality differs by task:(a)For the hate recognition task, text descriptions of memes lead, while(b)for the food classification task, food images play a crucial role in prediction.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure2.jpg",
                "caption": "Figure 2:Experimental results on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets in the presence of dominant modality bias.(a)Performance visualization under different missing conditions (full, image only (missing text), text only (missing image)) for each dataset.(b)Illustration of learning curves for each modality across datasets.",
                "position": 181
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure3.jpg",
                "caption": "Figure 3:(a)The overall training framework of our proposedBalGrad. The final classifierf𝒯⁢(⋅)subscript𝑓𝒯⋅f_{\\mathcal{T}}(\\cdot)italic_f start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( ⋅ )is updated with the gradientg𝒯⟂subscriptsuperscript𝑔perpendicular-to𝒯g^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPTfor cross entropy (CE) loss. The image and text embedding layershv⁢(⋅),hl⁢(⋅)subscriptℎ𝑣⋅subscriptℎ𝑙⋅h_{v}(\\cdot),h_{l}(\\cdot)italic_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( ⋅ ) , italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( ⋅ )are also updated withg𝒯⟂subscriptsuperscript𝑔perpendicular-to𝒯g^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPTalong with the gradients of the CE loss for each modalityg𝒯v,g𝒯lsubscriptsuperscript𝑔𝑣𝒯subscriptsuperscript𝑔𝑙𝒯g^{v}_{\\mathcal{T}},g^{l}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT , italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, and the gradients of the KL divergence between the two modalities’ predictionsgk⁢lv,gk⁢llsubscriptsuperscript𝑔𝑣𝑘𝑙subscriptsuperscript𝑔𝑙𝑘𝑙g^{v}_{kl},g^{l}_{kl}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT , italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT.(b)Inter-modality gradient reweighting adjusts the magnitudes ofgk⁢lvsubscriptsuperscript𝑔𝑣𝑘𝑙g^{v}_{kl}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPTandgk⁢llsubscriptsuperscript𝑔𝑙𝑘𝑙g^{l}_{kl}italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPTto obtaingk⁢lsubscript𝑔𝑘𝑙g_{kl}italic_g start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT. If a conflict occurs, we projectg𝒯⟂subscriptsuperscript𝑔perpendicular-to𝒯g^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT ⟂ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPTon the orthogonal direction ofgk⁢lsubscript𝑔𝑘𝑙g_{kl}italic_g start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPTby inter-task gradient projection.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure4.jpg",
                "caption": "Figure 4:Evaluation on robustness to different missing ratior𝑟ritalic_rofBalGradand existing methods on UPMC Food-101, Hateful Memes, and MM-IMDb datasets.",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/BLIP_gap.png",
                "caption": "Figure 5:Bar plots comparing the performance of existing methods andBalGradusing BLIP. Each bar representsΔGapsubscriptΔGap\\Delta_{\\textit{Gap}}roman_Δ start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), defined as the performance difference between missing image and missing text conditions.",
                "position": 886
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure5.png",
                "caption": "Figure 6:Training iteration loss curves for image and text modalities on the UPMC Food-101 and Hateful Memes datasets, comparing the effects of the existence of inter-modality gradient reweighting.",
                "position": 889
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure6.png",
                "caption": "Figure 7:Histogram visualization of the frequency of gradient conflicts between image and text gradients during training iterations on the UPMC Food-101 and Hateful Memes datasets.μw/osubscript𝜇𝑤𝑜\\mu_{w/o}italic_μ start_POSTSUBSCRIPT italic_w / italic_o end_POSTSUBSCRIPTandμw⁣/subscript𝜇𝑤\\mu_{w/}italic_μ start_POSTSUBSCRIPT italic_w / end_POSTSUBSCRIPTrepresent the average cosine similarity values w/o and w/ projection, respectively.",
                "position": 902
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix of Propositions",
        "images": []
    },
    {
        "header": "Appendix BFurther Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/ap_fusion_mechanism_2.jpg",
                "caption": "Figure 8:Bar plots illustrating the performance of existing methods andBalGradwith different fusion mechanisms:(a)addition and(b)attention, evaluated on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar indicatesΔGapsubscriptΔGap\\Delta_{\\textit{Gap}}roman_Δ start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), which quantifies the performance variation between missing image and missing text conditions.",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/ap_backbone_models.jpg",
                "caption": "Figure 9:Bar plots presenting the performance comparison between existing methods andBalGradacross different backbone models:(a)ResNet and DistilBERT, and(b)CLIP, on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar representsΔGapsubscriptΔGap\\Delta_{\\textit{Gap}}roman_Δ start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), measuring the performance discrepancy under missing image and missing text conditions.",
                "position": 1920
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": []
    }
]