[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure1-1.jpg",
                "caption": "Figure 1:Conceptual visualization of dominant modality bias. The key modality differs by task:(a)For the hate recognition task, text descriptions of memes lead, while(b)for the food classification task, food images play a crucial role in prediction.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure2.jpg",
                "caption": "Figure 2:Experimental results on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets in the presence of dominant modality bias.(a)Performance visualization under different missing conditions (full, image only (missing text), text only (missing image)) for each dataset.(b)Illustration of learning curves for each modality across datasets.",
                "position": 181
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure3.jpg",
                "caption": "Figure 3:(a)The overall training framework of our proposedBalGrad. The final classifierfğ’¯â¢(â‹…)subscriptğ‘“ğ’¯â‹…f_{\\mathcal{T}}(\\cdot)italic_f start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT ( â‹… )is updated with the gradientgğ’¯âŸ‚subscriptsuperscriptğ‘”perpendicular-toğ’¯g^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT âŸ‚ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPTfor cross entropy (CE) loss. The image and text embedding layershvâ¢(â‹…),hlâ¢(â‹…)subscriptâ„ğ‘£â‹…subscriptâ„ğ‘™â‹…h_{v}(\\cdot),h_{l}(\\cdot)italic_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( â‹… ) , italic_h start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( â‹… )are also updated withgğ’¯âŸ‚subscriptsuperscriptğ‘”perpendicular-toğ’¯g^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT âŸ‚ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPTalong with the gradients of the CE loss for each modalitygğ’¯v,gğ’¯lsubscriptsuperscriptğ‘”ğ‘£ğ’¯subscriptsuperscriptğ‘”ğ‘™ğ’¯g^{v}_{\\mathcal{T}},g^{l}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT , italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT, and the gradients of the KL divergence between the two modalitiesâ€™ predictionsgkâ¢lv,gkâ¢llsubscriptsuperscriptğ‘”ğ‘£ğ‘˜ğ‘™subscriptsuperscriptğ‘”ğ‘™ğ‘˜ğ‘™g^{v}_{kl},g^{l}_{kl}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT , italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT.(b)Inter-modality gradient reweighting adjusts the magnitudes ofgkâ¢lvsubscriptsuperscriptğ‘”ğ‘£ğ‘˜ğ‘™g^{v}_{kl}italic_g start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPTandgkâ¢llsubscriptsuperscriptğ‘”ğ‘™ğ‘˜ğ‘™g^{l}_{kl}italic_g start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPTto obtaingkâ¢lsubscriptğ‘”ğ‘˜ğ‘™g_{kl}italic_g start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPT. If a conflict occurs, we projectgğ’¯âŸ‚subscriptsuperscriptğ‘”perpendicular-toğ’¯g^{\\perp}_{\\mathcal{T}}italic_g start_POSTSUPERSCRIPT âŸ‚ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPTon the orthogonal direction ofgkâ¢lsubscriptğ‘”ğ‘˜ğ‘™g_{kl}italic_g start_POSTSUBSCRIPT italic_k italic_l end_POSTSUBSCRIPTby inter-task gradient projection.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure4.jpg",
                "caption": "Figure 4:Evaluation on robustness to different missing ratiorğ‘Ÿritalic_rofBalGradand existing methods on UPMC Food-101, Hateful Memes, and MM-IMDb datasets.",
                "position": 679
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/BLIP_gap.png",
                "caption": "Figure 5:Bar plots comparing the performance of existing methods andBalGradusing BLIP. Each bar representsÎ”GapsubscriptÎ”Gap\\Delta_{\\textit{Gap}}roman_Î” start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), defined as the performance difference between missing image and missing text conditions.",
                "position": 886
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure5.png",
                "caption": "Figure 6:Training iteration loss curves for image and text modalities on the UPMC Food-101 and Hateful Memes datasets, comparing the effects of the existence of inter-modality gradient reweighting.",
                "position": 889
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/figure6.png",
                "caption": "Figure 7:Histogram visualization of the frequency of gradient conflicts between image and text gradients during training iterations on the UPMC Food-101 and Hateful Memes datasets.Î¼w/osubscriptğœ‡ğ‘¤ğ‘œ\\mu_{w/o}italic_Î¼ start_POSTSUBSCRIPT italic_w / italic_o end_POSTSUBSCRIPTandÎ¼wâ£/subscriptğœ‡ğ‘¤\\mu_{w/}italic_Î¼ start_POSTSUBSCRIPT italic_w / end_POSTSUBSCRIPTrepresent the average cosine similarity values w/o and w/ projection, respectively.",
                "position": 902
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix of Propositions",
        "images": []
    },
    {
        "header": "Appendix BFurther Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/ap_fusion_mechanism_2.jpg",
                "caption": "Figure 8:Bar plots illustrating the performance of existing methods andBalGradwith different fusion mechanisms:(a)addition and(b)attention, evaluated on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar indicatesÎ”GapsubscriptÎ”Gap\\Delta_{\\textit{Gap}}roman_Î” start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), which quantifies the performance variation between missing image and missing text conditions.",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2503.13834/extracted/6288576/fig/ap_backbone_models.jpg",
                "caption": "Figure 9:Bar plots presenting the performance comparison between existing methods andBalGradacross different backbone models:(a)ResNet and DistilBERT, and(b)CLIP, on the UPMC Food-101, Hateful Memes, and MM-IMDb datasets. Each bar representsÎ”GapsubscriptÎ”Gap\\Delta_{\\textit{Gap}}roman_Î” start_POSTSUBSCRIPT Gap end_POSTSUBSCRIPT(%), measuring the performance discrepancy under missing image and missing text conditions.",
                "position": 1920
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": []
    }
]