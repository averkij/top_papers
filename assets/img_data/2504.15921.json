[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15921/x1.png",
                "caption": "(a)Verb frequency: Segments vs Summaries. Illustration of the semantic gap between segment descriptions and video summaries in Ego4D-HCap(Islam et al.,2024)dataset.",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2504.15921/x1.png",
                "caption": "(a)Verb frequency: Segments vs Summaries. Illustration of the semantic gap between segment descriptions and video summaries in Ego4D-HCap(Islam et al.,2024)dataset.",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2504.15921/x2.png",
                "caption": "(b)t-SNE visualization between Ego4D-HCap and Youcook2 datasets.Illustration of the distributional shift between these two dataset.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2504.15921/x3.png",
                "caption": "Figure 2.Motivation of our VisMaP. Most existing video summarisation models focus on minute-level short-form videos, while hour-long videos, which are more common in real-world scenarios, are often overlooked due to their length, content complexity, and the prohibitively high cost of manual annotation. We propose a cross-domain unsupervised approach for hour-long video summarisation. It leverages the inductive power of multiple LLMs to generate high-quality pseudo-summaries from short video segments via meta-prompting. These pseudo-summaries are then used to train a model, enabling effective summarisation of long videos without costly human annotations.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2504.15921/x4.png",
                "caption": "Figure 3.An overview of our VisMaP.(a) First stage:we use\n180-second source videovssuperscript𝑣𝑠v^{s}italic_v start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPTfor supervised pretraining to establish basic summary capabilities.(b) Second stage:we split hour-long target videosvtsuperscript𝑣𝑡v^{t}italic_v start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTinto 3-minute segments set𝐕itsuperscriptsubscript𝐕𝑖𝑡\\mathbf{V}_{i}^{t}bold_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTand process them through the first-stage summary model to generate pseudo captionsC^tsuperscript^𝐶𝑡\\widehat{C}^{t}over^ start_ARG italic_C end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT.C^tsuperscript^𝐶𝑡\\widehat{C}^{t}over^ start_ARG italic_C end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTare then refined through a meta-prompting process withK𝐾Kitalic_Kiterations, using Gemini as the evaluator and GPT-3.5\nas the optimiser and the generator, to create more tailored promptsP⁢rt𝑃superscript𝑟𝑡{Pr}^{t}italic_P italic_r start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTand summariesY^tsuperscript^𝑌𝑡\\widehat{Y}^{t}over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT.(c) Third stage:RefinedY^tsuperscript^𝑌𝑡\\widehat{Y}^{t}over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTpseudo-summaries are utilised to fine-tune the summary model for effective hour-long video summary.",
                "position": 138
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.Methodology",
        "images": []
    },
    {
        "header": "4.Theoretical Analysis",
        "images": []
    },
    {
        "header": "5.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15921/x5.png",
                "caption": "Figure 4.An example of summaries from ViSMaP on the Ego4D-HCap dataset.",
                "position": 1138
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7.Code Release",
        "images": []
    },
    {
        "header": "8.More Implement Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15921/x6.png",
                "caption": "Figure 5.Qualitative Results on Ego4D dataset.",
                "position": 2117
            }
        ]
    },
    {
        "header": "9.Theoretical Details",
        "images": []
    },
    {
        "header": "10.Evaluating LLMs in meta-prompting.",
        "images": []
    },
    {
        "header": "11.Limitations",
        "images": []
    }
]