[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05940/x1.png",
                "caption": "Figure 1:The Framework of TRIT.Our framework consists of two stages: Cross-Lingual Reasoning filters questions by\naccuracy thresholdθ\\theta, and Translation-Reasoning Integration & Feedback trains both\ntranslation and target-language reasoning using filtered questions (Translation errors are denoted with red color, which results in wrong reasoning results, and get 0 asrtransr_{\\text{trans}}).",
                "position": 214
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05940/x2.png",
                "caption": "Figure 2:Evolution of translation quality. (a) In-domain evaluation on MATH500 (Win/Tie/Lose rates vs. Base). (b) Cross-domain generalization on Flores200 (Comet Scores).",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2602.05940/x3.png",
                "caption": "Figure 3:Cross-lingual question alignment across model layers (DeepSeek-Distill-Qwen-1.5B).Layer-wise cosine similarity between English and target-language question representations\nfor TRIT and External-Translation (ET, without translation training).",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2602.05940/x4.png",
                "caption": "Figure 4:Impact of Stage 1 Filtering Threshold (θ\\theta) on Final Multilingual Reasoning Performance",
                "position": 1360
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Repetition Analysis",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAlignment Analysis of Translation Quality and Reasoning Accuracy",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05940/x5.png",
                "caption": "Figure 5:Translation quality correlates with reasoning accuracy.Distribution of translation quality (Win/Lose/Tie judged by DeepSeek-V3.2) for\nquestion pairs with (a) moderate accuracy differences (Δ​Acc>0.2\\Delta\\text{Acc}>0.2)\nand (b) critical failures (Acc = 0 vs. Acc > 0). Better translations consistently\ncorrespond to higher reasoning accuracy.",
                "position": 1859
            }
        ]
    },
    {
        "header": "Appendix DWhy M-Thinker Failed",
        "images": []
    },
    {
        "header": "Appendix ENoise Analysis of Deferred Reasoning Feedback",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05940/x6.png",
                "caption": "Figure 7:Cross-lingual question alignment for Qwen3-4B.Similar to DeepSeek-Distill-Qwen-1.5B (Figure3), TRIT achieves\nhigher alignment than External-Translation (ET), particularly in later layers.",
                "position": 2009
            },
            {
                "img": "https://arxiv.org/html/2602.05940/figures/language-instruction.png",
                "caption": "Figure 8:Multilingual reasoning instructions.We use language-specific prompts to instruct the model to reason step-by-step in\nthe question language and place the final answer within\\\\boxed{}.\nAll prompts are semantically equivalent translations requesting step-by-step reasoning\nand formatted output.",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2602.05940/figures/language-prefix-and-control.png",
                "caption": "Figure 9:Two language control strategies.Left: Language prefixes (e.g.,<think>\\nOkay) prepended to\nthe input to guide the model to respond in the corresponding language. We use it in data construction.\nRight: Explicit language instruction prompts that directly instruct the model to\nthink and answer in the target language. We use it in Prompt Control baseline.",
                "position": 2025
            }
        ]
    },
    {
        "header": "Appendix FAdditional Figures",
        "images": []
    }
]