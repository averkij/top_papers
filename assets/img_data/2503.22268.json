[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22268/x1.png",
                "caption": "Figure 1:Our method is capable of handling challenging scenarios, including articulated structures, shadow reflections, dynamic background motion, and drastic camera movements, while producing per object level fine-grained moving object masks.",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2503.22268/x2.png",
                "caption": "Figure 2:The effectiveness of long-range tracks. Over longer periods of time, if a moving object experiences factors such as occlusion or changes in lighting, it can negatively affect the tracking performance of optical-flow-based methods for that object.",
                "position": 101
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22268/x3.png",
                "caption": "Figure 3:Overview of Our Pipeline.We take 2D tracks and depth maps generated by off-the-shelf models[15,66]as input, which are then processed by a motion encoder to capture motion patterns, producing featured tracks. Next, we use tracks decoder that integrates DINO feature[45]to decode the featured tracks by decoupling motion and semantic information and ultimately obtain the dynamic trajectories(a). Finally, using SAM2[51], we group dynamic tracks belonging to the same object and generate fine-grained moving object masks(b).",
                "position": 143
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22268/x4.png",
                "caption": "Figure 4:Qualitative comparison on DAVIS17-moving benchmarks.\nFor each sequence we show moving object mask results.\nOur method successfully handles water reflections (left), camouflage appearances (middle), and drastic camera motion (right).",
                "position": 245
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22268/x5.png",
                "caption": "Figure 5:Qualitative comparison on FBMS-59 benchmarks. The masks produced by us are geometrically more complete and detailed.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2503.22268/x6.png",
                "caption": "Figure 6:Qualitative comparison on SegTrack v2 benchmarks. Our method succeeds even under motion blur conditions.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2503.22268/x7.png",
                "caption": "Figure 7:Qualitative comparison on Fine-grained MOS task which will produce per-object level masks.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2503.22268/x8.png",
                "caption": "Figure 8:Visual comparison for the ablation study on two critical and challenging cases. The top sequence shows scenarios involves drastic camera motion and complex motion patterns, while the bottom sequence with both static and dynamic objects of the same category. The experimental setup is detailed in Sec.4.5.",
                "position": 540
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]