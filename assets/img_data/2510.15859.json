[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15859/x1.png",
                "caption": "Figure 1:The overall pipeline of ORBIT, a useful method to align LLMs on open-ended complex tasks via rubrics-based incremental training.\n(a) Sampling medical consultation cases, truncating dialogues to form realistic queries, and generating distilled supervision data for SFT training.\n(b) During the RL phase, we adopt the GRPO framework, where a dynamic, query-specific rubric is generated for each medical case.\nEach response is evaluated by an external Judge model based on this rubric, and the resulting score serves as the reward signal for policy optimization.\n(c) For each medical consultation case, a Retrieval-Augmented Generation (RAG) framework constructs a tailored, multi-dimensional rubric.\nThe retrieval module references HealthBench rubrics as exemplars, while the generation model is guided to avoid direct duplication.\nThese rubrics are used solely by the external Judge model for evaluation, ensuring that the agent remains unbiased and enabling stable policy exploration.\nThe RAG framework is also extensible, supporting integration of larger and more diverse medical consultation datasets.",
                "position": 205
            }
        ]
    },
    {
        "header": "3ORBIT",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15859/x2.png",
                "caption": "Figure 2:Multi-dimensional performance comparison of ORBIT models.This figure presents a comprehensive performance analysis categorized byThemeandAxis.\nThe results are divided into two groups:(a, c)Internal methodological comparisons among the base instructor-tuned model (Qwen3-4B-Instruct), the supervised fine-tuning baseline (Qwen3-4B-SFT), and our proposed models (Qwen3-4B-ORBIT and SFT-4B-ORBIT).(b, d)Large-scale benchmark comparisons between our best-performing model, SFT-4B-ORBIT, and significantly larger proprietary and open-source models, including Qwen3-30B-Instruct, GPT-4.1, Baichuan-M2-32B, and Qwen3-30B-Thinking.\nFor brevity, “Health Data T.” refers to “Health Data Tasks,” and “Emerg. Referrals” denotes “Emergency Referrals.”",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2510.15859/x3.png",
                "caption": "Figure 3:Training dynamics under different pass@k filtering strategies.\nBoth rubric- and sample-based filtering notably enhance training efficiency, while adjustable thresholds enable fine-grained control over the trade-off between computational cost and final performance.",
                "position": 1629
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARubrics Generator",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15859/x4.png",
                "caption": "Figure 4:Overall details of the designed system prompt for the rubrics geneation pipeline.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2510.15859/x5.png",
                "caption": "Figure 5:Overall details of the designed rubrics generation prompt for the input medical case.\nRetrieved reference cases from the RAG system are used as in-context examples to guide a rubric-generation model in producing multi-dimensional scoring rubrics with both positive and negative criteria.",
                "position": 2302
            }
        ]
    },
    {
        "header": "Appendix BThe Selection of Evaluation Model",
        "images": []
    },
    {
        "header": "Appendix CThe Evaluation of Some Medical Models",
        "images": []
    },
    {
        "header": "Appendix DDetails of Some Parameters Settings in Training and Evaluation Processes",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15859/x6.png",
                "caption": "Figure 6:The case study for Qwen3-4B-Instruct and our Qwen3-4B-ORBIT for the input medicine consultation dialogue problem.",
                "position": 3006
            }
        ]
    },
    {
        "header": "Appendix ECase Study",
        "images": []
    }
]