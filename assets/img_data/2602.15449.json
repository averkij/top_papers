[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15449/x1.png",
                "caption": "Figure 1:Overview of TAROT framework.(top) Build a four-tier test suite (basic/intermediate/complex/edge) per problem using frontier LLMs and verify them against the reference solution.\n(bottom) Reinforcement fine-tuning under a capability-conditioned, reward-decoupled curriculum. Less capable models perform best with basic→\\rightarrowcomplex, whereas more capable models perform best with complex→\\rightarrowbasic.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15449/x2.png",
                "caption": "Figure 2:Quantitative and qualitative validation of the TAROT dataset. The KDE plots show the distribution of structural complexity, where the x-axis represents the metric’s magnitude. Token Diversity (unique/total tokens) and Transitions (character class changes) serve as proxies for lexical and syntactic complexity, respectively. The systematic rightward shift confirms increasing difficulty across tiers. GPT-4o validation on the right confirms that complex tiers target algorithmic complexity, while edge tiers focus on boundary conditions.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2602.15449/x3.png",
                "caption": "Figure 3:Experimental results for Qwen2.5-Instruct and Qwen2.5-Coder-Instruct on HumanEval, HumanEval+, MBPP, and MBPP+. Scores are pass@1. Numbers above bars indicate gains in percentage points relative to each model’s base checkpoint. Labels inside bars indicate the best performing curriculum strategy.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2602.15449/x4.png",
                "caption": "Figure 4:Experimental results for Qwen2.5-Instruct and Qwen2.5-Coder-Instruct models on CodeForces, LiveCodeBench v5 (LCBv5), and CruxEval. Scores are the overall accuracy across easy, medium, and hard problems. Numbers above bars indicate gains in percentage points relative to each model’s base checkpoint. Labels inside bars indicate the best performing curriculum strategy.",
                "position": 465
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATest Case Generation Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15449/x5.png",
                "caption": "Table 5:The prompt template used to generate a tiered test suite given a coding problem. The problem statement and the default test case from the original source are injected into{problem_statement}and{baseline_test_case}placeholders, respectively.",
                "position": 1245
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CGeneration and Execution Environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15449/x5.png",
                "caption": "Figure 5:Performance sensitivity to the GRPO hyperparameterβ\\beta. The plots show the final pass@1 or accuracy scores on various benchmarks asβ\\betais varied. The optimal value is task-dependent; for instance, HumanEval and HumanEval+ benefit from a smallerβ\\beta(0.01) that allows greater policy exploration, whereas MBPP and CodeForces achieve peak performance with a largerβ\\beta(0.05) that enforces stronger regularization.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2602.15449/x6.png",
                "caption": "Figure 6:Performance sensitivity to the sampling temperature during training. The plots illustrate the final benchmark scores for different training temperatures. A higher temperature of 1.0, which encourages greater exploration, is optimal for benchmarks like HumanEval and HumanEval+. In contrast, other benchmarks such as MBPP show a preference for a more moderate temperature of 0.7, highlighting that the ideal exploration-exploitation balance is task-specific.",
                "position": 1438
            }
        ]
    },
    {
        "header": "Appendix DHyperparameter Sensitivity Analysis",
        "images": []
    },
    {
        "header": "Appendix EImpact of Maximum Completion Tokens at Inference Time",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15449/x7.png",
                "caption": "Figure 7:Performance sensitivity to the maximum completion token limit at inference time for Qwen3-4B-Instruct-2507 fine-tuned on various curriculum strategies. The results reveal a clear, benchmark-dependent dichotomy. For function-completion tasks like HumanEval and HumanEval+, performance tends to degrade as the token limit increases beyond 4,096, suggesting that a larger generation space may encourage verbose, error-prone solutions. Conversely, for benchmarks like MBPP and MBPP+, a larger token limit is generally beneficial, indicating that their problem structures may require more extensive code to solve correctly.",
                "position": 1463
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results on Gemma2-2B-IT",
        "images": []
    },
    {
        "header": "Appendix GFull Benchmark Tables (Qwen2.5 & Qwen3-4B)",
        "images": []
    },
    {
        "header": "Appendix HTraining Dynamics Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.15449/x8.png",
                "caption": "Figure 8:Training dynamics vs. downstream performance. (a) and (b) show the reward and the mean completion length curves during reinforcement fine-tuning, and the annotations mark the curriculum strategy with the best average downstream performance. (c) and (d) show the Pearson correlation coefficientrrof the final rewards vs. benchmark scores and the mean completion length vs. benchmark scores, respectively. Light, semi-transparent lines represent alternative curriculum strategies, while the solid, annotated lines correspond to the best-performing strategy for each model. Some trajectories terminate earlier than others because different model sizes utilize varying batch sizes and gradient accumulation steps under a fixed total compute budget.",
                "position": 2337
            }
        ]
    },
    {
        "header": "Appendix IThe Use of AI Assistants",
        "images": []
    },
    {
        "header": "Appendix JSample Tiered Test Cases",
        "images": []
    }
]