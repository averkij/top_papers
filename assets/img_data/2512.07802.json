[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07802/x1.png",
                "caption": "Figure 1:Coherent multi-shot generations withOneStory.Each example shows 10-shots of a minute-long video.OneStoryhandles both image-to-multi-shot (top) and text-to-multi-shot (middle) generation within the same model, and generalizes well to out-of-domain scenes (bottom). It maintains consistent characters and environments while faithfully following complex and evolving prompts to produce coherent long-form narratives. A representative segment of each prompt is given with the corresponding shot.We recommend referring to ourProject Pagefor better visualization.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3High-quality Multi-shot Video Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07802/x2.png",
                "caption": "Figure 2:Multi-shot video data curation pipeline.From raw videos, we obtain high-quality multi-shot sequences via three steps:\n(i)Shot detection,\n(ii)Two-stage captioning, and\n(iii)Quality filtering.\nIn the second stage, each shot is first captioned independently and thenrewritten into referential formbased on preceding shots.\nUnlike prior datasets, no global captions are used, and only shot-level captions with progressive narrative flow are retained to ensure flexibility, while reflecting real-world storytelling.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07802/x3.png",
                "caption": "Figure 3:Overview of the proposedOneStory.Our model reframes multi-shot video generation (MSV) as anext-shot generationtask.\n(a) During training, the model learns to generate the final shot conditioned on the preceding two; when only two shots are available, we inflate with a synthetic shot to enable unified three-shot training.\n(b) At inference, it maintains a memory bank of past shots and generates multi-shot videos autoregressively.\nThe model is comprised of two key components: (c) aFrame Selectionmodule that selects semantically-relevant frames from preceding shots to construct a global context, and (d) anAdaptive Conditionerthat dynamically compresses the selected context and injects it directly into the generator for efficient conditioning.\nTogether,OneStoryrealizes adaptive memory modeling, enabling global yet compact cross-shot context for coherent narrative generation.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2512.07802/x4.png",
                "caption": "Figure 4:Patchification Comparison.Left:Priorfixed temporal schemestypically consider the most recent block of contiguous frames and assign patchifiers by temporal order (e.g., the finest patchifier for the latest frame).Right:Ouradaptive schemeselects non-contiguous frames and allocates patchifiers based on content importance (i.e., finest patchifier for the most-important frame).",
                "position": 359
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07802/x5.png",
                "caption": "Figure 5:Qualitative results.For a fair comparison, the given multi-shot generations share the same first shot (generated byWan2.2) as the initial condition, except forStoryDiff.+Wan2.1, which does not rely on visual conditioning. The baseline methods fail to maintain narrative consistency across shots, struggling with prompt adherence, reappearance, and compositional scenes, whereasOneStory(ours) faithfully follows shot-level captions and produces coherent shots. A representative segment of each prompt is given with the corresponding shot.",
                "position": 1048
            },
            {
                "img": "https://arxiv.org/html/2512.07802/x6.png",
                "caption": "Figure 6:Qualitative comparison of frame selection strategies.(a) The sixth-shot generation using the five preceding shots as context (shown in the last row ofFigure˜5). (b) Fine-grained cases where the first shot involves dynamic motion. In each, the first gray row shows a few sampled frames (first, middle, and last) depicting the motion range in the first shot. The subsequent row shows the generated next shot from each strategy. Both baselines fail to maintain visual coherence, whereas our method identifies semantically relevant frames and produces consistent shots.",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2512.07802/x6.png",
                "caption": "Figure 6:Qualitative comparison of frame selection strategies.(a) The sixth-shot generation using the five preceding shots as context (shown in the last row ofFigure˜5). (b) Fine-grained cases where the first shot involves dynamic motion. In each, the first gray row shows a few sampled frames (first, middle, and last) depicting the motion range in the first shot. The subsequent row shows the generated next shot from each strategy. Both baselines fail to maintain visual coherence, whereas our method identifies semantically relevant frames and produces consistent shots.",
                "position": 1186
            },
            {
                "img": "https://arxiv.org/html/2512.07802/x7.png",
                "caption": "Figure 7:Advanced narrative modeling inOneStory.(a-b)Appearance changes:the character’s identity remains consistent under appearance variations.\n(c–d)Zoom-in effects:the model accurately localizes intended regions and preserves fine details when zooming in.\n(e–f)Human–object interactions:the model correctly continues event progression, maintaining coherent relationships between humans and surrounding objects across shots.",
                "position": 1192
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "7Additional Training Details",
        "images": []
    },
    {
        "header": "8Additional Details on Evaluation Benchmark",
        "images": []
    },
    {
        "header": "9Additional Qualitative Results",
        "images": []
    }
]