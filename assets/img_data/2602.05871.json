[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05871/x1.png",
                "caption": "Figure 2:Comparison of sampling strategies.TheOriginal Pathsuffers from error accumulation, while theSink-based Pathcollapses into aSink Point(dynamic collapse). In contrast, ourTTC strategyavoids these failures by employing reference-conditioned denoising and explicitRe-noising, effectively steering the trajectory away from the sink to preserve target distribution.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05871/x2.png",
                "caption": "Figure 3:Variants of autoregressive video generation.Discrete AR uses single-step deterministic prediction, multi-step diffusion follows a deterministic ODE trajectory, while few-step distilled diffusion performs stochastic sampling with intermediate noise injection.",
                "position": 119
            }
        ]
    },
    {
        "header": "3Test-time Optimization for Distilled Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05871/x3.png",
                "caption": "Figure 4:Comparison of two toy test-time optimization variants based on LoRA fine-tuning.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2602.05871/x4.png",
                "caption": "Figure 5:Overall pipeline of our method.A sparse set of correction steps is inserted into the stochastic sampling path until the global structure stabilizes. At selected steps, TTC performs reference-conditioned denoising using the initial frame to obtain a corrected prediction, which is then re-noised to the current timestep to remain consistent with the expected noise distribution. This on-path, training-free correction suppresses long-term error accumulation and stabilizes long-horizon generation.",
                "position": 235
            }
        ]
    },
    {
        "header": "4From Test-Time Optimization to Test-Time Correction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05871/x5.png",
                "caption": "Figure 6:Intermediate predictions along the stochastic sampling path.High-noise steps determine global structure, while low-noise steps refine appearance details under a fixed layout.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2602.05871/x6.png",
                "caption": "Figure 7:Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods.",
                "position": 740
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05871/x7.png",
                "caption": "Figure 8:Comparison between the sink-based method and path-wise correction.The sink-based method overly constrains intermediate states, leading to degraded motion dynamics and reduced temporal variation.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2602.05871/x8.png",
                "caption": "Figure 9:Comparison of single-point and path-wise correction.Single-point correction causes temporal discontinuities, while on-path re-noising improves temporal stability and reduces flickering.",
                "position": 1155
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on Samplers.",
        "images": []
    },
    {
        "header": "Appendix BDetails on Evaluations",
        "images": []
    },
    {
        "header": "Appendix CDetails on Methods.",
        "images": []
    },
    {
        "header": "Appendix DFurther Quantitative Results.",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05871/x9.png",
                "caption": "Figure 11:Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods.",
                "position": 2231
            },
            {
                "img": "https://arxiv.org/html/2602.05871/x10.png",
                "caption": "Figure 12:Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2602.05871/x11.png",
                "caption": "Figure 13:Qualitative comparison of 30-second long-horizon video generation with Self-Forcing, Rolling Forcing, and LongLive. Our method significantly outperforms Self-Forcing and achieves temporal coherence and visual quality comparable to training-based methods.",
                "position": 2237
            }
        ]
    },
    {
        "header": "Appendix EFurther Qualitative Results.",
        "images": []
    }
]