[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x2.png",
                "caption": "Figure 2:Qualitative results:CAT4D can generate high-quality dynamic 3D scenes from a single input monocular video. For each example, we show four rendered images, varying in time along the vertical axis, and varying in viewpoint along the horizontal axis. We also show a depth map (bottom right) and a frame from the input video (top right) at the same timestamp as the second column of renders.",
                "position": 84
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x3.png",
                "caption": "Figure 3:What is a multi-view video model?Given one or several input images (grey), different generative models have the ability to create novel images (orange) at certain collections of camera viewpoints and timestamps.Video modelsgenerate frames at all timestamps, but without control over camera.Multi-view modelsgenerate at controllable cameras but at a fixed timestamp.Camera-controlled video modelsenable the choice of camera per timestamp, but cannot generate multiple cameras per timestamp.Multi-view video modelscan generate all views at all timestamps.",
                "position": 115
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x4.png",
                "caption": "Figure 4:Illustration of the method:Given a monocular video (top), we generate the missing frames (orange frames) of virtual stationary video cameras positioned at all input poses (gray circles) and novel poses (blue circles) using our multi-view video diffusion model. These frames are then used to reconstruct the dynamic 3D scene as deforming 3D Gaussians. Note that although the input trajectory is visualized with changing viewpoints, our method also works for fixed-viewpoint videos.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2411.18613/x5.png",
                "caption": "Figure 5:Illustrating our alternating sampling strategy:Given a diffusion model that generatesNùëÅNitalic_Noutput views (here,N=3ùëÅ3N=3italic_N = 3), we use SDEdit[44]to alternate between multi-view and temporal sampling to generate a grid of images atKùêæKitalic_Kcameras andLùêøLitalic_Ltime steps (top, hereK=4ùêæ4K=4italic_K = 4andL=4ùêø4L=4italic_L = 4). In multi-view sampling, we generate each sliding window of size3333for each column and take the median of the results (middle). Temporal sampling follows a similar process for rows (bottom). Generations for each column or row can be executed in parallel.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2411.18613/x6.png",
                "caption": "Figure 6:Qualitative comparison, disentangled control:The camera-time grid on the left shows the positions of three input images (gray cells, images visualized in top row) and output images (green) in three different target sampling settings. One frame from each setting (orange cell) is visualized in each row, comparing our model with 4DiM[71]and ground truth.",
                "position": 432
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x7.png",
                "caption": "Figure 7:Qualitative comparison, sparse-view ‚Äúbullet-time‚Äù 3D reconstruction:The three input images are shown on the top, where the first one is the target bullet-time frame.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2411.18613/x8.png",
                "caption": "Figure 8:Qualitative comparison, sampling strategies:A comparison of different sampling strategies using space-time slices, where the vertical axis represents time and the horizontal axis shows a spatial slice of the image (red line). Our alternating sampling strategy best matches the ground truth motion.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2411.18613/x9.png",
                "caption": "Figure 9:Qualitative comparison, 4D reconstruction:We compare 4D reconstructions on the DyCheck dataset[16]. The rightmost column shows the input frame, at the same timestamp as the desired target image. Evaluation-excluded co-visibility masks are highlighted green. The visual improvement of our rendering over 4D-GS[72](which our system leverages) demonstrates the value of our proposed multi-view video model. Renderings from Shape-of-Motion[70]and MoSca[33]were graciously provided by the authors.",
                "position": 577
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMethod Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x10.png",
                "caption": "Figure 10:Camera trajectories (where we generate novel views) for different types of input videos. Within each panel, we show the trajectories from two different viewpoints. The input views are colored red, and the anchoring sample views are colored blue with the remaining sample views are colored by their index. For videos with sufficient view coverage (a), we only generate anchor views picked from the input camera trajectory.",
                "position": 1995
            }
        ]
    },
    {
        "header": "Appendix BDatasets Details",
        "images": []
    },
    {
        "header": "Appendix CBaselines Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18613/x11.png",
                "caption": "Figure 11:A comparison of models trained with different datasets on in-the-wild input images. The three input images are shown on the left-most column. Top: space-time slices of generated videos of ‚Äúfixed viewpoint, varying time‚Äù. Pixels of static background should be straight vertical lines on the slices and pixels of dynamic object should be smooth curves on the slices. Bottom: one frame of generated videos of ‚Äúvarying viewpoint, fixed time‚Äù.",
                "position": 2081
            }
        ]
    },
    {
        "header": "Appendix DAblation Study of Training Data",
        "images": []
    }
]