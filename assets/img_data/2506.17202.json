[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17202/extracted/6558765/figure/fig_vis_gen.png",
                "caption": "Figure 1:Text-to-image generation results by UniFork in 384×384 resolution.",
                "position": 76
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17202/x1.png",
                "caption": "Figure 2:Modality alignment analysis.We visualize how text-image feature alignment evolves across Transformer layers for both image understanding and generation tasks:\n(a) Image generation exhibits a rise-then-fall alignment trend across layers.\n(b) Image understanding shows an increasing alignment pattern.\n(c) When using a fully shared Transformer for both tasks under the next-token prediction objective, the alignment curves converge, reflecting representational compromise between generation and understanding.\n(d) Models fine-tuned on Emu3-base(Wang et al.,2024)for each individual task recover their distinct trends, consistent with those observed in expert models.",
                "position": 106
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17202/x2.png",
                "caption": "Figure 3:Overall framework of UniFork.UniFork adopts a Y-shaped Transformer backbone. The early layers are shared across both image generation and understanding tasks to facilitate joint semantic representation learning, while the later layers are split into task-specific branches to learn specialized representations. Und.: understanding. Gen.: generation. Proj.: projection.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2506.17202/x3.png",
                "caption": "Figure 4:Three-stage training pipeline for UniFork.The first stage focuses on aligning visual and textual modalities. The second stage performs joint training to enhance both image understanding and generation capabilities. In the third stage, task-specific parameters are alternately optimized using data from each task. Modules involved in training are highlighted inred.",
                "position": 210
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17202/x4.png",
                "caption": "Figure 5:Modality alignment analysis on MJHQ-30K.The observed alignment patterns on this dataset are consistent with those reported in Section3.1.",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2506.17202/x5.png",
                "caption": "Figure 6:Qualitative results on the text-to-image generation task.We compare image samples generated by SDv1.5, LlamaGen, and UniFork, with respective resolutions of512×512512512512\\times 512512 × 512,512×512512512512\\times 512512 × 512, and384×384384384384\\times 384384 × 384.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2506.17202/x6.png",
                "caption": "Figure 7:Qualitative results on the image understanding task.The key points in the answers are highlighted inred.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2506.17202/x7.png",
                "caption": "Figure 8:Modality alignment analysis for UniFork.",
                "position": 825
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]