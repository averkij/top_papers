[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods: Wikontic",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00590/x1.png",
                "caption": "Figure 1:Overview ofWikontic: an ontology-guided pipeline that constructs a Wikidata-aligned KG from text.\n(1) An LLM extracts candidate (subject, relation, object) triplets (gray). (2) The extracted triplets are then refined using Wikidata’s ontology: entity types are assigned (colored nodes), and relations that violate ontology constraints are corrected or removed. (3) Finally, entities names are normalized and duplicate surface forms are merged. The resulting graph is de-duplicated, ontology-consistent, and ready for downstream tasks.",
                "position": 148
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00590/x2.png",
                "caption": "Figure 2:Distribution of MINE-1 scores across 100 articles for GraphRAG, KGGen, and Wikontic.\nDotted vertical lines are averaged scores. Wikontic scored 84% on average, substantially\noutperforming GraphRAG 47.80% and KGGen 66%.",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2512.00590/x3.png",
                "caption": "Figure 3:Wikontic produces the most dense KGs for MuSiQue questions. For each question, subgraphs are constructed around its entities, and their sizes are reported relative to the full KG. The figure shows the relative sizes of 1– to 10-hop neighborhoods and the entire connected component containing the question, defined as all nodes reachable from any question node.",
                "position": 436
            }
        ]
    },
    {
        "header": "4Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.00590/x4.png",
                "caption": "Figure 4:Overview of the multi-stage pipeline for KG extraction from unstructured text. The process consists of (1) LLM-based triplet extraction, (2) ontology-based validation of triplet structure, and (3) entity linking and normalization.",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2512.00590/x5.png",
                "caption": "Figure 5:Ontology-based triplet refinement process. For each extracted triplet, we retrieve and extend candidate entity types using Wikidata’s type hierarchy, identify valid relations allowed to use between extracted entities based on ontology constraints, and re-rank relation candidates using semantic similarity. The final triplet configuration is selected by an LLM.",
                "position": 1281
            },
            {
                "img": "https://arxiv.org/html/2512.00590/x6.png",
                "caption": "Figure 6:Entity refinement step for KG construction. For each refined triplet, candidate subject and object entities are retrieved from the existing KG based on their type and semantic similarity. An LLM determines whether the extracted entity matches an existing one or should be preserved as a new entry. This process reduces redundancy and supports incremental KG updates.",
                "position": 1284
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]