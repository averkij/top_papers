[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3From Log-Based Attribution to Intervention-Based Debugging",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06749/x1.png",
                "caption": "Figure 1:Failure trace of Case 3 in WW-HC, illustrating ambiguity in failure attribution. The session consists of four distinct trials, each initiated by a plan update and executed via a ReAct-styleReactloop. Different strategies (e.g., direct scrolling in Trial 1 vs. calendar navigation in Trial 2) yield separate error points, making single-step attribution across the session inherently ambiguous. Trial 2 (Steps 53–55) further shows inter-agent misalignment: the Orchestrator issued an invalid instruction, while the WebSurfer compounded the error by executing an unrelated action.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06749/x2.png",
                "caption": "Figure 2:DoVer(Do–then–Verify) Debugging Pipeline.\n(1)Trial segmentation: split the failed session log into trials using re-plan steps as cut points.\n(2)Failure attribution: for each trial, propose a hypothesishih_{i}that marks a faulty step or agent.\n(3)Intervention generation: turnhih_{i}into an actionable intervention that edits either the plan or the attributed message or step in the original log.\n(4)Intervention execution: replay the trajectoryin place, i.e., preserve all steps before the intervened step, then execute the intervention and measure progress of the new log.\nColors indicate plan/re-plan (blue), execution (green), attributed failure (red), terminal failure (dark red), terminal success (dark green), intervention (yellow), new plan/re-plan (blue hatch), and new execution (green hatch).",
                "position": 229
            }
        ]
    },
    {
        "header": "5Experiment Results",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Generalizability",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ARevisit Log-based Failure Attribution on the Who&When Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06749/x3.png",
                "caption": "Figure 3:Extended prompt template:orangemarks explicit step indices, andbluemarks the embedded concise reminder of annotators’ guidance.",
                "position": 788
            }
        ]
    },
    {
        "header": "Appendix BPrompts Used in DoVer",
        "images": []
    },
    {
        "header": "Appendix CEnabling DoVer on AutoGen2",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06749/figures/AG2_webui.png",
                "caption": "Figure 4:Web-based intervention user interface for the AG2 MathChat system.\n(1) List of recorded math problem sessions.\n(2) Input box for submitting a new math task.\n(3) Main conversation panel showing the multi-agent trace and intermediate reasoning.\n(4) Intervention panel, where a user can select a specific agent and step to edit the message or plan.\n(5) History panel showing checkpoints and continuations after interventions.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x4.png",
                "caption": "Figure 5:Trial segmenter prompt: log decomposition of full session into planning–execution trials.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x5.png",
                "caption": "Figure 6:Trial summarizer + failure proposer prompt: per‑trial summary and root‑cause localization, part 1.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x6.png",
                "caption": "Figure 7:Trial summarizer + failure proposer prompt: continuation of Figure6, part 2.",
                "position": 1891
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x7.png",
                "caption": "Figure 8:Intervention recommender prompt: minimal, executable fix classification; JSON category and replacement text.",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x8.png",
                "caption": "Figure 9:Ground‑truth milestone extractor prompt:≤\\leq5 tool‑agnostic milestones.",
                "position": 1897
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x9.png",
                "caption": "Figure 10:Milestone evaluator prompt: progress vs milestones and new‑path assessment.",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2512.06749/x10.png",
                "caption": "Figure 11:Post‑intervention outcome classifier prompt: mislocalization or insufficient fix proposer.",
                "position": 1903
            }
        ]
    },
    {
        "header": "Appendix DSample “Validated” and “Partially Validated” Cases",
        "images": []
    }
]