[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Complex Question Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08276/x1.png",
                "caption": "Figure 1:Overall pipeline for complex question construction.",
                "position": 160
            }
        ]
    },
    {
        "header": "3Reinforcement Learning with Dynamic Context Window",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08276/x2.png",
                "caption": "Figure 2:Preliminary experiments with gpt-oss-120b on the BrowseComp dataset. Left: Length distribution of incorrect trajectories. Right: Context length changes across rounds with/without sliding window. Without sliding window, tool responses grow exponentially and squeeze assistant context. With sliding window, tool length stays constant while assistant content grows normally.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x2.png",
                "caption": "",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x3.png",
                "caption": "",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x4.png",
                "caption": "Figure 3:Sliding window mechanism for dynamic context management. Upper: inference trajectory with sliding window (size=3, slide=2) where older tool responses are replaced by omission tokens while assistant outputs are preserved. Lower: corresponding training sequences where only newly generated responses receive gradient updates while previous outputs serve as fixed context.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08276/x5.png",
                "caption": "Figure 4:Training Dynamics of DeepMiner Reinforcement Learning.",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x5.png",
                "caption": "",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x6.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x7.png",
                "caption": "",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x8.png",
                "caption": "Figure 5:Scaling on tool call budget and context length.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x8.png",
                "caption": "",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2510.08276/x9.png",
                "caption": "",
                "position": 566
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BEthical Considerations",
        "images": []
    },
    {
        "header": "Appendix CEnhanced Tool Suite",
        "images": []
    },
    {
        "header": "Appendix DTemplates",
        "images": []
    },
    {
        "header": "Appendix EQuestion Examples",
        "images": []
    }
]