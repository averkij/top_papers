[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x1.png",
                "caption": "Figure 1:Performance of GAEA and other LMMson global scale image geo-localization. GAEA makes correct predictions when asked different questions about summarizing a scene, location, and geographical context. While GPT-4o-mini can give correct suggestions relevant to the region, GAEA provides a correct amenity with proximity to the location of the image.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x2.png",
                "caption": "Figure 2:Data Collection and Annotation Pipeline.GAEA-1.6M includes geographically diverse visual samples from various data sources, such as MP-16, GLD-v2, and CityGuesser68k(left). We also incorporate OpenStreetMap (OSM) metadata and auxiliary context for each image, ranging from climate zones to geographical clues about the country(middle). Using open-source LLMs and GPT-4o, we generate four diverse question-answer pairs across geolocation, reasoning, and conversational subsets(right).",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x3.png",
                "caption": "Figure 3:Overview of GAEA-Bench.GAEA-Bench is designed to evaluate the conversational abilities of various LMMs across different question types, including MCQs, T/F, and both short and long VQAs. We have carefully selected a subset of 4k samples from MP-16 and generated corresponding OSM metadata to generate QA pairs using GPT-4o. GAEA-Bench aims to fill the gap in conversational benchmarks by incorporating geolocalization capabilities.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x4.png",
                "caption": "Figure 4:Qualitative examplesshowcasing various question-types, including multiple-choice, true/false, short and long VQAs generated using an open-source model on our GAEA-1.6M dataset. We carefully select geographical tags from OSM metadata to generate QA pairs.",
                "position": 162
            }
        ]
    },
    {
        "header": "3GAEA-1.6M",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x5.png",
                "caption": "Figure 5:Evaluation pipelinehighlights various question types we introduce in our GAEA-1.6M. We use GPT-4o as a judge to score such responses.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x6.png",
                "caption": "Figure 6:GAEA architecturewith a single-stage training strategy including trainable MLP layers and LLM weights.",
                "position": 235
            }
        ]
    },
    {
        "header": "4GAEA Architecture",
        "images": []
    },
    {
        "header": "5Benchmarking and Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x7.png",
                "caption": "Figure 7:Our classification accuracy pipeline evaluates city and country predictions by comparing them against ground truth annotations derived from GPS coordinates, with GPT-4o serving as the evaluator.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x8.png",
                "caption": "Figure 8:Classification accuracy for both city and country labels, where GAEA establishes itself as a strong baseline, surpassing several recent LMMs in performance.",
                "position": 548
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x9.png",
                "caption": "Figure 9:Geo-localization qualitative example.GAEA’s performance on geo-localization tasks is compared to open-source LMMs using CityGuessr, DollarStreet, and GeoDE datasets.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x10.png",
                "caption": "Figure 10:SVQA qualitative example.GAEA’s performance on SVQA tasks is compared to open-source and proprietary LMMs.",
                "position": 2066
            }
        ]
    },
    {
        "header": "7Addendum to the Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x11.png",
                "caption": "Figure 11:These two images display examples of what we consider as easy questions. Easy questions include the questions that pertain to easily identifiable landmarks that are associated with celebrated locations.",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x12.png",
                "caption": "Figure 12:The two images above denote examples of what we consider as hard questions. Hard questions include the questions that prompt the model to answer specific details pertaining to locations.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x13.png",
                "caption": "Figure 13:Through ourexplanatory captions(LVQA), we introduce the reasoning capabilities in our GAEAto identify the geographical information about that visual sample. Outlined in bold, we provide explanations of the correlation between specific visual cues and their associated geographical contexts, encouraging GAEAto refine its reasoning capabilities.",
                "position": 2086
            }
        ]
    },
    {
        "header": "8Addendum to Baseline and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16423/x14.png",
                "caption": "Figure 14:MCQ qualitative example.GAEA’s performance on MCQ answering tasks is compared to open-source and proprietary LMMs.",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x15.png",
                "caption": "Figure 15:T/F qualitative example.GAEA’s performance on T/F answering tasks is compared to open-source and proprietary LMMs.",
                "position": 2211
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x16.png",
                "caption": "Figure 17:We showcase the performance of various LMMs on four diverse question types. GAEA outperforms on average across all question forms. GPT-4o achieves the highest accuracy on long questions.",
                "position": 2277
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x17.png",
                "caption": "Figure 18:Task-specific prompts used to train and evaluate GAEA",
                "position": 2280
            },
            {
                "img": "https://arxiv.org/html/2503.16423/x18.png",
                "caption": "Figure 19:Example of the country-specific clues we used to generate reasoning questions.",
                "position": 2285
            }
        ]
    },
    {
        "header": "9Reproducibility, Privacy, Safety, and Broader Impact",
        "images": []
    }
]