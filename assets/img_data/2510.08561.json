[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08561/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08561/x2.png",
                "caption": "Figure 2:Overview of our MultiCOIN pipeline. Given a videoXX, we extract multi-modal motion controls through two generators: the Sparse Motion Generator via optical flow and the Sparse Depth Generator for depth maps, both producing sparse RGB points for trajectory and depth control. An Augmented Frame Generator computes target regions and masks to enable fine-grained content control. All control signals are encoded via a dual-branch embedder architecture that separately captures motion and content features. In addition, a text prompt condition is processed by a text encoder to provide semantic guidance over the generated content. At inference, the model flexibly integrates these multi-modal controls for interpolation.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08561/x3.png",
                "caption": "Figure 3:Sparse Motion and Depth Generator. Given videoXX, dense optical flow and depth maps are computed. Trajectories are selected from high-motion regions along which flow/depth points are sampled and expanded with 2D filters to get sparse RGB inputs.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2510.08561/x4.png",
                "caption": "Figure 4:Example of a witch moving the Jack-oâ€™-Lantern along the same path, with motion inward (top) or outward (bottom), depending on midpoint depth (blue vs. red dot).",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2510.08561/x5.png",
                "caption": "Figure 5:Our results illustrate several ways multi-modal controls can be applied to frame interpolation. In the top section, we show trajectory control on its own, followed by two depth variations that place the cat either in front of or behind the pumpkin. Combining trajectory with depth produces richer motion: the balloon recedes along the z-axis while the weights with the cat are pushed outward. Prompts can also be paired with trajectories, where the trajectory sets the overall movement and the prompt refines details. In the bottom section, we highlight target region control. The temporal placement of target regions determines content editing at that point: in the first case, they are inserted in the middle with both first and last frames given, while in the second they appear at the end serving as a soft replacement for the last frame.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08561/x6.png",
                "caption": "Figure 6:Comparison with Framer[40]. The top row highlights our reduced distortion for trajectory control, while the bottom rows showcase the benefits of additional controls such as depth control and text prompt.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2510.08561/x7.png",
                "caption": "Figure 7:Results with more than 2 input frames, both with and without motion input.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2510.08561/x8.png",
                "caption": "Figure 8:Results showcasing image deformation.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2510.08561/x9.png",
                "caption": "Figure 9:Ablation Results: w/o stage-wise: Skipping stage-wise training degrades performance; without dense flow the model mislocalizes motion, and without dense depth it misinterprets depth control.\nw/o dual-branch: Removing the dual-branch design increases artifacts and causes depth confusion.",
                "position": 336
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]