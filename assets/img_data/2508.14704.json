[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x1.png",
                "caption": "Figure 1:Example from MCP-Universe illustrating realistic challenges, including real-world tool usage, long-horizon multi-turn tool calls, long context windows, scattered evidence, and large tool spaces. Unlike prior work, MCP-Universe is grounded in real-world MCP servers connected to actual data sources and environments.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x2.png",
                "caption": "Figure 2:Overview of the MCP-Universe evaluation framework. The framework dynamically configures LLM agents, MCP servers, and execution-based evaluators according to task specifications. Each evaluation involves the agent-server interactions mediated via the MCP protocol, followed by an objective assessment conducted by automated execution-based evaluators to determine the success of task completion.",
                "position": 213
            }
        ]
    },
    {
        "header": "3MCP-Universe",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x3.png",
                "caption": "Figure 3:Distribution of tasks in MCP-Universe across different application domains.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x3.png",
                "caption": "Figure 3:Distribution of tasks in MCP-Universe across different application domains.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/OpenAI.png",
                "caption": "Table 3:Comparison on our MCP-Universe benchmark. For our main experiments, all LLMs follow the ReAct agent pipeline, except GPT-OSS, which has poor instruction-following abilities and therefore cannot follow the ReAct prompt; for this model, we use the OpenAI Agent SDK instead. We report the success rate (SR, %) for each domain and all tasks. Additionally, we calculate the average percentage of evaluators passed for each task, which we refer to as the average evaluator score (AE). Moreover, we also report the average number of steps (AS) for each successful task. Since GPT-OSS does not follow ReAct, it does not have AS scores.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/grok.png",
                "caption": "",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/Anthropic.png",
                "caption": "",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/google.png",
                "caption": "",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/zhipu.png",
                "caption": "",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/kimi.png",
                "caption": "",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/Qwen.png",
                "caption": "",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2508.14704/logos/deepseek.png",
                "caption": "",
                "position": 663
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x4.png",
                "caption": "Figure 4:(Left) Growth of average context length (in tokens) as the number of interaction steps increases in MCP-Universe tasks, illustrating the long context challenge. (Right) Effect of introducing a summarization agent on LLM agent performance across selected domains.",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x5.png",
                "caption": "",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x6.png",
                "caption": "Figure 5:(Left) An example of the unknown tool challenges. (Right) Effect of introducing the exploration phase on LLM agent performance across selected domains.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x7.png",
                "caption": "",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x8.png",
                "caption": "Figure 6:Effect of connecting with more unrelated MCP servers.",
                "position": 853
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMCP Servers",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x9.png",
                "caption": "Table 13:An Example of Location Navigation Evaluators.",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x9.png",
                "caption": "Table 14:An Example of Repository Management Evaluators.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2508.14704/x9.png",
                "caption": "Table 15:An Example of Financial Analysis Evaluators.",
                "position": 2073
            }
        ]
    },
    {
        "header": "Appendix BTasks and Evaluators Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x9.png",
                "caption": "Figure 7:The ReAct prompt in our experiments.",
                "position": 2265
            }
        ]
    },
    {
        "header": "Appendix CSetup",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x10.png",
                "caption": "Figure 8:Naive Error of o3",
                "position": 2274
            }
        ]
    },
    {
        "header": "Appendix DNaive Error",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x11.png",
                "caption": "Figure 9:The summarization prompt in our experiments.",
                "position": 2283
            }
        ]
    },
    {
        "header": "Appendix ESummarization Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14704/x12.png",
                "caption": "Figure 10:The exploration prompt in our experiments.",
                "position": 2292
            }
        ]
    },
    {
        "header": "Appendix FExploration Agent",
        "images": []
    }
]