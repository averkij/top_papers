[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x1.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x2.png",
                "caption": "",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x3.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x4.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x5.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x6.png",
                "caption": "Figure 1:Selected samples from our model.Our model takes in starting frames from each player as input and generates action-conditioned videos. The action descriptions shown here are summaries of the fine-grained action sequences given to the model that span many frames. The third-person ground truth visualizations are not given to the model.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x7.png",
                "caption": "Figure 2:SolarisEngine Overview.(Left)Docker-based orchestration of containerized game server, camera, and controller bots. Cameras mirror Controllers’ state and actions via a custom server-side plugin; Controllers are Mineflayer bots that run episode code and log low-level actions.(Right)Episodes compose reusable skill primitives from a shared library. Simplified “collector” episode code is shown.",
                "position": 192
            }
        ]
    },
    {
        "header": "3SolarisEngine: A Framework for Multiplayer Gameplay at Scale",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x8.png",
                "caption": "Figure 3:Dataset Statistics of our training dataset.(Left)The dataset consists of four different episode categories focusing on building, combat, movement, and mining scenarios, respectively.(Middle)It has a total of 9,240 episodes and 6.32 M frames per player, for a combined 12.64 M frames. Episode types are chosen randomly with weights that decrease with respect to the typical episode length.(Right)Most episode lengths range from 128 to 512 frames or 6.4 to 25.6 seconds (we record at 20 fps).",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x9.png",
                "caption": "Figure 4:Episode Demonstrations from our training dataset.We show the recorded frames from 3 different training episodes at various points in time. Note that the third-person “start state” and “end state” screenshots are for visualization only and are not part of the dataset.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x10.png",
                "caption": "Figure 5:Our modified DiT block achieves multiplayer modeling through visual interleaving along the sequence dimension. We denote the number of players withNNand the number of tokens per video withMM. Multiplayer information is exchanged through a shared self-attention block. The other modules are unchanged from Matrix Game 2.0 and applied independently per player.",
                "position": 303
            }
        ]
    },
    {
        "header": "4Solaris Model Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x11.png",
                "caption": "Figure 6:An overview of the full training pipeline.Starting with a pretrained bidirectional video diffusion model, we first finetune it with single-player and then multiplayer data. We then finetune it with a causal mask before using Self Forcing to achieve stable long-horizon autoregressive generations.",
                "position": 341
            }
        ]
    },
    {
        "header": "5Multiplayer Training Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x12.png",
                "caption": "Figure 7:An overview of Checkpointed Self Forcing.(A)A video is generated with a sliding-window KV cache. The computation graph in Jax contains redundant tensors, making naive backpropagation memory prohibitive.(B)Our recomputation step simulates the last step of denoising for every frame in parallel. Clean context frames and final noisy frames are cached from the previous step.(C)An illustration of the attention mask used in part B (black squares denote attention). This attention mask allows us to simulate the final denoising step of the rollout in parallel.(D)Leveraging these memory savings, we enable backpropagation through KV layers (avoiding the standard stop-gradient of Self Forcing), which we show improves visual generation inSection˜7. Specifically, we allow gradients to flow in line 29 ofAlgorithm˜1.(E)Peak memory comparison between naive and Checkpointed Self Forcing across varying depths. Scaled-down networks are used to prevent OOM errors in the naive baseline.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2602.22208/x13.png",
                "caption": "Figure 9:Qualitative comparison.Our model Solaris is able to produce stable and coherent frame generations for long-horizon, as shown here with 224 frames. Unlike the baselines, it maintains realistic fighting gameplay and displays complex terrain that maintains realistic texture. In contrast, the frame concatenation baseline shows severe degradation for the second player and flattened texture for the first player. “Solaris w/o pretrain\" exhibits unnatural behavior such as duplicating player bodies, showing incorrect pop-up notifications, and degenerating to an unrealistic underwater setting.",
                "position": 544
            }
        ]
    },
    {
        "header": "6Evaluation Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/x14.png",
                "caption": "Figure 10:Qualitative examples of learned capabilities. We present generated videos demonstrating the model’s ability to simulate complex game dynamics. The rows illustrate: (1) fine-grained state tracking, specifically inventory counter synchronization following block placement; (2) global environmental consistency, shown by the simultaneous onset of rain; (3) inventory active item synchronization, torch placement, and generating accurate mining animations; and (4) coherent PVP on complex terrain.",
                "position": 570
            }
        ]
    },
    {
        "header": "7Experiments",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASolaris Multiplayer Framework",
        "images": []
    },
    {
        "header": "Appendix BMultiplayer Training Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.22208/figs/assets/underwater_episode.jpg",
                "caption": "Figure 11:An episode where a bot has been teleported underwater.",
                "position": 1878
            },
            {
                "img": "https://arxiv.org/html/2602.22208/figs/assets/bubbles_gui.png",
                "caption": "Figure 12:The “bubbles” HUD template PNG used to filter underwater episodes.",
                "position": 1881
            },
            {
                "img": "https://arxiv.org/html/2602.22208/figs/assets/mouse_act_histogram.png",
                "caption": "Figure 13:A histogram showing the mouse action magnitude distribution in our training dataset.",
                "position": 2128
            }
        ]
    },
    {
        "header": "Appendix CModel Training",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    }
]