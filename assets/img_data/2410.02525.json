[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/main_arch.png",
                "caption": "Figure 1:Overview of our system for contextual document embeddings (CDE). Our model operates in two stages: a first stage used to characterize the dataset from samples, and a second stage used to embed the final document.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Methods",
        "images": []
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/result11_cluster_hardness.png",
                "caption": "Figure 2:Performance vs. average batch difficulty (as measured by loss at the end of pre-training and supervised training) across batch sizes, after supervised contrastive training. Within a given batch size, we observe a clear increase in performance by making individual batches harder. Correlations are Pearson.",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/result11_cluster_hardness.png",
                "caption": "Figure 2:Performance vs. average batch difficulty (as measured by loss at the end of pre-training and supervised training) across batch sizes, after supervised contrastive training. Within a given batch size, we observe a clear increase in performance by making individual batches harder. Correlations are Pearson.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x1.png",
                "caption": "Figure 3:Biencoder performance with filtering (left) and without (right) across batch and cluster sizes during unsupervised contrastive pre-training. With filtering, small cluster sizes clearly improve performance, and larger batch sizes do not.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x2.png",
                "caption": "",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x3.png",
                "caption": "Figure 4:Impact of filtering during training across various batch and cluster sizes. Each dot is a biencoder pretrained with a different batch and cluster size.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x3.png",
                "caption": "Figure 4:Impact of filtering during training across various batch and cluster sizes. Each dot is a biencoder pretrained with a different batch and cluster size.",
                "position": 970
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/result30_supervised_epoch.png",
                "caption": "Figure 5:Performance on MTEB across epochs of supervised training on the Nomic and BGE supervised meta-datasets.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/analysis_cluster_hardness.png",
                "caption": "Figure 6:Average difficulty of in-batch negatives as measured by a surrogate model as cluster size and batch size change.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/analysis_cluster_hardness.png",
                "caption": "Figure 6:Average difficulty of in-batch negatives as measured by a surrogate model as cluster size and batch size change.",
                "position": 984
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/cqadupstack.png",
                "caption": "Figure 7:Impact of context by testing our model with different Stackexchange forum input types. Y-axis indicates the input domain, X-axis indicates the test domain. Dark squares come within one point NDCG@10.",
                "position": 989
            }
        ]
    },
    {
        "header": "7Analysis",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02525/x4.png",
                "caption": "Figure 8:Analysis of domain shift for popular neural retrieval methods. Performance difference from BM25 (y-axis) correlates with the different in IDF of the test corpusùíüùíü\\cal Dcaligraphic_Dform the training corpusùíüTsubscriptùíüùëá{\\cal D}_{T}caligraphic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT.",
                "position": 1820
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x5.png",
                "caption": "Figure 9:Contextual performance with filtering (left) and without (right) across batch and cluster sizes during unsupervised contrastive pre-training. Here, clustering with small cluster sizes clearly improves performance, and larger batch sizes do not.",
                "position": 2014
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x6.png",
                "caption": "",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/app_cluster_hardness_supervised.png",
                "caption": "Figure 10:Correlation between batch difficulty and perforamnce after supervised training.",
                "position": 2031
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x7.png",
                "caption": "Figure 11:Performance of all supervised models, across numbers of hard negatives.",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/result20_filter_cluster_size.png",
                "caption": "Figure 12:Model performance vs. cluster size with and without filtering. When false negative filtering is enabled, we see more improvements in performance from clustering at small cluster sizes.",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/result20_filter_cluster_size.png",
                "caption": "Figure 12:Model performance vs. cluster size with and without filtering. When false negative filtering is enabled, we see more improvements in performance from clustering at small cluster sizes.",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/result21_filter_batch_size.png",
                "caption": "Figure 13:Model performance vs. batch size with and without filtering. With and without filtering, the optimal batch size ranges between102superscript10210^{2}10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTand104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT; performance starts to decrease as batch size grows too large.",
                "position": 2062
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x8.png",
                "caption": "Figure 14:Pre-training with TSP vs. random batching across cluster sizes.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x9.png",
                "caption": "Figure 15:Each color indicates a single document inputdùëëditalic_d. Different points represent different valuesœï‚Å¢(d;ùíü)italic-œïùëëùíü\\phi(d;{\\cal D})italic_œï ( italic_d ; caligraphic_D )for different contexts.",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x9.png",
                "caption": "Figure 15:Each color indicates a single document inputdùëëditalic_d. Different points represent different valuesœï‚Å¢(d;ùíü)italic-œïùëëùíü\\phi(d;{\\cal D})italic_œï ( italic_d ; caligraphic_D )for different contexts.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x10.png",
                "caption": "Figure 16:Performance of CDE model as the number of contextual examples increases.",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2410.02525/extracted/5986290/figs/app_scaling_first_stage_model.png",
                "caption": "Figure 17:System performance (training accuracy) as we scale the size of the first-stage model encoder only.",
                "position": 2855
            },
            {
                "img": "https://arxiv.org/html/2410.02525/x11.png",
                "caption": "Figure 18:Performance per-dataset as we scale tokens-per-document, while keeping the total number of contextual tokens fixed. Different domains prefer a different number of tokens per document.",
                "position": 2865
            }
        ]
    },
    {
        "header": "10Supplementary Material",
        "images": []
    }
]