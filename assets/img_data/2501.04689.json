[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04689/x1.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04689/x2.png",
                "caption": "Figure 2:SPAR3D Overview.Conditioned on the input image, SPAR3D first leverages a point diffusion model to generate a sparse point cloud. The triplane transformer then uses the sampled point cloud and image features to produce high-resolution triplane features. The triplane features are then queried to reconstruct the geometry, texture, and illumination of the object in the image.",
                "position": 149
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04689/x3.png",
                "caption": "Figure 3:Our Differentiable Renderer.We estimate geometry, albedo, lighting, and normal maps from the triplane and metallic/roughness values from the image. We rasterize and interpolate these values as input to our shader (omitted here for simplicity). Our shader uses the Disney BRDF[3]and performs Monte Carlo integration. We further perform visibility testing to improve shadow modeling. Finally, we compare the rendered image with the GT image and minimize the rendering loss.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x3.png",
                "caption": "Figure 3:Our Differentiable Renderer.We estimate geometry, albedo, lighting, and normal maps from the triplane and metallic/roughness values from the image. We rasterize and interpolate these values as input to our shader (omitted here for simplicity). Our shader uses the Disney BRDF[3]and performs Monte Carlo integration. We further perform visibility testing to improve shadow modeling. Finally, we compare the rendered image with the GT image and minimize the rendering loss.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x4.png",
                "caption": "Figure 4:Shadow Modeling.We perform visibility testing in screen-space by marching along sampled rays. If any point along the ray has a ray depth which is farther away than the depth map, we consider the entire ray as shadowed.",
                "position": 233
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04689/x5.png",
                "caption": "Figure 5:Qualitative Comparison.We compare SPAR3D to other state-of-the-art methods visually. SPAR3D not only aligns better with the visible surfaces from images, but also generates higher-quality geometries and textures for the occluded surfaces.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x6.png",
                "caption": "Figure 6:Generalization Results.We show qualitative results of SPAR3D on in-the-wild images from 2D generative models (top 2 rows) and ImageNet (bottom 2 rows). The reconstructed meshes exhibit accurate geometric structures with great textures, demonstrating a strong generalization performance of SPAR3D.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x7.png",
                "caption": "Figure 7:Editing Results.We show qualitative examples of interactive editing with SPAR3D. On the left two examples, we add a handle to the mug and a tail to the elephant doll by duplicating existing points. On the right two examples, we move or delete points to fix imperfections and to improve local details on the mesh. All the edits are performed in Blender within a minute.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x8.png",
                "caption": "Figure 8:Generated Mesh with Conflicting Cues.Under conflicting cues from images and point clouds, our model reconstructs the visible surface based on the image, while generating the backside surface based on the point cloud.",
                "position": 722
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04689/x9.png",
                "caption": "",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x10.png",
                "caption": "",
                "position": 1676
            }
        ]
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BAdditional Illustrations of our Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04689/x11.png",
                "caption": "Figure 11:Point Cloud Denoiser Architecture.We illustrate the architecture of our point cloud denoiser. The point cloud denoiser takes the noisy point cloud and the image as input, and produces a denoised point cloud. The image and the noisy point cloud are encoded as latent vectors and concatenated together. The concatenated latent vectors are processed by a set of transformer blocks and decoded as the denoised point cloud.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2501.04689/x12.png",
                "caption": "Figure 12:Meshing Model Architecture.We illustrate the architecture of our meshing model, which takes the point cloud and the image as input, and produces a textured mesh and an environment map as output. Specifically, the meshing model first encodes the image and the point cloud as latent vectors. The learnable triplane tokens are then processed by the triplane transformer conditioned on the latent vectors. We query the triplane with MLPs to obtain albedo, density, vertex deformation and surface normal, which are converted to a textured mesh using DMTet. The triplane also produces an environment map using the illumination prior from RENI++. The metallic and roughness values are estimated from the image directly and are omitted here for simplicity.",
                "position": 1704
            }
        ]
    },
    {
        "header": "Appendix CDecomposition Results",
        "images": []
    },
    {
        "header": "Appendix DAdditional In-the-wild Results",
        "images": []
    }
]