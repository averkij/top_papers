[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18153/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18153/x2.png",
                "caption": "Figure 2:The training process ofDepthLab.First, we apply random masking to the ground truth depth to create the masked depth, followed by interpolation. Both the interpolated masked depth and the original depth undergo random scale normalization before being fed into the encoder. The Reference U-Net extracts RGB features, while the Estimation U-Net takes the noisy depth, masked depth, and encoded mask as input. Layer-by-layer feature fusion allows for finer-grained visual guidance, achieving high-quality depth predictions even in large or complex masked regions.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2412.18153/x3.png",
                "caption": "Figure 3:Qualitative comparison of various methods on different datasets.In the second column, black represents the known regions, while white indicates the predicted areas.\nNotably, to emphasize the contrast, we reattach the known ground truth depth to the corresponding positions in the right-side visualizations of the depth maps. Other methods exhibit significant geometric inconsistency.",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2412.18153/x4.png",
                "caption": "Figure 4:Visualization of gaussian inpainting.By projecting depth directly into three-dimensional space as initial points, natural 3D consistency is maintained, enabling texture editing and object addition. Please zoom in to view more details.",
                "position": 474
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18153/x5.png",
                "caption": "Figure 5:Visualization of 3d scene generation.Left: Depth comparison. ”Align” represents the least-square method and shows clear geometric inconsistencies at boundaries. While LucidDreamer reduces these inconsistencies, it compromises the accuracy of the newly estimated depth.\nIn contrast, our model produces consistent and accurate depth.Right: The improved depth estimation from our model leads to superior 3D scene generation results.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2412.18153/x6.png",
                "caption": "Figure 6:Visualization of sparse-view reconstruction with DUST3R.Left: Compared to InstantSplat[14], which directly uses point cloud from DUST3R as initialization, our method produces sharper and clearer depth in non-matching regions.Right: Using our method for improved initialization results in higher-quality Gaussian splatting rendering. Please zoom in for details.",
                "position": 504
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]