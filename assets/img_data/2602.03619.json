[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03619/x1.png",
                "caption": "Figure 1:Overview of our method.(a)We construct diverse reporting queries and generate candidate reports. Human experts provide pairwise preference annotations based on usefulness, coherence, completeness, and alignment.(b)Given a query and its preferred and rejected reports, we train a rubric generator via GRPO to produce weighted evaluation rubrics, with rewards based on preference consistency, LLM-as-a-judge scores, and format validity.(c)Multi-Agent Markov-state (MaMs) workflow. A shared policy executes a structured workflow with search, chunking, state update, and report generation, interacting with external tools. Query-specific rubrics are used to compute rewards of the rollout reports under an individual query.",
                "position": 265
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03619/x2.png",
                "caption": "Figure 2:Topic distribution of our created human preference dataset for DeepResearch reports.",
                "position": 323
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03619/x3.png",
                "caption": "Figure 3:Comparison between GSPO and GRPO during training rubric generators (Qwen3-30B-A3B).\n(a) Reward curves of generated rollouts under the two algorithms, showing nearly identical reward values.\n(b) Entropy of generated rollouts, where GSPO consistently exhibits higher entropy than GRPO.",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2602.03619/x3.png",
                "caption": "",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2602.03619/x4.png",
                "caption": "",
                "position": 1001
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Accessibility",
        "images": []
    },
    {
        "header": "Software and Data",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACase Study on Query Rewriting",
        "images": []
    },
    {
        "header": "Appendix BGlobal Algorithm of the MaMs workflow",
        "images": []
    },
    {
        "header": "Appendix CPrompts Used in MaMs Workflow and LLM-as-a-Judge",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EMetrics for Human Preference",
        "images": []
    },
    {
        "header": "Appendix FRollout Speed-up for MaMs workflow",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03619/x5.png",
                "caption": "Figure 4:Speed-up achieved by overlapping multiple micro-batches using the asynchronous event loop. The concurrency-limited scheduling allows high-latency API calls to run in parallel, maximizing resource utilization and reducing the effective runtime of the stage from linear in the dataset size|ùíü||\\mathcal{D}|to approximately|ùíü|/C|\\mathcal{D}|/C, whereCCis the concurrency limit.",
                "position": 2969
            }
        ]
    },
    {
        "header": "Appendix GPreference Performance of Rubric Generator trained with GSPO",
        "images": []
    },
    {
        "header": "Appendix HAnalysis on Tool Calling",
        "images": []
    },
    {
        "header": "Appendix ICase Study of Rubric List",
        "images": []
    },
    {
        "header": "Appendix JLimitations and Future work",
        "images": []
    }
]