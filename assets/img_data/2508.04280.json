[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04280/x1.png",
                "caption": "Figure 1:Real-world skill transfer after synthetic training. Our method, VL-DAC, improves agentic control, spatial planning, and embodied reasoning on BALROG, VSI-Bench, and ERQA. It demonstrates effective transfer from synthetic environments to real-world benchmarks.",
                "position": 130
            }
        ]
    },
    {
        "header": "Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04280/x2.png",
                "caption": "Figure 2:Vision-Language Decoupled Actor-Critic (VL-DAC) pipeline.A vision-language model receives RGB frames and text context, predicts token-wise actions via PPO, and learns a step-level value head whose gradients are stopped at the backbone.",
                "position": 307
            }
        ]
    },
    {
        "header": "Vision-Language Decoupled Actor-Critic (VL-DAC) Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04280/x3.png",
                "caption": "Figure 3:Episode success rates across environments. Success rates (%) of our method vs. RL4VLM (averaged over multipleλ\\lambdavalues) on six environments: MiniWorld-Hallway, OneRoom, FourRooms (top row), WallGap, EZPoints, ALFWorld (bottom row). While RL4VLM requires tuningλ\\lambdaper environment, our method performs robustly without tuning.",
                "position": 383
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04280/x4.png",
                "caption": "Figure 4:Ablation study of stabilization tricks.Adding KL regularization, value warm-up, and stop-gradient cuts variance sequentially; replacing the step-level policy loss with VL-DAC’s token-level objective yields the smooth ascent reported in Figure3.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2508.04280/x5.png",
                "caption": "Figure 5:Long-horizon credit assignment: VL-DAC vs. LOOP. On four sparse-reward MiniWorld tasks, LOOP plateaus once early successes exhaust its high-variance sequence-level gradient, whereas VL-DAC continues improving. Token-wise advantages coupled with a step-wise critic unlock sustained learning.",
                "position": 525
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix A: Input example",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04280/x6.png",
                "caption": "Figure 6:Example of a prompt template for MiniWorld environments.",
                "position": 1462
            }
        ]
    },
    {
        "header": "Appendix BAppendix B: Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix CAppendix C: Qwen2-VL-7b Evaluation Setup",
        "images": []
    },
    {
        "header": "Appendix DAppendix D: Detailed Training Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04280/x7.png",
                "caption": "Figure 7:Episode success rates without averaging over the thought-probability coefficient.",
                "position": 1730
            },
            {
                "img": "https://arxiv.org/html/2508.04280/x8.png",
                "caption": "Figure 8:20Q LMRL performance curves for on-policy and off-policy ArCHer.",
                "position": 1740
            }
        ]
    },
    {
        "header": "Appendix EAppendix E: ArCHer On-policy runs",
        "images": []
    }
]