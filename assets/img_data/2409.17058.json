[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17058/x1.png",
                "caption": "Figure 1:Comparison of performance and complexity among DM-based SR methods on the DIV2K-Val dataset(Agustsson and Timofte,2017). Metrics like LPIPS, DISTS, NIQE, FID, and inference time, where smaller scores indicate better image quality, are inverted. All metrics are normalized for better visualization.S3Diffattains top-tier performance in both image quality and complexity with just a single forward pass.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x2.png",
                "caption": "Figure 2:Qualitative comparisons on one typical real-world example of the proposed method and the most recent state-of-the-arts, including SinSR(Wang et al.,2024b)and OSEDiff(Wu et al.,2024). (Zoom in for details)",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17058/x3.png",
                "caption": "Figure 3:Overview ofS3Diff. We enhance a pre-trained diffusion model for one-step SR by injecting LoRA layers into the VAE encoder and UNet. Additionally, we employ a pre-trained Degradation Estimation Network to assess image degradation that is used to guide the LoRAs with the introduced block ID embeddings. We tailor a new training pipeline that includes an online negative prompting, reusing generated LR images with negative text prompts. The network is trained with a combination of a reconstruction loss and a GAN loss.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x4.png",
                "caption": "Figure 4:We demonstrate images generated from various steps using the pre-trained SD-Turbo, both with and without text prompts.",
                "position": 204
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17058/x5.png",
                "caption": "Figure 5:Qualitative comparisons of different methods on the synthesis dataset,DIV2K-Val(Agustsson and Timofte,2017).(Zoom in for details)",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x6.png",
                "caption": "Figure 6:Qualitative comparisons of different methods on the real-world dataset.(Zoom in for details)",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x7.png",
                "caption": "",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x8.png",
                "caption": "Figure 7:Qualitative comparisons of different guidance scales using CFG. (Zoom in for details)",
                "position": 1271
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x9.png",
                "caption": "Figure 8:Qualitative comparisons of injecting noise of different levels into LR images as input. (Zoom in for details)",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x10.png",
                "caption": "Figure 9:Comparisons of SR performance (LPIPS and MANIQA scores) and convergence speed between different diffusion priors. Visualization results on the RealSR dataset demonstrate the advantages of using diffusion priors. (Zoom in for details)",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x11.png",
                "caption": "Figure 10:Qualitative comparisons of using different text prompts. (Zoom in for details)",
                "position": 1433
            },
            {
                "img": "https://arxiv.org/html/2409.17058/x12.png",
                "caption": "Figure 11:Visual comparison on different degradation inputs. We replace the noise score or the blur score of estimated degradation with 0, 0.5, or 1.0 in our experiments. (Zoom in for details)",
                "position": 1514
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Data Availability Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]