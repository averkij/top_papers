[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/x1.png",
                "caption": "Figure 1:Comparison of existing GUI RL framework with our HCPO framework. HCPO jointly improves the sampling and update phases of training by integrating Dynamic Context Sampling(DCS)and Anchor-guided History Compression(AHC).",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/figures/iclr_bar_winrate.png",
                "caption": "Figure 2:Different samples prefer different history lengths.Left: For each sample we evaluate a set of different history lengthsτ\\tauand take theτ\\tauthat yields the highest mean reward. The preferredτ\\taudiffers across samples and action types.Right: Providing more history does not necessarily yield the optimal result, suggesting effective usage of historical information is under exploration.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2512.01763/figures/hist_positive_delta_hlt2_gt_h2.png",
                "caption": "",
                "position": 159
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Rethinking History Usage: Limitations of Fixed Context and the Anchoring Role of Actions",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/x2.png",
                "caption": "Figure 3:Layer-wise token-drop analysis.Left:Schematic of the layer-wise token-drop probe, illustrating the information flow of image-drop and action-drop.Right:DroppingAhisA_{\\mathrm{his}}at shallow depths (k<12k{<}12) causes a much larger decline than droppingVhisV_{\\mathrm{his}}. Even\nif rich visual information is retained, later layers cannot directly extract effective cues fromVhisV_{\\mathrm{his}}without the action anchors. Askkincreases, the action-drop curve rises toward the image-drop curve and the image–action drop curve converges rapidly.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2512.01763/x3.png",
                "caption": "Figure 4:Overview of our history context-aware optimization framework for building HiconAgent. HCPO improves both the sampling and update phases of policy optimization by incorporating two key components: (1)Dynamic Context Sampling (DCS), which introduces varied history lengths during training to encourage context-effective decision-making, and (2)Anchor-guided History Compression (AHC), which adopts a dual-branch architecture where both branches share sampled responses and group-wise advantages. The compressed branch is trained using policy gradients, aligned with the uncompressed branch via a history-enhanced alignment loss.",
                "position": 248
            }
        ]
    },
    {
        "header": "5HiconAgent",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/figures/tau_comparison.png",
                "caption": "Table 1:Performances onAndroidControl-HighandGUI-Odyssey.Redindicates improvement,greenindicates degradation compared to GUI-R1-7B. Our 3B model outperforms GUI-R1-7B by+8.46% grounding and+11.32% SR on GUI-Odyssey.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2512.01763/figures/tau_comparison.png",
                "caption": "Figure 5:Evolution of the short-vs-long history reward ratio under uniformτ\\tausampling.\nThe declining ratio reflects the gradual degradation of short-history response quality during training.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2512.01763/x4.png",
                "caption": "Figure 6:To illustrate HCPO’s enhancement in leveraging historical information, we present two scenarios:\nLeft (Flight Booking) and Right (Shopping Task). Our model correctly inputsDelhiby reasoning over historical context and selectsRed Chiefdespite visual redundancy. While the base model trained without HCPO misinterprets history and fails in both cases.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2512.01763/figures/iclr_hcpo_android.png",
                "caption": "Figure 7:Per-action accuracy comparison before and after applying HCPO. Both AndroidControl (left) and GUI-Odyssey (right) benefit from history-compressed optimization, especially onfinishedactions, showing improved sequential decision quality.",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2512.01763/figures/iclr_hcpo_odyssey.png",
                "caption": "",
                "position": 900
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/figures/training_curve.png",
                "caption": "Figure 8:Training accuracy curves of Hicon-Agent with and without DCS under the AHC framework. Models trained with DCS exhibit consistently higher accuracy and faster convergence, demonstrating that adaptive history sampling facilitates more effective learning.",
                "position": 915
            }
        ]
    },
    {
        "header": "A. Visualization of SSR curve during training",
        "images": []
    },
    {
        "header": "B. Visualization of sampling distribution in DCS.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/figures/distribution.png",
                "caption": "Figure 9:Evolution of the sampling distribution from uniform exploration to exponentially biased selection as training progresses.",
                "position": 925
            }
        ]
    },
    {
        "header": "C. GUI datasets",
        "images": []
    },
    {
        "header": "D. Effect of history observation and compression",
        "images": []
    },
    {
        "header": "E. Algorithm details",
        "images": []
    },
    {
        "header": "F. Prompts for training and evaluation",
        "images": []
    },
    {
        "header": "G. Model Behavior Across Different History Lengths",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01763/x5.png",
                "caption": "Figure 10:Case study of model behavior under different history length context.",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2512.01763/x6.png",
                "caption": "(a)Trajectory example 1",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2512.01763/x6.png",
                "caption": "(a)Trajectory example 1",
                "position": 1603
            },
            {
                "img": "https://arxiv.org/html/2512.01763/x7.png",
                "caption": "(b)Trajectory example 2",
                "position": 1608
            }
        ]
    },
    {
        "header": "H. Visualization of successful trajectories",
        "images": []
    }
]