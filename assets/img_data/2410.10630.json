[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Thought Preference Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10630/x1.png",
                "caption": "Figure 1:Thought Preference Optimization:We start by prompting the LLM to generate thoughts before its response. After sampling different outputs, we feed the response parts to the judge model which determines the best and worst ones. Then we use the corresponding full outputs as chosen and rejected pairs for DPO optimization. We perform multiple iterations of this training.",
                "position": 164
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10630/x2.png",
                "caption": "Figure 3:Training iterations on AlpacaEval (left) and Arena-Hard (right), comparing our TPO method to the direct baseline starting from the seed (iteration 0) model.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x3.png",
                "caption": "",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x4.png",
                "caption": "Figure 4:Fine-Grained evaluation on unseen instructions from UltraFeedback, broken down by category.We measure the win rate of TPO against the direct baseline as judged by GPT4.",
                "position": 594
            }
        ]
    },
    {
        "header": "4Related work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFine-grained evaluation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10630/x5.png",
                "caption": "Figure 7:Training instruction distribution: we categorize our training instructions into 20 topics. The language and translation category has the most examples, while the math category is only 2.2% of the data.",
                "position": 1559
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x6.png",
                "caption": "Figure 8:Additional Fine-grained evaluation results.We show the win rate of TPO against the direct baseline trained in the same setup. The different plots correspond to setups differing in evaluation judge model, thought prompt type, and the training judge.",
                "position": 1562
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x7.png",
                "caption": "",
                "position": 1568
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x8.png",
                "caption": "",
                "position": 1570
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x9.png",
                "caption": "",
                "position": 1572
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x10.png",
                "caption": "Figure 9:Fine-grained evaluation on AlpacaEval instructions. We measure length-controlled win rates within each category for each model.",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x11.png",
                "caption": "Figure 14:Length of thoughts generated by TPO models.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2410.10630/x12.png",
                "caption": "",
                "position": 1718
            }
        ]
    },
    {
        "header": "Appendix BELO computation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10630/extracted/5925631/figs/armo_reward.png",
                "caption": "Figure 21:The final model response lengths are shown with their corresponding ArmoRM average score on the UltraFeedback validation set. All models shown are at iteration 4.",
                "position": 2363
            }
        ]
    },
    {
        "header": "Appendix CThought Examples",
        "images": []
    }
]