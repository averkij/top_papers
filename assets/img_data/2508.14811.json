[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/teaser.png",
                "caption": "Figure 1:Compared with prior 3D editing approaches,Tinkerremoves the necessity of labor-intensive per-scene fine-tuning–—whether for generating multi-view-consistent edited inputs for 3DGS optimization or for preserving consistency through scene-specific training.Moreover,Tinkeris capable of performing both object-level and scene-level 3D editing, and achieves high-quality results in few-shot as well as one-shot settings.Please\nrefer to FiguresS4,S5,S6,S7for more compelling visualizations.",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14811/x1.png",
                "caption": "Figure 2:(a) FLUX Kontext achieves multi-view consistent image editing by horizontally concatenating two images and editing them jointly. Although it ensures consistency between the concatenated image pair, significant inconsistencies remain across different image pairs.\n(b) We also evaluated whether FLUX Kontext can edit one half of the concatenated image by referencing the other half. The results demonstrate that the model lacks this capability.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14811/x2.png",
                "caption": "Figure 3:Overview of our editing process. We first apply our multi-view consistent editing model to obtain coherent sparse views. Leveraging depth constraints from the rendered results, we generate a large number of consistent edited images. The edited images are used to optimize the 3DGS to achieve high-quality 3D editing.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x3.png",
                "caption": "Figure 4:(a) We leverage the base FLUX Kontext model to generate a large number of consistent image pairs and discard the inconsistent and fail-to-edit results.\n(b) The data generated in (a) is used to fine-tune the model. Specifically, we horizontally concatenate the input image and an edited image, and train the model using LoRA to learn how to perform referring-based editing.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x4.png",
                "caption": "Figure 5:We build our Scene Completion Model on top of WAN2.1. In this figure, contours with the same color indicate the elements using the same positional embedding. During training, we compute the flow matching loss only between the model’s outputs corresponding to the noisy latents and the target video.",
                "position": 306
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/exp.jpg",
                "caption": "Figure 6:Qualitative comparisons of novel views in different methods.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x5.png",
                "caption": "Figure 7:Qualitative comparisons before and after multi-view consistent image editing fine-tuning. After fine-tuning, our model can perform edits guided by the provided reference image, effectively ensuring the global consistency.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x6.png",
                "caption": "Figure 8:Effect of the number of horizontally concatenated images on visual quality. Concatenating too many images leads to a significant degradation in image quality, while concatenating two images yields the best results.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x7.png",
                "caption": "Figure 9:Tinkerdemonstrates the ability to refine blurry regions, recovering sharper structures and finer details while maintaining overall visual consistency.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/video_recon.png",
                "caption": "Figure 10:Tinkerdemonstrates the capability of high-quality video reconstruction with only the first frame and depth maps as input.",
                "position": 747
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/3dgs.png",
                "caption": "Figure S1:Visualizations of edited 3DGS and renderings using NeRFStudio.",
                "position": 1746
            }
        ]
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14811/x8.png",
                "caption": "Figure S2:Qualitative comparisons of different scene completion methods. Conditioning on depth produces results with one-to-one corresponding camera poses, while achieving superior geometry and detail preservation without being restricted to first and last frame as inputs.",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x9.png",
                "caption": "Figure S3:Comparison with VACE in both depth-guided video generation and mask-based editing. Our method demonstrates superior multi-view consistency and better preservation of fine details.",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/oneshot1.jpg",
                "caption": "Figure S4:Additional one-shot editing results without per-scene fine-tuning.",
                "position": 1918
            },
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/oneshot2.jpg",
                "caption": "Figure S5:Additional one-shot editing results without per-scene fine-tuning.",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/fewshot1.jpg",
                "caption": "Figure S6:Additional few-shot editing results without per-scene fine-tuning.",
                "position": 1928
            },
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/fewshot2.jpg",
                "caption": "Figure S7:Additional few-shot editing results without per-scene fine-tuning.",
                "position": 1933
            },
            {
                "img": "https://arxiv.org/html/2508.14811/x10.png",
                "caption": "Figure S8:Input to a multi-modal large model for the generation of editing prompts.",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2508.14811/tinker/Figures/data_sample.png",
                "caption": "Figure S9:Examples from our synthesized multi-view consistent editing dataset. The dataset covers a wide variety of editing, including different weather conditions, lighting setups, and artistic styles.",
                "position": 1943
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Discussions",
        "images": []
    }
]