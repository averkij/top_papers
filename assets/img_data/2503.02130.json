[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02130/x1.png",
                "caption": "Figure 1:Default architecture of FoX. (left) A single FoX block. (right) A single FoX (Pro) layer. All RMSNorms on the right are applied independently to each head.œÉùúé\\sigmaitalic_œÉis the sigmoid function.‚äótensor-product\\otimes‚äóis element-wise multiplication.ShiftLinearimplements the computation in Equation14.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x2.png",
                "caption": "",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x3.png",
                "caption": "Figure 2:(left) Per-token lossL‚Å¢(i)ùêøùëñL(i)italic_L ( italic_i )at different token positioniùëñiitalic_i. (right) Validation perplexityP‚Å¢(l)ùëÉùëôP(l)italic_P ( italic_l )over different validation context lengthlùëôlitalic_l. The vertical dashed line indicates the training context length. The per-token loss is smoothed using a moving average sliding window of101101101101tokens.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x4.png",
                "caption": "Figure 3:Visualization of the forget gate weight matrixùë≠ùë≠{\\bm{F}}bold_italic_Fand the attention score matrixùë®ùë®{\\bm{A}}bold_italic_Afrom two heads in different layers. Sinceùë®ùë®{\\bm{A}}bold_italic_Ais very sparse, we only show entries with scores larger than0.50.50.50.5. These results use FoX (Pro). More examples can be found in AppendixF.10.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x5.png",
                "caption": "",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x6.png",
                "caption": "",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x7.png",
                "caption": "Figure 4:Needle-in-the-haystack analysis for different models. We show results for the easy mode on the left and the standard mode on the right. The results are scored on a scale of1111to10101010by GPT-4o-2024-08-06. The vertical dashed line indicates the training context length.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x8.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x9.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x10.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x11.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x12.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x13.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x14.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x15.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x16.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x17.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x18.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x19.png",
                "caption": "",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x20.png",
                "caption": "Figure 5:FoX (Pro) easy mode needle-in-the-haystack results and per-token loss for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length. More results can be found in AppendixF.7.",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x21.png",
                "caption": "",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x22.png",
                "caption": "Figure 6:Per-token loss given different model sizes, numbers of training tokens, and training context lengths. At each token indexiùëñiitalic_i, we report the averaged loss over a window of101101101101centered atiùëñiitalic_i. We only show results within the training context length to reduce visual clutter. See AppendixF.6for additional results, including length extrapolation and 125M-parameter model results.",
                "position": 922
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x23.png",
                "caption": "",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x24.png",
                "caption": "Figure 7:Data-dependent forget gate (data-dep) vs. data-independent (data-indep) and fixed forget gate.\n(leftandmiddle-left) Comparison using the LLaMA architecture. (middle-rightandright) Comparison using the Pro architecture. We use360360360360M-parameter models trained on roughly7.57.57.57.5B tokens on LongCrawl64. All per-token loss curves are smoothed with a moving average sliding window of1001100110011001tokens. The vertical dashed line indicates the training context length.",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x25.png",
                "caption": "",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x26.png",
                "caption": "",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x27.png",
                "caption": "",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x28.png",
                "caption": "Figure 8:Per-token loss for the incremental-style ablation studies presented in Section4.5. All models are roughly360360360360M parameters and are trained on roughly7.57.57.57.5B tokens on LongCrawl64. The vertical line indicates the training context length.",
                "position": 2852
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x29.png",
                "caption": "Figure 9:Per-token loss for the perturbation-style ablation studies presented in Section4.5. All models are roughly360360360360M parameters and are trained on roughly7.57.57.57.5B tokens on LongCrawl64. The vertical line indicates the training context length.",
                "position": 2858
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x30.png",
                "caption": "Figure 10:Removing RoPE from Transformer (LLaMA) results in poor performance. All models are roughly360360360360M parameters and are trained on roughly7.57.57.57.5B tokens on LongCrawl64. The vertical line indicates the training context length.",
                "position": 2864
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x31.png",
                "caption": "Figure 11:Incremental style ablation study for Transformer (Pro). All models are roughly125125125125M parameters and are trained on roughly2.72.72.72.7B tokens on LongCrawl64. The vertical line indicates the training context length.",
                "position": 2877
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x32.png",
                "caption": "Figure 12:Results on SlimPajama with a training context length of2048204820482048tokens. All models have roughly340340340340M non-embedding parameters and are trained on roughly15151515B tokens on SlimPajama. The vertical line indicates the training context length.",
                "position": 2893
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x33.png",
                "caption": "Figure 13:Additional comparison with Samba and Transformer-SWA. (left) Per-token lossL‚Å¢(i)ùêøùëñL(i)italic_L ( italic_i )at different token positioniùëñiitalic_i. (right) Validation perplexityP‚Å¢(l)ùëÉùëôP(l)italic_P ( italic_l )over different validation context lengthlùëôlitalic_l. All models have760760760760M parameters and are trained on roughly16161616B tokens. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using a moving average sliding window of101101101101tokens. In this plot1‚Å¢k=10241ùëò10241k=10241 italic_k = 1024.",
                "position": 3033
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x34.png",
                "caption": "Figure 14:Easy mode needle-in-the-haystack analysis for FoX, the Transformer, Mamba-2, Samba, and the Transformer with sliding window attention. These are 760M-parameter models trained on 16B tokens on LongCrawl64. The results are scored on a scale of1111(red) to10101010(green) by GPT-4o. The vertical dashed line indicates the training context length.",
                "position": 3396
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x35.png",
                "caption": "",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x36.png",
                "caption": "",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x37.png",
                "caption": "",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x38.png",
                "caption": "",
                "position": 3406
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x39.png",
                "caption": "",
                "position": 3406
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x40.png",
                "caption": "",
                "position": 3406
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x41.png",
                "caption": "Figure 15:Needle-in-the-haystack analysis for HGRN2. The results are scored on a scale of1111(red) to10101010(green) by GPT-4o. The vertical dashed line indicates the training context length.",
                "position": 3419
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x42.png",
                "caption": "",
                "position": 3421
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x43.png",
                "caption": "Figure 16:Results with 125M-parameter models trained on roughly 2.7B tokens. (left) Per-token lossL‚Å¢(i)ùêøùëñL(i)italic_L ( italic_i )at different token positioniùëñiitalic_i. (right) Validation perplexityP‚Å¢(l)ùëÉùëôP(l)italic_P ( italic_l )over different validation context lengthlùëôlitalic_l. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using a moving average sliding window of101101101101tokens. In this plot1‚Å¢k=10241ùëò10241k=10241 italic_k = 1024.",
                "position": 3443
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x44.png",
                "caption": "Figure 17:Results with 360M-parameter models trained on roughly 7.5B tokens. (left) Per-token lossL‚Å¢(i)ùêøùëñL(i)italic_L ( italic_i )at different token positioniùëñiitalic_i. (right) Validation perplexityP‚Å¢(l)ùëÉùëôP(l)italic_P ( italic_l )over different validation context lengthlùëôlitalic_l. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using a moving average sliding window of101101101101tokens. In this plot1‚Å¢k=10241ùëò10241k=10241 italic_k = 1024.",
                "position": 3446
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x45.png",
                "caption": "Figure 18:Results with 760M-parameter models trained on roughly 16B tokens. (left) Per-token lossL‚Å¢(i)ùêøùëñL(i)italic_L ( italic_i )at different token positioniùëñiitalic_i. (right) Validation perplexityP‚Å¢(l)ùëÉùëôP(l)italic_P ( italic_l )over different validation context lengthlùëôlitalic_l. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using a moving average sliding window of101101101101tokens. In this plot1‚Å¢k=10241ùëò10241k=10241 italic_k = 1024.",
                "position": 3449
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x46.png",
                "caption": "Figure 19:Per-token loss given different training context lengths for the 125M-parameter/2.7B-token setting. (left) Results for the LLaMA models. (right) Results for the Pro models. At each token indexiùëñiitalic_i, we report the averaged loss over a window of101101101101centered atiùëñiitalic_i.",
                "position": 3452
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x47.png",
                "caption": "",
                "position": 3454
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x48.png",
                "caption": "Figure 20:Per-token loss given different training context lengths for the 350M-parameter/7.5B-token setting. (left) Results for the LLaMA models. (right) Results for the Pro models. At each token indexiùëñiitalic_i, we report the averaged loss over a window of101101101101centered atiùëñiitalic_i.",
                "position": 3458
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x49.png",
                "caption": "",
                "position": 3460
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x50.png",
                "caption": "Figure 21:Per-token loss given different training context lengths for the 760M-parameter/16B token setting. (left) Results for the LLaMA models. (right) Results for the Pro models. At each token indexiùëñiitalic_i, we report the averaged loss over a window of101101101101centered atiùëñiitalic_i.",
                "position": 3464
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x51.png",
                "caption": "",
                "position": 3466
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x52.png",
                "caption": "Figure 22:FoX (Pro) and Transformer (Pro) easy mode needle-in-the-haystack results for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length.",
                "position": 3478
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x53.png",
                "caption": "",
                "position": 3488
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x54.png",
                "caption": "Figure 23:FoX (Pro) and Transformer (Pro) standard mode needle-in-the-haystack results for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length.",
                "position": 3494
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x55.png",
                "caption": "",
                "position": 3504
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x56.png",
                "caption": "Figure 24:FoX (Pro) and Transformer (Pro) per-token loss for different numbers of training tokens and learning rates. The vertical dashed line indicates the training context length.",
                "position": 3510
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x57.png",
                "caption": "",
                "position": 3519
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x58.png",
                "caption": "Figure 25:Training curves of different models presented in Figure2. These curves show the training loss averaged every512√ó220512superscript220512\\times 2^{20}512 √ó 2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPTtokens. All models have760760760760M parameters.",
                "position": 3532
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x59.png",
                "caption": "Figure 26:Result stability across seeds with360360360360M-parameter FoX (LLaMA). All models are trained on roughly7.57.57.57.5B tokens. The vertical dashed line indicates the training context length. The per-token loss is typically noisy, so we smooth the curve using a moving average sliding window of101101101101tokens. In this plot1‚Å¢k=10241ùëò10241k=10241 italic_k = 1024.",
                "position": 3542
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x60.png",
                "caption": "Figure 27:Visualization of the forget gate weight matrixùë≠ùë≠{\\bm{F}}bold_italic_Ffrom 16 heads in 4 different layers. These results use FoX (Pro).",
                "position": 3552
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x61.png",
                "caption": "",
                "position": 3556
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x62.png",
                "caption": "",
                "position": 3556
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x63.png",
                "caption": "",
                "position": 3556
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x64.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x65.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x66.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x67.png",
                "caption": "",
                "position": 3562
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x68.png",
                "caption": "",
                "position": 3568
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x69.png",
                "caption": "",
                "position": 3568
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x70.png",
                "caption": "",
                "position": 3568
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x71.png",
                "caption": "",
                "position": 3568
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x72.png",
                "caption": "",
                "position": 3574
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x73.png",
                "caption": "",
                "position": 3574
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x74.png",
                "caption": "",
                "position": 3574
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x75.png",
                "caption": "",
                "position": 3574
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x76.png",
                "caption": "Figure 28:Visualization of the attention score matrixùë®ùë®{\\bm{A}}bold_italic_Afrom 16 heads in 4 different layers. These results use FoX (Pro).",
                "position": 3580
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x77.png",
                "caption": "",
                "position": 3584
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x78.png",
                "caption": "",
                "position": 3584
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x79.png",
                "caption": "",
                "position": 3584
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x80.png",
                "caption": "",
                "position": 3590
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x81.png",
                "caption": "",
                "position": 3590
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x82.png",
                "caption": "",
                "position": 3590
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x83.png",
                "caption": "",
                "position": 3590
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x84.png",
                "caption": "",
                "position": 3596
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x85.png",
                "caption": "",
                "position": 3596
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x86.png",
                "caption": "",
                "position": 3596
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x87.png",
                "caption": "",
                "position": 3596
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x88.png",
                "caption": "",
                "position": 3602
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x89.png",
                "caption": "",
                "position": 3602
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x90.png",
                "caption": "",
                "position": 3602
            },
            {
                "img": "https://arxiv.org/html/2503.02130/x91.png",
                "caption": "",
                "position": 3602
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]