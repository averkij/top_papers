[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15885/x1.png",
                "caption": "Figure 1:Visualization comparison between the baseline (MCTformerXuet al.(2022)) and our method. The baseline model results exhibit background noise (object co-occurrence) and less precise localization (under-activation), as highlighted by red arrows and white bounding boxes. In contrast, our proposed method effectively reduces background activation and enhances focus on target regions, as highlighted by yellow arrows.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2502.15885/x2.png",
                "caption": "Figure 2:(a) The image and query targets (⋆⋆\\star⋆). (b) The self-attention maps in the Transformer block capture semantic-level relationships at various granularities. The high-activation regions learned by each layer not only provide critical information that subsequent layers may miss but also generate CAMs that often focus on different regions. This broadens the coverage of target feature areas, effectively mitigating the tendency of activation maps to focus excessively on local salient regions.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15885/x3.png",
                "caption": "Figure 3:An overview of the proposed method. The RGB image undergoes transformation into class tokens and patch tokens via the embedding layer. The dot-product attention mechanism is then employed to compute the attention score matrix and generate output tokens. To further refine the attention score matrix, the cosine similarity between the RGB values of the original image and the tokens is used to adjust the distribution of attention weights. Furthermore, the CPDO and PPDO methods are customized to amplify high-confidence information and suppress the influence of low-confidence information. Finally, the optimized tokens are incorporated into the original tokens as residuals, producing refined output tokens for subsequent computations in the encoder.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15885/x4.png",
                "caption": "Figure 4:The impact of different hyper-parameter values in the CPDO and PPDO modules on mIoU.",
                "position": 829
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Datasets and Evaluation protocol",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15885/x5.png",
                "caption": "Figure 1:Impact of varying numbers of embedding optimization layers on the mIoU, FP, and FN of the final CAM.",
                "position": 1563
            }
        ]
    },
    {
        "header": "2Parameter Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15885/x6.png",
                "caption": "Figure 2:Visualization of the CAMs of input images generated by different methods. (a)(e) Input; (b)(f) Baseline; (c)(g) Dual Optimization of Embedding Information (DOEI); (d)(h) GT;",
                "position": 1579
            },
            {
                "img": "https://arxiv.org/html/2502.15885/x7.png",
                "caption": "Figure 3:Impact of varying numbers of embedding optimization layers on the mIoU, FP, and FN of the final CAM.",
                "position": 1591
            }
        ]
    },
    {
        "header": "3Qualitative Analysis",
        "images": []
    }
]