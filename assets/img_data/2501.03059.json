[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03059/x1.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03059/x2.png",
                "caption": "Figure 2:Overview of our I2V framework, transforming a reference imagex(0)superscriptğ‘¥0x^{(0)}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPTand text promptcğ‘citalic_cinto a coherent video sequencex^^ğ‘¥\\hat{x}over^ start_ARG italic_x end_ARG. A pre-trained LLM is used to derive the motion-specific promptcmâ¢oâ¢tâ¢iâ¢oâ¢nsubscriptğ‘ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›c_{motion}italic_c start_POSTSUBSCRIPT italic_m italic_o italic_t italic_i italic_o italic_n end_POSTSUBSCRIPTand object-specific promptsclâ¢oâ¢câ¢aâ¢l={clâ¢oâ¢câ¢aâ¢l(1),â€¦,clâ¢oâ¢câ¢aâ¢l(L)}subscriptğ‘ğ‘™ğ‘œğ‘ğ‘ğ‘™superscriptsubscriptğ‘ğ‘™ğ‘œğ‘ğ‘ğ‘™1â€¦superscriptsubscriptğ‘ğ‘™ğ‘œğ‘ğ‘ğ‘™ğ¿c_{local}=\\{c_{local}^{(1)},\\dots,c_{local}^{(L)}\\}italic_c start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT = { italic_c start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , â€¦ , italic_c start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT }, capturing each objectâ€™s intended motion. We generate an initial segmentation masks(0)superscriptğ‘ 0s^{(0)}italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPTfromx(0)superscriptğ‘¥0x^{(0)}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPTusing SAM2. In Stage 1, the Image-to-Motion utilizesx(0)superscriptğ‘¥0x^{(0)}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT,s(0)superscriptğ‘ 0s^{(0)}italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, andcmâ¢oâ¢tâ¢iâ¢oâ¢nsubscriptğ‘ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›c_{motion}italic_c start_POSTSUBSCRIPT italic_m italic_o italic_t italic_i italic_o italic_n end_POSTSUBSCRIPTto generate mask-based motion trajectoriess^^ğ‘ \\hat{s}over^ start_ARG italic_s end_ARGthat represent object-specific movement paths. In Stage 2, the Motion-to-Video takes as inputx(0)superscriptğ‘¥0x^{(0)}italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, the generated trajectoriess^^ğ‘ \\hat{s}over^ start_ARG italic_s end_ARG, the text promptcğ‘citalic_cas a global condition, and object-specific promptsclâ¢oâ¢câ¢aâ¢lsubscriptğ‘ğ‘™ğ‘œğ‘ğ‘ğ‘™c_{local}italic_c start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPTthrough a masked attention blocks (Section3.3), producing the final videox^^ğ‘¥\\hat{x}over^ start_ARG italic_x end_ARG.",
                "position": 137
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03059/x3.png",
                "caption": "Figure 3:Illustration of the masked attention block.Squares represent video latent patches, color-coded to indicate objects (e.g., cat or dog). Triangles denote prompt tokens: gray for global prompts and object-specific colors for local prompts. The pipeline features self-attention for all patches, masked self-attention restricted to each object, cross-attention integrating global prompts, and masked cross-attention aligning object-specific prompts.",
                "position": 211
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03059/x4.png",
                "caption": "Figure 4:Qualitative comparison: Visual examples of generated videos forThrough-The-Maskcompared to the TI2V baseline on examples from theSA-V-128benchmark.",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2501.03059/x5.png",
                "caption": "Figure 5:Qualitative comparison of generated videos using segmentation masks vs optical flow as an intermediate motion representation.\nThe first row shows the input image and text, the second row displays the generated masks, and the third row presents the generated optical flow. The fourth and fifth rows show the generated videos, with the fourth row using our mask-based model and the fifth using our flow-based model.",
                "position": 782
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Additional Results",
        "images": []
    },
    {
        "header": "7Motion and Object-Specific Prompts Details",
        "images": []
    },
    {
        "header": "8Motion-capable Objectsâ€™ Prompt Extraction Details",
        "images": []
    },
    {
        "header": "9Inference",
        "images": []
    },
    {
        "header": "10Implementation Details",
        "images": []
    },
    {
        "header": "11SA-V-128Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03059/x6.png",
                "caption": "Figure 6:Qualitative comparison of generated videos for each configuration ofThrough-The-Mask. The results highlight differences when applying masked cross-attention (With cross-attn), self-attention (With self-attn), both (Ours), or no masked attention layers (No mask attn). Without masked attention, the cartoon superhero fails to perform a prayer. With masked self-attention, the superhero also fails, but the movement appears smoother and more consistent. With masked cross-attention, the superhero successfully performs the prayer, though his fingers turn blue. When integrating the full masked attention mechanism, the superhero performs the action correctly.",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2501.03059/x7.png",
                "caption": "Figure 7:Qualitative comparison of generated videos using segmentation masks vs optical flow as an intermediate motion representation. The first row shows the input image and text, the second row displays the generated masks, and the third row presents the generated optical flow. The fourth and fifth rows show the generated videos, with the fourth row using our mask-based model and\nthe fifth using our flow-based model.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2501.03059/x8.png",
                "caption": "Figure 8:Qualitative comparison of video generations produced byThrough-The-Mask(DiT-based) and the TI2V baseline (DiT-based).",
                "position": 1611
            },
            {
                "img": "https://arxiv.org/html/2501.03059/x9.png",
                "caption": "Figure 9:Qualitative comparison of video generations produced byThrough-The-Mask(U-Net-based) and TI2V (U-Net-based), ConsistI2V, Motion-I2V, DynamiCrafter, and VideoCrafter.",
                "position": 1614
            }
        ]
    },
    {
        "header": "12Image-Animation-Bench",
        "images": []
    }
]