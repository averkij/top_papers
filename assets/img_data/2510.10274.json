[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x1.png",
                "caption": "Figure 1:X-VLAemploys distinctive learnable embeddings, referred to assoft prompt, to effectively address the heterogeneity present in cross-embodiment datasets. This approach, combined with stacking simple self-attention transformer blocks, provides a scalable solution for integrating diverse pretraining datasets and finetuning for a variety of domain-specific applications. Evaluated across 6 simulation benchmark including one autonomous driving bench and 3 real-world robots,X-VLAachieves SOTA performance over most benchmark suites and real-world robotic tasks.",
                "position": 176
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x2.png",
                "caption": "Figure 2:Comparison among four methods in handling heterogeneity in cross-embodiment training.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2510.10274/x3.png",
                "caption": "Figure 3:The recipe for mixed data used in pretraining experiments.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2510.10274/Figures/line_plot_scaling_steps.png",
                "caption": "Figure 4:Training curves for various methods of handling heterogeneity.",
                "position": 249
            }
        ]
    },
    {
        "header": "3Heterogeneous Soft Prompt Learning",
        "images": []
    },
    {
        "header": "4X-VLA: Soft-Prompted Transformer Enhanced VLA model",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x4.png",
                "caption": "Figure 5:With increased compute, data diversity, and data volume,X-VLAcan output reduced validation prediction error, which can lead to enhanced adaptation performance as discussed by Tab.1.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2510.10274/x5.png",
                "caption": "Figure 6:The evaluated setups in adaptation experiments.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2510.10274/x6.png",
                "caption": "Figure 7:Real-World Evaluation Results. We evaluate ourX-VLAmodel on three distinct real-world embodiments, each under specific task setups, including simple manipulation, dexterous manipulation, and fast adaptation experiments using PEFT techniques. See AppendixJfor for details",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2510.10274/x7.png",
                "caption": "Figure 8:T-SNE visualization of soft prompts on 7 data sources.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2510.10274/Figures/prompt_comparison.png",
                "caption": "Figure 9:Comparison of different prompts on PEFT.",
                "position": 1022
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALLM Usage and Ethics Statement",
        "images": []
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix CArchitecture Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x8.png",
                "caption": "Figure 10:Illustration of the detailed architecture of our model. Most parameters are shared across different embodiments, with the exception of the soft prompt and input/output linear projections for action-related tokens. These unshared parameters account for only a small fraction of the total parameters (0.04%). For each dataset, the corresponding domain-specific parameters are queried. The image inputs and language instructions are processed by pretrained Vision-Language Models (VLMs). Notably, only the main view is passed through the entire VLM, while additional views, such as the wrist view, are directed only to the vision encoder. This approach helps preserve the pretrained VLM’s capability, as current VLMs have limited multi-view perception. The proprioception and flow-matching time variables are repeated and concatenated with the noise action chunk, which is then projected using its specific projections. These features, along with the soft prompt and multi-modal tokens, are processed by stacking standard self-attention transformer blocks, enabling bi-directional information flow and effective fusion of all modalities. Finally, the control tokens are projected back to action chunks using domain-specific output projections.",
                "position": 2400
            }
        ]
    },
    {
        "header": "Appendix DMore Results",
        "images": []
    },
    {
        "header": "Appendix EFailure Attempts for Absorbing Heterogeneity",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x9.png",
                "caption": "Figure 11:The illustration of our proposed Soft-Fold datasets.",
                "position": 2559
            }
        ]
    },
    {
        "header": "Appendix FSoft-FOLD: Superior Dexterous Manipulation Model with a high-quality cloth folding dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x10.png",
                "caption": "Figure 12:The folding progress ofX-VLA-0.9B.",
                "position": 2575
            }
        ]
    },
    {
        "header": "Appendix GPretraining Details",
        "images": []
    },
    {
        "header": "Appendix HFinetuning Details",
        "images": []
    },
    {
        "header": "Appendix ITraining Details For Preliminary Experiments",
        "images": []
    },
    {
        "header": "Appendix JEvaluation Details in Real-World Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10274/x11.png",
                "caption": "Figure 13:Illustration of tasks used in the WidowX pick-and-place experiments. The selected tasks evaluate different aspects of generalization—Visual, Motion, Physical, and Semantic—following the setup in OpenVLA(Kim et al.,2024).",
                "position": 2859
            },
            {
                "img": "https://arxiv.org/html/2510.10274/x12.png",
                "caption": "Figure 14:Illustration of the hardware setups used in real-world experiments. We evaluate on three robotic embodiments, including WidowX, AgileX, and AIRBOT, covering diverse camera configurations and task domains to form a heterogeneous validation environment.",
                "position": 2871
            }
        ]
    },
    {
        "header": "Appendix KTraining Details of Baselines in Real-World Experiments",
        "images": []
    },
    {
        "header": "Appendix LEvaluation Details on Autonomous Driving Simulation Benchmark",
        "images": []
    },
    {
        "header": "Appendix MEvaluation Details on Robotics Simulation",
        "images": []
    },
    {
        "header": "Appendix NLimitations and future works",
        "images": []
    },
    {
        "header": "Appendix OContributions and Acknowledgments",
        "images": []
    }
]