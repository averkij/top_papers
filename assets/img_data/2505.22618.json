[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22618/x1.png",
                "caption": "(a)Throughput vs. Accuracy across methods",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x1.png",
                "caption": "(a)Throughput vs. Accuracy across methods",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x2.png",
                "caption": "(b)Throughput and tokens per step across methods",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x3.png",
                "caption": "(c)End-to-end speedup over vanilla LLaDA baseline.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22618/x4.png",
                "caption": "Figure 2:Illustration of our Key-Value Cache for Block-Wise Decoding.(a) During prefix-only caching, the KV cache is computed once for the prompt and reused across multiple decoding steps within each block. The cache is updated after completing a block to maintain consistency, with negligible overhead.\n(b) DualCache extends this approach by caching both prefix and masked suffix tokens, further accelerating decoding. The high similarity of KV activations across steps allows effective reuse with minimal approximation error.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x5.png",
                "caption": "(a)Prompt block",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x5.png",
                "caption": "(a)Prompt block",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x6.png",
                "caption": "(b)Last block",
                "position": 326
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22618/x7.png",
                "caption": "Figure 4:Impact of Cache Block Size on Accuracy and Throughput.The orange line illustrates the effect of varying cache block size on throughput, while the blue line depicts the corresponding impact on accuracy.",
                "position": 928
            },
            {
                "img": "https://arxiv.org/html/2505.22618/x8.png",
                "caption": "Figure 5:(a) The red line shows the GSM8K (5-shot) accuracy across different confidence thresholds. Numbers along the red line indicate the average number of tokens decoded at each step. The three dashed lines represent the accuracy of the baseline method when selecting the top 2, 4, or 8 tokens per step.\n(b) The number of inference steps required under varying confidence thresholds.\n(c) A comparison between our method and the baseline on GSM8K (5-shot) accuracy, plotted against the average number of tokens per step. Our method consistently outperforms the baseline.",
                "position": 937
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AProof",
        "images": []
    },
    {
        "header": "Appendix BCase Study",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]