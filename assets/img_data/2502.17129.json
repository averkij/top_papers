[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17129/x1.png",
                "caption": "Figure 2:Long-context performance of various LLMs across multiple benchmarks, perplexity (PPL)(Press et al.,2022), NIAH(Kamradt,2023), and RULER(Hsieh et al.,2024a). The horizontal axis represents the release time, while the vertical axis indicates the effective context length achieved by the LLMs on the corresponding task. The line associated with each task represents the state-of-the-art performance at a given point in time.",
                "position": 440
            }
        ]
    },
    {
        "header": "2Length Extrapolation",
        "images": []
    },
    {
        "header": "3KV Cache Optimization",
        "images": []
    },
    {
        "header": "4Memory Management",
        "images": []
    },
    {
        "header": "5Architecture Innovation",
        "images": []
    },
    {
        "header": "6Training Infrastructure",
        "images": []
    },
    {
        "header": "7Inference Infrastructure",
        "images": []
    },
    {
        "header": "8Long-Context Pre-training",
        "images": []
    },
    {
        "header": "9Long-Context Post-training",
        "images": []
    },
    {
        "header": "10Long-Context MLLM",
        "images": []
    },
    {
        "header": "11Long-Context Evaluation",
        "images": []
    },
    {
        "header": "12Unanswered Questions",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]