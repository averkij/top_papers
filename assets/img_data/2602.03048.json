[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03048/x1.png",
                "caption": "Figure 1:Comparison between GRPO-based methods and CoBA-RL.(a) GRPO employs a uniform strategy independent of training progress. (b) CoBA-RL dynamically self-calibrates the allocation strategy throughout the training process. It autonomously directs the rollout budget toward instances with high training value, aligned with the model’s evolving capability. In this visualization,pip_{i}denotes the pass rate corresponding to the task instancexix_{i}.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03048/x2.png",
                "caption": "Figure 2:Overview of CoBA-RL.(a) The training pipeline of CoBA-RL. (b) The adaptive value function influenced by model capability. (c) Illustration of the heap-based greedy allocation.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2602.03048/x3.png",
                "caption": "Figure 3:Performance comparison of different models and methods on the Olympiad benchmark (avg@16). The curves track the validation accuracy over training steps for GRPO, Knapsack, and CoBA-RL across varying model scales.",
                "position": 618
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03048/x4.png",
                "caption": "Figure 4:Visualization of different budget allocation across varying model capabilities on Qwen2.5-7B-instruct.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2602.03048/x5.png",
                "caption": "Figure 5:Evolution ofαt\\alpha_{t}on Qwen2.5-7B-Instruct.Left:The ”Exploit→\\toExplore” strategy, whereαt\\alpha_{t}exhibits a fluctuating downward trend.Right:The ”Explore→\\toExploit” strategy, whereαt\\alpha_{t}shows a fluctuating upward trend.",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2602.03048/x6.png",
                "caption": "Figure 6:Performance comparison under different exploration budget.",
                "position": 779
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition 3.2",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details of Main Training Loop",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03048/x7.png",
                "caption": "Figure 7:Task transition matrices for Qwen2.5-7B-Instruct during training. The cell(i,j)(i,j)indicates the percentage of samples transitioning from the initial statusiito the final statusjj.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2602.03048/x8.png",
                "caption": "Figure 8:Ablation study results illustrating the impact of different sum parameterκ∈{7,11,15,21}\\kappa\\in\\{7,11,15,21\\}on model performance.",
                "position": 1606
            },
            {
                "img": "https://arxiv.org/html/2602.03048/x9.png",
                "caption": "Figure 9:Visualization of the baseline strategies.Left & Middle:The probability density functions of the static Beta distributions used for exploitation ((α,β)=(10.5,1.5)(\\alpha,\\beta)=(10.5,1.5)) and exploration ((α,β)=(1.5,10.5)(\\alpha,\\beta)=(1.5,10.5)).Right:The pre-defined annealing schedule forα\\alphain the Linear Step Decay heuristic baseline.",
                "position": 1631
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]