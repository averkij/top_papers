[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14761/x1.png",
                "caption": "Figure 1:Three-stage Autoregressive U-Net (AU-Net).\nThe model executes from left to right. The contracting path compresses the sequence in two steps: Stage 1 processes raw bytes, Stage 2 keeps only the vector at each word boundary, and Stage 3 keeps one vector per two words. Each contraction and expansion step supports arbitrary pooling and upsampling patterns.\nAfter the deepest stage, the expanding path reverses the contracting path by duplicating each coarse vector and applying position-specific linear layers.\nThese are combined with skip connections from the contracting path, gradually restoring sequence length and blending in high-level information.\nDeeper stages predict further ahead and capture broad semantics, while shallower stages refine local detail.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14761/x2.png",
                "caption": "Figure 2:Pooling simply selects the vectors at the positions specified by the splitting function.\nUpsampling then expands each pooled vector to fill the next segment, applying a separate linear layer for each position.\nFor instance, the pooled vector representing the word ‘SAT␣’ is used to help predict ‘ON␣’.\nThis offset lets deeper stages predict further ahead in the sequence.\nWhen using 4 stages, for example, this results in the deepest stage helping for the prediction of the next four words.",
                "position": 278
            }
        ]
    },
    {
        "header": "3Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14761/x3.png",
                "caption": "Figure 3:Downstream task performance scaling with compute (1e19-1e22 FLOPs).\nAU-Net (2/3 stages) generally tracks a strong BPE Transformer baseline, which itself performs competitively against much larger models (e.g., LLaMa 3.1 8B on 15T tokens   100x compute).\nWhile AU-Net matches the baseline on tasks like Hellaswag and ARC Easy, and catches up on TQA at higher compute, its performance improvement phase on MMLU and GSM8K appears to start later.\nThe general underperformance on GSM8K is also linked to limited math data in the DCLM pretraining corpus.",
                "position": 2282
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Scaling Laws",
        "images": []
    },
    {
        "header": "7Regular expression",
        "images": []
    },
    {
        "header": "8Ablation",
        "images": []
    },
    {
        "header": "9Hyperparameters",
        "images": []
    },
    {
        "header": "10CUTE Benchmark Detailed results",
        "images": []
    },
    {
        "header": "11Evaluation Benchmarks Details",
        "images": []
    },
    {
        "header": "12List of Models",
        "images": []
    },
    {
        "header": "13Model Configuration Tables",
        "images": []
    }
]