[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07317/x1.png",
                "caption": "Figure 1:(a) During RL training, some array-sorting problems that were appropriately challenging become too easy, while others that were too hard become learnable as the policy improves (given the upward movement of thedark regioncontaining many problems for which some rollouts are correct, and others are not).\n(b)RLVEtrains an LM on verifiable environments that dynamically adjust problem difficulty based on its performance over time.\n(c) Starting fromProRL-1.5B-v2(Hu et al.,2025a),continuing training withRLVEyields a 3.37% absolute average improvement across six reasoning benchmarks, whereascontinuing the original RLVR trainingachieves a 0.49% average absolute gain using more than 3×\\timesthe compute.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07317/x2.png",
                "caption": "Figure 2:Illustration of adaptive difficulty enabled byRLVEwhen training a policy modelπ\\pion theSortingenvironment. Shown are theadaptive difficulty levelhπh_{\\pi}and the modelπ\\pi’saccuracy on problems generated from this levelat each step.\nWhenever the accuracy exceeds the thresholdτacc\\tau_{\\mathrm{acc}}(90%),RLVEincrementshπh_{\\pi}by 1, shifting the difficulty distribution to harder problems.",
                "position": 247
            }
        ]
    },
    {
        "header": "3RLVE-Gym: A Suite of 400 Environments Created through Environment Engineering",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07317/x3.png",
                "caption": "Figure 3:Comparison ofRLVE(using dynamically adjusted difficulty range) against three types of static difficulty ranges.\n(a) reports the effective prompt ratio, defined as the percentage of prompts retained after dynamic sampling whose rollouts yield non-identical rewards;\na higher ratio indicates fewer wasted rollouts and thus generally better learning efficiency.\n(b) shows in-distribution (ID) accuracies on the same training environment, and (c) shows out-of-distribution (OOD) accuracies on the 50 held-out verifiable environments.Adaptive difficulty maintains the highest effective prompt ratio and achieves superior ID and OOD performance,whereas static difficulty suffers from either early saturation or inefficient learning.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2511.07317/x4.png",
                "caption": "Figure 4:(a) shows the frequency distribution of the upper-bound difficulty levelshπ(i)h_{\\pi}^{(i)}reached by adaptive environments at step 400.\n(b) compares training jointly on 256 environments withadaptiveversusstaticdifficulty distributions.Despite covering all adaptive environments’ distributions, training on the static environments consistently underperforms.",
                "position": 439
            }
        ]
    },
    {
        "header": "4Analyzing Components ofRLVE",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07317/x5.png",
                "caption": "Figure 5:Comparison ofRLVEwith joint training on collections of four different sizes of verifiable environments, all under identical training setups.\nEach larger collection strictly contains all smaller ones.\nShown are the accuracies on 50 held-out verifiable environments throughout training.Expanding the collection of training environments consistently leads to better performance on held-out environments (unseen during training) across all model types.",
                "position": 475
            }
        ]
    },
    {
        "header": "5Scaling Up RL Training withRLVE",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07317/x6.png",
                "caption": "Figure 6:Comparison of RL training usingRLVE(jointly across all 400 verifiable environments inRLVE-Gym)versuscontinuing original RLVR training.\nThey both start fromProRL-1.5B-v2(Hu et al.,2025a), which was originally trained to saturation with RLVR on the ProRL dataset(Liu et al.,2025e).\nThe checkpoint for continued original training, provided byHu et al. (2025b), was obtained by further training on the same ProRL dataset.\nShown is the average performance across six reasoning benchmarks throughout training.RLVEeffectively scales RL training when the model has already saturated on a strong RLVR dataset.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2511.07317/x7.png",
                "caption": "Figure 7:Comparison of RL training usingRLVEjointly on all 400 verifiable environments inRLVE-GymagainstRLVR on the strong dataset DeepMath-103K(He et al.,2025), both starting from the same strong SFT modelOpenThinker3-1.5B(Guha et al.,2025).\nShown is the average performance across six reasoning benchmarks throughout training.Training withRLVEconsistently outperforms training on this existing high-quality RLVR dataset under the identical training setup.",
                "position": 594
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion on Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07317/x8.png",
                "caption": "Figure 8:Results of Figure6shown separately for each of the six reasoning benchmarks, as detailed in Section5.\nFor clarity, each curve has a corresponding horizontal line indicating its highest point.",
                "position": 1477
            },
            {
                "img": "https://arxiv.org/html/2511.07317/x9.png",
                "caption": "Figure 9:Results of Figure7shown separately for each of the six reasoning benchmarks, as detailed in Section5. For clarity, each curve has a corresponding dotted horizontal line indicating its highest point.",
                "position": 1482
            }
        ]
    },
    {
        "header": "Appendix ADetails ofRLVE",
        "images": []
    },
    {
        "header": "Appendix BDetails ofRLVE-Gym",
        "images": []
    },
    {
        "header": "Appendix CRL Training Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix EDetails of Training Environment Collection",
        "images": []
    }
]