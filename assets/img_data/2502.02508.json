[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02508/extracted/6178419/Figures/icon_wu.png",
                "caption": "",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02508/x1.png",
                "caption": "Figure 1:A High-level Overview of Satori Training Framework: Format Tuning (FT) + Self-improvement.First, Satori learns COAT reasoning format through imitation learning onsmall-scaledemonstration trajectories. Next, Satori further leverages COAT reasoning format to self-improve vialarge-scalereinforcement learning.",
                "position": 247
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02508/x2.png",
                "caption": "Figure 2:Number of Training Samples of Satori-Qwen-7B and Qwen-2.5-Math-7B-Instruct.Satori-Qwen-7B requires significantly less supervision (small-scale FT) and relies more on self-improvement (large-scale RL).",
                "position": 508
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02508/x3.png",
                "caption": "Figure 3:Policy Training Acc. & Response length v.s. RL Train-time Compute.Through RL training, Satori learns to improve its reasoning performance through longer thinking.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x4.png",
                "caption": "Figure 4:Above: Test-time Response Length v.s. Problem Difficulty Level; Below: Test-time Accuracy v.s. Problem Difficulty Level.Compared to FT model (Satori-Qwen-FT), Satori-Qwen uses more test-time compute to tackle more challenging problems.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x5.png",
                "caption": "Figure 5:Format Tuning v.s. Distillation.Distilling from a Stronger model (Satori-Qwen-7B) to weaker base models (Llama-8B and Granite-8B) are more effective than directly applying format tuning on weaker base models.",
                "position": 982
            }
        ]
    },
    {
        "header": "7Concluding Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASatori‚Äôs Demo Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02508/x6.png",
                "caption": "Figure 6:Math Domain Example.Satori verifies the correctness of the intermediate steps and proceeds to the next reasoning step.",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x7.png",
                "caption": "Figure 7:Math Domain Example.Satori identifies the mistakes in the previous solution and proposes an alternative correct solution.",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x8.png",
                "caption": "Figure 8:Math Domain Example.Satori verifies the correctness of previous solution and initiates a different solution.",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x9.png",
                "caption": "Figure 9:Math Domain Example.Satori verifies the correctness of previous solution and further explores a simpler solution.",
                "position": 1713
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x10.png",
                "caption": "Figure 10:Math Domain Example.1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.",
                "position": 1716
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x11.png",
                "caption": "Figure 11:Out-of-domain Example.1) Satori identifies the potential mistakes in intermediate steps and initiates another solution. 2) Satori realizes that the pervious solution is still erroneous and then proposes an alternative correct solution.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x12.png",
                "caption": "Figure 12:Out-of-domain Example.Satori identifies the potential mistakes in intermediate steps and initiates another correct solution.",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x13.png",
                "caption": "Figure 13:Out-of-domain Example.1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x14.png",
                "caption": "Figure 14:Out-of-domain Example.Satori engages in multiple self-reflection processes during intermediate reasoning steps.",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x15.png",
                "caption": "Figure 15:Out-of-domain Example.1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2502.02508/x16.png",
                "caption": "Figure 16:Out-of-domain Example.Satori identifies the mistakes in previous solution and proposes an alternative correct solution.",
                "position": 1734
            }
        ]
    },
    {
        "header": "Appendix BAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix CDetails about Data Synthesis Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02508/x17.png",
                "caption": "Figure 17:Demonstration Trajectories Synthesis.First, multiple initial reasoning trajectories are sampled from the generator and sent to critic to ask for feedback. The critic model identifies the mistake for trajectories with incorrect final answers and proposes an alternative solution. For trajectories with correct final answers, the critic model provides verification of its correctness. Based on the feedback, the generator self-refines its previous trajectories, and the incorrect trajectories are sent to the critic again for additional feedback with maximummùëömitalic_miterations. At each step, those trajectories with successful refinements are preserved and finally, a reward model rates and collects high-quality demonstration trajectories to form the synthetic datasetùíüsynsubscriptùíüsyn\\mathcal{D}_{\\text{syn}}caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT.",
                "position": 1771
            }
        ]
    },
    {
        "header": "Appendix DExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]