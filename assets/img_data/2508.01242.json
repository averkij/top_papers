[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x1.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x2.png",
                "caption": "Figure 2:Differences between LLaMA-Mesh and MeshLLM.\nLLaMA-Mesh applies a single text-mesh alignment optimization strategy on only 31k available meshes. In contrast, our proposed MeshLLM leverages local mesh patches and thus expands the trainable data to 1500k, enhancing model performance through vertex-to-face prediction and local mesh assembly strategies.",
                "position": 107
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x3.png",
                "caption": "Figure 3:Illustration of Primitive-Mesh.\nWe utilize both KNN clustering and semantic segmentation to partition meshes into Primitive-Meshes that retain local structural information. This strategy greatly expands the scale of the trainable dataset.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2508.01242/x4.png",
                "caption": "Figure 4:Illustration of the MeshLLM framework.\nWe adopt a progressive training process:Stage1: Training on Primitive-Meshes obtained through KNN clustering, where two tasks are performed: predicting faces from vertices and assembling complete meshes from Primitive-Meshes.Stage2: Training on more refined Primitive-Meshes generated by semantic segmentation, performing the same tasks as in Stage 1.Stage3: Training on tasks specific to mesh generation and understanding.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2508.01242/x5.png",
                "caption": "Figure 5:Example of the constructed SFT data for training LLM.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2508.01242/x6.png",
                "caption": "Figure 6:Gallery results. MeshLLM demonstrates an ability to generate diverse and high-quality meshes.",
                "position": 196
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x7.png",
                "caption": "Figure 7:Dialogue results.\nMeshLLM extends the capabilities of LLMs to the domain of 3D mesh while retaining their advanced dialogue abilities, such as question-answering and mathematical reasoning. This expansion enables MeshLLM to understand and generate 3D meshes through natural and intuitive language interactions, further solidifying LLMs as versatile and powerful tools.",
                "position": 319
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x8.png",
                "caption": "Figure 8:Comparisons on the mesh generation. MeshLLM generates 3D meshes with clean geometric details, outperforming the LLM-based LLaMA-Mesh and achieving performance comparable to Polygen and MeshXL, which are specifically designed for mesh generation.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2508.01242/x9.png",
                "caption": "Figure 9:Comparisons on the mesh understanding. Thegreentext is generated by LLaMA-Mesh, while thepurpletext is produced by our MeshLLM.\nMeshLLM better captures the semantic information of the mesh, generating more accurate textual descriptions.",
                "position": 370
            }
        ]
    },
    {
        "header": "5Limitation and Future Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x10.png",
                "caption": "Figure 10:Examples of the constructed Primitive-Mesh. (a) The KNN-based method is simple and efficient, enabling the rapid construction of large-scale trainable mesh parts while preserving meaningful spatial structures. (b) The Semantic-based method generates mesh parts at the semantic level and includes corresponding textual annotations, which better aid LLMs in accurately understanding and generating meshes.",
                "position": 1733
            }
        ]
    },
    {
        "header": "7Additional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01242/x11.png",
                "caption": "Figure 11:Shape novelty. We compute the Chamfer Distance between the generated meshes and those in the training set, selecting the three closest matches. The notable differences observed among them indicate that MeshLLM exhibits creativity.",
                "position": 1805
            },
            {
                "img": "https://arxiv.org/html/2508.01242/x12.png",
                "caption": "Figure 12:Failure case. The limited semantic dataset size reduces text-geometry alignment for more fine-grained generations.",
                "position": 1901
            }
        ]
    },
    {
        "header": "8Additional Results",
        "images": []
    }
]