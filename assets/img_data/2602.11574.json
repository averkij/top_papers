[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x1.png",
                "caption": "Figure 1:(a) Shows how our method learns to configure optimal configuration across thousands of possibilities for the given input. (b) Shows improvement by our method over multiple datasets. (These results are for Qwen 2.5 7B Instruct model.)",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x2.png",
                "caption": "Figure 2:Training pipeline.The structure policy selects workflows, tools, and budgets while the prompt policy composes instructions. During RL training, episodes are stored in a memory buffer. After RL converges, high-reward episodes are filtered and used for supervised fine-tuning (SFT), which consolidates successful strategies and improves consistency.",
                "position": 160
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x3.png",
                "caption": "Figure 3:Action masking reduces the effective action-sequence within the RL policy.",
                "position": 377
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x4.png",
                "caption": "Figure 4:Accuracy Vs. Cost trade-off on GSM8K.Each point shows average test accuracy versus inference cost for a method. The dashed curve denotes the Pareto frontier, representing non-dominated methods that achieve the best possible accuracy for a given cost. Red points correspond to our ARC variants, which lie on or define the Pareto frontier, indicating superior accuracy–cost efficiency compared to existing baselines.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x5.png",
                "caption": "Figure 5:Scaling trends of model accuracy with capacity.Accuracy as a function of model size for the Qwen 2.5 family (7B, 32B, 72B) across four benchmarks. Performance improves consistently with scale, with gains varying by task complexity.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x6.png",
                "caption": "Figure 6:Error distribution across benchmarks.Reasoning tasks exhibit primarily reasoning errors, while tool-use tasks are dominated by knowledge gap errors. Policy configuration errors remain minimal (<<10%) across all datasets.",
                "position": 1075
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAgentic Workflows",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x7.png",
                "caption": "Figure 7:Overview of the nine agentic workflows: Direct (0), Reason+Ans (1), Reason+Verify+Ans (2), Routing (3), Parallel-Sectioning (4), Parallel-Voting (5), Orchestrator-Workers (6), Evaluator-Optimizer (7), and Autonomous-Agent (8). Each workflow defines a distinct pattern of LLM calls and agent interactions.",
                "position": 1460
            }
        ]
    },
    {
        "header": "Appendix BComputational Resources",
        "images": []
    },
    {
        "header": "Appendix CProofs of Theoretical Guarantees",
        "images": []
    },
    {
        "header": "Appendix DAblation: Identifying the Best Embedding for State Representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/figs/pareto_metaclip.png",
                "caption": "Figure 8:Overall score vs. runtime vs. multimodality with the Pareto frontier under a three-dimensional dominance criterion (runtime↓\\downarrow, score↑\\uparrow, multimodality↑\\uparrow). Circles denote text-only models and triangles denote multimodal models. The selected multimodal model (MetaCLIP-H14) is highlighted.",
                "position": 1789
            }
        ]
    },
    {
        "header": "Appendix EAblation: Identifying the Best Prompt Generator",
        "images": []
    },
    {
        "header": "Appendix FTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x8.png",
                "caption": "Figure 9:Training dynamics of ARC across datasets.Left: cumulative reward over episodes, showing steady improvement as the policy discovers higher-value configurations on GSM8K, DROP, MedQA, HotpotQA, and GAIA. Middle: rolling mean±\\pmstandard deviation of per-episode reward, indicating reduced variance and stabilization over time. Right: running validation accuracy, demonstrating that reward gains translate into improved task performance with dataset-specific convergence levels.",
                "position": 2890
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x9.png",
                "caption": "",
                "position": 2893
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x10.png",
                "caption": "",
                "position": 2894
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x11.png",
                "caption": "Figure 10:Tool usage during training.Running average number of tools used per episode for each dataset. ARC quickly learns sparse tool usage and gradually adjusts invocation patterns, with different steady-state levels reflecting task-specific reliance on tools.",
                "position": 2898
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x12.png",
                "caption": "Figure 11:Evolution of workflow selection during training.Stacked area plots show, for GSM8K, HotpotQA, and GAIA, the fraction of episodes assigned to each workflow as training progresses. The structure policy quickly prunes suboptimal patterns and concentrates mass on a small set of task-appropriate workflows (e.g., Evaluator–Optimizer on GSM8K, Orchestrator–Workers on HotpotQA).",
                "position": 2907
            }
        ]
    },
    {
        "header": "Appendix GAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11574/x13.png",
                "caption": "Figure 12:Accuracy by workflow and dataset.Each bar shows the average accuracy of a fixed workflow on a given benchmark. Performance varies substantially across workflows and tasks no single workflow is uniformly optimal—highlighting the importance of learning query-adaptive configurations rather than relying on a fixed architecture.",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2602.11574/x14.png",
                "caption": "Figure 13:Reward distribution by workflow.Violin plots show the distribution of per-episode rewards for each workflow across datasets. Higher-performing workflows exhibit both higher central reward and tighter spread, illustrating that certain structural patterns not only achieve better returns but also yield more stable behavior during training.",
                "position": 2920
            }
        ]
    },
    {
        "header": "Appendix HAlternative Training Objectives",
        "images": []
    },
    {
        "header": "Appendix IError Categorization Methodology",
        "images": []
    }
]