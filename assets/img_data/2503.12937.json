[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12937/extracted/6286192/figures/fig1.png",
                "caption": "Figure 1:For MLLMs, online reinforcement learning with outcome-level reward, like in Deepseek-R1‚Äôs GRPO[30], often suffers from sparse reward issues, where only a few reasoning paths can receive positive/high rewards during training, ultimately leading to poor exploration efficiency and unstable learning process.\nTo tackle this, we propose a novel online reinforcement learning framework that incorporates step-wise reasoning rewards in addition to outcome-level rewards, encouraging MLLMs to iteratively refine their reasoning with dense rewards and resulting in a more stable training process and improved reasoning capability.\nThe experiments are conducted on Qwen2-VL-7b over MathVista.",
                "position": 91
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12937/x1.png",
                "caption": "Figure 2:Overview of the proposed StepGRPO. StepGRPO consists of two phases: a policy warm-up phase and a step-wise online policy optimization phase. After the warm-up, the policy modelœÄŒ∏subscriptùúãùúÉ\\pi_{\\theta}italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPTgenerates a group of reasoning paths{ùêúi}i=1Msuperscriptsubscriptsuperscriptùêúùëñùëñ1ùëÄ\\{\\mathbf{c}^{i}\\}_{i=1}^{M}{ bold_c start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPTand assigns step-wise rewards using two proposed mechanisms: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR).\nStepRAR rewards reasoning paths that contain key intermediate steps, identified using a soft key-step matching technique.\nStepRVR rewards reasoning paths based on completeness and logical consistency, ensuring they are well-structured.\nStepGRPO then estimates the advantageA^^ùê¥\\hat{A}over^ start_ARG italic_A end_ARGfor policy optimization by using the average step-wise reasoning reward of a group of sampled reasoning paths as a baseline. Examples for StepRAR and StepRVR are illustrated in (a) and (b), respectively.",
                "position": 169
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12937/extracted/6286192/figures/fig3.png",
                "caption": "Figure 3:Comparison between StepGRPO and SFT. The experiments are conducted on Qwen2-VL-7B over MathVista.",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2503.12937/x2.png",
                "caption": "Figure 4:Qualitative comparison.",
                "position": 885
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]