[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03646/x1.png",
                "caption": "Figure 1:(Left)LLM reasoning mirrors a human-like hierarchical reasoning: high-level strategic planning and low-level procedural executions.(Right)Hierarchical reasoning emerges during RL training via a two-phase dynamic. Phase ① consolidates low-level skills, marked by a token-entropy drop in execution tokens. The learning frontier then shifts to Phase ②, where the model explores and masters high-level planning, marked by increased semantic diversity, sustained reasoning enhancement and length scaling.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2The Emergent Reasoning Hierarchy",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03646/x2.png",
                "caption": "Figure 2:Reasoning from Qwen3-4B-GRPO with planning tokens (strategic grams) highlighted. Planning tokens function as the high-level strategic moves of reasoning, includinglogical deduction,branchingandbacktracing.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x3.png",
                "caption": "Figure 3:We track the training Dynamics of representative model families. The curves reveal a two-phase dynamics. Seen from the first two columns, the model has an initial focus on procedural consolidation, marked by sharp decrease in model perplexity (greater confidence) and token entropy (more certain) of execution tokens. This follows a shift to exploring strategic planning, evident from the third column. The diversity of strategic plans (semantic entropy) steadily increases on Qwen models or takes a turn to increase on Llama, correlating with consistently improved accuracy and longer reasoning chains (fourth column).",
                "position": 280
            }
        ]
    },
    {
        "header": "3HICRA: Hierarchy-Aware Credit Assignment",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03646/x4.png",
                "caption": "Figure 4:Training Dynamics of Error Types.Across all models, the number ofPlanning & Strategyerrors (red) decreases more significantly than other procedural errors (gray), indicating that RL’s primary benefit comes from correcting high-level strategic faults.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x5.png",
                "caption": "Figure 5:HICRA (red) consistently achieves highersemantic entropythan the GRPO baseline (gray), indicating more diverse strategic exploration.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x6.png",
                "caption": "Figure 6:HICRA vs. Entropy Regularization on Qwen2.5-7B-Base.While entropy regularization increases token-level entropy, it fails to consistently improve accuracy and leads to uncontrolled length scaling. In contrast, HICRA boostssemantic entropy, which strongly correlates with validation accuracy, demonstrating the superiority of targeted strategic exploration.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x7.png",
                "caption": "Figure 7:HICRA on Llama-3.1-Instruct-8B.",
                "position": 824
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x8.png",
                "caption": "Figure 8:Training Dynamics on MiMO-VL-Instruct-7B.This experiment highlights that token entropy can collapse while semantic entropy remains high and predictive of validation accuracy. Furthermore, while Pass@8 saturates and is indistinguishable between methods, semantic entropy reveals a persistent exploration advantage for HICRA that translates to better final performance.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x9.png",
                "caption": "Figure 9:Planning Tokens vs. High-Entropy Tokens.(Left) A majority of our functionally-defined planning tokens are also high-entropy (top 30%). (Right) However, the reverse is not true; most high-entropy tokens are not planning tokens.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2509.03646/x10.png",
                "caption": "Figure 10:Planning Tokens,High-Entropy TokensandShared Tokensare highlighted with different colors. This concrete example suggests how these two definitions differ: Planning Tokens function as strategic skeletons of a reasoning solution and are thus sparse, with more than half of these semantic units also having higher entropy. In contrast, a majority of high-entropy tokens only exhibits high-variations in its phrasing, spreading across low-level executions and high-level planning. Fig.9reveals that less than 10% high-entropy tokens serve the semantic function of planning.",
                "position": 867
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion and Promising Directions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFull Training Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.03646/x11.png",
                "caption": "Figure 11:Training Dynamics across different LLMs and VLMs.",
                "position": 1494
            }
        ]
    },
    {
        "header": "Appendix BThe Distribution Matching Perspective of Policy Gradients",
        "images": []
    }
]