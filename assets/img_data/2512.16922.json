[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16922/x1.png",
                "caption": "Figure 1:Next-Embedding Predictive Autoregression (NEPA). An image is split into patches and embedded into a sequence. An autoregressive model predicts the next embedding from previous ones, mirroring next-token prediction in language models.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16922/x2.png",
                "caption": "Figure 2:Images are tokenized via a Conv2d patch embedder before entering a pre-norm Transformer with LayerNorm. Modern stabilization components (RoPE[su2024roformer], LayerScale[touvron2021cait], SwiGLU[shazeer2020glu], and QK-Norm[henry2020qknorm]) are applied at all layers.",
                "position": 393
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16922/x3.png",
                "caption": "Figure 3:Ablation of key components in NEPA pretraining.Left:EMA accuracy with and without AR shift. Without the autoregressive shift, training diverges early.Middle-left:Training loss with and without stop-grad; removing stop-grad causes representation collapse.Middle-right:Training loss with and without LayerScale; LayerScale stabilizes optimization and accelerates convergence.Right:Gradient norm with and without QK-Norm; QK-Norm suppresses gradient explosion and improves smoothness.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x4.png",
                "caption": "",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x5.png",
                "caption": "",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x6.png",
                "caption": "",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x7.png",
                "caption": "Figure 4:ImageNet-1K validation Top-1 accuracy versus training epochs. For each epochâ€™s checkpoint, we perform a lightweight hyperparameter search and report the best accuracy. Fine-tuning uses causal attention. The top plot corresponds to the base model, and the bottom plot to the large model.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x8.png",
                "caption": "",
                "position": 794
            }
        ]
    },
    {
        "header": "5Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16922/x9.png",
                "caption": "(a)ImageNet-1K validation samples (unseen during pretraining).",
                "position": 1073
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x9.png",
                "caption": "(a)ImageNet-1K validation samples (unseen during pretraining).",
                "position": 1076
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x10.png",
                "caption": "(b)MSCOCO validation samples (out of distribution during pretraining).",
                "position": 1082
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Appendix ABroader Impacts",
        "images": []
    },
    {
        "header": "Appendix BMethodology Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16922/x11.png",
                "caption": "Figure 6:Comparison between JEPA and NEPA.",
                "position": 1171
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16922/x12.png",
                "caption": "Figure 7:Pre-training dynamics of NEPA.Top: pre-training loss curves;\nBottom: evolution of NEPA-L attention maps.",
                "position": 1344
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x13.png",
                "caption": "",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x14.png",
                "caption": "",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x15.png",
                "caption": "Figure 8:Quantitative Failure Examples.In a multi-object scene with strong reflections, the model confuses highlights on metal surfaces with object embeddings; under backlighting, shadowed regions are misinterpreted as trees; for the animal, shaded skin is treated as background; and in the last example, bright reflective regions of the object are also mistaken for background.",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2512.16922/x16.png",
                "caption": "Figure 9:Additional attention and embedding visualizations.",
                "position": 1462
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": []
    }
]