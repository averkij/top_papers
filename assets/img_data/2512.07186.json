[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07186/x1.png",
                "caption": "Figure 1:A: the overview of the START, which leverages spatial and textual learning for chart understanding. B: challenging question sample from the CharXiv[59]. Answering the question requires chart element grounding and step-by-step reasoning.",
                "position": 76
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07186/x2.png",
                "caption": "Figure 2:START’s reward design in reinforcement learning.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x3.png",
                "caption": "Figure 3:A: The analysis of the existing chart datasets and B: the overview of the START-dataset generation pipeline.",
                "position": 165
            }
        ]
    },
    {
        "header": "3Spatial and Textual Learning for Chart Understanding (START)",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07186/x4.png",
                "caption": "Figure 4:A: The dataset sample visualization and B: the START-SFT and START-RL dataset statistics.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x5.png",
                "caption": "Figure 5:A: The samples from refChartQA[56], the locations are related to limited types of chart components, and focus on single-subplot chart images. B: the CS-Bench statistics. C: the samples from CS-Bench with the visualized target region under a red mask.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x6.png",
                "caption": "Figure 6:The visualization of the predictions from START verse Qwen2.5-VL[1]. Benefit from the spatial and temporal learning, START produces better predictions in chart question answering (Subplot A), chart element grounding (Subplot B), and chart-to-code (Subplot C), reflecting the enhancement in MLLM’s spatial and textual understanding toward the charts.",
                "position": 605
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": []
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASTART-Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07186/x7.png",
                "caption": "Figure 7:The prompt we use for converting chart to code, and the template we use for preparing the annotations inDcD_{c}.",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x8.png",
                "caption": "Figure 8:Chart element location annotationDgD_{g}preparation template.",
                "position": 1762
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x9.png",
                "caption": "Figure 9:The QA samples generated by the MLLM and the LLM. We highlight the questions that are improperly hard or the incorrect answers in red.",
                "position": 1781
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x10.png",
                "caption": "Figure 10:The question-answer pairs and the corresponding verdicts. We highlight the questions that are improperly in red and the corresponding verdicts in green.",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x11.png",
                "caption": "Figure 11:learning curve of START-RL-7B, trained with chart question answering, chart element grounding, and the chart-to-code task.",
                "position": 1812
            }
        ]
    },
    {
        "header": "Appendix BCS-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.07186/x12.png",
                "caption": "Figure 12:Prompt for code evolution.",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x13.png",
                "caption": "Figure 13:Prompt for QA generation.",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x14.png",
                "caption": "Figure 14:Prompt for code generation during the RL training.",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x15.png",
                "caption": "Figure 15:The visualization of the samples in the Chart Spatial understanding Benchmark (CS-Bench). We visualize the bounding box region in red.",
                "position": 1913
            },
            {
                "img": "https://arxiv.org/html/2512.07186/x16.png",
                "caption": "Figure 16:Visualization of reproduced charts using different chart-to-code methods.",
                "position": 1916
            }
        ]
    },
    {
        "header": "Appendix CSTART Experiments",
        "images": []
    }
]