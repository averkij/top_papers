[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x1.png",
                "caption": "(a)Ultra-large proprietary model/human annotator-dependent methods",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x1.png",
                "caption": "(a)Ultra-large proprietary model/human annotator-dependent methods",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x2.png",
                "caption": "(b)Response rewriting method",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x3.png",
                "caption": "(c)SENTINEL(Ours)",
                "position": 123
            }
        ]
    },
    {
        "header": "2Background and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x4.png",
                "caption": "(a)",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x4.png",
                "caption": "(a)",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x5.png",
                "caption": "(b)",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x6.png",
                "caption": "Figure 3:The overview of SENTINEL.The proposed SENTINEL takes six essential steps:(1)Generate multiple in-domain responses conditioned on the input image, prompt, and contextùíÑùíÑ\\boldsymbol{c}bold_italic_c.(2)Identify and extract all mentioned objects from each generated sentence.(3)Utilizing two object detectors to validate the existence of extracted objects through cross-referencing.(4)Categorize generated sentences into hallucinated and non-hallucinated groups based on detection results.(5)Extend the generation context with verified non-hallucinated sentences to guide subsequent outputs.(6)Fine-tune the model using the context-aware DPO (C-DPO) loss with the in-domain, style-consistent, and context-varying preference data.",
                "position": 274
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x7.png",
                "caption": "Figure 4:Categories of in-domain candidates.The in-domain candidates fall into three types. Employing non-hallucinated, context-coherent descriptions (ùíöw+subscriptsuperscriptùíöùë§\\boldsymbol{y}^{+}_{w}bold_italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT) as positive samples, paired with hallucinated descriptions (ùíölsubscriptùíöùëô\\boldsymbol{y}_{l}bold_italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT), enhances the model‚Äôs generalization performance and robustness.",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x8.png",
                "caption": "Figure 5:Visualization of the Iterative Contextual Bootstrapping (ICB) framework.Given an input image and corresponding question, this pipeline iteratively generates diverse contextual samples, enabling robust hallucination mitigation across varying contexts and significantly improving model generalization.",
                "position": 410
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x9.png",
                "caption": "Figure 6:Impact on different hallucination types.Comparison between multiple methods shows that our method reduces hallucination in all six hallucination types.",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x10.png",
                "caption": "Figure 7:Qualitative results of SENTINEL.Our method can effectively eliminate hallucinations in MLLMs while enhancing the model‚Äôs general capabilities.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x11.png",
                "caption": "Figure 8:Impact of training data quantity on hallucination rate in Object Halbench[55].The results show that SENTINEL demonstrates better efficiency, effectiveness, and scalability, while effectively reducing hallucination rates across varying data scales.",
                "position": 1035
            }
        ]
    },
    {
        "header": "5Concluding Remarks",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix AMotivation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x12.png",
                "caption": "Figure 9:Effect of intermediate hallucination mitigation on subsequent generations.Showing the effectiveness of early-stage intervention in mitigating the propagation of hallucinations.",
                "position": 2441
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x13.png",
                "caption": "Figure 10:Time cost analysis of decode-based methods.Decode-based early intervention increases inference time, primarily due to the additional generation steps required by MLLM sampling, whereas the object detector remains highly efficient.",
                "position": 2444
            }
        ]
    },
    {
        "header": "Appendix BMethod Details",
        "images": []
    },
    {
        "header": "Appendix CTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x14.png",
                "caption": "(a)Logps comparison",
                "position": 3151
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x14.png",
                "caption": "(a)Logps comparison",
                "position": 3154
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x15.png",
                "caption": "(b)Loss comparison",
                "position": 3160
            }
        ]
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x16.png",
                "caption": "(a)Training log probabilities (logps) comparison",
                "position": 3912
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x16.png",
                "caption": "(a)Training log probabilities (logps) comparison",
                "position": 3915
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x17.png",
                "caption": "(b)Training loss comparison",
                "position": 3921
            }
        ]
    },
    {
        "header": "Appendix ESENTINEL with Other Baselines",
        "images": []
    },
    {
        "header": "Appendix FRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12455/x18.png",
                "caption": "Figure 13:Comparing general image description results between SENTINAL and its base model LLaVA-v1.5-7B.Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing a more detailed description.",
                "position": 4267
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x19.png",
                "caption": "Figure 14:Comparing detailed image description results between SENTINAL and its base model LLaVA-v1.5-7B.Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing a more detailed description.",
                "position": 4270
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x20.png",
                "caption": "Figure 15:Comparing visual question answering results between SENTINAL and its base model LLaVA-v1.5-7B.Our method effectively mitigates hallucinations while enhancing general performance of the base model, leading to more accurate and detailed answers.",
                "position": 4273
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x21.png",
                "caption": "Figure 16:Comparing general image descriptions between SENTINAL and its base model LLaVA-v1.5-13B.Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing a more detailed description.",
                "position": 4276
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x22.png",
                "caption": "Figure 17:Comparing detailed image descriptions between SENTINAL and its base model LLaVA-v1.5-13B.Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing a more detailed description.",
                "position": 4279
            },
            {
                "img": "https://arxiv.org/html/2507.12455/x23.png",
                "caption": "Figure 18:Comparing visual question answering between SENTINAL and its base model LLaVA-v1.5-13B.Our method effectively mitigates hallucinations while enhancing the general performance of the base model, leading to more accurate answers.",
                "position": 4282
            }
        ]
    },
    {
        "header": "Appendix GAdditional Case Studies",
        "images": []
    }
]