[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16929/x1.png",
                "caption": "Figure 1:A ‚Äúperiodic‚Äù table of representation learning methods unified by the I-Con framework.By choosing different types of conditional probability distributions over neighbors, I-Con generalizes over 23 commonly used representation learning methods.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16929/x2.png",
                "caption": "(a)High-level I-Con architecture.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x2.png",
                "caption": "(a)High-level I-Con architecture.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x3.png",
                "caption": "(b)Illustrative examples of distribution families forpŒ∏subscriptùëùùúÉp_{\\theta}italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPTorqœïsubscriptùëûitalic-œïq_{\\phi}italic_q start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT.",
                "position": 170
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16929/x4.png",
                "caption": "(a)SNE (dimensionality reduction)",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x4.png",
                "caption": "(a)SNE (dimensionality reduction)",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x5.png",
                "caption": "(b)SimCLR (contrastive learning)",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x6.png",
                "caption": "(c)K-Means (clustering)",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x7.png",
                "caption": "Table 1:I-Con unifies representation learnersunder different choices ofpŒ∏‚Å¢(j|i)subscriptùëùùúÉconditionalùëóùëñp_{\\theta}(j|i)italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_j | italic_i )andqœï‚Å¢(j|i)subscriptùëûitalic-œïconditionalùëóùëñq_{\\phi}(j|i)italic_q start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT ( italic_j | italic_i ). Proofs of the propositions in this table can be found in the supplement.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x7.png",
                "caption": "(a)Continuous distance-based distributions control neighborhood width via hyperparameters.",
                "position": 856
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x7.png",
                "caption": "(a)Continuous distance-based distributions control neighborhood width via hyperparameters.",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2504.16929/x8.png",
                "caption": "(b)Graph-based distributions expand neighborhoods through structural strategies.",
                "position": 865
            },
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/figure3_w_pcal.png",
                "caption": "Figure 5:Left: Debiasing cluster learning improves performance on ImageNet-1K across batch sizes. Center: Distribution of maximum predicted probabilities for the biased model (Œ±=0ùõº0\\alpha=0italic_Œ± = 0) showing poor calibration, with overconfident predictions.  Right: Distribution of maximum predicted probabilities for the debiased model (Œ±=0.4ùõº0.4\\alpha=0.4italic_Œ± = 0.4), demonstrating improved probability calibration. Debiased training alleviates optimization stiffness by reducing the prevalence of saturated logits, mitigating vanishing gradient issues, and fostering more robust and well-calibrated learning dynamics.",
                "position": 946
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16929/x9.png",
                "caption": "Figure 6:Effects of increasing the debias weightŒ±ùõº\\alphaitalic_Œ±on the supervisory neighborhood (blue line) and both the learned and supervisory neighborhood (red line). Adding some amount of debiasing helps in all cases, with a double debiasing yielding the largest improvements.",
                "position": 1128
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiments on Debiasing Feature Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/stl-debais.png",
                "caption": "(a)STL-10 embeddings for SimCLR & DCL",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/stl-debais.png",
                "caption": "(a)STL-10 embeddings for SimCLR & DCL",
                "position": 1978
            },
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/dcl-cifar.png",
                "caption": "(b)CIFAR-10 embeddings for SimCLR & DCL",
                "position": 1983
            },
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/gaussian_embeddings.png",
                "caption": "(c)CIFAR10 embeddings for models trained on with Gaussian distributionqœïsubscriptùëûitalic-œïq_{\\phi}italic_q start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT",
                "position": 1989
            },
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/t-embeddings.png",
                "caption": "(d)CIFAR10 features for models trained with Student‚Äôs t-distributionqœïsubscriptùëûitalic-œïq_{\\phi}italic_q start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2504.16929/extracted/6381858/img/stl10-studet-t-tsne.png",
                "caption": "(e)STL-10 features for models trained with Student‚Äôs t-distributionqœïsubscriptùëûitalic-œïq_{\\phi}italic_q start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT",
                "position": 2001
            }
        ]
    },
    {
        "header": "Appendix BProofs for Unifying Dimensionality Reduction Methods",
        "images": []
    },
    {
        "header": "Appendix CProofs for Unifying Feature Learning Methods",
        "images": []
    },
    {
        "header": "Appendix DProofs for Unifying Clustering Methods",
        "images": []
    },
    {
        "header": "Appendix EI-Con as a Variational Method",
        "images": []
    },
    {
        "header": "Appendix FWhy do we need to unify representation learners?",
        "images": []
    },
    {
        "header": "Appendix GHow to choose neighborhood distributions for your problem",
        "images": []
    },
    {
        "header": "Appendix HComparing I-Con, MLE, and the KL Divergence",
        "images": []
    },
    {
        "header": "Appendix IOn I-Con‚Äôs Hyperparameters",
        "images": []
    }
]