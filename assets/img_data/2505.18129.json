[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18129/x1.png",
                "caption": "",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x2.png",
                "caption": "Figure 1:Performance of Orsta on MEGA-Bench Tasks. V-Triune is evaluated across visual reasoning and visual perception tasks—Math, Science, Charting, Puzzle, Detection, Grounding, Counting, and OCR, demonstrating notable performance gains of Orsta over the backbone: +3.2%, +14.1%, and +2.1% in different model variants.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18129/x3.png",
                "caption": "Figure 2:V-Triune System. It integrates three complementary components:Sample-Level Data Formatting(to unify diverse task inputs),Verifier-Level Reward Computation(for custom rewards via specialized verifiers), andSource-Level Metric Monitoring(to diagnose data-source level problems). Additionally, a novel Dynamic IoU reward offers adaptive, progressive feedback for perception tasks.",
                "position": 328
            }
        ]
    },
    {
        "header": "3V-Triune: Visual Triple Unified Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18129/extracted/6472593/images/reward_server.png",
                "caption": "Figure 4:Architecture of the Asynchronous Reward Server.The RL trainer interacts with a remote server via client-server proxies, where specialized verifiers (e.g., MathVerify, Detection) compute rewards using task-specific logic and dynamic thresholds (e.g., dynamic IoU threshold).",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x4.png",
                "caption": "Figure 5:COCO Test Set Performance with Various Reward Designs.(a) Comparison between IoU-based and mAP-based rewards on a selected COCO multi-object subset; (b) Comparison between vanilla IoU reward and rule-based IoU reward on a selected COCO single-object subset; (c, d) Comparison between rule-based IoU reward and dynamic IoU reward on the COCO multi-object subset and the OVDEval negation subset.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x4.png",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x5.png",
                "caption": "",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x6.png",
                "caption": "",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x7.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x8.png",
                "caption": "Figure 6:Training accuracy rewards under Dynamic IoU versus a fixed Rule-based IoU (IoU@99).",
                "position": 507
            }
        ]
    },
    {
        "header": "4Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18129/x9.png",
                "caption": "Figure 7:Analysis of ViT Training Instability.(a) COCO testset (OOD) performance comparison. (b) Sum of gradient norms under different training schemes. (c) Layer-wise gradient norms of ViT and LLM during full parameter training. Notably, incorporating ViT training leads to a performance decline and highly unstable gradients; remarkably, ViT’s gradients amplify during back-propagation, contrasting with the stable layer-wise gradients of the LLM.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2505.18129/extracted/6472593/images/image_pad.png",
                "caption": "Figure 8:An Example of Spurious Image Tokens.",
                "position": 547
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18129/x10.png",
                "caption": "Figure 9:Data Curation Process. First, visual reasoning and visual perception data pass through a rule-based filter, which removes samples that do not meet preset criteria. Subsequently, the data enters a difficulty filter, which removes samples that are too easy or too hard based on model performance, ultimately producing the Curated Dataset.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x11.png",
                "caption": "Figure 11:Training Trends of On-Policy vs Off-Policy Across Three Model Variants on MEGA-Bench core (7B, 32B-0321, 32B-0326).\nModels are evaluated every 5 steps from step 0 to 135. Starting points and peak performances are annotated on the curves.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x11.png",
                "caption": "",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x12.png",
                "caption": "",
                "position": 1127
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x13.png",
                "caption": "",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x14.png",
                "caption": "Figure 12:Training Trends of Orsta-32B-0321 on MEGA-Bench core.The dark line denotes the overall MEGA-Bench Core, linking to the performance shown inFig.11.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x15.png",
                "caption": "Figure 13:Training dynamics of response length (top row), reflection ratio (middle row), and correct ratio in reflection responses (bottom row) during training steps for Math (MMK12), Puzzle (PuzzleVQA), Detection (V3Det), and OCR (estvqa) tasks using the Orsta-32B-0321 off-policy setting. Each column corresponds to a different task, and each row represents a distinct metric.",
                "position": 1301
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x16.png",
                "caption": "Figure 14:Ablation Study on Training Strategies (a), Task Composition (b), and Learning Rates (c).(a) Various training strategies evaluated on the 7B model (LLM-only, ViT-only, joint training); (b) Different task compositions evaluated on the 7B model (reasoning, perception, or both); (c) Effects of learning rates (1e-6, 1.5e-6, 2e-6, 3e-6) on the performance of the 32B model. Each subfigure shows the score progression over training steps.",
                "position": 1320
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x16.png",
                "caption": "",
                "position": 1323
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x17.png",
                "caption": "",
                "position": 1327
            },
            {
                "img": "https://arxiv.org/html/2505.18129/x18.png",
                "caption": "",
                "position": 1331
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion & Future Work",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]