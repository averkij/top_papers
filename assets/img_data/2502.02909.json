[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIBackground",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02909/extracted/6179662/figures/revised_figure.jpg",
                "caption": "Figure 1:Overview of SPARC:(a)The subspace of the new dataset is computed using PCA. By measuring the cosine similarity between this new subspace and previously learned prompt subspaces, the framework determines whether a similar prompt already exists. If a match is found, the existing prompt is reused for initialization, enhancing knowledge retention. Otherwise, a new prompt is initialized in an orthogonal subspace to the existing prompts, ensuring differentiation and efficient adaptation.(b)The prompt embeddings consist of two key components: tunable soft tokens and a PCA-based transformation matrix. This design significantly reduces the number of trainable parameters compared to traditional prompt tuning methods, making the approach more efficient while preserving model adaptability.",
                "position": 156
            }
        ]
    },
    {
        "header": "IIISPARC: Subspace-Aware Prompt Tuning for Continual Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02909/extracted/6179662/figures/heatmaps.jpg",
                "caption": "Figure 2:Per-token accuracy with varying numbers of soft tokens and principal components:(a) 100 principal components, 20 soft tokens.\n(b) 300 principal components, 20 soft tokens.\n(c) 100 principal components, 40 soft tokens. (d) Accuracy with different number of PCA components",
                "position": 352
            }
        ]
    },
    {
        "header": "IVResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02909/extracted/6179662/figures/Continual_vs_non_continual.jpg",
                "caption": "Figure 3:Accuracy in Domain Incremental Learning.(a) Non-Continual learning Accuracy is calculated by fine-tuning the model individually on each dataset, while final accuracy is obtained by sequentially fine-tuning the model across datasets. (b) Forgetting ratio for each dataset after the completion of sequential training.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2502.02909/extracted/6179662/figures/forgetting.jpg",
                "caption": "",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2502.02909/extracted/6179662/figures/tcl_table.jpg",
                "caption": "Figure 4:Performance Comparison of Task-Incremental\nLearning Methods:Accuracy results for three approaches, PCA-Based Learning,\nFull Finetuning, and LoRA-Integrated Prompt-Based Learning.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2502.02909/extracted/6179662/figures/TCL_Comparison.jpg",
                "caption": "Figure 5:Task Incremental Learning:Accuracy comparison of PCA-Based Continual Learning, PCA-Based Non-Continual Learning, Full Finetuning, and Zero-Shot Inference.",
                "position": 417
            }
        ]
    },
    {
        "header": "VConclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]