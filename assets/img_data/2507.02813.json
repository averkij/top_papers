[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02813/x1.png",
                "caption": "",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02813/x2.png",
                "caption": "Figure 2:Pipeline of LangScene-X.Given two sparse-view images as input, we first generate a sequence of 3D consistent RGB images, normal maps, and segmentation maps from TriMap video diffusion model, which provides the dense frames for later 3D scene reconstruction and understanding. Then we project high-dimensional semantic features into low-dimensional discrete space through a generalizable Language Quantized Compressor (LQC). Finally, we reconstruct the 3D language-embedded scenes with generated and compressed information, which supports open-ended language queries in any viewpoint.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2507.02813/x3.png",
                "caption": "Figure 3:The illustration of Language Quantized Compressor (LQC). By leveraging learnable embedding and vector quantisation strategy, it compresses high-dimensional language features into discreteDùê∑Ditalic_D-channel latent representation, facilitating efficient language fields reconstructions and render.",
                "position": 184
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02813/x4.png",
                "caption": "Figure 4:2D Segmentation Results on LERF-OVS[17]Dataset.Here, we showcase two cases (i.e., Teatime, Kitchen) with multiple segmentation masks with text query. On the top, we display the rendered results of our method and other methods, along with the corresponding ground truth annotations. On the bottom, we present the images alongside the queried texts.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2507.02813/x5.png",
                "caption": "Figure 5:2D Segmentation Results on Scannet[7]Dataset.Here, we showcase two cases (i.e., 0085_00, 0114_00) with multiple segmentation masks with text query. The masks predicted by ours contain more comprehensive regions and sharper boundaries than other methods, such as the ‚ÄúCabinet‚Äù prompt, which also surpasses the GT masks.",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2507.02813/x6.png",
                "caption": "Figure 6:Feature Matchingcomparison between our method and vanilla video diffusion mdoel .",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2507.02813/x7.png",
                "caption": "Figure 7:Training Curvecomparison between our LQC and regular autoencoder technique.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2507.02813/x8.png",
                "caption": "Figure 8:Qualitative Comparison on LERF-OVS[17]. We visualize text-query activation masks with various text prompts.",
                "position": 576
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]