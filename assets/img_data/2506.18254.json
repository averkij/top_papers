[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18254/x1.png",
                "caption": "",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x2.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x3.png",
                "caption": "Figure 1:Overall performance on general-domain and mathematical reasoning benchmarks. By simply replacing the rule-based verifier reward of RLVR with the proposed LLM‚Äôs intrinsic probability reward,RLPRachieves consistent improvements in both mathematical and general domains, even outperforming strong RL methods driven by model-based verifier reward. Average: average accuracy of five benchmarks. Verifier requirements of different methods are listed in parentheses.",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18254/x4.png",
                "caption": "Figure 2:Existing RLVR methods rely on specialized verifiers for each domain, suffering from high complexity and limited scalability. We propose theRLPRframework, which replaces the complex verifier-based reward with a simple probability-based reward generated by the policy modelœÄŒ∏subscriptùúãùúÉ\\pi_{\\theta}italic_œÄ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT.QùëÑQitalic_Q: input question,zùëßzitalic_z: generated reasoning content before final answer,yùë¶yitalic_y: generated final answer,y‚àósuperscriptùë¶y^{*}italic_y start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT: reference answer. As shown in the example on the right side, rules and verifier models wrongly label bothy2subscriptùë¶2y_{2}italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTandy3subscriptùë¶3y_{3}italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTas incorrect due to their limited capability of handling natural language complexity.",
                "position": 120
            }
        ]
    },
    {
        "header": "2RLPR",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18254/x5.png",
                "caption": "Figure 3:Token-level probability visualization, where deeper colors represent higher values. The underlined part highlights that probabilities precisely reflect that response sequenceo‚Å¢2ùëú2o2italic_o 2incorrectly place option B after A, resulting lower scores at the corresponding positions in the reference answer. The question is shown in Figure2.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x6.png",
                "caption": "Figure 4:Reward quality comparison. We report the AUC on both math data and general data, and highlight the average score with the dashed line. Qwn: Qwen2.5 models.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x7.png",
                "caption": "Figure 5:Robustness across different training prompt templates.RLPRyields consistently higher performance compared withVeriFree. Left: average performance on seven benchmarks. Middle: response length. Right: response entropy during training.",
                "position": 797
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18254/x8.png",
                "caption": "(a)Mean response length",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x8.png",
                "caption": "(a)Mean response length",
                "position": 1433
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x9.png",
                "caption": "(b)Format reward",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x10.png",
                "caption": "(c)Token entropy",
                "position": 1443
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x11.png",
                "caption": "Figure 7:Pass@k curves forRLPRand baselines.",
                "position": 1450
            },
            {
                "img": "https://arxiv.org/html/2506.18254/x12.png",
                "caption": "",
                "position": 1453
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]