[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x1.png",
                "caption": "Figure 1:Animations produced byAnimate-Xwhich extends beyond human to anthropomorphic characters with various body structures,e.g., without limbs, from games, animations, and posters.",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x2.png",
                "caption": "Figure 2:(a) The overview of ourAnimate-X. Given a reference imageIrsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, we first extract CLIP image featurefφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPTand latent featurefersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTvia CLIP image encoderΦΦ\\Phiroman_Φand VAE encoderℰℰ\\mathcal{E}caligraphic_E.\nThe proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI)\nproduce motion featurefisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand pose featurefesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, respectively.fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTis concatenated with the noised inputϵitalic-ϵ\\epsilonitalic_ϵalong the channel dimension, then further concatenated withfersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTalong the temporal dimension. This serves as the input to the diffusion modelϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPTfor progressive denoising. During the denoising process,fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPTandfisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTprovide appearance condition fromIrsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPTand motion condition fromI1:Fdsubscriptsuperscript𝐼𝑑:1𝐹I^{d}_{1:F}italic_I start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_F end_POSTSUBSCRIPT.\nAt last, a VAE decoder𝒟𝒟\\mathcal{D}caligraphic_Dis adopted to map the generated latent representationz0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTto the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator.",
                "position": 180
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x3.png",
                "caption": "Figure 3:Examples from ourA2superscript𝐴2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBench.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2410.10306/extracted/5924712/figs/setting_demo.png",
                "caption": "Figure 4:The illustration of comparison settings.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x4.png",
                "caption": "Figure 5:Qualitative comparisons with state-of-the-art methods.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x5.png",
                "caption": "Figure 6:Qualitative comparisons with Unianimate in terms of long video generation.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x6.png",
                "caption": "Figure 7:Visualization of ablation study on IPI and EPI.",
                "position": 649
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ANetwork Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x7.png",
                "caption": "Figure 8:More example for EPI.",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x8.png",
                "caption": "Figure 9:The difference of training and inference pipeline. During training, the reference image and the driven video come from the same video, while in the inference pipeline, the reference image and the driven video can be from any sources and appreciably different.",
                "position": 1919
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x9.png",
                "caption": "Figure 10:Detailed pipeline for buildingA2superscript𝐴2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBenchbased on large-scale pretrained models, including Open-ChatGPT 4o and KLing AI.",
                "position": 1925
            }
        ]
    },
    {
        "header": "Appendix BBenchmark Details",
        "images": []
    },
    {
        "header": "Appendix CUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x10.png",
                "caption": "Figure 11:Visualization of cases in the user study",
                "position": 1999
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x11.png",
                "caption": "Figure 12:Visualization of the robustness ofAnimate-X.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x12.png",
                "caption": "Figure 13:Ablation study on the weightα𝛼\\alphaitalic_αof Implicit Pose Indicator. To better visualize the impact ofα𝛼\\alphaitalic_αon performance, we normalize all the values to the range of 0 to 1.",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x13.png",
                "caption": "Figure 14:Visualization comparison on TikTok dataset andA2superscript𝐴2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBench.",
                "position": 2668
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x14.png",
                "caption": "Figure 15:Comparison with more SOTAs onA2superscript𝐴2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBench.",
                "position": 2671
            }
        ]
    },
    {
        "header": "Appendix EDiscussion",
        "images": []
    }
]