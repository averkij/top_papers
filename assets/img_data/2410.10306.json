[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x1.png",
                "caption": "Figure 1:Animations produced byAnimate-Xwhich extends beyond human to anthropomorphic characters with various body structures,e.g., without limbs, from games, animations, and posters.",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x2.png",
                "caption": "Figure 2:(a) The overview of ourAnimate-X. Given a reference imageIrsuperscript拣I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, we first extract CLIP image featurefrsubscriptsuperscriptf^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPTand latent featurefersubscriptsuperscriptf^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTvia CLIP image encoder桅桅\\Phiroman_桅and VAE encoder扳\\mathcal{E}caligraphic_E.\nThe proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI)\nproduce motion featurefisubscriptf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand pose featurefesubscriptf_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, respectively.fesubscriptf_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTis concatenated with the noised input系italic-系\\epsilonitalic_系along the channel dimension, then further concatenated withfersubscriptsuperscriptf^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTalong the temporal dimension. This serves as the input to the diffusion model系胃subscriptitalic-系\\epsilon_{\\theta}italic_系 start_POSTSUBSCRIPT italic_胃 end_POSTSUBSCRIPTfor progressive denoising. During the denoising process,frsubscriptsuperscriptf^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPTandfisubscriptf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTprovide appearance condition fromIrsuperscript拣I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPTand motion condition fromI1:Fdsubscriptsuperscript拣:1I^{d}_{1:F}italic_I start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_F end_POSTSUBSCRIPT.\nAt last, a VAE decoder\\mathcal{D}caligraphic_Dis adopted to map the generated latent representationz0subscript0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTto the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator.",
                "position": 180
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x3.png",
                "caption": "Figure 3:Examples from ourA2superscript2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBench.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2410.10306/extracted/5924712/figs/setting_demo.png",
                "caption": "Figure 4:The illustration of comparison settings.",
                "position": 286
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x4.png",
                "caption": "Figure 5:Qualitative comparisons with state-of-the-art methods.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x5.png",
                "caption": "Figure 6:Qualitative comparisons with Unianimate in terms of long video generation.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x6.png",
                "caption": "Figure 7:Visualization of ablation study on IPI and EPI.",
                "position": 649
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ANetwork Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x7.png",
                "caption": "Figure 8:More example for EPI.",
                "position": 1916
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x8.png",
                "caption": "Figure 9:The difference of training and inference pipeline. During training, the reference image and the driven video come from the same video, while in the inference pipeline, the reference image and the driven video can be from any sources and appreciably different.",
                "position": 1919
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x9.png",
                "caption": "Figure 10:Detailed pipeline for buildingA2superscript2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBenchbased on large-scale pretrained models, including Open-ChatGPT 4o and KLing AI.",
                "position": 1925
            }
        ]
    },
    {
        "header": "Appendix BBenchmark Details",
        "images": []
    },
    {
        "header": "Appendix CUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x10.png",
                "caption": "Figure 11:Visualization of cases in the user study",
                "position": 1999
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10306/x11.png",
                "caption": "Figure 12:Visualization of the robustness ofAnimate-X.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x12.png",
                "caption": "Figure 13:Ablation study on the weight伪\\alphaitalic_伪of Implicit Pose Indicator. To better visualize the impact of伪\\alphaitalic_伪on performance, we normalize all the values to the range of 0 to 1.",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x13.png",
                "caption": "Figure 14:Visualization comparison on TikTok dataset andA2superscript2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBench.",
                "position": 2668
            },
            {
                "img": "https://arxiv.org/html/2410.10306/x14.png",
                "caption": "Figure 15:Comparison with more SOTAs onA2superscript2A^{2}italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTBench.",
                "position": 2671
            }
        ]
    },
    {
        "header": "Appendix EDiscussion",
        "images": []
    }
]