[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/x1.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Retrieval Augmented Personalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/x2.png",
                "caption": "Figure 2:Retrieval-Augmented Personalization Framework. Region-of-interest detected by an open world detector are used to retrieve concepts from the database. The images and information of the retrieved concepts are then integrated into the input for the MLLM.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x3.png",
                "caption": "Figure 3:Our Pipeline for Data Collection.We first crop the target concept from the image based on the dataset annotations and then query Gemini[12]to generate its personalized description. We also apply data augmentation to diversify these cropped images. Then we combine them with the original image to derive a series of instructions and answers from Gemini. When noise concepts are included in the additional information, the answer remains unchanged, helping to train the MLLMsâ€™ ability to filter out irrelevant concepts.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x4.png",
                "caption": "Table 2:Qualitative Comparison on Image Captioning.Image examples of target concepts are shown in the left and captions generated are shown in the right. We usegreentext to denote correct target concepts.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x5.png",
                "caption": "",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x6.png",
                "caption": "",
                "position": 356
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/x7.png",
                "caption": "Table 3:Quantitative Evaluation on Image Captioning.We report Recall, Precision and F1-score in the table, the best result in each metric is bold and the second is underlined.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x7.png",
                "caption": "Figure 4:Performance undervarying number of personalized concepts.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x8.png",
                "caption": "Figure 5:Performance of Our Retriever. Top-K recall rates under varying database size N.",
                "position": 647
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview",
        "images": []
    },
    {
        "header": "Appendix BAdditional Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/x9.png",
                "caption": "Figure 6:Time Cost of Personalization. We conduct experiment with 2 A800 GPUs.",
                "position": 1731
            }
        ]
    },
    {
        "header": "Appendix CMore Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/x10.png",
                "caption": "Figure 7:Composition of our dataset.",
                "position": 1767
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x10.png",
                "caption": "Figure 7:Composition of our dataset.",
                "position": 1770
            }
        ]
    },
    {
        "header": "Appendix DDetails of Dataset",
        "images": []
    },
    {
        "header": "Appendix EAdditional Demonstrations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/maeve_dog.jpeg",
                "caption": "Table 11:Examples of Concept Editing. Based on the information recorded in the database, our RAP-LLaVA can provide reliable and accurate answers.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/concept_editing/maeve_dog_11.jpg",
                "caption": "",
                "position": 2041
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/Bingo.jpg",
                "caption": "Table 12:Examples of Concept Updating. The first caption is generated when toy2 not yet stored in the database. Once the new concept is added, RAP-LLaVA can recognize both toy1 and toy2.",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/Bluey.jpg",
                "caption": "",
                "position": 2112
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/concept_editing/Bluey_and_Bingo_10.jpg",
                "caption": "",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/my_cat_5.jpeg",
                "caption": "Table 13:Examples of personalized conversations obtained by RAP-LLaVA.",
                "position": 2170
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/my_cat_4.jpg",
                "caption": "",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/oong_0.png",
                "caption": "",
                "position": 2259
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/oong_4.png",
                "caption": "",
                "position": 2287
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/dug_3.png",
                "caption": "",
                "position": 2334
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/phuc-map.png",
                "caption": "",
                "position": 2394
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/bo_6.png",
                "caption": "Table 14:Examples of personalized conversations obtained by RAP-Phi3-V.",
                "position": 2445
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/bo.png",
                "caption": "",
                "position": 2487
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/Bond.jpg",
                "caption": "",
                "position": 2554
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/Anya_and_Bond_4.jpg",
                "caption": "",
                "position": 2582
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/additional_demo/QA/ciin_3.png",
                "caption": "",
                "position": 2620
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x11.png",
                "caption": "Table 15:Additional qualitative comparison on image captioning between RAP-LLaVA and other methods.",
                "position": 2679
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x12.png",
                "caption": "",
                "position": 2719
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x13.png",
                "caption": "",
                "position": 2746
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x14.png",
                "caption": "",
                "position": 2773
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x15.png",
                "caption": "",
                "position": 2800
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x16.png",
                "caption": "",
                "position": 2827
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x17.png",
                "caption": "Table 16:Additional qualitative comparison on image captioning between RAP-Phi3-V and other methods.",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x18.png",
                "caption": "",
                "position": 2890
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x19.png",
                "caption": "",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x20.png",
                "caption": "",
                "position": 2944
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x21.png",
                "caption": "",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x22.png",
                "caption": "",
                "position": 2998
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x23.png",
                "caption": "Table 17:Qualitative results of personalized image description obtained by RAP-LLaVA.",
                "position": 3021
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x24.png",
                "caption": "",
                "position": 3059
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x25.png",
                "caption": "",
                "position": 3084
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x26.png",
                "caption": "",
                "position": 3109
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x27.png",
                "caption": "",
                "position": 3134
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x28.png",
                "caption": "",
                "position": 3159
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x29.png",
                "caption": "Table 18:Qualitative results of personalized image description obtained by RAP-Phi3-V.",
                "position": 3180
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x30.png",
                "caption": "",
                "position": 3218
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x31.png",
                "caption": "",
                "position": 3243
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x32.png",
                "caption": "",
                "position": 3268
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x33.png",
                "caption": "",
                "position": 3293
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x34.png",
                "caption": "",
                "position": 3318
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x35.png",
                "caption": "Table 19:Qualitative results of personalized concept recognition obtained by RAP-LLaVA. We usegreenrectangle to show the bounding box in the image.",
                "position": 3339
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x36.png",
                "caption": "",
                "position": 3384
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x37.png",
                "caption": "",
                "position": 3409
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x38.png",
                "caption": "",
                "position": 3441
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x39.png",
                "caption": "",
                "position": 3466
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x40.png",
                "caption": "",
                "position": 3498
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x41.png",
                "caption": "Table 20:Qualitative results of personalized concept recognition obtained by RAP-LLaVA. We usegreenrectangle to show the bounding box in the image.",
                "position": 3519
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x42.png",
                "caption": "",
                "position": 3564
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x43.png",
                "caption": "",
                "position": 3589
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x44.png",
                "caption": "",
                "position": 3621
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x45.png",
                "caption": "",
                "position": 3646
            },
            {
                "img": "https://arxiv.org/html/2410.13360/x46.png",
                "caption": "",
                "position": 3678
            }
        ]
    },
    {
        "header": "Appendix FLimitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/Anya.jpg",
                "caption": "Table 25:Examples of our database. A concept should be provided with an image and its personalized description.",
                "position": 4043
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/asian_doll.jpg",
                "caption": "",
                "position": 4085
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/cat_statue.jpg",
                "caption": "",
                "position": 4169
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/ginger_cat.jpg",
                "caption": "",
                "position": 4197
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/Hans.jpg",
                "caption": "",
                "position": 4225
            },
            {
                "img": "https://arxiv.org/html/2410.13360/extracted/6007792/figure/database/Teri.jpg",
                "caption": "",
                "position": 4281
            }
        ]
    },
    {
        "header": "Appendix GExamples of the Personalized Database",
        "images": []
    }
]