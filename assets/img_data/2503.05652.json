[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/x1.png",
                "caption": "Figure 2:Ecological distributions of task-relevant objects involved in daily household activities.Left:The horizontal distance distribution follows a long-tail distribution.Right:The vertical distance distribution exhibits multiple distinct modes, located at1.43mtimes1.43meter1.43\\text{\\,}\\mathrm{m}start_ARG 1.43 end_ARG start_ARG times end_ARG start_ARG roman_m end_ARG,0.94mtimes0.94meter0.94\\text{\\,}\\mathrm{m}start_ARG 0.94 end_ARG start_ARG times end_ARG start_ARG roman_m end_ARG,0.49mtimes0.49meter0.49\\text{\\,}\\mathrm{m}start_ARG 0.49 end_ARG start_ARG times end_ARG start_ARG roman_m end_ARG, and0.09mtimes0.09meter0.09\\text{\\,}\\mathrm{m}start_ARG 0.09 end_ARG start_ARG times end_ARG start_ARG roman_m end_ARG, representing heights at which household objects are typically found.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x2.png",
                "caption": "Figure 3:BRShardware system.Left:The R1 robot’s dimensions, range of motion, and onboard sensors. The robot features two 6-DoF arms, each equipped with a parallel jaw gripper, and a 4-DoF torso. The torso is mounted on an omnidirectional mobile base with three wheel motors and three steering motors.Right:TheJoyLosystem, consisting of two kinematic-twin arms constructed using 3D-printed components and low-cost Dynamixel motors. Compact, off-the-shelf Nintendo Joy-Con controllers are mounted at the one end of the arms, serving as the interface for controlling the grippers, torso, and mobile base. To ensure sufficient stall torque for the shoulder joints, two Dynamixel motors are coupled together.",
                "position": 163
            }
        ]
    },
    {
        "header": "IIHardware System",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/x3.png",
                "caption": "Figure 4:WB-VIMAmodel architecture for imitation learning.WB-VIMAautoregressively denoises whole-body actions within the embodiment space and dynamically aggregates multi-modal observations using self-attention. By leveraging the hierarchical interdependencies within the robot’s embodiment and the rich information provided by multi-modal sensory inputs,WB-VIMAenables effective whole-body policy learning.",
                "position": 300
            }
        ]
    },
    {
        "header": "IIILearning Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/x4.png",
                "caption": "Figure 5:Success rate for five representative household activities.“ET” denotes the entire task and “ST” denotes sub-task. Numerical results are provided in AppendixD-B.",
                "position": 437
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/x5.png",
                "caption": "(a)",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x5.png",
                "caption": "(a)",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x6.png",
                "caption": "(b)",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x7.png",
                "caption": "Figure 7:Ablation study results for tasks “put items onto shelves” and “lay clothes out”.“w/o W.B. Action Denoising” refers to the variant without autoregressive whole-body action denoising.\n“w/o Multi-Modal Obs. Attn.” refers to the variant without multi-modal observation attention.",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x8.png",
                "caption": "Figure 8:Emergent behaviors of learnedWB-VIMApolicies.(a) and (b):The trained policies leverage the torso and mobile base to improve maneuverability. In (a), the robot bends its hip forward and advances the mobile base to push the door open. In (b), after grasping the dishwasher handle, the robot moves its base backward to pull the dishwasher open.(c):The trained policy exhibits failure recovery behavior. On the first attempt to close the toilet cover, the robot’s gripper is too far to reach it. The policy adjusts by tilting the torso forward, bringing the gripper closer, and successfully closing the cover.",
                "position": 716
            }
        ]
    },
    {
        "header": "VRelated Work",
        "images": []
    },
    {
        "header": "VILimitations and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/extracted/6261453/appendix/figs/arm_diagram.png",
                "caption": "(a)",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2503.05652/extracted/6261453/appendix/figs/arm_diagram.png",
                "caption": "(a)",
                "position": 1960
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x9.png",
                "caption": "(b)",
                "position": 1966
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x10.png",
                "caption": "(c)",
                "position": 1971
            }
        ]
    },
    {
        "header": "Appendix ARobot Hardware Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/extracted/6261453/appendix/figs/fused_pcd_visualization-fig.png",
                "caption": "Figure A.2:Visualization of the fused, ego-centric colored point clouds.Left:The colored point cloud observation, aligned with the robot’s coordinate frame.Right:The robot’s orientation and its surrounding environment.",
                "position": 2181
            }
        ]
    },
    {
        "header": "Appendix BJoyLoDetails",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/extracted/6261453/appendix/figs/joylo_disassembled_2k.png",
                "caption": "Figure A.3:IndividualJoyLoarm links.",
                "position": 2291
            }
        ]
    },
    {
        "header": "Appendix CModel Architectures, Policy Training, and Deployment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05652/x11.png",
                "caption": "Figure A.4:Generalization settings for the task “clean house after a wild party”.From left to right: seen and unseen bowl variations, robot’s starting region, and initial object placements on the gaming table.",
                "position": 2598
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x12.png",
                "caption": "Figure A.5:Generalization settings for the task “clean the toilet”.From left to right: robot’s starting region, sponge variations, and initial placements.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x13.png",
                "caption": "Figure A.6:Generalization settings for the task “take trash outside”.From left to right: initial placement region of the trash bag and robot’s starting region.",
                "position": 2637
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x14.png",
                "caption": "Figure A.7:Generalization settings for the task “put items onto shelves”.From left to right: robot’s starting region, box placements, and shelf configurations.",
                "position": 2647
            },
            {
                "img": "https://arxiv.org/html/2503.05652/x15.png",
                "caption": "Figure A.8:Generalization settings for the task “lay clothes out”.From left to right: robot’s starting region, clothing placements, and clothing variations.",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2503.05652/extracted/6261453/appendix/figs/user_study_example-fig.png",
                "caption": "Figure A.9:Successful task completion by a participant.The robot navigates to a dishwasher and opens it, moves to a table to collect teacups, returns to the dishwasher, places the teacups inside, and closes it.",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2503.05652/extracted/6261453/appendix/figs/user_study_annotation_gui-fig.png",
                "caption": "Figure A.10:GUI for annotating user study rollouts.",
                "position": 2723
            }
        ]
    },
    {
        "header": "Appendix DTask Definition and Evaluation Details",
        "images": []
    }
]