[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17540/x1.png",
                "caption": "Figure 1:Given the user prompt \"a photo of a couch below a vase\", existing models like DELL-E3 generate rich language descriptions but often produce unrealistic or physically implausible compositions. In contrast, our RePrompt performs explicit chain-of-thought reasoning to resolve spatial relations, resulting in enhanced prompts that guide text-to-image models towards realistic and semantically aligned generations.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x2.png",
                "caption": "Figure 2:Overview of the proposed RePrompt. For each input prompt, RePrompt generates multiple reasoning trace and enhanced prompt pairs. The reasoning trace guides the model to produce more detailed, image-grounded prompts. These are used to synthesize candidate images via a T2I model, which are then scored by a reward model. Feedback is used to update RePrompt via GRPO.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17540/x3.png",
                "caption": "Figure 3:The Visual-Reasoning Reward.",
                "position": 204
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17540/extracted/6470159/Figure/grouped_barplot.png",
                "caption": "Figure 4:Impact of our method across different base T2I models on the GenEval benchmark. Our method consistently improves the compositional understanding across all base models.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x4.png",
                "caption": "Figure 5:Qualitative results on compositional prompts. Compared to vanilla T2I models, our RePrompt improves spatial layout and object relations by generating enhanced prompts with explicit reasoning, leading to more faithful compositions.",
                "position": 898
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVariance Reduction via Structured Reasoning",
        "images": []
    },
    {
        "header": "Appendix BExperiment Setting",
        "images": []
    },
    {
        "header": "Appendix CAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17540/extracted/6470159/Figure/line.png",
                "caption": "Figure 6:Training curve of our RePrompt during reinforcement learning. The curve shows steady reward improvement, indicating stable training dynamics and effective reward signal.",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x5.png",
                "caption": "Figure 7:Training curve with different visual reasoning rewards. Using a single reward model leads to instability and suboptimal performance during training. In contrast, our balanced reward design—combining multiple specialized reward signals—yields more stable convergence and higher final reward values.",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x6.png",
                "caption": "Figure 8:Qualitative results on compositional prompts (Position). We compare DELL-E3, the intermediate reasoning process, and our final RePrompt outputs. While DELL-E3 often struggles with spatial relations (e.g., object positions), our reasoning-guided approach enables more accurate and faithful generations that better match the prompt’s compositional constraints.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x7.png",
                "caption": "Figure 9:Qualitative results on compositional prompts (Two-object). We show comparisons among DELL-E3, our reasoning process, and RePrompt outputs on prompts involving two objects. DELL-E3 often fails to generate both entities accurately, whereas our method uses explicit reasoning to guide the model in generating semantically correct and compositionally faithful images.",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x8.png",
                "caption": "Figure 10:Qualitative results on compositional prompts (Color). We present qualitative comparisons on prompts involving specific color attributes. While DELL-E3 tends to ignore or misinterpret color constraints, our approach leverages explicit reasoning to ensure accurate color grounding for each object, resulting in more faithful visual compositions.",
                "position": 2348
            },
            {
                "img": "https://arxiv.org/html/2505.17540/x9.png",
                "caption": "Figure 11:Qualitative results on compositional prompts (Attribute binding). We show examples where prompts specify multiple attributes that must be correctly associated with the corresponding objects. Our method utilizes step-by-step reasoning to disambiguate attribute-object bindings, avoiding attribute swaps or omissions that are common in baseline outputs.",
                "position": 2351
            }
        ]
    },
    {
        "header": "Appendix DMore Cases",
        "images": []
    }
]