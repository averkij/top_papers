[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01790/x1.png",
                "caption": "Figure 1:When provided with diverse prompt templates, LLMs provide different but semantically equivalent responses. Heuristic evaluation fails to match the different answers with the ground truth, exaggerating prompt sensitivity. In contrast, an LLM judge is able to identify the semantic equivalence consistently.111This is an example from NarrativeQA. Heuristic method uses word-level F1; “Incorrect” is shown here for illustration purposes, indicating a lower score than a correct answer.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01790/x2.png",
                "caption": "Figure 2:The mean and standard deviation of performance across different prompt templates. For all 6 datasets, we show the statistics for all pairs of evaluation methods and models, excluding the cases when the model’s context length is not enough for the task or when the heuristic evaluation method is not available. The standard deviation of the LLM-as-a-Judge method is always low. For NarrativeQA, the absence of results for Mistral and Qwen is primarily due to the long-context requirement of this dataset.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2509.01790/x3.png",
                "caption": "",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2509.01790/x4.png",
                "caption": "",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2509.01790/x5.png",
                "caption": "",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2509.01790/x6.png",
                "caption": "",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2509.01790/x7.png",
                "caption": "",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2509.01790/x8.png",
                "caption": "",
                "position": 249
            }
        ]
    },
    {
        "header": "4LLM-as-a-Judge Evaluation Aligns with Human Annotations",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADiverse Prompts",
        "images": []
    },
    {
        "header": "Appendix BPrompts for LLM-as-a-Judge",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01790/x9.png",
                "caption": "Figure 3:An example of a judging prompt. After filling in the question, reference answer, and model prediction, we send the prompt to an LLM judge to get the result.",
                "position": 1269
            }
        ]
    },
    {
        "header": "Appendix CHeuristic Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix DHuman Annotation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results on Older LLMs",
        "images": []
    }
]