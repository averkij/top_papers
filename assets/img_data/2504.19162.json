[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19162/x1.png",
                "caption": "Figure 1:We continuously generate reinforcement training samples for the critic through adversarial games. The sneaky generator aims to create subtle erroneous steps to challenge the critic, while the critic must accurately distinguish between correct and incorrect steps from a mixed input of them. Benefiting from the opposing optimization objectives, both models can evolutionally learn from each other, akin to how humans improve their skills in board games through competition.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19162/x2.png",
                "caption": "Figure 2:The framework of our proposed SPC. We randomly select a correct step along with the partial solution before that step and feed them into the sneaky generator, which first selects one of the predefined error types and then converts the correct step into an incorrect step. The successfully generated incorrect step is then fed to the critic for error detection. If the critic successfully identifies the error, it receives a reward of +1, while the sneaky generator incurs a reward of -1. If the critic is deceived, the critic and sneaky generator are rewarded -1 and +1, respectively.",
                "position": 143
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19162/x3.png",
                "caption": "Figure 3:Ablation study of our critic and sneaky generator. Left: the impact of different strategies on evolving critic models. Right: the success rate of sneaky generator attacking LLM solver and its win rate against the round 0 and round 1 critics.",
                "position": 853
            }
        ]
    },
    {
        "header": "5Conclusion and Societal Impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix APrompts",
        "images": []
    },
    {
        "header": "Appendix BMore Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.19162/x4.png",
                "caption": "Figure 4:Prompt for querying GPT-4 to collect raw data of sneaky transformation CoT.",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2504.19162/x5.png",
                "caption": "Figure 5:Prompt for querying DeepSeek-R1-Distill-Qwen-7B to collect raw critiques with long CoT.",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2504.19162/x6.png",
                "caption": "Figure 6:Prompt for querying GPT-4o to rewrite a long critique into a brief and standardized critique.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2504.19162/x7.png",
                "caption": "Figure 7:Prompt for training the sneaky generator.",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2504.19162/x8.png",
                "caption": "Figure 8:Prompt for training the critic model.",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2504.19162/x9.png",
                "caption": "Figure 9:SPC critiques on ProcessBench before and after self-play training.",
                "position": 1698
            }
        ]
    },
    {
        "header": "Appendix CCase Analysis",
        "images": []
    }
]