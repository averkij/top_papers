[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19851/extracted/6568123/assets/teaser.png",
                "caption": "Figure 1.Diverse articulated 3D models animated usingAnimaX. The created animation, spanning various categories including humanoids, animals, and fictional models, demonstrates the versatility of our method. Selected models are visualized with keyframes of their predicted animations on the conveyor belts.",
                "position": 140
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19851/x1.png",
                "caption": "Figure 2.Animation results on generalized 3D models, including biped 3D assets, animals, chests, robotic arms.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2506.19851/x2.png",
                "caption": "Figure 3.Illustration ofAnimaX. Given an articulated 3D mesh,AnimaXcreates a sequence of 3D animation in minutes.AnimaXhas two stages: (1) generating multi-view consistent videos and corresponding pose sequences simultaneously, conditioned on rendered template views and pose maps from the input mesh, with a textual description of the motion; and (2) recovering 3D joint positions per frame using multi-view triangulation(Hartley and Sturm,1997)and applying inverse kinematics to obtain the joint angles and animate the mesh.",
                "position": 335
            }
        ]
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19851/x3.png",
                "caption": "Figure 4.Input and output examples of our multi-view video-pose diffusion models, with input prompts “A wolf is attacking something” and “A girl casting spell with hands forward”. We show two views here, but the model actually generates four views.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2506.19851/x4.png",
                "caption": "Figure 5.Comparison with state-of-the-art generalizable 4D generation methods. We compare our method with representative 3D-to-4D methods, including MotionDreamer(Uzolas et al.,2024)and Animate3D(Jiang et al.,2024a). Our model can synthesize more correct and authentic animation clips compared to these methods which rely on optimization of neural deformation fields and do not involve low-level skeleton-based representation.",
                "position": 422
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19851/x5.png",
                "caption": "Figure 6.Qualitative ablation results on the joint video-pose diffusion model.",
                "position": 495
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]