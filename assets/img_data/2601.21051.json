[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21051/x1.png",
                "caption": "Figure 1:Performance comparison of Foundation-Sec-8B-Reasoning against baseline models. (a) On cybersecurity benchmarks (CTIBench-MCQA,CTIBench-RCM, CTI-Reasoning, CWE-Prediction, SecBench, SecEval), our model performs on par with the 70B model (Llama-3.3-70B-Instruct) while significantly outperforming our previous instruction-tuned model (Foundation-Sec-8B-Instruct). (b) On general-purpose benchmarks (AlpacaEval2,BBH,IFEval,GSM8K,HumanEval,MATH), our model achieves comparable performance to Llama-3.1-8B-Instruct on most tasks, with significantly better performance onAlpacaEval2.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2601.21051/x2.png",
                "caption": "",
                "position": 283
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21051/x3.png",
                "caption": "Figure 2:Data composition for the SFT and RL training stages. The SFT stage uses a diverse mix of data to instill broad reasoning abilities, while the RL stage emphasizes instruction following, cybersecurity, and mathematical reasoning to further refine the model’s reasoning abilities.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2601.21051/x4.png",
                "caption": "",
                "position": 370
            }
        ]
    },
    {
        "header": "4Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21051/x5.png",
                "caption": "Figure 3:Comparison of selected models across 6 key cybersecurity benchmarks. The benchmarks are organized in two rows: (top row)CTIBench-MCQA,CTIBench-RCM, andCTI-Reasoning; (bottom row)CWE-Prediction,SecBench-Reasoning, andSecEval. Foundation-Sec-8B-Reasoning demonstrates consistently strong performance across diverse tasks, particularly excelling onCTIBench-RCM (75.3%) andCWE-Prediction(70.4%). The visualization highlights the model’s competitive performance with significantly larger models while maintaining robust capabilities across both knowledge-based and reasoning-intensive benchmarks.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2601.21051/x6.png",
                "caption": "Figure 4:Comparison of model performance across 6 key general-purpose benchmarks. The benchmarks are organized in two rows: (top row)AlpacaEval2 (human preference),BBH(reasoning), andIFEval(instruction following); (bottom row)GSM8K(grade school math),HumanEval(coding), andMATH(competition mathematics). Error bars represent one standard deviation. Foundation-Sec-8B-Reasoning (highlighted with hatched pattern) demonstrates strong performance across diverse capabilities, with exceptional results onAlpacaEval2 (62.6%) and competitive performance on reasoning and coding tasks.",
                "position": 1398
            },
            {
                "img": "https://arxiv.org/html/2601.21051/x7.png",
                "caption": "Figure 5:HarmBench safety evaluation results showing pass rates (percentage of harmful prompts appropriately refused) across different model configurations. Foundation-Sec-8B-Instruct achieves 95.00% pass rate, while Foundation-Sec-8B-Reasoning achieves93.00%with proper system prompts. When further protected by Llama-Guard-3-8B, Foundation-Sec-8B-Reasoning achieves98.25%pass rate, demonstrating that our reasoning model with appropriate safety measures delivers strong protection against adversarial prompts.",
                "position": 1433
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]