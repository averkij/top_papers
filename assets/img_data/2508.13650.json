[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13650/figures/method-main.png",
                "caption": "Figure 1:Overview of CRISP:(1) We identify features that are frequently and strongly activated by the target corpus—but not by the benign corpus—using pre-trained sparse autoencoders (SAEs). (2) We then fine-tune the model to suppress these features on the target corpus, while preserving their activations on the benign corpus.",
                "position": 185
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13650/x1.png",
                "caption": "Table 1:Evaluation results on the test sets across six metrics: Unlearn accuracy (lower is better), Retain accuracy, MMLU (general knowledge), Fluency score, Concept score, and the Overall score—computed as the harmonic mean of all metrics after normalization (seeSection˜4.4). CRISP outperforms competing methods in overall performance across all settings and most individual metrics. Standard deviations for the Fluency and Concept scores are inTable˜7.",
                "position": 440
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13650/x1.png",
                "caption": "Figure 2:Qualitative comparison of generations after different unlearning methods.We prompt about non-harmful biomedical knowledge that is topically related to harmful concepts from the WMDP-Bio dataset.\nWhile existing methods disrupt fluency or inject artifacts (e.g., repetition, formatting tokens), CRISP retains coherent and informative generations, demonstrating effective preservation of general-domain capabilities.",
                "position": 668
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13650/x2.png",
                "caption": "(a)Llama-3.1-8B",
                "position": 785
            },
            {
                "img": "https://arxiv.org/html/2508.13650/x2.png",
                "caption": "(a)Llama-3.1-8B",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2508.13650/x3.png",
                "caption": "(b)Gemma-2-2B",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2508.13650/x4.png",
                "caption": "(a)Target WMDP-Bio features in Llama-3.1-8B Layer 24.",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2508.13650/x4.png",
                "caption": "(a)Target WMDP-Bio features in Llama-3.1-8B Layer 24.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2508.13650/x5.png",
                "caption": "(b)Target WMDP-Bio features in Gemma-2-2B Layer 14.",
                "position": 827
            }
        ]
    },
    {
        "header": "6Feature Analysis",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGemma-2-2B Hyperparameters Tradeoff",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13650/x6.png",
                "caption": "Figure 5:Trade-off between Retain Accuracy (y-axis) and Unlearn Accuracy (x-axis) on the WMDP-Cyber benchmark. Top: Llama-3.1-8B, Bottom: Gemma-2-2B. Each point shows one of 200 hyperparameter settings per method. The red star indicates the ideal outcome—complete forgetting with no loss in retain accuracy. The solid line traces the best result per unlearning bucket, forming the Pareto frontier.",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2508.13650/x7.png",
                "caption": "",
                "position": 1436
            }
        ]
    },
    {
        "header": "Appendix BFeature Analysis and Explanation Tables",
        "images": []
    },
    {
        "header": "Appendix CCoherency Set",
        "images": []
    },
    {
        "header": "Appendix DFluency and Concept Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix EHyperparameters",
        "images": []
    },
    {
        "header": "Appendix FHardware Details",
        "images": []
    },
    {
        "header": "Appendix GLicenses and Third-Party Usage",
        "images": []
    }
]