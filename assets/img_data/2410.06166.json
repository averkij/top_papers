[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06166/x1.png",
                "caption": "Figure 1:Two popular Video LLMs struggle with basic temporal reasoning (left). We mitigate this issue via textual temporal transfer (middle), which demonstrates consistent improvement (right).",
                "position": 138
            }
        ]
    },
    {
        "header": "2Pinpointing Video LLM Temporal Reasoning Bottleneck",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06166/x2.png",
                "caption": "Figure 2:Example of videos and questions that focus on different temporal reasoning abilities.",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x3.png",
                "caption": "Figure 3:Temporal probing results for LongVA (upper) and VILA (lower). The probe on visual representations achieve>90absent90>90> 90accuracy in most cases, while the LLM decoders still have large room for improvement even with textual inputs, leading to the poor temporal understanding ability of Video LLMs. Detailed results of the sub-categories are reported in AppendixA.4.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Textual Temporal Understanding Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06166/x4.png",
                "caption": "Figure 4:Temporal-oriented question-answering pairs with textual image captions as context.",
                "position": 240
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06166/x5.png",
                "caption": "Figure 5:Textual temporal reasoning accuracy correlates positively with video understanding results on three benchmarks.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x6.png",
                "caption": "(a)Compared to MLP modules, self-attention modules are more important for temporal reasoning.",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x6.png",
                "caption": "(a)Compared to MLP modules, self-attention modules are more important for temporal reasoning.",
                "position": 898
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x7.png",
                "caption": "(b)Our temporal-enhanced models leverage more input video frames better.",
                "position": 903
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details of Video LLM Temporal Analysis",
        "images": []
    },
    {
        "header": "Appendix BMore Details of Textual Temporal Understanding Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.06166/x8.png",
                "caption": "Table 9:Example of a video focusing on theOrderaspect and corresponding frame captions used to evaluate the LLM decoder. We show four frames here but in practice eight frames are used. The words inbluehighlight the major subject depicted in each frame.",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x9.png",
                "caption": "Table 10:Example of a video focusing on theBrightness Attributeaspect and corresponding frame captions used to evaluate the LLM decoder. The key information of brightness change is highlighted inblue.",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x10.png",
                "caption": "Table 11:Example of a video focusing on theShape Attributeaspect and corresponding frame captions used to evaluate the LLM decoder. The key information of shape change is highlighted inblue.",
                "position": 2177
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x11.png",
                "caption": "Figure 7:Detailed temporal analysis results of LongVA.",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x12.png",
                "caption": "Figure 8:Detailed temporal analysis results of VILA.",
                "position": 2429
            },
            {
                "img": "https://arxiv.org/html/2410.06166/x13.png",
                "caption": "Figure 9:(Left) Exploration of mixing ratios between image-text instruction tuning and textual temporal QA. (Right) Dataset scaling analysis.",
                "position": 3221
            }
        ]
    },
    {
        "header": "Appendix CTraining Details",
        "images": []
    }
]