[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21978/x1.png",
                "caption": "Figure 1:General capabilities comparison of base VLMs (blue) and their reasoning-tuned variants (green/purple/red) on six representative, non-reasoning benchmarks (higher is better):\nA-OKVQA (knowledge-based VQA), AesBench(Huang et al.,2024b)(image aesthetics) , VStar(Wu & Xie,2023)(spatio-temporal reasoning), VisOnly(Kamoi et al.,2025)(vision-only recognition aggregate) , OCRBench(Liu et al.,2024b)(text recognition), and R-Bench-Dis(Li et al.,2025b)(distribution-shift robustness).\nAcross both Qwen2.5-VL families, reasoning-finetuned models generally underperform their base models on perception and robustness tasks, whereas MiMo-VL-7B-RL remains close to its SFT baseline.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x2.png",
                "caption": "Figure 2:Performance comparison between the model finetuned solely on math data and with RECAP. Our model not only preserves the base model performance but also improves it (2%) while the reasoning model quickly falls behind the base model after 100 iterations.",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x3.png",
                "caption": "Figure 3:Overview of our RECAP.Along with the target reasoning task, we sample data from general domains in order to maintain that knowledge during finetuning. Initially, the objectives of interest are reweighted uniformly to optimize the main model. After a few iterations, we record the convergence behavior of individual objectives. Based on this, we adjust the focus to prevent the dominance of any objectives and put less weight on saturated ones.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Our Method: Replay-Enhanced CApability Preservation (RECAP)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21978/x4.png",
                "caption": "Figure 4:Different rewards have different behaviors of convergence.While theformatreward is easy to optimize and obtains the highest rate of convergence, it quickly saturates and thus yields low convergence rate (c∼1c\\sim 1) and instability (i∼0i\\sim 0) after 50 steps. By contrast, the reasoningaccuracyfluctuates the most, thereby steering the optimization toward the corresponding objective.IoUandntpindicate the IoU reward and the next-token-prediction accuracy during training. The result is obtained on the first setting in our experiment section below.",
                "position": 321
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21978/x5.png",
                "caption": "Figure 5:Evolution of format and accuracy rewards on the reasoning domain during training:Training curves for the format reward over full training (left), an early-training zoom (middle), and the accuracy reward (right). While the uniform baseline is better in maximizing the format reward, it falls behind our proposed method later in terms of accuracy, as we prioritize correct solutions over formatting once the model can follow the predefined template. Curves are smoothed with an exponential moving average for readability.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x6.png",
                "caption": "",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x7.png",
                "caption": "",
                "position": 700
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21978/x8.png",
                "caption": "Figure 8:Example conversation for finetuningWe ask the model to produce its reasoning chain within <think>…\\dots</think> and the final answer in <answer>…\\dots</answer> tags.",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x9.png",
                "caption": "Figure 9:Generated answer from reasoning-only finetuned model and our modelWhile the reasoning-only finetuned model generates long reasoning chains for simple visual questions, our model produces more concise answers, especially on non-math tasks.",
                "position": 2485
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x10.png",
                "caption": "Figure 10:Final performance across metrics.We compare a uniform baseline with our dynamic reweighting. The gains on correctness-oriented metrics indicate that reallocating weight away from saturated format rewards toward harder objectives yields better solutions without sacrificing adherence to templates.",
                "position": 2549
            },
            {
                "img": "https://arxiv.org/html/2510.21978/x11.png",
                "caption": "Figure 11:Reward dynamics and variability during RLVR training.Per-step rewards (light traces) and sliding-window means (dark curves) for six metrics: Next-Token Prediction, Answer-Format, Thinking-Format, IoU, Accuracy, and the Total-Reward Standard Deviation (lower-right). Asynchronous convergence and high variances motivate short-horizon statistics for dynamic objective reweighting rather than per-iteration magnitudes.",
                "position": 2559
            }
        ]
    },
    {
        "header": "7Experimental setup",
        "images": []
    }
]