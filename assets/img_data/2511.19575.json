[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/huggingface.jpeg",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/github.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2511.19575/x1.png",
                "caption": "Figure 1:Performance comparison of HunyuanOCR and other SOTA models.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/sun.png",
                "caption": "Table 1:Performance comparison of different VLMs and OCR systems across multiple tasks.indicates Supported and High-Performing,indicates Supported with Moderate Performance, andindicates Supported but Underperforming. Otherwise, it is Not Supported.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/moon.png",
                "caption": "",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/star.png",
                "caption": "",
                "position": 143
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Model Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19575/x2.png",
                "caption": "Figure 2:The Architecture of HunyuanOCR: An end-to-end framework integrating Native Resolution Visual Encoder, Adaptive MLP Connector, and a Lightweight Language Model for diverse OCR tasks, including: spotting, parsing, information extraction, visual question answering, and text image translation.",
                "position": 431
            }
        ]
    },
    {
        "header": "4Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19575/x3.png",
                "caption": "Figure 3:Illustration of image data synthesis and data augmentation results for the HunyuanOCR data pipeline.\n(a) Multilingual synthetic data with right-to-left (RTL) reading order.\n(b) Long-document, paragraph-level synthesis with controllable line-level font, language, rotation, and RGB values.\n(c) Document image warping with realistic defects, including perspective distortion, blur, and local lighting variation.\n(d) Cross-task data reuse: from spotting data to automated QA generation.\n(e) Cross-task data reuse: from multilingual parsing data to real-world text translation.",
                "position": 600
            }
        ]
    },
    {
        "header": "5Training Recipe",
        "images": []
    },
    {
        "header": "6Evaluation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Contributors",
        "images": []
    },
    {
        "header": "Appendix ARecommended Instruction",
        "images": []
    },
    {
        "header": "Appendix BCommon Supported IE Categories",
        "images": []
    },
    {
        "header": "Appendix CReinforcement Learning Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.19575/x4.png",
                "caption": "Figure 4:Training dynamics of RL. We show the proportions of all-one rewards and the mean reward value, which increases steadily over the course of training.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/spotting/art_16.jpg",
                "caption": "Figure 5:Robust Text Spotting Results of HunyuanOCR on Artistic Font.",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/spotting/vis_art_16.jpg",
                "caption": "",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/spotting_origin.png",
                "caption": "Figure 6:Robust Text Spotting Results of HunyuanOCR on Dense Documents.",
                "position": 2120
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/spotting.png",
                "caption": "",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/spotting/document_23.jpg",
                "caption": "Figure 7:Robust Text Spotting Performance of HunyuanOCR in Complex Document Scenarios.",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/spotting/vis_document_23.jpg",
                "caption": "",
                "position": 2163
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/parsing_fig.png",
                "caption": "Figure 8:Robust Parsing Performance of HunyuanOCR in Complex Figure Scenarios.",
                "position": 2181
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/vis_parsing_fig.png",
                "caption": "",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/parsing_table.png",
                "caption": "Figure 9:Robust Parsing Performance of HunyuanOCR in Complex Table Scenarios.",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/vis_parsing_table.png",
                "caption": "",
                "position": 2261
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/parsing_table_2.png",
                "caption": "Figure 10:Robust Parsing Performance of HunyuanOCR in Complex Table Scenarios.",
                "position": 2279
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/table_vis_gl.png",
                "caption": "",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/wildomni.jpg",
                "caption": "Figure 11:Robust Parsing Performance of HunyuanOCR in Wild-OmniDocBench.",
                "position": 2311
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/parsing_rgsj.png",
                "caption": "Figure 12:Robust Parsing Performance of HunyuanOCR in Diverse Real-World Visual Scenarios.",
                "position": 2358
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/parsing_rgsjz_2.png",
                "caption": "",
                "position": 2376
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/qikai1.png",
                "caption": "",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/parsing/guwan1.png",
                "caption": "",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/docml.png",
                "caption": "Figure 13:Robust Parsing Performance of HunyuanOCR in DocML.",
                "position": 2414
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/Docml_2.png",
                "caption": "",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/DocML3.png",
                "caption": "",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/liuchengtu.png",
                "caption": "Figure 14:Robust Parsing Performance of HunyuanOCR in Flowchart Scenarios.",
                "position": 2482
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/liuchengtu_vis.png",
                "caption": "",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/zhuzhuang.png",
                "caption": "Figure 15:Robust Parsing Performance of HunyuanOCR in Chart Scenarios.",
                "position": 2538
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/zhuzhuang_vis.png",
                "caption": "",
                "position": 2550
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/xiangxian.png",
                "caption": "",
                "position": 2580
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/xiangxian_vis.png",
                "caption": "",
                "position": 2585
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/Indonesia_Books_71016.jpg",
                "caption": "Figure 16:Translation Performance of HunyuanOCR.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/translation/translation2.png",
                "caption": "",
                "position": 2628
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/ie2.jpg",
                "caption": "Figure 17:Information Extraction (IE) Performance of HunyuanOCR on Receipts.",
                "position": 2656
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/ie_parallel.jpg",
                "caption": "",
                "position": 2672
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/zimu2.jpg",
                "caption": "Figure 18:Video Subtitle Extraction Performance of HunyuanOCR.",
                "position": 2697
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/zimu3.jpg",
                "caption": "",
                "position": 2714
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/zimu5.jpg",
                "caption": "",
                "position": 2726
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/gzyh0227_9.png",
                "caption": "Figure 19:Document & Chart Visual Question Answering (VQA) performance of HunyuanOCR.",
                "position": 2741
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/multi_col_60150.png",
                "caption": "",
                "position": 2757
            },
            {
                "img": "https://arxiv.org/html/2511.19575/imgs/appendix/IE_VQA/two_col_23773.png",
                "caption": "",
                "position": 2769
            }
        ]
    },
    {
        "header": "Appendix DQualitative examples",
        "images": []
    }
]