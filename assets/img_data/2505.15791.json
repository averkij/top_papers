[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15791/x1.png",
                "caption": "Figure 1:Illustration of the proposed VARD.Different colors represent distinct diffusion trajectories, indexed by the superscripts on states (xtisubscriptsuperscriptùë•ùëñùë°x^{i}_{t}italic_x start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) and final rewards (RisuperscriptùëÖùëñR^{i}italic_R start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT). At intermediate steps, VARD‚Äôs learned PRM calculates the value as the expected final reward, estimated by averagingRisubscriptùëÖùëñR_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTof relevant trajectories considered to pass through the current state representation.\nFor instance, as depicted for the intermediate state representationxt11superscriptsubscriptùë•subscriptùë°11x_{t_{1}}^{1}italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, the value is computed asR3+R2+R03superscriptùëÖ3superscriptùëÖ2superscriptùëÖ03\\frac{R^{3}+R^{2}+R^{0}}{3}divide start_ARG italic_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT + italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_R start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_ARG start_ARG 3 end_ARG, thereby aggregating information from trajectories 0, 2, and 3.\nThis predicted value serves as a dense, global supervisory signal, utilized alongside KL regularization for backpropagation throughout the diffusion process.",
                "position": 143
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15791/x2.png",
                "caption": "Figure 2:Qualitative analysis of generated protein structure from FoldFlow-2 base and VARD version.Alpha-helices, beta-sheets and coils are coloredyellow,greenandgray, respectively. The VARD version produces structures with significantly more beta-sheet regions, whereas the FoldFlow-2 base model generates almost exclusively alpha-helix regions.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x3.png",
                "caption": "Figure 3:Secondary structure distribution comparison between FoldFlow-2-base and FoldFlow-2-VARD generated proteins.Proteins are sampled at 50-residue increments between lengths 100-300, with 64 samples per length.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x4.png",
                "caption": "Figure 4:Qualitative analysis of compressibility vs. incompressibility rewards.Generated images correspond to checkpoints saved at 20-step intervals during training.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x5.png",
                "caption": "Figure 5:Comparative visualization.Compared to ReFL and DRaFT-1 aligned with specified reward preferences, VARD achieves demonstrably superior image quality and enhanced realism.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x6.png",
                "caption": "Figure 6:Reward v.s. sampled images.Both VARD w/o KL and VARD attain higher reward values than baselines with substantially fewer training samples.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x7.png",
                "caption": "Figure 7:A example of reward hacking.Directly backpropagation without KL catastrophically loses ability to generate prompt-aligned images, degenerating into texture-like noise patterns. All of the images is generated from checkpoints optimized with 500 steps.",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x8.png",
                "caption": "Figure 8:Evaluated by HPSv2.VARD consistently attains the highest HPSv2 scores while simultaneously preserving a modest reward.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x9.png",
                "caption": "Figure 9:Prior maintenance.The image generated by VARD remains the closest to the base model as the reward increases.",
                "position": 755
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitation",
        "images": []
    },
    {
        "header": "8Social impact",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15791/x10.png",
                "caption": "Figure 10:Training trend of FID (left) and reward (right) under KL regularization.Color intensity correlates withŒ∑ùúÇ\\etaitalic_Œ∑magnitude (darker corresponds to higher values). HigherŒ∑ùúÇ\\etaitalic_Œ∑configurations maintain lower FID scores (better prior preservation) while exhibiting slower reward growth, quantitatively demonstrating the preservation-optimization tradeoff.",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x11.png",
                "caption": "Figure 11:Uncurated protein structures generated by FoldFlow-2-VARD.The showcased samples include proteins with various secondary structures, such as those containing beta-sheets. This illustrates that FoldFlow-2-VARD maintains a broad generative capacity, retaining its ability to produce entirely alpha-helical proteins alongside more complex structures.",
                "position": 2224
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x12.png",
                "caption": "Figure 12:Uncurated samples of fine-tuning on compressibility and incompressibility as rewards.",
                "position": 2227
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x13.png",
                "caption": "Figure 13:Uncurated samples of fine-tuning on Aesthetic Score as the reward.",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x14.png",
                "caption": "Figure 14:Further comparison of the base model and VARD fine-tuned version with PickScore as reward.",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x15.png",
                "caption": "Figure 15:Further comparison of the base model and VARD fine-tuned version with ImageReward as reward.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2505.15791/x16.png",
                "caption": "Figure 16:Further comparison of the base model and VARD fine-tuned version with HPSv2 as reward.",
                "position": 2239
            }
        ]
    },
    {
        "header": "Appendix AAppendix / supplemental material",
        "images": []
    }
]