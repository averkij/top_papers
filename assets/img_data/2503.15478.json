[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15478/x1.png",
                "caption": "Figure 1:(Left)Overview of our ColBench including Backend Programming and Frontend Designthat supports cheap and reliable evaluation of multi-turn RL algorithms for agents in realistic settings.(Right)The high-level motivation behind SWEET-RLthat uses additional training-time information along with appropriate Bradley-Terry (BT) objective to perform effective credit assignment.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Collaborative Agent Benchmark (ColBench)",
        "images": []
    },
    {
        "header": "4SWEET-RL: RL withStep-WisEEvaluation fromTraining-Time Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15478/x2.png",
                "caption": "Figure 2:textbfAn overview of the training procedure of SWEET-RL. At a high level, we first apply Bradley-Terry objective to directly train a step-wise advantage function with access to additional training-time information. Once the advantage function is trained, we perform policy improvement by using the advantage function as a reward model for each turn.",
                "position": 280
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15478/x3.png",
                "caption": "Figure 3:(a)Scaling curve of Best-of-N sampling with respect to different step reward model on Backend Programming.Results show that SWEET can best tell good actions on a turn-wise basis, resulting in the best scaling with respect to Best-of-N sampling. Note that this curve is different from test-time scaling curve because SWEET exploits additional training-time information.(b)Performance scaling of different multi-turn RL algorithms with respect to the amount of fine-tuning data on Backend Programming.While SWEET-RL takes more data initially to learn a reliable critic, it quickly catches up and achieves a better converging performance.",
                "position": 531
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyperparameters",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Justifications",
        "images": []
    },
    {
        "header": "Appendix CPrompts",
        "images": []
    },
    {
        "header": "Appendix DQualitative Comparisons of Different Credit Assignment Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15478/x4.png",
                "caption": "Figure 6:Qualitative comparisons between different credit assignment methods.A fixed LLM-as-a-Judge can be easily distracted by length and formats of the actions without considering their actual utility. A value function generalizes poorly to unseen tasks. In contrast, SWEET can attend to the actual utility of the action for task success and generalize well.",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x5.png",
                "caption": "Figure 7:Example full trajectory for Backend Programming with SWEET-RL Llama-3.1-8B-Instruct.After training, the LLM agent has learnt back-and-forth information seeking behaviors before giving the final answer.",
                "position": 1980
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x6.png",
                "caption": "Figure 8:Example full trajectory for Backend Programming with SWEET-RL Llama-3.1-8B-Instruct (Continued).After training, the LLM agent has learnt back-and-forth information seeking behaviors before giving the final answer.",
                "position": 1983
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x7.png",
                "caption": "Figure 9:Example full trajectory for Backend Programming with Zeroshot Llama-3.1-8B-Instruct.While the agent has asked a few questions, it quickly jumps into conclusions, resulting in a wrong final answer.",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x8.png",
                "caption": "Figure 10:Example full trajectory for Backend Programming with Zeroshot GPT-4O.While the agent does propose critical questions to the human collaborator, it also has the issue of jumping into conclusions.",
                "position": 1989
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x9.png",
                "caption": "Figure 11:Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (1).After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward.",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x10.png",
                "caption": "Figure 12:Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (2).After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward.",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x11.png",
                "caption": "Figure 13:Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (3).After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x12.png",
                "caption": "Figure 14:Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (4).After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x13.png",
                "caption": "Figure 15:Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (5).After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward.",
                "position": 2004
            },
            {
                "img": "https://arxiv.org/html/2503.15478/x14.png",
                "caption": "Figure 16:Example full trajectory for Frontend Design with SWEET-RL Llama-3.1-8B-Instruct (6).After training, the LLM agent has learnt sophisticated task-specific strategies to optimize the final reward.",
                "position": 2007
            }
        ]
    },
    {
        "header": "Appendix EFull Qualitative Examples",
        "images": []
    }
]