[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10894/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10894/x2.png",
                "caption": "Figure 2:Model architecture of DuetSVG.As a unified model, DuetSVG accepts multimodal inputs, including text prompts, SVG code and raster images.\nWe use Janus-Pro text tokenizer[chen2025janus]for text prompts.\nFor images, an Understanding (Und.) Encoder extracts semantic features, while a Generation (Gen.) Encoder converts images into discrete visual tokens.\nTwo MLP aligners project the encoder outputs into the same feature space as the text embeddings.\nA Generation (Gen.) head predicts image tokens, and a language modeling (LM) head predicts SVG tokens.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x3.png",
                "caption": "Figure 3:Test-time scaling with image-guided SVG resampling.(a) We first generateNNraster image candidates with CFG. Since image-token sequences are much shorter than SVG-token sequences, this step is efficient. A CLIP-based verifier[radford2021learning]selects the best candidateI∗I^{*}.\n(b) Using the selected image tokens as internal guidance, we iteratively generate SVG tokens. At each iteration, we render a raster imageRtR_{t}from the current SVG codes and accept the update only if the LPIPS distanced​(Rt,I∗)d(R_{t},I^{*})does not increase; otherwise, we reject and resample.",
                "position": 249
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10894/x4.png",
                "caption": "Figure 4:Qualitative comparison on text-to-SVG generation task.Our DuetSVG aligns better with the text prompts and generates high-quality visual outputs with detailed structures.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x5.png",
                "caption": "Figure 5:Qualitative comparison on image-to-SVG conversion task.Ours preserves more visual details than other previous baselines.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x6.png",
                "caption": "Figure 6:Ablation results.(a) Withoutvisual output, SVG-only generation struggles to produce semantically accurate and structurally coherent SVGs.\n(b) WithoutT2I pretraining, the model lacks strong visual and semantic priors, resulting in simpler and less visually appealing SVGs.\n(c) Withouttest-time scaling (TTS), autoregressive decoding may introduce local geometric distortions when handling complex inputs.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x7.png",
                "caption": "Figure 7:Sampling efficiency comparison of test-time scaling strategies.Our strategy leverages image-level candidate selection and SVG resampling to achieve efficient test-time scaling, with substantially lower sampling cost than best-of-NN.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x8.png",
                "caption": "Figure 8:Downstream applications.After fine-tuning, DuetSVG supports:\n(a)SVG completion: given the partial SVG and its rendered image, DuetSVG completes coherent and visually appealing SVGs.\n(b)SVG editing: DuetSVG enables instruction-based editing for SVGs.",
                "position": 965
            }
        ]
    },
    {
        "header": "5Applications",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BAdditional Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10894/x9.png",
                "caption": "Figure 9:Generated SVGs and their nearest neighbor samples.",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x10.png",
                "caption": "Figure 10:Additional comparisons with Flux + I2SVG pipeline.",
                "position": 1217
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x11.png",
                "caption": "Figure 11:Failure cases.",
                "position": 1220
            }
        ]
    },
    {
        "header": "Appendix CLimitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10894/x12.png",
                "caption": "Figure 12:Short prompt template.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x13.png",
                "caption": "Figure 13:Medium description template.",
                "position": 1243
            },
            {
                "img": "https://arxiv.org/html/2512.10894/x14.png",
                "caption": "Figure 14:Detailed annotation template.",
                "position": 1246
            }
        ]
    },
    {
        "header": "Appendix DDetails of the captioning pipeline",
        "images": []
    }
]