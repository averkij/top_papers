[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07774/x1.png",
                "caption": "",
                "position": 68
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07774/x2.png",
                "caption": "Figure 2:Overall pipeline of UniReal.We formulate image generation and editing tasks as discontinuous frame generation.\nFirst, input images are encoded into latent space by VAE encoder.\nThen, we patchify the image latent and noise latent into visual tokens.\nAfterward, we add index embeddings andimage prompt (asset/canvas/control)to the visual tokens.\nAt the same time, thecontext promptandbase promptare processed by the T5 encoder.\nWe concatenate all the latent patches and text embeddings as a long 1D tensor and send them to the transformer.\nFinally, we decode the denoised results to get the desired output images.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07774/x3.png",
                "caption": "Figure 3:Data construction pipeline.Starting from raw videos, we use off-the-shelf models to construct data for different kinds of tasks.\nTwo examples of instructive editing and image customization data (we segment objects from one frame to generate another frame) are given at the bottom of the image.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2412.07774/x4.png",
                "caption": "Figure 4:Comparison results for instructive image editing.We compare with the state-of-the-art methods OmniGen[60], UltraEdit[72], MGIE[13], InstructPix2Pix[1], and CosXL[52].\nOur UniReal shows significant advantages in the aspects of instruction-following and generation quality.\nWe generate multiple results for each model and pick the best ones for demonstration.",
                "position": 340
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07774/x5.png",
                "caption": "Figure 5:Qualitative comparison for image customization.For single subject, we compare with OmniGen[60], Emu2[53], BLIP-Diffusion[27], ELITE[58], and IP-Adapter[68]with Flux[62]backbone.\nFor multiple subjects, we chose OmniGen and Emu2 as competitors.\nThe listed prompts are in the formats of UniReal, and they are formulated according to the requirements of each method.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2412.07774/x6.png",
                "caption": "Figure 6:Our preference rates against other methodsevaluated by user studies.\nWe compare SuTI[8], OmniGen[60], UltraEdit[72]and AnyDoor[10]for different tasks.",
                "position": 671
            },
            {
                "img": "https://arxiv.org/html/2412.07774/x7.png",
                "caption": "Figure 7:Comparison results for object insertion.Our method could automatically adjust the status of the reference object according to the environment\nand strictly preserve the background.\nOur method does not require any mask as input.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2412.07774/x8.png",
                "caption": "Figure 8:Effects of hierarchical prompt.The same input could correspond to various types of targets when given different image prompts (row 1) and context prompts (row 2).",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2412.07774/x9.png",
                "caption": "Figure 9:Ablation study for the training data.We visualize the results for models that are trained onVideo Frame2Framedataset, task-specific expert dataset, and our multi-task full dataset.\nIt is impressive that the model trained only on video data could master many editing tasks (e.g., add, remove, attribute/pose changing), even for tasks with multiple input images.",
                "position": 703
            },
            {
                "img": "https://arxiv.org/html/2412.07774/x10.png",
                "caption": "Figure 10:More applications supported by UniReal.Our method supports a large range of applications.\nThe left block demonstrates more tasks for which we constructed corresponding training data.\nThe right block shows some zero-shot novel applications achieved by task composition and generalization.",
                "position": 709
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]