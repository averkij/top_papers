[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05920/x1.png",
                "caption": "Figure 1:Examples of models trained with multi-turn grounding-based RL on high-resolution real-world tasks. The model first identifies key regions, which are then automatically cropped and returned as sub-images. Notably, despite only a binary reward function derived from the correctness of the final answer, the model gradually emerge robust grounding capability throughout the RL process. The conversation in the figure only shows key parts, the full conversation is provided in Appendix9.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05920/x2.png",
                "caption": "Figure 2:Comparison of different post-training paradigms for LMMs. Our MGPO automatically crops and returns sub-image to the model based on its predicted grounding coordinates, enabling the model to iteratively focus on key regions and effectively solve high-resolution visual tasks.",
                "position": 175
            }
        ]
    },
    {
        "header": "3Multi-turn Grounding-Based RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05920/x3.png",
                "caption": "Figure 3:Fixed multi-turn grounding template, which eliminate cold start SFT process.",
                "position": 287
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05920/x4.png",
                "caption": "Figure 4:Distribution of image resolutions (width×\\times×height) across different datasets.",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2507.05920/x5.png",
                "caption": "",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2507.05920/x6.png",
                "caption": "",
                "position": 348
            }
        ]
    },
    {
        "header": "5Limitation",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05920/x7.png",
                "caption": "Table 4:Performance comparison of image count task. Additional point reward do not lead to significant performance improvements.",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2507.05920/x7.png",
                "caption": "Figure 7:Visualization of point predictions from the GRPO model trained with only accuracy reward.",
                "position": 1367
            },
            {
                "img": "https://arxiv.org/html/2507.05920/x8.png",
                "caption": "Figure 8:A illustration of cropping sub-image based on grounding coordinates.",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2507.05920/x9.png",
                "caption": "Figure 9:A full conversation example of MGPO post-trained model on high-resolution image tasks.",
                "position": 1377
            }
        ]
    },
    {
        "header": "Appendix BFurther Experiments on Image Counting Tasks",
        "images": []
    }
]