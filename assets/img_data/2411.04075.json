[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/extracted/5978284/logos/yale.jpeg",
                "caption": "",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2411.04075/extracted/5978284/logos/ai2.jpg",
                "caption": "",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x1.png",
                "caption": "Figure 1:(Top) The common workflow of comparative analysis in scientific research, particularly when a result, such as a figure/table in theInformation Value paper (anchor paper)(Giulianelli et al.,2023), prompts further examination of related research, such as details fromDialoGPT (reference paper)(Zhang et al.,2020b).(Bottom)A demonstration of the workflow for constructing avisual context question,reference-based question, and combined question.",
                "position": 199
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/x2.png",
                "caption": "Figure 2:An overview ofM3SciQAquestion construction pipeline.",
                "position": 254
            }
        ]
    },
    {
        "header": "2TheM3SciQABenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/x3.png",
                "caption": "Figure 3:Distribution of reasoning types ofvisual contextandreference-basedquestions inM3SciQA.",
                "position": 297
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/x4.png",
                "caption": "Figure 4:Three examples from GPT-4o in answering visual context questions.",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x5.png",
                "caption": "Figure 5:Performance scores of Mistral, Llama 3 70B, GPT-3.5, and GPT-4 in differentretrievalsettings.",
                "position": 1158
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Collection Guidelines",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/x6.png",
                "caption": "Figure 6:Examples of four visual context reasoning categories inM3SciQA.",
                "position": 2102
            }
        ]
    },
    {
        "header": "Appendix BExpert Annotation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/x7.png",
                "caption": "(a)The distribution of the number of tokens per visual context question inM3SciQA- Part 1 of 3.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x7.png",
                "caption": "(a)The distribution of the number of tokens per visual context question inM3SciQA- Part 1 of 3.",
                "position": 2236
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x8.png",
                "caption": "(b)The distribution of the number of tokens per reference-based question inM3SciQA- Part 2 of 3.",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x9.png",
                "caption": "",
                "position": 2243
            }
        ]
    },
    {
        "header": "Appendix DMore Result Analysis",
        "images": []
    },
    {
        "header": "Appendix EMore Details On the Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04075/x10.png",
                "caption": "Figure 8:Qwen example output on visual context question - Part 1 of 3.",
                "position": 3328
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x11.png",
                "caption": "Figure 9:Qwen example output on visual context question - Part 2 of 3.",
                "position": 3333
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x12.png",
                "caption": "Figure 10:Qwen example output on visual context question - Part 3 of 3.",
                "position": 3338
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x13.png",
                "caption": "Figure 11:GPT-4V(ision) example output 1 on visual context question.",
                "position": 3343
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x14.png",
                "caption": "Figure 12:GPT-4V(ision) example output 2 on visual context question.",
                "position": 3348
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x15.png",
                "caption": "Figure 13:Claude-3-Opus example output 1 on visual context question.",
                "position": 3353
            },
            {
                "img": "https://arxiv.org/html/2411.04075/x16.png",
                "caption": "Figure 14:Claude-3-Opus example output 2 on visual context question.",
                "position": 3358
            }
        ]
    },
    {
        "header": "Appendix FA Comparative Study of LMMs in Answering Visual Context Questions",
        "images": []
    }
]