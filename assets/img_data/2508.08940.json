[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08940/x1.png",
                "caption": "Figure 1:Curriculum Learning GRPO Overview.Our proposed setting performs GRPO with a length reward being applied to the generated thinking trace. The budget is decayed exponentially with a user specific decay factor and decay interval. In this example the decay factorγ\\gammais set to0.70.7and the decay intervalTTis set to100100. An initial budget of256256tokens is given at start and decayed later down to3030. The figure demonstrates that the model learns to answer the same question with a way smaller token budget reaching the same solution.",
                "position": 199
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08940/x2.png",
                "caption": "Figure 2:Curriculum vs. fixed-budget training on GSM8K and MATH500.For GSM8K (top), models trained with our curriculum (256→\\rightarrow87 tokens) achieve higher in-distribution accuracy than fixed-budget GRPO at the same final budget, while using fewer tokens.\nFor MATH500 (bottom), even for harder, longer-form problems, curriculum learning improves accuracy while reducing average reasoning length, showing that progressive budget tightening can compress solutions while maintaining high accuracy.",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2508.08940/x3.png",
                "caption": "",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2508.08940/x4.png",
                "caption": "Figure 3:Length-heavy reward weighting.Increasing the weight on the length reward (λc=0.3,λℓ=0.6\\lambda_{c}=0.3,\\ \\lambda_{\\ell}=0.6) yields highly compressed reasoning traces while retaining accuracy gains over the base model.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2508.08940/x5.png",
                "caption": "Figure 4:Correctness-heavy reward weighting.Prioritizing correctness (λc=0.6,λℓ=0.3\\lambda_{c}=0.6,\\ \\lambda_{\\ell}=0.3) produces slightly longer outputs than the length-heavy setting but improves accuracy on both in-distribution and out-of-distribution benchmarks.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2508.08940/x6.png",
                "caption": "Figure 5:Triangular vs. Band length reward.The triangular shape encourages exploration up to the budgetLLbefore compression, whereas the band shape gives maximum reward immediately for any output≤L\\leq L, often leading to shorter but less accurate reasoning traces. We refer to ‘band’ here as ‘flat-then-decay’.",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2508.08940/x7.png",
                "caption": "Figure 6:Exponential vs. Linear decay schedules.Linear decay reduces the budget in equal steps, leading to slightly longer outputs but improved performance on harder reasoning tasks.",
                "position": 813
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]