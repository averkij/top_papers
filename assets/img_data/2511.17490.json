[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17490/fig/teaser.png",
                "caption": "Figure 1:Video-R4 performs iterative visual rumination by selecting frames, zooming into regions, and re-encoding pixels, forming a closed-loopread–retrieve–refocus–reinforcecycle for grounded video reasoning.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2511.17490/fig/bar.png",
                "caption": "Figure 2:Our Video-R4-7B model achieves state-of-the-art performance on the text-rich video understanding dataset M4-ViteVQA, and is also compatible with the LMMs with the same size on the general video QA benchmarks.",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17490/fig/data_pipeline.png",
                "caption": "Figure 3:Data curation pipeline for creating the Video-R4-CoT-17k for supervised deliberate rumination practice fine-tuning (DRP-SFT) and compositional rumination practice fine-tuning (CRP-SFT), as well as the Video-R4-RL-30k dataset for reinforcement learning. Thelight blue partsare intended to be used as the model’s inputs, while thepink partsare expected to be produced by the model as outputs.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Method: Video-R4",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17490/fig/training_framework.png",
                "caption": "Figure 4:Overview of multi-stage rumination training framework.",
                "position": 290
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17490/fig/cot_stat.png",
                "caption": "Figure 5:Overall statistics of the Video-R4-CoT-17k dataset, including the ratio of video versus image samples, word cloud of frequently appearing terms, question length distribution, distribution of visual operation counts per sample, and conversation turn count distribution.",
                "position": 1175
            },
            {
                "img": "https://arxiv.org/html/2511.17490/fig/rl_stat.png",
                "caption": "Figure 6:Overall statistics of the Video-R4-RL-30k dataset.",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2511.17490/fig/app_line_chart.png",
                "caption": "Figure 7:Comparison of training behaviors across fine-tuning strategies. Subfigures (a) and (b) show that models pre-finetuned on DRP-SFT data converge more quickly and achieve lower final loss when training on CRP-SFT, indicating that decomposing visual operations before interleaved training is beneficial. (c) Video-R4-7B progressively increases its response length during RL, suggesting emergent allocation of more thinking time. (d) Correspondingly, the average reward improves and remains stable across iterations.",
                "position": 1280
            }
        ]
    },
    {
        "header": "8Evaluation Metrics",
        "images": []
    },
    {
        "header": "9More Evidence to Support our Findings",
        "images": []
    },
    {
        "header": "10Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17490/fig/anno_tool.png",
                "caption": "Figure 8:Interface of the quality control tool used to review QA queries and synthesized chain-of-thought trajectories. The tool enables rapid browsing, frame inspection, saving or dropping samples, and in-place editing of both textual and visual reasoning steps to streamline annotation and correction workflows.",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2511.17490/fig/vis_1.png",
                "caption": "Figure 9:Trajectories visualization.",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2511.17490/fig/vis_2.png",
                "caption": "Figure 10:More visualization with longer trajectories.",
                "position": 1649
            }
        ]
    },
    {
        "header": "11More Visualization Results",
        "images": []
    }
]