[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07976/figures/main-rl-results.png",
                "caption": "Figure 1:(Left) Asynchronous RL brings substantial improvements:Through RL training, our agent, ASearcher-Web-QwQ, obtains 20.8%, 46.7%, and 20.4% improvements on GAIA, xBench, and Frames, respectively.(Middle) & (Right) Through RL training, ASearcher-Web-QwQ learns to conduct long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training. The agent alsolearns expert-level search strategies(See case study in Sec.2)",
                "position": 157
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07976/figures/agent_comparison.png",
                "caption": "Figure 2:Comparison between ASearcher and Search-R1. (Left) Search-R1 is only equipped with search tools and lacks web browsing capability. (Right) ASearcher utilizes a simple agent design with two basic tools including search and browsing tools, without relying on any external LLM. ASearcher is a comprehensive agent capable of both reasoning and summarizing lengthy web contents. Notably, both reasoning and summarization abilities are optimized through end-to-end RL training.",
                "position": 196
            }
        ]
    },
    {
        "header": "2Limitations of Existing Open-source Approaches",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07976/figures/gaia-case-study-clean.png",
                "caption": "Figure 3:A case study on a complex query from GAIA.Search-R1-32Bis unable to break down the complex question and has severe hallucinations.Search-o1 (QwQ)can identify the corrects articles through extensive tool calls, but easily misses key information and fails to verify wrong conclusions. Our end-to-end RL agent,ASearcher-Web-QwQ, exhibits key behaviors featuring Search Intelligence:uncertainty-aware reasoning(list and examine candidate answers),precise extractionfrom noisy contents,cross-document inference, andgrounded verification.",
                "position": 234
            }
        ]
    },
    {
        "header": "3ASearcher",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07976/figures/data_synthesis_agent.png",
                "caption": "Figure 4:Data Synthesis Agent. Starting from a seed QA, the data synthesis agent iteratively modifies the question through two actions,InjectionandFuzz. Throughinjection, the agent enriches the question by adding some external facts. ThroughFuzz, the agent blurs certain information to increase uncertainty and difficulty. The related fact to the question are tracked during the synthesis process. Each time the question is modified, a quality verification step is applied to ensure quality and difficulty of the synthetic questions.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/data_stats.png",
                "caption": "Figure 5:Statistics from our data synthesis process. (Left) The distribution of the number of supporting facts. (Middle) The distribution of the number of fuzz actions and injection actions. (Right) The accuracy distribution of QwQ-32B in answering the generated questions without using any tools.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/acc_wrt_turn.png",
                "caption": "Figure 6:(Left) Test scaling of ASearcher-Web-QwQ.\nData points are obtained by enforcing different minimum turns.The accuracy is averaged over GAIA, xBench-DeepSearch, and Frames. (Middle) Number of tool calls versus training steps. During training time, long trajectories require much more tool calls than short ones. (Right) Number of generated tokens versus training steps. The number of output tokens exhibits significant variance, with long trajectories exceeding short ones by up to two orders of magnitude.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/qwq_turns_wrt_step.png",
                "caption": "",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/qwq_tokens_wrt_step_log.png",
                "caption": "",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/one-step_vs_fully-async.png",
                "caption": "Figure 7:One-Step-off RL v.s. Fully Asynchronous RL. In batch generation systems, a batch should wait for the longest trajectory, leading to significant GPU idle time. In contrast, fully asynchronous RL achieves faster training than batch generation RL by fully decoupling training and trajectory generation, achieving near-full resource utilization for trajectory generation.",
                "position": 726
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07976/figures/before_after_rl.png",
                "caption": "Figure 8:Comparison of the performance of QwQ-32B agent before and after RL Training.",
                "position": 1472
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/7b_tokens_wrt_step.png",
                "caption": "(a)Generated Tokens",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/7b_tokens_wrt_step.png",
                "caption": "(a)Generated Tokens",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/7b_url_queries_wrt_step.png",
                "caption": "(b)Search Queries",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/7b_url_accesses_wrt_step.png",
                "caption": "(c)URL Accesses",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/14b_tokens_wrt_step.png",
                "caption": "(a)Generated Tokens",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/14b_tokens_wrt_step.png",
                "caption": "(a)Generated Tokens",
                "position": 1503
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/14b_url_queries_wrt_step.png",
                "caption": "(b)Search Queries",
                "position": 1508
            },
            {
                "img": "https://arxiv.org/html/2508.07976/figures/14b_url_accesses_wrt_step.png",
                "caption": "(c)URL Accesses",
                "position": 1513
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07976/figures/gaia-case-study.png",
                "caption": "Figure 11:A case study on a complex query from GAIA.Search-R1-32Bis unable to break down the complex question and has severe hallucinations.Search-o1 (QwQ)can identify the corrects articles through extensive tool calls, but easily misses key information and fails to verify wrong conclusions. Our end-to-end RL agent,ASearcher-Web-QwQ, exhibits key behaviors featuring Search Intelligence:uncertainty-aware reasoning(list and examine candidate answers),precise extractionfrom noisy contents,cross-document inference, andrigorous confirmation.",
                "position": 2231
            }
        ]
    },
    {
        "header": "Appendix AFull Case Study",
        "images": []
    }
]