[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11543/x1.png",
                "caption": "Figure 1:Comparison of different pretraining paradigms for LLM.Left: centralized training, which requires high-memory GPUs and high-bandwidth interconnects (e.g., RDMA) for its tightly coupled model or data parallelism.Middle: existing decentralized training (e.g., DiLiCo, Photon), where each node trains a full model locally, reducing bandwidth needs but still demanding high-memory GPUs.Right: our proposed SPES, a memory-efficient decentralized method for training MoE-based LLMs, where each node trains only a subset of experts, substantially reducing both per-GPU memory usage and communication overhead.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Memory-Efficient Decentralized Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11543/x2.png",
                "caption": "Figure 2:(a)Illustration of our model structure, in which we utilize an MoE LLM comprising standard self-attention blocks, normalization layers, and routed feed-forward modules. (b)Illustration of SPES, where each node performs local training on a disjoint subset of experts to reduce memory consumption. During weight synchronization, only the trained parameters are transmitted to the parameter server, minimizing communication overhead. To improve data utilization, we propose an expert-merging strategy that merges similar experts to facilitate knowledge sharing.",
                "position": 162
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11543/x3.png",
                "caption": "Figure 3:Memory and communication costs across training paradigms.Experiments are conducted with a batch size of 2 and a sequence length of 2048. For the 2B model, we employ PyTorch DDP. For the 7B model, we utilize FSDP across 8 GPUs.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2602.11543/x4.png",
                "caption": "Figure 4:Performance comparison across different training paradigms.Performance during training is evaluated using the evaluation suite integrated into the open-source OLMo codebase.",
                "position": 384
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis of SPES",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CDetails of Datasets and Sampling Ratio",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11543/x5.png",
                "caption": "Figure A1:Ablation on key hyper-parameters in expert merging.The reported average is computed over ARC(e), SciQ, PIQA, WinoGrande, ARC(c), OBQA, OpenBookQA, and SIQA.",
                "position": 3723
            },
            {
                "img": "https://arxiv.org/html/2602.11543/x6.png",
                "caption": "Figure A2:Ablation on synchronization steps.The reported average is computed over eight benchmarks in total, additionally including ARC(c), OBQA, OpenBookQA, and SIQA.",
                "position": 3726
            }
        ]
    },
    {
        "header": "Appendix FDeclaration of LLM Assistance",
        "images": []
    }
]