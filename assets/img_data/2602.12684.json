[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12684/x1.png",
                "caption": "Figure 1:Overview.Xiaomi-Robotics-0achieves state-of-the-art performance in three widely-used simulation benchmarks.\nIt also attains high throughput on two challenging real-robot bimanual manipulation tasks.\nFurthermore, it matches the underlying pre-trained VLM on several VLM benchmarks.",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Xiaomi-Robotics-0",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12684/x2.png",
                "caption": "Figure 2:Data.Xiaomi-Robotics-0leverages both robot trajectory data and vision-language (VL) data during pre-training.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2602.12684/x3.png",
                "caption": "Figure 3:Model & Training.(a) During the first step of pre-training, we train the VLM on both vision-language data (left) and robot trajectory data (right).\nVision-language data are trained via a next-token-prediction objective.\nWe adopt the training paradigm in Choice Policies[51]to train the VLM for action prediction on the robot trajectory data.\n(b) In the second step of pre-training, we freeze the VLM and train the diffusion transformer for generating actions via flow-matching.\n(c) During post-training for asynchrnnous execution, we prepend clean action prefix to the noisy action tokens.",
                "position": 164
            },
            {
                "img": "https://arxiv.org/html/2602.12684/x4.png",
                "caption": "Figure 4:TheΛ\\Lambda-Shape Attention Mask for Post-Training.\nA noisy action token can only attend to the vision and language tokens via the VLM KV cache, the sink token, the state token, and the action tokens of the previouswwtimesteps.\nThe number in each token indicates the RoPE positional index of the token.\nNote that we add an offset of 10 to the positional indices of the noisy action tokens to allow the model to distinguish them from the clean action prefix tokens.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2602.12684/x5.png",
                "caption": "Figure 5:Asynchronous Execution.We show two consecutive chunks and how they are stitched together during robot rollout.\nSee Sec.2.3for more details.",
                "position": 269
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12684/x6.png",
                "caption": "Figure 6:Real-Robot Experiments.(a) We show the setting for Lego Disassembly evaluation. (b) We show the setting for Towel Folding evaluation and all the towels used during evaluation. (c) Quantitative results of different methods on the two tasks.",
                "position": 879
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ACase Study for Preservation of Vision-Language Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12684/x7.png",
                "caption": "Figure 7:Qualitative results ofXiaomi-Robotics-0on vision-language tasks (I).",
                "position": 2184
            },
            {
                "img": "https://arxiv.org/html/2602.12684/x8.png",
                "caption": "Figure 8:Qualitative results ofXiaomi-Robotics-0on vision-language tasks (II). “…” indicates omitted content for space constraints.",
                "position": 2188
            },
            {
                "img": "https://arxiv.org/html/2602.12684/x9.png",
                "caption": "Figure 9:Qualitative comparison ofXiaomi-Robotics-0against baseline methods.\nIn particular, the bottom row illustrates challenging failure cases, highlighting limitations in complex numerical reasoning on dense charts and minor format-following errors (e.g., outputting words instead of digits in counting tasks).",
                "position": 2192
            }
        ]
    },
    {
        "header": "Appendix BDetailed results on SimplerEnv",
        "images": []
    },
    {
        "header": "Appendix CVLM Benchmark Details",
        "images": []
    }
]