[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x1.png",
                "caption": "Figure 1:Video prediction in representation space (V-JEPA) achieves an understanding of intuitive physics.(A)Video models are evaluated on three intuitive physics datasets using the Violation of Expectation paradigm (IntPhys, GRASP, and InfLevel). V-JEPA is significantly more ‚Äòsurprised‚Äô by implausible videos. Random initializations of V-JEPA (untrained networks) show near-chance performance, and state-of-the-art video models based on text or pixel prediction are much closer to chance. Confidence intervals at 95% are obtained via bootstrapping, except for untrained networks (n=20ùëõ20n=20italic_n = 20) which use a normal distribution assumption.(B)V-JEPA is trained to ‚Äôinpaint‚Äô natural videos in a learned representation space. Starting from a video and a corrupted version, representations are first extracted. The goal is then to predict the representation of the original video from the representation of the corrupted ones.(C)From a trained V-JEPA, we compute a surprise metric by predicting representations of N future frames based on M past ones and comparing the predictions to the representations of observed events. The surprise metric is then used to decide which of the two videos contains a physical violation.",
                "position": 100
            }
        ]
    },
    {
        "header": "Measuring intutive physics understanding",
        "images": []
    },
    {
        "header": "Representation prediction learns to detect violations of intuitive physics",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x2.png",
                "caption": "Figure 2:V-JEPA accuracy increase relative to randomly-initialized models and humans across different physical properties and benchmarks.(A)Because some benchmarks contain low-level biases, we test the model performance against a set of randomly initialized networks (n=20ùëõ20n=20italic_n = 20). V-JEPA models (n=5ùëõ5n=5italic_n = 5) have higher relative classification accuracy on intuitive physics benchmarks for most, but not all concepts.(B)V-JEPA relative (left) and absolute (right) accuracy on the IntPhys test set across different conditions compared to naive human performance, showing a high correlation between human and machine errors. The V-JEPA score uses the maximum surprise from each video, which generalizes better for single-video classification. Human data are taken from(Riochet et¬†al.,2022).",
                "position": 185
            }
        ]
    },
    {
        "header": "Per property analysis of V-JEPA",
        "images": []
    },
    {
        "header": "Keys to intuitive physics understanding",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x3.png",
                "caption": "Figure 3:Influence of type of mask, type and amount of training data, and model size on V-JEPA IntPhys scores.(A)When pretrained on VM2M, V-JEPA exhibits an understanding of intuitive physics with every masking strategy.(B)Of the three training datasets, two give high accuracies when trained separately (K710 and Howto100M). High scores are found with only 1289 hours of Howto100M (the largest dataset), and even 128h gives better than chance performance.(C)While larger encoders improve performance, we find that the performance remains non-trivial across sizes when pretraining on HowTo100M. Confidence intervals obtained via bootstrapping.",
                "position": 214
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Materials and Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x4.png",
                "caption": "Figure S1:Different surprise measures are better suited for different tasks.Focusing on IntPhys, we find that looking at the average surprise over a video leads to better performance when comparing pairs of videos. A one-sample t-test was performed to see if the relative surprises are greater than zero(left). However, when looking at individual videos‚Äô surprise, choosing the maximum surprise over a video leads to a better separation between possible and impossible videos. A two-sample t-test was performed to see if impossible videos have higher surprise than possible ones.(rigt).",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x5.png",
                "caption": "",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x6.png",
                "caption": "Figure S2:Normalized probabilities output by Qwen2-VL-72B.When presented with a pair of videos, we find that the model outputs similar probabilities for possible and impossible videos.",
                "position": 1781
            }
        ]
    },
    {
        "header": "2Choice of prediction hyperparameters and influence on 0-shot performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x7.png",
                "caption": "Figure S3:Models perform suboptimally with a fixed context size.Due to limitations in how long of a video models can process, we find drops in performance when using a single context size across all properties and datasets. Performance remains non-trivial for V-JEPA in this scenario.",
                "position": 1941
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x8.png",
                "caption": "Figure S4:Variation of performance when changing context size for predictions.While models tend to perform better with smaller context sizes, we find the optimal context size to be dependent on the property and dataset. GRASP exhibits the most variation whereas IntPhys and InfLevel-lab are less sensitive in general.",
                "position": 1944
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x9.png",
                "caption": "",
                "position": 1948
            }
        ]
    },
    {
        "header": "3Influence of semantic and motion diversity in videos",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x10.png",
                "caption": "Figure S5:Influence of motion and scene diversity.By pretraining V-JEPA-L on subsets of HowTo100M, we investigate how the diversity in motion and scenes affects performance on IntPhys.(left)By subsampling videos, we reduce the diversity in scenes, where we find that the model can still reach good performance with 128h of unique videos.(right)By subsampling frames in videos, we reduce the diversity of motions in each scene. Here we find lower performance than when subsampling videos, but the model still achieves good performance with 2% of the frames (2579h).",
                "position": 1988
            }
        ]
    },
    {
        "header": "4Results on the IntPhys challenge",
        "images": []
    },
    {
        "header": "5Importance of contextualization events on InfLevel",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11831/x11.png",
                "caption": "Figure S6:Relabeling InfLevel to remove contextualization events.Gravity and solidity both require to remember the properties about the containers shown in a video before the actual experiment. By relabeling the videos such that the prefix video is not necessary, we find a significant increase in performance for both V-JEPA and VideoMAE. However, this relabeling breaks the assumption that the possible and impossible videos have the same difficulty.",
                "position": 2221
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x12.png",
                "caption": "Figure S7:Complete results for V-JEPA-L.The models (n=5ùëõ5n=5italic_n = 5) achieve accuracies higher than untrained networks on most properties. Black dots represent the performance of 5 seeds.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x13.png",
                "caption": "Figure S8:Complete results for V-JEPA-H.The model achieves accuracies higher than untrained networks on most properties. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping.",
                "position": 2237
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x14.png",
                "caption": "Figure S9:Complete results for VideoMAEv2.The model achieves performance on par or slightly higher than untrained networks across properties, apart from solidity and collision. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping.",
                "position": 2240
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x15.png",
                "caption": "Figure S10:Complete results for Qwen2-VL-72B.The model achieves performance on par or slightly higher than untrained networks across properties, except for color constancy and support. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2502.11831/x16.png",
                "caption": "Figure S11:Complete results for Gemini 1.5 pro.The model achieves performance on par or slightly higher than untrained networks across properties. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping.",
                "position": 2246
            }
        ]
    },
    {
        "header": "6Per-property performance of methods",
        "images": []
    }
]