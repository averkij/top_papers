[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3EditReward-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23909/x1.png",
                "caption": "Figure 1:Illustration of the annotation process. Annotators are presented with five candidate output images and are asked to rank them according to three evaluation dimensions. The final ranking is determined through consensus among multiple annotators. For example,1​|234|​51|234|5indicates that the first image is preferred over images 2, 3 and 4, which are in turn preferred over image 5.",
                "position": 342
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Reward Model Performance on EditReward-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23909/x2.png",
                "caption": "Figure 2:Self-ensembling offers a superior efficiency-performance trade-off compared to simply scaling model parameters. The colored solid lines show the performance scaling of our 7B, 32B, and 72B models as the number of ensemble passes (K) increases. The gray dashed line connects the single-pass (K=1) performance of these models, serving as a baseline for scaling model size alone. The results clearly indicate that scaling the number of forward passes yields a significantly higher accuracy gain per unit of computational cost.",
                "position": 682
            }
        ]
    },
    {
        "header": "6Application of EditScore in Image Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23909/x3.png",
                "caption": "(a)",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2509.23909/x3.png",
                "caption": "(a)",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2509.23909/x4.png",
                "caption": "(b)",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2509.23909/x5.png",
                "caption": "(c)",
                "position": 785
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AStatement on the Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix BDerivation of the SDE Formulation for Flow Matching",
        "images": []
    },
    {
        "header": "Appendix CGRPO on SDE Flow Matching",
        "images": []
    },
    {
        "header": "Appendix DImages categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23909/x6.png",
                "caption": "Figure 4:Input images’ categories.",
                "position": 2034
            }
        ]
    },
    {
        "header": "Appendix EQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23909/x7.png",
                "caption": "Figure 5:Qualitative results on image editing task.",
                "position": 2046
            }
        ]
    },
    {
        "header": "Appendix FData Annotation User Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23909/figures/gradio1.png",
                "caption": "Figure 6:Screenshots of our custom-built annotation interface for EditRewardBench.(Top)The upper panel presents the expert rater with the visual stimuli: the original input image and a set of five candidate edited outputs from various models.(Bottom)The lower panel contains the interactive components. It displays the user instruction (“Task Description”) and provides three separate input fields for our core evaluation dimensions: Instruction Following, Consistency, and Overall Quality. This panel clearly shows the tiered ranking mechanism, where the pipe symbol (‘||’) is used to group images of similar quality.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2509.23909/figures/gradio2.png",
                "caption": "",
                "position": 2060
            }
        ]
    },
    {
        "header": "Appendix GDetails on the Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix HVLM Evaluation Prompts",
        "images": []
    }
]