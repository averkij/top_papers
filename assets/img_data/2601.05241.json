[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05241/x1.png",
                "caption": "Figure 1:Overview of our RoboVIP Workflow.(1) We extract observation videos from robotics manipulation data with corresponding action data to segment the robot arm and interacted objects for inpainting-based augmentation.\n(2) A large-scale pool of visual identity prompts is curated from robotics datasets and used as conditioning inputs for our multi-view video diffusion model to conduct diverse visual augmentation.\n(3) The augmented videos, paired with action information from original robotics manipulation data, are utilized for downstream VLA and visuomotor policy training.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05241/x2.png",
                "caption": "Figure 2:Segmentation Pipeline.Our segmentation pipeline comprises two parallel streams: one for robot-arm segmentation and one for interacted-object segmentation. We first use the gripper-action signal to identify accurate keyframe ranges, which is helpful to locate the interacted objects that are not visible in the first or last frame. We then leverage off-the-shelf models such as Cosmos-Reason1[4]and SAM2[36], together with several heuristic refinements, to obtain accurate masks in a fully plug-and-play manner.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x3.png",
                "caption": "Figure 3:Video Diffusion Model Architecture.Our video diffusion model is conditioned on the segmented multi-view video sequence, structured text prompt, and visual identity prompting to achieve consistent visual augmentation.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x4.png",
                "caption": "Figure 4:Visual Identity Curation and Processing Pipeline.Our visual identity is curated by panoptic segmentation from the large-scale robotics dataset[14,45,24], followed by several scoring criteria filters. In augmentation, we randomly select some from the pool and pack them into one image frame to serve as conditioning for our video diffusion model.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05241/x5.png",
                "caption": "Figure 5:Qualitative comparisons of different models on Droid[24].\nOur method produces temporally consistent and visually diverse results, outperforming RoboEngine[53], which is a single-image-based method, and Cosmos-Transfer2.5[3], which struggles to generalize beyond appearance-level edge conditioning.Zoom infor the best view.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x6.png",
                "caption": "Figure 6:Augmented BridgeV2 Data by our RoboVIP for VLA Training.\nOur visual identity prompting enriches tabletop contents and introduces additional distractors to create more challenging settings for the policy model. The visual identity is randomly selected from our proposed pools.Zoom infor the best view.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x7.png",
                "caption": "Figure 7:Policy success rate vs. conditioned history length (frames).\nBars show task-averaged success rates for RoboEngine (baseline) and our RoboVIP (Text prompt with ID) on Octo[42].\nThe error bars denote standard deviation across tasks.\nThe bar plot indicates that RoboVIP consistently outperforms RoboEngine, whose average success falls to zero at six history frames.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x8.png",
                "caption": "Figure 8:Real Robot Experiment on Diffusion Policy.Both policies were trained using identical parameter settings and measured over 10 trials.",
                "position": 717
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Overview",
        "images": []
    },
    {
        "header": "7More Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05241/x9.png",
                "caption": "Figure 9:π0\\pi_{0}rollouts in SimplerEnv.Visualization of rollouts for theπ0\\pi_{0}policy on the same four tasks\nand in the same order as Figure10.\nAs before, each row corresponds to a single task and shows frames sampled\nuniformly in time from left (initial state) to right (final state).\nComparing this figure with Figure10provides a qualitative\nview of the differences in behavior and convergence between the two policies.",
                "position": 1643
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x10.png",
                "caption": "Figure 10:Octo rollouts in SimplerEnv.Each row shows a rollout of the Octo policy on one task.\nFrom top to bottom:spoon_on_towel,carrot_on_plate,stack_cube, andput_eggplant_in_basket.\nWithin each row, frames are sampled uniformly in time from an episode\nand arranged from left (start of the rollout) to right (end of the rollout),\nillustrating how the policy gradually moves the object toward the goal region.",
                "position": 1652
            }
        ]
    },
    {
        "header": "8Additional Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05241/x11.png",
                "caption": "Figure 11:Real-World Robot Rollout Results.Baseline methods[12,53,3]fail during the grasping stage under the cluttered setup, whereas our approach achieves a successful grasp and final placement.\nMore samples are available on our website.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2601.05241/x12.png",
                "caption": "Figure 12:Real-World Robot Zero-Shot Long-Horizon Augmentation by our RoboVIP.We showcase long-horizon real-world video augmented by our proposed RoboVIP.\nSince real robot videos contain far more frames than what current video diffusion models can process directly, we split the video into 33-frame chunks and generate them sequentially with different visual identities selected.\nThe figure presents equally sampled frames across the full trajectory.\nMore samples are available on our website.",
                "position": 1847
            }
        ]
    },
    {
        "header": "9More Visualization",
        "images": []
    }
]