[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20512/x1.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x2.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2506.20512/extracted/6570652/assets/octothinker-logo.png",
                "caption": "",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x3.png",
                "caption": "Figure 1:Our strategic mid-training incentivizes Llama’s RL scaling, matching Qwen2.5 performance.",
                "position": 162
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20512/x4.png",
                "caption": "Figure 2:Training dynamics comparison (downstream performance and the average length of correct responses) between Llama-3.2-3B and Qwen2.5-3B. The dashed line indicates the few-shot evaluation performance and average length of correct responses of the corresponding base models.",
                "position": 204
            }
        ]
    },
    {
        "header": "3Digging Deeper: Exploring Key Factors through Controllable Mid-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20512/x5.png",
                "caption": "Figure 3:Potential factors in mid-training that could impact the post-training stage.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x6.png",
                "caption": "Table 1:Statistics and Types of different datasets used in our experiments.⋄We use theTULU3-sft-personna-instruction-followingsubset.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x6.png",
                "caption": "Figure 4:Comparison between our fasttext-recalled corpus (w/o LLM refinement) and MegaMath-Web, following its yearly dump comparison setup under a 5B-token pre-training budget. Recall thresholds shown accordingly.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x7.png",
                "caption": "Figure 5:The effect of different math web corpora during mid-training. We performed mid-training on each corpus with a 20B-token training budget.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x8.png",
                "caption": "Figure 6:Impact of incorporating CoT data with varying characteristics during mid-training (9:1 mixture ratio). The figure also illustrates performance and average lengths of correct responses for Llama-3.2-3B-Base and its mid-trained variants for reference (in dashed line with different colors).",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x9.png",
                "caption": "Figure 7:Impact of incorporating instruction-following data during mid-training with a mixture of web, short-CoT and instruction data in a ratio of 89: 10: 1 . The maximum response length is 4,096. The figure also illustrates performance and average lengths of correct responses for Llama-3.2-3B-Base and its mid-trained variants for reference (in dashed line with different colors).",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x10.png",
                "caption": "Figure 8:Impact of incorporating instruction-following data during mid-training with a mixture of web, long-CoT and instruction data in a ratio of 89: 10: 1. The maximum response length is 8,192. The figure also illustrates performance and average lengths of correct responses for Llama-3.2-3B-Base and its mid-trained variants for reference (in dashed line with different colors).",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x11.png",
                "caption": "Figure 9:Impact of different RL prompt templates. The figure also illustrates performance and average lengths of correct responses for Llama-3.2-3B-Base (in dashed line with different colors).",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x12.png",
                "caption": "Figure 10:Impact of the maximum length scheduler on the model response. The figure also illustrates performance and average lengths of correct responses for Llama-3.2-3B-Base in a dashed line.",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x13.png",
                "caption": "Figure 11:Impact of scaling up the mid-training budget. The figure also illustrates performance and average lengths of correct responses for Llama-3.2-3B-Base and its mid-trained variants for reference (in dashed lines with different colors).",
                "position": 398
            }
        ]
    },
    {
        "header": "4OctoThinker-Base: Branching Reasoning Foundations via 2-Stage Mid-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Table 2:Dataset composition and weights in the first-stage.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Table 2:Dataset composition and weights in the first-stage.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Table 4:Hyper-parameters for decay stage.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Table 6:Evaluation results of Llama-3.2-1B and OctoThinker-1B series.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Table 7:Evaluation results of Llama-3.2-3B and OctoThinker-3B series.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Table 8:Evaluation results of Llama-3.1-8B and OctoThinker-8B series.",
                "position": 758
            }
        ]
    },
    {
        "header": "5OctoThinker-Zero Families: RL Scaling with Diverse Thinking Behaviors",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20512/x14.png",
                "caption": "Figure 12:The RL training dynamics across different branches for OctoThinker-1B series",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x15.png",
                "caption": "Figure 13:The RL training dynamics across different branches for OctoThinker-3B series",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x16.png",
                "caption": "Figure 14:RL training dynamics among Llama-3.2-3B-Base, OctoThinker series and Qwen2.5-Base.",
                "position": 807
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.20512/x17.png",
                "caption": "Figure 17:RL dynamics under different QA datasets and mixing ratios during the decay stage.",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x18.png",
                "caption": "",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2506.20512/x19.png",
                "caption": "",
                "position": 1708
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]