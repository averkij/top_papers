[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x1.png",
                "caption": "Figure 1:The proposedComposeAnythingframework enables text-to-image generation for complex compositions involving surreal spatial relationships and high object counts. It enhances both visual quality and faithfulness to the input text compared to diffusion-based models (e.g., SD3[11], FLUX[3]) and 2D layout conditioned models (e.g., RPG[56]and CreatiLayout[58]).",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x2.png",
                "caption": "Figure 2:TheComposeAnythingframework, which enhances text-to-image diffusion modelse.g.SD3-M[11]with layouts and composite object priors for complex compositional generation.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x3.png",
                "caption": "Figure 3:Chain-of-thought LLM planning for generating 2.5D semantic layouts from text.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x4.png",
                "caption": "Figure 4:Overview of prior-guided diffusion. The spatial-controlled denoising is applied for each aligned text and region pair to strengthen spatial control, and we further re-inject the object priorztpopsubscriptsuperscriptùëßsubscriptùëúùëùsubscriptùë°ùëùz^{o_{p}}_{t_{p}}italic_z start_POSTSUPERSCRIPT italic_o start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPTinto predictedzt‚àí1subscriptùëßùë°1z_{t-1}italic_z start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPTto reinforce the prior.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x5.png",
                "caption": "Figure 5:State-of-the-art comparison against SD3-M[11], FLUX[3], RPG[56], and Creatilayout[58], on complex surreal prompts.",
                "position": 586
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x6.png",
                "caption": "Figure 6:Human evaluations against RPG and CreatiLayout.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x7.png",
                "caption": "Figure 7:LLM generated object prior and their corresponding final image generation.",
                "position": 672
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABroader Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x8.png",
                "caption": "Figure 8:Effect of Object Prior Reinforcement and Spatial-Controlled Denoising. Increasing either strength enhances appearance fidelity and spatial precision, but reduces generative flexibility.",
                "position": 1632
            }
        ]
    },
    {
        "header": "Appendix BImpact of Key Hyper-parameters",
        "images": []
    },
    {
        "header": "Appendix CDetailed Evaluation of Object Priors and Corresponding Final Images",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x9.png",
                "caption": "Figure 9:Human evaluation results on the correctness of prior and final image pairs on the three categories of T2I-Compbench dataset.",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x10.png",
                "caption": "Figure 10:Labeling interface for evaluating object prior and the final image.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x11.png",
                "caption": "Figure 11:Object prior and the corresponding generation for 2D-Spatial compositions from T2I-compbench",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x12.png",
                "caption": "Figure 12:Object prior and the corresponding generation for Complex compositions from T2I-Compbench.",
                "position": 1698
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x13.png",
                "caption": "Figure 13:Object prior and the corresponding generation for 3D-Spatial compositions from T2I-compbench",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x14.png",
                "caption": "Figure 14:Object prior and the corresponding generation for Numeracy compositions from T2I-compbench",
                "position": 1705
            }
        ]
    },
    {
        "header": "Appendix DAdditional results on DDIM based SDXL as a base model",
        "images": []
    },
    {
        "header": "Appendix ELabeling interface for human evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x15.png",
                "caption": "Figure 15:Labeling interface for human evaluations.",
                "position": 1774
            }
        ]
    },
    {
        "header": "Appendix FLLM Planning Instructions",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24086/x16.png",
                "caption": "Figure 16:Instructions for LLM planning (to be continued).",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x17.png",
                "caption": "Figure 17:Instructions for LLM planning (to be continued).",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x18.png",
                "caption": "Figure 18:Instructions for LLM planning (to be continued).",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x19.png",
                "caption": "Figure 19:Instructions for LLM planning (to be continued).",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x20.png",
                "caption": "Figure 20:Instructions for LLM planning.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x21.png",
                "caption": "Figure 21:LLM planning for object prior generation, with final generated image.",
                "position": 1805
            },
            {
                "img": "https://arxiv.org/html/2505.24086/x22.png",
                "caption": "Figure 22:LLM instructions for evaluating 3D-spatial relations.",
                "position": 1817
            }
        ]
    },
    {
        "header": "Appendix G3D metric evaluation with LLM",
        "images": []
    }
]