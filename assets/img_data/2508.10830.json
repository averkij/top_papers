[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10830/x1.png",
                "caption": "Figure 1:Speech Separation Model Performance on WSJ0-2mix over Time. The size of the points represents the number of parameters of the model.",
                "position": 172
            }
        ]
    },
    {
        "header": "IIProblem Formulation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10830/x2.png",
                "caption": "Figure 2:Overview of speech separation with known/unknown source counts.",
                "position": 316
            }
        ]
    },
    {
        "header": "IIILearning Paradigms",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10830/x3.png",
                "caption": "Figure 3:The supervised speech separation workflow. The diagram presents two mainstream approaches: Deep Clustering (DPCL) and Permutation Invariant Training (PIT). The number of speakers is assumed to be 2.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2508.10830/x4.png",
                "caption": "Figure 4:The pipeline of the deep clustering method.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2508.10830/x5.png",
                "caption": "Figure 5:The pipeline of the permutation invariant training method.",
                "position": 501
            }
        ]
    },
    {
        "header": "IVArchitectures",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10830/x6.png",
                "caption": "Figure 6:Overall pipeline of speech separation.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2508.10830/x7.png",
                "caption": "Figure 7:The process of feature extraction by the encoder and waveform reconstruction by the decoder in speech separation.",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2508.10830/x8.png",
                "caption": "Figure 8:Overview of the evolution of separator architectures in speech separation. The figure illustrates the development from single-structure models to hybrid architectures, highlighting the emergence of representative paradigms including RNN-based, CNN-based, Attention-based, and Mixture-based approaches. Note: As many publications involve authors from multiple institutions, the affiliation for each method is determined by the corresponding author’s institution, or the first author’s institution if a corresponding author is not specified.",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2508.10830/x9.png",
                "caption": "Figure 9:Overall pipeline of DPRNN.",
                "position": 1757
            }
        ]
    },
    {
        "header": "VEvaluation Metrics",
        "images": []
    },
    {
        "header": "VIDatasets",
        "images": []
    },
    {
        "header": "VIIResults with Different Models and Different Datasets",
        "images": []
    },
    {
        "header": "VIIIPlatforms",
        "images": []
    },
    {
        "header": "IXChallenges & Explorations",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10830/x10.png",
                "caption": "Figure 10:Causal speech separation pipeline and causal network architectures. (a) Causal convolutional layers, (b) unidirectional recurrent neural networks, and Transformer models with causal masks. Causal convolutional layers apply masked convolutions along the temporal dimension to ensure that each output frame depends only on the current and previous frames. Unidirectional RNNs capture local sequential dependencies through forward recurrence. The Transformer module employs a triangular causal mask in the self-attention mechanism to block access to future frames.",
                "position": 3688
            }
        ]
    },
    {
        "header": "XConclusions",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/lk.jpg",
                "caption": "",
                "position": 6074
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/cg.jpg",
                "caption": "",
                "position": 6087
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/swd.jpg",
                "caption": "",
                "position": 6100
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/ly.jpg",
                "caption": "",
                "position": 6113
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/cz.png",
                "caption": "",
                "position": 6125
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/ws.jpg",
                "caption": "",
                "position": 6138
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/hsl.jpg",
                "caption": "",
                "position": 6150
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/wzq.png",
                "caption": "",
                "position": 6163
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/lad.jpg",
                "caption": "",
                "position": 6176
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/wzy.png",
                "caption": "",
                "position": 6189
            },
            {
                "img": "https://arxiv.org/html/2508.10830/bio_img/xlhu.jpg",
                "caption": "",
                "position": 6202
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]