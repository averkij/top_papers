[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/Fig1.png",
                "caption": "Figure 1:A: LLMs Game Process.LLMs select two elements per trial based on the inventory and trial history.B: Human Game Interface.Players select two elements to discover new elements, added to the inventory.C: LLMs and Human Performance.",
                "position": 351
            }
        ]
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/Fig2.png",
                "caption": "Figure 2:A: Human and LLMs different Temperatures’ Performance.LLM and Human Performance Across Temperatures. For LLMs, we set four temperatures(0, 0.3, 0.7, 1). LLMs (gpt-4o,LLaMA3.1-8B,LLaMA3.1-70B) achieve their best performance attemperature=1temperature1\\text{temperature}=1temperature = 1.B: Human and LLMs Best Temperatures’ Behaviors.According to whether the combination selected by each trial is repeated, successful, and initial, the behavior of each LLM trial is divided into 5 categories. Compare the temperature at which LLM performs best with humans ando1behavior.C: LLM Inventory Performance Relative to Human Percentiles.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/Fig3.png",
                "caption": "Figure 3:Regression Estimates by Temperature and Model. All models show lower empowerment weights than humans, excepto1. As temperature increases, uncertainty weights rise, witho1showing the highest weights across all models and humans.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/Fig4.png",
                "caption": "Figure 4:A: SAE Correlation Analysis.Maximum correlation of uncertainty values across layers, peaking at layer 2. Maximum correlation of empowerment values across layers, peaking at layer 72. Maximum correlation of choices across layers, peaking at layer 1.B: LLaMA3.1-70B Intervention Regression Results.The regression estimates for empowerment, and uncertainty under the original condition(LLaMA3.1-70B, temperature = 1), empowerment intervention (set to 0), and uncertainty intervention (set to 0).C: LLaMA3.1-70B Average Inventory of Interventions.Uncertainty intervention significantly reduces the average inventory, indicating its essential role in model performance.",
                "position": 391
            }
        ]
    },
    {
        "header": "4Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Game Difficulty",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/average_game_difficulty.png",
                "caption": "Figure 5:Game Difficulty vs. Inventory Size.Based on the real game tree, each inventory size has a different success probability.",
                "position": 743
            }
        ]
    },
    {
        "header": "Appendix BThe LLM Behavior Across Different Temperatures",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/Behavior.png",
                "caption": "Figure 6:Behavioral Categories of LLMs at Different Temperatures.Each trial is categorized into five conditions:(1)Failure with Existing Combination: The trial repeats a previous combination that does not generate a new element. (2)Failure with New Combination: The trial uses a new combination for the first time, but it fails to generate a new element. (3)Success with New Combination: The trial uses a new combination for the first time, successfully generating a new element. (4)Success with Existing Combination: The trial repeats a previous combination that successfully generates an element. (5)Invalid Trial: The chosen one or two elements are not present in the current inventory.",
                "position": 754
            }
        ]
    },
    {
        "header": "Appendix CAttempts for Model Improvements",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/improved_prompt.png",
                "caption": "Figure 7:A: LLMs Game Original Prompt.The prompt for each trial consists of three parts: the system prompt, which provides the game rule guide; the current inventory including those from the beginning and discoveries during the game; and the trial history.B: LLMs Game Prompt Engineering.Each colored section highlights a specific goal: The green section encourages models to explore more creative combinations by reminding them that a wider variety of elements can unlock new possibilities. The blue section emphasizes avoiding repeated behavior by explicitly instructing the model to check past attempts.",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/prompt_performance.png",
                "caption": "Figure 8:A: Best Temperature of Each Model and Human Performance.B: Best Temperature of Each Model and Human Behaviors.Choose the LLM models’(gpt-4o, LLaMA3.1-8B, LLaMA3.1-70B) best performance at temperature = 1, and compare it with human and o1, gpt-4o prompt-engineering(temperature = 1). Compare each model’s performance and behaviors.",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/reason_performance.png",
                "caption": "Figure 9:A: Best Temperature of Each Model and Human Performance.B: Best Temperature of Each Model and Human Behaviors.Choose the LLM models’(gpt-4o, LLaMA3.1-8B, LLaMA3.1-70B) best performance at temperature = 1, and compare it with human and o1, deepseek-reasoner(temperature = 1). Compare each model’s performance and behaviors.C: Regression estimates by temperature and model.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/reason_200.png",
                "caption": "Figure 10:The reasoning process of DeepSeek-R1 in Trial 200: The model explores possible combinations(blue color part) for discovering a new element in the game by systematically reviewing inventory elements(orange color part), prior attempts(blue color part), and logical inferences to make the decision(red color part). It also found some more empowerment elements(green color part) but didn’t choose them.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/70B_intervention_result.png",
                "caption": "Figure 11:LLaMA3.1-70B Average Inventory of Uncertainty Intervention.Set 5 different levels of uncertainty intervention(0.0, 0.5, 0.7, 1.0, 2.0). Increasing the uncertainty intervention progressively disrupts the model’s ability to complete the task, indicating the critical role of early uncertainty layers in processing task history and context.LLaMA3.1-70B Average Inventory of Empowerment Intervention.Performance remains closer to the original level when the intervention is set to 1.5, whereas other levels of intervention result in performance degradation.",
                "position": 808
            }
        ]
    },
    {
        "header": "Appendix DSAE Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/SAE_TrainingProcess.png",
                "caption": "Figure 12:Sparse Autoencoder (SAE) Training Metrics.Each row represents different model architectures. From left to right, the panels illustrate the layer-wise test L2 norm, test reconstruction loss, and the number of active neurons during training. The top row corresponds to a smaller model(LLaMA3.1-8B), and the bottom row corresponds to a larger model(LLaMA3.1-70B) with more layers. Test Accuracy Between Original and Reconstructed Data. In both cases, reconstructed data achieves higher accuracy across layers, demonstrating the SAE’s ability to preserve essential features during encoding and reconstruction.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2501.18009/extracted/6165125/8B_intervention.png",
                "caption": "Figure 13:A: SAE Correlation Analysis.Maximum correlation of uncertainty values across layers, peaking at layer 7. Maximum correlation of empowerment values across layers, peaking at layer 22. Maximum correlation of choices across layers, peaking at layer 15.B: LLaMA3.1-8B Intervention Regression Results.The bars represent the regression estimates for empowerment, and uncertainty under the original condition(LLaMA3.1-8B, temperature = 1), empowerment intervention (set to zero), and uncertainty intervention (set to zero).C: LLaMA3.1-8B Average Inventory of Interventions.Uncertainty intervention leads to a significant reduction in the average inventory, indicating its essential role in model performance.",
                "position": 831
            }
        ]
    },
    {
        "header": "Appendix EReplicated SAE Result in LLaMA3.1-8B",
        "images": []
    }
]