[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x1.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2503.08893/extracted/6271440/logos/globe.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x2.png",
                "caption": "Figure 1:(a)EvalTreeautomatically constructs a capability tree given an LMâ€™s performance on every individual benchmark instance, and then generates a weakness profile by extracting nodes with statistically low performance (weakness profiling).\n(b) Training data collection guided by weakness profiling effectively improves LM performance, e.g., achieving a 2.5Ã—\\timesÃ—accuracy gain on MATH compared to being guided by a generic capability.",
                "position": 143
            }
        ]
    },
    {
        "header": "2LM Weakness Profiles",
        "images": []
    },
    {
        "header": "3EvalTree: A Tree-Based Method for Profiling LM Weaknesses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x3.png",
                "caption": "Figure 2:EvalTreeâ€™s four-stage tree construction pipeline.(1) Capability Annotationprompts an LM to identify a natural language description of each instanceâ€™s capability.(2) Capability Embeddingmaps instances to a vector space using sentence embeddings of their annotated capabilities.(3) Recursive Clustering-Based Constructionbuilds the tree by clustering capability embeddings using K-Means recursively.(4) Capability Descriptionassigns each node a natural language summary of its childrenâ€™s capabilities using an LM.",
                "position": 343
            }
        ]
    },
    {
        "header": "4Baseline Methods for Profiling LM Weaknesses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x4.png",
                "caption": "Figure 3:Comparison of weakness profiling methods using Low-Performance Identification Assessment.\nThe first row shows how the average LM performance across identified weaknesses changes as we vary the minimum weakness profile sizeMâ€²superscriptğ‘€â€²M^{\\prime}italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT.\nThe second row shows how the overall performance on all associated instances changes as we vary the minimum number of associated instancesNâ€²superscriptğ‘â€²N^{\\prime}italic_N start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT.\nExperiments in (a) were conducted on MATH with Llama 3.1 8B Instruct(Dubey etÂ al.,2024)and DART-Math-Llama3-8B (Uniform)(Tong etÂ al.,2024), and experiments in (b) were conducted on WildChat10K, where the win-rate is the percentage of instances in which Llama 3.2 3B Instruct(Meta,2024)is preferred over Gemma 2 IT 2B(RiviÃ¨re etÂ al.,2024).\nA lower curve indicates more precise identification of true low-performing weaknesses andEvalTreeconsistently achieves the lowest curve.",
                "position": 429
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x5.png",
                "caption": "Figure 4:Comparison of weakness profiling methods using Ground-Truth Weakness Assessment.\nThe plot shows F1 score curves ofTextDiff,QualEval, andEvalTree, where the weakness profile size varies from 1 to 20;\nthe F1 score measures how precisely and comprehensively ground-truth weaknesses are captured.\nA horizontal line indicates each methodâ€™s highest score.dğ‘‘ditalic_dis a hyperparameter to control the sampling probability.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x6.png",
                "caption": "Figure 5:Accuracy of different LMs on MATH and DS-1000 test sets.\nEach chart includes the accuracy of the initial LM (Llama 3.1 8B Instruct and DeepSeek-Coder-Base 6.7B for MATH and DS-1000).\nFor all other results, bars represent the accuracy of LMs trained on data collected by the corresponding strategy, with error bars indicating the standard error across 5 seeds.\nBars for LMs trained on directly sampled data are included for reference, although they have an unfair advantage and are not a direct point of comparison.\nData collection guided byEvalTree-identified weaknesses yields the highest accuracy gain.",
                "position": 539
            }
        ]
    },
    {
        "header": "6Further Applications ofEvalTree",
        "images": []
    },
    {
        "header": "7Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x7.png",
                "caption": "Figure 6:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the MATH benchmark(Hendrycks etÂ al.,2021b).\nExperiments were conducted with GPT-4o mini(OpenAI,2024a), Llama 3.1 8B Instruct(Dubey etÂ al.,2024), and DART-Math-Llama3-8B (Uniform)(Tong etÂ al.,2024).\nâ€œAll Instancesâ€ in the legend refers to all instances in the test set.\nAy=xğ‘¦ğ‘¥y=xitalic_y = italic_xline is included in all figures to indicate the thresholdÏ„ğœ\\tauitalic_Ï„.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x8.png",
                "caption": "Figure 7:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the MMLU benchmark(Hendrycks etÂ al.,2021a).\nExperiments were conducted with GPT-4o mini(OpenAI,2024a), Llama 3.1 8B Instruct(Dubey etÂ al.,2024), and TÃœLU 3 8B(Lambert etÂ al.,2024).\nâ€œAll Instancesâ€ in the legend refers to all instances in the test set.\nAy=xğ‘¦ğ‘¥y=xitalic_y = italic_xline is included in all figures to indicate the thresholdÏ„ğœ\\tauitalic_Ï„.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x9.png",
                "caption": "Figure 8:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the DS-1000 benchmark(Lai etÂ al.,2023).\nExperiments were conducted with GPT-4o(OpenAI,2024b), GPT-3.5 Turbo(OpenAI,2022), and DeepSeek-Coder-Base 6.7B(Guo etÂ al.,2024).\nâ€œAll Instancesâ€ in the legend refers to all instances in the test set.\nAy=xğ‘¦ğ‘¥y=xitalic_y = italic_xline is included in all figures to indicate the thresholdÏ„ğœ\\tauitalic_Ï„.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x10.png",
                "caption": "Figure 9:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the MATH benchmark(Hendrycks etÂ al.,2021b)as the profiling set and the CollegeMath benchmark(Tang etÂ al.,2024)as the test set.\nExperiments were conducted with GPT-4o mini(OpenAI,2024a), Llama 3.1 8B Instruct(Dubey etÂ al.,2024), and DART-Math-Llama3-8B (Uniform)(Tong etÂ al.,2024).\nâ€œAll Instancesâ€ in the legend refers to all instances in the test set.\nNote that they=xğ‘¦ğ‘¥y=xitalic_y = italic_xline of the thresholdÏ„ğœ\\tauitalic_Ï„used in the node extraction algorithm is not drawn here, as comparing accuracies with the threshold directly is not meaningful due to the differing distributions of the profiling and test sets, which are from two different benchmarks.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1389
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x11.png",
                "caption": "Figure 10:(a) Win-rate curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the WildChat10K benchmark(Zhao etÂ al.,2024a).\n(b) Win-rate curves of weakness instances and strength instances (from the test set) extracted using the WildChat10K benchmark as the profiling set, with the ShareGPT10K and Chatbot Arena(Chiang etÂ al.,2024)benchmarks serving as the respective test sets.\nThe win-rate refers to the win-rate of Llama 3.2 3B Instruct(Meta,2024)compared to Gemma 2 IT 2B(RiviÃ¨re etÂ al.,2024), as evaluated by the LM judge(Zheng etÂ al.,2023; Dubois etÂ al.,2023).\nâ€œIDâ€ indicates that the profiling and test sets are from the same benchmark (WildChat10K), whereas â€œOODâ€ indicates that they are from different benchmarks.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1397
            }
        ]
    },
    {
        "header": "Appendix AImplementation Details of Automatic Capability Tree Construction",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details of Extracting Nodes with Low Performance",
        "images": []
    },
    {
        "header": "Appendix CDefault Experimental Configurations",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details of Baseline Methods for Profiling LM Weaknesses",
        "images": []
    },
    {
        "header": "Appendix EExperimental Details of Assessing Weakness Profiling Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x12.png",
                "caption": "Figure 11:Precision score curves ofTextDiff,QualEval, andEvalTree, with the weakness profile size varying from 1 to 20.dğ‘‘ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x13.png",
                "caption": "Figure 12:Recall score curves ofTextDiff,QualEval, andEvalTree, with the weakness profile size varying from 1 to 20.dğ‘‘ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).",
                "position": 2333
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x14.png",
                "caption": "Figure 13:F1 score curves ofTextDiff,QualEval, andEvalTree, with the weakness profile size varying from 1 to 20.\nPrecision, Recall, and thus F1 (more specifically,Ağ´Aitalic_Ain the formulas outlined in AppendixE.3.1) are computed on a separate test set, distinct from the profiling set used to generate the synthetic evaluation results.\nA horizontal line indicates each methodâ€™s highest score.dğ‘‘ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).",
                "position": 2658
            }
        ]
    },
    {
        "header": "Appendix FQuantitative Analysis of Flaws in Chatbot Arenaâ€™s Evaluation Practice",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x15.png",
                "caption": "Figure 14:Curves ofminâ¡{âˆ‘wiâˆˆWÏ„Fâ¢(Aâ¢(wi))/|WÏ„|âˆ£âˆ€Ï„,|WÏ„|â‰¥Mâ€²}conditionalsubscriptsubscriptğ‘¤ğ‘–subscriptğ‘Šğœğ¹ğ´subscriptğ‘¤ğ‘–subscriptğ‘Šğœfor-allğœsubscriptğ‘Šğœsuperscriptğ‘€â€²\\min\\{\\sum_{w_{i}\\in W_{\\tau}}F(A(w_{i}))/|W_{\\tau}|\\mid\\forall{\\tau},|W_{\\tau%\n}|\\geq M^{\\prime}\\}roman_min { âˆ‘ start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ italic_W start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_F ( italic_A ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) / | italic_W start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT | âˆ£ âˆ€ italic_Ï„ , | italic_W start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT | â‰¥ italic_M start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT }(the first row) andminâ¡{Fâ¢(SÏ„)âˆ£âˆ€Ï„,|SÏ„|â‰¥Nâ€²}conditionalğ¹subscriptğ‘†ğœfor-allğœsubscriptğ‘†ğœsuperscriptğ‘â€²\\min\\{F(S_{\\tau})\\mid\\forall{\\tau},|S_{\\tau}|\\geq N^{\\prime}\\}roman_min { italic_F ( italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT ) âˆ£ âˆ€ italic_Ï„ , | italic_S start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT | â‰¥ italic_N start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT }(the second row).\nSee Section5.1for the experimental setup.\nExperiments in (a) were conducted on MATH with Llama 3.1 8B Instruct(Dubey etÂ al.,2024)and DART-Math-Llama3-8B (Uniform)(Tong etÂ al.,2024), and experiments in (b) were conducted on WildChat10K, where the win-rate is the percentage of instances in which Llama 3.2 3B Instruct(Meta,2024)is preferred over Gemma 2 IT 2B(RiviÃ¨re etÂ al.,2024).\nWe compareEvalTreeusing the default capability tree construction pipeline withEvalTreeusing the capability tree built with the hierarchical clustering algorithm here.",
                "position": 2836
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x16.png",
                "caption": "Figure 15:F1 score curves ofEvalTreeusing two different capability tree construction pipelines, with the weakness profile size varying from 1 to 20.\nSee Section5.2for the experimental setup.\nA horizontal line indicates each methodâ€™s highest score.dğ‘‘ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).\nWe compareEvalTreeusing the default capability tree construction pipeline withEvalTreeusing the capability tree built with the hierarchical clustering algorithm here.",
                "position": 2846
            }
        ]
    },
    {
        "header": "Appendix GAblation Study: Alternative Approach to Tree Construction",
        "images": []
    }
]