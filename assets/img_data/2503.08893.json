[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x1.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2503.08893/extracted/6271440/logos/globe.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x2.png",
                "caption": "Figure 1:(a)EvalTreeautomatically constructs a capability tree given an LM’s performance on every individual benchmark instance, and then generates a weakness profile by extracting nodes with statistically low performance (weakness profiling).\n(b) Training data collection guided by weakness profiling effectively improves LM performance, e.g., achieving a 2.5×\\times×accuracy gain on MATH compared to being guided by a generic capability.",
                "position": 143
            }
        ]
    },
    {
        "header": "2LM Weakness Profiles",
        "images": []
    },
    {
        "header": "3EvalTree: A Tree-Based Method for Profiling LM Weaknesses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x3.png",
                "caption": "Figure 2:EvalTree’s four-stage tree construction pipeline.(1) Capability Annotationprompts an LM to identify a natural language description of each instance’s capability.(2) Capability Embeddingmaps instances to a vector space using sentence embeddings of their annotated capabilities.(3) Recursive Clustering-Based Constructionbuilds the tree by clustering capability embeddings using K-Means recursively.(4) Capability Descriptionassigns each node a natural language summary of its children’s capabilities using an LM.",
                "position": 343
            }
        ]
    },
    {
        "header": "4Baseline Methods for Profiling LM Weaknesses",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x4.png",
                "caption": "Figure 3:Comparison of weakness profiling methods using Low-Performance Identification Assessment.\nThe first row shows how the average LM performance across identified weaknesses changes as we vary the minimum weakness profile sizeM′superscript𝑀′M^{\\prime}italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT.\nThe second row shows how the overall performance on all associated instances changes as we vary the minimum number of associated instancesN′superscript𝑁′N^{\\prime}italic_N start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT.\nExperiments in (a) were conducted on MATH with Llama 3.1 8B Instruct(Dubey et al.,2024)and DART-Math-Llama3-8B (Uniform)(Tong et al.,2024), and experiments in (b) were conducted on WildChat10K, where the win-rate is the percentage of instances in which Llama 3.2 3B Instruct(Meta,2024)is preferred over Gemma 2 IT 2B(Rivière et al.,2024).\nA lower curve indicates more precise identification of true low-performing weaknesses andEvalTreeconsistently achieves the lowest curve.",
                "position": 429
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x5.png",
                "caption": "Figure 4:Comparison of weakness profiling methods using Ground-Truth Weakness Assessment.\nThe plot shows F1 score curves ofTextDiff,QualEval, andEvalTree, where the weakness profile size varies from 1 to 20;\nthe F1 score measures how precisely and comprehensively ground-truth weaknesses are captured.\nA horizontal line indicates each method’s highest score.d𝑑ditalic_dis a hyperparameter to control the sampling probability.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x6.png",
                "caption": "Figure 5:Accuracy of different LMs on MATH and DS-1000 test sets.\nEach chart includes the accuracy of the initial LM (Llama 3.1 8B Instruct and DeepSeek-Coder-Base 6.7B for MATH and DS-1000).\nFor all other results, bars represent the accuracy of LMs trained on data collected by the corresponding strategy, with error bars indicating the standard error across 5 seeds.\nBars for LMs trained on directly sampled data are included for reference, although they have an unfair advantage and are not a direct point of comparison.\nData collection guided byEvalTree-identified weaknesses yields the highest accuracy gain.",
                "position": 539
            }
        ]
    },
    {
        "header": "6Further Applications ofEvalTree",
        "images": []
    },
    {
        "header": "7Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x7.png",
                "caption": "Figure 6:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the MATH benchmark(Hendrycks et al.,2021b).\nExperiments were conducted with GPT-4o mini(OpenAI,2024a), Llama 3.1 8B Instruct(Dubey et al.,2024), and DART-Math-Llama3-8B (Uniform)(Tong et al.,2024).\n“All Instances” in the legend refers to all instances in the test set.\nAy=x𝑦𝑥y=xitalic_y = italic_xline is included in all figures to indicate the thresholdτ𝜏\\tauitalic_τ.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x8.png",
                "caption": "Figure 7:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the MMLU benchmark(Hendrycks et al.,2021a).\nExperiments were conducted with GPT-4o mini(OpenAI,2024a), Llama 3.1 8B Instruct(Dubey et al.,2024), and TÜLU 3 8B(Lambert et al.,2024).\n“All Instances” in the legend refers to all instances in the test set.\nAy=x𝑦𝑥y=xitalic_y = italic_xline is included in all figures to indicate the thresholdτ𝜏\\tauitalic_τ.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1373
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x9.png",
                "caption": "Figure 8:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the DS-1000 benchmark(Lai et al.,2023).\nExperiments were conducted with GPT-4o(OpenAI,2024b), GPT-3.5 Turbo(OpenAI,2022), and DeepSeek-Coder-Base 6.7B(Guo et al.,2024).\n“All Instances” in the legend refers to all instances in the test set.\nAy=x𝑦𝑥y=xitalic_y = italic_xline is included in all figures to indicate the thresholdτ𝜏\\tauitalic_τ.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x10.png",
                "caption": "Figure 9:Accuracy curves of weakness instances and strength instances (from the test set) extracted using the MATH benchmark(Hendrycks et al.,2021b)as the profiling set and the CollegeMath benchmark(Tang et al.,2024)as the test set.\nExperiments were conducted with GPT-4o mini(OpenAI,2024a), Llama 3.1 8B Instruct(Dubey et al.,2024), and DART-Math-Llama3-8B (Uniform)(Tong et al.,2024).\n“All Instances” in the legend refers to all instances in the test set.\nNote that they=x𝑦𝑥y=xitalic_y = italic_xline of the thresholdτ𝜏\\tauitalic_τused in the node extraction algorithm is not drawn here, as comparing accuracies with the threshold directly is not meaningful due to the differing distributions of the profiling and test sets, which are from two different benchmarks.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1389
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x11.png",
                "caption": "Figure 10:(a) Win-rate curves of weakness instances and strength instances (from the test set) extracted using the random profiling/test split of the WildChat10K benchmark(Zhao et al.,2024a).\n(b) Win-rate curves of weakness instances and strength instances (from the test set) extracted using the WildChat10K benchmark as the profiling set, with the ShareGPT10K and Chatbot Arena(Chiang et al.,2024)benchmarks serving as the respective test sets.\nThe win-rate refers to the win-rate of Llama 3.2 3B Instruct(Meta,2024)compared to Gemma 2 IT 2B(Rivière et al.,2024), as evaluated by the LM judge(Zheng et al.,2023; Dubois et al.,2023).\n“ID” indicates that the profiling and test sets are from the same benchmark (WildChat10K), whereas “OOD” indicates that they are from different benchmarks.\nThe number of weakness/strength instances is shown as a reference; when the number is very low, the curve may exhibit significant fluctuations, affecting the general trend.",
                "position": 1397
            }
        ]
    },
    {
        "header": "Appendix AImplementation Details of Automatic Capability Tree Construction",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details of Extracting Nodes with Low Performance",
        "images": []
    },
    {
        "header": "Appendix CDefault Experimental Configurations",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details of Baseline Methods for Profiling LM Weaknesses",
        "images": []
    },
    {
        "header": "Appendix EExperimental Details of Assessing Weakness Profiling Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x12.png",
                "caption": "Figure 11:Precision score curves ofTextDiff,QualEval, andEvalTree, with the weakness profile size varying from 1 to 20.d𝑑ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).",
                "position": 2329
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x13.png",
                "caption": "Figure 12:Recall score curves ofTextDiff,QualEval, andEvalTree, with the weakness profile size varying from 1 to 20.d𝑑ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).",
                "position": 2333
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x14.png",
                "caption": "Figure 13:F1 score curves ofTextDiff,QualEval, andEvalTree, with the weakness profile size varying from 1 to 20.\nPrecision, Recall, and thus F1 (more specifically,A𝐴Aitalic_Ain the formulas outlined in AppendixE.3.1) are computed on a separate test set, distinct from the profiling set used to generate the synthetic evaluation results.\nA horizontal line indicates each method’s highest score.d𝑑ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).",
                "position": 2658
            }
        ]
    },
    {
        "header": "Appendix FQuantitative Analysis of Flaws in Chatbot Arena’s Evaluation Practice",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08893/x15.png",
                "caption": "Figure 14:Curves ofmin⁡{∑wi∈WτF⁢(A⁢(wi))/|Wτ|∣∀τ,|Wτ|≥M′}conditionalsubscriptsubscript𝑤𝑖subscript𝑊𝜏𝐹𝐴subscript𝑤𝑖subscript𝑊𝜏for-all𝜏subscript𝑊𝜏superscript𝑀′\\min\\{\\sum_{w_{i}\\in W_{\\tau}}F(A(w_{i}))/|W_{\\tau}|\\mid\\forall{\\tau},|W_{\\tau%\n}|\\geq M^{\\prime}\\}roman_min { ∑ start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_W start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_F ( italic_A ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) / | italic_W start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT | ∣ ∀ italic_τ , | italic_W start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT | ≥ italic_M start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT }(the first row) andmin⁡{F⁢(Sτ)∣∀τ,|Sτ|≥N′}conditional𝐹subscript𝑆𝜏for-all𝜏subscript𝑆𝜏superscript𝑁′\\min\\{F(S_{\\tau})\\mid\\forall{\\tau},|S_{\\tau}|\\geq N^{\\prime}\\}roman_min { italic_F ( italic_S start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT ) ∣ ∀ italic_τ , | italic_S start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT | ≥ italic_N start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT }(the second row).\nSee Section5.1for the experimental setup.\nExperiments in (a) were conducted on MATH with Llama 3.1 8B Instruct(Dubey et al.,2024)and DART-Math-Llama3-8B (Uniform)(Tong et al.,2024), and experiments in (b) were conducted on WildChat10K, where the win-rate is the percentage of instances in which Llama 3.2 3B Instruct(Meta,2024)is preferred over Gemma 2 IT 2B(Rivière et al.,2024).\nWe compareEvalTreeusing the default capability tree construction pipeline withEvalTreeusing the capability tree built with the hierarchical clustering algorithm here.",
                "position": 2836
            },
            {
                "img": "https://arxiv.org/html/2503.08893/x16.png",
                "caption": "Figure 15:F1 score curves ofEvalTreeusing two different capability tree construction pipelines, with the weakness profile size varying from 1 to 20.\nSee Section5.2for the experimental setup.\nA horizontal line indicates each method’s highest score.d𝑑ditalic_dis a hyperparameter to control the sampling probability (see AppendixE.3.1).\nWe compareEvalTreeusing the default capability tree construction pipeline withEvalTreeusing the capability tree built with the hierarchical clustering algorithm here.",
                "position": 2846
            }
        ]
    },
    {
        "header": "Appendix GAblation Study: Alternative Approach to Tree Construction",
        "images": []
    }
]