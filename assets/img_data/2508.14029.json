[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14029/x1.png",
                "caption": "Figure 1:We trainQwen2.5-32B-Instructon theDAPO-17kdataset using ourSvSstrategy and standard RLVR.SvSachieves superior efficiency and effectiveness on competition-level AIME benchmarks, showing significant improvements inPass@32andPass@1(average 32 times) scores.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x1.png",
                "caption": "",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x2.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14029/x3.png",
                "caption": "Figure 2:Policy entropy andPass@kduring RLVR training under different data strategies. The dashed line indicates policy entropy on evaluated competition-level benchmarks in the right figure. The augmented problems in theAugmentexperiment are updated at the 300th step.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x3.png",
                "caption": "",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x4.png",
                "caption": "",
                "position": 156
            }
        ]
    },
    {
        "header": "2Rethinking the Entropyâ€“Performance trade-off in RLVR",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14029/x5.png",
                "caption": "Figure 3:The data workflow of ourSvSin a training iteration, comprising original problem solving, variational problem synthesis, synthetic problem solving, and policy update data filtering.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x6.png",
                "caption": "Figure 4:Illustrations of a challenging problem, its correct solution from policy, the synthetic variational problems from the solution, and the reward-shaping strategy for the synthetic problems.",
                "position": 327
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14029/x7.png",
                "caption": "Figure 5:Policy entropy trajectories during training for standard RLVR and our proposedSvSstrategy across various models and datasets.",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x8.png",
                "caption": "Figure 6:Evaluating the scaled-upPass@kperformance on the AIME-24, AIME-25, Beyond-AIME, and MATH-500 benchmarks. The maximum response tokens here is set to 24k.",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2508.14029/x9.png",
                "caption": "Figure 7:Comparison of instance-level accuracy between standard RLVR andSvStrained model. For each problem, the accuracy is averaged over 1024 generations on both AIME24 and AIME25.",
                "position": 1085
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    }
]