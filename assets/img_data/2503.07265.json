[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07265/x1.png",
                "caption": "Figure 1:Comparison of previous straightforward benchmarks and our proposed WISE. (a) Previous benchmarks typically use simple prompts, such as “A photo of two bananas” in GenEval[9], which only require shallow text-image alignment. (b) WISE, in contrast, uses prompts that demand world knowledge and reasoning, such as “Einstein’s favorite musical instrument,” to evaluate a model’s ability to generate images based on deeper understanding.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2503.07265/x2.png",
                "caption": "Figure 2:Illustrative samples of WISE from 3 core dimensions with 25 subdomains. By employing non-straightforward semantic prompts, it requires T2I models to perform logical inference grounded in world knowledge for accurate generation of target entities.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2503.07265/x3.png",
                "caption": "Figure 3:Detailed composition of WISE, consisting of 3 categories and 25 subdomains.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2503.07265/x4.png",
                "caption": "Figure 4:Illustration of the WISE framework, which employs a four-phase verification process (Panel I to IV) to systematically evaluate generated content across three core dimensions. The two representative cases, science-domain input “candle in space” violates oxygen-dependent combustion principles, while spatiotemporal-domain “close-up of summer maple leaf” contradicts botanical seasonal patterns, both receiving 0 in consistency (see Evaluation Metrics in Panel III), confirming the benchmark’s sensitivity in world knowledge conflicts.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Our benchmark: WISE",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07265/x5.png",
                "caption": "Figure 5:WISE assess image quality based on three criteria: how accurately the image aligns with the prompt (Consistency), its level of realism (Realism), and its overall artistic appeal (Aesthetic Quality). Each metric is scored on a scale from 0 (Rejected) to 2 (Exemplary), providing a comprehensive assessment of the image’s fidelity, believability, and visual excellence.",
                "position": 188
            }
        ]
    },
    {
        "header": "4Evaluation Results",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07265/x6.png",
                "caption": "Figure 6:We utilize GPT-4o to evaluate the performance of text-to-image models. Above is the instruction we provided to GPT-4o.",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2503.07265/x7.png",
                "caption": "Figure 7:We utilize GPT-4 rewrite the prompts in our WISE benchmark, transforming them from complex, knowledge-demanding prompts into direct prompts. Above is the instruction we provided to GPT-4o.",
                "position": 1779
            }
        ]
    },
    {
        "header": "Appendix AWISE Category Descriptions",
        "images": []
    }
]