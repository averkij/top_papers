[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Linear-MoE System",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05447/x1.png",
                "caption": "Figure 1:Linear-MoE Architecture.In each Linear-MoE block, there is both an LSM layer and an MoE layer, with each layer preceded by its own normalization layer. The LSM layer is designed as a flexible abstraction of LSM methods, including: linear attention, SSM, and linear RNN, which follows a unified recurrence framework.",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2503.05447/x2.png",
                "caption": "Figure 2:Sequence Parallelism Approach on Hybrid Linear-MoE models.We exemplify the parallelism on the hybrid layers of LSM and standard attention with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reduce-scatter in backward, RS/AG: reduce-scatter in forward and all-gather in backward, AG/No: all-gather in forward and no-op in backward, No/AG: no-op in forward and all-gather in backward. Note that the SP communication operations for linear attention operate on the memory stateùêås‚àà‚Ñùd√ódsubscriptùêåùë†superscript‚Ñùùëëùëë\\mathbf{M}_{s}\\in\\mathbb{R}^{d\\times d}bold_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT, while for standard attention, they operate on statesùêäs,ùêïs‚àà‚ÑùC√ódsubscriptùêäùë†subscriptùêïùë†superscript‚Ñùùê∂ùëë\\mathbf{K}_{s},\\mathbf{V}_{s}\\in\\mathbb{R}^{C\\times d}bold_K start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C √ó italic_d end_POSTSUPERSCRIPT.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2503.05447/x3.png",
                "caption": "Figure 3:Linear-MoE System Implementation.The Linear-MoE system is composed of two main subsystems: Modeling and Training. It is developed in a non-intrusive manner, utilizing the latest version of Megatron-Core. All components within the system are designed with extensibility in mind, encompassing the LSM modules, base models, examples, and training technologies. This design allows for future enhancements and extensions of the system.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2503.05447/x4.png",
                "caption": "Figure 4:Training Throughput (Tokens/s).As sequence length increases, the throughput of Baseline declines significantly, whereas LSM models maintain stable training efficiency.",
                "position": 619
            }
        ]
    },
    {
        "header": "3Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05447/x5.png",
                "caption": "Figure 5:Inference Efficiency of A0.3B-2B Model Instances.We variate the decoding length from 1K to 128K with fixed batch size of 16 on single A800 80GB GPU to evaluate the Baseline w/ FlashAttention-2 and the Linear-MoE w/ Basic Linear Attention in terms of inference latency time and GPU memory usage.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2503.05447/x6.png",
                "caption": "Figure 6:Training Loss Curves of A0.3B-2B Model Instances.Left: pure Linear-MoE models; Right: hybrid Linear-MoE models. Linear-MoE shows competitive training convergence performance compared to the standard attention Baseline.",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2503.05447/x7.png",
                "caption": "",
                "position": 853
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05447/x8.png",
                "caption": "Figure 7:Training Loss Curves of A1B-7B Model Instances.",
                "position": 2357
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]