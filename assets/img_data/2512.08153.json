[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08153/x1.png",
                "caption": "Figure 1:The proposedTreeGRPOachieves the best pareto performance across the rewards and training efficiency, where the single-GPU runtime is the normalized wall-clock time. In (a), following the normalized metrics in RL domains(mnih2013playing), thenromalized reward scoreshere is calculated by(r−rs​d​3.5)/(rm​a​x−rs​d​3.5)(r-r_{sd3.5})/(r_{max}-r_{sd3.5}), where therm​a​xr_{max}in the HPS, ImageReward, Asethetic, ClipScore reward models are{1.0,2.0,10.0,1.0}\\{1.0,2.0,10.0,1.0\\}respectively.",
                "position": 106
            },
            {
                "img": "https://arxiv.org/html/2512.08153/x2.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08153/x3.png",
                "caption": "Figure 2:Introduction of TreeGRPO: Our framework optimizes the denoising process of diffusion/flow models by constructing search trees. Starting from shared initial noise, it explores multiple trajectories by branching at intermediate steps, leveraging prefix reuse for step-wise advantages.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Theoratical Analysis of TreeGRPO",
        "images": []
    },
    {
        "header": "6Experiment",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    }
]