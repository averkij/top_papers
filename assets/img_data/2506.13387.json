[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13387/x1.png",
                "caption": "Figure 1:Illustration of motivation. (a) Metric Depth Estimation typically restricts to a single domain or relies on camera parameters or sensors to enhance generalization. (b) Relative Depth Estimation generalizes better but is ambiguous in scale. (c) We therefore seek to transfer generalizable relative depth to metric depth by pixel-wise rescaling maps given image and readily accessible text description, which succeeds in obtaining metric depth for various domains with one lightweight trainable architecture. (d) Examples of qualitative results.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13387/x2.png",
                "caption": "Figure 2:Illustration of the proposed TR2M framework. Image and text embedding features are first obtained with separate frozen encoders, and a cross-modality attention module is proposed to integrate them. The scale and shift maps are predicted with different decoders to transfer relative depth to metric depth. The scale-oriented contrast enables embedding features more consistent with the depth distribution, thus enhancing scale perception capability.",
                "position": 211
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13387/x3.png",
                "caption": "Figure 3:Qualitative results on NYUv2. Our method consistently produces better predictions with much less error.ŒîŒî\\Deltaroman_ŒîdenotesA‚Å¢b‚Å¢s‚Å¢R‚Å¢e‚Å¢lùê¥ùëèùë†ùëÖùëíùëôAbsRelitalic_A italic_b italic_s italic_R italic_e italic_lranging from lowest (Balck) to highest (Red).",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2506.13387/x4.png",
                "caption": "Figure 4:The depth value of the relative depth map within the red rectangles is inconsistent with the ground truth. Our method can correct such errors when transferring to metric depth.",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2506.13387/x4.png",
                "caption": "Figure 4:The depth value of the relative depth map within the red rectangles is inconsistent with the ground truth. Our method can correct such errors when transferring to metric depth.",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2506.13387/x5.png",
                "caption": "Figure 5:Visualization of embedding space with t-SNE. Features with the same depth ranges are clustered more closely (Compare the circles with the same color) with‚Ñís‚Å¢o‚Å¢csubscript‚Ñíùë†ùëúùëê\\mathcal{L}_{soc}caligraphic_L start_POSTSUBSCRIPT italic_s italic_o italic_c end_POSTSUBSCRIPT. The feature distribution is more in line with the variation in depth, which is beneficial for the robustness of depth estimation.",
                "position": 1132
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13387/x6.png",
                "caption": "Figure 6:Examples of the datasets and the supplementary text descriptions which are generated by VLMs[37]with input images and prompts.",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2506.13387/x7.png",
                "caption": "Figure 7:Example qualitative results on the datasets utilized in the paper. We compared our method with the DepthAnything-Large[65]model fine-tuned on separate datasets for metric depth estimation. The darker color denotes the closer distance.",
                "position": 2723
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]