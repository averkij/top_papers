[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x1.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x2.png",
                "caption": "Figure 2:Multi-event video generation results from state-of-the-art video generators and MinT.We run two open-source models CogVideoX-5B[100]and Mochi 1[82], and two commercial models Kling 1.5[2]and Gen-3 Alpha[1]with a text prompt containing four consecutive events.\nAll of them only generate a subset of events while ignoring the remaining ones.\nIn contrast, MinT generates a natural video with all events smoothly connected.\nPlease refer toSec.C.6and ourproject pagefor more comparisons.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x3.png",
                "caption": "Figure 3:MinT framework.(a) Our model takes in a global caption describing the overall video, and a list of temporal captions specifying the sequential events.\nWe bind each event to a time range, enabling temporal control of the generated events.\n(b) To condition the video DiT on temporal captions, we introduce a new temporal cross-attention layer in each DiT block, which (c) concatenates the text embedding of all event prompts and leverages a time-aware positional encoding (Pos.Enc.) method to associate each event to its corresponding frames based on the event timestamps.\nMinT supports an additional scene cut conditioning, which can control the shot transition of the video.",
                "position": 202
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x4.png",
                "caption": "Figure 4:Comparison of vanilla RoPE and our Rescaled RoPE.We use the same random vector for video tokens and text embeddings to only visualize the bias introduced by positional encoding.\n(a) Vanilla RoPE uses raw timestamps as the rotation angle, where frames within one event might be biased to the wrong text.\n(b) We instead rescale all events to have the same lengthLùêøLitalic_L, so that video tokens always attend the most to the current event.\nIn addition, frames at event boundaries attend to adjacent events equally.",
                "position": 335
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x5.png",
                "caption": "Figure 5:T2V results on HoldOut and StoryBench.For CogVideoX and Mochi we concatenated the events into a single prompt, similar to the Concat baseline. Metrics in the first row measure visual quality, while those in the second row focus on the text alignment and transition smoothness between events.\nMinT performs the best in event-related metrics while maintaining a high visual quality.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x6.png",
                "caption": "Figure 6:Qualitative results of T2V.Concatenating all events into a single prompt (Concat) can only generate the first event.\nAutoregressive generation (AutoReg) suffers from video stagnation and cannot generate the third event.\nMEVG struggles to preserve the person‚Äôs identity and produces abrupt event transitions.\nMinT is the only method that generates all events with smooth transitions and consistent content.\nSeeSec.C.1for more qualitative results and ourproject pagefor video results.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x7.png",
                "caption": "Figure 7:Human preference evaluation against T2V baselines.MinT is better or competitive in visual quality, and significantly outperforms baselines in the other three event-related metrics.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x8.png",
                "caption": "Figure 8:Qualitative comparison of prompt enhancement results.The original short prompt is ‚Äúa cat drinking water‚Äù.",
                "position": 788
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails on Rotary Position Embedding",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x9.png",
                "caption": "Figure 9:Comparison of ReRoPE with different rescaling lengthLùêøLitalic_L.We use the same random vector for video tokens and text embeddings to only visualize the bias introduced by positional encoding.\nWe visualize the case where videos have a temporal dimension of 50, and there are three temporal captions.",
                "position": 2570
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x10.png",
                "caption": "Figure 10:Basic statistics of our training dataset.We show the distribution of video length, the number of events per video, and the length of individual events.\nMost videos contain 2 to 4 events, and most events are under 5s.",
                "position": 2588
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x10.png",
                "caption": "",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x11.png",
                "caption": "",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x12.png",
                "caption": "",
                "position": 2599
            }
        ]
    },
    {
        "header": "Appendix BDetailed Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x13.png",
                "caption": "Figure 11:Qualitative comparisons of T2V.",
                "position": 2718
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x14.png",
                "caption": "Figure 12:More T2V results from MinT.Please see ourproject pagefor more results.",
                "position": 2723
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x15.png",
                "caption": "Figure 13:Prompt enhancement results on VBench.We can generate more interesting videos from a simple prompt.\nThis highlights the flexible dynamics control ability brought by the temporal captions.\nPlease see ourproject pagefor video results.",
                "position": 2770
            }
        ]
    },
    {
        "header": "Appendix CMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05263/x16.png",
                "caption": "Figure 14:Generated videos with and without scene cut input.In each example, the first row is generated by inputting the scene cut at the illustrated timestamps, while the second row is by zeroing out the scene cut input.\nWhen using the scene cut, the model is able to generate a shot transition at desired timestamps, while keeping the subject consistent.\nIn the second example, the model generates smooth zoom-in and zoom-out effects when zeroing out scene cuts.\nPlease see ourproject pagefor more results.",
                "position": 2783
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x17.png",
                "caption": "Figure 15:Generated videos with different event time spans.In each example, we offset the start and end timestamps of all events by a specific number of seconds.\nResults show that MinT enables fine-grained event timing control while keeping the subjects‚Äô appearances to be roughly the same.\nThis capability is very useful for controllable video generation.\nPlease see ourproject pagefor more results.",
                "position": 2846
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x18.png",
                "caption": "Figure 16:Generated videos with out-of-distribution prompts.After fine-tuning, MinT still possesses the base model‚Äôs ability to generate novel concepts.\nPlease see ourproject pagefor more results.",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x19.png",
                "caption": "Figure 17:More comparisons with SOTA video generators.We run SOTA open-source models CogVideoX[100]and Mochi[82], and commercial models Kling 1.5[2]and Gen-3 Alpha[1]using their online APIs.\nPlease see ourproject pagefor video results.",
                "position": 3020
            },
            {
                "img": "https://arxiv.org/html/2412.05263/x20.png",
                "caption": "Figure 18:More comparisons with SOTA video generators.We run SOTA open-source models CogVideoX[100]and Mochi[82], and commercial models Kling 1.5[2]and Gen-3 Alpha[1]using their online APIs.\nPlease see ourproject pagefor video results.",
                "position": 3028
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Works",
        "images": []
    }
]