[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03981/x1.png",
                "caption": "Figure 1:X-Reasoner(■■\\blacksquare■blue bars), fine-tuned solely on general domain text, shows strong generalization across both modalities (e.g., multimodality) and domains (e.g., medicine), surpassing prior SOTA (Table8) trained with in-domain multimodal data.X-Reasoner-Med(■■\\blacksquare■red bars), its medical-specialized variant, sets new SOTA on numerous medical benchmarks.",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x7.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x8.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x9.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x10.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x11.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2A Journey Towards Generalizable Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03981/x12.png",
                "caption": "Figure 2:(Left) Our recipe for generalizable reasoning: based on an instruction-tuned VLM, we first conduct SFT on general-domain text data with distilled long CoTs. This is followed by RLVR on mathematical textual questions. This resulting model,X-Reasoner, exhibits significantly enhanced reasoning capabilities across modalities and domains. (Right) Model performance on MMMU-Pro (multimodal task) steadily improves at each stage of our recipe.",
                "position": 171
            }
        ]
    },
    {
        "header": "3A Comprehensive Evaluation ofX-Reasoner",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03981/x13.png",
                "caption": "Figure 3:ComparingX-Reasonerand baseline on text-only benchmarks and multi-modal benchmarks. Despite being trained with general-domain text-only data,X-Reasonercan significantly improve multi-modal benchmarks, showing the generalization ofX-Reasoner’s reasoning.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x14.png",
                "caption": "Figure 4:ComparingX-Reasoner,X-Reasoner-Medand baseline on text and multimodal medical benchmarks.X-Reasoner, trained with general-domain text-only data brings consistent improvement across medical tasks.X-Reasoner-Med, obtained by continued training ofX-Reasoneron medical text data, further improves medical domain performance.",
                "position": 760
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussions and Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGRPO",
        "images": []
    },
    {
        "header": "Appendix BHyperparameters",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Tasks",
        "images": []
    },
    {
        "header": "Appendix EPrevious SOTA Results",
        "images": []
    },
    {
        "header": "Appendix FComparison of RL and SFT on MedQA data",
        "images": []
    },
    {
        "header": "Appendix GTraining Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-reward.png",
                "caption": "Figure 5:RL training dynamics ofX-ReasonerandX-Reasoner-Med",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-reward.png",
                "caption": "",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-pg-loss.png",
                "caption": "",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-adv.png",
                "caption": "",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-med-reward.png",
                "caption": "",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-med-pg-loss.png",
                "caption": "",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-med-adv.png",
                "caption": "",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-res-len.png",
                "caption": "",
                "position": 2209
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-clip-ratio.png",
                "caption": "",
                "position": 2213
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-val.png",
                "caption": "",
                "position": 2217
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-med-res-len.png",
                "caption": "",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-med-clip-ratio.png",
                "caption": "",
                "position": 2226
            },
            {
                "img": "https://arxiv.org/html/2505.03981/extracted/6413881/figures/x-reasoner-med-val.png",
                "caption": "",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x15.png",
                "caption": "Table 11:Reasoning output comparison between the baselineQwen2.5-VL-7B-Instructmodel andX-Reasoneron a multimodal question.",
                "position": 2303
            },
            {
                "img": "https://arxiv.org/html/2505.03981/x16.png",
                "caption": "Table 12:Reasoning output comparison between the baselineQwen2.5-VL-7B-Instructmodel andX-Reasoneron an medical multimodal question.",
                "position": 2392
            }
        ]
    },
    {
        "header": "Appendix HQualitative Analysis",
        "images": []
    }
]