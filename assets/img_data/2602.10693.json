[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10693/x1.png",
                "caption": "Figure 1:Left: VESPO reformulates IS weight reshaping as finding a proposalQ∗Q^{*}that balances proximity toμ\\muandπ\\piunder a variance constraint.Right: Training reward (gbs/mbs=4) on Qwen3-30B-A3B-Base.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x2.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3VESPO: Variational Sequence-Level Soft Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10693/x3.png",
                "caption": "Figure 2:Surrogate objectivesf​(w)f(w)(top) and gradient scaling factorsϕ​(w)=w⋅f′​(w)\\phi(w)=w\\cdot f^{\\prime}(w)(bottom) for positive and negative advantages. Hard clipping zerosϕ\\phiabruptly at the boundary; VESPO peaks nearw=1w{=}1and decays smoothly.",
                "position": 448
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10693/x4.png",
                "caption": "Figure 3:Training reward across staleness levels (N∈{4,8,16,32,64}N\\in\\{4,8,16,32,64\\}) on Qwen3-30B-A3B-Base. Each panel shows one method with differentNNvalues. VESPO maintains stable, consistent training curves across all staleness levels.",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x5.png",
                "caption": "Figure 4:Training dynamics across staleness levels (NN= gbs/mbs∈{4,8,16,32,64}\\in\\{4,8,16,32,64\\}) on Qwen3-30B-A3B-Base. Each row corresponds to a differentNN; columns show training reward, AIME25 accuracy, response length, KL divergence, entropy, and PG loss. VESPO (red) maintains stable training across all conditions, while baselines exhibit characteristic failure modes.",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x6.png",
                "caption": "Figure 5:Training dynamics under fully asynchronous training on Qwen3-30B-A3B-Base. VESPO maintains stable training and achieves the highest reward and benchmark accuracy.",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x7.png",
                "caption": "Figure 6:Training stability under train-inference mismatch on Qwen3-30B-A3B-Base. VESPO maintains stable training without specialized fixes; combining VESPO with R2 achieves the best performance.",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x8.png",
                "caption": "Figure 7:Ablation on length normalization. VESPO without normalization achieves stable training; addingT\\sqrt{T}orTTnormalization causes instability and collapse.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x9.png",
                "caption": "Figure 8:Ablation on asymmetric hyperparameters.",
                "position": 988
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplicit Proposal Distributions of Existing Methods",
        "images": []
    },
    {
        "header": "Appendix BDerivation of the Proposal Distribution",
        "images": []
    },
    {
        "header": "Appendix CLength Normalization Introduces Bias",
        "images": []
    },
    {
        "header": "Appendix DAdditional Analysis of Baseline Failure Modes",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10693/x10.png",
                "caption": "Figure 9:GRPO:N=4N=4(blue) vsN=8N=8(orange). Entropy decreases more rapidly atN=4N=4, limiting exploration.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x11.png",
                "caption": "Figure 10:GSPO:N=4N=4(blue) vsN=8N=8(orange). Response length grows to∼{\\sim}3,000 tokens atN=4N=4before collapsing around step 1,200.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2602.10693/x12.png",
                "caption": "Figure 11:SAPO:N=4N=4(blue) vsN=8N=8(orange). Training collapses atN=8N=8due to insufficient suppression for negative advantages.",
                "position": 1800
            }
        ]
    },
    {
        "header": "Appendix EAlgorithm Pseudocode",
        "images": []
    },
    {
        "header": "Appendix FTraining Hyperparameters",
        "images": []
    }
]