[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13787/x1.png",
                "caption": "Figure 1:Left:Each LLM predicts its own behavior better than a second model can.The green bars represent each modelâ€™s accuracy in predicting its own hypothetical responses across unseen datasets after finetuning on facts about itself. The blue bars show how well a second model, finetuned on the same facts about the first model, can predict the first model. The results imply that models have privileged access to information about themselves (introspection). Error bars show 95% confidence intervals calculated from the standard error of the mean.Right:Our task for testing self-prediction.A model is asked to predict properties of its behavior on ahypotheticalprompt. This self-prediction is evaluated against the modelâ€™s ground-truth behavior (object-level) on the prompt. The figure shows a single example from one task, but results (Left) average over many examples and many tasks (FigureÂ 3).",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x2.png",
                "caption": "",
                "position": 197
            }
        ]
    },
    {
        "header": "2Overview of Methods",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13787/x3.png",
                "caption": "Figure 4:Self-prediction training setup and results.Left:Models are finetuned to correctly answer questions about the properties of their hypothetical behavior. Properties are extracted from the modelâ€™s ground-truth object-level behavior. Models are trained on a range of datasets and properties.Right:Self-prediction training increases accuracy on held-out datasets (p<0.01ð‘0.01p<0.01italic_p < 0.01).â˜…â˜…\\bigstarâ˜…refers to the baseline of always predicting the most common answer for a type of question.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x3.png",
                "caption": "",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x4.png",
                "caption": "",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x5.png",
                "caption": "Figure 5:Left:Cross-prediction training setup.Models are trained to predict the object-level behavior ofanothermodel, creating cross-trained modelsMâ¢2ð‘€2M2italic_M 2. We investigate if self-trained modelsMâ¢1ð‘€1M1italic_M 1have an advantage overMâ¢2ð‘€2M2italic_M 2models in predicting the behavior ofMâ¢1ð‘€1M1italic_M 1.Right:Models have an advantage when predicting their own behavior compared to being predicted by other models. The green bar shows the self-prediction accuracy of a model trained on its own behavior. The blue bars to their right show how well a subset of different models trained to predict the first model can predict it.â˜…â˜…\\bigstarâ˜…refers to the baseline of always predicting the most common answer for a type of question. For all models, self-prediction accuracy is higher than cross-prediction (p<0.01ð‘0.01p<0.01italic_p < 0.01). Results are shown for a set of tasks not observed during training. The pattern of results holds for the training set of tasks (SectionÂ A.2.2).",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x6.png",
                "caption": "",
                "position": 735
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x7.png",
                "caption": "Figure 6:Self-prediction trained models are better calibrated than cross-prediction trained models on held-out datasets.Left: Example of a well-calibrated prediction, showing close alignment between object-level behavior and hypothetical prediction distributions. Right: Calibration curves for Llama 70B and GPT-4o. Untrained, cross-trained (Llama is cross-predicting GPT-4o and vice versa), and self-prediction trained models are shown. The dotted diagonal shows perfect calibration.\nCurves show the probability of a hypothetical answer for an object-level behavior of a certain probability.\nSelf-prediction trained models have curves closer to the diagonal, indicating better calibration.",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x8.png",
                "caption": "",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x9.png",
                "caption": "",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x10.png",
                "caption": "Figure 7:Setup to test if models predict their changed behavior.We use the previously self-prediction trainedMâ¢1ð‘€1M1italic_M 1(here, GPT-4o) and change its behavior through further finetuning on the object-level behavior of another model (Claude 3.5 Sonnet), creating modelMCsubscriptð‘€ð¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT.MCsubscriptð‘€ð¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPThas changed behavior on held-out prompts. We then evaluate ifMCsubscriptð‘€ð¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTpredicts its changed behavior on these held-out prompts. The finetuning samples to change the modelâ€™s behavior do not include any hypothetical questions, only object-level behavior.",
                "position": 857
            }
        ]
    },
    {
        "header": "4Further experiments and negative results",
        "images": []
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Discussion and limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13787/x12.png",
                "caption": "Figure 9:Self-simulation: a possible mechanism for introspection.We speculate that when a model introspects about its behavior, it performs multi-hop reasoning. The first hop simulates its next-word output if the input was only â€œNear the summits of Mountâ€, and the second hop computes a property of the simulated output (resulting in the outputâ€œuâ€).",
                "position": 984
            }
        ]
    },
    {
        "header": "7Motivation: Benefits and Risks of introspection in LLMs",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13787/x13.png",
                "caption": "Figure 10:Self-prediction training effect across multiple models and response properties.The self-prediction accuracy of multiple models on a set of representative behavior properties is shown before (purple) and after training (green). We show generalization to held-out datasets â€“ for example, we train models to predict their ethical stance for preferring wealth and test on datasets regarding myopic preferences.",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x14.png",
                "caption": "",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x15.png",
                "caption": "",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x16.png",
                "caption": "Figure 11:The rate of compliant responses on the object-level (top) and for hypothetical questions (bottom) is shown. Models do not refuse and correctly follow most requests, except for untrained models being asked hypothetical questions.",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x17.png",
                "caption": "",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x18.png",
                "caption": "Figure 12:Do the self-predictions of untrained models match their own object-level behavior (dark pink) more than the behavior of another model (light pink)?",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x19.png",
                "caption": "",
                "position": 2231
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x20.png",
                "caption": "Figure 13:The self/cross prediction accuracy is shown on the set of tasks that the models were trained on. The self-prediction advantage holds for all models except for GPT-3.5 (p<0.01ð‘0.01p<0.01italic_p < 0.01).",
                "position": 2248
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x21.png",
                "caption": "Figure 14:The self-/cross-prediction results are shown for a selection of behavior properties.",
                "position": 2266
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x22.png",
                "caption": "",
                "position": 2270
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x23.png",
                "caption": "",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x24.png",
                "caption": "",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x25.png",
                "caption": "Figure 15:The heatmap shows how well the hypothetical predictions of any model (on the y-axis) match the object-level behavior of another (on the x-axis).",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x26.png",
                "caption": "Figure 16:We do not observe a self-prediction advantage when the Llama-70b has to predict whether or not it would change its answer in the presence of â€œAre you sure?â€.",
                "position": 2307
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x27.png",
                "caption": "Figure 17:Cross-prediction data-scaling trends.Both graphs show cross-prediction accuracy as a function of increasing cross-prediction training samples (1,000 to 30,000). The green lines indicate the self-prediction accuracy for each model at 30,000 training samples (49.6% for GPT-4, 48.5% for Llama 70b). Despite increasing training samples, cross-prediction accuracy plateaus well below self-prediction accuracy. This suggests that the self-prediction advantage is not due to insufficient cross-prediction training data.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x28.png",
                "caption": "",
                "position": 2341
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x29.png",
                "caption": "Figure 18:For each model, the self-prediction accuracy of the model before training (purple), self-prediction trained (green) and cross-prediction trained alternative models predicting the first.â˜…â˜…\\bigstarâ˜…denotes the baseline of guessing the most common response. Since the self-prediction target of the untrained model is the untrained model, it has a separate baseline from the other models in a group. Results are shown on a set of tasks held-out from training.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x30.png",
                "caption": "Figure 19:Same asFigureÂ 18, but on the set of tasks used during self- & cross-prediction training.",
                "position": 2382
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x31.png",
                "caption": "Figure 20:Calibration for second character prediction in GPT-4o and Llama 70B.The colors show the calibration for the different top behaviors. Both models are well-calibrated for the second and third most common behaviors.",
                "position": 2455
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x32.png",
                "caption": "",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x33.png",
                "caption": "Figure 21:Self-prediction advantage in calibration across multiple behavior properties for GPT-4o.We find that for GPT-4o, the self-prediction advantage in calibration persists across multiple behavior properties.",
                "position": 2483
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x34.png",
                "caption": "",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x35.png",
                "caption": "",
                "position": 2501
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x36.png",
                "caption": "",
                "position": 2507
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x37.png",
                "caption": "Figure 22:Self-prediction advantage in calibration across multiple behavior properties for Llama 70B.For Llama 70B, the self-prediction advantage is clear in the â€œfirst wordâ€ and â€œsecond characterâ€ behavior properties, but not in â€œEthical stanceâ€ and â€œAmong optionsâ€.",
                "position": 2513
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x38.png",
                "caption": "",
                "position": 2524
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x39.png",
                "caption": "",
                "position": 2531
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x40.png",
                "caption": "",
                "position": 2537
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x11.png",
                "caption": "Figure 23:Behavioral change results for multiple models.The green bars show the model predicting its changed behavior. The brown bars show the model predicting its old behavior. In GPT-4o and GPT-4, most of the green bars are higher than the brown bars, indicating that the models adapt their predictions to their new behavior properties.",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x41.png",
                "caption": "",
                "position": 2570
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x42.png",
                "caption": "",
                "position": 2575
            },
            {
                "img": "https://arxiv.org/html/2410.13787/x43.png",
                "caption": "Figure 24:Evidence for introspection in GPT-4o, after adjusting for mode collapse.We adjust such that the test sets ofMâ¢1ð‘€1M1italic_M 1andMCsubscriptð‘€ð¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPThave the same most common behavior baseline. We observe thatMCsubscriptð‘€ð¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTstill predicts its new behavior more on the balanced subset, supporting the introspection hypothesis.",
                "position": 2599
            },
            {
                "img": "https://arxiv.org/html/2410.13787/extracted/5930289/figures/sandbagging.png",
                "caption": "Figure 25:Sandbagging results for GPT-4o and GPT-3.5",
                "position": 2733
            },
            {
                "img": "https://arxiv.org/html/2410.13787/extracted/5930289/figures/schelling_point.png",
                "caption": "Figure 26:Schelling Point Results for GPT-4o and GPT-3.5",
                "position": 2736
            },
            {
                "img": "https://arxiv.org/html/2410.13787/extracted/5930289/figures/steganography.png",
                "caption": "Figure 27:Steganography Performance for GPT-4o and GPT-3.5",
                "position": 2739
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]