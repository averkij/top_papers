[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19054/x1.png",
                "caption": "Figure 1:(a) and (b):Illustration of Text-to-CAD, which converts a texutal description into CAD parametric sequences.(b) and (c):Illustration of multimodal characteristics. CAD models are created using parametric sequences and rendered as visual objects for practical use.(d):Illustration of many-to-one rendering characteristics. Different parametric sequences can produce identical visual objects.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19054/x2.png",
                "caption": "Figure 2:Overview of CADFusion.(a):The sequential learning stage trains LLMs using ground-truth CAD parametric sequences.(b):The visual feedback stage rewards CAD parametric sequences that render into preferred visual objects and penalizes those that do not.(c):The two stages are alternated to preserve contributions of both signals.",
                "position": 244
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19054/x3.png",
                "caption": "Figure 3:Illustration of preference data construction.(a):Sample CAD parametric sequences and render them into visual objects.(b):Score the visual objects using LVMs with multi-aspect grading criteria.(c):Construct preference data based on LVM-generated scores.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2501.19054/x4.png",
                "caption": "Figure 4:An illustrative example of the multi-aspect evaluation criteria used in LVM scoring.\nNote that the illustrations are simplified to conceptually represent each criterion.",
                "position": 389
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19054/x5.png",
                "caption": "Figure 5:Qualitative results.\nThe input prompt is shown at the top of each subsection.\nImages are arranged from left to right in the following order: ground truth, CADFusion, GPT-4o, and Text2CAD.\nOutputs that cannot be rendered are marked with a red cross.\nCADFusion outperforms all baselines in understanding instructions and generating CAD objects that are both sequentially and visually high quality.\nGPT-4o frequently produces invalid samples and pays little attention to shape details.\nText2CAD generates well-formed basic shapes with a regular appearance but struggles to accurately follow input instructions and represent complex geometries.",
                "position": 515
            }
        ]
    },
    {
        "header": "5Limitation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19054/x6.png",
                "caption": "Figure 6:An overview of the data preprocessing steps. The original dataset is transformed into captions that serve as textual inputs, while the corresponding stringified CAD representations are used as ground truth references.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2501.19054/x7.png",
                "caption": "Figure 7:An overview of multiple CAD representations and their corresponding captions. Left: A CAD representation in the raw SEM format alongside its stringified sequence, with values highlighted in different colors based on the padding used for decoding. Right: Captions generated by the LVM and refined by human annotation. Phrases removed during human fine-tuning are marked in red, while those added by humans are marked in green. All representations and captions correspond to the same CAD figure, which is displayed in the bottom-right corner.",
                "position": 1249
            }
        ]
    },
    {
        "header": "Appendix AAdditional Dataset Construction Detail",
        "images": []
    },
    {
        "header": "Appendix BAdditional Training Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.19054/x8.png",
                "caption": "Figure 8:Additional qualitative results, Part 1. The results are grouped by categories such as panels and circular objects. In each sub-figure, the left image shows the figure rendered from the ground truth, while the right image displays the generation by CADFusion. The corresponding textual instructions are provided at the bottom.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2501.19054/x9.png",
                "caption": "Figure 9:Additional qualitative results, Part 2. The results are grouped by categories such as multiple distinct items and complex shapes. In each sub-figure, the left image shows the figure rendered from the ground truth, while the right image displays the generation by CADFusion. The corresponding textual instructions are provided at the bottom.",
                "position": 1450
            },
            {
                "img": "https://arxiv.org/html/2501.19054/x10.png",
                "caption": "Figure 10:An overview of the generation of CAD instances with slight variations from a single prompt. In each sub-figure, the top-left image shows the ground truth generation, while the remaining three represent CADFusion’s outputs, which exhibit variations in thickness, width, and cutout size. The prompt is displayed at the top of each sub-figure.",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2501.19054/x11.png",
                "caption": "Figure 11:Invalid and discrepant samples. CADFusion generates invalid samples when the instructions are too complex or involve word shape knowledge, and produces discrepant outcomes when there are too many distinct items to generate or when complex merges are required to form the final CAD instance.",
                "position": 1863
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": []
    }
]