[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14033/x1.png",
                "caption": "Figure 1:Performance comparison betweenSAIL-VL2 (2B/8B/A3B)and other LVMs (both open-source and large-scale closed-source). SAIL-VL2 demonstrates strong performance across multiple dimensions.",
                "position": 118
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14033/x2.png",
                "caption": "Figure 2:Overview of the SAIL-VL2 framework.The architecture is composed of a vision encoder (SAIL-ViT) that aligns visual inputs into the representation space of the LLM. A lightweight adapter further transforms visual embeddings into tokenized representations, which are jointly processed with linguistic embeddings for multimodal reasoning and prediction. SAIL-VL2 accommodates multiple LLM backbones, ensuring flexibility and scalability across model configurations.",
                "position": 223
            }
        ]
    },
    {
        "header": "Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14033/x3.png",
                "caption": "Figure 3:Data construction pipeline for SAIL-VL2 training.High-quality multimodal corpora are constructed by curating and filtering open-source datasets, and generating synthetic data, with both components systematically organized to meet the requirements of different training stages.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2509.14033/x4.png",
                "caption": "Figure 4:Scaling curves of SAIL-VL2-2B during the multi-task pre-training stage. Results are reported on overall benchmarks, natural-scene VQA datasets, and OCR VQA tasks. ’BMK Score’ denotes the average benchmark score.",
                "position": 618
            }
        ]
    },
    {
        "header": "Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14033/x5.png",
                "caption": "Figure 5:Analysis of instruction data quality and scale on model performance. Using SAIL-VL2-2B as the base architecture after multi-task pre-training, we compare SFT results across different datasets and data scales.",
                "position": 678
            }
        ]
    },
    {
        "header": "Training Infrastructure",
        "images": []
    },
    {
        "header": "Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.14033/x6.png",
                "caption": "Figure 6:Visualization of token embedding distributions across different models and combination strategies. Each row corresponds to a model variant (Qwen3-0.6B, Qwen3-1.7B, Qwen3-8B, InternLM-1.8B), and each column illustrates a grouping strategy: combining SAIL-ViT with text embeddings, AIMv2 with text embeddings, or all three jointly. Colors and markers indicate ’SAIL-ViT’, ’AIMv2’, and ’text’ tokens, highlighting spatial relationships and overlaps between embedding spaces.",
                "position": 1291
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Contributors and Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]