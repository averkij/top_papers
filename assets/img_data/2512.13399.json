[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13399/x1.png",
                "caption": "Figure 1:Illustration ofDERLand performance of Meta-Reward.Left: Overview ofDERLversus traditional approaches. InDERL, a Meta-Optimizer generates a parameterized Meta-Reward to guide policy model evolution. Crucially, the validation performance serves as a feedback signal to update the Meta-Optimizer via policy gradients, establishing adifferentiable, closed-loopoptimization process.DERLeliminates the need for heuristic design or expensive human annotation.Right: Performance comparison of outcome reward, avg reward (i.e.,average over atomic primitives) and Meta-Reward. Our Meta-Reward consistently outperforms all baselines in different tasks, demonstrating the effectiveness ofDERL.",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Differentiable Evolution Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13399/x2.png",
                "caption": "Figure 2:Bi-level evolutionary training forDERL.Blue Block: The evolution of Meta-Optimizerœà\\psiwithnngenerated Meta-RewardsRR(i.e., rollouts). Taking a fixed task instruction as input, the Meta-Optimizer updates the parameterŒ¶\\Phiof the Meta-RewardRRwith the signal from validation performancevv.Green Block: The inner-loop training for policy modelŒ∏i\\theta_{i}with Meta-RewardRœïiR_{\\phi_{i}}by GRPO. We evaluate the validation performance for eachŒ∏i\\theta_{i}as the reward ofRœïiR_{\\phi_{i}}, making it a differentiable signal for the Meta-Optimizer to evolve through reinforcement learning.",
                "position": 272
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13399/x3.png",
                "caption": "Figure 3:Optimization dynamics on ALFWorld, GSM8k and MATH Benchmark. The plots illustrate the training trajectories of the Meta-Optimizer, where the horizontal axis represents the number of training steps in the outer-loop. The blue and orange lines represent the average validation and testing performance over Meta-Reward (i.e., ‚Äúrollouts‚Äù), respectively. The results show thatDERLoptimizes and converges as the training step increases, without overfitting.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2512.13399/x4.png",
                "caption": "",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2512.13399/x5.png",
                "caption": "",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2512.13399/x6.png",
                "caption": "",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2512.13399/x7.png",
                "caption": "Figure 4:Evolution dynamics of reward structures on ALFWorld. We visualize the proportion of Stable Structures and Unstable Structures over outer-loop steps. The consistent upward trend of stable structures demonstrates the Meta-Optimizer‚Äôs selection preference for mathematical robustness.",
                "position": 700
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix APreliminary Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13399/x8.png",
                "caption": "Figure 5:Demonstration of the training loop our differentiable evolutionary reward. We adopt GRPO as an example RL algorithm for the inner-loop. In the outer-loop, we leverage a graph neural network to represent general computational graphs and obtain the final reward functionŒ¶t\\Phi_{t}parameterized byœït={wa‚Äãd‚Äãd,ws‚Äãu‚Äãb,‚Ä¶}\\phi_{t}=\\{w_{add},w_{sub},...\\}through differentiable optimization.",
                "position": 777
            }
        ]
    },
    {
        "header": "Appendix BGradient Propagation onDERL",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13399/",
                "caption": "Figure 6:Illustration of Gradient Propagation in bi-level evolutionary training loop.\nThe top row (Orange) represents the trajectory of the optimizeeŒ∏\\theta, updated via instructionsœï\\phiderived from the optimizer.\nThe bottom row (Green) represents the evolution of the Meta-Optimizerœà\\psi.\nThe blue nodesvtv_{t}denote the validation performance evaluation.\nUnlike static methods, our framework computes the meta-gradient‚àáùí•outer‚Äã(œàt)\\nabla\\mathcal{J}^{\\text{outer}}(\\psi_{t})(vertical arrows), allowing the optimizer to update its own parametersœà\\psito explicitly maximize the optimizee‚Äôs performance.",
                "position": 858
            }
        ]
    },
    {
        "header": "Appendix CComputational Cost Analysis",
        "images": []
    },
    {
        "header": "Appendix DTraining Dynamics ofDERL-pop.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13399/x10.png",
                "caption": "Figure 7:Training dynamics ofDERL-population. We present the training dynamics ofDERL-pop. and GRPO w/ Avg Reward to demonstrate the superiority of the population method.",
                "position": 1001
            }
        ]
    },
    {
        "header": "Appendix EExamples of Outer-loop Evolution",
        "images": []
    }
]