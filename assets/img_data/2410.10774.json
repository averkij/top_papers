[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10774/x1.png",
                "caption": "Figure 1:An overview of Cavia is shown in (a). We introduce view-integrated attention modules, namely cross-view attentions and cross-frame attentions, which enforce viewpoint and temporal consistency of the generated frames, respectively. As illustrated in (b) and (c), our view-integrated attention incorporates additional feature dimensions into the attention mechanism, enhancing consistency across views and frames.",
                "position": 208
            }
        ]
    },
    {
        "header": "4Joint Training Strategy on Curated Data Mixtures",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10774/x2.png",
                "caption": "Figure 2:Statistics of the (a) point cloud size, (b) aesthetic score, and (c) camera motion classification result for our monocular video dataset.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2410.10774/extracted/5924086/figures/data.png",
                "caption": "Figure 3:Sources of our training videos.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2410.10774/x3.png",
                "caption": "Figure 4:Per-video qualitative comparisons. The first frame in each reference set is the input image. Neither the image nor the camera trajectories were seen during model training. Video results are provided in supplementary for clearer qualitative comparisons.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10774/x4.png",
                "caption": "Figure 5:Qualitative comparisons for 2-view video generations. Each generation consists of two rows, where each row represents a sequence of generated frames, with columns showing frames at the same timestep. Neither the image nor the camera trajectories were used during model training.Reddotted lines are annotated to highlight object motion. Video results are included in the supplementary material for clearer comparisons.",
                "position": 386
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Data Curation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10774/x5.png",
                "caption": "Figure 6:Ablation studies on Plücker coordinates and Cross-frame Attention. Video results are provided in supplementary for clearer qualitative comparisons.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2410.10774/x6.png",
                "caption": "Figure 7:Ablation studies on Cross-view Attention. Video results are provided in supplementary for clearer qualitative comparisons.",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2410.10774/x7.png",
                "caption": "Figure 8:Ablation studies on the joint training strategy on monocular videos. Video results are provided in supplementary for clearer qualitative comparisons.",
                "position": 1860
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10774/x8.png",
                "caption": "Figure 9:Four-view video comparison. The result of CVD is taken from their website. CVD tends to generate black border pixels, potentially due to its homography warping augmentations during training. In comparison, our method produces frames with better geometric consistency and perceptual quality.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2410.10774/x9.png",
                "caption": "Figure 10:3D Reconstruction comparison. We render the reconstructed 3D Gaussians from an elliptical trajectory consisting of 16 novel views. The result of CVD is taken from their website. CVD’s reconstruction results suffer from floaters and blurry artifacts due to the inconsistency in their generated frames. In comparison, our method produces sharper results with clearer visual quality.",
                "position": 1891
            }
        ]
    },
    {
        "header": "Appendix EApplications",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    }
]