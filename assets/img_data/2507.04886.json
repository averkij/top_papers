[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1   INTRODUCTION",
        "images": []
    },
    {
        "header": "2   RELATED WORK",
        "images": []
    },
    {
        "header": "3   METHOD",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image2.png",
                "caption": "Figure 1:Average characters per token. Lower values indicate lower compression efficiency. Our bvv241 variants prioritize character-level granularity.",
                "position": 239
            }
        ]
    },
    {
        "header": "4   EXPERIMENTAL SETUP",
        "images": []
    },
    {
        "header": "5   RESULTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image3.png",
                "caption": "Figure 2:Learning curves (best_bvv_ru) for frozen embedding.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image4.png",
                "caption": "Figure 3:Learning curves (best_bvv_unfrozen_ru) for trainable embedding.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image5.png",
                "caption": "Figure 4:Performance Comparison: MMLU sub tasks with score greater than 27%.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image6.png",
                "caption": "Figure 5:Performance Comparison: Frozen vs. Trainable Embedding Models.",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image7.png",
                "caption": "Figure 6:t-SNE visualization of input token embeddings. In the trainable Mistral Nemo embeddings, semantic groups (e.g., numbers, professions, animals) tend to lie closer to one another, yet, globally, the point cloud remains mostly uniform.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2507.04886/extracted/6588038/image8.png",
                "caption": "Figure 7:t-SNE visualization of input token embeddings. In our frozen, visual Unicode embeddings (bvv241-nemo), clusters are sharp and distinct, but reflect only surface or formal properties (such as token length), with no alignment to the semantic categories. This highlights that emergent meaning in our models must arise above the embedding layer.",
                "position": 429
            }
        ]
    },
    {
        "header": "6   DISCUSSION",
        "images": []
    },
    {
        "header": "7   CONCLUSION",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]