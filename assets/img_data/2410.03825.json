[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03825/x1.png",
                "caption": "Figure 1:MonST3Rprocesses a dynamic video to produce a time-varying dynamic point cloud, along with per-frame camera poses and intrinsics, in a predominantly feed-forward manner. This representation then enables the efficient computation of downstream tasks, such as video depth estimation and dynamic/static scene segmentation.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03825/",
                "caption": "Figure 2:Limitation of DUSt3R on dynamic scenes.Left: DUSt3R aligns the moving foreground subject and misaligns the background points as it is only trained on static scenes. Right: DUSt3R fails to estimate the depth of a foreground subject, placing it in the background.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03825/x3.png",
                "caption": "Figure 3:Dynamic global point cloud and camera pose estimation.Given a fixed sized of temporal window, we compute pairwise pointmap for each frame pair with MonST3R and optical flow from off-the-shelf method. These intermediates then serve as inputs to optimize a global point cloud and per-frame camera poses. Video depth can be directly derived from this unified representation.",
                "position": 782
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03825/x4.png",
                "caption": "Figure 4:Qualitative comparison.Compared to CasualSAM and DUSt3R, our method outputs both reliable camera trajectories and geometry of dynamic scenes. Refer toFig.A5for more results.",
                "position": 1470
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03825/x5.png",
                "caption": "Figure A1:Video depth estimation comparison on Bonn dataset.Evaluation protocol is per-sequence scale & shift. We visualize the prediction resultafteralignment.\nNote, in the first row, our depth estimation is more aligned with the GT depth (e.g., the wall) compared to DepthCrafter‚Äôs.",
                "position": 2626
            },
            {
                "img": "https://arxiv.org/html/2410.03825/x6.png",
                "caption": "Figure A2:Video depth estimation comparison on KITTI dataset.Evaluation protocol is per-sequence scale & shift.\nFor each case, the upper row is for input frame and depth prediction; the lower row is for ground truth depth annotation and error map. Prediction result isafteralignment.",
                "position": 2631
            },
            {
                "img": "https://arxiv.org/html/2410.03825/x7.png",
                "caption": "Figure A3:Camera pose estimation comparison on the Sintel dataset.The trajectories are plotted along the two axes with the highest variance to capture the most significant motion. The predicted trajectory (solid blue line) is aligned to match the ground truth trajectory (dashed gray line). Our MonST3R is more robust at challenging scenes, ‚Äútemple_3‚Äù and ‚Äúcave_2‚Äù (the last two rows).",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2410.03825/x8.png",
                "caption": "Figure A4:Camera pose estimation comparison on the Scannet dataset.The trajectories are plotted along the two axes with the highest variance to capture the most significant motion.",
                "position": 2648
            },
            {
                "img": "https://arxiv.org/html/2410.03825/x9.png",
                "caption": "Figure A5:Qualitative comparison on Davis.Compared to CasualSAM and DUSt3R, our method outputs both reliable camera trajectories and geometry of dynamic scenes.",
                "position": 2661
            },
            {
                "img": "https://arxiv.org/html/2410.03825/x10.png",
                "caption": "Figure A6:Qualitative comparison of feed-forward pairwise pointmaps prediction.",
                "position": 2672
            },
            {
                "img": "https://arxiv.org/html/2410.03825/x11.png",
                "caption": "Figure A7:Qualitative comparison of static/dynamic mask.We visualize both the continuous error map (upper row) and binary static/dynamic mask (lower row). The thresholdŒ±ùõº\\alphaitalic_Œ±is fixed.",
                "position": 2684
            }
        ]
    },
    {
        "header": "Appendix BMore quantitative results",
        "images": []
    }
]