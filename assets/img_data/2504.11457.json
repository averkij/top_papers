[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11457/x1.png",
                "caption": "Figure 1:We demonstrate the gaps between a generative denoising process and perception tasks using referring image segmentation (RIS), where the diffusion model learns to color the referred object with red masks.(a)(b)The perception quality (Intersection-over-Union, IoU) at intermediate denoising steps, which come from the same denoising trajectory, reveals theuneven contribution of timestepsandtraining-denoising distribution shift, addressed by our enhancedlearning objectiveandtraining data.(c)We discover that the generative denoising process is also a uniqueuser interfacefor discriminative perception, because of its capabilities tointeract with the correctional guidancefrom users or foundation models.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11457/x2.png",
                "caption": "Figure 2:Method overview. We align the generative diffusion models with perception tasks fromlearning objective,training data, anduser interface. Notations follow DDPM(Ho et¬†al.,2020).",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x3.png",
                "caption": "Figure 3:Evolution ofŒ¥1subscriptùõø1\\delta_{1}italic_Œ¥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(intuitively the ‚Äúaccuracy‚Äù for depth estimation) from Marigold(Ke et¬†al.,2024)shows smoother patterns than RIS. We copy the RIS curve from Fig.1ahere for easier comparison.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x4.png",
                "caption": "Figure 4:Data augmentation of(a)Gaussian blurring for depth estimation, and(b)color / shape / location for RIS. We use large / small intensities of augmentations to simulate different scales of distribution shifts at the earlier / later steps of denoising.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x5.png",
                "caption": "Figure 5:Interacting with correctional guidanceD‚àísuperscriptùê∑D^{-}italic_D start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT.",
                "position": 299
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11457/x6.png",
                "caption": "Figure 6:Interactive interface enables diffusion models to adaptively correct their predictions via language models‚Äô guidanceD‚àísuperscriptùê∑D^{-}italic_D start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT. Such capabilities ofprogressivenessare beyond conventional discriminative models and are an emerging advantage of the generative denoising process in perception.",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x7.png",
                "caption": "Figure 7:IoU-Timestep curves. Our data augmentation decreases the training-denoising distribution shifts.",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x8.png",
                "caption": "Figure 8:Augmentation Intensity.",
                "position": 886
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11457/x9.png",
                "caption": "Figure A:Comparison of the sampling weights derived from normalizedct2superscriptsubscriptùëêùë°2c_{t}^{2}italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, estimated by diffusion formulation or RIS/Marigold/Generalist perception statistics (Sec.3.1). The dashed blue line denotes the probability of uniform sampling from one of the ten timestamp groups.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x10.png",
                "caption": "Figure B:Our workflow of generating correctional prompts shows the advantage of the interactivity of diffusion-based perception.",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x11.png",
                "caption": "Figure C:Comparison of mask encoding methods for RIS. Note that ground-truth masks are used here for demonstration purposes.",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x12.png",
                "caption": "Figure D:Comparison of post-processing. The solid red mask simplifies the post-processing for evaluation, which also encourages us to adopt it as the RIS format.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2504.11457/x13.png",
                "caption": "Figure E:Observations with different sampling steps.",
                "position": 2612
            }
        ]
    },
    {
        "header": "Appendix BAdditional Ablation Studies",
        "images": []
    }
]