[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06888/x1.png",
                "caption": "Figure 1:Our inverse temperature sampling ratio throughout training from Fineweb2 data, fromÏ„\\tauof 0.7 to 0.5 to 0.3. Note that other sources are excluded from this chart and are not temperature sampled. For full language percent details, see AppendixD. Our training starts out more high-resource biased but becomes increasingly uniform. We also include another 50 languages in mid-training and 1723 more for the decay phase not visualized.",
                "position": 378
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06888/x2.png",
                "caption": "Figure 2:Performance of models using different decay phases on two languages (Tigray and Faroese) only added during the decay phase. We see thatmmBERTwith the 1833 language decay phase shows rapid performance improvements despite only having the models in the last 100B tokens of training. The finalmmBERTmodels shows improvements by merging together checkpoints.",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2509.06888/x3.png",
                "caption": "Figure 3:Throughput efficiency for various sequence lengths and variable input length (top row is small models, bottom row is base models).mmBERTis more efficient because of the use of Flash Attention 2 and unpadding techniques it inherits from ModernBERT. Empty bars indicate that the model cannot use a sequence length greater than 512. Error bars show standard error over five seeds.",
                "position": 1288
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArchitecture Details",
        "images": []
    },
    {
        "header": "Appendix BHyperparameter and Compute Details",
        "images": []
    },
    {
        "header": "Appendix CComparison with DeBERTa",
        "images": []
    },
    {
        "header": "Appendix DLanguage Distributions",
        "images": []
    }
]