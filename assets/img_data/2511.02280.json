[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02280/x1.png",
                "caption": "Figure 1:Performance comparison between SAIL-VL2-Thinking (SAIL-VL2 post-trained with our SAIL-RL) and other LVMs.SAIL-VL2-Thinking achieves clear advantages on both general understanding and mathematical reasoning benchmarks, surpassing open-source baselines at the 8B scale and delivering competitive performance against large-scale closed-source models.",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02280/x2.png",
                "caption": "Figure 2:Limitations of current MLLMs in reasoning.Left:Lucky success where the model reaches the correct answer through a flawed reasoning process.Right:Overthinking where the model applies a needlessly complex reasoning process to a simple problem, resulting in an incorrect answer.",
                "position": 140
            }
        ]
    },
    {
        "header": "4SAIL-RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02280/x3.png",
                "caption": "Figure 3:An overview of the SAIL-RL’s multi-dimensional reward system. The system evaluates a model’s response across four dimensions: Format, Answer, Thinking, and Judging. The nuanced semantic rewards for Thinking and Judging are provided by Gemini acting as a reward-judger.",
                "position": 228
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02280/x4.png",
                "caption": "Figure 4:Evaluation results on thinking trigger.",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2511.02280/x5.png",
                "caption": "Figure 5:Ablation on training dynamics of thinking reward.Our method (blue) consistently improves all three thinking score over the answer-only baseline (orange), which stagnates or degrades.",
                "position": 877
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02280/x6.png",
                "caption": "Figure 6:Visualizing behavior on an OCR task under two different reasoning strategies.Orange: The output from a baseline that is forced to think.Blue: The output from our model guided by the proposed Judge Reward, which dynamically decides when to think.",
                "position": 1510
            },
            {
                "img": "https://arxiv.org/html/2511.02280/x7.png",
                "caption": "Figure 7:Visualizing reasoning on a math problem under two different reward systems.Orange: The output from a baseline trained with an answer-only reward.Blue: The output from our model trained with the proposed Thinking Quality Reward.",
                "position": 1513
            }
        ]
    },
    {
        "header": "Appendix BReward Prompt",
        "images": []
    }
]