[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11650/figures/teaser_renders/18_render_diamid.jpg",
                "caption": "Figure 1.Usingtext description,explicit body shape,cloth imageas input, our 3D human generative method,InfiniHuman, can automatically create a variety of realistic 3D humans with high-fidelity texture and geometry. Our InfiniHuman allows for generating infinite 3D humans with precise user control.",
                "position": 144
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11650/x1.png",
                "caption": "Figure 2.Examples from InfiniHumanData.a)Diverse human identities covering a wide range of ethnicities, age groups (including children), clothing styles, hair types, and skin tones, which are visually indistinguishable from real scans rendering (Sec.4.2).b)Multi-modal annotations per each subject, including I) multi-view RGB images (full-body and head), II) SMPL parameters, III) clothing asset images, and IV) multi-granularity text descriptions.",
                "position": 150
            }
        ]
    },
    {
        "header": "2.Related work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11650/x2.png",
                "caption": "Figure 3.Overview of data generation frameworkinInfiniHumanData. The process is fully automated by leveraging foundation models. Desired outputs are marked with flags:A)Structured text descriptions,C)Clothing style images,E)Body shape in SMPL format plus face and hand keypoints,F)Orthographic multi-view images with controlled lighting conditions suitable for 3D lifting.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x3.png",
                "caption": "Figure 4.Overview of a) Gen-Schnell and b) Gen-HRes in InfiniHumanGen. Taking text description, explicit SMPL shape, and a cloth image as input, Gen-Schnell generates 3D-GS end-to-end, while Gen-HRes generates high-resolution textured mesh, both matched to input conditions.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x4.png",
                "caption": "Figure 5.Fine-grained text controllability in Gen-HResover (a) overall subject identity, such as ethnicity, age, gender, etc. By fixing the initial Gaussian noise, Gen-HRes can generate (b) same identity with different detailed accessory appearance, such as watch, glasses, and colors of wearing assets.",
                "position": 498
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11650/x5.png",
                "caption": "Figure 6.Generate avatars with given garment from fashion industry. The identity is preserved while TryOn garment is changing.",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x6.png",
                "caption": "Figure 7.Generate avatars with precise pose shape control and text-based editing. The identity is preserved during shape and text-based editing.",
                "position": 649
            }
        ]
    },
    {
        "header": "5.Limitations and Future Works",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11650/x7.png",
                "caption": "Figure 8.Qualitative comparison to SOTA text-to-3D avatar generators.We compare with SDS-based avatar generation methods and a mesh-based avatar generation method Chupa(Kim et al.,2023). Our generator can follow the text very well and also achieve outstanding generation quality.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x7.png",
                "caption": "Figure 8.Qualitative comparison to SOTA text-to-3D avatar generators.We compare with SDS-based avatar generation methods and a mesh-based avatar generation method Chupa(Kim et al.,2023). Our generator can follow the text very well and also achieve outstanding generation quality.",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x8.png",
                "caption": "Figure 9.Generated famous people and characters by names in Gen-HRes. Please zoom in for details.",
                "position": 780
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x9.png",
                "caption": "Figure 10.Generated avatars with garments in real person photos. We can extract clean garments from photos and use for TryOn.Images from Shutterstock.",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x10.png",
                "caption": "Figure 11.Qualitative comparison to SOTA text-to-3D avatar approaches.Gen-HRes avoids Janus/artifacts, aligns to prompts, and is8×8\\timesfaster (Tab.2).",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x11.png",
                "caption": "Figure 12.Qualitative appearance and geometry comparison to SOTA text-to-3D avatar approaches. Please refer to Supp. Mat. for more comparisons.",
                "position": 800
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x12.png",
                "caption": "Figure 13.Re-animation (left) and Fabrication (right) of Gen-HRes avatars.",
                "position": 803
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x12.png",
                "caption": "Figure 13.Re-animation (left) and Fabrication (right) of Gen-HRes avatars.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x13.png",
                "caption": "Figure 14.Misaligned joints cause bad face generation (left). Our pipeline tolerates bad SMPL estimation for children, yielding good multi-views (right).",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x14.png",
                "caption": "Figure 15.Orthographic and Perspective in Multi-View Attention (left). Org. FLUX gives complex lighting, degrading multi-view generation (right).",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x14.png",
                "caption": "Figure 15.Orthographic and Perspective in Multi-View Attention (left). Org. FLUX gives complex lighting, degrading multi-view generation (right).",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x15.png",
                "caption": "Figure 16.Our finetuned FLUX can generate desired images from text prompt with orthographic view and uniform lighting, similar to the scan rendering.",
                "position": 821
            }
        ]
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11650/x16.png",
                "caption": "Figure 17.Precise control capability in InfiniHumanGen.",
                "position": 1645
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x17.png",
                "caption": "Figure 18.Additional Examples in InfiniHumanData. Our orthographic multi-view diffusion model generates high-quality view-consistent images of head views and full-body view.",
                "position": 1648
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x18.png",
                "caption": "Figure 19.Qualitative appearance and geometry comparison to SDS-based text-to-3D avatar approaches.",
                "position": 1651
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x19.png",
                "caption": "",
                "position": 1655
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x20.png",
                "caption": "Figure 20.Qualitative appearance and geometry comparison to feed-forward text-to-3D avatar approaches.",
                "position": 1659
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x21.png",
                "caption": "",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x22.png",
                "caption": "Figure 21.Qualitative comparison with IDOL Dataset. Our InfiniHumanData achieves better visual realism and multi-view consistency.",
                "position": 1667
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x23.png",
                "caption": "Figure 22.Detailed scan captioning prompt and example output.",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x24.png",
                "caption": "Figure 23.Multi-granularity text summarization prompt and generated examples.",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x25.png",
                "caption": "Figure 24.Garment selection prompt for negative samples rejection.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x26.png",
                "caption": "Figure 25.Pose refinement using reprojection loss w.r.t. OpenPose 2D joints.As stated in Sec.3.1 E in main paper, we use OpenPose 2D joints to further refine body pose parameters using reprojection loss. This additional operation eliminates the misalignment between image and SMPL body, and enhances the pixel-level accuracy in face and hand region.",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x27.png",
                "caption": "Figure 26.Comparison between generated multi-view images in our model and PSHuman.Both models generate high-resolution full-body images. However, our model generates multi-view high-resolution head-centric images, while PSHuman only generates a single blurry head-view image.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2510.11650/x28.png",
                "caption": "Figure 27.User study for realism assessment of InfiniHumanData.We present randomly paired InfiniHumanData subjects and scan subjects to participants. InfiniHumanData receives 765 votes over 746 votes for scan, which demonstrates the superior realism of our synthetic dataset.",
                "position": 1685
            }
        ]
    },
    {
        "header": "Appendix AAdditional Visualitation",
        "images": []
    },
    {
        "header": "Appendix BInfiniHumanData Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CInfiniHumanGen Implementation Details",
        "images": []
    }
]