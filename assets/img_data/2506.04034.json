[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04034/x1.png",
                "caption": "Figure 1:An example of Rex-Thinker for object referring with CoT reasoning of planning (task decomposition), action (evaluating each candidate), and summarization (final decision). Each step is grounded in a specific hint box (as denoted in the left image), enabling interpretable predictions.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Chain-of-Thought Reasoning Referring Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04034/x2.png",
                "caption": "Figure 2:Overview of the proposed CoT reasoning referring data engine. We prompt GPT-4o to generate a three-step CoT reasoning process, including planning, action, and summarization.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04034/x3.png",
                "caption": "Figure 3:Overview of the Rex-Thinker architecture and our two-stage training methods",
                "position": 292
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04034/x4.png",
                "caption": "Figure 4:The out-of-domain result. We use Rex-Thinker-GPRO trained on HumanRef-CoT to infer an unseen category (i.e., fish), resulting in a strong generalization. Boxes in the image denote hints.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x5.png",
                "caption": "Table 3:Out-of-domain evaluation results on RefCOCOg.∗Fine-tuned on RefCOCOg using GRPO.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x5.png",
                "caption": "Figure 5:Predictions from a model that was trained with GRPO only, without CoT-based supervised fine-tuning as cold-start initialization. Boxes in the image denote answers.",
                "position": 869
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04034/x6.png",
                "caption": "Figure 6:The system prompt used to instruct GPT-4o on visual reasoning for HumanRef-CoT. It specifies output format, reasoning steps, symbol conventions, and the expected alignment between intermediate analysis and final answers.",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x7.png",
                "caption": "Figure 7:GRPO training curves showing accuracy reward, format reward, and completion length over time.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x8.png",
                "caption": "Figure 8:Example of reasoning–answer mismatch. The number of predicted objects differs between reasoning and the final output.",
                "position": 2502
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x9.png",
                "caption": "Figure 9:Attribute referring example.",
                "position": 2526
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x10.png",
                "caption": "Figure 10:Attribute referring example.",
                "position": 2529
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x11.png",
                "caption": "Figure 11:Attribute referring example.",
                "position": 2532
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x12.png",
                "caption": "Figure 12:Attribute referring example.",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x13.png",
                "caption": "Figure 13:Attribute referring example.",
                "position": 2538
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x14.png",
                "caption": "Figure 14:Attribute referring example.",
                "position": 2541
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x15.png",
                "caption": "Figure 15:Interaction referring example.",
                "position": 2544
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x16.png",
                "caption": "Figure 16:Interaction referring example.",
                "position": 2547
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x17.png",
                "caption": "Figure 17:Interaction referring example.",
                "position": 2550
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x18.png",
                "caption": "Figure 18:Interaction referring example.",
                "position": 2553
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x19.png",
                "caption": "Figure 19:Position referring example.",
                "position": 2556
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x20.png",
                "caption": "Figure 20:Position referring example.",
                "position": 2559
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x21.png",
                "caption": "Figure 21:Position referring example.",
                "position": 2562
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x22.png",
                "caption": "Figure 22:Celebrity referring example.",
                "position": 2565
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x23.png",
                "caption": "Figure 23:Reasoning referring example.",
                "position": 2568
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x24.png",
                "caption": "Figure 24:Reasoning referring example.",
                "position": 2571
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x25.png",
                "caption": "Figure 25:Rejection referring example.",
                "position": 2574
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x26.png",
                "caption": "Figure 26:Rejection referring example.",
                "position": 2577
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x27.png",
                "caption": "Figure 27:Rejection referring example.",
                "position": 2580
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x28.png",
                "caption": "Figure 28:Reasoning referring example with multi-task chat.",
                "position": 2583
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x29.png",
                "caption": "Figure 29:In-context prompt forattributesubset in HumanRef-CoT.",
                "position": 2586
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x30.png",
                "caption": "Figure 30:Visualization of GPT-4o’s output on theattributesubset.",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x31.png",
                "caption": "Figure 31:In-context prompt forposition (inner)subset in HumanRef-CoT.",
                "position": 2592
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x32.png",
                "caption": "Figure 32:Visualization of GPT-4o’s output on theposition (inner)subset.",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x33.png",
                "caption": "Figure 33:In-context prompt forposition (outer)subset in HumanRef-CoT.",
                "position": 2598
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x34.png",
                "caption": "Figure 34:Visualization of GPT-4o’s output on theposition (outer)subset.",
                "position": 2601
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x35.png",
                "caption": "Figure 35:In-context prompt forinteraction (inner)subset in HumanRef-CoT.",
                "position": 2604
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x36.png",
                "caption": "Figure 36:Visualization of GPT-4o’s output on theinteraction (inner)subset.",
                "position": 2607
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x37.png",
                "caption": "Figure 37:In-context prompt forinteraction (outer)subset in HumanRef-CoT.",
                "position": 2610
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x38.png",
                "caption": "Figure 38:Visualization of GPT-4o’s output on theinteraction (outer)subset.",
                "position": 2613
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x39.png",
                "caption": "Figure 39:In-context prompt forreasoning (inner position)subset in HumanRef-CoT.",
                "position": 2616
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x40.png",
                "caption": "Figure 40:Visualization of GPT-4o’s output on thereasoning (inner position)subset.",
                "position": 2619
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x41.png",
                "caption": "Figure 41:In-context prompt forreasoning (attribute)subset in HumanRef-CoT.",
                "position": 2629
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x42.png",
                "caption": "Figure 42:Visualization of GPT-4o’s output on thereasoning (attribute)",
                "position": 2632
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x43.png",
                "caption": "Figure 43:In-context prompt forcelebrity recognitionsubset in HumanRef-CoT.",
                "position": 2642
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x44.png",
                "caption": "Figure 44:Visualization of GPT-4o’s output on thecelebrity recognition",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x45.png",
                "caption": "Figure 45:In-context prompt forrejectionsubset in HumanRef-CoT.",
                "position": 2655
            },
            {
                "img": "https://arxiv.org/html/2506.04034/x46.png",
                "caption": "Figure 46:Visualization of GPT-4o’s output on therejection",
                "position": 2658
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]