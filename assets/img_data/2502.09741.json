[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/x1.png",
                "caption": "Figure 1:(a) We extract all the numbers from the input sequence.\n(b) For each number, we use FoNE to directly map the number to its embedding. The first two entries in the embedding represent18mod10modulo181018\\bmod 1018 roman_mod 10, while the next two entries represent18mod100modulo1810018\\bmod 10018 roman_mod 100.\n(c) We pad the FoNE with zeros, add it to the word embeddings, and then feed the combined embeddings into the model.\n(d) For each digit, we take every two entries from the last hidden state and find the number whose representation is closest to these two entries.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Empirical Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddecimaldataacc.png",
                "caption": "((a))6-digit decimal addition: Acc. vs. Training Data Size",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddecimaldataacc.png",
                "caption": "((a))6-digit decimal addition: Acc. vs. Training Data Size",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddecimalmodelacc.png",
                "caption": "((b))6-digit decimal addition: Acc. vs. Model Size",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add1dataacc_small.png",
                "caption": "((a))6-digit integer addition: Model&Data size vs. Acc.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add1dataacc_small.png",
                "caption": "((a))6-digit integer addition: Model&Data size vs. Acc.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add1modelacc_small.png",
                "caption": "",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/sub1dataacc_small.png",
                "caption": "((b))5-digit integer subtraction: Model&Data size vs. Acc.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/sub1modelacc_small.png",
                "caption": "",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/mul1dataacc_small.png",
                "caption": "((c))3-digit integer multiplication: Model&Data size vs. Acc.",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/mul1modelacc_small.png",
                "caption": "",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/mul2dataacc_small.png",
                "caption": "((d))4-digit integer multiplication: Model&Data size vs. Acc.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/mul2modelacc_small.png",
                "caption": "",
                "position": 619
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/abacus60.png",
                "caption": "((a))Test accuracy of 60-digit addition with FoNE",
                "position": 907
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/abacus60.png",
                "caption": "((a))Test accuracy of 60-digit addition with FoNE",
                "position": 910
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/lengendiff.png",
                "caption": "((b))Impact of combining FoNE with Abacus embedding",
                "position": 915
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFourier Number Final Loss & Prediction",
        "images": []
    },
    {
        "header": "Appendix BFoNE on Binary Classification Task",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/classdataacc.png",
                "caption": "((a))Accuracy vs. Training Data Size",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/classdataacc.png",
                "caption": "((a))Accuracy vs. Training Data Size",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/classmodelacc.png",
                "caption": "((b))Accuracy vs. Model Size",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/class2dataacc.png",
                "caption": "((a))Accuracy vs. Training Data Size",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/class2dataacc.png",
                "caption": "((a))Accuracy vs. Training Data Size",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/class2modelacc.png",
                "caption": "((b))Accuracy vs. Model Size",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix CPreliminaries and Missing Proof",
        "images": []
    },
    {
        "header": "Appendix DMore evidence about Fourier features",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/pretrained_embeddings/pythia_number_embedding_checkpoints.png",
                "caption": "Figure 7:Fourier analysis of the Pythia model’s number embeddings across pre-training checkpoints. The figure illustrates how the Fourier features are progressively learned during pre-training, showing the emergence of specific frequency components. Models of varying sizes exhibit a similar trend, gradually learning the same frequency components over time.",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/pretrained_embeddings/pythia_number_embedding_last.png",
                "caption": "((a))pre-trained Pythia",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/pretrained_embeddings/pythia_number_embedding_last.png",
                "caption": "((a))pre-trained Pythia",
                "position": 2140
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/pretrained_embeddings/llama3.2.png",
                "caption": "((b))fine-tuned Llama3.2",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/pretrained_embeddings/opt-1.3b.png",
                "caption": "((c))pre-trained OPT",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/pretrained_embeddings/gpt2-large.png",
                "caption": "((d))pre-trained GPT2",
                "position": 2156
            }
        ]
    },
    {
        "header": "Appendix EFoNE for 60-digit Integer Addition in One Forward Pass",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/6060addition.png",
                "caption": "Figure 9:Accuracy of an 8-layer transformer on 60-digit addition tasks, illustrating the effectiveness of FoNE embeddings in handling long sequences. The model achieves an average accuracy of 97.42% across different operand lengths, showcasing its capability in numerical precision and sequence representation.",
                "position": 2170
            }
        ]
    },
    {
        "header": "Appendix FCombine FoNE with Abacus",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/diff1.png",
                "caption": "Figure 10:Heatmaps of accuracy percentages for “FoNE+Abacus” (left column) and “Abacus” (right column) across three different random seeds. Each heatmap represents accuracy as a function of the first and second number lengths, with lighter blue shades indicating higher accuracy. The color scale ranges from white (low accuracy) to blue (high accuracy). These visualizations highlight FoNE can combine with Abacus to improve performance.",
                "position": 2197
            }
        ]
    },
    {
        "header": "Appendix GExperiment Setting",
        "images": []
    },
    {
        "header": "Appendix HSimilar Results on GPT2-Large Based Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add2datagpt.png",
                "caption": "((a))6-digit decimal addition: Accuracy vs. Training Data Size",
                "position": 2475
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add2datagpt.png",
                "caption": "((a))6-digit decimal addition: Accuracy vs. Training Data Size",
                "position": 2478
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add2modelgpt.png",
                "caption": "((b))6-digit decimal addition: Accuracy vs. Model Size",
                "position": 2483
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddecimaldatar2.png",
                "caption": "((a))Data size vs. Accuracy",
                "position": 2510
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddecimaldatar2.png",
                "caption": "((a))Data size vs. Accuracy",
                "position": 2513
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddecimalmodelr2.png",
                "caption": "((b))Model size vs. Accuracy",
                "position": 2518
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add2datar2.png",
                "caption": "((a))6-digit integer addition: Model&Data size vs. Accuracy",
                "position": 2527
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add2datar2.png",
                "caption": "((a))6-digit integer addition: Model&Data size vs. Accuracy",
                "position": 2530
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/add2modelr2.png",
                "caption": "",
                "position": 2532
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/adddatar2.png",
                "caption": "((b))5-digit integer addition: Model&Data size vs. Accuracy",
                "position": 2538
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/addmodelr2.png",
                "caption": "",
                "position": 2540
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/subdatar2.png",
                "caption": "((c))5-digit integer subtraction: Model&Data size vs. Accuracy",
                "position": 2547
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/submodelr2.png",
                "caption": "",
                "position": 2549
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/muldatar2.png",
                "caption": "((d))3-digit integer multiplication: Model&Data size vs. Accuracy",
                "position": 2555
            },
            {
                "img": "https://arxiv.org/html/2502.09741/extracted/6199122/figures/results/mulmodelr2.png",
                "caption": "",
                "position": 2557
            }
        ]
    },
    {
        "header": "Appendix IR2superscript𝑅2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTcomparison",
        "images": []
    }
]