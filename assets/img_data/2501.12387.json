[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12387/x1.png",
                "caption": "Figure 1:Continuous 3D Perception.Given a stream of RGB images as input, our approach enables dense 3D reconstruction in an online, continuous manner, estimating both camera parameters and dense 3D geometry with each incoming frame. Our framework supports various 3D tasks, processes inputs from video sequences and sparse photo collections, and can handle both static and dynamic scenes.",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12387/x2.png",
                "caption": "Figure 2:Querying Unseen Regions.In addition to reconstructing a scene from images, our method can also infer structure forunseenparts of the scene, given a virtual camera query (shown in blue).",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12387/x3.png",
                "caption": "Figure 3:Method Overview. Our method performs online dense 3D reconstruction from a stream of images (video frames or a photo collection) by using a persistent state. Each input image is encoded into visual tokens via a shared-weight ViT encoder. These tokens interact with state tokens, wherestate updateintegrates the current image into the state, andstate readoutretrieves the past context stored in the state for predictions. Both processes occur simultaneously through two interconnected ViT decoders. Outputs include pointmaps in world and camera frames (only world pointmaps are shown) and the camera-to-world transformation. On the right, we demonstrate our methodâ€™s ability to predict unseen views: given a query camera (as a raymap), it reads information from the state to predict its corresponding pointmap, even for unobserved regions. For these readouts, we do not update the state. The hallucinated pointmap is highlighted with a blue border.",
                "position": 148
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12387/x4.png",
                "caption": "Figure 4:Qualitative Results on In-the-wild Internet Videos.We compare our method with concurrent works Spann3R[101]and MonST3R[125]. Our method achieves the best qualitative results.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2501.12387/x5.png",
                "caption": "Figure 5:State Update Analysis.Compared to online, revisiting incorporates global context which improves overall reconstruction results, especially in the highlighted regions.",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2501.12387/x6.png",
                "caption": "Figure 6:Inferring New Structure via. State Readout. From top to bottom: the input image; the ground truth (GT) image, used to query the state via its camera parameters (note: GT image is not given to the model); the depth map from the predicted pointmap;\nthe pointmap prediction of the input image alone; and the pointmap combined with the predicted pointmap in a shared coordinate frame.",
                "position": 1696
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining Datasets",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Comparisons",
        "images": []
    }
]