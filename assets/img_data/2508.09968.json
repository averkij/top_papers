[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09968/x1.png",
                "caption": "Figure 1:The same initial random noise is used for the base generation and the initialization of noise hypernetwork.HyperNoisesignificantly improves upon the initially generated image with respect to both prompt faithfulness and aesthetic quality for both SANA-Sprint and FLUX-Schnell.",
                "position": 209
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09968/x2.png",
                "caption": "Figure 2:Illustration of our proposed HyperNoise approach. During training, the LoRA parameters are trained to predict improved noises and are optimized by reward maximization subject to KL regularization. During inference, the noise hypernetwork directly predicts the improved noise initialization which is used for the final generation.",
                "position": 304
            }
        ]
    },
    {
        "header": "3Noise Hypernetworks",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09968/x3.png",
                "caption": "Figure 3:An illustrative example of optimizing for learning the tilted distribution with an image redness reward. We show direct LoRA fine-tuning of SANA-Sprint[11]in comparison to training a noise hypernetwork with our proposed objective. Notably, when training with our objective, the model optimizes the desired reward while staying considerably closer topbasep^{\\text{base}}, as showcased by the model not diverging from the image manifold, unlike in direct LoRA fine-tuning.",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2508.09968/x4.png",
                "caption": "Figure 4:Trade-off between the redness reward objective and an image quality metric, ImageReward, for direct fine-tuning and Noise Hypernetworks. As opposed to direct fine-tuning, our proposed method optimizes the redness objective while not significantly dropping image quality as indicated by the ImageReward score.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2508.09968/x5.png",
                "caption": "Figure 5:Qualitative comparison our proposed noise hypernetwork with popular distilled models such as Flux-Schnell, SD3.5-Turbo, SANA-Sprint for 4-step generation. Both SANA-Sprint and FLUX-Schnell share the initial noise for the base and HyperNoise generation.",
                "position": 748
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Derivations",
        "images": []
    },
    {
        "header": "Appendix BExperimental and Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09968/x6.png",
                "caption": "Figure 6:Examples of artifacts introduced by directly Direct Fine-tuning diffusion models on rewards[69,12,49]for the same reward objective in comparison to Noise Hypernetwork training with same initial noise.",
                "position": 3930
            },
            {
                "img": "https://arxiv.org/html/2508.09968/x7.png",
                "caption": "Figure 7:More qualitative results on the human-preference reward setting. Base SANA-Sprint compared to HyperNoise with same initial noise.",
                "position": 3933
            },
            {
                "img": "https://arxiv.org/html/2508.09968/x8.png",
                "caption": "Figure 8:Non-cherry picked results on the human-preference reward setting. Base SANA-Sprint compared to HyperNoise with same initial noise.",
                "position": 3936
            }
        ]
    },
    {
        "header": "Appendix CAdditional results",
        "images": []
    }
]