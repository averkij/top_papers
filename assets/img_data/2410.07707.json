[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07707/x1.png",
                "caption": "Figure 1:(a) Gaussian flow under different supervision.We model Gaussian flow under the supervision of optical flow and motion flow respectively. The latter can produce a more direct description of object motion, thereby effectively guiding the deformation of 3D Gaussians.(b) The decoupling of optical flow.We decouple the optical flow into motion flow which is only related to object motion and camera flow which is only related to camera motion.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07707/x2.png",
                "caption": "Figure 2:The overall architecture of MotionGS.It can be viewed as two data streams: (1) The 2D data stream utilizes the optical flow decoupling module to obtain the motion flow as the 2D motion prior; (2) The 3D data stream involves the deformation and transformation of Gaussians to render the image for the next frame. During training, we alternately optimize 3DGS and camera poses through the camera pose refinement module.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x3.png",
                "caption": "Figure 3:Flow calculation.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x3.png",
                "caption": "Figure 3:Flow calculation.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x4.png",
                "caption": "Figure 4:Pose refinement on iterative training.",
                "position": 426
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07707/x5.png",
                "caption": "Figure 5:Qualitative comparison on NeRF-DS dataset.Refer toFigure12for more scenes.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x6.png",
                "caption": "",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x7.png",
                "caption": "",
                "position": 946
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x8.png",
                "caption": "Figure 8:Visualization of the camera trajectories optimized by our method and COLMAP.",
                "position": 966
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07707/x9.png",
                "caption": "Figure 9:The formulation of Gaussian flow.We first project the pointxtsubscript洧논洧노x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTcorresponding to thei洧녰iitalic_i-th Gaussian at timet洧노titalic_tinto the canonical Gaussian space, and then reproject this point from the canonical Gaussian space to thei洧녰iitalic_i-th Gaussian at timet+1洧노1t+1italic_t + 1.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x10.png",
                "caption": "Figure 10:Rendered depth from 3D Gaussian splatting (ours) and off-the-shelf monocular depth estimator (MiDas).Our rendered depth has richer details and is scale-aligned with the scene. MiDas rendered depth is usually more smooth and suffers from scale ambiguity.",
                "position": 2364
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x11.png",
                "caption": "Figure 11:Failure case in DyNeRF dataset.Since the viewpoints are fixed and sparse, neither motion flow nor optical flow can help our method avoid floating artifacts.",
                "position": 2402
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x12.png",
                "caption": "Figure 12:Qualitative comparison on NeRF-DS dataset per-scene.Compared with the state-of-the-art methods, our method can render more reasonable details, especially on dynamic objects.",
                "position": 2429
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x13.png",
                "caption": "Figure 13:Qualitative comparison on HyperNeRF dataset per-scene.Compared with the state-of-the-art methods, our method is more robust in reconstructing dynamic scenes. Even if the input camera pose is not accurate on HyperNeRF dataset, our method can adaptively optimize the camera poses and produce reasonable rendering results.",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2410.07707/x14.png",
                "caption": "Figure 14:Visualization of all data flows.In order: ground truth ofItsubscript洧냪洧노I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, ground truth ofIt+1subscript洧냪洧노1I_{t+1}italic_I start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT, rendered image ofItsubscript洧냪洧노I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, rendered depth of frameItsubscript洧냪洧노I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, optical flow, camera flow, motion flow, Gaussian flow.",
                "position": 2435
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]