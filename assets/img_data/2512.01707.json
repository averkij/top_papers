[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x1.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2512.01707/fig/worldwide.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x2.png",
                "caption": "",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x3.png",
                "caption": "Table 1:Comparison of streaming video understanding benchmarks withStreamGaze.MC: Multiple-choice; OE: Open-ended; All-time: Covers past, present, and future tasks; Proac.: Proactive.✓marks the presence of a feature, and△\\triangleindicates partial inclusion.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x3.png",
                "caption": "Figure 2:MLLMs performance acrossStreamGazetasks.",
                "position": 402
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Gaze-Guided Streaming Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x4.png",
                "caption": "Figure 3:Gaze-guided streaming data construction pipeline forStreamGaze.Given egocentric video sources and raw gaze projections (Sec.˜3.1), we first extract fixation moments across the entire video (Sec.˜3.2).\nNext, we divide each frame into FOV and out-of-FOV regions and extract objects within the gaze area (Sec.˜3.3).\nFinally, we construct scanpaths and generate streaming QA pairs (Sec.˜4).",
                "position": 459
            }
        ]
    },
    {
        "header": "4StreamGaze",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x5.png",
                "caption": "Figure 4:Task-wise effect of gaze input strategies onStreamGaze.Different tasks respond unevenly to textual, visual, and salience-map prompts, revealing strong task-specific behavior in streaming gaze understanding.",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x6.png",
                "caption": "Figure 5:Offline MLLMs’ behavior onStreamGazeproactive tasks.We evaluate type1 (false positive) and type2 (false negative) error from Qwen2.5-VL, InternVL-3.5, and GPT4o.",
                "position": 1330
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AStreamGazeData Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x7.png",
                "caption": "Figure 6:Data statistics ofStreamGaze.We report domain proportion, video length, world cloud for FOV extracted objects.",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x8.png",
                "caption": "Figure 7:Task proportion ofStreamGaze.We summarize the sample distribution for each task inStreamGaze.",
                "position": 2389
            }
        ]
    },
    {
        "header": "Appendix BStreamGazeEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x9.png",
                "caption": "Figure 9:Gaze prompting strategies forStreamGazeWe adopt three strategies for inputting gaze information to MLLMs.",
                "position": 2654
            }
        ]
    },
    {
        "header": "Appendix DStreamGazeData Details",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01707/x10.png",
                "caption": "Figure 10:Qualitative comparison with text reasoning and gaze-based reasoning.We visualize examples from the scene reconstruction task comparing (a) text and (b) gaze-based reasoning.",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x11.png",
                "caption": "Figure 11:Qualitative comparison with text reasoning and visual-based reasoning.We visualize examples from the object identification task comparing (a) text and (b) visual-based reasoning.",
                "position": 2955
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x12.png",
                "caption": "Figure 12:StreamGazedata example for OTP and NFI task.",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x13.png",
                "caption": "Figure 13:StreamGazedata example for SR and GSM task.",
                "position": 2966
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x14.png",
                "caption": "Figure 14:StreamGazedata example for OI (Easy/Hard) task.",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x15.png",
                "caption": "Figure 15:StreamGazedata example for OAR and FAP task.",
                "position": 2976
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x16.png",
                "caption": "Figure 16:StreamGazedata example for GTA and OAA task.",
                "position": 2981
            },
            {
                "img": "https://arxiv.org/html/2512.01707/fig/human_anno_html.png",
                "caption": "Figure 22:HTML for human verification ofStreamGazedata construction.",
                "position": 3117
            },
            {
                "img": "https://arxiv.org/html/2512.01707/fig/qa_html.png",
                "caption": "Figure 23:HTML for human oracle evaluation ofStreamGaze.",
                "position": 3122
            },
            {
                "img": "https://arxiv.org/html/2512.01707/x17.png",
                "caption": "Figure 24:Instructions provided to human annotators.",
                "position": 3127
            }
        ]
    },
    {
        "header": "Appendix FLicense",
        "images": []
    }
]