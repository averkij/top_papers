[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x1.png",
                "caption": "Figure 1:Left: The comparison chart of our grounding model results across five GUI grounding benchmarks. Our model, trained specifically for the agent setting, achieved SOTA results on all benchmarks under this focus. Even in the general end-to-end model setting, our model attained SOTA results on three of the benchmarks.Right: The relationship between model performance and computational cost on ScreenSpot-pro demonstrates that our model supports the Pareto frontier, indicating its efficiency. Most GUI research traditionally considers only the parameter countNNitalic_Nfor comparison, but our experiments highlight that computational cost during testing, such as the number of image tokens, also significantly impacts performance. The X-axis in the right figure representsN​DNDitalic_N italic_D, whereDDitalic_Dis the number of image tokens. Training and inference latency are more linearly correlated withN​DNDitalic_N italic_Dthan withNNitalic_N. A graph using latency as the X-axis closely resembles the right figure, but latency is often influenced by hardware and acceleration libraries such as vllm, so we did not use latency as X-axis.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x2.png",
                "caption": "Figure 2:Agent evolution across physical and virtual worlds. Traditional systems rely on fixed controllers and pre-defined workflows to execute domain-specific tasks, either in physical environments (e.g., task-specific robots) or virtual environments (e.g., API-based Web/APP agents). In the modern era, intelligent automation has emerged. In the physical world, general-purpose robots perform versatile limb-based operations. In the virtual world, Computer Use Agents (CUAs) achieve human-level behaviors through general purpose planner, GUI grounding, enabling them to complete any virtual task achievable via mouse and keyboard interactions.",
                "position": 191
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x3.png",
                "caption": "Figure 3:Three levels of task of CUAs. Each coral block represent an action step. We focus on the click action, as it is the most important and common operation. Others such as keyboard input can be effectively handled by MLLM.",
                "position": 214
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x4.png",
                "caption": "Figure 4:In terms of coordinate representation and loss strategies, we experimented with the following approaches: (a) tokenized coordinate representation, (b) label smoothing for coordinates, (c) loss reweighting, and (d) direct textual representation of coordinates.",
                "position": 234
            }
        ]
    },
    {
        "header": "3Data Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x5.png",
                "caption": "Figure 5:CommonCrawl data processing pipeline.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x6.png",
                "caption": "Figure 6:Filtered and kept image of the Web Search Data with the classifier.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x6.png",
                "caption": "Figure 6:Filtered and kept image of the Web Search Data with the classifier.",
                "position": 317
            }
        ]
    },
    {
        "header": "4Evaluation Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x7.png",
                "caption": "Figure 7:Examples of benchmarks used in evaluation.",
                "position": 431
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x8.png",
                "caption": "Figure 8:Illustration of the impact of modal input order on model training.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x9.png",
                "caption": "Figure 9:Data distribution.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x9.png",
                "caption": "Figure 9:Data distribution.",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x10.png",
                "caption": "Figure 10:Center point distribution of training and evaluation data.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x11.png",
                "caption": "Figure 11:■\\blacksquare■:DPO.■\\blacksquare■:Curriculum learning.■\\blacksquare■:Reject sampling finetuning.X+Y=NX+Y=Nitalic_X + italic_Y = italic_N",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x11.png",
                "caption": "Figure 11:■\\blacksquare■:DPO.■\\blacksquare■:Curriculum learning.■\\blacksquare■:Reject sampling finetuning.X+Y=NX+Y=Nitalic_X + italic_Y = italic_N",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x12.png",
                "caption": "Table 8:Detailed training data proportion for Phi-Ground-4B-16C with 40M training volume.†\\dagger†: The number of samples here does not refer to the quantity of images or elements. In fact, because each element has three types of references—positional, appearance, and functional—we randomly combine them during training as model inputs. This combination could involve one, two, or all three types. Consequently, a single element can be paired with various references, resulting in multiple samples. This explains why the numbers here differ from those described earlier.",
                "position": 1217
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x13.png",
                "caption": "Figure 13:Illustration of the evaluation results in relation to the training computation load. The Y-axis represents the benchmark scores in click accuracy, while the X-axis denotes the training computation per sample in TFLOPs. This training computation is estimated using the formulaF​L​O​P​s=6​N​DFLOPs=6NDitalic_F italic_L italic_O italic_P italic_s = 6 italic_N italic_D, whereNNitalic_Nis the number of image tokens andDDitalic_Dis the number of model parameters.",
                "position": 1429
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x14.png",
                "caption": "Figure 14:Multi-turns DPO vs. Offline DPO for in-domain post-training.",
                "position": 1444
            }
        ]
    },
    {
        "header": "6Evaluation and Case Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x15.png",
                "caption": "Figure 15:Types and Proportions of Errors on the ScreenSpot-pro Benchmark. In each image, the red rectangles represent the regions corresponding to the ground truth. Red circles indicate erroneous outputs from the previous stage, while green circles denote correct outputs from the current stage. The centers of the green circles fall within the ground truth boundaries. To avoid obstructing the image content, we have enlarged the green circles in some of the images.",
                "position": 1815
            }
        ]
    },
    {
        "header": "7Social Impacts and Open Questions",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Settings",
        "images": []
    },
    {
        "header": "Appendix BDetailed Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix CCoordinates Representations and Loss",
        "images": []
    },
    {
        "header": "Appendix DData Pre-processing Details",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23779/x16.png",
                "caption": "",
                "position": 8008
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x17.png",
                "caption": "",
                "position": 8009
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x18.png",
                "caption": "",
                "position": 8110
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x19.png",
                "caption": "",
                "position": 8173
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x20.png",
                "caption": "",
                "position": 8187
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x21.png",
                "caption": "",
                "position": 8201
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x22.png",
                "caption": "",
                "position": 8215
            },
            {
                "img": "https://arxiv.org/html/2507.23779/x23.png",
                "caption": "",
                "position": 8229
            }
        ]
    },
    {
        "header": "Appendix FMore Cases Study",
        "images": []
    }
]