[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10291/x1.png",
                "caption": "Figure 1:The overall Best-of-8 evaluation results across seven multimodal reasoning benchmarks with different critic models.Our VisualPRM greatly enhances the overall performance, while InternVL2.5-8B struggles to be an effective critic model.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x2.png",
                "caption": "Figure 2:Data examples in VisualPRM400K and VisualProcessBench.For VisualPRM400K, we generate the data using an automatic data pipeline. The key idea is to estimate the expected accuracym⁢ci𝑚subscript𝑐𝑖mc_{i}italic_m italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTof the given steps≤isubscript𝑠absent𝑖s_{\\leq i}italic_s start_POSTSUBSCRIPT ≤ italic_i end_POSTSUBSCRIPTbased on Monte Carlo sampling and consider the step correct ifm⁢ci>0𝑚subscript𝑐𝑖0mc_{i}>0italic_m italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0. During the training process of VisualPRM, the data is formulated as multi-turn conversations and the model is required to predict the correctness of each step conditioned on the image, question, and previous steps.\nFor VisualProcessBench, we collect questions from existing multimodal reasoning benchmarks[90,78,93,99,60]and generate the solutions using leading MLLMs[58,4,15,82,72]. Based on these questions and solutions, we employ a team of human experts with at least a university degree to manually annotate the correctness of each step in the solutions.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10291/x3.png",
                "caption": "Figure 3:Different modeling methods for PRMs.PRMs are developed to estimate the quality of each step in a given solution.\nFor value-based PRMs, the quality of a certain step is determined by its expected accuracym⁢ci𝑚subscript𝑐𝑖mc_{i}italic_m italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where a step is considered correct ifm⁢ci>0𝑚subscript𝑐𝑖0mc_{i}>0italic_m italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0.\nFor advantage-based PRMs, the quality of a certain step is determined by the improvement ofm⁢ci𝑚subscript𝑐𝑖mc_{i}italic_m italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPToverm⁢ci−1𝑚subscript𝑐𝑖1mc_{i-1}italic_m italic_c start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, where a step is considered good ifm⁢ci−m⁢ci−1>0𝑚subscript𝑐𝑖𝑚subscript𝑐𝑖10mc_{i}-mc_{i-1}>0italic_m italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_m italic_c start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT > 0.\nDuring the training stage, the output space of PRMs is discretized into specific tokens, while during the inference stage, we compute the step score as the weighted sum of the generation probability for these discretized tokens.",
                "position": 212
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10291/x4.png",
                "caption": "(a)",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x4.png",
                "caption": "(a)",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x5.png",
                "caption": "(b)",
                "position": 995
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Training Hyper-parameters",
        "images": []
    },
    {
        "header": "7More Ablation Studies",
        "images": []
    },
    {
        "header": "8More Statistics for VisualProcessBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10291/x6.png",
                "caption": "(a)",
                "position": 3186
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x6.png",
                "caption": "(a)",
                "position": 3189
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x7.png",
                "caption": "(b)",
                "position": 3195
            }
        ]
    },
    {
        "header": "9More Data Examples in VisualPRM400K",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10291/x8.png",
                "caption": "(a)",
                "position": 3228
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x8.png",
                "caption": "(a)",
                "position": 3231
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x9.png",
                "caption": "(b)",
                "position": 3237
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x10.png",
                "caption": "(c)",
                "position": 3243
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x10.png",
                "caption": "(c)",
                "position": 3246
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x11.png",
                "caption": "(d)",
                "position": 3252
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x12.png",
                "caption": "(e)",
                "position": 3258
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x13.png",
                "caption": "(f)",
                "position": 3264
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x14.png",
                "caption": "(a)",
                "position": 3273
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x14.png",
                "caption": "(a)",
                "position": 3276
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x15.png",
                "caption": "(b)",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x16.png",
                "caption": "(c)",
                "position": 3288
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x17.png",
                "caption": "(d)",
                "position": 3294
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x18.png",
                "caption": "(e)",
                "position": 3300
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x18.png",
                "caption": "(e)",
                "position": 3303
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x19.png",
                "caption": "(f)",
                "position": 3309
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x20.png",
                "caption": "(g)",
                "position": 3315
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x21.png",
                "caption": "(h)",
                "position": 3321
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x21.png",
                "caption": "(h)",
                "position": 3324
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x22.png",
                "caption": "(i)",
                "position": 3330
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x23.png",
                "caption": "(j)",
                "position": 3336
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x24.png",
                "caption": "(k)",
                "position": 3342
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x25.png",
                "caption": "(l)",
                "position": 3348
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x25.png",
                "caption": "(l)",
                "position": 3351
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x26.png",
                "caption": "(m)",
                "position": 3357
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x27.png",
                "caption": "(n)",
                "position": 3363
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x28.png",
                "caption": "(o)",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2503.10291/x29.png",
                "caption": "Figure 8:Data example with model reflection from VisualProcessBench.Redhighlights the incorrect answer,orangehighlights the reflection words, andgreenhighlights the correct answer.",
                "position": 3378
            }
        ]
    },
    {
        "header": "10More Data Examples in VisualProcessBench",
        "images": []
    }
]