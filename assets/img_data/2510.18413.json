[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18413/x1.png",
                "caption": "Figure 1:Illustration of Adamas compared with existing methods. While StreamingLLM employs a fixed sparse pattern and Quest inherently selects pages, Adamas dynamically selects KV pairs at the token level, thereby achieving better preservation of model accuracy and inference efficiency.",
                "position": 83
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18413/x2.png",
                "caption": "Figure 2:Overview of Adamas. QueriesQQand keysKKare processed through Hadamard transform, bucketization, and 2-bit compression. The transformed keysùêáK\\mathbf{H}_{K}are then compared against the transformed queryùêáQ\\mathbf{H}_{Q}in Manhattan-distance estimator, based on which the top-kkKV pairs are selected. Finally, Adamas performs sparse attention usingQQand the selected KV pairs.",
                "position": 146
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18413/x3.png",
                "caption": "Figure 3:Evaluation results on LongBench. Adamas exhibits the smallest performance drop compared to full attention while maintaining high sparsity.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2510.18413/x4.png",
                "caption": "Figure 4:Perplexity results of StreamingLLM, Quest, Adamas, and full attention evaluated on PG19 with LongChat-7b-v1.5-32k under varying token budgets. Adamas consistently matches full attention and even shows lower perplexity at larger token budget.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2510.18413/x5.png",
                "caption": "Figure 5:Kernel-level breakdown of Adamas self-attention compared to full attention (FlashInfer).",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2510.18413/x6.png",
                "caption": "Figure 6:Ablation studies on the Hadamard transform, bucketization, and distance metrics.",
                "position": 480
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComputational Workloads and Memory Access Analysis",
        "images": []
    },
    {
        "header": "Appendix BLongBench Evaluations",
        "images": []
    },
    {
        "header": "Appendix CAblation Studies",
        "images": []
    }
]