[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08683/x1.png",
                "caption": "Figure 1:Visual intelligence as codec-aligned predictive compression.Visual intelligence as a compression problem, where scalable learning emerges from alignment with the predictive structure of the world. Video exemplifies this principle: most visual content is redundant and predictable, while meaningful information arises sparsely as motion and residual change. Video codecs make this structure explicit by decomposing visual signals into stable spatial context and sparse temporal updates. Grounded in this codec principle, OV-Encoder reframes visual modeling as predictive compression, serving as a scalable engine for universal multimodal intelligence that sees, updates, and reasons over time.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08683/x2.png",
                "caption": "Figure 2:Overview of the OneVision-Encoder framework.Left:Input formulation. The framework integrates three Codec Patchification strategies: Dense Video-Codec Patchification, Chunk-wise Patchification, and (Sigle-Image/Frame) Spatial Patchification. All inputs are processed by a shared-parameter OneVision-Encoder.Right:Unified cluster discrimination objective. Image and video embeddings are aligned through contrastive learning against a global set of class centers, jointly optimizing object-level and action-level representations within a single encoder.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2602.08683/x3.png",
                "caption": "Figure 3:Contrastive learning vs. cluster discrimination.(a) Standard contrastive learning contrasts samples against batch-local negatives, constraining the view of the embedding space. (b) Cluster discrimination contrasts samples against a global concept bank of clustered centers at scale, yielding discriminative and structurally separated representations.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2602.08683/x4.png",
                "caption": "Figure 4:3D-RoPE for Codec Patchification.A unified relative positional encoding scheme is adopted for Codec Patchification. (a) encodes full spatiotemporal offsets(Δ​t,Δ​x,Δ​y)(\\Delta t,\\Delta x,\\Delta y)over I/P-frame sequences to preserve motion-driven inter-frame structure. (b) defines temporal offsets at the chunk level, enabling structured reasoning under non-uniform temporal sampling. (c) degenerates the formulation to purely spatial offsets(0,Δ​x,Δ​y)(0,\\Delta x,\\Delta y)for static inputs. 3D-RoPE preserves structural consistency, enabling coherent attention over sparse and irregular token layouts.",
                "position": 360
            }
        ]
    },
    {
        "header": "3Pretraining Dataset",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08683/x5.png",
                "caption": "Figure 5:Visualization of I- and P-frame decomposition in HEVC. I-frames retain complete spatial structure, whereas P-frames encode motion-compensated residuals highlighting motion. Bright areas denote high residual magnitudes, while dark areas indicate static content.",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2602.08683/x6.png",
                "caption": "Figure 6:Comparison of video processing pipelines for spatiotemporal representation learning. (a) original dense video input with full temporal context, (b) uniform frame sampling that sparsely selects evenly spaced frames, (c) temporal saliency detection that identifies motion- and event-centric regions across all frames, and (d) Codec patch extraction that selectively retains temporally salient patches under a fixed token budget.",
                "position": 1454
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contributors",
        "images": []
    },
    {
        "header": "8Implementation Details",
        "images": []
    },
    {
        "header": "9Controlled Evaluation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08683/x7.png",
                "caption": "Figure 7:Controlled evaluation pipeline decoupling the encoder for fair comparison against Qwen3-ViT and SigLIP2",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2602.08683/x8.png",
                "caption": "Figure 8:Spatial Bias Analysis of Selected Visual Patches under a given setting.",
                "position": 1812
            }
        ]
    },
    {
        "header": "10Spatial Bias Analysis",
        "images": []
    },
    {
        "header": "11Token Allocation Case Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08683/x9.png",
                "caption": "Figure 9:Case study 1 (Diving): dense evidence for continuous motion.Under a fixed budget of 2048 tokens, codec-style patch extraction distributes tokens over 64 sampled frames to capture dense pose transitions, while uniform sampling allocates the same budget to only 8 frames and may miss brief but critical instants. Phases:P10.0–1.4s (takeoff + angular momentum),P21.4–3.4s (tuck/pike set; body shape),P33.4–11.6s (twist+somersault progression; dense cues),P411.6–14.2s (spot water + open/align),P514.2–15.0s (entry alignment; line/splash). Diving is high-speed and continuous, so discriminative cues are temporally dense and benefit from dense coverage.",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2602.08683/x10.png",
                "caption": "Figure 10:Case study 2 (Cooking): sparse key frames under instantaneous motion.We analyze a 130-second cooking video where decisive evidence concentrates in short, discrete transitions (e.g., brief pours) rather than being uniformly distributed over time. Under a fixed budget of 4096 tokens, codec-style allocation spreads tokens over a 64-frame timeline and reallocates P-frame tokens toward high-saliency moments, while uniform sampling allocates the same budget to only 16 frames and may miss brief segments entirely (misplacement). Phases:P10.0–25.0s (context),P225.0–27.0s (pour raw materials),P328.0–37.0s (mix raw materials),P438.0–39.0s (pour raw materials),P540.0–45.0s (mix raw materials),P646.0–48.0s (pour raw materials),P749.0–51.0s (mix raw materials),P852.0–54.0s (pour raw materials),P955.0–71.0s (mix raw materials),P1072.0–98.0s (pour raw materials),P1198.0–130.0s (context).",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2602.08683/x11.png",
                "caption": "Figure 11:Comparison of video processing pipelines for spatiotemporal representation learning. (a) original dense video input with full temporal context, (b) uniform frame sampling that sparsely selects evenly spaced frames, (c) temporal saliency detection that identifies motion- and event-centric regions across all frames, and (d) Codec patch extraction that selectively retains temporally salient patches under a fixed token budget.",
                "position": 1852
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]