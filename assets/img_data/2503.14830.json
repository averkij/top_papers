[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x1.png",
                "caption": "",
                "position": 207
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x2.png",
                "caption": "Figure 2:Overview ofDP-Recon.We first use reconstruction loss‚Ñír‚Å¢e‚Å¢c‚Å¢o‚Å¢nsubscript‚Ñíùëüùëíùëêùëúùëõ\\mathcal{L}_{recon}caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_c italic_o italic_n end_POSTSUBSCRIPTfor decompositional neural reconstruction, followed by the prior-guided geometry optimization stage that incorporatesSDSloss‚ÑíSDSg‚àívsuperscriptsubscript‚ÑíSDSùëîùë£\\mathcal{L}_{\\text{SDS}}^{g-v}caligraphic_L start_POSTSUBSCRIPT SDS end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_g - italic_v end_POSTSUPERSCRIPT. We finally export the object meshes and optimize their appearance with‚ÑíSDSa‚àívsuperscriptsubscript‚ÑíSDSùëéùë£\\mathcal{L}_{\\text{SDS}}^{a-v}caligraphic_L start_POSTSUBSCRIPT SDS end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a - italic_v end_POSTSUPERSCRIPT. The visibility\nbalances the guidance from prior and reconstruction by dynamically adjusting per-pixelSDSloss.",
                "position": 281
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x3.png",
                "caption": "Figure 3:Qualitative comparison of 10-views reconstruction.We present examples from ScanNet++[99]and Replica[80]. In each example, the first row shows the background, the second the full scene, and the third individual objects. We reconstruct more complete and reasonable 3D geometry, especially in less captured and occluded regions, such as the chair behind the table and the background.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2503.14830/x4.png",
                "caption": "Figure 4:Qualitative results of novel view synthesis. Our method significantly improves rendering quality, particularly in less captured regions with low visibility, shown in darker colors in the visibility maps, such as the highlighted corner of the wall.",
                "position": 609
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x5.png",
                "caption": "Figure 5:Visualized novel view instance masks.Our method can synthesize consistent and complete novel view instance masks.",
                "position": 1217
            },
            {
                "img": "https://arxiv.org/html/2503.14830/x6.png",
                "caption": "Figure 6:Qualitative ablation comparison.We show the meshes in the first row along with their textures in the second, demonstrating that prior knowledge can supplement missing information while the visibility modeling ensures consistency with input views.",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2503.14830/x7.png",
                "caption": "Figure 7:Examples of scene editing. Based on reconstructed scenes, our model seamlessly supports flexible text-guided geometry and appearance editing, as well asVFXediting.",
                "position": 1280
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGeneralizability to in-the-wild videos",
        "images": []
    },
    {
        "header": "Appendix BDecompositional Reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x8.png",
                "caption": "Figure S.8:Generalizability to YouTube videos with 15 input views.The reconstruction results highlight our model‚Äôs robust ability to generalize to in-the-wild indoor scenes.",
                "position": 2739
            },
            {
                "img": "https://arxiv.org/html/2503.14830/x9.png",
                "caption": "Figure S.9:Panorama inpainting of background. We present the original panorama map, the inpainted panorama map, the visibility panorama map, as well as the mask and depth guidance used in the depth-guided panorama inpainting process.",
                "position": 3025
            }
        ]
    },
    {
        "header": "Appendix CMore Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x10.png",
                "caption": "Figure S.10:Failure case of directly optimizing background withSDSloss.Incorporating appearanceSDSfor the background may result in hallucinated non-existent objects in low-visibility regions, even when the smooth normal map indicates no objects in the background. On the contrary, the background map obtained after optimizing depth-guided panorama inpainting produces a cleaner and more reasonable background texture.",
                "position": 3096
            },
            {
                "img": "https://arxiv.org/html/2503.14830/x11.png",
                "caption": "Figure S.11:Visualization of UV mapping and rendering results.Our method produces completed meshes with detailed UV maps, enabling photorealistic rendering in common 3D software such as Blender.",
                "position": 3158
            }
        ]
    },
    {
        "header": "Appendix DMore Experiment Details",
        "images": []
    },
    {
        "header": "Appendix EScene Editing Details",
        "images": []
    },
    {
        "header": "Appendix FComparison with Image-to-3D Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14830/x12.png",
                "caption": "Figure S.12:Comparison with image-to-3D method Trellis[94].For better visualization, we adjust the location and rotation of Trellis results manually.",
                "position": 3452
            },
            {
                "img": "https://arxiv.org/html/2503.14830/x13.png",
                "caption": "Figure S.13:Qualitative Examples for Failure Cases.We present failure cases for both geometry optimization and appearance optimization. The first column displays the input view, while the second and third columns show our results rendered from the input view and a novel view, respectively. The final column provides the corresponding results without applying the geometry prior (top) or appearance prior (bottom), highlighting the improvements introduced by our generative prior.",
                "position": 3459
            }
        ]
    },
    {
        "header": "Appendix GFailure Cases and Limitation",
        "images": []
    }
]