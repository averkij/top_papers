[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22525/x1.png",
                "caption": "",
                "position": 51
            },
            {
                "img": "https://arxiv.org/html/2512.22525/x2.png",
                "caption": "",
                "position": 60
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22525/x3.png",
                "caption": "Figure 2:The overview of DreamOmni3’s training data construction and framework.\nThe overview of DreamOmni3’s training data construction and framework:(a)We create scribble-based editing training data. For scribble and multimodal instruction-based editing, we use Referseg to locate edit objects and paste corresponding scribbles onto the source and reference images to create training pairs. For scribble and instruction-based editing, we omit the reference image. For doodle editing, we use a dedicated model to convert the edit objects into abstract sketches and paste them back into the source image. For image fusion, we crop objects from the reference image and paste them into the corresponding position on the source image to build training pairs.(b)Scribble-based generation training data is created similarly to editing, except the source image is replaced with a blank white canvas.(c)DreamOmni3 builds on the framework of DreamOmni2[dreamomni2], introducing a joint input scheme for scribble inputs. We also apply the same encoding scheme to both the source and scribbled images, ensuring better pixel alignment and perfect compatibility with previous image and language instruction editing.",
                "position": 143
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22525/x4.png",
                "caption": "Figure 3:Visual comparison of scribble-based editing. Compared to other competitive methods and even closed-source commercial models (GPT-4o and Nano Banana), DreamOmni3 shows more accurate editing results and better consistency.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2512.22525/x5.png",
                "caption": "Figure 4:Visual comparison of scribble-based generation. Our DreamOmni3 significantly outperforms current open-source models and achieves generation results comparable to closed-source commercial models (GPT-4o and Nano Banana).",
                "position": 363
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]