[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02785/x1.png",
                "caption": "Figure 1:We propose DreamStyle, a unified video stylization framework, which provides a flexible and practical tool for users to create high-quality stylized videos. Given an input video and the reference styles in forms of text, style image, or stylized first frame, DreamStyle faithfully generates videos that align with the desired styleâ€”while preserving the main content of the input video.",
                "position": 86
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02785/x2.png",
                "caption": "Figure 2:Data Curation Pipeline. We propose generating the training data with two key steps: image stylization followed by image to video. Considering the characteristics of different image stylization techniques, we construct a CT dataset and a SFT dataset, where SDXL (equipped with ControlNet, InstantStyle, and ID plugin) and Seedream 4.0 are selected as their stylization models, respectively. For image to video, we utilize ControlNets to enhance the motion consistency between the generated stylized and raw videos. To ensure the data quality, we additionally apply automatic filtering for CT data and manual filtering for SFT data.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2601.02785/x3.png",
                "caption": "Figure 3:Example that depth fails to capture accurate detail. (a) The raw video frame, (b) the extracted depth map, (c) the generated realistic frame, (d) the generated stylized frame.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2601.02785/x4.png",
                "caption": "Figure 4:Overview of DreamStyle Framework. DreamStyle is built on the Wan14B-I2V model, integrating the text and raw-video conditions through the cross-attention and image channels of the base model, while the first-frame and style-image conditions serve as additional frames concatenated to the start and end of the frame sequence. We train it using a standard flow matching loss and a token-specific LoRA that contributes to distinguishing different condition tokens.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02785/x5.png",
                "caption": "Figure 5:Qualitative comparison on three video stylization tasks.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2601.02785/x6.png",
                "caption": "Figure 6:Visual results of multi-style fusion.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2601.02785/x7.png",
                "caption": "Figure 7:Impact of token-specific LoRA.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2601.02785/x8.png",
                "caption": "Figure 8:Visual results of long-video stylization.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2601.02785/x9.png",
                "caption": "Figure 9:Visual comparison across different datasets.",
                "position": 614
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    }
]