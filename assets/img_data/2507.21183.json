[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary & Problem Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21183/x1.png",
                "caption": "Figure 1:Under the standard MLE-based DPO (left), empirical studies(Pal et al.,2024; Rafailov et al.,2024; Tajwar et al.,2024; Ren & Sutherland,2024)demonstrated that training tends to simultaneously downscale (with different magnitudes) both the chosen and rejected responses to increase their gap. Our MaP-based method (right) mitigates this harmful tendency by re-weighting the rejected response based on prior knowledge. Here, the x-axis denotes the initial modelθ1\\theta_{1}italic_θ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand a potentially harmful modelθ2\\theta_{2}italic_θ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTthat may arise during training, while the y-axis shows the log-likelihood of a fixed preference pair under different policies.",
                "position": 326
            }
        ]
    },
    {
        "header": "4MaPPO Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21183/x2.png",
                "caption": "Figure 3:Illustration of the iterative MaPPO pipeline in each iterationkkitalic_k.",
                "position": 651
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Results",
        "images": []
    },
    {
        "header": "Appendix BSupplementary Experiments",
        "images": []
    },
    {
        "header": "Appendix CFurther Discussions",
        "images": []
    }
]