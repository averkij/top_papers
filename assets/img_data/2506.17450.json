[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17450/x1.png",
                "caption": "Figure 1:BlenderFusion integrates the 3D-grounded editing capabilities of graphics software into the strong synthesis abilities of diffusion models.\nDespite fine-tuned only on video frames of simple object transformations with entangled camera motion, it learns precise object control, inherits Blender‚Äôs rich editing functionalities (e.g., attribute modification, deformation, novel asset insertion), and generalizes to highly fine-grained multi-object editing and scene composition tasks (Figure¬†6).",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x2.png",
                "caption": "Figure 2:BlenderFusion employs a layering-editing-compositing process:\n1) segment and lift objects from the source images into editable 3D elements;\n2) edit the visual elements in Blender to transform the initial scene to the target scene; and\n3) use a dual-stream diffusion compositor to blend the target image.\nThe video training data consists of object and camera pose annotations.\nWe use these annotations to simulate the test-time transformations in Blender.\nThe text input for each stream encodes a set of tuples consisting of object category labels and 3D bounding boxes.\nThe source masking strategy, indicated by overlaidorangebounding boxes inIsrcsuperscriptùêºsrc{I}^{\\text{src}}italic_I start_POSTSUPERSCRIPT src end_POSTSUPERSCRIPTandRsrcsuperscriptùëÖsrc{R}^{\\text{src}}italic_R start_POSTSUPERSCRIPT src end_POSTSUPERSCRIPT, is detailed in ¬ß3.2.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x3.png",
                "caption": "Figure 3:The simulated object jittering training strategy for improving disentangled object control.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x4.png",
                "caption": "Figure 4:Qualitative results with the standard video frame setup on three datasets. BlenderFusion outperforms the baselines in visual quality and object identity preservation. The 3D bounding boxes in MOVi-E are omitted for clear visualization.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x5.png",
                "caption": "Figure 5:Qualitative comparison results on disentangled visual control tasks, using a source image from either Objectron or WOD. BlenderFusion demonstrates more precise control, more consistent object identity, and better disentanglement of camera and object.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x6.png",
                "caption": "Figure 6:Qualitative results on fine-grained multi-object editing and compositing tasks. The edit intent demonstrates the expected output geometry while the color encodes the object identity.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x7.png",
                "caption": "Figure 7:Top:BlenderFusion shows reasonable generalization to in-the-wild images from SUN-RGBD[44], ARKitScenes[4], and Hypersim[34]datasets.Bottom:our framework inherits the versatile editing capabilities of graphics software, enabling diverse object control tasks beyond the training data. Images are resized to facilitate visualization.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x8.png",
                "caption": "Figure 8:Qualitative ablation results over the key designs, including using Blender renders as the 3D-grounded control signals as well as the training strategies of the diffusion compositor.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x9.png",
                "caption": "Figure 9:Failure cases on object rotation.",
                "position": 1836
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x10.png",
                "caption": "Figure 10:Additional qualitative results of the standard evaluation setting (source and target frames from a video), extendingFigure¬†4of the main paper. 3DIT is omitted in this figure.",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2506.17450/x11.png",
                "caption": "Figure 11:Additional qualitative results of disentangled object control with fixed camera, extendingFigure¬†5of the main paper.",
                "position": 1848
            }
        ]
    },
    {
        "header": "Appendices",
        "images": []
    }
]