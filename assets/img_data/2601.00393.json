[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00393/x1.png",
                "caption": "",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00393/x2.png",
                "caption": "Figure 2:Framework of NeoVerse.In the reconstruction part, we propose a pose-free feed-forward 4DGS reconstruction model (Sec.3.1) with bidirectional motion modeling.\nThe degraded renderings in novel viewpoints from 4DGS are input to the generation model as conditions.\nDuring training, the degraded rendering conditions are simulated from monocular videos (Sec.3.2), and the original videos themselves serve as targets.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x3.png",
                "caption": "Figure 3:Training pairs with degradation simulation.",
                "position": 399
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00393/x4.png",
                "caption": "Figure 4:Generation with large camera motions on challenging in-the-wild videos.We compare our method against other related work on “Pan left” (left) and “Move right” (right) cases. Our NeoVerse achieves better generation quality while maintaining precise camera controllability. Yellow boxes highlight artifacts.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x5.png",
                "caption": "Figure 5:Qualitative comparison with state-of-the-art methods instatic scenes. Red boundaries indicate inconsistent renderings due to inaccurate pose prediction. Yellow boxes indicate artifacts.",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x6.png",
                "caption": "Figure 6:Qualitative comparison with state-of-the-art methods indynamic scenes. Yellow boxes indicate artifacts.\nNote that the black regions in our prediction arenot errorbut mainly caused by partial observations of input frames.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x7.png",
                "caption": "Figure 7:Effectiveness of degradation simulation.The model learns to suppress artifacts and hallucinate realistic details in occluded or distorted regions through degradation simulation.",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x8.png",
                "caption": "Figure 8:Visualization about global motion tracking and aggregation. (a) Input video. (b) Aggregated static Gaussians separated by predicted velocities. (c) Aggregated static Gaussians separated with global motion tracking.",
                "position": 993
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x9.png",
                "caption": "Figure 9:Visualization of 3D tracking.For better visualization, we only show the Gaussian centers.",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x10.png",
                "caption": "Figure 10:Video editing.Left: The white car is edited to be red. Right: The mirror teapot is edited to be transparent.",
                "position": 1019
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CDataset Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Protocol",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00393/x11.png",
                "caption": "Figure S1:Failure cases.Top: Text generation failure. Bottom: Novel view generation on 2D data.",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x12.png",
                "caption": "Figure S2:Image to world.Starting from a single view, NeoVerse can reconstruct a 3D scene, generate an exploration video, and iteratively expand the visible area.",
                "position": 1477
            },
            {
                "img": "https://arxiv.org/html/2601.00393/x13.png",
                "caption": "Figure S3:Single-view to multi-view generation.Starting from a single front-view video, NeoVerse can generate multi-view consistent videos.",
                "position": 1487
            }
        ]
    },
    {
        "header": "Appendix FAdditional Qualitative Results",
        "images": []
    }
]