[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x1.png",
                "caption": "Figure 1.Audio-driven\nSpatially-aware Video Generation targets to synthesize realistic videos that are semantically and spatially aligned with input audio recordings. Our proposed SpA2V framework accomplishes this task by decomposing generation process into two stages:Audio-guided Video PlanningandLayout-grounded Video Generation, achieving audio-video correspondence via leveraging VSLs as intermediate representation to capture auditory cues and guide the generation process respectively. Hereground-truth videosare for visual comparisons withgenerated videosonly and are not inputted into our framework.",
                "position": 99
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x2.png",
                "caption": "Figure 2.Different frameworks for audio-driven video generation. From top to bottom are the typical Audio‚Üí\\rightarrow‚ÜíVideo direct approach, two-stage Audio‚Üí\\rightarrow‚ÜíText‚Üí\\rightarrow‚ÜíVideo method, and our proposed novel Audio‚Üí\\rightarrow‚ÜíVideo Scene Layout‚Üí\\rightarrow‚ÜíVideo pipeline respectively.",
                "position": 123
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x3.png",
                "caption": "Figure 3.Illustration for the overall framework of SpA2V which is decomposed into two stages: Audio-guided Video Planning and Layout-grounded Video Generation. In the first stage (Section3.1), given an input audioùíú\\mathcal{A}caligraphic_A, we retrievekkitalic_kexample conversations{‚Ñ∞1,‚Ñ∞2,‚Ä¶‚Äã‚Ñ∞k}\\{\\mathcal{E}_{1},\\mathcal{E}_{2},\\dots\\mathcal{E}_{k}\\}{ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ caligraphic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }from candidate databaseùíü\\mathcal{D}caligraphic_Dvia Retrieval Module and feed them together with a System Instruction and the audio itself into the MLLM Video Planner to perform reasoning and generate a desired VSL sequence‚Ñí\\mathcal{L}caligraphic_LcontainingNNitalic_Nconsecutive keyframe layouts{‚Ñí1,‚Ñí2,‚Ä¶‚Äã‚ÑíN}\\{\\mathcal{L}_{1},\\mathcal{L}_{2},\\dots\\mathcal{L}_{N}\\}{ caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ caligraphic_L start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }with respective global video caption and local frame captions. In the second stage (Section3.2), the obtained VSL‚Ñí\\mathcal{L}caligraphic_Land its captions are incorporated to guide a video diffusion model consisting of pretrained Base, Motion, and Spatial Grounding Modules to generate the final videoùí±\\mathcal{V}caligraphic_Vthat is semantically and spatially coherent with the inputùíú\\mathcal{A}caligraphic_A.",
                "position": 197
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x4.png",
                "caption": "Figure 4.In-context Learning helps guide the MLLM to derive the correct spatial cues from the right physical sound properties and hence generate a highly-aligned VSL.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2508.00782/x5.png",
                "caption": "Figure 5.Qualitative comparisons of our SpA2V with prior SOTA works in audio-to-video generation. Here GT denotes ground-truth videos and VSLs for illustration of visual elements present in input audios. Zoom-in for details.",
                "position": 270
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": []
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BBenchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x6.png",
                "caption": "Figure 6.Our system instruction for the MLLM Video Planner to generate VSLs based on input audios.",
                "position": 2128
            },
            {
                "img": "https://arxiv.org/html/2508.00782/x7.png",
                "caption": "Figure 7.The template of our in-context example conversations to provide context for the MLLM Video Planner.",
                "position": 2139
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x8.png",
                "caption": "Figure 8.Breakdown statistics of AVLBench.",
                "position": 2154
            }
        ]
    },
    {
        "header": "Appendix DLimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00782/x9.png",
                "caption": "Figure 9.Illustration for limitations of our SpA2V.",
                "position": 2547
            }
        ]
    },
    {
        "header": "Appendix ESocietal Impacts",
        "images": []
    }
]