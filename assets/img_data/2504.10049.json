[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/input-settings.png",
                "caption": "Figure 1.VLMs are able to process a multimodal presentation in various unimodal and multimodal representations.",
                "position": 104
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/venn_coverage.png",
                "caption": "Figure 2.Average token count in speech transcript and OCR, and their overlap",
                "position": 254
            }
        ]
    },
    {
        "header": "4.Benchmark",
        "images": []
    },
    {
        "header": "5.Fine-grained Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/slides_vs_video.png",
                "caption": "Figure 3.Unimodal performance at different token budgets",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/register_tokens.png",
                "caption": "Figure 4.The addition of structure improves the Rouge score compared to the transcript alone",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/visual_tokens.png",
                "caption": "Figure 5.Rouge score with different visual token budgets",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/model_scaling.png",
                "caption": "Figure 6.Rouge score with varying visual token budgets for different model sizes",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/average_length.png",
                "caption": "Figure 7.Slides-only and structured multimodal representations constitute input-length-optimal representations for different input lengths. Bigger icons depict higher visual token budget.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_coverage.png",
                "caption": "(a)Unimodal or multimodal input",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_coverage.png",
                "caption": "(a)Unimodal or multimodal input",
                "position": 741
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_coverage_struct.png",
                "caption": "(b)Structured or unstructured input",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_density.png",
                "caption": "(a)Unimodal or multimodal input",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_density.png",
                "caption": "(a)Unimodal or multimodal input",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_density_strcut.png",
                "caption": "(b)Structured or unstructured input",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_relevance_score.png",
                "caption": "(a)Unimodal or multimodal input",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_relevance_score.png",
                "caption": "(a)Unimodal or multimodal input",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_relevance_score_struct.png",
                "caption": "(b)Structured or unstructured input",
                "position": 780
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/relevance_extractiveness.png",
                "caption": "(a)Coverage",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/relevance_extractiveness.png",
                "caption": "(a)Coverage",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/relevance_extractiveness-dens.png",
                "caption": "(b)Density",
                "position": 798
            }
        ]
    },
    {
        "header": "6.Discussion",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]