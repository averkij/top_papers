[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03771/x1.png",
                "caption": "Figure 1:An overview of Doublespeak attack. The harmful token in examples (left) is replaced with an innocuous substitute to form adversarial in-context examples (right). This malicious input bypasses safety mechanisms, using a seemingly innocent question, but triggering a dangerous response suitable for the original harmful token.The specific instructions generated by the model have been omitted for safety considerations.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3In-Context Representation Hijacking Attack",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03771/x2.png",
                "caption": "Figure 2:Applying Patchscopes to Doublespeak.On Llama-3-8B-Instruct. The interpretations of the target word (“carrot”) under Doublespeak attack along the 32 layers of the model. In blue, we measure the interpretation to be the original word (“carrot”) and in orange the malicious word (“bomb”). As can be seen, in the first layers, the interpretation is benign, and in later layers, it is malicious. The refusal direction layer(arditi2024refusal)is in the benign region. Additional details in AppendixI.",
                "position": 426
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03771/x3.png",
                "caption": "Figure 3:Effect of context-length scaling on Llama-3-70B ASR.A single in-context example achieves the highest ASR (75%) on Llama-3-70B. The score is compared to directly prompting the model with the malicious instruction (baseline). Success@10 measures Doublespeak’s score over the 10 context sizes (1, 4, 7, …, 28) for each malicious instruction, yielding an overall ASR of 90%.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2512.03771/x4.png",
                "caption": "Figure 4:Analysis of Doublespeak Responses on Gemma Models.\nWe illustrate the distribution of attack outcomes across varying numbers of in-context examples for Gemma-3. Outcomes are categorized as: Malicious, Rejected, and Benign. Smaller models require more in-context examples for successful representation hijacking. Larger models tend to reject inputs with an excessive number of in-context examples, indicating their capacity to detect and refuse malicious intent. Similar to the Llama-3 results, larger models tend to be more vulnerable. See SectionsGandFfor more details.",
                "position": 613
            }
        ]
    },
    {
        "header": "5Discussion, limitations, and future work",
        "images": []
    },
    {
        "header": "Ethics statement",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "Appendix AExamples on other models",
        "images": []
    },
    {
        "header": "Appendix BHarmless In-Context Representation Hijacking",
        "images": []
    },
    {
        "header": "Appendix CMulti-Token Hijacking",
        "images": []
    },
    {
        "header": "Appendix DRobustness to Token Selection",
        "images": []
    },
    {
        "header": "Appendix EExperiment Implementation Details",
        "images": []
    },
    {
        "header": "Appendix FLLM-as-a-Judge Prompt",
        "images": []
    },
    {
        "header": "Appendix Gfailure mode examples",
        "images": []
    },
    {
        "header": "Appendix HFailure of Logit-Lens on Llama3.1-8B-instruct",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03771/x5.png",
                "caption": "Figure 16:Normalized Patchscopes Interpretation Scores.To improve the interpretability of the raw scores (cf. Figure2), we normalize them by a baseline, yielding a probability ratio. This view confirms the underlying dynamic: the score for the benign keyword is initially high but drops in later layers as the score for the harmful keyword rises. Notably, the refusal-triggering layer operates in a region where the benign keyword’s score is still dominant.",
                "position": 1374
            }
        ]
    },
    {
        "header": "Appendix IDistilling Patchscopes outputs",
        "images": []
    }
]