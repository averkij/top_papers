[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10783/x1.png",
                "caption": "Figure 1:Static benchmark contamination.As training data increases, the risk for test set contamination grows and static benchmarks becomes saturated, reflecting falsely improved capabilities.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x2.png",
                "caption": "Figure 2:We propose LiveXiv, a new method for generating Live multi-modal dataset for Visual Question-Answering based on ArXiv content. Our pipeline automatically generates scalable and reliable questions along with an efficient evaluation method to reduce the computational and logistic overheads required for continually evaluating past and present models on new versions of the dataset.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3LiveXiv",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10783/x3.png",
                "caption": "Figure 3:Our live dataset generation consists of several stages. We first extract the images and their corresponding metadata (i.e.captions and table contents), then we classifying the figures into categories using meta-prompting. All the extracted data is then fed to GPT4o to generate multiple questions-answer pairs per image. Since generative models are prone to errors, we apply several filtering steps, using an LLM and LMM to ensure that our dataset is truly multi-modal and reliable.",
                "position": 212
            }
        ]
    },
    {
        "header": "4Results & Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10783/x4.png",
                "caption": "Figure 4:Performance prediction results of our efficient re-evaluation method on the hypothetical second (next) version of the LiveXiv benchmark (please see text for details). The table on the left shows the mean absolute error (MAE) and Spearman rank correlation when comparing true and predicted accuracies across individual ArXiv domains, while the graph on the right presents the overall benchmark performance. The results demonstrate that re-evaluating only 5 out of 17 models is sufficient to accurately predict the performance of the remaining models, as well as maintain high rank correlation, validating the effectiveness of our approach.",
                "position": 1008
            }
        ]
    },
    {
        "header": "5Limitations and Conclusions",
        "images": []
    },
    {
        "header": "6Ethics Statement",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10783/x5.png",
                "caption": "Figure 5:Domain sensitivity according to domains.We visualize the performance of each model across all domains. Clear trends revealed where old models or models with a small LLM are ”under-fitting” and perform worse across all domains. In the middle we have the mid-level models that are sensitive to the domain, indicating their lack of generalization across domain without any additional training. Lastly the newest models (open-source and proprietary) are robust to domain shifts and present a stable performance across the domains.",
                "position": 1222
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x6.png",
                "caption": "Figure 6:LMMs performance based on domain.To complement our analysis form Figure5we visualize the statistical properties of each domain. One clear trend is that across all modesl, the performance on cs.CV and cs.AI is the most concentrated, hinting lower variance between models.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x7.png",
                "caption": "Figure 7:Domain sensitivity according to domains.As opposed to the high variance some models demonstrated in Figure5, in TQA the tasks and he visual content are more limited thus shrunken the performance variance greatly.",
                "position": 1228
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x8.png",
                "caption": "Figure 8:LMMs performance based on domain.The domains are very similar in their statistical properties showing high variance in performance. This is probably due to wide range of models that differ significantly in their performance.",
                "position": 1231
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x9.png",
                "caption": "Figure 9:A detailed example for VQA questions generation.",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x10.png",
                "caption": "Figure 10:A detailed example for TQA question generation.",
                "position": 2200
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x11.png",
                "caption": "Figure 11:Performance prediction results of our efficient re-evaluation method on the hypothetical second version of the LiveXiv benchmark when re-evaluating 3 models.",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x12.png",
                "caption": "Figure 12:Performance prediction results of our efficient re-evaluation method on the hypothetical second version of the LiveXiv benchmark when re-evaluating 8 models.",
                "position": 2319
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x13.png",
                "caption": "Figure 13:Number of testing samples negatively correlates with prediction error, suggesting that our efficient evaluation strategy will perform even better in practical situations in which test sets are larger. The plots represent the cases for 3, 5, and 8 re-evaluated models.",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x14.png",
                "caption": "",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x15.png",
                "caption": "",
                "position": 2431
            },
            {
                "img": "https://arxiv.org/html/2410.10783/x16.png",
                "caption": "Figure 14:The results for MM-LiveBench are optimistic and we check that our method could be successfully applied in this other context.",
                "position": 2445
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]