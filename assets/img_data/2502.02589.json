[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02589/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02589/x2.png",
                "caption": "Figure 2:Annotation Pipeline.Given an input image, human-annotated panoptic segmentation masks are overlaid using set-of-marks[66]visualization techniques to prompt the vision-language model (VLM). After generating an initial draft, human effort is investigated for editing and verification. Finally, the annotated metadata will be formatted to construct the datasets for various tasks at instruction tuning or finetuning stage.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x3.png",
                "caption": "Figure 3:Designed Prompt Template.By giving the concatenated set-of-marks images, the right side (round-1) shows the initial response and the corresponding human edits. Once finalized by humans, these edits will be merged into a single detailed caption grounded with panoptic segmentation masks, as shown in the left side (round-2).",
                "position": 437
            }
        ]
    },
    {
        "header": "3COCONut-PanCapDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02589/extracted/6178518/figures/nouns_freq.png",
                "caption": "Figure 4:Frequency of Extracted Nouns from the COCONut-PanCap Dataset. The top 10 most frequent nouns are: people, table, room, street, dining, man, person, cars, chairs, and field.",
                "position": 472
            }
        ]
    },
    {
        "header": "4PGC Baseline: PanCaper",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02589/extracted/6178518/figures/user_study.png",
                "caption": "Figure 5:Caption Quality via User Study.The study involved human evaluators assessing a random sample of 1,000 captions, with a strong preference shown for captions from our dataset.",
                "position": 499
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02589/extracted/6178518/figures/fid_capture.png",
                "caption": "Figure 6:Varying Synthetic and Human-Annotation Ratios.CAPTURE is used to evaluate the performance of LLaVA-NeXT on detailed captioning, while FID assesses the performance of SD3-medium on text-conditioned image generation.",
                "position": 1085
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02589/x4.png",
                "caption": "(a)LLaVA-NeXt-AnyRes",
                "position": 2079
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x4.png",
                "caption": "(a)LLaVA-NeXt-AnyRes",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x5.png",
                "caption": "(b)our LLaVA-NeXt-pool",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x6.png",
                "caption": "Figure 8:Architecture of PanCaper.We utilize a pretrained vision encoder from kMaX-DeepLab[67]as our vision backbone, which effectively extracts dense features essential for panoptic segmentation.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x7.png",
                "caption": "Figure 9:Visualization of the Panoptic Grounded Caption.Our annotated captions ground the panoptic segmentation masks.",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x8.png",
                "caption": "Figure 10:Visualization of the Panoptic Grounded Caption.Our annotated captions ground the panoptic segmentation masks.",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x9.png",
                "caption": "Figure 11:Tier Examples for the User Study.Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x10.png",
                "caption": "Figure 12:Tier Examples for the User Study.Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2502.02589/x11.png",
                "caption": "Figure 13:Tier Examples for the User Study.Our COCONut-PanCap annotations are tied with GPT-4V annotations for some simple cases.",
                "position": 2171
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Results",
        "images": []
    }
]