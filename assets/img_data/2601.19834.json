[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19834/x1.png",
                "caption": "Figure 1:Overview of a world-model perspective on multimodal reasoning. (a) Humans construct mental models of the world, representing information and knowledge through two complementary channels–verbal and visual–to support reasoning, planning, and decision-making. (b) Recent advances in large language models (LLMs) and vision language models (VLMs) largely rely on verbal chain-of-thought reasoning, leveraging primarily verbal and symbolic world knowledge. (c) Unified multimodal models (UMMs) open a new paradigm by using visual generation for visual world modeling, advancing more human-like reasoning on tasks grounded in the physical world. Examples of reasoning with verbal world modeling are adapted fromGuo et al. [18], Du et al. [14], Chen et al. [9], Zhang et al. [72].",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3A World Model Perspective on Multimodal Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19834/x2.png",
                "caption": "Figure 2:Theoretical formulation of the world model perspective on multimodal reasoning. (a) Observations of the same underlying world state can span multiple modalities, including verbal and visual observations, each reflecting different views or emphases. (b) Two atomic capabilities of world models are defined:world reconstruction, which infers complete structure from partial observations and enables novel view synthesis, andworld simulation, which models dynamics to predict future observations. (c) Chain-of-thought reasoning includes internal world modeling, by explicitly maintaining an evolving sequence of observations, generated through either of the atomic world model capabilities.",
                "position": 203
            }
        ]
    },
    {
        "header": "4Experiment Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19834/x3.png",
                "caption": "Figure 3:The VisWorld-Eval suite for assessing multimodal reasoning with visual world modeling. VisWorld-Eval comprises seven tasks spanning both synthetic and real-world domains, each designed to isolate and demand specific atomic world-model capabilities.",
                "position": 361
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19834/x4.png",
                "caption": "Figure 4:Performance of SFT-trained UMMs with different world model-based chain-of-thought formulations across seven tasks from VisWorld-Eval. Refer to Table1for zero-shot performance of advanced VLMs.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x5.png",
                "caption": "Figure 5:Probing implicit world models, by training a set of probes, i.e., MLPs which infer the masked point coordinates during reasoning from internal representations.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x6.png",
                "caption": "(a)Sample efficiency.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x6.png",
                "caption": "(a)Sample efficiency.",
                "position": 692
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x7.png",
                "caption": "(b)World model fidelity.",
                "position": 697
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x8.png",
                "caption": "(c)Implicit world modeling.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x9.png",
                "caption": "Figure 7:Performance of SFT-trained VLMs compared with UMMs across three tasks.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x10.png",
                "caption": "Figure 8:Performance of RLVR-trained VLMs and UMMs with different world-model-based CoT formulations across three tasks.",
                "position": 712
            }
        ]
    },
    {
        "header": "6Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19834/x11.png",
                "caption": "Figure 9:Showcases of interleaved verbal-visual chain-of-thought reasoning, generated by post-trained UMMs, where visual generation serves as world models.<image>denotes a placeholder indicating the position of a generated image.",
                "position": 743
            }
        ]
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Theorectical Analysis",
        "images": []
    },
    {
        "header": "8Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19834/x12.png",
                "caption": "Figure 10:Examples of chain-of-thought SFT data for the paper folding task, under visual world modeling (left) and verbal world modeling (right).",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x13.png",
                "caption": "Figure 11:Examples of chain-of-thought SFT data for the ball tracking and multi-hop manipulation task.",
                "position": 2473
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x14.png",
                "caption": "Figure 12:Examples of chain-of-thought SFT data for the maze and sokoban task.",
                "position": 2476
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x15.png",
                "caption": "Figure 13:Examples of chain-of-thought SFT data for the cube 3-view projection task, under visual world modeling (left) and verbal world modeling (right).",
                "position": 2479
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x16.png",
                "caption": "Figure 14:Examples of chain-of-thought SFT data for the real-world spatial reasoning task.",
                "position": 2482
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x17.png",
                "caption": "Figure 15:Showcases of reasoning generateed by post-trained UMMs in the real-world spatial reasoning task. We highlight hallucinations or incorrect reasoning steps in red.",
                "position": 2568
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x18.png",
                "caption": "Figure 16:Showcases of reasoning generated by post-trained UMMs in the paper folding task. We highlight hallucinations or incorrect reasoning steps in red, but also mark correctly generated visual unfolding intermediate steps with green borders.",
                "position": 2571
            },
            {
                "img": "https://arxiv.org/html/2601.19834/x19.png",
                "caption": "Figure 17:Showcases of reasoning generated by post-trained UMMs in the paper folding task. We mark correct and incorrect generated cube views with green and red borders, respectively. For incorrect generations, the corresponding ground-truth views are provided for reference (note that these are shown only for readers and are never provided to the models during reasoning).",
                "position": 2574
            }
        ]
    },
    {
        "header": "9Extended Experimental Results",
        "images": []
    }
]