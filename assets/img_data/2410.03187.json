[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/x1.png",
                "caption": "Figure 1.AutonomousHSIsynthesis. Our proposed method generates realistic character motion in 3D scenes based on a single textual instruction and goal location, incorporating seamless transitions between locomotion andHOIautonomously.",
                "position": 197
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/x2.png",
                "caption": "Figure 2.Overview of our method. Our method uses an auto-regressive diffusion model that generates the next motion segment based on existing motions (Section3.2). The 3D environment is captured through a dual voxel scene encoder (Section3.3). The text instructions are encoded with the time frame to provide precise and time-specific semantic guidance (Section3.4). The goal encoder (Section3.5) embeds the sub-goal locations for different interaction stages, which are automatically determined by our autonomous scheduler (Section3.6).",
                "position": 256
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/x3.png",
                "caption": "Figure 3.Comparison results.We qualitatively compare our method with TRUMANS(Jiang etÂ al.,2024). The left side shows the locomotion along a trajectory, and the right side shows the interaction of sitting on the sofa. Our method generates characters that actively avoid penetrating the scene and exhibit natural cues of scene awareness. For more qualitative results, we refer readers to the supplementary video.",
                "position": 311
            }
        ]
    },
    {
        "header": "4.LINGO dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/extracted/5910568/selected_scenes.png",
                "caption": "(a)Selected frames from the LINGO dataset",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2410.03187/extracted/5910568/selected_scenes.png",
                "caption": "(a)Selected frames from the LINGO dataset",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2410.03187/extracted/5910568/mocap.jpg",
                "caption": "(b)MoCapsetup",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2410.03187/extracted/5910568/vr.jpg",
                "caption": "(c)View in VR",
                "position": 425
            }
        ]
    },
    {
        "header": "5.Experiments",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/x4.png",
                "caption": "(a)",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2410.03187/x4.png",
                "caption": "(a)",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2410.03187/x5.png",
                "caption": "(b)",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2410.03187/x6.png",
                "caption": "(c)",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2410.03187/x7.png",
                "caption": "(d)",
                "position": 1276
            },
            {
                "img": "https://arxiv.org/html/2410.03187/x8.png",
                "caption": "Figure 6.Number of occurrences of each motion type in LINGO dataset.",
                "position": 1283
            },
            {
                "img": "https://arxiv.org/html/2410.03187/extracted/5910568/wordcloud.png",
                "caption": "Figure 7.Word cloud built from the language annotations in the LINGO dataset.",
                "position": 1288
            }
        ]
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/x9.png",
                "caption": "Figure A1.Motion Planner.A Markov Chain generates the next instruction to guarantee plausible interaction and maintain a balanced distribution of motion types. The Motion Planner provides language instructions to the Actor.",
                "position": 1350
            }
        ]
    },
    {
        "header": "Appendix BLINGO Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03187/x10.png",
                "caption": "Figure A2.Motion length distribution of each motion type in LINGO dataset.",
                "position": 1429
            },
            {
                "img": "https://arxiv.org/html/2410.03187/x11.png",
                "caption": "Figure A3.Distribution of goal locations for all locomotion clips in the local coordinate system of the first frame.The character is aligned to initially face the y-axis direction. Unit: meter.",
                "position": 1432
            }
        ]
    },
    {
        "header": "Appendix CTask Planner",
        "images": []
    }
]