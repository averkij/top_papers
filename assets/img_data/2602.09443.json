[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09443/x1.png",
                "caption": "Figure 1:P1-VL-235B-A22Bstands as the state-of-the-art open-source VLM in the Physics Olympiad benchmark (HiPhO), placing No.3 behind Gemini-3-Pro(high) and GPT-5.2(high) and achieving 12 gold medals. Even at mid-scale,P1-VL-30B-A3Bachieved 9 gold medals, with a higher average score than most of the open-source models except P1-235B-A22B and DeepSeek-V3.2-Thinking. With the PhysicsMinions agent framework,P1-VL-235B-A22B+PhysicsMinionsranks No.2 on HiPhO.",
                "position": 291
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09443/x2.png",
                "caption": "Figure 2:A question sample from the International Physics Olympiad 2025 (IPhO 2025), where the question requires measuring the radius of bubbles and estimating their velocity in Fig 2.",
                "position": 359
            }
        ]
    },
    {
        "header": "2Physics Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09443/x3.png",
                "caption": "Table 1:Statistics of the multi-modal training data.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2602.09443/x3.png",
                "caption": "Figure 3:Distribution of the training data.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2602.09443/x4.png",
                "caption": "Figure 4:Data collection pipeline for physics data.",
                "position": 536
            }
        ]
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09443/x5.png",
                "caption": "Figure 6:The training dynamics of P1-VL curriculum RL. The upper side shows the dynamics of P1-VL-30B-A3B, and the lower shows those of P1-VL-235B-A22B. The training is set in three stages with curriculum difficulty expansion, group size expansion, and generation window expansion. The average response length represents the generation length during training, and the overall points refer to the sum score of all the exams in HiPhO with model-based verifier (Qwen-3-30B-Instruct-2507).",
                "position": 873
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09443/x6.png",
                "caption": "Figure 7:Evaluation of P1-VL models against their respective base models on out-of-domain benchmarks. The evaluation suite encompasses both text-only and multi-modal tasks, spanning mathematical reasoning and broader STEM disciplines.",
                "position": 2320
            },
            {
                "img": "https://arxiv.org/html/2602.09443/x7.png",
                "caption": "Figure 8:Driven by training-inference mismatch, the RL training of Qwen3-VL-30B-A3B-Thinking MoE model suffers from severe collapse. The implementation of Masked Importance Sampling (MIS) proves effective in stabilizing the training.",
                "position": 2332
            },
            {
                "img": "https://arxiv.org/html/2602.09443/x8.png",
                "caption": "Figure 9:Comparison of Qwen3-VL-4B-Thinking training performance on the image-text data versus the mixed data (text-only + image-text).",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2602.09443/x9.png",
                "caption": "Figure 10:Training dynamics of RL training with and without curriculum difficulty expansion.",
                "position": 2357
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowlegement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09443/figs/app_fig1.png",
                "caption": "",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2602.09443/figs/app_fig2.png",
                "caption": "",
                "position": 2574
            },
            {
                "img": "https://arxiv.org/html/2602.09443/figs/app_fig3.png",
                "caption": "",
                "position": 2581
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]