[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09330/x1.png",
                "caption": "Figure 1:Comparing unlearning approaches.Previous works assume the forget set to be uniformly distributed. However, real-life unlearning requests do not comply with the uniform distribution assumption[3]. If the forget set distribution is predominant in some groups (e.g., old males), it can lead to performance degradation in such dominant forget groups (i.e., the blue group in the figure).\nGroup-robust Unlearning prevents this from happening.",
                "position": 86
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09330/x2.png",
                "caption": "Figure 2:Unlearning non-uniformly distributed data.We test standard model retraining, and popular approximate unlearning methods (L1-sparse[23],SalUn[13], andSCRUB[28]) in group-robust unlearning.\nThe moreattractivemalesare unlearned from CelebA[31], the lower the model accuracy on that group.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2503.09330/x3.png",
                "caption": "Figure 3:reweightvs.group-DRO[42].Retrain+reweightachieves a better test and group accuracy alignment with the original model (higher is better). Thus, it better preserves original performance after unlearning.",
                "position": 280
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09330/x4.png",
                "caption": "Figure 4:reweightfor group-robust unlearning.As inFig.2, we test different methods andreweightin group-robust unlearning on CelebA[31]. Darker colors are used for methods without the reweighting, while lightened ones correspond to methods coupled withreweight.\nAs the unlearning ratio grows, methods GA degrade. Instead, addingreweightrestores the original GA.",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2503.09330/x5.png",
                "caption": "Figure 5:Group-robust unlearning across different unlearning ratios.We compareL1-sparse[23],SalUn[13], andSCRUB[28]against our approach while using thereweightstrategy on all methods.MIUachieves overall the best Avg. Gap when varying the unlearning ratio.",
                "position": 2381
            },
            {
                "img": "https://arxiv.org/html/2503.09330/x6.png",
                "caption": "Figure 6:Sampling the forget set from multiple groups.We evaluate our method againstL1-sparse[23],SalUn[13], andSCRUB[28]when the forget set is sampled from multiple FairFace[24]groups.MIUis more consistent across experiments, always achieving the best result.",
                "position": 2398
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09330/x7.png",
                "caption": "Figure 7:Unlearning non-uniformly sampled data.We test standard model retraining, and popular approximate unlearning methods (L1-sparse[23],SalUn[13],SCRUB[28]) in group-robust unlearning. The more samples from a specified group are unlearned, the lower the model accuracy on that group. While the drop is more evident in CelebA[31], methods also show significant performance degradation in Waterbirds[42]and FairFace[24]overall.",
                "position": 3715
            }
        ]
    },
    {
        "header": "Appendix AImplementation details",
        "images": []
    },
    {
        "header": "Appendix BMetrics",
        "images": []
    },
    {
        "header": "Appendix CAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09330/x8.png",
                "caption": "Figure 8:Ablating parameterλ𝜆\\lambdaitalic_λ.MIUAvg. Gap when varying parameterλ𝜆\\lambdaitalic_λin CelebA[31], Waterbirds[42], and FairFace[24]. Whileλ=1𝜆1\\lambda=1italic_λ = 1is optimal in CelebA[31]and FairFace[24], Waterbirds[42]benefits from higher lambdas.",
                "position": 10193
            }
        ]
    },
    {
        "header": "Appendix DLimitations and negative societal impacts",
        "images": []
    }
]