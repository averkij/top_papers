[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x1.png",
                "caption": "Figure 1:Overview of the conversion from a vanillaN-layer Transformer to a Recursive Transformer withN/KùëÅùêæN/Kitalic_N / italic_Kblocks ofKshared layers. The Recursive Transformer is obtained by repeating a single block ofKlayers multiple times, resulting in a looped architecture. The Recursive Transformer can also be converted into a Relaxed Recursive Transformer by adding layer-specific LoRA modules. This preserves many of the advantages of weight sharing, but also allows for better performance.",
                "position": 229
            }
        ]
    },
    {
        "header": "2Effective Model Compression with Recursive Patterns",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x2.png",
                "caption": "Figure 2:Left:An example of unshared, full-size model with 6 layers.Middle:Three proposed methodologies for initializing looped layers in a Recursive Transformer.\nEach layer number indicates the source layer in the full-size model used for initialization.Right:Example of a Relaxed Recursive Transformer initialized by SVD method.\nHere, looped layers are initialized using the Average method.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x3.png",
                "caption": "(a)Vanilla Batching",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x3.png",
                "caption": "(a)Vanilla Batching",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x4.png",
                "caption": "(b)Depth-wise Batching",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x5.png",
                "caption": "(c)With Early-exiting",
                "position": 422
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x6.png",
                "caption": "(a)Gemma",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x6.png",
                "caption": "(a)Gemma",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x7.png",
                "caption": "(b)TinyLlama",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x8.png",
                "caption": "(c)Pythia",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x9.png",
                "caption": "(a)Loss curves for Gemma",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x9.png",
                "caption": "(a)Loss curves for Gemma",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x10.png",
                "caption": "(b)Average few-shot performance",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x11.png",
                "caption": "(c)Recursive model performance",
                "position": 807
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x12.png",
                "caption": "(a)Loss changes in Gemma",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x12.png",
                "caption": "(a)Loss changes in Gemma",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x13.png",
                "caption": "(b)Accuracy gains from relaxation",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x14.png",
                "caption": "(c)Effects of SVD initialization",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x15.png",
                "caption": "(a)Performance by LoRA rank",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x15.png",
                "caption": "(a)Performance by LoRA rank",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x16.png",
                "caption": "(b)Gains from longer KD training",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x17.png",
                "caption": "(c)Overall performance",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x18.png",
                "caption": "Figure 8:Continuous depth-wise batching (CDB) with early exiting enables Recursive Transformers to theoretically achieve significant throughput improvements.\nThroughput‚Äâ(tokens/sec) was averaged across SlimPajama, RedPajama, and PG19, and then normalized to the throughput of the vanilla Pythia model. The accompanying table gives detailed throughout and performance measurements for Gemma.ŒîVsubscriptŒîùëâ\\Delta_{V}roman_Œî start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPTmeasures throughput relative to the vanilla Gemma model, whileŒîS‚Å¢e‚Å¢qsubscriptŒîùëÜùëíùëû\\Delta_{Seq}roman_Œî start_POSTSUBSCRIPT italic_S italic_e italic_q end_POSTSUBSCRIPTmeasures throughput relative to the vanilla Gemma model with continuous sequence-wise batching (CSB).",
                "position": 1148
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComponents in Transformer Architecture",
        "images": []
    },
    {
        "header": "Appendix BParameter Sharing Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x19.png",
                "caption": "(a)SEQUENCE",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x19.png",
                "caption": "(a)SEQUENCE",
                "position": 3165
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x20.png",
                "caption": "(b)CYCLE",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x21.png",
                "caption": "(c)CYCLE‚Äâ(REV)",
                "position": 3175
            }
        ]
    },
    {
        "header": "Appendix CIllustrative Examples of SVD Initialization in Relaxed Recursive Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x22.png",
                "caption": "Figure C.1:We visualize LoRA modules to show which residual matrices they target for initialization\nunder three different looping initialization methods, assuming a full-size model with six layers and two looping blocks. For ease of understanding,Amatrices are colored according to the full-size model weights at the corresponding depth, whileBmatrices are colored based on the looped layer weights. WhiteBmatrices indicate cases where the full-size model and looped model weights are identical, resulting in standard zero initialization.",
                "position": 3193
            }
        ]
    },
    {
        "header": "Appendix DOverview of Three Pretrained LLMs",
        "images": []
    },
    {
        "header": "Appendix EExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix FExpanded Results of Initialization Methods for Looped Layers",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x23.png",
                "caption": "(a)Stepwise ablations",
                "position": 3387
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x23.png",
                "caption": "",
                "position": 3390
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x24.png",
                "caption": "(a)Stepwise ablations",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x25.png",
                "caption": "(b)Average ablations",
                "position": 3400
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x26.png",
                "caption": "(a)Gemma (2 blocks)",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x26.png",
                "caption": "",
                "position": 3420
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x27.png",
                "caption": "(a)Gemma (2 blocks)",
                "position": 3425
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x28.png",
                "caption": "(b)Gemma (3 blocks)",
                "position": 3430
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x29.png",
                "caption": "(c)TinyLlama (2 blocks)",
                "position": 3435
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x30.png",
                "caption": "(d)Pythia (2 blocks)",
                "position": 3440
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x31.png",
                "caption": "(a)Recursive Gemma with 2 blocks",
                "position": 3941
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x31.png",
                "caption": "(a)Recursive Gemma with 2 blocks",
                "position": 3944
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x32.png",
                "caption": "(b)Recursive Gemma with 3 blocks",
                "position": 3950
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x33.png",
                "caption": "(c)Recursive TinyLlama with 2 blocks",
                "position": 3956
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x34.png",
                "caption": "(d)Recursive Pythia with 2 blocks",
                "position": 3962
            }
        ]
    },
    {
        "header": "Appendix GExpanded Results of Relaxed Recursive Transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x35.png",
                "caption": "(a)Gemma",
                "position": 3981
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x35.png",
                "caption": "",
                "position": 3984
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x36.png",
                "caption": "(a)Gemma",
                "position": 3989
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x37.png",
                "caption": "(b)TinyLlama",
                "position": 3994
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x38.png",
                "caption": "(c)Pythia",
                "position": 3999
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x39.png",
                "caption": "(a)Gemma",
                "position": 4016
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x39.png",
                "caption": "(a)Gemma",
                "position": 4019
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x40.png",
                "caption": "(b)TinyLlama",
                "position": 4024
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x41.png",
                "caption": "(c)Pythia",
                "position": 4029
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x42.png",
                "caption": "(a)Gemma",
                "position": 4044
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x42.png",
                "caption": "",
                "position": 4047
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x43.png",
                "caption": "(a)Gemma",
                "position": 4052
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x44.png",
                "caption": "(b)TinyLlama",
                "position": 4057
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x45.png",
                "caption": "(c)Pythia",
                "position": 4062
            }
        ]
    },
    {
        "header": "Appendix HRelaxation of Parameter Sharing with Prefix Tuning",
        "images": []
    },
    {
        "header": "Appendix IExpanded Results of Extended Uptraining and Knowledge Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20672/x46.png",
                "caption": "(a)Gemma",
                "position": 6884
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x46.png",
                "caption": "",
                "position": 6887
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x47.png",
                "caption": "(a)Gemma",
                "position": 6892
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x48.png",
                "caption": "(b)TinyLlama",
                "position": 6897
            },
            {
                "img": "https://arxiv.org/html/2410.20672/x49.png",
                "caption": "(c)Pythia",
                "position": 6902
            }
        ]
    },
    {
        "header": "Appendix JExpanded Results of Early-Exit Training",
        "images": []
    },
    {
        "header": "Appendix KExpanded Results of Hypothetical Generation Speedup",
        "images": []
    }
]