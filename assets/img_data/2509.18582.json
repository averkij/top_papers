[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Learning with Photography Enthusiasts and Professionals",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18582/x1.png",
                "caption": "Figure 1:An example from the dpchallenge platform. 3 of the total 64 comments for the photo are presented for illustration.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x2.png",
                "caption": "Figure 2:Data Generation Pipeline. The generation of one critique from a group of comments of one image is shown as an example.",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x3.png",
                "caption": "Figure 3:Top:Examples of PhotoCritique.Bottom:Comparison with examples in Q-Instruct[30].",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x4.png",
                "caption": "Figure 4:Comparison of description length Distributions of aesthetic comments in PhotoCritique and Q-Instruct. Scale of PhotoCritique isBlueand that of Q-Instruct isRed.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x5.png",
                "caption": "Figure 5:Top:Distribution of Top 40 categories.Bottom:Examples of photos from a subset of categories.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x5.png",
                "caption": "",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x6.png",
                "caption": "",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x7.png",
                "caption": "Figure 6:Framework of PhotoEye. Given an input photo and a language instruction, visual features from different visual encoders are fused in our vision fusor with language guidance, then fed into the LLM with language embeddings to generate LLM response.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Improving Visual Aesthetics Perception",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18582/x8.png",
                "caption": "Figure 7:Comparison of aesthetics-related visual feature discriminability of ours and CLIP[15]in existing works[8,30,38].",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2509.18582/x9.png",
                "caption": "Figure 8:We show one rejected example, several accepted examples, topic distribution, and some sub-topics of a subset of topics of questions in PhotoBench.",
                "position": 344
            }
        ]
    },
    {
        "header": "5Benchmarking Aesthetic Visual Understanding with Professionals",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18582/x10.png",
                "caption": "Figure 9:Examples in related works from their original papers.",
                "position": 954
            },
            {
                "img": "https://arxiv.org/html/2509.18582/sec/img/layer.jpg",
                "caption": "Figure 10:Weights of vision encoders across different layers.",
                "position": 966
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]