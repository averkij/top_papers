[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x1.png",
                "caption": "Figure 1:The overview of self-invoking code generation in HumanEval Pro and MBPP Pro. Given a base problem and a related, more complex problem, they are required to solve the base problem and use its solution to address the complex problems.",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x2.png",
                "caption": "Figure 2:The overview of benchmark construction. An example is shown inFigure8. We summarize the entire benchmark construction process as follows:\n(1)Self-invoking problem Generation: We use Deepseek-V2.5 to generate the self-invoking problems, as well as their candidate solutions and test inputs.\n(2)Solutions Generation: We execute the generated solution with the test inputs in a controlled Python environment to obtain ground truth outputs.\n(3)Test Cases Generation:\nWe employ an iterative method involving Python execution check and manual review to ensure that all test cases pass successfully.\nThe final execution results are then used to construct complete test cases withassertcommand.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmark Construction",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x3.png",
                "caption": "Figure 3:Performance Comparison: HumanEval Pro (and MBPP Pro) vs. HumanEval (and MBPP).",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x4.png",
                "caption": "Figure 4:HumanEval (or MBPP) scores against the results on HumanEval Pro and MBPP Pro (HumanEval+ and MBPP+). We presents the comparison between base model and instruct model.",
                "position": 564
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x5.png",
                "caption": "(a)Qwen2.5-Coder-7B-base",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x5.png",
                "caption": "(a)Qwen2.5-Coder-7B-base",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x6.png",
                "caption": "(b)Qwen2.5-Coder-32B-base",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x7.png",
                "caption": "(c)Qwen2.5-Coder-7B-instruct",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x8.png",
                "caption": "(d)Qwen2.5-Coder-32B-instruct",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x9.png",
                "caption": "Figure 6:Error types of GPT-4o with and without CoT reasoning on HumanEval Pro.",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x10.png",
                "caption": "Figure 7:Statistics of error type across different LLMs on HumanEval Pro and MBPP Pro. We sum up all kinds of errors on the two benchmarks. Exact number is shown inTable9.",
                "position": 781
            }
        ]
    },
    {
        "header": "6Generalization Study of Self-invoking Code Generation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results",
        "images": []
    },
    {
        "header": "Appendix BExample in Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x11.png",
                "caption": "Figure 8:An example of self-invoking problems in HumanEval\nPro",
                "position": 1774
            }
        ]
    },
    {
        "header": "Appendix CModel Information",
        "images": []
    },
    {
        "header": "Appendix DComparison between HumanEval (Pro), MBPP (Pro) and BigCodeBench-Lite (Pro)",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x12.png",
                "caption": "Figure 9:Comparison between HumanEval Family, MBPP Family and BigCodeBench-Lite Family.",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x13.png",
                "caption": "",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x14.png",
                "caption": "",
                "position": 1949
            }
        ]
    },
    {
        "header": "Appendix EDiscussion about Self-invoking Problems and Solutions",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21199/x15.png",
                "caption": "Figure 10:Complexity comparison between base problem and self-invoking problem. We use the line count of the canonical solution for both the base problem and the self-invoking problem as a measure of the problemâ€™s complexity.",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2412.21199/x16.png",
                "caption": "",
                "position": 1965
            }
        ]
    },
    {
        "header": "Appendix FPrompts",
        "images": []
    },
    {
        "header": "Appendix GExamples of Different Error Types",
        "images": []
    },
    {
        "header": "Appendix HError Statistics across Different Models",
        "images": []
    }
]