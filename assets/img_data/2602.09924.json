[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Predicting Difficulty",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09924/figs/E2H-AMC_GPT_Reasoning_Plot.png",
                "caption": "Figure 1:Human and model difficulty diverge with increased reasoning.On E2H-AMC, as the reasoning level in GPT-OSS-20B is increased, difficulty becomes less human-aligned and more model-specific.Left: (A)Alignment between probe-predicted model difficulty and human IRT difficulty decreases with higher reasoning, indicating that correctness-related signals become less linearly accessible as models solve questions that are typically difficult for humans.Right: (B)Despite this divergence, probe-based predictions consistently outperform human difficulty for predicting Maj@5 failure across reasoning modes, demonstrating that internal activations encode a model-relative notion of difficulty that is distinct from human difficulty.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2602.09924/figs/divergence_human_llm_difficulty_plot.png",
                "caption": "Figure 2:Chain-of-thought length tracks human difficulty but diverges from model success.We plot binned chain-of-thought length (total output tokens, log-scale) against expected values (means) of normalized human IRT difficulty, empirical correctness, empirical success rates, and probe-predicted success (SR@5 and Maj@5) for GPT-OSS-20B at low, medium, and high reasoning modes.\nAcross all settings, output length is positively correlated with human difficulty and negatively correlated with both empirical and predicted success.\nThis effect strengthens with increased reasoning mode, indicating that extended reasoning causes models to allocate more computation to problems humans find difficult, even when those problems are unlikely to be failed. thereby decoupling generation length from model-relative uncertainty.",
                "position": 632
            }
        ]
    },
    {
        "header": "4Probe-Guided Routing",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.09924/figs/pareto_cascade_MATH_7b_gpt_20b.png",
                "caption": "Figure 3:Probe-based routing achieves strong performance-cost tradeoffs on MATH.Left (Cascade):Binary routing between Qwen2.5-Math-7B-Instruct and GPT-OSS-20B-medium.\nThe cascade strategy (red curve) substantially outperforms random routing (gray diamond) across the Pareto frontier, matching GPT-OSS-20B-medium accuracy (orange circle) at 17% lower cost.Right (Utility):Model selection from a pool of five models with varying capabilities and costs.\nThe utility router (red curve) achieves a Pareto improvement over all single-model baselines, exceeding GPT-OSS-20B-high accuracy (red circle) while reducing cost by approximately 70%.\nBoth strategies route difficult queries (lowp^\\hat{p}) to more capable models.\nOracle performance (gold star) represents an upper bound with perfect difficulty prediction.\nAll results use maj@5 withK=5K=5generations.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2602.09924/figs/pareto_route_utility_MATH_platt_n7b.png",
                "caption": "",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2602.09924/figs/pareto_route_utility_AIME_platt_n7b.png",
                "caption": "Figure 4:Probe-based routing generalizes across diverse reasoning benchmarks.Left (AIME 2025):Utility-based routing on a challenging competition mathematics benchmark. The router (red curve) traces a Pareto frontier that dominates all individual models, matching GPT-OSS-20B-high’s 93.3% accuracy (red circle) at approximately 37% lower cost ($1.15 vs $1.75). The oracle (gold star, 95.6%) represents the theoretical upper bound with perfect prediction, while oracle utility (blue dashed line) shows the cost-optimal oracle policy. Our router matches oracle accuracy but at a higher cost.Right (GSM8K):Utility-based routing on a saturated benchmark with models achieving high accuracies. The router (red curve) substantially outperforms random routing (gray diamond) and efficiently identifies the cost-optimal operating point near Math-7B (red crosses, 94.5%). In this saturated regime, the router correctly avoids expensive high-reasoning models (GPT-OSS-20B-high: 94.4% at $2.4) in favor of the cheaper Math-7B with comparable accuracy—demonstrating cost-aware selection rather than simple accuracy maximization. Both benchmarks show that probe-guided routing adapts to difficulty distributions: preferring stronger models when accuracy varies widely (AIME), and selecting efficient models when performance plateaus (GSM8K). All results use maj@5 withK=5K{=}5generations. The full table of results is in the Appendix Section5.5.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2602.09924/figs/pareto_route_utility_GSM8K_platt_n7b.png",
                "caption": "",
                "position": 757
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "5Appendix",
        "images": []
    }
]