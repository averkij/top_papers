[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary Analyses: Are Current LLMs Aware of Environment Dynamics in Web Navigation?",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13232/x1.png",
                "caption": "Figure 1:LLMs‚Äô performance in next state prediction.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x2.png",
                "caption": "Figure 2:LLMs‚Äô performance in action selection (w/ and w/o next states).",
                "position": 305
            }
        ]
    },
    {
        "header": "4World-Model-Augmented Web Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13232/x3.png",
                "caption": "Figure 3:Framework overview. We first collect training data for world models (top).\nAfter training, we perform policy optimization by selecting the action leading to an optimal next state (bottom).",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x4.png",
                "caption": "Figure 4:Sequence length distribution of different observation representations.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x5.png",
                "caption": "Figure 5:The overview of transition-focused observation abstraction.",
                "position": 423
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13232/x6.png",
                "caption": "Figure 6:Ablation on the number of sampled actions (kùëòkitalic_k).",
                "position": 976
            }
        ]
    },
    {
        "header": "6Further Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13232/x7.png",
                "caption": "Figure 7:Statistics of error types in erroneous observations predicted byœïitalic-œï\\phiitalic_œï.",
                "position": 1055
            }
        ]
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details of Preliminary analyses",
        "images": []
    },
    {
        "header": "Appendix CImplementation details",
        "images": []
    },
    {
        "header": "Appendix DDetails of further analyses",
        "images": []
    },
    {
        "header": "Appendix EExamples of Successful Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13232/x8.png",
                "caption": "Figure 8:Human annotation interface for preliminary analysis I insection3.1.",
                "position": 2120
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x9.png",
                "caption": "Figure 9:Erroneous example (Low competence in web elements/functions).Although the agent does not delete old texts on the search bar before entering the new keyword ‚Äòrestaurants near CMU Posner Hall‚Äô, the world model still expects the next observation to show the desired search results.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2410.13232/extracted/5924365/figure/next_state_hallucinations.png",
                "caption": "Figure 10:Erroneous example (Counterfactual imagination).The model predicts that specific products (96 TY CITY86 Bmw 740i Limited Collector Hoodie Men‚Äòs Close; Toyota 86 Bad Institute Monkey Champagne Cup, Volkswagen A9 Bug Pick Dead Red) will appear in the next observation, while this specific page does not list them as the products for sell.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2410.13232/extracted/5924365/figure/generic.png",
                "caption": "Figure 11:Erroneous example (Correct yet overly generic statements).‚ÄúComprehensive layout‚Äù and ‚Äùvarious order-related functionalities‚Äù are ambiguous and unclear expressions.",
                "position": 2129
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x10.png",
                "caption": "Figure 12:Erroneous example (Others).The predicted next state (i.e.,¬†contributions and activities) is actually several steps further away from the current time step.",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x11.png",
                "caption": "Figure 13:Successful example (Mind2Web).WMA web agent successfully inferences on the Mind2Web benchmark (menards task #0). Using the policy model (i.e.,¬†GPT-4o), WMA web agent selects the most proper actionclick [208]by leveraging its learned environment dynamics.",
                "position": 2135
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x12.png",
                "caption": "Figure 14:Successful example (WebArena).WMA web agent successfully inferences on Gitlab domain in the WebArena benchmark (instance #175). Using the policy model (i.e.,¬†GPT-4o), WMA web agent selects the most proper actionclick [88]by leveraging its learned environment dynamics.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x13.png",
                "caption": "Figure 15:The prompt for preliminary analysis I insection3.1: Next state prediction",
                "position": 2141
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x14.png",
                "caption": "Figure 16:The prompt for preliminary analysis II insection3.2: Action selection w/o next state",
                "position": 2144
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x15.png",
                "caption": "Figure 17:The prompt for preliminary analysis II insection3.2: Action selection w/ next state",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x16.png",
                "caption": "Figure 18:The prompt for refining TaO output before generating final Transition-focused observation abstraction insection4.1.2",
                "position": 2150
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x17.png",
                "caption": "Figure 19:The prompt for transition-focused observation abstraction insection4.1.2",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x18.png",
                "caption": "Figure 20:The prompt used for the next state prediction of the world modelsection4.2",
                "position": 2156
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x19.png",
                "caption": "Figure 21:The prompt for reward calculation using the value function insection4.2",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x20.png",
                "caption": "Figure 22:The prompt used for baseline comparison with accessibility tree input using CoT insection5.4",
                "position": 2162
            },
            {
                "img": "https://arxiv.org/html/2410.13232/x21.png",
                "caption": "Figure 23:The prompt for self-refine insection6.1",
                "position": 2165
            }
        ]
    },
    {
        "header": "Appendix FPrompts",
        "images": []
    }
]