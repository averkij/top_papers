[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11647/x1.png",
                "caption": "Figure 1:Overview of our proposed IVEBench.1)We construct a diverse video corpus consisting of 600 high-quality source videos systematically organized across 7 semantic dimensions.2)For source videos, we design carefully crafted edit prompts, covering 8 major editing task categories with 35 subcategories.3)We establish a comprehensive three-dimensional evaluation protocol comprising 12 metrics, enabling human-aligned benchmarking of state-of-the-art IVE methods.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3IVEBenchDatabase",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11647/x2.png",
                "caption": "Figure 2:Data acquisition and processing pipeline ofIVEBenchincludes:1)Curation process to 600 high-quality diverse videos.2)Well-designed pipeline for comprehensive editing prompts.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2510.11647/x3.png",
                "caption": "Figure 3:Statistical distributions of IVEBench.",
                "position": 186
            }
        ]
    },
    {
        "header": "4Comprehensive Metrics ofIVEBench",
        "images": []
    },
    {
        "header": "5Discussion with Recent Video Editing Benchmarks",
        "images": []
    },
    {
        "header": "6Benchmarking Video Editing Method in IVEBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11647/x4.png",
                "caption": "Figure 4:IVEBench Evaluation Results of Video Editing Models.We visualize the evaluation results of four IVE models in 12 IVEBench metrics. We normalize the results per dimension for clearer comparisons. For comprehensive numerical results, please refer toTab.Ëœ2.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2510.11647/x5.png",
                "caption": "Figure 5:Qualitative comparison of state-of-the-art IVE methods.",
                "position": 836
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix ADescriptions of Various Categories of Edit Prompts",
        "images": []
    },
    {
        "header": "Appendix BMotion Fidelity Computation Details",
        "images": []
    },
    {
        "header": "Appendix CUnified Scoring Formulation and Details",
        "images": []
    },
    {
        "header": "Appendix DExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11647/x6.png",
                "caption": "Figure A1:Visualization of Model Output Comparison.We concatenate the first, middle, and last frames of the video to facilitate comparison of the temporal performance across different models.",
                "position": 2457
            }
        ]
    },
    {
        "header": "Appendix EDetailed quantitative comparison and analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11647/figs/userstudy.png",
                "caption": "Figure A2:Human Annotation Interface for Benchmark Validation.The interface presents the source video, the editing instruction, and the outputs of different models under a specified evaluation dimension, enabling annotators to conduct pairwise comparisons and judge which video better satisfies the given criterion.",
                "position": 2474
            }
        ]
    },
    {
        "header": "Appendix FHuman Alignment Details",
        "images": []
    },
    {
        "header": "Appendix GThe Use of Large Language Models",
        "images": []
    }
]