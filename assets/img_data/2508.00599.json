[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/x1.png",
                "caption": "",
                "position": 124
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/x2.png",
                "caption": "Figure 2:Overview of the DPoser-regularized optimization framework. Task inputs (e.g., 2D keypoints in human mesh recovery) and current poses are used to compute the measurement loss based on the degradation patternùíú‚Äã(‚ãÖ)\\mathcal{A}(\\cdot)caligraphic_A ( ‚ãÖ )(e.g., camera projection).\nMeanwhile, DPoser regularization introduces noise to the current pose and applies a one-step denoiser to compute DPoser lossLDPoserL_{\\text{DPoser}}italic_L start_POSTSUBSCRIPT DPoser end_POSTSUBSCRIPT.",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x3.png",
                "caption": "Figure 3:Illustration of the rationale behind the proposed truncated timestep scheduling. We employ the DDIM sampler[60]with limited steps and visualize the generated poses. Our observations reveal that pose refinement occurs at later timesteps.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x4.png",
                "caption": "Figure 4:Overview of the DPoser-X methodology. (a) The whole-body network consists of frozen part-only networks, and a fused module trained on whole-body datasets. (b) The mixed training strategy utilizes part-only datasets by applying loss only to available parts. To prevent arbitrary predictions on unavailable parts, the whole-body data is sometimes randomly masked, and loss is applied to all parts.",
                "position": 449
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/GANS.png",
                "caption": "(a)",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/GANS.png",
                "caption": "(a)",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/PoseNDF.png",
                "caption": "(b)",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/NRDF.png",
                "caption": "(c)",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/VPoser.png",
                "caption": "(d)",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/DPoser.png",
                "caption": "(e)",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/DPoser2.png",
                "caption": "(f)",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x5.png",
                "caption": "Figure 6:Visualization of human mesh recovery results (body only) on EHF[51]when fitting from scratch.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x6.png",
                "caption": "Figure 7:Visualization of face reconstruction on the NOW benchmark[57]. (a) Fitting from scratch. (b) Initialization using MICA[75]. *MICA predicts only face shape without expressions; translational and global orientation are fitted for visualization.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x7.png",
                "caption": "Figure 8:Visualization of whole-body mesh recovery on the ARCTIC dataset[16]. DPoser-X can recover plausible whole-body poses from imperfect 2D keypoints detected by RTMPose[28], while other methods struggle to handle noisy inputs effectively.",
                "position": 1228
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "5Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ARelated Work",
        "images": []
    },
    {
        "header": "BParameterization of Score-based Diffusion Models",
        "images": []
    },
    {
        "header": "CView DPoser as Score Distillation Sampling",
        "images": []
    },
    {
        "header": "DRuntime Comparison",
        "images": []
    },
    {
        "header": "EAnalysis of Test-time Timestep Scheduling",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Truncation/traj.png",
                "caption": "Figure S-1:Visualization of the impact of different timestep values in DPoser regularization. A largerttitalic_teffectively corrects undesirable poses but may excessively alter well-posed inputs, resulting in plausible yet unrelated poses. Conversely, a smallerttitalic_tbetter preserves the original pose but struggles to correct implausible ones.",
                "position": 2760
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x8.png",
                "caption": "Figure S-2:Image inpainting using standard and truncated timestep scheduling. The process evolution is shown over iterations with the middle row depicting the log-magnitude spectrum and the bottom row the phase spectrum. (a) The standard scheduling exhibits cohesive restoration with detail fidelity. (b) The truncated scheduling results in detail-rich patches that are perceptually incongruent with the original image context.",
                "position": 2800
            }
        ]
    },
    {
        "header": "FDataset Description",
        "images": []
    },
    {
        "header": "GExperimental Details",
        "images": []
    },
    {
        "header": "HAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/x9.png",
                "caption": "Figure S-3:Visual comparisons of body pose completion. Three hypotheses are drawn for each method. DPoser uniquely offers multiple plausible solutions for partial poses, a scenario where competitors often struggle due to limited generalization.",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Face/generation/vposer_merged_image.png",
                "caption": "(a)",
                "position": 3812
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Face/generation/vposer_merged_image.png",
                "caption": "(a)",
                "position": 3815
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Face/generation/dposer_merged_image.png",
                "caption": "(b)",
                "position": 3820
            }
        ]
    },
    {
        "header": "IAblated DPoser‚Äôs Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/x10.png",
                "caption": "Figure S-5:MPJPE evolution in DPoser training with different regularization loss settings for body pose completion, assessed on AMASS[45]with 10 hypotheses under legs occlusion scenarios.",
                "position": 4281
            }
        ]
    },
    {
        "header": "JExtended DPoser‚Äôs Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/x11.png",
                "caption": "Figure S-6:Failure cases of our method on challenging yoga poses. Inaccuracies in the estimated 2D keypoints (middle column), combined with our model‚Äôs limited exposure to such out-of-distribution poses during training, lead to flawed 3D mesh reconstructions (right column).",
                "position": 4397
            }
        ]
    },
    {
        "header": "KLimitation and future work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Generation/merged_image.png",
                "caption": "Figure S-7:Visualization of body pose generation. DPoser can generate diverse and realistic body poses.",
                "position": 4420
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x12.png",
                "caption": "(a)",
                "position": 4423
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x12.png",
                "caption": "(a)",
                "position": 4426
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x13.png",
                "caption": "(b)",
                "position": 4431
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x14.png",
                "caption": "(a)",
                "position": 4438
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x14.png",
                "caption": "(a)",
                "position": 4441
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x15.png",
                "caption": "(b)",
                "position": 4446
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x16.png",
                "caption": "(c)",
                "position": 4452
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x17.png",
                "caption": "(a)",
                "position": 4459
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x17.png",
                "caption": "(a)",
                "position": 4462
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x18.png",
                "caption": "(b)",
                "position": 4467
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x19.png",
                "caption": "(a)",
                "position": 4474
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x19.png",
                "caption": "(a)",
                "position": 4477
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x20.png",
                "caption": "(b)",
                "position": 4482
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Hand/generation/dposer_merged_image.png",
                "caption": "(a)",
                "position": 4489
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Hand/generation/dposer_merged_image.png",
                "caption": "(a)",
                "position": 4492
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Hand/generation/vposer_merged_image.png",
                "caption": "(b)",
                "position": 4497
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Hand/generation/nrdf_merged_image.png",
                "caption": "(c)",
                "position": 4503
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x21.png",
                "caption": "Figure S-13:Visualization of hand inverse kinematics under multiple challenging settings. Comparison across (a) noisy keypoints, (b) fingertip keypoints, (c) partial finger keypoints, and (d) sparse keypoints settings.",
                "position": 4510
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x22.png",
                "caption": "Figure S-14:Visualization of hand mesh recovery with mean pose initialization.",
                "position": 4513
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x23.png",
                "caption": "Figure S-15:Visualization of hand mesh recovery with Hand4Whole[47]initialization.",
                "position": 4516
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x24.png",
                "caption": "Figure S-16:Qualitative results of face inverse kinematics on the WCPA dataset[31]. Comparison across (a) 1 mm noise, (b) 5 mm noise, and (c) half-face occlusion. Better zoom in and compare the human eyes and chin.",
                "position": 4519
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x25.png",
                "caption": "Figure S-17:Visualization of face reconstruction results on the WCPA dataset[31]. Comparisons include (a) fitting from scratch and (b) initialization using EMOCA[10]results. *Ground truth lacks global orientation and translational data; these are fitted for visualization.",
                "position": 4522
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_paper/vae_merged_image.png",
                "caption": "(a)",
                "position": 4526
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_paper/vae_merged_image.png",
                "caption": "(a)",
                "position": 4529
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_paper/base_merged_image.png",
                "caption": "(b)",
                "position": 4534
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_paper/finetune_merged_image.png",
                "caption": "(c)",
                "position": 4540
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_paper/ambient_merged_image.png",
                "caption": "(d)",
                "position": 4545
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_appendix/ambient_merged_image.png",
                "caption": "(a)",
                "position": 4552
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_appendix/ambient_merged_image.png",
                "caption": "(a)",
                "position": 4555
            },
            {
                "img": "https://arxiv.org/html/2508.00599/figs/Wholebody/generation_appendix/finetune_merged_image.png",
                "caption": "(b)",
                "position": 4560
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x26.png",
                "caption": "Figure S-20:Visualization of whole-body mesh recovery on the Fit3d dataset[19].",
                "position": 4567
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x27.png",
                "caption": "Figure S-21:Qualitative comparison of whole-body pose completion. One hand is masked randomly.",
                "position": 4570
            },
            {
                "img": "https://arxiv.org/html/2508.00599/x28.png",
                "caption": "Figure S-22:Visualization of whole-body pose completion for three DPoser-X variants. One hand is masked randomly.",
                "position": 4573
            }
        ]
    },
    {
        "header": "LMore Qualitative Results",
        "images": []
    }
]