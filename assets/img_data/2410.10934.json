[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/x1.png",
                "caption": "Figure 1:We introduce the Agent-as-a-Judge framework wherein agentic systems are used to evaluate agentic systems.\nWe compare this to LLM-as-a-Judge, which uses LLMs to evaluate LLMs and for which Agent-as-a-Judge is a natural evolution, and Human-as-a-Judge, where skilled human labourers manually evaluate an agentic system.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/meta_light.png",
                "caption": "",
                "position": 394
            }
        ]
    },
    {
        "header": "2DevAI: A Dataset for Automated AI Development",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/x2.png",
                "caption": "Figure 2:Distribution of DevAI Tasks(1) DevAI focuses on AI development tasks and so terms such as “dataset,” “model,” and “results” are particularly common in the queries.\n(2) The first 53 tasks in DevAI all have a one-paragraph query but of varying lengths (note that task 54 and 55 are excluded here as they are outliers, representing the longest and most complex tasks in the dataset).\n(3) Each task has one or more tags. The prevalence of supervised learning here reflects the fact that it dominates many machine learning applications.\n(4) SVM classifiers(Cortes,1995)and LSTM models(Hochreiter,1997)are two of the most widely used architectures—a fact reflected by DevAI.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/meta_emoji_statistics.png",
                "caption": "Table 1:Preliminary Statistics of AI Developers.We compare three leading open-source code agents using metrics such as average cost, average time, and the average number of generated files.",
                "position": 508
            }
        ]
    },
    {
        "header": "3Human-as-a-Judge: Manual Evaluation on DevAI",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/meta_people.png",
                "caption": "Table 2:Human-as-a-Judge for AI Developers.(I)and(D)representindependentperformance versus performance considering taskdependencies.indicates multiple experts evolved, andmeans the evaluations use white-box testing (allowing access to the generated workspace, human-collected trajectories, and open-source codebases). The results were derived from expert judgments and deliberations (seeAppendixH).",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/meta_white_box.png",
                "caption": "",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/disagreement_meta.png",
                "caption": "Figure 4:Between the three human evaluators, a large amount of disagreement was observed in their individual evaluations—highlighting the inherent unreliability of a single human evaluation.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/human_error_rate_meta.png",
                "caption": "Figure 5:Mismatch between the individual evaluations and the consensus evaluation.\nIn particular, the majority vote classifier showed the smallest deviation from the consensus evaluation.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/meta_gray_box.png",
                "caption": "Table 3:AI Judges and Their Shift/Alignment with Human-as-a-Judge.We compare the results of LLM-as-a-Judge and Agent-as-a-Judge with Human-as-a-Judge.(I)represents performance on independent tasks, while(D)represents performance considering taskdependencies.Note:gray-box settings use carefully manually collected trajectory data (which is nearly inaccessible in practical situations, seeAppendixJ). In contrast,black-box setting doesn’t need to access to such data. The red scores represent the absolute judge shift compared with Human-as-a-Judge (e.g.,2.74%).",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/meta_black_box.png",
                "caption": "",
                "position": 753
            }
        ]
    },
    {
        "header": "4Agent-as-a-Judge: Evaluating Agents with Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/x3.png",
                "caption": "Figure 6:Initial diagram of Agent-as-a-Judge.",
                "position": 972
            },
            {
                "img": "https://arxiv.org/html/2410.10934/extracted/5932469/FILES_IMAGES/pr_curves_meta.png",
                "caption": "Figure 7:PR Curves comparing judge Methods.",
                "position": 1002
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOutline of this Paper",
        "images": []
    },
    {
        "header": "Appendix BExperiment Designs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/x4.png",
                "caption": "",
                "position": 2632
            }
        ]
    },
    {
        "header": "Appendix CAgent-as-a-Judge Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/x5.png",
                "caption": "Figure 9:The pipelines of developer agents and judge agent. Some materials in this figure are from original blog (https://www.factsmachine.ai/p/hidden-in-plain-sight).",
                "position": 2792
            }
        ]
    },
    {
        "header": "Appendix DExtend Related Work",
        "images": []
    },
    {
        "header": "Appendix EThe Procedures of Creating DevAI Dataset",
        "images": []
    },
    {
        "header": "Appendix FUser experiences of code-generation agentic systems",
        "images": []
    },
    {
        "header": "Appendix GMore DevAI dataset samples",
        "images": []
    },
    {
        "header": "Appendix HHuman Evaluation Procedure",
        "images": []
    },
    {
        "header": "Appendix ISuggest Constraints",
        "images": []
    },
    {
        "header": "Appendix JCollected Trajectories",
        "images": []
    },
    {
        "header": "Appendix KAblations of Agent-as-a-Judge",
        "images": []
    },
    {
        "header": "Appendix LPrompt Demos of Agent-as-a-Judge",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10934/x6.png",
                "caption": "",
                "position": 4527
            }
        ]
    },
    {
        "header": "Appendix MJudge Evidences Collected from Agent-as-a-Judge",
        "images": []
    }
]