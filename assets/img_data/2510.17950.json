[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17950/figs/apicall.png",
                "caption": "Figure 1:We served our robots online. A set of low-level api is formalized to provide the exact timestamp of observations and state of the action queue to enable fine-grained control. No docker images or model checkpoints are needed to be exchanged.",
                "position": 104
            }
        ]
    },
    {
        "header": "2RoboChallenge",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17950/figs/all_tasks.png",
                "caption": "Figure 2:Left: Thumbnails of the tasks. Right: ranklist of the baseline methods. Our first benchmark is a 30-task static armed robot testing set. It challenges a variety of aspects of the learning algorithms. We measured the end-to-end task-level success rate and a score that measures the partial progress of the tasks, and see a clear distinction between the models. Models marked byrc_baselineare finetuned by the authors of the report. Other models are finetuned by a group of college volunteers.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2510.17950/x1.png",
                "caption": "",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2510.17950/x2.png",
                "caption": "Figure 3:Variation of success rates caused by testers. We picked two tasks and tried three different testers, each with (1)experienced tester: the same one that collected the training data (2)ignorant tester: totally fresh tester seeing the prompt and props for the first time (3)adaptive tester: a tester with algorithm experience and managing to “improve” the success rate as much as he/she could.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2510.17950/x3.png",
                "caption": "Figure 4:The “Sweet-spot Effect”. We plot the positions of the box chosen by an “adaptive tester”, and use green and red color to indicate a successful or failed task. The tester managed to find the location and orientation of the box that the task is more likely to succeed, and exploited this for maximal performance. This biased the test.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2510.17950/x4.png",
                "caption": "Figure 5:The tester’s user interface for Visual Task Reproduction. A reference image is superimposed on the live camera stream. The tester is instructed to adjust the position of the objects and other factors so that the images match.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/robustness_a.png",
                "caption": "(a)Input augmentation. First image on the upper left is original image.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/robustness_a.png",
                "caption": "(a)Input augmentation. First image on the upper left is original image.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/robustness_b.png",
                "caption": "(b)The outputs of VLAs with and without input augmentation.",
                "position": 361
            }
        ]
    },
    {
        "header": "3Table30 Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17950/x5.png",
                "caption": "Figure 7:Distribution of our tasks. We tag our tasks either (1) by the difficulties encountered by a VLA solution (2) by the type of robot (3) by the intended location of the task scenario (4) by the property of the main target object. It shows good diversity and coverage.",
                "position": 741
            }
        ]
    },
    {
        "header": "4Results on Table30",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17950/x6.png",
                "caption": "Figure 8:Distribution of SR and scores. We sort the tasks by SR or score for each model to obtain the cumulative distribution.",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/flow.jpg",
                "caption": "Figure 9:Results of the models on our benchmark. The color is used to indicate the best result in the row and how far it is from a perfect run. The methods with /multi suffix follows the Generalist protocol. Others follow the Task-specific protocol.",
                "position": 764
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AWalkthrough of Submitting a Model for Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17950/figs/flow.jpg",
                "caption": "Figure 10:Intended workflow for participants.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/UR5_photo.png",
                "caption": "",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/Aloha_photo.png",
                "caption": "",
                "position": 1645
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/Franka_photo.png",
                "caption": "",
                "position": 1649
            },
            {
                "img": "https://arxiv.org/html/2510.17950/figs/ARX5.png",
                "caption": "",
                "position": 1653
            }
        ]
    },
    {
        "header": "Appendix BPhotos of the Robot Platforms",
        "images": []
    }
]