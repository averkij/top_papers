[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/teaser_v3.png",
                "caption": "Figure 1:Scene Generation.We are developing a foundation model for 3D intelligence that will support applications like scene generation. This winter village scene was generated through a multi-turn conversation with our prototype scene generation tool.",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15475/x1.png",
                "caption": "Figure 2:Overview.We present an important step towards the foundation model for 3D intelligence. Specifically, our report focuses on 3D shape tokenization‚Äîa technique for converting between shapes and discrete tokens. We also demonstrate how our tokenization scheme enables multiple applications including text-to-shape generation, shape-to-text generation, and text-to-scene generation.",
                "position": 89
            }
        ]
    },
    {
        "header": "2Shape Tokenization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15475/x2.png",
                "caption": "Figure 3:Shape Tokenization.Our pipeline encodes an input mesh into discrete tokens through several steps: (1) We sample points from the mesh surface and embed them using our Phased-Modulated Positional Encoding; (2) A Perceiver-based transformer(Jaegle et¬†al.,2021)encodes these points into continuous latent vectors, regularized with a self-supervised loss; (3) We apply optimal-transport vector quantization(Zhang et¬†al.,2024)to convert these vectors into discrete shape tokens;\n(4) These tokens can later be decoded into an occupancy field for mesh extraction. To improve training stability and the reconstruction quality, we also introduce a Stochastic Gradient Shortcut layer that allows the decoder to utilize the continuous latent vectors directly during training.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/pmpe/pe.png",
                "caption": "(a)Baseline Positional EncodingŒ≥‚Å¢(‚ãÖ)ùõæ‚ãÖ\\gamma(\\cdot)italic_Œ≥ ( ‚ãÖ )",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/pmpe/pe.png",
                "caption": "(a)Baseline Positional EncodingŒ≥‚Å¢(‚ãÖ)ùõæ‚ãÖ\\gamma(\\cdot)italic_Œ≥ ( ‚ãÖ )",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/pmpe/pmpe.png",
                "caption": "(b)Phase-Modulated Positional EncodingŒ≥‚Ä≤‚Å¢(‚ãÖ)superscriptùõæ‚Ä≤‚ãÖ\\gamma^{\\prime}(\\cdot)italic_Œ≥ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( ‚ãÖ )",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x3.png",
                "caption": "Figure 5:Latent Space Regularization with Self-supervised Loss.We regularize our latent space using a self-supervised loss inspired by DINOv2(Oquab et¬†al.,2023). This loss is computed as the cross entropy between features extracted from the student and teacher encoders, where the teacher model is an Exponential Moving Average (EMA) of the student. Input queries to the student encoder are randomly masked for robustness.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x4.png",
                "caption": "(a)Without self-supervised loss",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x4.png",
                "caption": "(a)Without self-supervised loss",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x5.png",
                "caption": "(b)With self-supervised loss",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x6.png",
                "caption": "Figure 7:Qualitative Analysis of Shape Reconstruction.Comparison between our method and Craftsman(Li et¬†al.,2024)demonstrates that both of our model variants achieve superior reconstruction quality, preserving finer geometric details while producing fewer artifacts.",
                "position": 265
            }
        ]
    },
    {
        "header": "3Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.15475/x7.png",
                "caption": "Figure 8:Text-to-Shape Generation Result Gallery.Our model can generate a diverse set of 3D meshes, capturing sharp edges, smooth surfaces and complex structures.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x8.png",
                "caption": "Figure 9:Shape-to-text examples.Shape-to-text captioning of example shapes from Toys4K dataset using short, medium and long captions. Words highlighted in blue indicate notable differences as captions increase in length. Short captions usually capture the shape category. More details about the shape‚Äôs geometry and style are added as caption length increases.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x9.png",
                "caption": "Figure 10:Shape cycle consistency.Our shape-to-text and text-to-shape models demonstrate the cycle consistency. Given a shape, we caption it using shape-to-text model, then then regenerate the 3D shape using the text-to-shape model. We highlight some keywords in blue for the shape category and style. The process preserves the overall geometry and key characteristics of the original shape, although there can be some loss in fine-scale details.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x10.png",
                "caption": "Figure 11:Scene graph with text-based object descriptions.For each scene object, the scene description includes the type of object (object_category) and a text description of its shape (object_caption). These descriptions facilitate LLM understanding of the scene. To support the simple case of orienting objects on a ground plane, our current implementation only supports Y axis rotations.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2503.15475/x11.png",
                "caption": "Figure 12:Scene generation examples.The layout, the rotation, position and scale of each object are produced by the LLM system, while the geometry and texture are generated by our text-to-shape and an in-house text-to-texture model built upon FlashTex(Deng et¬†al.,2024).",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2503.15475/extracted/6294257/figures/dinner_nocolor.png",
                "caption": "Table 2:Scene analysis and suggestion example.Given this diner scene, our scene suggestion assistant is able to summarize the scene, make suggestions for scene enhancements such as where to placement of condiments, what style of seating would be appropriate, and make background audio recommendations.",
                "position": 396
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AContributions and Acknowledgments",
        "images": []
    }
]