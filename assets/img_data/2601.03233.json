[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03233/assets/figures/fig-1-overview-v2.png",
                "caption": "Figure 1:Overview of theLTX-2architecture. Raw video and audio signals are encoded into modality-specific latent tokens via causal VAEs, while text is processed through a refined embedding pipeline. A dual-stream diffusion transformer jointly denoises audio and video latents with bidirectional audiovisual cross-attention and text conditioning, producing synchronized audiovisual outputs.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03233/x1.png",
                "caption": "Figure 3:Visualization of AV cross-attention maps.The maps are averaged across attention heads and the model layers; V2A and A2V maps correspond to the first and last 1/3 of inference steps, respectively. Red vertical lines on the audio waveform mark the timestamps of the displayed frames. The visualization demonstrates the modelâ€™s ability to spatially track a moving vehicle, dynamically shift attention from one speaker to another and then to both simultaneously, and focus on the lip region during close-up speech.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2601.03233/assets/figures/connector-v3.png",
                "caption": "Figure 4:Overview of the Text Understanding pipeline. The text prompt is encoded by Gemma3 and refined through theFeature ExtractorandText Connectorto condition the modality-specific DiT.",
                "position": 366
            }
        ]
    },
    {
        "header": "4Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03233/assets/figures/cross-modal-cfg.png",
                "caption": "Figure 5:Multimodal Classifier-Free Guidance with independent text and cross-modal control. The guided prediction is formed by combining the fully conditioned model output (orange) with two guidance directions: a text guidance term scaled bysts_{t}(green) and a cross-modal guidance term scaled bysms_{m}(blue). This supports independent control of textual conditioning and inter-modal alignment during inference.",
                "position": 428
            }
        ]
    },
    {
        "header": "5Training Data",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Social Impact",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03233/assets/figures/single.png",
                "caption": "Figure A2:Detailed view of a single stream of the model. The audio and video streams are identical in architecture.",
                "position": 562
            }
        ]
    },
    {
        "header": "Appendix ASupplementary Material",
        "images": []
    }
]