[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21343/x1.png",
                "caption": "Figure 1:Self-Improving pretraining:\nOur proposed model training streams pretraining documents and improves the nextKKgenerated tokens (suffix, given prefix) at each step with RL. A strong previously post-trained model is used to judge generation candidates at each RL step for quality, safety and hallucination, where the candidates are: (i)NNrollouts from the current policy; (ii) the original suffix; and (iii) a rewrite of the suffix by the strong post-trained model. The rewrite can improve the pretrain dataâ€™s quality or safety; in the latter case as the prefix remains unsafe the model is always learning how to steer away to a safe suffix. At the start of training model rollouts(i)are low quality, so training relies on candidates(ii)and(iii); later in training the judge starts rewarding winning rollouts.",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Self-Improving Pretraining",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21343/figures/judge_safety_valid.png",
                "caption": "Figure 5:Suffix judge validation rewards on safety and quality tasks. Initial performance of the model is close to random chance on either task, achieving scores above90%90\\%by the end of training.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2601.21343/figures/judge_coh_valid.png",
                "caption": "",
                "position": 502
            },
            {
                "img": "https://arxiv.org/html/2601.21343/figures/rewriter_safe.png",
                "caption": "Figure 6:Suffix rewriter validation rewards on safe and unsafe suffixes of the RedPajama dataset. Initial performance of the model is close to random chance on the safety task (0.5 score on unsafe suffixes), and near zero on copying safe suffixes (exact match reward score of 0.1), but still increasing after 500 steps.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2601.21343/figures/rewriter_unsafe.png",
                "caption": "",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2601.21343/figures/rewriter_tok_safe.png",
                "caption": "Figure 7:Token Overlap in Suffix Rewriter Validation on RedPajama Dataset.\nWe evaluate token overlap between original and rewritten suffixes for both safe and unsafe suffixes in the RedPajama dataset. Our objective is to produce safe rewrites that remain similar to the original suffix. Token overlap serves as a measure of this similarity. For safe suffixes, token overlap increases and approaches 1.0 as we optimize for exact matches. In contrast, token overlap for unsafe suffixes averages around 0.63 and remains close to its initial value, indicating less change (should not overlap).",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2601.21343/figures/rewr_tok_unsafe.png",
                "caption": "",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2601.21343/x2.png",
                "caption": "Figure 8:Rollout chosen rate on the training data during from-scratch pretraining (left) and continued pretraining (right). Initially RL reward for rollouts is low, and suffix or rewrite completions are chosen for training more often. As the model improves, RL rewards high-quality rollouts, resulting in higher rollout chosen rates.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2601.21343/x3.png",
                "caption": "",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2601.21343/x4.png",
                "caption": "Figure 9:Ablation results on the number of rollouts in online DPO training for models trained for Quality (left), Factuality (middle), and Safety (right).",
                "position": 1958
            },
            {
                "img": "https://arxiv.org/html/2601.21343/x5.png",
                "caption": "",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2601.21343/x6.png",
                "caption": "",
                "position": 1962
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Judge experiments",
        "images": []
    },
    {
        "header": "Appendix BSynthetic data generation",
        "images": []
    },
    {
        "header": "Appendix CEvaluation results",
        "images": []
    }
]