[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3DualityVidQA",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24271/x1.png",
                "caption": "Figure 1:Overview of video editing pipelines.There are three pipelines for different types of counterfactual context:Visual Anomaly: pixel-level video editing via OpenCVSemantic Anomaly: an MLLM selects an object for editing, followed by mask generation, VACE-based editing, and majority-vote verification using multiple SOTA MLLMs.Common Sense Anomaly: an MLLM propose commonsense violations, FLUX-Kontext edits frames, edits are re-verified by multiple MLLMs, and VACE interpolates the final video.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2512.24271/x2.png",
                "caption": "Figure 2:Overview of the DualityForge framework and DualityVidQA dataset.Starting with real, web-sourced videos, the DualityForge framework first embeds the counterfactual (CF) context, including visual, semantic, and commonsense, into it with video editing pipeline. The embedded context is then provided alongside the video to an MLLM to produce detailed captions and QA pairs. The dataset comprises three splits: DualityVidQA-SFT with real and counterfactual video-QA pairs (54K + 50K) for SFT; DualityVidQA-RL with 20K shared-question contrastive video-answer pairs (one question, two real/CF videos) for RL; and DualityVidQA-Test (600 pairs), which shares the same contrastive structure as DualityVidQA-RL and covers diverse counterfactual categories.",
                "position": 171
            }
        ]
    },
    {
        "header": "4DNA-Train",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24271/x3.png",
                "caption": "Figure 3:Overview of DNA-Train framework.We first perform SFT on our dual dataset to initialize the model. During RL, we sample a group of responses for both real and CF videos, compute their rewards based on task correctness, and calculate theâ„“1\\ell_{1}norm of intra-group advantages. Finally, we normalize the advantages across the dual groups to ensure balanced gradients.",
                "position": 262
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "ADatset Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24271/x4.png",
                "caption": "Figure A.5:Examples of DualityVidQA-SFT. We show the real video and counterfactual video pair and the question and answer pair generated based on the counterfactual video.",
                "position": 1953
            },
            {
                "img": "https://arxiv.org/html/2512.24271/x5.png",
                "caption": "Figure A.6:Examples of DualityVidQA-RL. We show the real video and counterfactual video pair and the generated question and answer.",
                "position": 1956
            },
            {
                "img": "https://arxiv.org/html/2512.24271/x6.png",
                "caption": "Figure A.7:Examples of DualityVidQA-Test. We show the real video and counterfactual video pair and the generated question. Answers for the counterfactual video are shown inred, and answers for the real video are shown ingreen.",
                "position": 1959
            }
        ]
    },
    {
        "header": "BDerivation",
        "images": []
    }
]