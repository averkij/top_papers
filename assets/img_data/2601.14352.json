[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14352/x1.png",
                "caption": "Figure 1:New Features of RoboBrain 2.5.Top: Precise 3D spatial reasoning with depth-aware grounding, metric measuring, and full manipulation trace generation under physical constraints. Bottom: Dense temporal value estimation for step-aware progress/regress prediction from state transitions across viewpoints and tasks; radar plots summarize performance gains on 2D/3D spatial and temporal benchmarks.",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2New Feature",
        "images": []
    },
    {
        "header": "3Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14352/x2.png",
                "caption": "Figure 2:Training Data Distribution for RoboBrain 2.5.The left pie chart illustrates the hierarchical composition of the dataset, structured into Temporal (red), General (teal), and Spatial (blue) domains. The right bar chart displays the sample count for each specific sub-task on a logarithmic scale, highlighting the extensive scale of Dense Value Estimation, High-Quality General Data, and 3D Spatial Reasoning.",
                "position": 422
            }
        ]
    },
    {
        "header": "4Training Strategy",
        "images": []
    },
    {
        "header": "5Infrastructure",
        "images": []
    },
    {
        "header": "6Evaluation Results",
        "images": []
    },
    {
        "header": "7Conclusion and Future Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Contributions and Author List",
        "images": []
    },
    {
        "header": "9Qualitative examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.14352/x3.png",
                "caption": "Figure 3:Visualization of TraceSpatial-Bench Rollouts and RoboBrain 2.5’s Predicted Traces.The red mask marks the ground-truth starting point, the purple 3D bounding box represents the ground-truth endpoint, and the 2D projection of RoboBrain 2.5’s predicted 3D spatial trace is displayed.",
                "position": 2669
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x4.png",
                "caption": "Figure 4:Visualization of TraceSpatial-Bench Rollouts and RoboBrain 2.5’s Predicted Traces.The red mask marks the ground-truth starting point, the purple 3D bounding box represents the ground-truth endpoint, and the 2D projection of RoboBrain 2.5’s predicted 3D spatial trace is displayed.",
                "position": 2672
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x5.png",
                "caption": "Figure 5:Visualization of TraceSpatial-Bench Rollouts and RoboBrain 2.5’s Predicted Traces.The red mask marks the ground-truth starting point, the purple 3D bounding box represents the ground-truth endpoint, and the 2D projection of RoboBrain 2.5’s predicted 3D spatial trace is displayed.",
                "position": 2675
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x6.png",
                "caption": "Figure 6:Visualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5’s Predicted Traces.Examples for AgiLex Dual-Arm tasks: Click Bell; Click Alarm clock; Blocks Ranking.",
                "position": 2678
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x7.png",
                "caption": "Figure 7:Visualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5’s Predicted Traces.Examples for AgiLex Dual-Arm tasks: Handover Block; Handover Mic; Hanging Mug; Move Can Pot.",
                "position": 2681
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x8.png",
                "caption": "Figure 8:Visualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5’s Predicted Traces.Examples for AgiLex Dual-Arm tasks: Move Playingcard Away; Move Stapler Pad; Open Laptop; Place A2B Left.",
                "position": 2684
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x9.png",
                "caption": "Figure 9:Visualization of RoboTwin 2.0 Rollouts and RoboBrain 2.5’s Predicted Traces.Examples for AgiLex Dual-Arm tasks: Place A2B Right; Place Bread Basket; Place Bread Skillet; Place Burger Fries.",
                "position": 2687
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x10.png",
                "caption": "Figure 10:RoboBrain 2.5 Progress Predictions across Diverse Tasks.We visualize the frame-wiseHop(instantaneous change) and accumulatedProgresspredicted by RoboBrain 2.5 on unseen validation tasks.",
                "position": 2714
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x11.png",
                "caption": "Figure 11:Progress Estimation Consistency across Sampling Intervals.We plot the reconstructed progress curves for the same trajectory using different frame strides (10, 25, 50, and 100 frames). The high overlap between curves demonstrates that our RoboBrain 2.5 is robust to temporal granularity and does not simply overfit to a specific frame rate.",
                "position": 2717
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x12.png",
                "caption": "Figure 12:RoboBrain 2.5 Progress Predictions across three modes.We visualize the frame-wiseHop(instantaneous change) and accumulatedProgresspredicted by RoboBrain 2.5 with incremental, forward-anchored and backward-anchored mode.",
                "position": 2720
            },
            {
                "img": "https://arxiv.org/html/2601.14352/x13.png",
                "caption": "Figure 13:Robustness to Artificial Disturbance during Real-World Execution.We visualize a rollout of the converged policy (success rate>95%>95\\%) under human interference. Each sub-figure shows the third-person view, the ego-centric view, and the real-time RoboBrain 2.5 inference (Top:Hop, Bottom:Progress).(a) Artificial Disturbance Position:A human hand intervenes and shifts the target board while the robot attempts to approach.(b) Fall Into Misalignment:The robot misses the new position. Note that the RoboBrain 2.5Progresscurve drops significantly (indicated by the red dot in the bottom inset), reflecting the failure state.(c) Misalignment Recovery:The policy reacts to the visual feedback and the drop in reward, adjusting the end-effector position.(d) Move to the top:The robot realigns directly above the target slot.(e) Align with the Slot:Precise fine-tuning before insertion.(f) Successful Insertion:The task is completed, with the progress estimation reaching its peak.",
                "position": 2723
            }
        ]
    },
    {
        "header": "10Proof of Bounded Global Progress",
        "images": []
    }
]