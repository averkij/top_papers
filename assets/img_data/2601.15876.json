[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/x1.png",
                "caption": "Figure 1:Performance comparison on the OSWorld-Verified benchmark.OurEvoCUA-32Bachieves state-of-the-art performance (56.7%) among open-weights models.",
                "position": 249
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/figs/evocua_cycle.png",
                "caption": "Figure 2:Overview ofEvoCUA. The diagram illustrates the paradigm shift from static imitation to an active evolving experience learning cycle (center). The approach unifies three core modules: theVerifiable Synthesis Engine(top left); theScalable Interaction Infrastructure(right); andIterative Optimization(bottom left).",
                "position": 303
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Verifiable Synthesis Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/figs/evocua_data.png",
                "caption": "Figure 3:Architecture of the Verifiable Synthesis Engine. The pipeline operates in three cascading stages: (1)Structured Task Space Constructionto define diverse scenarios from domain taxonomies and hybrid resources; (2)Agentic Dual-Stream Synthesis, where a Task Architect (VLM) co-generates instructions (gg) and executable validators (VgV_{g}) via a closed-loop feedback mechanism; and (3)Rigorous Quality Assuranceto filter outputs for high consistency and ensure decontamination, yielding the final verifiable dataset.",
                "position": 452
            }
        ]
    },
    {
        "header": "4Scalable Interaction Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/figs/evocua_infra.png",
                "caption": "Figure 4:Scalable Infrastructure.The architecture orchestrates massive interaction requests from the online RL loop (top-left) through an asynchronous gateway and distributed scheduler (top-right). The bottom layer deploys parallel sandbox clusters, highlighting theComputer Use Sandbox, which utilizes QEMU-KVM virtualization and a calibrated OS to ensure input determinism, rendering consistency, and runtime stability for high-fidelity environments.",
                "position": 543
            }
        ]
    },
    {
        "header": "5Evolving Paradigm via Learning from Experience",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/figs/evocua_dpo.png",
                "caption": "Figure 5:Overview of the Dual-Paradigm DPO.\nThe process begins at a critical forking pointt‚àót^{*}. Paradigm I (Action Correction) establishes a preference for the chosen action(zw,aw)(z_{w},a_{w})over the rejected action(zl,al)(z_{l},a_{l}). Paradigm II (Reflection) addresses the deviated state att‚àó+1t^{*}+1, prioritizing Reflection over Blind Continuation. Both paradigms define preference pairs that optimize the DPO Lossùí•‚Äã(Œ∏)\\mathcal{J}(\\theta)to maximize the margin between effective and ineffective strategies.",
                "position": 769
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": []
    },
    {
        "header": "7Future Work on Online Agentic RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/figs/online_agentic_rl_step40.png",
                "caption": "Figure 7:The values illustrated in this figure denote the 16-time average scores of the two methods, respectively.",
                "position": 1441
            }
        ]
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUnified Action Space",
        "images": []
    },
    {
        "header": "Appendix BCold Start: Hindsight Reasoning Generation",
        "images": []
    },
    {
        "header": "Appendix CAlgorithm for DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15876/figs/vis_1.png",
                "caption": "(a)Step 1: Goal Clarification (t=1t=1).The inspector visualizes the initial state. The reasoning panel displays the agent‚Äôs explicit paraphrasing of the instruction (‚ÄùFind the greatest value‚Ä¶ place it in Column G‚Äù), validating the goal grounding mechanism in our synthetic data.",
                "position": 2342
            },
            {
                "img": "https://arxiv.org/html/2601.15876/figs/vis_1.png",
                "caption": "(a)Step 1: Goal Clarification (t=1t=1).The inspector visualizes the initial state. The reasoning panel displays the agent‚Äôs explicit paraphrasing of the instruction (‚ÄùFind the greatest value‚Ä¶ place it in Column G‚Äù), validating the goal grounding mechanism in our synthetic data.",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2601.15876/figs/vis_2.png",
                "caption": "(b)Step 2: Text Entry (t=1t=1).The system captures the transition from planning to execution. The agent‚Äôs reasoning (‚ÄùType ‚ÄôMax‚Äô‚Ä¶‚Äù) is perfectly aligned with the generated atomic action sequence (press(‚ÄôM‚Äô), press(‚Äôa‚Äô), press(‚Äôx‚Äô)).",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2601.15876/figs/vis_9.png",
                "caption": "(a)Step 9: Stateful Interaction (t=9t=9).This view validates theUnified Action Space. The synthetic ground truth requires a stateful operation (Shift-Select). The inspector confirms the agent correctly executes thekey_down: shift‚Üí\\rightarrowclick‚Üí\\rightarrowkey_up: shiftsequence.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2601.15876/figs/vis_9.png",
                "caption": "(a)Step 9: Stateful Interaction (t=9t=9).This view validates theUnified Action Space. The synthetic ground truth requires a stateful operation (Shift-Select). The inspector confirms the agent correctly executes thekey_down: shift‚Üí\\rightarrowclick‚Üí\\rightarrowkey_up: shiftsequence.",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2601.15876/figs/vis_15.png",
                "caption": "(b)Step 15: Verified Termination (t=15t=15).The final frame validates theReasoning-Augmented Terminationschema. The tool highlights that the agent generates visual evidence (‚ÄùI can see‚Ä¶ Max column‚Ä¶ calculated‚Äù) to justify the successful termination status.",
                "position": 2379
            }
        ]
    },
    {
        "header": "Appendix DTrajectory Analysis and Visualization",
        "images": []
    }
]