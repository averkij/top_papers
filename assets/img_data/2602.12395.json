[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12395/x1.png",
                "caption": "Figure 1:Frankenstein-style Analysis Framework.The framework proceeds through three components:\n(1) functional localization via causal probing\nacross transformer depth,\n(2) update characterization via parameter\ncomparison to identify region-wise update pattern in post-training\n, and\n(3) transferability test via model merging, assessing whether the localized functionalities are preserved in layers.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12395/x2.png",
                "caption": "Figure 2:Average Benchmark Accuracy versus Fine-Grained Abilities (Vision, Reasoning, and Vision-to-Reasoning Alignment).Thegreen arrowsdenote model post-training pipelines (Base Model→\\rightarrowIN Model→\\rightarrowRL Model) that exhibit monotonic performance gains, whereas thepurple arrowsindicate model groups that do not.\nDespite apparent improvements on visual reasoning benchmarks, fine-grained evaluation metrics reveal that vision ability and reasoning ability do not improve monotonically from the base model to the IN model and then to the RL model.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x3.png",
                "caption": "Figure 3:Aggregated Attention from Reasoning Tokens to Vision Tokens.Compared to IN models, there is more attention from reasoning tokens to vision tokens in RL models’ inference.\nThe pattern is concentrated in later layers across training recipes, while absent in earlier layers.",
                "position": 371
            }
        ]
    },
    {
        "header": "3Frankenstein-Style Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12395/x4.png",
                "caption": "(a)Vision localization via vision-token swapping.",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x4.png",
                "caption": "(a)Vision localization via vision-token swapping.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x5.png",
                "caption": "(b)Reasoning localization via layer skipping.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x6.png",
                "caption": "Figure 5:Layer-wise parameter update norms comparison between IN and RL.Per-layer Frobenius norms of parameter updates for IN (solid) and RL (dashed).\nBoth training stages concentrate on optimization in theMidlayers, while RL exhibits a distinct\nredistribution of update magnitude compared to IN.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x7.png",
                "caption": "Figure 6:Singular value spectra of parameter updates.Each panel visualizes the\nlog-normalized singular value spectrum of the layer-wise update matrix.\nColor intensity reflects the steepness of the spectral decay.\nCompared to IN, RL updates exhibit a consistently steeper spectral decay in theMid-Latelayers, indicating that optimization is concentrated in a smaller number of\ndominant directions rather than being diffusely distributed.",
                "position": 572
            }
        ]
    },
    {
        "header": "4Necessity Validation via\nModel Freezing",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8More Related Work",
        "images": []
    },
    {
        "header": "9Fine-grained Metrics",
        "images": []
    },
    {
        "header": "10Functional Region Localization",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12395/x8.png",
                "caption": "Figure 7:Illustration for vision token swapping to measure layerii’s contribution to model’s response.",
                "position": 3039
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x9.png",
                "caption": "Figure 8:Examples of paired image datasets constructed to isolate visual functions.",
                "position": 3138
            },
            {
                "img": "https://arxiv.org/html/2602.12395/x10.png",
                "caption": "Figure 9:Illustration for layer skipping to measure layerii’s contribution to model’s response.",
                "position": 3253
            }
        ]
    },
    {
        "header": "11Model Merging",
        "images": []
    },
    {
        "header": "12Model Training with Region-wise Freezing",
        "images": []
    }
]