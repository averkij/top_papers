[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13835/x1.png",
                "caption": "Figure 1:Comparison with different data selection methodsLu et al. (2024); Liu et al. (2024b)on the Tulu3Lambert et al. (2024)pool using Llama3.1-8BTouvron et al. (2023), evaluated on (black) knowledge-based benchmarks and (red) human-preference benchmarks. See details in Sec.4.2.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13835/x2.png",
                "caption": "Figure 2:Illustration of (a) Data Selection Pipeline and (b) MIG Sampler. Given the raw data pool, our pipeline first applies a tagger and scorer to annotate data. Next, MIG constructs the label graph based on the label set and iteratively selects the data point that maximizes the information gain within the graph. The selected data are used for supervised fine-tuning (SFT) of LLMs.",
                "position": 183
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13835/extracted/6372957/figs/data-scaling-v2.png",
                "caption": "Figure 3:Data scaling experiments on Tulu3 using Llama3.1-8B. The score reported here is theAvgscore.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2504.13835/x3.png",
                "caption": "Figure 4:(a) Derivative of Information Score Functions. (b)AvgobjsubscriptAvgobj\\text{Avg}_{\\text{obj}}Avg start_POSTSUBSCRIPT obj end_POSTSUBSCRIPTon Different Information Score Functions. (c)AvgsubsubscriptAvgsub\\text{Avg}_{\\text{sub}}Avg start_POSTSUBSCRIPT sub end_POSTSUBSCRIPTon Different Quality Scores.",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2504.13835/x4.png",
                "caption": "Figure 5:Quantitative results on different quality metrics. DEITA scores achieve the best performance on both human-preference and knowledge-based evaluations.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2504.13835/x5.png",
                "caption": "Figure 6:Analysis of Parameters in the Label Graph. The reported score is the average ofAvgsubsubscriptAvgsub\\text{Avg}_{\\text{sub}}Avg start_POSTSUBSCRIPT sub end_POSTSUBSCRIPTandAvgobjsubscriptAvgobj\\text{Avg}_{\\text{obj}}Avg start_POSTSUBSCRIPT obj end_POSTSUBSCRIPT.\nPlease refer to Table789in Appx.Dfor detailed scores on all evaluated benchmarks.\n(a) Comparison of various node counts (label set size) in the label graph. (b) Comparison of different edge thresholds, with a lower threshold indicating a dense graph. (c) Comparison of different propagation weights, where a smaller weight corresponds to weak propagation.",
                "position": 1043
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Experiments Setup",
        "images": []
    },
    {
        "header": "Appendix BEfficiency Analysis",
        "images": []
    },
    {
        "header": "Appendix CTheoretical Analysis of Greedy Strategy in MIG",
        "images": []
    },
    {
        "header": "Appendix DDetailed Results on Benchmarks",
        "images": []
    }
]