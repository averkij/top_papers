[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23412/image/hf-logo.png",
                "caption": "",
                "position": 140
            }
        ]
    },
    {
        "header": "1  Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23412/x1.png",
                "caption": "Figure 1:MWE-Bench Performance of MindWatcher.",
                "position": 148
            }
        ]
    },
    {
        "header": "2  Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23412/x2.png",
                "caption": "Figure 2:The Working Paradigm of MindWatcher.To address complex multimodal question answering tasks, we train our model using continuous RL to develop Multimodal CoT capabilities. By integrating interleaved thinking, the model is able to interact with the environment and autonomously invoke tools in the toolbox. Furthermore, to facilitate more accurate and lowcost visual search, MindWatcher have constructed a large-scale local retrieval corpus spanning eight major categories.",
                "position": 178
            }
        ]
    },
    {
        "header": "3  Training Data and MWE-Bench",
        "images": []
    },
    {
        "header": "4  Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23412/x3.png",
                "caption": "(a)Open-sourced Benchmark.",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x3.png",
                "caption": "(a)Open-sourced Benchmark.",
                "position": 940
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x4.png",
                "caption": "(b)MWE-Benchmark.",
                "position": 946
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x5.png",
                "caption": "(a)MindWatcher vs GPT-5 mini.",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x5.png",
                "caption": "(a)MindWatcher vs GPT-5 mini.",
                "position": 1119
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x6.png",
                "caption": "(b)MindWatcher vs Qwen2.5-VL-32B.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2512.23412/image/level2_20.jpg",
                "caption": "",
                "position": 1140
            }
        ]
    },
    {
        "header": "5  Related Work",
        "images": []
    },
    {
        "header": "6  Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "Author List",
        "images": []
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23412/x7.png",
                "caption": "Figure 5:Step-wise Synchronous Sampling Framework of MindWatcher.",
                "position": 1287
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x8.png",
                "caption": "Figure 6:MindWatcher-2B vs Qwen3-VL 2B Thinking.",
                "position": 1300
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x8.png",
                "caption": "Figure 6:MindWatcher-2B vs Qwen3-VL 2B Thinking.",
                "position": 1303
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x9.png",
                "caption": "Figure 7:MindWatcher-3B vs Qwen2.5-VL-3B.",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2512.23412/x10.png",
                "caption": "Figure 8:MindWatcher-4B vs Qwen3-VL 4B Thinking.",
                "position": 1315
            },
            {
                "img": "https://arxiv.org/html/2512.23412/image/athlete.png",
                "caption": "",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2512.23412/image/mountain.png",
                "caption": "",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2512.23412/image/livis.png",
                "caption": "",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2512.23412/image/livis_crop.png",
                "caption": "",
                "position": 1787
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]