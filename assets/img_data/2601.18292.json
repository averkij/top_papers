[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18292/x1.png",
                "caption": "Figure 1:Overview of the proposed tri-role reinforcement learning framework, illustrating the closed-loop interaction amongMRedM_{\\mathrm{Red}},MBlueM_{\\mathrm{Blue}}, andMEvalM_{\\mathrm{Eval}}.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18292/x2.png",
                "caption": "Figure 2:The internal mechanism of training loop.MRedM_{\\mathrm{Red}}generates adversarial prompts using customized templates to attackMBlueM_{\\mathrm{Blue}}and other defense models. The reward signal for theMRedM_{\\mathrm{Red}}consists of a semantic reward, a diversity penalty, and a weighted score of theMBlueM_{\\mathrm{Blue}}responses as evaluated byMEvalM_{\\mathrm{Eval}}. Adversarial prompts produced byMRedM_{\\mathrm{Red}}are submitted toMBlueM_{\\mathrm{Blue}}, whose outputs are likewise assessed byMEvalM_{\\mathrm{Eval}}, with the evaluation scores serving as the reward signal for trainingMBlueM_{\\mathrm{Blue}}. The training data forMEvalM_{\\mathrm{Eval}}consist of adversarial prompts sampled fromPRedP_{\\mathrm{Red}}, the corresponding responses generated by all defense models, and labels determined via multi-expert majority voting.",
                "position": 191
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18292/x3.png",
                "caption": "Figure 3:ASR ofMRedM_{\\mathrm{Red}}across different training iterations. It can be observed that ASR steadily improves across the three different defense models. For example,MRedM_{\\mathrm{Red}}-14Bâ€™s ASR against DeepSeek-R1-0528-Qwen3-8B increase from 13% to 32%, on Qwen3-8B from 21.84% to 67.75%, and on Llama-3.1-Nemotron-Nano-8B-v1 from 60% to 90%.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2601.18292/x4.png",
                "caption": "Figure 4:Safety capability evaluation ofMBlueM_{\\mathrm{Blue}}across different training iterations. It shows that although some fluctuations occur during training iterations, the ASR of all three models show a downward trend. Particularly, the ASR ofMBlueM_{\\mathrm{Blue}}-14B in the last iteration is the lowest among all models, indicating its great safety capability.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2601.18292/x5.png",
                "caption": "Figure 5:Accuracy curves ofMEvalM_{\\mathrm{Eval}}on a curated dataset.\nThe accuracy of all three models steadily increases, which in turn yields more accurate and stable reward signals for optimizing bothMRedM_{\\mathrm{Red}}andMBlueM_{\\mathrm{Blue}}.",
                "position": 499
            }
        ]
    },
    {
        "header": "6Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18292/x6.png",
                "caption": "Figure 6:Effect of using multiple defense models duringPRedP_{\\mathrm{Red}}on ASR. It shows thatMRedM_{\\mathrm{Red}}training with multiple model achieves higher ASR against all three tested models compared toMRedM_{\\mathrm{Red}}deployed with single model.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2601.18292/x7.png",
                "caption": "Figure 7:Entropy curves in ablation experiments ofMRedM_{\\mathrm{Red}}. It can be observed that w/o L + w/o D model collapses to low entropy predictions, while introducing either component mitigates entropy collapse. The w/ L + w/ D model consistently maintains the highest entropy throughout training.",
                "position": 530
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Templates",
        "images": []
    },
    {
        "header": "Appendix BTraining Parameters & Expense",
        "images": []
    },
    {
        "header": "Appendix CUse of AI",
        "images": []
    }
]