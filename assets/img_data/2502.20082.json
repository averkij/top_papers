[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20082/extracted/6233605/ruler.png",
                "caption": "Figure 1:LongRoPE2-extended LLaMA3-8B achieves the best performance at a 128k context length among∼similar-to\\sim∼10B models.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Context Window Extension and Challenges",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20082/x1.png",
                "caption": "Figure 2:(a) RoPE OOD (red area) when extending context length from 2k to 4k. (b) Per-dimensional RoPE rescaling factor from different approaches for extending Phi3-mini from 2k to 128k, all aligning with RoPE OOD theory.\n(c) Performance of Phi3-mini-128k after fine-tuning. Existing methods fail to achieve an effective 128k context length and show noticeable short-context performance drop.",
                "position": 129
            }
        ]
    },
    {
        "header": "3LongRoPE2 Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20082/x2.png",
                "caption": "Figure 3:Sequence length required to span the theoretical period during Phi3-mini pre-training for different RoPE dimensions. Insufficient training in higher RoPE dimensions leads to shorter effective RoPE ranges and longer actual periods.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2502.20082/x3.png",
                "caption": "Figure 4:Scale factors across different RoPE rescaling approaches.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2502.20082/x4.png",
                "caption": "Figure 5:Mixed context window training to improve both short and long context capabilities.",
                "position": 487
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20082/x5.png",
                "caption": "Figure 6:LongRoPE2 (right) delivers near-perfect lossless performance in the ”Needle in a Haystack” pressure test.",
                "position": 983
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.20082/x6.png",
                "caption": "Figure 7:Needle in a Haystack full results for Phi3-mini (3.8B)-128k.",
                "position": 2706
            },
            {
                "img": "https://arxiv.org/html/2502.20082/x7.png",
                "caption": "Figure 8:Needle in a Haystack full results for LLaMA3-8B-128k.",
                "position": 2709
            },
            {
                "img": "https://arxiv.org/html/2502.20082/extracted/6233605/yarn-ntk.png",
                "caption": "Figure 9:The RoPE rescaling factor distributions of NTK/YaRN adjusted based on the real critical dimension (i.e., YaRN-rcd, NTK-rcd).",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2502.20082/x8.png",
                "caption": "Figure 10:The pseudocode for mixed context window training and inference.",
                "position": 2718
            }
        ]
    },
    {
        "header": "Appendix CSynthetic data sample",
        "images": []
    }
]