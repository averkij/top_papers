[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20618/x1.png",
                "caption": "Figure 1:Traditional single-pass MLLMs that ingest entire long videos in one contextâ€”typically (may through heavy downsampling and compression) often miss crucial evidence and produce wrong answers, whereasLongVideoAgentconductsmulti-agent,multi-round, andmultimodalreasoning to extract sparse, task-relevant cues and answer correctly.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20618/x2.png",
                "caption": "Figure 2:Architecture ofLongVideoAgent. AMasterAgentruns for up toKKrounds, collaborating with aGroundingAgentto localize relevant clips from videos and aVisionAgentto read fine-grained cues from the localized frames. Evidence accumulates until theMasterAgentfeels confident to answer the user.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    }
]