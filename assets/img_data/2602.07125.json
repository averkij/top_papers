[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07125/x1.png",
                "caption": "Figure 1:Overview of our reasoning-augmented multimodal representation. We use a strong VLM to (i) inject descriptive text into image-only inputs and (ii) refine captions for image–text pairs, making key visual semantics explicit. This externalizes implicit reasoning, allowing the retriever to focus on compression into robust embeddings rather than on-the-fly visual inference. Red highlights denote the matching components on both the query and corpus sides with the help of VLM-generated enhancements.",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.07125/x2.png",
                "caption": "Figure 2:In this entry of Fashion200K, the query asks for a description matching the image, which Qwen accurately captioned with keywords such as “black dress” and “flared sleeves”. This allowed the retrieval of multiple correct results, indicating how the one-ground-truth design is flawed. Furthermore, the ground truth’s “multicolored” description is simply untrue, reinforcing our observation over these benchmarks’ low quality nature.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2602.07125/x3.png",
                "caption": "Figure 3:Qualitative comparison on CIRR-7. The user’s instruction requests a modification of “more focus on its head.” The baseline model, relying on implicit visual features, exhibits a strong bias toward low-level visual similarity, retrieving images with matching poses (sleeping bodies) rather than the requested semantic change. In contrast, our enhanced model leverages dense corpus captions (e.g., “close-up,” “profile view”) to successfully align the modification instruction with the correct target.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2602.07125/x4.png",
                "caption": "",
                "position": 994
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompts Used",
        "images": []
    },
    {
        "header": "Appendix BExperiments on MVRB Composed Image Retrieval",
        "images": []
    },
    {
        "header": "Appendix CImpact of Hard Negative Mining and Data Enrichment",
        "images": []
    }
]