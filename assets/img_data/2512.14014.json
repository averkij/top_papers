[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14014/x1.png",
                "caption": "Figure 1:Model Performance on MobileWorldBench.We introduce MobileWorldBench, a world modeling benchmark that tests vision language models’ (VLMs) capability to serve as world models for mobile agents. We also introduce MobileWorld, a 1.4M dataset that can be used to improve VLM’s world modeling capability. Starting with Qwen3-VL-8B-Instruct as the base model (“Base”), finetuning on MobileWorld leads to considerable performance gain (“+SFT”) on the next-state-generation task.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x2.png",
                "caption": "Figure 2:Advantages of Semantic World Modeling.Pixel-space world modeling is particularly challenging as the model needs to identify changes, come up with the correct app content, and render them accurately. By contrast, Semantic World Modeling only focuses on abstracting relevant changes in GUI semantics, while being useful for decision-making. In the example shown, the frontier visual generative model GPT Image 1 struggles to accurately render GUI states, while frontier VLMs (GPT-4o[hurst2024gpt]) can accurately describe the expected GUI changes in text.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14014/x3.png",
                "caption": "Figure 3:The Semantic World Model Paradigm.(Top): we factorize the classic pixel world models into two components. We call the first component the semantic world models. It predicts the latent distributionp​(zt+1|Xt,at)p(z_{t+1}|X_{t},a_{t})encoding high-level semantics.zt+1z_{t+1}can be queried viap​(y|z)p(y|z)to produce text descriptions, or throughp​(oi|qi,z)p(o^{i}|q^{i},z)to produce yes-no answers. (Bottom). To use semantic world models for decision-making, we employ a model-based policy framework that combines a semantic WM with an action proposal model and value model.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x4.png",
                "caption": "Figure 4:Data Pipeline for MobileWorldBench.Our data pipeline consists of 5 steps: 1) Curating raw trajectories, 2) Using VLM to convert low-level actions to high-level actions and annotate state-change descriptions for reference, 3) We generate QA candidates for each state transition sampled, 4) We use VLM to filter these QA pairs through self-check and relevance metrics, 5) For the filtered data, we additionally use human verification to further filter the data based on its correctness and relevance.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x5.png",
                "caption": "Figure 5:Examples from MobileWorld Training Set.We show qualitative examples of Next-State-Generation and Next-State-QA tasks in the training set of MobileWorld, highlighting the effectiveness of our data pipeline.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x6.png",
                "caption": "Figure 6:Distribution of Tasks, Apps, and Actions in MobileWorldBench.MobileWorldBench covers a wide range of task categories, Apps, and action types. We visualize their distributions for reference.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14014/x7.png",
                "caption": "Figure 7:Model ELO ratings from human evaluation.We perform a user study that asks human evaluators to pick between the outputs of two models for the Next-State-Generation task. Finetuning on MobileWorld  significantly improves performance.",
                "position": 637
            }
        ]
    },
    {
        "header": "5Conclusion and Future Works.",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "7Additional Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14014/x8.png",
                "caption": "Figure 8:Distribution of Tasks, Apps, and Actions in MobileWorld.MobileWorld covers a wide range of task categories, Apps, and action types. We visualize their distributions for reference.",
                "position": 777
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x9.png",
                "caption": "Figure 9:Action Annotation via Visual Overlay.To better allow VLM annotators to understand the action being performed, in addition to integer coordinates, we provide additional visual annotations on the coordinates of user gestures visually.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x10.png",
                "caption": "(a)Correctness and Relevance Filter.",
                "position": 912
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x10.png",
                "caption": "(a)Correctness and Relevance Filter.",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x11.png",
                "caption": "(b)Ambiguity Filter.",
                "position": 921
            }
        ]
    },
    {
        "header": "8Additional Experiment Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14014/x12.png",
                "caption": "Figure 11:Human Evaluation Interface of Comparing Two Models.",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x13.png",
                "caption": "Figure 12:Qualitative Examples of MobileWorld  Evaluation Results.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2512.14014/x14.png",
                "caption": "Figure 13:Qualitative Results of Pixel ModellingWe combine the output of semantic world models with pixel renders to obtain image predictions. Gemini Nano Banana Pro achieves the best qualitative results. We demonstrate that using text as guidance in the generation process results in a prediction much closer to the ground truth.",
                "position": 1198
            }
        ]
    },
    {
        "header": "9Reproducibility Statements",
        "images": []
    },
    {
        "header": "10Additional Discussion with Related Works.",
        "images": []
    }
]