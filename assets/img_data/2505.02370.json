[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02370/extracted/6403359/images/hf_logo.png",
                "caption": "",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x1.png",
                "caption": "",
                "position": 122
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02370/x2.png",
                "caption": "Figure 2:Unlike existing efforts that attempt to(a)scale up edited images with noisy supervision[4,25],(b)introduce massive VLMs into editing model architecture[24,12], and(c)perform additional pre-training tasks[14,45],(d)we focus on improving the effectiveness of supervision signals, which is the fundamental issue of image editing.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x3.png",
                "caption": "Figure 3:(a)Existing work primarily uses LLMs and diffusion models to automatically generate edited images. However, current diffusion models often fail to accurately follow text prompts while maintaining the input images layout, resulting in mismatches between the original-edited image pairs and the editing instructions.(b)We perform instruction rectification (Step 3) based on the images constructed in Steps 1 and 2. We show VLMs can understand the differences between the images, enabling them to rectify editing instructions to be better aligned with original-edited image pairs.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02370/x4.png",
                "caption": "Figure 4:We show that the editing model follows consistent generation attributes at different sampling stages, independent of the editing instructions. The early, middle, and late sampling stages correspond toglobal,local, anddetailchanges, respectively, whilestylechanges occur at all stages. All the generated images here are DDIM 30-step sampled final images. Theorange progress barand thegrid progress barrepresent the sampling stages with and without the editing instructions, respectively.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x5.png",
                "caption": "Figure 5:(a)Based on the rectified editing instruction and original-edited image pair, we utilize the Vision-Language Models (VLM) to generate various image-related wrong instructions. These involve random substitutions of quantities, spatial locations, and objects within the rectified editing instructions according to the original-edited images context;(b)During each training iteration, we randomly select one wrong instructioncnegTsubscriptsuperscript{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}c^{T}_{neg}}italic_c start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n italic_e italic_g end_POSTSUBSCRIPTand input it along with the rectified instructioncposTsubscriptsuperscript{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}%\n\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}c^{T}_{pos}}italic_c start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPTinto the editing model to obtain predicted noises. The goal is to make the rectified instructions predicted noise系possubscriptitalic-系\\epsilon_{pos}italic_系 start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPTcloser to the sampled training diffusion noise系italic-系\\epsilonitalic_系, while ensuring the noise from incorrect instructions系negsubscriptitalic-系\\epsilon_{neg}italic_系 start_POSTSUBSCRIPT italic_n italic_e italic_g end_POSTSUBSCRIPTis farther. Best viewed in color.",
                "position": 276
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02370/x6.png",
                "caption": "Figure 6:Visual comparison with existing methods and the corresponding human-aligned GPT-4o evaluation scores (Following, Preserving, Quality Scores from left to right). We achieve better results while preserving the layout, quality, and details of the original image. Please note that we do not claim that our editing results are flawless. We provide more visual comparison results in the supplementary material.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x7.png",
                "caption": "Figure 7:Human evaluation on three evaluation criteria for image editing effects.(a)Following: whether the edited image adhere to the editing instructions;(b)Preserving: whether the image structure outside of the editing instructions has been preserved;(c)Quality: whether the overall quality/aesthetics of the edited image has been degraded compared to the input image. Our SuperEdit achieves the best results on all of these metrics.",
                "position": 594
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Overview of Supplementary",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8More Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02370/x8.png",
                "caption": "Figure 8:Existing metrics cannot reliably indicate editing quality.",
                "position": 1743
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x9.png",
                "caption": "Figure 9:We show the impact of incorporating the editing prompt at different inference timesteps on the edited image.(a)The global layout changes usually occur in the early stages of inference. Adding text editing instructions to modify the global layout at the mid or late stages does not effectively impact the global layout.(b)Local object attribute changes occur in the mid-stages of sampling. Adding text editing instructions in the early or late stages may result in incorrect editing outcomes.(c)The style changes happen across all inference stages, and the detail changes happen in the late stage (Please refer to the subtle differences between the last two images). Best viewed in color.",
                "position": 1870
            }
        ]
    },
    {
        "header": "9Diffusion Generation Prior",
        "images": []
    },
    {
        "header": "10GPT-4o Prompts for Constructing Rectified and Contrastive Editing Instructions",
        "images": []
    },
    {
        "header": "11Discussion and Limitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02370/x10.png",
                "caption": "Figure 10:GPT-4o prompts for constructing rectified and contrastive editing instructions",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x11.png",
                "caption": "Figure 11:Comparison of different vision-language models in rectifying editing instructions based on generation prior attributes. GPT-4o achieves more stable and accurate results in describing the differences between original-edited image pairs. Text in red represents incorrectly generated instructions.",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x12.png",
                "caption": "",
                "position": 2111
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x13.png",
                "caption": "Figure 13:More visual comparison with existing methods.",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2505.02370/x14.png",
                "caption": "Figure 14:More visual comparison with existing methods.",
                "position": 2122
            }
        ]
    },
    {
        "header": "12More Visualization Comparison and Results",
        "images": []
    }
]