[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12402/x1.png",
                "caption": "",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12402/x2.png",
                "caption": "Figure 1:Final validation loss vs. weight decay coefficientÎ»\\lambdafor 338M models trained on C4 under Chinchilla scaling. Our approach (red) achieves lower final loss than standard weight decay (blue) while preserving the optimizer-specific optimum inÎ»\\lambda. For each optimizer (AdamW,Lion,Muon), both methods use the same hyperparameters.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x2.png",
                "caption": "",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x3.png",
                "caption": "",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x4.png",
                "caption": "",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x5.png",
                "caption": "Figure 2:Trajectories ofAdam,AdamW, andAdam+CWDon a toy example.Adamhalts at a minimizer, whileAdamWminimizes the objective within a constrained region (green). In contrast,Adam+CWDexhibits sliding mode dynamics within the minimizer manifold.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Background and Motivation",
        "images": []
    },
    {
        "header": "3Cautious Weight Decay",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12402/x6.png",
                "caption": "Figure 3:Toy objectives and trajectories.Left:fâ€‹(x,y)=((yâˆ’3)2âˆ’(xâˆ’3)2âˆ’1)2f(x,y)=((y-3)^{2}-(x-3)^{2}-1)^{2}.Right:fâ€‹(x,y)=(yâˆ’3âˆ’(xâˆ’3)2)2f(x,y)=(y-3-(x-3)^{2})^{2}.\nWe compareAdam,AdamW, andAdam+CWD;AdamWandCWDuse the same weight decayÎ»\\lambda, and all other hyperparameters(Î·,Î²1,Î²2,Ïµ)(\\eta,\\beta_{1},\\beta_{2},\\epsilon)are identical.\nFor both objectives,Adamconverges to a generic point on the minimizer manifold, whereasAdamWconverges to a solution of the box-constrained problemminx,yâ¡fâ€‹(x,y)\\min_{x,y}f(x,y)subject tomaxâ¡{x,y}â‰¤1Î»\\max\\{x,y\\}\\leq\\frac{1}{\\lambda}.\nIn contrast,Adam+CWDconverges to the Pareto front of the minimizer manifold.",
                "position": 500
            }
        ]
    },
    {
        "header": "4Discrete-Time Analysis",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12402/x7.png",
                "caption": "Figure 4:Evaluation loss across scales.3Ã—\\times3 grid for 338M, 986M, and 2B Transformer models trained withAdamW,Lion, andMuonon C4 dataset.All panels show a zoom into the finalâˆ¼\\sim40% of training stepsto highlight late-stage behavior. Baseline curves (dashed blue) use standard weight decay with tuned hyperparameters (learning rate schedule,Î²\\betaâ€™s, weight decay, etc.; see AppendixF). Our method (solid red) follows Algorithm1and reuses the baseline hyperparameters without additional tuning.\nFull (non-zoomed) curves are in Figures8,9and10in AppendixG.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x8.png",
                "caption": "Figure 5:Training loss of OLMo 1B on 100B tokens.Left:AdamW.Right:Muon.",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x8.png",
                "caption": "",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x9.png",
                "caption": "",
                "position": 849
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotation and Definitions",
        "images": []
    },
    {
        "header": "Appendix BPseudocode of Optimizers withCWD",
        "images": []
    },
    {
        "header": "Appendix CFixed-Point Analysis",
        "images": []
    },
    {
        "header": "Appendix DLyapunov Functions",
        "images": []
    },
    {
        "header": "Appendix EDeferred Proofs",
        "images": []
    },
    {
        "header": "Appendix FModel & Experiment Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12402/x10.png",
                "caption": "Figure 6:Masked weight-decay activation ratiort:=â€–ð•€â€‹(ð®tâ€‹ð±t>ðŸŽ)â€–1dr_{t}:=\\frac{\\left\\lVert\\mathbb{I}(\\mathbf{u}_{t}\\mathbf{x}_{t}>\\mathbf{0})\\right\\rVert_{1}}{d}, i.e., the fraction of parameters for which the sign-selective mask is active at steptt(dd= number of parameters). Left:AdamW; right:Muon. Insets zoom into the first 2.5k steps to highlight early-training behavior. Model: Qwen-0.6B[70]trained on The Pile[18].",
                "position": 3458
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x10.png",
                "caption": "",
                "position": 3461
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x11.png",
                "caption": "",
                "position": 3465
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x12.png",
                "caption": "Figure 7:Training dynamics for the 986M-parameter Gemma model.",
                "position": 3471
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x13.png",
                "caption": "",
                "position": 3473
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x14.png",
                "caption": "",
                "position": 3473
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x15.png",
                "caption": "(a)338M parameters",
                "position": 3477
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x15.png",
                "caption": "(a)338M parameters",
                "position": 3480
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x16.png",
                "caption": "(b)986M parameters",
                "position": 3485
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x17.png",
                "caption": "(c)2B parameters",
                "position": 3490
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x18.png",
                "caption": "(a)338M parameters",
                "position": 3497
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x18.png",
                "caption": "(a)338M parameters",
                "position": 3500
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x19.png",
                "caption": "(b)986M parameters",
                "position": 3505
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x20.png",
                "caption": "(c)2B parameters",
                "position": 3510
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x21.png",
                "caption": "(a)338M parameters",
                "position": 3517
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x21.png",
                "caption": "(a)338M parameters",
                "position": 3520
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x22.png",
                "caption": "(b)986M parameters",
                "position": 3525
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x23.png",
                "caption": "(c)2B parameters",
                "position": 3530
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x24.png",
                "caption": "Figure 11:Comparison of gradient norms using RMS normalization across four model sizes: 111M, 338M, 986M, and 2B. All models are trained under Chinchilla settings.CWDachieves lower gradient norms across all configurations.",
                "position": 3537
            },
            {
                "img": "https://arxiv.org/html/2510.12402/x25.png",
                "caption": "Figure 12:Evolution of parameter norm (RMS) during training for a 986M parameter model. We compare three optimization strategies:AdamWwith weight decay 0.1 (orange), our proposed method (blue), andAdamwithout weight decay (green). Our method maintains stable parameter norms comparable toAdamWwhile achieving improved performance.",
                "position": 3540
            }
        ]
    },
    {
        "header": "Appendix GAdditional Experiment Results",
        "images": []
    }
]