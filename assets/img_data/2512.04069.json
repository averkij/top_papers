[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Double Interactive Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04069/x1.png",
                "caption": "Figure 1:Interactive reinforcement learning (IRL) with Toolshed.\nThe rollout module executes multi-turn trajectories under policyπθ\\pi_{\\theta}, alternating between reasoning and tool use before answering. Task rewards are aggregated and used to updateπθ\\pi_{\\theta}via GRPO with KL regularization againstπref\\pi_{\\text{ref}}.",
                "position": 509
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04069/x2.png",
                "caption": "Figure 2:Spatial reasoning examples of SpaceTools. It performs diverse spatial reasoning tasks including relative depth, pose, grasp, spatial compatibility, and spatial relationship by interleaving reasoning (gray) and vision tool calls (green) before producing the final answer. Images are taken from BLINK[fu2024blink], RoboSpatial-Home[song2025robospatial], and\nBOP-ASK[bhat2025bopask].",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2512.04069/x3.png",
                "caption": "Figure 3:Real-world robot manipulation fully controlled by SpaceTools. The model completes a multi-step task, “picking up the flashlight and placing it in the transparent bin”, via alternating reasoning (gray), vision tools (green) for perception, and robot tools (blue) for action.",
                "position": 954
            }
        ]
    },
    {
        "header": "6Discussion & Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Future Directions",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04069/x4.png",
                "caption": "Figure 4:TheToolshedinfrastructure linking a VLM with modular vision and robotic tools under a unified toolbox for perception and control.",
                "position": 1384
            }
        ]
    },
    {
        "header": "Appendix BDetails of Toolshed",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04069/Images/demo.png",
                "caption": "Figure 5:Interactive web demo illustrating Claude’s tool-augmented reasoning when integrated with Toolshed.",
                "position": 1619
            }
        ]
    },
    {
        "header": "Appendix CAdditional Method Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04069/x5.png",
                "caption": "Figure 6:System prompt.Instructional prompt guiding the model’s reasoning, tool-call, and answer process.",
                "position": 3215
            }
        ]
    },
    {
        "header": "Appendix DAdditional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04069/x6.png",
                "caption": "Figure 7:A hard real-world robot manipulation example with SpaceTools. The model successfully identifies the target object and completes the manipulation task in a cluttered and visually complex scene",
                "position": 3862
            },
            {
                "img": "https://arxiv.org/html/2512.04069/x7.png",
                "caption": "Figure 8:A failure case in real-world robot manipulation with SpaceTools. The model localizes a valid vacant area but selects a point too close to the boundary, resulting in a failed placement of the flashlight on the box boundary.",
                "position": 3866
            },
            {
                "img": "https://arxiv.org/html/2512.04069/x8.png",
                "caption": "Figure 9:A detailed example of tool-augmented reasoning of a grasp estimation question.",
                "position": 3869
            },
            {
                "img": "https://arxiv.org/html/2512.04069/x9.png",
                "caption": "Figure 10:Detailed examples of tool-augmented reasoning of relative depth questions.",
                "position": 3872
            },
            {
                "img": "https://arxiv.org/html/2512.04069/x10.png",
                "caption": "Figure 11:Failure cases for the grasp estimation task. Intermediate tool-augmented reasoning steps are omitted for clarity.",
                "position": 4104
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experimental Results",
        "images": []
    }
]