[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Problem setup and methods",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16301/x1.png",
                "caption": "Figure 1:Mixed training leads to robust cooperation.RL agents trained against a mix of tabular policies and learning agents converge to cooperation (solid lines).Ablations:Agents trained purely against other learning agents (dotted lines) or with access to explicit co-player identifications (dashed lines) converge to defection, highlighting that in-context inference is a critical factor for the learning of cooperative behaviors with standard decentralized MARL. Error bars indicate standard deviation across 10 random seeds.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2602.16301/x2.png",
                "caption": "Figure 2:A–B: Emergence of in-context best response.Performance of PPI agents (trained against random tabular opponents) when evaluated against specific fixed strategies. The agents demonstrate in-context learning, identifying the opponent and converging to the best response within the episode.C–D: Learning to extort in-context learners.Agents trained against a “Fixed In-Context Learner” (an agent pre-trained in Step 1 to best-respond to tabular policies) learn to extort it. The RL agent achieves a higher share of the reward by exploiting the in-context adaptation of its opponent.E–F: From mutual extortion to cooperation.When two agents initialized with extortion policies (from Step 2) play against each other, their mutual attempts to extort their co-player result in the shaping of each other’s policy towards more cooperative behavior, both within episodes through in-context learning (F) and across episodes through in-weight learning (E). Error bars indicate standard deviation across 10 random seeds.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional details on methods",
        "images": []
    },
    {
        "header": "Appendix BAdditional results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16301/x3.png",
                "caption": "Figure 3:Emergence of best-response in mixed training.We plot within-episode performance of models trained in Figure1before convergence. We observe that both A2C and PPI try to extort their counterpart at the beginning of the episode which subsequently leads to increased levels of cooperation. At the same time, identifying the opponent as a non-tit-for-tat-like tabular policy leads to high defection ratio. Error bars indicate standard deviation across 10 random seeds.",
                "position": 1237
            },
            {
                "img": "https://arxiv.org/html/2602.16301/x4.png",
                "caption": "Figure 4:A-B: Emergence of in-context best responsePerformance of A2C trained against random tabular opponents and evaluated after convergence on a set of specific static policies. We denote the final agent as “Fixed In-Context Learner”.C-D: Learning to extort in-context learners.Performance of a randomly initialized A2C agent against the Fixed In-Context Learner.E-F: From mutual extortion to cooperation.Two A2C extortion agents initially converge to cooperation when playing against each other, but with time they might collapse to mutual defection depending on the random seed. Error bars correspond to standard deviation over 5 random initializations.",
                "position": 1244
            }
        ]
    },
    {
        "header": "Appendix CDerivation of Predictive Policy Improvement (PPI)",
        "images": []
    },
    {
        "header": "Appendix DTheoretical Analysis of the Equilibrium Behavior of PPI Agents",
        "images": []
    },
    {
        "header": "Appendix ESoftware",
        "images": []
    }
]