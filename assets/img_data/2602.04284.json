[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/figure1.png",
                "caption": "Figure 1:Illustrative examples of how thought necessity and observation utility varies across turns. (a) Initial planning (e.g.,search for Trivor and Muztagh Ata) already determines the subsequent tool call action, making follow-up thought redundant; (b) Observations from early turns are unuseful in the last turn, becauseonly tool response in turn 4 is used for the answer summarization.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Analysis of Thought and Observation on Agent Effectiveness and Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/figure2.png",
                "caption": "Figure 2:Quantitative analysis of how thought and observation affect agent efficiency and effectiveness across interaction turns on WebShop environment using Qwen3-8B.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/figure3.png",
                "caption": "Figure 3:The effect of thought and observation omission on agent efficiency and effectiveness across turns on WebShop environment using Qwen3-8B.\nThe grey shaded region represents omitting at a specific turn could decrease token length without sacrificing accuracy.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/figure_method.png",
                "caption": "Figure 4:Overview of our proposed frameworkAgent-Omit.",
                "position": 316
            }
        ]
    },
    {
        "header": "4Efficient LLM Agents with Agent-Omit",
        "images": []
    },
    {
        "header": "5Theoretical Analysis",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/accuracy_vs_token_length.png",
                "caption": "Figure 5:Pass@1 accuracy of Agent-Omit variants on WebShop environment using Qwen3-8B backbone.",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/omission_statistics.png",
                "caption": "Figure 6:Statistics of average omission turns of Agent-Omit-8B-RL and its omission frequency across different turns.",
                "position": 1155
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix BImplement Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/agent_training_comparison_smooth.png",
                "caption": "Figure 7:The SFT training process visualization of Agent-Omit on five diverse domains.",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2602.04284/Figure/RL_training_analysis_smoothed.png",
                "caption": "Figure 8:The RL training process visualization of Agent-Omit on WebShop using Qwen3-8B.",
                "position": 2231
            }
        ]
    },
    {
        "header": "Appendix CIn-Depth Experimental Analysis",
        "images": []
    }
]