[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03651/x1.png",
                "caption": "Figure 1:Training paradigms for unified generative models. (a) Traditional training involves using paired image-text data and optimizing the unified model with paired losses for both image-to-text (I2T) and text-to-image (T2I) generation tasks. (b) In contrast, the proposed multimodal cycle training framework leverages unpaired images and texts. By using cycle-consistency losses, the unified model learns to maintain consistency between input and output across modalities, enabling adaptation without the need for extensive paired datasets.",
                "position": 82
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03651/x2.png",
                "caption": "Figure 2:The overview of T cycle (text-to-image-to-text) of the proposed DoraCycle. The I cycle is similar and is omitted in the figure for brevity.",
                "position": 140
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03651/x3.png",
                "caption": "Figure 3:Domain-oriented adaptation with different training setups. (a) Image generated by the base model without training for adoption. (b) Image generated by the model trained with 10 paired image-text samples. (c) Image generated by the model trained with 300 paired image-text samples. (d) Image generated by the model trained by DoraCycle on only unpaired data. (e) Image-to-Text-to-Image translation performed by the adapted model trained by DoraCycle.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2503.03651/x4.png",
                "caption": "Figure 4:Image-to-text and text-to-image generation by the unified models that adapted for two domains. The special tokens are omitted.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2503.03651/x5.png",
                "caption": "Figure 5:Effect of special tokens on character learning. (a) Base model without training. (b) Model trained without using special tokens, showing attribute confusion among characters. (c) Model trained with special tokens, improving character attribute alignment and reducing confusion.",
                "position": 299
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Details",
        "images": []
    },
    {
        "header": "Appendix BDifferentiable Sampling",
        "images": []
    },
    {
        "header": "Appendix CTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.03651/x6.png",
                "caption": "Figure 6:Image-to-text and text-to-image generation by the unified models that adapted for two domains. The special tokens are omitted.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2503.03651/x7.png",
                "caption": "Figure 7:Illustration of the progressive adaptation progress of the unified model to the target domain.",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2503.03651/x8.png",
                "caption": "Figure 8:Examples of the training data for DoraCycle to adapt the unified model to different target domains.",
                "position": 2032
            }
        ]
    },
    {
        "header": "Appendix DMore Results",
        "images": []
    },
    {
        "header": "Appendix ETraining Data Examples",
        "images": []
    },
    {
        "header": "Appendix FLimitations and Future Work",
        "images": []
    }
]