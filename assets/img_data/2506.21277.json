[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21277/x1.png",
                "caption": "Figure 1:Visualizations of the vanilla GRPO method applied in multimodal tasks. When the model is overconfident on questions, it tends to answer questions directly without considering the global context (left) or may overlook key multimodal inputs (right).",
                "position": 93
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3IntentBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21277/x2.png",
                "caption": "Figure 2:(a)(b)(c) are examples from Social-IQ 2.0, MDPE, and EMER, respectively. (d) is the statistic of the curated testing set from Social-IQ 2.0",
                "position": 168
            }
        ]
    },
    {
        "header": "4From Understanding to Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21277/x3.png",
                "caption": "Figure 3:The reasoning path of our model on an example from Social-IQ 2.0. The model first clearly understands the context information of the video clip in the multi-person talking scenario; then it starts reasoning with the multimodal clues to precisely answer the question.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2506.21277/x4.png",
                "caption": "Figure 4:Illustration of our method. We use Qwen2.5-Omni-Thinker[1]as our base model. For each training sample, we generate 8 completions and compute format and accuracy rewards with verifiable labels. Additionally, we assess reasoning-logical and context rewards by using a LLM as the judge, applying these rewards only to corresponding seen tokens for different rewards.",
                "position": 326
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21277/x5.png",
                "caption": "Figure 5:Visualization result of our method on IntentBench.",
                "position": 995
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21277/x6.png",
                "caption": "Figure 6:Visualization result of our method and the compared method on IntentBench.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2506.21277/x7.png",
                "caption": "Figure 7:Visualization result of our method and the compared method on IntentBench.",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2506.21277/x8.png",
                "caption": "Figure 8:Visualization result of our method and the compared method on IntentBench.",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2506.21277/x9.png",
                "caption": "Figure 9:Visualization result of our method and the compared method on IntentBench.",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2506.21277/x10.png",
                "caption": "Figure 13:Comparison of the original options in Social-IQ 2.0 and our refined options.",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2506.21277/x11.png",
                "caption": "Figure 14:Comparison of the original open-vocabulary answer and our designed multi-choice question with multiple answers in EMER.",
                "position": 1105
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]