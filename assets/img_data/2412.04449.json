[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04449/x1.png",
                "caption": "Figure 1:Comparison with the LLaVA-NeXT[24]baseline.Our model demonstrates comparable performance with the baseline model on 14 benchmarks across various domains, with 46.2% fewer KV cache storage and 44.4% fewer TFLOPs during inference.",
                "position": 85
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04449/x2.png",
                "caption": "Figure 2:Overview ofp-MoD. In the left, we present the detailed architecture of ourp-MoDlayers. Given the input tokens, the weight predictor first assigns weights to each token. Then, a subset of tokens with highest weights are selected according to a pre-defined token retention ratioRùëÖRitalic_Rand processed by the transformer layer, while the rest of the tokens are skipped. The weights are normalized byTanhNormmodule, and then both the selected and skipped tokens are symmetrically scaled by their corresponding weights in ourSTRingmodule. In the right, we demonstrate our crucial design, the progressive ratio decay(PRD) strategy, which gradually reduces the token retention ratioRùëÖRitalic_Rlayer by layer, following a shifted cos schedule.",
                "position": 121
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04449/x3.png",
                "caption": "(a)Illustration of our exploratory experiments on our MoD-based MLLM trained with a token retention ratio of 70% across all layers. Each sub-figure shows the inference token retention ratio across all layers of one experiment. Experimentaùëéaitalic_ais the baseline setting (70% ratio). In experimentsbùëèbitalic_b,cùëêcitalic_canddùëëditalic_d, we apply a lower token retention ratio (15%) to shallow, middle and deep layers.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2412.04449/x3.png",
                "caption": "(a)Illustration of our exploratory experiments on our MoD-based MLLM trained with a token retention ratio of 70% across all layers. Each sub-figure shows the inference token retention ratio across all layers of one experiment. Experimentaùëéaitalic_ais the baseline setting (70% ratio). In experimentsbùëèbitalic_b,cùëêcitalic_canddùëëditalic_d, we apply a lower token retention ratio (15%) to shallow, middle and deep layers.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2412.04449/x4.png",
                "caption": "(b)Average performance of our four experiments on 14 benchmarks listed in Section4.1. The performance drop becomes less significant as the low token retention ratio is applied to deeper layers, which indicates that vision tokens exhibit higher redundancy in deeper layers.",
                "position": 272
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04449/x5.png",
                "caption": "Table 5:Ablation on different token retention ratio schedules.Our default model is marked in gray.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2412.04449/x5.png",
                "caption": "Figure 4:Illustration of different ratio schedule functions in Table5.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2412.04449/x6.png",
                "caption": "Figure 5:Visualization of tokens selected by differentp-MoDlayers.The selected tokens are colored in red. We observe that vision tokens corresponding to regions with rich semantic information are selected across differentp-MoDlayers, even when the number of selected tokens becomes minimal in deeper layers.",
                "position": 984
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AComparison with Attention-Score-Based Pruning Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04449/extracted/6047298/sec/imgs/vis_all_layers.png",
                "caption": "Figure 6:Visualization of token selection decisions across differentp-MoDlayers.The horizontal axis denotes the token indexes, and the vertical axis denotes the layer indexes. It can be observed that everyp-MoDlayer independently selects important and informative tokens.",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix BVisualization ofp-MoDToken Selection Decisions",
        "images": []
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    }
]