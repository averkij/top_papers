[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17439/x1.png",
                "caption": "",
                "position": 144
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17439/x2.png",
                "caption": "Figure 2:Overview of FALCON framework.FALCON integrates a 2D VLM (e.g., Kosmos-2), an Embodied Spatial Model, and a Spatial-Enhanced Action Head. At timesteptt, the VLM processes visual observationsOtO_{t}and language instructionsLLto produce a semantic action tokenùê≠^act\\hat{\\mathbf{t}}_{\\text{act}}. Concurrently, the Embodied Spatial Model encodes a third-view imageIt3rdI^{\\text{3rd}}_{t}and optional geometric inputs into spatial tokensùêìspl\\mathbf{T}_{\\text{spl}}. These are fused by the Spatial-Enhanced Action Head to generate precise robot actionsAtA_{t}, enabling robust manipulation through joint semantic and spatial reasoning.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x3.png",
                "caption": "Figure 3:Different modality fusion strategies between spatial and semantic action tokens.",
                "position": 313
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17439/x4.png",
                "caption": "Figure 4:Evaluation of base tasks in cluttered scene.Base Taskscontains a total of nine distinct task suites, encompassing language grounding (cluttered scenes with random distractors) and semantic understanding (unseen object poses). Each task is evaluated over 10 different scene layouts with 10 trials, resulting in a total of 90 rollouts.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x5.png",
                "caption": "Figure 5:Performance comparison of different methods.Simplesetting (left): includeing four challenging tasks selected fromBase Tasks.Unseenscenarios (right): containing three unseen variations:Unseen Object, Background, andTask Descriptionto evaluate the robustness and generalization of all models.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x6.png",
                "caption": "Figure 6:Spatial Understanding Capability Evaluationsconsist of four tasks with varying levels of spatial complexity, designed to further investigate the spatial perception capabilities of FALCON.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x7.png",
                "caption": "Table 4:Performance comparison of different modality input on CALVIN benchmark.Kosmos-VLA (w/ rgb-d) is a point cloud-based variant where the ESM is replaced by a lightweight point cloud encoder[42]while retaining other components.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x7.png",
                "caption": "Figure 7:Performance comparison of different modality input on real-world tasks.Left:lift yellow pepper.Right:put white cup on pink cloth-cup height change.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x8.png",
                "caption": "Table 5:Ablation study on modality inputs for ESM on the CALVIN benchmark.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x8.png",
                "caption": "Figure 8:Depth Visualization.",
                "position": 944
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "6Training Paradigm",
        "images": []
    },
    {
        "header": "7Hyper-Parameters and Training Details",
        "images": []
    },
    {
        "header": "8Implementation Details",
        "images": []
    },
    {
        "header": "9Benchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17439/x9.png",
                "caption": "Figure 9:Real-world setup of the xArm 6 robotic system used in the experiments.The system is equipped with a side camera that provides both RGB and depth images for visual observation and spatial perception.",
                "position": 1840
            }
        ]
    },
    {
        "header": "10Ablation Study",
        "images": []
    },
    {
        "header": "11Potential Future Works",
        "images": []
    },
    {
        "header": "12Rollout Examples in CALVIN",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17439/x10.png",
                "caption": "",
                "position": 2035
            }
        ]
    },
    {
        "header": "13Rollout Examples in SimplerEnv",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17439/x11.png",
                "caption": "",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x12.png",
                "caption": "",
                "position": 2066
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x13.png",
                "caption": "Figure 13:Qualitative results forFew-Shot Adaptationtasks.",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x14.png",
                "caption": "Figure 14:Performance comparison of different methodsin theSimplesetting and four variants.",
                "position": 2085
            },
            {
                "img": "https://arxiv.org/html/2510.17439/x15.png",
                "caption": "",
                "position": 2484
            }
        ]
    },
    {
        "header": "14Rollout Examples in Real-World Tasks",
        "images": []
    }
]