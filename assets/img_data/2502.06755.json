[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/trimmed_dna-basic.png",
                "caption": "Figure 1:We compare the scientific method when applied to genomics and deep learning model interpretation.\nGrad-CAM(Selvaraju et¬†al.,2017)leverages saliency maps to produce hypothetical explanations for model predictions, but provides no natural way to experimentally validate the hypothesis.\nIn comparison, our proposed use of sparse autoencoders (SAEs) for vision models naturally enables experimental validation via feature suppression. SeeSection5for more examples of experimental validation.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/trimmed_dna-identified.png",
                "caption": "",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/trimmed_dna-knockout.png",
                "caption": "",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/gradcam-basic.png",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/gradcam-identified.png",
                "caption": "",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/gradcam-knockout.png",
                "caption": "",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/sae-basic.png",
                "caption": "",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/sae-identified.png",
                "caption": "",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/hook/sae-knockout.png",
                "caption": "",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/overview2.jpg",
                "caption": "Figure 2:Sparse autoencoders (SAEs) trained on pre-trained ViT activations discover a wide spread of features across both visual patterns and semantic structures. We show eight different features from an SAE trained on ImageNet-1K activations from a CLIP-trained ViT-B/16.",
                "position": 247
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06755/x1.png",
                "caption": "Figure 3:Given a picture and a set of highlighted patches, we find exemplar images by (1) getting ViT activations for each patch, (2) computing a sparse representation for each highlighted patch (Eqs.1and2), (3) summing over sparse representations, (4) choosing the topkùëòkitalic_kfeatures by activation magnitude and (5) finding existing images that maximize these features.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/legend.png",
                "caption": "Figure 4:CLIP learns robust cultural visual features.Top Left (a):A ‚ÄúBrazil‚Äù feature (CLIP-24K/6909) responds to distinctive Brazilian imagery including Rio de Janeiro‚Äôs urban landscape, the national flag, and the iconic sidewalk tile pattern of Copacabana BeachTop Right (b):CLIP-24K/6909does not respond to other South American symbols like Machu Picchu or the Argentinian flag.Bottom Left (c):We search DINOv2‚Äôs SAE for a similar ‚ÄúBrazil‚Äù feature and find thatDINOv2-24K/9823fires on Brazilian imagery.Bottom Right (d):However, maximally activating ImageNet-1K examples forDINOv2-24K/9823are of lamps, convincing us thatDINOv2-24K/9823does not reliably detect Brazilian cultural symbols.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-6909-positive-1.png",
                "caption": "",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-6909-positive-2.png",
                "caption": "",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-6909-positive-3.png",
                "caption": "",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-6909-positive-4.png",
                "caption": "",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-6909-negative-1.png",
                "caption": "",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-6909-negative-2.png",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9823-positive-1.png",
                "caption": "",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9823-positive-2.png",
                "caption": "",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9823-positive-3.png",
                "caption": "",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9823-positive-4.png",
                "caption": "",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9823-example-1.png",
                "caption": "",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9823-example-2.png",
                "caption": "",
                "position": 495
            }
        ]
    },
    {
        "header": "4SAE-Enabled Analysis of Vision Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/legend.png",
                "caption": "Figure 5:CLIP learns unified representations of abstract concepts that persist across visual styles.\nHighlighted patches indicate feature activation strength.Upper Left:We find a CLIP SAE feature (CLIP-24K/20652) that consistently activates on ‚Äúaccidents‚Äù or ‚Äúcrashes‚Äù: car accidents, plane crashes, cartoon depictions of crashes and generally damaged metal.Upper Right:Two exemplar images from ImageNet-1K for featureCLIP-24K/20652.Lower Left:We probe an SAE trained on DINOv2 activations.DINOv2-24K/9762is the closest feature, but does not reliably fire on all the examples.Lower Right:Two exemplar images from ImageNet-1K for featureDINOv2-24K/9762clarifies that it does not match the semantic concept of ‚Äúcrash.‚Äù",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-crash-positive-1.png",
                "caption": "",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-crash-positive-2.png",
                "caption": "",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-crash-positive-3.png",
                "caption": "",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-crash-positive-4.png",
                "caption": "",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-crash-positive-5.png",
                "caption": "",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-20652-1.png",
                "caption": "",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-20652-2.png",
                "caption": "",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-crash-positive-1.png",
                "caption": "",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-crash-positive-2.png",
                "caption": "",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-crash-positive-3.png",
                "caption": "",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-crash-positive-4.png",
                "caption": "",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-crash-positive-5.png",
                "caption": "",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9672-1.png",
                "caption": "",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/dinov2-9672-2.png",
                "caption": "",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2502.06755/x2.png",
                "caption": "Figure 6:Demonstrating the scientific method for understanding vision model behavior using sparse autoencoders (SAEs).Left:We observe that CLIP predicts ‚ÄúBlue Jay.‚ÄùUpper Middle:We select the bird‚Äôs wing in the input image; the SAE proposes a hypothesis that the most salient feature is ‚Äúblue feathers‚Äù via exemplar images.Lower Middle:We validate this hypothesis through controlled intervention by suppressing the identified ‚Äúblue feathers‚Äù feature in the model‚Äôs activation space.Right:we observe a change in behavior: the predicted class shifts away from ‚ÄúBlue Jay‚Äù towards ‚ÄúClark Nutcracker‚Äù, a similar bird besides the lack of blue plumage.\nThis three-step process of observation, hypothesis formation, and experimental validation enables systematic investigation of how vision models process visual information.",
                "position": 628
            }
        ]
    },
    {
        "header": "5Validating Hypotheses of Vision Model Behavior",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06755/x3.png",
                "caption": "Figure 7:Far Left:We train a linear head to predict semantic segmentation classes for each patch.Middle Left:We choose all sand-filled patches in the input image to inspect.Middle Right:Our SAE proposes exemplar images for the maximally activating sparse dimensions, as inSection5.1, suggesting that DINOv2 is learning a sand feature.Far Right:We suppress the sand feature in not just the selected patches, butallpatches. We modify all activation vectors and pass them to DINOv2‚Äôs final transformer layer followed by our trained linear segmentation head. We see that the head predicts ‚Äúearth, ground‚Äù and ‚Äúwater‚Äù for the former sand patches. Both classes are good second choices if ‚Äúsand‚Äù is unavailable. Notably, other patches are not meaningfully affected, demonstrating the pseudo-orthogonality of the SAE‚Äôs learned feature vectors.",
                "position": 677
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendices",
        "images": []
    },
    {
        "header": "Appendix ASAE Training Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-usa-positive-1.png",
                "caption": "Figure B1:Additional examples of cultural features learned by CLIP.Top:CLIP-24K/7622responds to symbolism from the United States of America, including a portrait of George Washington, but not to a portrait of King Louis XIV of France.Bottom:CLIP-24K/13871activates strongly on the Brandenburg Gate and other German symbols, but not on visually similar flags like the Belgian flag.",
                "position": 1730
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-usa-positive-2.png",
                "caption": "",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-usa-positive-3.png",
                "caption": "",
                "position": 1746
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-usa-positive-5.png",
                "caption": "",
                "position": 1747
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-usa-negative-1.png",
                "caption": "",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-13871-positive-1.png",
                "caption": "",
                "position": 1759
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-13871-positive-2.png",
                "caption": "",
                "position": 1760
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-13871-positive-3.png",
                "caption": "",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-13871-positive-5.png",
                "caption": "",
                "position": 1762
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/clip-vs-dinov2/clip-13871-negative-2.png",
                "caption": "",
                "position": 1763
            }
        ]
    },
    {
        "header": "Appendix CClassification Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/classification-extra1.jpg",
                "caption": "Figure C2:Tropical Kingbirds have a distinctive yellow chest. When we suppress a ‚Äúyellow feathers‚Äù feature, our linear classifier predicts Gray Kingbird, a similar species but with a gray chest. This example is available athttps://osu-nlp-group.github.io/SAE-V/demos/classification?example=5099",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/classification-extra2.jpg",
                "caption": "Figure C3:Canada Warblers have a distinctive black necklace on the chest.CLIP-24K/20376fires on similar patterns; when we suppress this feature, the linear classifier predicts Wilson Warbler, a similar species without the distinctive black necklace. This example is available athttps://osu-nlp-group.github.io/SAE-V/demos/classification?example=1129",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2502.06755/extracted/6193151/figures/classification-extra3.jpg",
                "caption": "Figure C4:Purple finches have bright red coloration on the head and neck area; when we suppressCLIP-24K/10273, which appears to be a ‚Äúred feathers‚Äù feature, our classifier predicts Field Sparrow, which has similar wing banding but no red coloration.\nThis example is available athttps://osu-nlp-group.github.io/SAE-V/demos/classification?example=4139",
                "position": 1829
            },
            {
                "img": "https://arxiv.org/html/2502.06755/x4.png",
                "caption": "Figure D5:DINOv2 correctly identifies the framed painting. We find thatDINOv2-24K/16446fires for paintings, and that suppressing this feature removes the painting without meaningfully affecting other parts of the image, despite modifying all patches. This example is available athttps://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1633.",
                "position": 1896
            },
            {
                "img": "https://arxiv.org/html/2502.06755/x5.png",
                "caption": "Figure D6:We find that featureDINOv2-24K/5876fires for toilets, and that suppressing this feature removes the toilet without meaningfully affecting other parts of the image, despite modifying all patches. This example is available athttps://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1099.",
                "position": 1899
            },
            {
                "img": "https://arxiv.org/html/2502.06755/x6.png",
                "caption": "Figure D7:After suppressing a ‚Äúbed‚Äù feature (DINOv2-24K/18834), the segmentation head predicts ‚Äúpillow‚Äù for the pillows and ‚Äútable‚Äù for the bed spread. This examples is available athttps://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1117.",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2502.06755/x7.png",
                "caption": "Figure D8:We remove all cars from the scene by suppressing a ‚Äúcar‚Äù-like feature (DINOv2-24K/7235). Notably, the van on the right of the image is also removed. This examples is available athttps://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1852.",
                "position": 1905
            }
        ]
    },
    {
        "header": "Appendix DSemantic Segmentation Details",
        "images": []
    }
]