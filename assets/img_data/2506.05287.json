[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/extracted/6516339/figures/icons/icon1.png",
                "caption": "",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x1.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x2.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x3.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x4.png",
                "caption": "Figure 1:Overview ofEOC-Bench.EOC-Benchassesses Embodied Object Cognition capabilities of MLLMs in egocentric videos across three dimensions - Past, Present, and Future - encompassing11categories.EOC-Benchincludes3,277object-level QA pairs utilizing a mixed-formathuman-in-the-loopannotation framework across diverse tasks and conditions.EOC-Benchaims to reveal the limitations of MLLMs and promote the development of robust egocentric cognition systems.",
                "position": 365
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x5.png",
                "caption": "Figure 2:Overall data distribution ofEOC-Bench.(a)EOC-Benchencompasses three temporal dimensions: Past, Present, and Future, comprehensively evaluating 11 embodied cognitive abilities. (b) The dataset comprises videos from four distinct open video sources as well as self-recorded videos. (c) It spans a wide range of scenarios, offering a rich diversity of contexts for analysis.",
                "position": 405
            }
        ]
    },
    {
        "header": "3EOC-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x6.png",
                "caption": "Figure 3:Statistic analysisofEOC-Bench: (a) substantial diversity in object categories and usage taxonomies, (b) a wide range of video durations correlated with question count, and (c) a balanced distribution of response options across each question type.",
                "position": 545
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x7.png",
                "caption": "Figure 4:Comparison of mainstream MLLMs onEOC-Bench. Left:Performance on 11 evaluation tasks withinEOC-Bench.Right:Performance across different question types spanning Past, Present and Future categories.",
                "position": 1699
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional Details on EOCBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x8.png",
                "caption": "Figure 5:Statistics distribution of human error ratio.(a) displays a histogram depicting the density and spread of human error ratios; (b) presents a pie chart categorizing the error ratios into quantitative segments.",
                "position": 2786
            }
        ]
    },
    {
        "header": "Appendix BMore Experimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x9.png",
                "caption": "Figure 6:Quantitative error analysis by type for choice-based questions inEOC-Bench.",
                "position": 3040
            }
        ]
    },
    {
        "header": "Appendix CExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x10.png",
                "caption": "Figure 7:Quantitative error analysis for open-ended questions inEOC-Bench.Left:Density analysis of temporal perception deviations (error ratio) among humans and models.Right:Model accuracy across different time thresholds for dynamic error margins.",
                "position": 3073
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x11.png",
                "caption": "Figure 8:Illustrative examples of annotation formats for visual prompts.",
                "position": 3417
            }
        ]
    },
    {
        "header": "Appendix DAdditional Dataset Analysis",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Broader Impacts",
        "images": []
    },
    {
        "header": "Appendix FAsset License and Consent",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05287/x12.png",
                "caption": "Figure 9:Worldcloud of questions and answers inEOC-Bench.",
                "position": 3594
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x13.png",
                "caption": "Figure 10:Failure cases of the top-performing GPT-4o onEOC-Bench.",
                "position": 3597
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x14.png",
                "caption": "Figure 11:Visualization of samples inObject State Retrospection (Past).",
                "position": 3600
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x15.png",
                "caption": "Figure 12:Visualization of samples inLocation Retrospection (Past).",
                "position": 3603
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x16.png",
                "caption": "Figure 13:Visualization of samples inObject Relationship Evolution (Past).",
                "position": 3606
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x17.png",
                "caption": "Figure 14:Visualization of samples inAbsolute Time Perception (Past).",
                "position": 3609
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x18.png",
                "caption": "Figure 15:Visualization of samples inImmediate State Recognition (Present).",
                "position": 3612
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x19.png",
                "caption": "Figure 16:Visualization of samples inObject Relationship (Present).",
                "position": 3615
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x20.png",
                "caption": "Figure 17:Visualization of samples inPurpose and Function Inference (Present).",
                "position": 3618
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x21.png",
                "caption": "Figure 18:Visualization of samples inAnomaly Perception (Present).",
                "position": 3621
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x22.png",
                "caption": "Figure 19:Visualization of samples inTrajectory and Motion Prediction (Future).",
                "position": 3624
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x23.png",
                "caption": "Figure 20:Visualization of samples inState Change Prediction (Future).",
                "position": 3627
            },
            {
                "img": "https://arxiv.org/html/2506.05287/x24.png",
                "caption": "Figure 21:Visualization of samples inDynamic Relationship Prediction (Future).",
                "position": 3630
            }
        ]
    },
    {
        "header": "Appendix GMore Exemplar Visualizations",
        "images": []
    }
]