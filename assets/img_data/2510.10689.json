[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10689/figures/logo.png",
                "caption": "",
                "position": 64
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10689/x1.png",
                "caption": "Figure 1:Examples in OmniVideoBench (“V” presents vision and “A” presents audio), and we present the atomic reasoning traces for these examples.",
                "position": 90
            }
        ]
    },
    {
        "header": "2OmniVideoBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10689/x2.png",
                "caption": "Figure 2:The complete pipeline of data collection, annotation, and refinement, where filtering and refinement serve as two key processes for quality assurance.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x3.png",
                "caption": "Figure 3:(a) OmniVideoBench covers 8 major categories and 68 subcategories. (b) OmniVideoBench comprises 13 task types.\nThe above part shows the video duration distribution across different tasks, while the durations are categorized into four groups:“Short”for less than 1 minute,“Medium”for 1–5 minutes,“Long”for 5–10 minutes, and“Ultralong”for more than 10 minutes. The lower part illustrates the distribution of three types of audio (i.e.,Speech,SoundandMusic). (c) Distribution of video durations across four time intervals. (d) Distribution of three audio types.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x4.png",
                "caption": "Figure 4:Performance comparison of selected models on OmniVideoBench and Daily-Omni. “Red line” denotes random guessing.",
                "position": 394
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10689/x5.png",
                "caption": "Figure 5:Performance Comparison of some Open-Source and Closed-Source Omni Models on 13 Tasks in OmniVideoBench. Here, “Attr”: Attribute Comparison, “Bac&Mu”: Background and Music Understanding, “Caus”: Cause and Effect Reasoning, “Coun”: Counting, “Ego”: Ego Reasoning, “Fine”: Fine-grained Perception, “Hypo”: Hypothetical Reasoning, “Ref”: Referential Reasoning, “Rela”: Relationship Reasoning, “Senti”: Sentiment Analysis, “Spati”: Spatial Reasoning, “Summ”: Summarization, “Tempo”: Temporal Sequencing Understanding.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x6.png",
                "caption": "(a)Accuracy rates of selected MLLMs under different inputs.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x6.png",
                "caption": "(a)Accuracy rates of selected MLLMs under different inputs.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x7.png",
                "caption": "(b)Accuracy of Gemini-2.0-Flash on videos with different audio types.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x8.png",
                "caption": "(a)Performance of Qwen2.5-Omni-7B and Qwen3-Omni-30B-A3B at different numbers of frames.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x8.png",
                "caption": "(a)Performance of Qwen2.5-Omni-7B and Qwen3-Omni-30B-A3B at different numbers of frames.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2510.10689/x9.png",
                "caption": "(b)Accuracy of Qwen3-Omni-30B-A3B on questions with videos of varying durations across different numbers of frames.",
                "position": 678
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFull Video Category Taxonomy",
        "images": []
    },
    {
        "header": "Appendix BDetailed Principles of Video Collection",
        "images": []
    },
    {
        "header": "Appendix CPrompts Used in This Work",
        "images": []
    }
]