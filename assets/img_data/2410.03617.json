[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03617/x1.png",
                "caption": "(a)Merging 2 experts, Held-In.",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x1.png",
                "caption": "(a)Merging 2 experts, Held-In.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x2.png",
                "caption": "(b)Merging 8 experts, Held-In.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03617/x3.png",
                "caption": "Figure 2:Merged experts created from big and strong base models generalize better than multitask models.We find that for strong base models as we merge more experts (x-axis,â†’â†’\\rightarrowâ†’), the merged modelâ€™s generalization performance (y-axis,â†‘â†‘\\uparrowâ†‘) monotonically increases to approach and eventually surpasses multitask baseline. (yellow line). More details in Section4.3.",
                "position": 186
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Large Scale Evaluation of Model Merging",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03617/x4.png",
                "caption": "Figure 3:Instruction-tuned models facilitate easier merging.ğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸â¢-â¢ğ™¸ğšƒğ™¿ğšŠğ™»ğ™¼-2-ğ™¸ğšƒ\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_IT(â€¢) consistently outperformsğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸ğ™¿ğšŠğ™»ğ™¼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2(â€¢) as shown by the huge gap between the green point (â€¢) being higher than red points (â€¢), across various merging methods, model sizes, and numbers of constituent models, indicating that stronger instruction-tuned base models enhance the performance of merged models. The dashed lines denoted the performance of the experts trained on the held-in tasks as defined in Â§3. For more details see Section4.1.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x5.png",
                "caption": "Figure 4:Bigger models merge better.On Held-In evaluations, we find that bigger models always perform better compared to smaller models, barring a few outliers. We find that large instruction tuned models likeğŸ¼ğŸºâ¢ğ™±64ğ™±\\mathtt{64B}typewriter_64 typewriter_Bğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸â¢-â¢ğ™¸ğšƒğ™¿ğšŠğ™»ğ™¼-2-ğ™¸ğšƒ\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITare the easiest to merge. For more details see Section4.2.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x6.png",
                "caption": "Figure 5:Merged models at scale generalize better.We plot the held-out generalization of the merged model for two merging methods. We also include the performance of base model (dashed line) and the multitask baseline (yellow line) which trained on a mixture of held-in tasks. We find that the number of constituent expert models (x-axis,â†’â†’\\rightarrowâ†’) had little effect on zero-shot generalization as shown in the left and center plots. However, increasing model size significantly toğŸ¼ğŸºâ¢ğ™±64ğ™±\\mathtt{64B}typewriter_64 typewriter_Bimproved the merged modelâ€™s performance over the base model (right plot). For more details see Section4.3.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x7.png",
                "caption": "Figure 6:Bigger model sizes can merge more experts.We merge experts of various sizes created fromğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸ğ™¿ğšŠğ™»ğ™¼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2andğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸â¢-â¢ğ™¸ğšƒğ™¿ğšŠğ™»ğ™¼-2-ğ™¸ğšƒ\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITmodels and plot the held-in (left) and held-out (right) performance of merged models. Whileğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸ğ™¿ğšŠğ™»ğ™¼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2â€™s held-in performance degrades with more experts,ğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸â¢-â¢ğ™¸ğšƒğ™¿ğšŠğ™»ğ™¼-2-ğ™¸ğšƒ\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITâ€™s performance stabilizes at a much higher level. Bothğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸ğ™¿ğšŠğ™»ğ™¼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2andğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸â¢-â¢ğ™¸ğšƒğ™¿ğšŠğ™»ğ™¼-2-ğ™¸ğšƒ\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITmodels consistently improve held-out generalization, particularly at 24B and 64B scales with increasing expert count. For more details see Section4.4.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x8.png",
                "caption": "Figure 7:Different merging methods become similar at scale.We plot the held-in and held-out performances of mergedğŸ¼ğŸºâ¢ğ™±64ğ™±\\mathtt{64B}typewriter_64 typewriter_Bğ™¿ğšŠğ™»ğ™¼â¢-â¢ğŸ¸â¢-â¢ğ™¸ğšƒğ™¿ğšŠğ™»ğ™¼-2-ğ™¸ğšƒ\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITmodels across different merging methods and numbers of constituent models. For more details see Section4.5.",
                "position": 447
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Task Descriptions.",
        "images": []
    },
    {
        "header": "Appendix BExpert Training Details",
        "images": []
    },
    {
        "header": "Appendix CFull Result Tables",
        "images": []
    }
]