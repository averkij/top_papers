[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03617/x1.png",
                "caption": "(a)Merging 2 experts, Held-In.",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x1.png",
                "caption": "(a)Merging 2 experts, Held-In.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x2.png",
                "caption": "(b)Merging 8 experts, Held-In.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03617/x3.png",
                "caption": "Figure 2:Merged experts created from big and strong base models generalize better than multitask models.We find that for strong base models as we merge more experts (x-axis,→→\\rightarrow→), the merged model’s generalization performance (y-axis,↑↑\\uparrow↑) monotonically increases to approach and eventually surpasses multitask baseline. (yellow line). More details in Section4.3.",
                "position": 186
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Large Scale Evaluation of Model Merging",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.03617/x4.png",
                "caption": "Figure 3:Instruction-tuned models facilitate easier merging.𝙿𝚊𝙻𝙼⁢-⁢𝟸⁢-⁢𝙸𝚃𝙿𝚊𝙻𝙼-2-𝙸𝚃\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_IT(•) consistently outperforms𝙿𝚊𝙻𝙼⁢-⁢𝟸𝙿𝚊𝙻𝙼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2(•) as shown by the huge gap between the green point (•) being higher than red points (•), across various merging methods, model sizes, and numbers of constituent models, indicating that stronger instruction-tuned base models enhance the performance of merged models. The dashed lines denoted the performance of the experts trained on the held-in tasks as defined in §3. For more details see Section4.1.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x5.png",
                "caption": "Figure 4:Bigger models merge better.On Held-In evaluations, we find that bigger models always perform better compared to smaller models, barring a few outliers. We find that large instruction tuned models like𝟼𝟺⁢𝙱64𝙱\\mathtt{64B}typewriter_64 typewriter_B𝙿𝚊𝙻𝙼⁢-⁢𝟸⁢-⁢𝙸𝚃𝙿𝚊𝙻𝙼-2-𝙸𝚃\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITare the easiest to merge. For more details see Section4.2.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x6.png",
                "caption": "Figure 5:Merged models at scale generalize better.We plot the held-out generalization of the merged model for two merging methods. We also include the performance of base model (dashed line) and the multitask baseline (yellow line) which trained on a mixture of held-in tasks. We find that the number of constituent expert models (x-axis,→→\\rightarrow→) had little effect on zero-shot generalization as shown in the left and center plots. However, increasing model size significantly to𝟼𝟺⁢𝙱64𝙱\\mathtt{64B}typewriter_64 typewriter_Bimproved the merged model’s performance over the base model (right plot). For more details see Section4.3.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x7.png",
                "caption": "Figure 6:Bigger model sizes can merge more experts.We merge experts of various sizes created from𝙿𝚊𝙻𝙼⁢-⁢𝟸𝙿𝚊𝙻𝙼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2and𝙿𝚊𝙻𝙼⁢-⁢𝟸⁢-⁢𝙸𝚃𝙿𝚊𝙻𝙼-2-𝙸𝚃\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITmodels and plot the held-in (left) and held-out (right) performance of merged models. While𝙿𝚊𝙻𝙼⁢-⁢𝟸𝙿𝚊𝙻𝙼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2’s held-in performance degrades with more experts,𝙿𝚊𝙻𝙼⁢-⁢𝟸⁢-⁢𝙸𝚃𝙿𝚊𝙻𝙼-2-𝙸𝚃\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_IT’s performance stabilizes at a much higher level. Both𝙿𝚊𝙻𝙼⁢-⁢𝟸𝙿𝚊𝙻𝙼-2\\mathtt{PaLM}\\texttt{-}\\mathtt{2}typewriter_PaLM - typewriter_2and𝙿𝚊𝙻𝙼⁢-⁢𝟸⁢-⁢𝙸𝚃𝙿𝚊𝙻𝙼-2-𝙸𝚃\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITmodels consistently improve held-out generalization, particularly at 24B and 64B scales with increasing expert count. For more details see Section4.4.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2410.03617/x8.png",
                "caption": "Figure 7:Different merging methods become similar at scale.We plot the held-in and held-out performances of merged𝟼𝟺⁢𝙱64𝙱\\mathtt{64B}typewriter_64 typewriter_B𝙿𝚊𝙻𝙼⁢-⁢𝟸⁢-⁢𝙸𝚃𝙿𝚊𝙻𝙼-2-𝙸𝚃\\mathtt{PaLM}\\texttt{-}\\mathtt{2}\\texttt{-}\\mathtt{IT}typewriter_PaLM - typewriter_2 - typewriter_ITmodels across different merging methods and numbers of constituent models. For more details see Section4.5.",
                "position": 447
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Task Descriptions.",
        "images": []
    },
    {
        "header": "Appendix BExpert Training Details",
        "images": []
    },
    {
        "header": "Appendix CFull Result Tables",
        "images": []
    }
]