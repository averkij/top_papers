[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x1.png",
                "caption": "Figure 1:Comparison of drag-based image editing methods. Our approach (rightmost columns) enables both precise edits and large-scale occlusion handling with real-time preview (0.01 sec) followed by rapid inpainting (0.3 sec). Users selectdeformable regionsand drag fromhandle pointstotarget positions, with the preview column showing grid overlays in areas requiring inpainting. In contrast, existing methods (leftmost columns) require substantially longer processing times without providing interactive feedback during editing, and often struggle with precise manipulations due to latent-space operations.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x2.png",
                "caption": "Figure 2:Overview of ouroptionalmask refinement module. Users can achieve precise object boundaries for coherent deformation through SAM[17], constrained by eroded and dilated mask boundaries to preserve user intent.",
                "position": 135
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x3.png",
                "caption": "Figure 3:Overview of our bidirectional warping pipeline. Given a user mask and control points, we first extract region contours and establish point associations. The forward warping step then maps these contours to their target locations and builds initial correspondences. Finally, through backward mapping, we generate the warped image with complete pixel coverage and its corresponding warped mask, which will be used for subsequent inpainting.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x4.png",
                "caption": "Figure 4:Overview of inpainting mask computation. The final mask (MinpaintM_{\\text{inpaint}}) combines dilated unmapped regions (MtempM_{\\text{temp}}) from areas absent after warping and dilated boundaries of the warped mask (∂Mwarped\\partial M_{\\text{warped}}). This ensures smooth transitions between warped and generated regions in the final result.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x5.png",
                "caption": "Figure 5:Qualitative comparison with state-of-the-art methods on challenging cases from DragBench-S[28]and DragBench-D[40]. Our method enables users to specifydeformable regionsand manipulate them by dragginghandle pointstowardtarget destinations. The grid overlay in the preview column indicates areas to be inpainted. Our approach shows advantages over existing methods by effectively preserving local details in dragged regions through our bidirectional warping algorithm while demonstrating strong capability in generating occluded regions. The real-time preview (∼\\sim10ms) allows users to interactively adjust edits before executing the inpainting process (∼\\sim0.3s).",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x6.png",
                "caption": "Figure 6:Qualitative results of our mask refinement moduleSec.3.1.Left:User-provided initial input mask.Middle:Raw segmentation predictions from Segment Anything Model (SAM).Right:Final refined results where the green dashed boundary, derived from dilating the initial input mask, effectively filters out undesired SAM predictions beyond the user’s intended scope.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x7.png",
                "caption": "",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x8.png",
                "caption": "Figure 7:Unidirectionalvs.bidirectional warping (ours). Our method fixes sampling gaps using backward pixel mapping.",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x9.png",
                "caption": "",
                "position": 537
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x10.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x11.png",
                "caption": "",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x12.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x13.png",
                "caption": "",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x14.png",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x15.png",
                "caption": "",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x16.png",
                "caption": "",
                "position": 548
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S1Supplementary Videos",
        "images": []
    },
    {
        "header": "S2Integration with More Inpainting Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x17.png",
                "caption": "Figure S1:Qualitative comparison of different inpainting methods. The figure illustrates how various inpainting approaches affect drag editing results, highlighting differences in visual fidelity, artifact handling, and preservation of semantic content across traditional and generative model-based techniques.",
                "position": 1415
            }
        ]
    },
    {
        "header": "S3Multi-round Interactive Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x18.png",
                "caption": "Figure S2:Multi-round interactive drag editing demonstrated through a three-move checkmate sequence including five consecutive edits. Users selectdeformable regions(chess pieces) and drag them fromhandle pointstotarget positions. Grid overlays in the preview columns indicate areas requiring inpainting. Our method provides real-time preview (∼\\sim10ms) of the warping effect, followed by high-quality inpainting results (∼\\sim0.3s). Existing approaches typically require minutes for inference and fail during the initial interaction.",
                "position": 1426
            }
        ]
    },
    {
        "header": "S4Discussion of Input Ambiguity",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x19.png",
                "caption": "Figure S3:Precise control over ambiguous drag operations.Left: Ambiguous sparse input from previous methods can represent at least five different user intentions.Right: Through our explicit deformation-based control interface (bottom-right insets), we precisely implement each distinct user intention, effectively eliminating ambiguity while maintaining intuitive interaction.",
                "position": 1437
            }
        ]
    },
    {
        "header": "S5More Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04582/x20.png",
                "caption": "Figure S4:Qualitative comparison of Inpaint4Drag with state-of-the-art methods: wildlife, artworks, flowers, birds, and landscapes.",
                "position": 1448
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x21.png",
                "caption": "Figure S5:Qualitative comparison of Inpaint4Drag with state-of-the-art methods: portraits, interiors, statues, wildlife, still life, and sports.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2509.04582/x22.png",
                "caption": "Figure S6:Qualitative comparison of Inpaint4Drag with state-of-the-art methods: urban scenes, landscapes, animals, pets, and reptiles.",
                "position": 1454
            }
        ]
    },
    {
        "header": "S6Pseudo Code for Inpaint4Drag",
        "images": []
    },
    {
        "header": "S7Limitations",
        "images": []
    }
]