[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08406/x1.png",
                "caption": "Figure 1:Illustration of temporally consistent Human Mesh Recovery (HMR) from\nvideos. (a) Input video frames. (b) Identity-consistent human masks, where each\nperson is highlighted with a unique and consistent colour across frames. (c)\nVanilla image-to-video HMR baseline using SAM 3D Body with automatic human\ndetection and per-frame inference. Note that only the meshes corresponding to\nthe masks in (b) are visualised here; if a mesh does not appear in a certain\nframe, it indicates that the corresponding person is not detected in that\nframe. (d) Our spatial-temporal consistent HMR, where the temporal continuity\nin masklets is directly propagated into the 4D human meshes. (e) Our full\nSAM-Body4D with occlusion-aware refinement. Across the 2nd–5th columns,\nSAM-Body4D recovers plausible and temporally stable reconstructions under\nocclusion. As these humans\nare heavily occluded, their complete meshes are visualised in the bottom-left\ncorner for clearer observation.",
                "position": 80
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08406/x2.png",
                "caption": "Figure 2:Overall framework of the proposed SAM-Body4D. Given an input video with human prompts, SAM-Body4D operates on three main modules in a training-free manner. TheMasklet Generatorderives identity-consistent temporal masklets from the video to provide spatio-temporal tracking cues. TheOcclusion-Aware Masklet Refinerenriches these masklets by recovering invisible body regions and stabilizing temporal alignment. Finally, theMask-Guided HMR moduleuses refined masklets as spatial prompts to predict accurate and temporally coherent human meshes across the entire sequence.",
                "position": 187
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08406/x3.png",
                "caption": "Figure 3:Visualised comparisons between the vanilla image-to-video extension of SAM 3D-Body and our SAM-Body4D. (a) Input video frames. (b) Identity-consistent human masks. (c) Vanilla per-frame HMR results using SAM 3D-Body with automatic human detection, where missed detections lead to missing meshes. (d) Our SAM-Body4D maintains temporally continuous and identity-preserving mesh trajectories throughout the video by leveraging spatial-temporal masklet guidance.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2512.08406/x4.png",
                "caption": "Figure 4:Visualised comparisons between SAM-Body4D w/o and w/ Occlusion-Aware Masklet Refiner. (a) Input video frames; (b) Temporally consistent human masks, where each person is highlighted with a unique and consistent color across frames; (c) SAM-Body4D without Occlusion-Aware Masklet Refiner; (d) SAM-Body4D with Occlusion-Aware Masklet Refiner. Across the 2nd–6th columns, SAM-Body4D produces more robust reconstructions under occlusion (e.g., the blue-rendered person in the 2nd column, the purple-rendered people in the 3rd/4th column, and the green-rendered people in the 5th and 6th columns). Since these subjects are heavily occluded, their meshes without occlusion are shown at the bottom-left/bottom-right for clearer observation.",
                "position": 350
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]