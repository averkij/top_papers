[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x1.png",
                "caption": "",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x2.png",
                "caption": "",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x3.png",
                "caption": "Figure 2:Conceptual illustration and overview of the FMA-Net++ framework.",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x4.png",
                "caption": "Figure 3:Architecture of FMA-Net++ for joint video super-resolution and deblurring (VSRDB).",
                "position": 190
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x5.png",
                "caption": "Figure 4:Details of an HRBP block. (a) Structure of the HRBP block at (j+1)-threfinement step fori-thframe (Sec.3.3). (b) Structure of Multi-Attention. FFN refers to the feed-forward network of the transformer[vaswani2017attention,dosovitskiy2020image].",
                "position": 250
            }
        ]
    },
    {
        "header": "4Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x6.png",
                "caption": "Figure 5:Qualitative comparisons of√ó4\\times 4VSRDB on REDS4-ME-5:55\\!:\\!5and GoPro[nah2017deep]. Each scene contains severe motion blur with different characteristics.Best viewed in zoom.",
                "position": 586
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Detailed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x7.png",
                "caption": "Figure 6:An overview of our three-stage training strategy.",
                "position": 1056
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x8.png",
                "caption": "Figure 7:Example frames from our REDS-ME dataset across five exposure levels and two different scenes. Each column corresponds to an exposure ratio from5:15\\!:\\!1(shortest exposure) to5:55\\!:\\!5(longest exposure). Longer exposures lead to increasingly severe motion blur, effectively simulating real-world camera motion.",
                "position": 1110
            }
        ]
    },
    {
        "header": "8Detailed Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x9.png",
                "caption": "Figure 8:t-SNE[van2008visualizing]visualization of exposure time-aware featuresùíñi\\bm{u}_{i}extracted by ETE, showing their distinguishability across different exposure levels.",
                "position": 1149
            }
        ]
    },
    {
        "header": "9Further Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x10.png",
                "caption": "Figure 9:Effect of ETE guidance on the exposure-aware degradation kernels predicted by NetD. For a severely blurred frame from REDS4-ME-5:55\\!:\\!5, the kernel becomes spatially diffuse with correct exposure guidance (5:55\\!:\\!5) and highly concentrated with incorrect guidance (5:15\\!:\\!1), demonstrating exposure-dependent behavior.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x11.png",
                "caption": "Figure 10:Effect of the number of multi-flow-mask pairs (nn) on predicted optical flow for a severely blurred scene.",
                "position": 1280
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x12.png",
                "caption": "Figure 11:Visualization of the progressive feature refinement through HRBP blocks across four iterations.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x13.png",
                "caption": "Figure 12:Visualization of synthesized exposure trajectories in the REDS-RE benchmark. Each colored line represents the evolution of the exposure level for a different test scene. The trajectories follow a step-wise random walk with varying update intervals, simulating the stable yet dynamic nature of real-world auto-exposure adjustments.",
                "position": 1292
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x14.png",
                "caption": "Figure 13:Qualitative comparison between FMA-Net++ (Ours) and FMA-Net‚àó[Youk_2024_CVPR]in a challenging scene featuring facial details and severe motion blur.",
                "position": 1295
            }
        ]
    },
    {
        "header": "10Additional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04390/x15.png",
                "caption": "Figure 14:Additional qualitative comparisons on the REDS4-ME-5:55\\!:\\!5and GoPro[nah2017deep]datasets. These scenes feature severe motion blur and complex textures, representing challenging degradation scenarios. FMA-Net++ consistently reconstructs sharper structural details and cleaner edges while effectively suppressing motion artifacts, outperforming state-of-the-art methods.Best viewed in zoom.",
                "position": 1421
            },
            {
                "img": "https://arxiv.org/html/2512.04390/x16.png",
                "caption": "Figure 15:Additional qualitative comparisons on challenging real-world videos captured with smartphones. These videos contain continuous auto-exposure transitions and non-uniform motion blur, deviating significantly from the discrete synthetic conditions used during training. FMA-Net++ exhibits strong generalization, recovering legible text and fine textures while preserving natural exposure characteristics, and consistently achieves the best perceptual scores (NIQE‚Üì\\downarrow/ MUSIQ‚Üë\\uparrow).Best viewed in zoom.",
                "position": 1424
            }
        ]
    },
    {
        "header": "11Limitations",
        "images": []
    }
]