[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x1.png",
                "caption": "",
                "position": 169
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x2.png",
                "caption": "Figure 2:Our method generates coherent blends that focus on the most relevant attributes shared by the input imagesâ€”the hairstyle. Diffusion-based morphing methods like DiffMorpher[48]and Yu et al.[47]struggle to produce realistic blends of distant concepts, and AID[21]fails to capture hairstyle as the relevant attribute.",
                "position": 233
            }
        ]
    },
    {
        "header": "3Creative Path Finding",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x3.png",
                "caption": "Figure 3:Top:Using a 2D point cloud example, the forward mapping involves computing the(a.)affinity graphð–\\mathbf{W}of the pointsð±\\mathbf{x}on the manifold and(b.)generalized eigenvectorsðš¿â€‹(ð±)\\mathbf{\\Psi}(\\mathbf{x})of the graph Laplacianð‹\\mathbf{L}as manifold coordinates of the point cloudð±\\mathbf{x}.(c.)The inverse mapping performs linear interpolation in the manifold space and uses graph diffusion inversion to obtain the corresponding path in the original point cloud space.Bottom:(d.)On real images, we extract patch tokens from DINO features as graph nodes and compute token-wise affinityð–\\mathbf{W}.(e.)The topmmgraph eigenvectors produce co-salient segments across two images for blending, and manifold coordinates for expressing â€œvibeâ€ features for blending.(f.)Similar to the point cloud example, we apply graph diffusion inversion to obtain a path in CLIP space. We â€œrenderâ€ pixel images from CLIP features using a frozen IP-Adapter[45]. We train two lightweight MLP networks in under 30 seconds: an encoder to simulate and compress the forward mapping, and a decoder to mimic the inverse mapping.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x4.png",
                "caption": "Figure 4:From Vibe Blending to Vibe Analogy. Vibe Space enables creative connections between input imagesAAandBB. A path approximately linear in Vibe Space results in a continuous manifold-following path in the ambient feature space, such as CLIP. We can lift the â€œvibeâ€ðš«Aâ†’B\\mathbf{\\Delta}_{A\\to B}to a non-trivial but related imageAâ€²A^{\\prime}to extrapolate an analogous path in the ambient space, resulting in imageBâ€²B^{\\prime}that reflects the same vibe. For example, we can morph Leonardo DiCaprioâ€™s face into a playing card.",
                "position": 300
            }
        ]
    },
    {
        "header": "4Vibe Space and Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x5.png",
                "caption": "Figure 5:Negative vibe control. Vibe attributes are implicitly extracted by Vibe Space. The blending pair defines desired vibes (rotation + style). The negative pair defines vibes to suppress (style). Blending without negative examples transfers both attributes. Subtracting the negative vibe, only rotation is blended.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x6.png",
                "caption": "Figure 6:Left:To gauge human perceptions of creativity, we ask raters to compare image pairs along two axes:Creative Potentialrefers to how interesting a blend might be, andBlend Difficultyindicates how challenging it is to form a coherent blend. Image pairs with higher Blend Difficulty tend to have higher Creative Potential and are often more conceptually different. Numbers in each cell indicate the number of examples.Right:When evaluating blend creativity across methods, raters first identify the key shared attributes in the input images.",
                "position": 478
            }
        ]
    },
    {
        "header": "5How to Measure the Creativity of a Blend?",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x7.png",
                "caption": "Figure 7:Top:Vibe-decoded (curved) vs. straight path in CLIP space for asingleimage pair; we sample 20 points to capture curvature. Pairs that humans judge as more difficult exhibit more curvature.Bottom:For eachcomparison pair(two different image pairs shown together), we plotÎ”â€‹PNS=PNSpreferredâˆ’PNSother\\Delta\\text{PNS}=\\text{PNS}_{\\text{preferred}}-\\text{PNS}_{\\text{other}}. Positive values indicate that PNS agrees with the human choice. LargerÎ”â€‹PNS\\Delta\\text{PNS}correlates with higher rater consensus. Negative values indicate disagreement with human consensus.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x8.png",
                "caption": "Figure 8:Our method creatively blends the relevant visual attributes that are similar between the inputs, or â€œvibeâ€. For example, the vibe between the map and chicken is the shape, not color or texture. In contrast, the baselines often fail to recognize the key attributes or fuse them effectively. CLIP Avg refers to averaging the input image embeddings and feeding the resulting embedding into IP-Adapter[45], akin to using weight 0.5 in CLIP linear interpolation. Gemini[12]and GPT[30]often perform a part-level composition or style transfer.",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x9.png",
                "caption": "Figure 9:LLM judges offer a useful approximation of human preferences but can exhibit failure modes. The LLM correctly identifies the key attribute as the hairstyle. However, the LLM also references irrelevant attributes like color and bases its preference on texture and body composition rather than focusing on hairstyle.",
                "position": 772
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AComparisons to Current Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/metric_limitations.jpg",
                "caption": "Figure 10:Limitation of quantitative evaluation. While CLIP Avg achieved a higher score on the first three pairs, it fails to capture the main attributes (mouth in the first row, hairstyle in the second row, hairstyle in the third row) and instead blends all possible attributes. Our Vibe Space, on the other hand, successfully blends the main attributes without blending distracting attributes.",
                "position": 1016
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/02140_High.jpg",
                "caption": "(a)User rated blend difficulty is: High.User annotated main attribute: â€œHair Styleâ€",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/02140_High.jpg",
                "caption": "(a)User rated blend difficulty is: High.User annotated main attribute: â€œHair Styleâ€",
                "position": 1022
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/02268_High.jpg",
                "caption": "(b)User rated blend difficulty is: High.User annotated main attribute: â€œTeeth and Eyesâ€",
                "position": 1028
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/02718_High.jpg",
                "caption": "(c)User rated blend difficulty is: High.User annotated main attribute: â€œMouth Shapeâ€",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/04624_High.jpg",
                "caption": "(d)User rated blend difficulty is: High.User annotated main attribute: â€œCurly Hairâ€",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/01577_Medium.jpg",
                "caption": "(a)User rated blend difficulty is: Medium.User annotated main attribute: â€œFacial Expressionâ€",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/01577_Medium.jpg",
                "caption": "(a)User rated blend difficulty is: Medium.User annotated main attribute: â€œFacial Expressionâ€",
                "position": 1053
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/01988_Medium.jpg",
                "caption": "(b)User rated blend difficulty is: Medium.User annotated main attribute: â€œHair Styleâ€",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/02145_Medium.jpg",
                "caption": "(c)User rated blend difficulty is: Medium.User annotated main attribute: â€œGrinâ€",
                "position": 1066
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/05989_Medium.jpg",
                "caption": "(d)User rated blend difficulty is: Medium.User annotated main attribute: â€œHair Styleâ€",
                "position": 1072
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/05783_Low.jpg",
                "caption": "(a)User rated blend difficulty is: Low.User annotated main attribute: â€œSpiky Hairâ€",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/05783_Low.jpg",
                "caption": "(a)User rated blend difficulty is: Low.User annotated main attribute: â€œSpiky Hairâ€",
                "position": 1084
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/00666_Low.jpg",
                "caption": "(b)User rated blend difficulty is: Low.User annotated main attribute: â€œGrimaceâ€",
                "position": 1090
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/01028_Low.jpg",
                "caption": "(c)User rated blend difficulty is: Low.User annotated main attribute: â€œHair Styleâ€",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/03432_Low.jpg",
                "caption": "(d)User rated blend difficulty is: Low.User annotated main attribute: â€œClothingâ€",
                "position": 1103
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/001-015_no_difficulty.jpg",
                "caption": "(a)Architecture dataset.",
                "position": 1112
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/001-015_no_difficulty.jpg",
                "caption": "(a)Architecture dataset.",
                "position": 1115
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/026-184_no_difficulty.jpg",
                "caption": "(b)Architecture dataset.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/056-187_no_difficulty.jpg",
                "caption": "(c)Architecture dataset.",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/082-156_no_difficulty.jpg",
                "caption": "(d)Architecture dataset.",
                "position": 1131
            }
        ]
    },
    {
        "header": "Appendix BAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/05698_Low.jpg",
                "caption": "Figure 15:Ablation study on Flag Loss.Insights:without flag loss, the blending failed to capture the main attribute.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/01208_Medium.jpg",
                "caption": "",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/01577_Medium_ablation.jpg",
                "caption": "",
                "position": 1606
            }
        ]
    },
    {
        "header": "Appendix CPath Finding Details",
        "images": []
    },
    {
        "header": "Appendix DVibe Space Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/correspondance_example.jpg",
                "caption": "Figure 16:Examples of correspondence matching. We use NCut clustering[43]and Hungarian matching to compute correspondence between segments.",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x10.png",
                "caption": "Figure 17:Vibe Analogy examples. Vibe Analogy can transfer the vibe of imagesAâ†’BA\\to Bto another non-trivial but related imageAâ€²A^{\\prime}to generateBâ€²B^{\\prime}. Examples of transferred vibes are human-object interaction and art styles between similar facial expressions.",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x11.png",
                "caption": "Figure 18:Negative vibe control. Vibe attributes are implicitly extracted by Vibe Space. The blending pair defines desired vibes (rounded shape and material). The negative pair defines vibes to suppress (angular shape and white color). Blending without negative examples may transfer more attributes than desired. Subtracting the negative vibes, we better preserve the rounded shape and tan color of the building.",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x12.png",
                "caption": "Figure 19:Heatmaps illustrating the top-ranked DINO channels corresponding to the coarse and fine attributes shared by the input images. For example, in the second row, the coarse attributes may include the body pose and object identity, while the fine-grained attributes include the hand-object interaction.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x13.png",
                "caption": "Figure 20:Our method generates a continuous path of blends for any interpolation weightÎ±âˆˆ[0,1]\\alpha\\in[0,1]. We propose an automated procedure to select the bestÎ±\\alphaby identifying the point of greatest conceptual transition along the blend path. Specifically, we compute aconsistency scoreto measure the difference between an â€œidealâ€ path decoded from Vibe Space and a â€œrealizedâ€ path obtained by re-encoding the generated images. In this example, the optimal weightÎ±âˆ—=0.6\\alpha^{*}=0.6as determined by the â€œdipâ€ in the score. Qualitatively, we observe that theÎ±âˆ—\\alpha^{*}obtained via this algorithm achieves the best creative blend of the inputs, and we use this approach for our evaluation.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x14.png",
                "caption": "Figure 21:Examples of blended images selected via the optimal blend weightÎ±âˆ—\\alpha^{*}determined by our â€œdipâ€ in CLIP consistency score, described inFig.20. We observe that the â€œdipâ€ in consistency score generally results in the best blend of the relevant attributes in both input images.",
                "position": 2284
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/interpolation_extra_images.jpg",
                "caption": "Figure 22:Vibe Blending with extra image training. We compare training Vibe Space with only two images (middle row) and with five images (bottom row), input images and extra images are in the first row. Training with extra glass window images helps capture the â€œglass windowâ€ vibe, resulting in a glass texture pyramid in the middle-blend image.",
                "position": 2408
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x15.png",
                "caption": "Figure 23:Paths computed by Vibe Blending can be extrapolated to exaggerate attributes.Top:Extrapolating the path from a dog to a fish emphasizes the body shape of the fish and some elements of the background. However, extrapolation behavior is not well controlled at higher weights.Bottom:Extrapolating from an orange sports car to a red sedan continues the color shift, resulting in further darkening.",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures/dog_ram_horse.jpg",
                "caption": "Figure 24:N-Image Blending extends Vibe Blending to an arbitrary number of images. In this visualization, we show 3-Image Blending between a dog, ram, and horse. The top image (dog) is used as the base image to anchor correspondence matching and blending.",
                "position": 2479
            }
        ]
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    },
    {
        "header": "Appendix FAdditional Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x16.png",
                "caption": "Figure 25:Example chat with Gemini[12]to generate a creative blend.",
                "position": 2684
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x17.png",
                "caption": "Figure 26:Example chat with the LLM judge[29]to select the best blended output.",
                "position": 2706
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x18.png",
                "caption": "Figure 27:Examples where human raters and the LLM judge agree (left) and disagree (right) on the best blended output.",
                "position": 2712
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x19.png",
                "caption": "Figure 28:Extension ofFig.6. To gauge human perceptions of creativity, we ask raters to compare image pairs along two axes:Creative Potentialrefers to how interesting a blend might be, andBlend Difficultyindicates how challenging it is to form a coherent blend. Image pairs with higher Blend Difficulty tend to have higher Creative Potential and are often more conceptually different. Numbers in each cell indicate the number of examples.",
                "position": 2719
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x20.png",
                "caption": "Figure 29:User study for evaluating the Creative Potential and Blend Difficulty of image pairs.Top:The prompt shown to users.Bottom:Two examples with high rater agreement. Participants select which image pair has higher Creative Potential and higher Blend Difficulty.",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x21.png",
                "caption": "Figure 30:User study for assessing human preference of creative blends.(a.)The prompt shown to users.(b.)First, we ask participants to describe the main visual attributes shared by the two input images.(c.)Second, raters rank the blended output images from different methods based on how well they blend the main attributes.",
                "position": 2733
            }
        ]
    },
    {
        "header": "Appendix GFailure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14884/x22.png",
                "caption": "Figure 31:Negative vibe failure case. The positive inputs capture both a style change based on the types of car and a color change. The negative inputs intend to capture only color change. However, the attributes of style and color are entangled, making them difficult to separate with negative examples.",
                "position": 2842
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x23.png",
                "caption": "Figure 32:Failure case of Vibe Blending extrapolation. Extrapolating beyondÎ±>1\\alpha>1does not produce further rotation of the object in the input images.",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2512.14884/x24.png",
                "caption": "Figure 33:Failure cases of unsupervised correspondence. Our method depends on correspondence matching to identify which semantic regions should be blended. Different random seeds lead to different clusterings and therefore different correspondence maps. Good correspondence (top row) yields clean and semantically meaningful blends, whereas poor correspondence (middle and bottom rows) leads to low-quality blends with incorrect part-level mixing.",
                "position": 2867
            },
            {
                "img": "https://arxiv.org/html/2512.14884/figures_appendix/ipa_recon.jpg",
                "caption": "Figure 34:Failure cases of IP-Adapter reconstruction. Left column is input images; right four columns are reconstructed images from 4 random seeds. IP-Adapter reliably reconstructs images that lie within the Stable Diffusion training distribution (top row), producing consistent outputs across random seeds. In contrast, for out-of-distribution inputs (middle and bottom rows), IP-Adapter consistently fails to reproduce the original image, illustrating a fundamental limitation of SD-based decoders when applied to concept-level blending tasks.",
                "position": 2872
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]