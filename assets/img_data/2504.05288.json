[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05288/x1.png",
                "caption": "",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2LiveVQA: The Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05288/x2.png",
                "caption": "Figure 2:Pipline ofLiveVQAdata engine.Our pipeline consists of three modules: news collector, data filter, and Q&A pairs builder. It collects illustrated news from mainstream media, performs multi-level data filtering, and generates foundational and detailed Q&A pairs for training multimodal question-answering models.",
                "position": 288
            }
        ]
    },
    {
        "header": "3Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/1_1.png",
                "caption": "Figure 3:Large visual reasoning model QvQ-72B-Preview perform best on cross-modality multi-hop pproblems.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2504.05288/x3.png",
                "caption": "Figure 4:MMSearch[12]enables GPT-4o[16]to answer a knowledge-intensive visual question correctly by retrieving external evidence, whereas it fails without retrieval.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/recognition.png",
                "caption": "(a)Recognition Error",
                "position": 730
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/recognition.png",
                "caption": "(a)Recognition Error",
                "position": 733
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/reasoning.png",
                "caption": "(b)Reasoning Error",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/Ambiguous.png",
                "caption": "(c)Vague Answer",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/privacy.png",
                "caption": "(d)Privacy Restriction",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2504.05288/x4.png",
                "caption": "Figure 6:Distribution of Error types of GPT-4o[16]inLiveVQA.",
                "position": 885
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ackownledgement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Additional Experiment Results",
        "images": []
    },
    {
        "header": "7Prompt",
        "images": []
    },
    {
        "header": "8Taxonomy",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/2.png",
                "caption": "Figure 7:The search functionality enhances Gemini-2.0-Flashâ€™s performance.",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/p1.png",
                "caption": "Figure 8:An example of Event type of question-answer.",
                "position": 2253
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/p2.png",
                "caption": "Figure 9:An example of Person type of question-answer.",
                "position": 2267
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/p3.png",
                "caption": "Figure 10:An example of Location type of question-answer.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/p4.png",
                "caption": "Figure 11:An example of Time type of question-answer.",
                "position": 2295
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/org.png",
                "caption": "Figure 12:An example of Organization type of question-answer.",
                "position": 2309
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/obj.jpg",
                "caption": "Figure 13:An example of Object type of question-answer.",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/111.png",
                "caption": "Figure 14:An example of Reason type of question-answer.",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2504.05288/extracted/6342913/figures/222.png",
                "caption": "Figure 15:An example of Other type of question-answer.",
                "position": 2353
            }
        ]
    },
    {
        "header": "9Case Study",
        "images": []
    }
]