[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09343/x1.png",
                "caption": "Figure 1.Basic architecture of DeepSeek-V3. Built upon DeepSeek-V2â€™s MLA and DeepSeekMoE, a Multi-Token Prediction Module and FP8 mixed-precision training are introduced to enhance inference and training efficiency. The figure indicates the precision used for computations in different parts of the architecture. All components take inputs and outputs in BF16.",
                "position": 251
            }
        ]
    },
    {
        "header": "2.Design Principles for DeepSeek Models",
        "images": []
    },
    {
        "header": "3.Low-Precision Driven Design",
        "images": []
    },
    {
        "header": "4.Interconnection Driven Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09343/x2.png",
                "caption": "Figure 2.H800 node interconnection.",
                "position": 634
            }
        ]
    },
    {
        "header": "5.Large Scale Network Driven Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09343/x3.png",
                "caption": "Figure 3.Eight-plane two-layer fat-tree scalue-out network: Each GPU and IB NIC pair belongs to one network plane. Cross-plane traffic must use another NIC and PCIe or NVLink for intra-node forwarding.",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2505.09343/x4.png",
                "caption": "Figure 4.Ideal Multi-Plane Network: Each NIC is equipped with multiple physical ports, each connected to a distinct network plane. A single queue pair (QP) can simultaneously utilize all available ports for transmitting and receiving packets, which necessitates native support for out-of-order placement within the NIC.",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2505.09343/x5.png",
                "caption": "Figure 5.NCCL all-to-all performance from 32 to 128 GPUs for MRFT and MPFT networks.",
                "position": 942
            },
            {
                "img": "https://arxiv.org/html/2505.09343/x6.png",
                "caption": "Figure 6.Latency comparison between MPFT and MRFT networks in NCCL all-to-all test under different message sizes, showing that their performance is nearly identical.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2505.09343/x7.png",
                "caption": "Figure 7.DeepEP performance on MPFT: The EP dispatch and combine kernel communicates across 16 to 128 GPUs using all-to-all. Each GPU processes 4096 tokens. The observed throughput nearly saturates the 400Gps NIC bandwidth.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2505.09343/x8.png",
                "caption": "Figure 8.RoCE network bandwidth of AllGather and ReduceScatter communication primitives under different routing methods (ECMP, AR, Static Routing) and TP dimensions.",
                "position": 1135
            }
        ]
    },
    {
        "header": "6.Discussion and Insights for Future Hardware Architecture Design",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]