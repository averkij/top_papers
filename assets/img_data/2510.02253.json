[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02253/x1.png",
                "caption": "Figure 1:Comparison of drag-editing results between baselines and our method,DragFlow. DragFlow successfully unleashes FLUX‚Äôs stronger generative prior, removing the distortions that previous methods produced on challenging scenarios.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x2.png",
                "caption": "Figure 2:Comparison of feature maps extracted from UNet and DiT at the same denoising step. UNet produces spatially compact, highly compressed features that capture high-level semantic information, whereas DiT generates finer-grained, spatially precise representations.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x3.png",
                "caption": "Figure 3:Overview of the DragFlow framework.The original image is inverted into a noisy latent space and iteratively optimized under the proposed region-level affine supervision. Subject consistency is reinforced through key-value (KV) injection and our adapter-enhanced inversion, while background fidelity is maintained via gradient mask-based hard constraints. In addition, a multimodal large language model (MLLM) is employed to better interpret and clarify user intents.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x4.png",
                "caption": "Figure 4:Visualization of the effect of adapter-enhanced inversion on subject consistency, compared with KV injection alone.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x5.png",
                "caption": "Figure 5:Qualitative comparison of our method with multiple baselines in challenging scenarios.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x6.png",
                "caption": "Figure 6:Qualitative ablation study comparing different variants of our framework.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x7.png",
                "caption": "Figure 7:Visualization of DiT latent features (Sample 1 out of 2) based on PCA using the top55principal components.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x8.png",
                "caption": "Figure 8:Visualization of DiT latent features (Sample 2 out of 2) based on PCA using the top55principal components.",
                "position": 2275
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x9.png",
                "caption": "Figure 9:Region operation masksùêåi(k)\\mathbf{M}_{i}^{(k)}created by progressive affine transformations at each stepkkacross multiple subtasks. Each dragging process consists of5050steps (k=0k=0to4949), followed by2020additional steps (k=50k=50to6969) that repeat the final motion iteration to further refine the feature quality of the post-dragging region.",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x10.png",
                "caption": "Figure 10:For IFs2tand IFs2s, the criterion computation considers only the feature discrepancies within the labeled blocks. In IFs2t, the purple region on the left image denotes(ùë¥i(K)‚äôùíôaff)(\\bm{M}^{(K)}_{i}\\odot\\bm{x}_{\\text{aff}}), corresponding to the source region of(ùë¥i(0)‚äôùíô)(\\bm{M}^{(0)}_{i}\\odot\\bm{x}), while the red patch on the right image represents the post-drag target(ùë¥i(K)‚äôùíô‚Ä≤)(\\bm{M}^{(K)}_{i}\\odot\\bm{x}^{\\prime}). By contrast, the criterion IFs2scompares the same purple original regionùë¥i(0)\\bm{M}^{(0)}_{i}across two images: the sourceùíô\\bm{x}(left) and the dragged resultùíô‚Ä≤\\bm{x^{\\prime}}(right). Lastly, IFbgevaluates all uneditable areas, as indicated by the black areas on the gradient mask as(1‚àíùë©)(1-\\bm{B}).",
                "position": 2517
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x11.png",
                "caption": "Figure 11:Examples of real data samples from our ReD benchmark: the first row shows the expected dragging results, where the green region is estimated from the user-specified target centroid (see instruction); the second row presents the source images, while the third row highlights the user-marked operation regions in the form of masks, which may include multiple valid regions; and the last row depicts the adaptively generated masks derived from AppendixD.2. Those two examples correspond to the instructions provided in AppendixG.3.",
                "position": 2668
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x12.png",
                "caption": "Figure 12:Extra qualitative comparison (Part 1 out of 2) of DragFlow with multiple baselines.",
                "position": 2834
            },
            {
                "img": "https://arxiv.org/html/2510.02253/x13.png",
                "caption": "Figure 13:Extra qualitative comparison (Part 2 out of 2) of DragFlow with multiple baselines.",
                "position": 2838
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]