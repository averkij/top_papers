[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x1.png",
                "caption": "Figure 1:Depth Ambiguity Issue. As shown in the legend in the upper left corner of the figure, when there are specular reflections, there will be two depths at the mirror position: the depth of the mirror surfacedSd_{S}and the depth of the objectâ€™s reflectiondRd_{R}. In the real physical world, these two points are warped separately according to their respective disentangled depths. However, depth estimation algorithms cannot predict multiple depths at the same position. Therefore, the inverse relationship between depth and disparity breaks down. This will cause Depth-Warp-Inpaint (DWI) type methods to predict results with incorrect disparity.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2512.16915/x2.png",
                "caption": "Figure 2:Parallel vs. Converged.In the parallel setup, when both eyes observe the same subject, the projected image points on the left and right views are denoted asğ—ğ‹\\mathbf{X_{L}}andğ—ğ‘\\mathbf{X_{R}}, and their absolute difference|ğ—ğ‹âˆ’ğ—ğ‘||\\mathbf{X_{L}}-\\mathbf{X_{R}}|defines the disparityğ’”\\boldsymbol{s}.\nAccording to geometric relationships derived from similar triangles,ğ’ƒ\\boldsymbol{b},ğ’‡\\boldsymbol{f},ğ’…\\boldsymbol{d}, andğ’”\\boldsymbol{s}satisfy an inverse proportionality between disparity and depth when the baselineğ’ƒ\\boldsymbol{b}and focal lengthğ’‡\\boldsymbol{f}remain constant.\nIn the converged configuration, aZero-disparity Projection Planeis presentâ€”objects in front of this plane yield positive disparity, while those behind it produce negative disparity.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x3.png",
                "caption": "Figure 3:The inherent stochasticity of generative models can cause them to fabricate objects not present in the source view. As this figure illustrates, the right view generated by ReCamMaster erroneously introduces new artifacts, e.g., a car and a man (highlighted in red bounding box), that do not exist in the original input.",
                "position": 221
            }
        ]
    },
    {
        "header": "3Dataset Construction â€” UniStereo",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x4.png",
                "caption": "Figure 4:UniStereo processing pipeline.We use green icons with numbered steps to depict theStereo4Dpipeline: starting from the raw VR180 videos, we set hfov = 90Â° and specify the projection resolution to produce the final left- and right-eye monocular videos. Simultaneously, blue icons with numbered steps denote the3DMoviepipeline: we segment the source films into clips, filter out non-informative segments, convert from side-by-side (SBS) to left/right monocular views, and remove black borders. All resulting videos are captioned using ShareGPT4Video[chen2024sharegpt4video].",
                "position": 235
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x5.png",
                "caption": "Figure 5:The training framework of the proposed StereoPilot.StereoPilot uses asingle-step feed-forwardarchitecture (Diffusion as Feed-Forward) that incorporates alearnable domain switcherssto unify conversion for both parallel and converged stereo formats. The entire model is optimized using acycle-consistent training strategy, combining reconstruction and cycle-consistency losses to ensure high fidelity and precise geometric alignment.\nTheblueandorangelines represent the Left-to-Right and Right-to-Left reconstruction processes, and theorange dashed linedenotes theLâ†’Râ†’LL\\rightarrow R\\rightarrow Lcycle-consistency path.",
                "position": 268
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x6.png",
                "caption": "Figure 6:Qualitative Results.Our method achieves more accurate disparity estimation and preserves finer visual details on both Parallel and Converged data compared with existing baselines.",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2512.16915/x7.png",
                "caption": "Figure 7:Results w/ and w/o domain switcher.Our proposed domain switcher enhances the modelâ€™s generalization ability.",
                "position": 694
            }
        ]
    },
    {
        "header": "6Depth Ambiguity Details",
        "images": []
    },
    {
        "header": "7UniStereo Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x8.png",
                "caption": "Figure 8:Detailed Analysis of Depth Ambiguity.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2512.16915/x9.png",
                "caption": "Figure 9:Stereo4D processing pipeline.The detailed process of Stereo4D data processing.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2512.16915/x10.png",
                "caption": "Figure 10:3DMovie processing pipeline.The detailed process of 3DMovie data processing.",
                "position": 753
            }
        ]
    },
    {
        "header": "8UE5 Synthetic Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16915/x11.png",
                "caption": "Figure 11:Examples of UE5-Rendered Stereo Video Data.",
                "position": 777
            }
        ]
    },
    {
        "header": "9Geometry Relationship in Stereo Vision",
        "images": []
    }
]