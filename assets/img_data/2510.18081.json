[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x1.png",
                "caption": "Figure 1:Refusal rates on AdvBench under harmful assistant-prefill attacks. Base models (dashed lines) exhibit a catastrophic drop in safety as the prefill depth increases. In contrast, applying ourAny-Depth Alignment(ADA) method (solid lines) restores robust, near-100% refusal rates across all tested depths.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2510.18081/alt_versions/andrew_with_without.png",
                "caption": "Figure 2:Overview of the Any–Depth Alignment (ADA) mechanism.(Top Left)Without ADA, a model that starts generating harmful content will typically continue to do so.(Top Right)ADA intervenes at a safety checkpoint by leveraging model’s own alignment.(a)ADA-Rethinking (ADA (RK))re-injects the header to trigger a refusal.(b)ADA-Linear Probe (ADA (LP))achieves the same outcome more effectively and efficiently by directly probing the strong safety signal present in the header’s hidden states with a linear classifier.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2510.18081/alt_versions/andrew_rk_vs_lp.png",
                "caption": "",
                "position": 141
            }
        ]
    },
    {
        "header": "2Unlocking Innate Safety Alignment to Any-Depth Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x2.png",
                "caption": "Figure 3:t-SNE of hidden states across depths (Llama-3.1-8B-Instruct, layer 15).As generation depth increases, features frominjected Safety Tokens(bottom) – where we read the hidden state of theassistanttoken from the assistant header – become highly separable, while those from thelast generated token(top) remain entangled. This indicates that the model’s internal safety awareness strengthens with context but is cleanly revealed only via Safety Tokens. The accuracy shown in each panel is from a linear classifier trained on the 2D embeddings.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x3.png",
                "caption": "Figure 4:Left:Across all model families, injected Safety Tokens (assistant headers) yield higher accuracy than the last-generated token for all layers.Right:Ablation on token choice (Llama-3.1-8B-Instruct) shows that tokens tied to the assistant header consistently provide stronger harmfulness signals than generic tokens such as a newline.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x4.png",
                "caption": "",
                "position": 256
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Safety Awareness at Any Depth: Adversarial Prefill Attacks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x5.png",
                "caption": "Figure 5:Average refusal rates under deep prefill attacks across diverse LLMs.Results are averaged over four harmful datasets (AdvBench, JailbreakBench, StrongREJECT, and HEx-PHI). OurADA (LP)(red line) achieves robust, depth-invariant safety, consistently outperforming all baselines. Detailed statistics by dataset and model are provided inSection˜16.1.",
                "position": 339
            }
        ]
    },
    {
        "header": "5Robustness Under Adversarial Prompt Attacks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x6.png",
                "caption": "Figure 6:Adversarial prompt robustness.On a subset of AdvBench, we report attack success rates for four common attacks (GCG, AutoDAN, PAIR, TAP) onGemma-2-9b-itandLlama-2-7b-chat-hf.ADA (LP)drives ASR tonear 0%across all attacks, outperforming Deep Alignment and Self-Defense whilematching or exceedingstrong external guardrails by unlocking the base model’s own alignment prior. Results on additional models and datasets are deferred toSection˜16.2.",
                "position": 558
            }
        ]
    },
    {
        "header": "6Robustness under Supervised Fine-Tuning (SFT) Attacks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x7.png",
                "caption": "Figure 7:Robustness\nunder Benign and Adversarial SFT on Gemma-2-9b-it.The plots show the refusal rate against deep prefill attacks as models undergo SFT.(Left)Benign SFT on Alpaca quickly undoes the safety of Deep Alignment.(Right)Adversarial SFT is\nstronger,\nbutADA (LP)remains the most resilient defense.",
                "position": 580
            }
        ]
    },
    {
        "header": "7Evaluating Over-Refusal on Benign Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x8.png",
                "caption": "Figure 8:Over-refusal rates on standard benign datasets.The plot shows the average refusal rate during generation on seven benign benchmarks (GSM8K, MATH, BBH, HumanEval, MMLU, SimpleQA, GPQA Diamond.).OurADA (LP)exhibit the lowest over-refusal, maintaining near-zero rates, while several baselines show higher rates of false positives. Detailed results on other models are shown inSection˜16.4.",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x9.png",
                "caption": "Figure 9:Over-refusal rates on the XSTest benchmark.XSTest contains benign prompts with sensitive keywords designed to trigger false positives.ADA (LP)remain highly precise with near-zero over-refusal, and consistently beat all other baselines. Detailed results on other models are shown inSection˜16.4.",
                "position": 615
            }
        ]
    },
    {
        "header": "8Inference Cost",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/figs/time.png",
                "caption": "Figure 10:Inference cost comparison.Guardrail models requires a full forward pass, so both latency and memory grow linearly with context length.\nIn contrast,ADA (LP)reuses the base model’s KV cache, keeping both latency and memory low and nearly constant, matching standard next-token generation (orange).\nAll models shown are 8B with Flash Attention 2[11].",
                "position": 641
            }
        ]
    },
    {
        "header": "9Related Work",
        "images": []
    },
    {
        "header": "10Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "11Related Work",
        "images": []
    },
    {
        "header": "12Training Details on theADA (LP)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x10.png",
                "caption": "Figure 11:Left:Across all model families, injected Safety Tokens (assistant headers) yield higher training accuracy than the last-generated token for all layers.Right:Ablation on feature readout position (Gemma-2-9B-Instruct).A strong, linearly separable safety signal is detectable at all tested readout locations (>96%>96\\%accuracy).\nLayerNorm interfaces yield the highest and most stable probe accuracy, outperforming the more volatile signals from the MLP and Self-Attention outputs.",
                "position": 1587
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x11.png",
                "caption": "",
                "position": 1590
            }
        ]
    },
    {
        "header": "13Refusal Activation via Transcoder",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x12.png",
                "caption": "Figure 12:Safety-Token reactivation in the transcoder features.Compressed activations of several :high-impact CLT features (Layers 18–24) for the prompt inSection˜13. Activations are negligible across the harmful continuation butspikeon the injected assistant header tokens<end_of_turn>\\n<start_of_turn>model\\n, especially on themodeltoken and its following newline\\n.",
                "position": 1617
            }
        ]
    },
    {
        "header": "14Jailbreaking GPT via SFT",
        "images": []
    },
    {
        "header": "15Examples of Prefill Continuations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18081/x13.png",
                "caption": "Figure 17:AdvBench (nine models).Refusal rate vs. prefill depth under harmful assistant-prefills.\nBase Models and Deep Alignment degrade with depth;ADA (RK)is competitive without training;ADA (LP)sustainsnear-100% refusalacross depths, consistent with the main results.",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x14.png",
                "caption": "Figure 18:Deep prefill attacks onJailbreakBench(nine models).Refusal rate vs. prefill depth under harmful assistant-prefills. Shallow defenses degrade as depth increases, whereasADA (LP)remainsnear-100%across all depths and models.",
                "position": 1935
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x15.png",
                "caption": "Figure 19:Deep prefill attacks onHexPhi(nine models).Baseline defenses lose robustness with depth;ADA (RK)is strong without training; andADA (LP)maintainsnear-100% refusaluniformly across depths.",
                "position": 1939
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x16.png",
                "caption": "Figure 20:Deep prefill attacks onStrongReject(nine models).Consistent with the main findings, depth erodes standard defenses, whileADA (LP)continues to achieve(approximately) 100% refusalat all tested depths.",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x17.png",
                "caption": "(a)AdvBench (Claude Sonnet 4).",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x17.png",
                "caption": "(a)AdvBench (Claude Sonnet 4).",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x18.png",
                "caption": "(b)JailbreakBench (Claude Sonnet 4).",
                "position": 1955
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x19.png",
                "caption": "(c)HexPhi (Claude Sonnet 4).",
                "position": 1961
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x20.png",
                "caption": "(d)StrongReject (Claude Sonnet 4).",
                "position": 1966
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x21.png",
                "caption": "Figure 22:Gemma-2-9B-IT under Benign and Adversarial SFT with adapter ablation.We report refusal rate under deep prefill attacks at depths 100 (solid) and 1000 (dashed) as SFT progresses.\nCurves compare the Base Model, Deep Alignment,ADA (RK), andADA (LP)with the LP adapter eitherenabledordisabledduring the forward pass on the Safety Tokens.\nDuring normal generation the adapter remains enabled.\nHigher is better.\nBenign SFT quickly erodes Deep Alignment, whileADA (LP)stays near 100% across steps and depths.\nEnabling or disabling the adapter on the Safety Tokens produces almost identical refusal curves, indicating robustness of the probe to the LoRA path.",
                "position": 2552
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x22.png",
                "caption": "Figure 23:Llama-2-7B under Benign and Adversarial SFT with adapter ablation.Refusal rates are shown versus SFT steps at depths 100 (solid) and 1000 (dashed) for the same set of methods asFigure˜22.ADA (LP)maintains near perfect refusal throughout training, and theEnablevsDisablesettings on the Safety Tokens forward pass yield overlapping trajectories.\nThis confirms that the linear probe reads a depth-invariant safety signal that is preserved regardless of whether the LoRA adapter is active on the probe branch.",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x23.png",
                "caption": "Figure 24:Depth-resolved over-refusal on standard benign datasets across nine models.Each panel plots refusal rate as a function of prefill depth on GSM8K, MATH, BBH, HumanEval, MMLU, SimpleQA, and GPQA Diamond.\nCurves compare the Base Model, Deep Alignment, Self-Defense, two external guardrails (Llama-Guard-4-12B and Granite-Guardian-3.3-8B), and our methodsADA (RK)andADA (LP)Ȧcross models and depths up to 600 tokens,ADA (LP)remains near zero while several baselines exhibit higher false positives that often increase with depth.",
                "position": 2798
            },
            {
                "img": "https://arxiv.org/html/2510.18081/x24.png",
                "caption": "Figure 25:Depth-resolved over-refusal on XSTest across nine models.XSTest contains benign prompts that include sensitive keywords designed to trigger spurious refusals.\nAs inFigure˜24, we report refusal rate versus prefill depth for the same set of systems.ADA (LP)again stays near zero across depths and models, while Deep Alignment and Self-Defense show substantially higher over-refusal and stronger depth sensitivity.\nExternal guardrails vary by model and can exceed the Base Model on this targeted benign suite.",
                "position": 2804
            }
        ]
    },
    {
        "header": "16Detailed Additional Results",
        "images": []
    }
]