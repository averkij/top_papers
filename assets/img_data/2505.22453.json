[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22453/x1.png",
                "caption": "Figure 1:Overview of the MM-UPT framework. Given an unlabeled multi-modal input, the MLLM samples multiple responses, and uses majority voting to determine the pseudo-label. The MLLM is then updated via GRPO, enabling self-improvement without external supervision.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3The Framework of Multi-Modal Unsupervised Post-Training",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22453/x2.jpg",
                "caption": "",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2505.22453/x3.jpg",
                "caption": "",
                "position": 1833
            }
        ]
    },
    {
        "header": "5Deeper Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22453/x4.png",
                "caption": "Figure 2:Training dynamics of MM-UPT using Qwen2.5-VL-7B on the MMR1 dataset.\nWe plot the majority voting reward, semantic entropy, and average benchmark accuracy over the course of unsupervised post-training.",
                "position": 2204
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BCompute Resources",
        "images": []
    }
]