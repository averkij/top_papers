[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17158/x1.png",
                "caption": "Figure 1:\\gaiatwobudget scaling curve: for eachmax_budget, we plot∑𝟙​{scenario_result=True∧scenario_cost<max_budget}\\sum\\mathbbm{1}\\{\\text{scenario\\_result}=\\text{True}\\land\\text{scenario\\_cost}<\\text{max\\_budget}\\}. Equipped with a simple ReAct-like scaffold (see Section2.4), no model evaluated here dominates across the intelligence spectrum—each trades off capability, efficiency, and budget. At equal cost, some models fare better, yet all curves plateau, suggesting that standard scaffolds and/or models miss key ingredients for sustained progress. Cost estimates fromArtificial Analysismodel pricing data (accessed September 10, 2025).",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/foundations/main-diagram.png",
                "caption": "Figure 2:\\areenvironments are event-based, time-driven simulations, that run asynchronously from the agent and the user.\\areenvironments allow to play scenarios, which typically contain tasks for the agent and verification logic. Whether initiated by agent or user, interactions happen through the same interfaces and can be either tool calls, or tool output/notification observations. Extensive simulation control and logging allow precise study of agents behavior.",
                "position": 371
            }
        ]
    },
    {
        "header": "2\\are: A Research Platform to Create Environments and Run Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17158/assets/foundations/complex-dag.png",
                "caption": "Figure 3:Event dependency graph illustrating\\arescheduling patterns. EventsE1andE5execute in parallel after simulation start, andE2/E3executing in parallel after their prerequisites, both need to be executed forE4to execute. Conditional execution is shown throughCond1leading to validation (Val) with true/false outcomes.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/agent/sequence_diagram.png",
                "caption": "Figure 4:Sequence diagram of a multi-turn scenario in\\are. The agent is paused between turns, i.e., between callingsend_message_to_userand receivingsend_message_to_agent, and adapts its strategy in response to an asynchronous notification from the environment, a new email.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x2.png",
                "caption": "Figure 5:App usage distribution across the 12Mobileapps in\\gaiatwofor Llama 4 Maverick.",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/verifier/verifier_matching_3.png",
                "caption": "Figure 6:Illustration of a failure (top) and a success (down) of the matching trajectory process.",
                "position": 720
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/ui_annotation0.png",
                "caption": "Table 1:\\areVerifier and In-context Verifier results on 450 trajectories annotated with human labels.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/ui_annotation0.png",
                "caption": "Figure 7:\\arescenario view with event DAG (top), scenario run (bottom left) and agent logs (bottom right).",
                "position": 796
            }
        ]
    },
    {
        "header": "3\\gaiatwo: Expanding General Agent Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17158/x3.png",
                "caption": "Figure 8:Overall Gaia2 benchmark performance across some major AI models using pass@1 evaluation. Proprietary frontier models (GPT-5, Claude-4 Sonnet, Gemini 2.5-Pro) significantly outperform open-source alternatives, with GPT-5 achieving the highest score with “high” reasoning. Among open-source models, Kimi-K2 leads.",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/agent/clean_agent2agent_w_apps.png",
                "caption": "Figure 9:In Agent2Agent scenarios, a proportion “r” of the apps in Gaia2 scenarios are replaced by autonomous agents with access to the corresponding APIs and/or tools. The main agent (instantiated by the user) can communicate with app agents through a channel, but cannot use their tools or see their tool call outputs directly. Agents now have to send messages in order to coordinate actions, share state, set sub-goals, and collaboratively solve user tasks. By default,\\gaiatwoevaluates LLMs on the full Agent2Agent (“r = 1”) setting.",
                "position": 954
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17158/x4.png",
                "caption": "Table 2:Pass@1 scores and standard errors on\\gaiatwoscenarios per model and capability split. All models are evaluated with the same ReAct loop scaffolding described in Section2.4. The overall score is the average across all splits, each run three times to account for variance.",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x4.png",
                "caption": "Figure 10:\\gaiatwoscores per capability split. Models are reranked independently for each capability, highlighting where they excel or struggle.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x5.png",
                "caption": "Figure 11:Left: Gaia2 score vs average scenario cost in USD. Right: Time taken per model to successfully solve\\gaiatwoscenarios compared to Humans.",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x6.png",
                "caption": "Figure 12:Left:\\gaiatwopass@1 versus average model calls per scenario. The performance of models is highly correlated to the number of tool calls, emphasizing the importance of exploration.\nRight:\\gaiatwopass@1 score versus average output tokens per scenario (log scale). Claude 4 Sonnet, while costing a lot, lies beyond the Pareto frontier.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x7.png",
                "caption": "Figure 13:Left: Pass@1 scores on\\gaiatwo-time with default mode vs. instant mode. Right: Inverse Scaling for Time: Frontier models perform poorly on the Time split (default mode), due to their time-consuming reasoning capabilities.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x8.png",
                "caption": "",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/results/agent2agent_task_decomp_viz.png",
                "caption": "Figure 14:Agent2Agent tests whether LLM agents can collaborate through message passing in order to solve Gaia2 tasks via sub-task decomposition. For lighter-weight LLMs, collaboration in Agent2Agent results in a lower incidence of tool call errors. Left: Sample exchange between Llama 4 Maverick main vs app agent in an Agent2Agent scenario. Right: Frequency of errors per tool call (lower is better) on Gaia2-mini for Llama 4 Maverick and Claude 4 Sonnet.",
                "position": 1222
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x9.png",
                "caption": "Figure 15:Increasing the number of multi-agent collaborators in Gaia2 scenarios by increasing the Agent2Agent ratiorrimproves pass@k scaling laws for Llama 4 Maverick, but does not improve token cost vs score tradeoffs with repeated sampling for Claude 4 Sonnet.",
                "position": 1225
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x10.png",
                "caption": "Table 3:Probing cross-model collaboration in\\gaiatwo-mini Agent2Agent scenarios: we evaluate pass@1 across main- vs app-agent pairings with Llama 4 Maverick and Claude 4 Sonnet in the fully collaborative Agent2Agent setting (r=1r=1). The results are averaged over three runs and presented with the standard error.",
                "position": 1245
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Authorship",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6\\areAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17158/x10.png",
                "caption": "Figure 16:The user sends a follow-up instruction while the agent is running. The notification system injects the user message into the agent context, conditioning the rest of the trace on the new information provided.",
                "position": 2524
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/foundations/universe_generation_pipeline.png",
                "caption": "Figure 17:The dependency graph ofMobileapps. Shopping and File system are independent apps. Contacts is the root for rest of the apps.",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/search_scenario.png",
                "caption": "Figure 18:Search scenario. It requires multiplereadactions to find the answer to the user’s question and only onewriteaction to report the final answer to the user (send_message_to_user)",
                "position": 2873
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/execution_scenario.png",
                "caption": "Figure 19:Execution scenario. It requires 9writeactions to be solved.",
                "position": 2876
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/adaptability_scenario.png",
                "caption": "Figure 20:Adaptability scenario. The agent must execute a few actions, and then report back to the user. After receiving a message from the user’s friend (green event boxcreate_and_add_message), the agent is expected to adapt its actions to the event.",
                "position": 2879
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/time_scenario.png",
                "caption": "Figure 21:Time scenario. It involves awriteaction (order_ride) that must be executed at a specific point in time (180 seconds after sending the messages).",
                "position": 2882
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/ui/ambiguity_scenario.png",
                "caption": "Figure 22:Ambiguity scenario. The agent is expected to complete all unambiguous parts of the task, and report to the user the ambiguous part that cannot be solved without further user input. In this scenario, the ambiguity is due to multiple Chats conversations titled “Catch-up and Startup Updates”.",
                "position": 2885
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/verifier/turn_trigger.png",
                "caption": "Figure 23:Insertion of a conditional trigger event in a multi-turn scenario.",
                "position": 2922
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/agent/meta_pre_post_steps.png",
                "caption": "Table 5:Evaluation of the\\areVerifier with different models on 450 hand-labeled trajectories.",
                "position": 2950
            },
            {
                "img": "https://arxiv.org/html/2509.17158/assets/agent/meta_pre_post_steps.png",
                "caption": "Figure 25:Proposed ReAct loop with pre/post steps in\\gaiatwo, allowing flexible behaviors.",
                "position": 3000
            },
            {
                "img": "https://arxiv.org/html/2509.17158/x11.png",
                "caption": "Figure 26:Average number of agents spawned in Agent2Agent evaluations on\\gaiatwo-mini tasks across models. In any Agent2Agent scenario, main-agents can (in principle) spawn an unlimited number of app-agents before scenario timeout. In practice, behavior in Agent2Agent settings is relatively consistent across model families.",
                "position": 3044
            }
        ]
    },
    {
        "header": "7\\gaiatwoappendix",
        "images": []
    }
]