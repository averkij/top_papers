[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06949/x1.png",
                "caption": "Figure 1:Illustration of Forgetting Attention with and without ACP. Each cell represents a block in the FlashAttention algorithm. Darker colors represent decay bias values farther below00and thus stronger decay. The arrows indicate the set of blocks that would be visited (in the indicated order) in the FlashAttention iterations.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Preliminaries: Forgetting Transformer",
        "images": []
    },
    {
        "header": "3Adaptive Computation Pruning",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06949/x2.png",
                "caption": "Figure 2:(left) Percentage of pruned FLOPs in the attention operation. (right) Percentage of throughput improvement, measured asTPacpTPno-acp‚àí1subscriptTPacpsubscriptTPno-acp1\\frac{\\mathrm{TP}_{\\text{acp}}}{\\mathrm{TP}_{\\text{no-acp}}}-1divide start_ARG roman_TP start_POSTSUBSCRIPT acp end_POSTSUBSCRIPT end_ARG start_ARG roman_TP start_POSTSUBSCRIPT no-acp end_POSTSUBSCRIPT end_ARG - 1, whereTPacpsubscriptTPacp\\mathrm{TP}_{\\text{acp}}roman_TP start_POSTSUBSCRIPT acp end_POSTSUBSCRIPTandTPno-acpsubscriptTPno-acp\\mathrm{TP}_{\\text{no-acp}}roman_TP start_POSTSUBSCRIPT no-acp end_POSTSUBSCRIPTare the training throughput with and without ACP, respectively. Within each bar we also show the actual values ofTPacpsubscriptTPacp\\mathrm{TP}_{\\text{acp}}roman_TP start_POSTSUBSCRIPT acp end_POSTSUBSCRIPTandTPno-acpsubscriptTPno-acp\\mathrm{TP}_{\\text{no-acp}}roman_TP start_POSTSUBSCRIPT no-acp end_POSTSUBSCRIPT. The unit of throughput is tokens per second. Throughput is measured on 4 NVIDIA L40S GPUs.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x3.png",
                "caption": "",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x4.png",
                "caption": "Figure 3:(left) Per-token loss given different training context lengths for the 760M-parameter/16B-token setting. This is measured on a 2B-token validation set of the LongCrawl64. At each token indexiùëñiitalic_i, we report the averaged loss over a window of101101101101centered atiùëñiitalic_i. (right) Easy-mode needle-in-a-haystack results for the 760M-parameter models with a training context length ofL=16‚Å¢kùêø16ùëòL=16kitalic_L = 16 italic_ktokens.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x5.png",
                "caption": "",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x6.png",
                "caption": "",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x7.png",
                "caption": "",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x8.png",
                "caption": "Figure 4:Distribution of per-head FLOP savings in a 760M-parameter FoX (Pro) model with a 4k training context length.Specifically, we divide FLOPs savings into 20 bins[0%,5%),[5%,10%),‚Ä¶,[95%,100%]percent0percent5percent5percent10‚Ä¶percent95percent100[0\\%,5\\%),[5\\%,10\\%),\\ldots,[95\\%,100\\%][ 0 % , 5 % ) , [ 5 % , 10 % ) , ‚Ä¶ , [ 95 % , 100 % ], and for each bin we count the number of heads in the model whose percentage of pruned attention FLOPs falls into that bin. The counts are then normalized to obtain a distribution.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x9.png",
                "caption": "Figure 5:Distribution of per-head FLOP savingsin each layer.Each column can be seen as a90‚àòsuperscript9090^{\\circ}90 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT-rotated (and flipped) version of Figure4, except the distribution is calculated within each layer. The x-axis of each column is the percentage of heads in the corresponding layer whose percentage of pruned FLOPs falls within a specific bin. The range of the x-axis of each column is from 0% to 100%.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x10.png",
                "caption": "Figure 6:Visualization of the decay matricesDùê∑{\\bm{D}}bold_italic_D(top row) and the corresponding attention weight matricesAùê¥{\\bm{A}}bold_italic_A(bottom row) from three heads in different layers.The orange line shows the pruning boundary. Sinceùë®ùë®{\\bm{A}}bold_italic_Ais very sparse, we only show entries with scores larger than0.10.10.10.1. These results use a 760M-parameter FoX (Pro) model with a context length of 4k tokens.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x11.png",
                "caption": "",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x12.png",
                "caption": "",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x13.png",
                "caption": "Figure 7:Impact ofŒµùúÄ\\varepsilonitalic_Œµon FLOP savings for a 125M-parameter model with a training context length of 16k tokens.For each data point we also label the corresponding validation loss.",
                "position": 546
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of upper bound of total pruned attention weights",
        "images": []
    },
    {
        "header": "Appendix BExperimental details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.06949/x14.png",
                "caption": "Figure 8:(left) Per-token loss given different training context lengths for the 125M-parameter/2.7B-token and 360M-parameter/7.5B-token setting. This is measured on a 2B-token validation set of the LongCrawl64. At each token indexiùëñiitalic_i, we report the averaged loss over a window of101101101101centered atiùëñiitalic_i.",
                "position": 1159
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x15.png",
                "caption": "",
                "position": 1168
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x16.png",
                "caption": "Figure 9:Easy-mode needle-in-a-haystack results for the 760M-parameter models with training context lengths of 4k and 8k tokens.",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x17.png",
                "caption": "",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x18.png",
                "caption": "",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x19.png",
                "caption": "",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x20.png",
                "caption": "",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2504.06949/x21.png",
                "caption": "",
                "position": 1184
            }
        ]
    },
    {
        "header": "Appendix CAdditional results",
        "images": []
    }
]