[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/figures/abs.png",
                "caption": "Figure 1:Figures (A, D): Initial GRPO training with different data types shows only utilizing Video-QA data decreases response length. Figures (B, E): Continual GRPO training with/without Verifier-guided DPO (VerIPO) demonstrates VerIPO improves accuracy and response length. Figure (C): Inconsistency rate (thinking vs. final answer) at different stages reveals our method lowers contextual inconsistency of long CoTs while GRPO increases it. Figure (F): Performance on challenging video reasoning dataset VSI-Bench[81]shows VerIPO (trained with Qwen2.5-VL-7B) outperforms strong LMMs including GPT-4o[23], Video-R1[18], and Kimi-VL[61].",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4VerIPO: Verifier-Guided Iterative Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/figures/model.png",
                "caption": "Figure 2:Overview of VerIPO workflow. This training loop is guided by the Verifier’s continuous evaluation and selection of training samples. The optimization process progressively improves the model’s long reasoning capability by learning from high-quality and informative reasoning examples.",
                "position": 299
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/figures/experiment_case.png",
                "caption": "Figure 3:Figure (A): Performance comparison after removingReflective Preference PairsandInference Consistency Pairsduring DPO (I-2) stage. The reported values represent the average metric across the MMVU (mc) and TOMATO. For visualization, the response length has been scaled down to 0.25 of the original. Figure (B): Inconsistency rate (thinking vs. final answer) at Cold Start and different stages of VerIPO. The reported values represent the average scores across the MMVU (mc) and TOMATO. The statistical inconsistency rate is inA.2. Figure (C): The number of repeated responses generated by VerIPO at different training stages over the evaluation datasets. The reported values are computed as the sum of VSI-Bench, Video-MMMU, MMVU (mc) and TOMATO.",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/figures/case.png",
                "caption": "Figure 4:A case from Video-MMMU shows the comparative performance of GRPO and VerIPO. Our method can generate longer CoTs with accurate and logical formulas to solve physical problems.",
                "position": 1110
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Training and Evaluation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/appendix/figures/example1.png",
                "caption": "Figure 5:A case from VSI-Bench shows the comparative performance of GRPO and VerIPO. Our method is capable of generating longer responses and employing self-validation to address spatial reasoning tasks.",
                "position": 2447
            },
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/appendix/figures/example2.png",
                "caption": "Figure 6:Another case from VSI-Bench shows the comparative performance of GRPO and VerIPO. Our method is capable of generating longer responses and employing self-validation to address spatial reasoning tasks.",
                "position": 2450
            },
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/appendix/figures/example3.png",
                "caption": "Figure 7:A case from Video-MMMU shows the comparative performance of GRPO and VerIPO. Our method can identify situations where the reasoning path is correct but an incorrect answer is chosen, through reflection, then re-selects the correct option that aligns with the reasoning content.",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/appendix/figures/example5.png",
                "caption": "Figure 8:A case from Video-MME shows the comparative performance of GRPO and VerIPO. Our method also demonstrates strong capabilities in reflection and reasoning on general-domain question-answering tasks.",
                "position": 2456
            },
            {
                "img": "https://arxiv.org/html/2505.19000/extracted/6475759/appendix/figures/example4.png",
                "caption": "Figure 9:A case from TOMATO shows the comparative performance of GRPO and VerIPO. Our method is capable of generating longer responses and performing accurate temporal reasoning by self-validation.",
                "position": 2459
            }
        ]
    },
    {
        "header": "Appendix BQualitative Analysis",
        "images": []
    }
]