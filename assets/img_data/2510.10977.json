[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10977/x1.png",
                "caption": "Figure 1:The performance dynamics for the model interpolation between Instruct and Thinking models.Think #Rdenotes the ratio of samples with</think>token in responses.Token #Ndenotes the number of tokens in responses.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2510.10977/x2.png",
                "caption": "Figure 2:Performance of vanilla Instruct, Thinking, and model merging methods on AIME’25, IFEval, and GPQA-Diamond.MIdenotes the model interpolation and the suffix for the interpolation coefficientλ\\lambda.\nThe results indicate thatMIsurpasses these baselines on both efficiency and effectiveness.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Revisiting Model Interpolation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10977/x3.png",
                "caption": "Figure 3:The performance dynamics of model interpolation (MI) onQwen3-4B-Instruct-2507andQwen3-4B-Thinking-2507.\nThe dynamics follow a three-stage evolutionary paradigm colored ingrey,green, andblue.λ\\lambdadenotes the interpolation coefficient ranging from 0 to 1.",
                "position": 448
            }
        ]
    },
    {
        "header": "5Extensive Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10977/x4.png",
                "caption": "Figure 4:Performance of MI-0.4 on IFEval and GPQA-Diamond under different decoding strategies on Qwen3-4B.\nWe search for the temperatureTTand Top-p.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2510.10977/x5.png",
                "caption": "Figure 5:Ablation on modules to apply model interpolation.\nAttn denotes the MHA sub-layers and FFN for FFN sublayers.\nWe report the results on AIME’25.\nLength Ratio denotes the ratio to the Thinking model.",
                "position": 952
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetails of Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BResults on Qwen3-30B-A3B",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10977/x6.png",
                "caption": "Figure 6:The performance dynamics of model interpolation (MI) onQwen3-30B-A3B-Instruct-2507andQwen3-30B-A3B-Thinking-2507.\nThe dynamics follow a three-stage evolutionary paradigm, while the division range ofλ\\lambdais different.",
                "position": 1642
            }
        ]
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]