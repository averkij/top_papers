[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13604/x1.png",
                "caption": "",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13604/x2.png",
                "caption": "Figure 2:Unstable long-term generation.As the generated duration increases, current video world models gradually lose controllability, visual fidelity, and temporal consistency.",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x3.png",
                "caption": "Figure 3:Framework of LongVie 2.LongVie 2serves as a controllable video world model that integrates both dense and sparse control signals to provide world-level guidance for enhanced controllability. A degradation-aware training strategy improves long-term visual quality, while tail frames from preceding clips are incorporated as historical context to maintain temporal consistency over extended durations.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13604/x4.png",
                "caption": "Figure 4:Training pipeline of LongVie 2.We first train the model using a standard ControlNet-based pipeline. In the second stage, we introduce degradation to the first frame to bridge the domain gap between ground truth and generated frames. Finally, to ensure temporal consistency, we incorporate historical frame information.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x5.png",
                "caption": "Figure 5:Details of Frame Degradation.We apply two types of degradation—VAE encoding and denoising—to simulate the quality decay that occurs during long-term generation.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x6.png",
                "caption": "Figure 6:Visual comparison across training stages.The results indicate that the second stage alleviates visual degradation but introduces intra-clip inconsistency, which is subsequently resolved by the final stage of training.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x7.png",
                "caption": "Figure 7:Controllability comparison between LongVie 2 and baselines.LongVie 2demonstrates markedly superior controllability, maintaining precise structural alignment and realistic appearance across frames compared with the baselines.GWFdenotesGo-With-The-Flow, andDASdenotesDiffusion as Shader.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x8.png",
                "caption": "Figure 8:Long-term generation demos.We present several videos generated byLongVie 2, each lasting over two minutes, demonstrating sustained visual quality and long-term temporal consistency, further validating the effectiveness of our training strategy.",
                "position": 523
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Overview of Supplementary Material",
        "images": []
    },
    {
        "header": "7More Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13604/x9.png",
                "caption": "Figure 9:Caption Refinement via MLLM.Given a new first-frame image and its original caption, we use Qwen-2.5-VL to refine the caption so that the prompt for video generation more accurately matches the visual content and remains stylistically consistent with the input image.",
                "position": 961
            }
        ]
    },
    {
        "header": "8LongVGenBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13604/x10.png",
                "caption": "Figure 10:Examples from LongVGenBench.We show several videos from both real-world and synthetic scenarios in LongVGenBench, covering a variety of indoor and outdoor environments to evaluate the controllable long video generation ability of our model.",
                "position": 993
            }
        ]
    },
    {
        "header": "9Additional Ablation Studies",
        "images": []
    },
    {
        "header": "10More Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13604/x11.png",
                "caption": "Figure 11:Subject-driven 1-minute scenario.A man riding a horse, transferred into multiple seasonal styles while preserving motion dynamics and scene structure.",
                "position": 1129
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x12.png",
                "caption": "Figure 12:Subject-free scenario.A drone-like car-drive sequence through a mountain valley, transferred across different seasonal styles with consistent global appearance.",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x13.png",
                "caption": "Figure 13:3-minute scenario.A car driving through a mountain valley is transferred into multiple seasonal styles, demonstrating LongVie’s ability to maintain long-range temporal consistency and coherent global appearance over extended durations.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2512.13604/x14.png",
                "caption": "Figure 14:5-minute scenarios.We present two ultra-long video cases: a subject-driven sequence and a subject-free sequence. These results illustrate LongVie’s robustness in preserving structural stability, motion coherence, and style consistency across 5-minute continuous generations.",
                "position": 1138
            }
        ]
    },
    {
        "header": "11Limitations and Future Work",
        "images": []
    }
]