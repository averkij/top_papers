[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24786/x1.png",
                "caption": "Figure 1:Illustration of the workflow of LOVE-R1. Our model first takes densely sampled small-resolution frames from the whole video as inputs to understand the video globally. If needed, it can adaptively zoom in on a video clip to gain fine-grained spatial details. The workflow is implemented as a multi-step reasoning process.",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3A Dynamic Frame Processing Mechanism",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24786/x2.png",
                "caption": "Figure 2:Different slow-fast video templates. Templates (a) and (b) will replace the original fast video segments with the slow videos. Template (a) treats multiple video segments as a whole video while Template (b) explicitly separates them with identifiers (<|vision_start|>, <|vision_end|>). Template (c) appends the additional slow videos at the end of the fast video without removing the corresponding fast video segments. We adopt Template (c).",
                "position": 130
            }
        ]
    },
    {
        "header": "4A Three-Stage Post-Training Recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24786/x3.png",
                "caption": "Figure 3:Illustration of decoupled reinforcement finetuning. (a) For questions without ground truth timespans, we apply the standard GRPO algorithm to optimize multi-step CoTs as a whole. (b) To provide fine-grained process rewards, we decouple multi-step reasoning into multiple single-step reasoning and optimize the single zoom-in step explicitly by appending the zoom-in prefix.",
                "position": 171
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24786/x4.png",
                "caption": "Figure 4:Visualization of LOVE-R1 inference results. The video is taken from Video-MME (vid: edAu5_O4C54).",
                "position": 963
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Experiment Results",
        "images": []
    },
    {
        "header": "Appendix BCoT Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24786/x5.png",
                "caption": "Figure 5:Our CoT data construction pipeline. To ensure the data quality, we perform strict data pre-processing and post-processing by filtering out low-quality annotations and CoTs. We also use a strong reasoning model, Gemini 2.5 pro, to annotate CoT data, ensuring the content of CoTs is reasonable and high-quality.",
                "position": 1921
            }
        ]
    },
    {
        "header": "Appendix CLLM Usage",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24786/x6.png",
                "caption": "Figure 6:Visualization of LOVE-R1 inference results. The video is taken from Video-MME (vid: -XpJeDGh8No).",
                "position": 1983
            },
            {
                "img": "https://arxiv.org/html/2509.24786/x7.png",
                "caption": "Figure 7:Visualization of LOVE-R1 inference results. The video is taken from Video-MME (vid: -qTAeVGl_e8).",
                "position": 1986
            }
        ]
    },
    {
        "header": "Appendix DMore Visualization",
        "images": []
    }
]