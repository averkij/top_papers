[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18693/x1.png",
                "caption": "Figure 1:Left:Results of different models on MATH, where-ScaleQuestdenotes ours.Right:Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2ScaleQuest: Scaling Question Synthesis from Scratch",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18693/x2.png",
                "caption": "Figure 2:Overview of our ScaleQuest method.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2410.18693/x3.png",
                "caption": "Figure 3:The difficulty distribution of two real-world datasets and two synthetic datasets. The difficulty score is calculated based solely on the problem part.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2410.18693/x4.png",
                "caption": "Figure 4:The solvability and difficulty of the raw questions generated by the QFT model and the optimized ones.",
                "position": 218
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18693/extracted/5951424/img/deepseek.png",
                "caption": "Table 1:Main results on four mathematical reasoning benchmarks.Boldmeans the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-4o, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure5.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2410.18693/x5.png",
                "caption": "Figure 5:A comparison of the synthetic dataset generated by the raw instruct model, the model after QFT, the model after QPO, and the final dataset after applying reward filtering. The evaluation covers question solvability, difficulty, and instruction tuning effectiveness on Llama3-8B.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2410.18693/extracted/5951424/img/qwen2.png",
                "caption": "Table 2:We directly compared the question quality of different open-source datasets. To ensure consistency, all responses were generated using Qwen2-Math-7B-Instruct.",
                "position": 810
            },
            {
                "img": "https://arxiv.org/html/2410.18693/extracted/5951424/img/deepseek.png",
                "caption": "Table 3:The performance of Mistral-7B-v0.1 fine-tuned on ScaleQuest-DSMath, ScaleQuest-Qwen2, and a mix of both. In this setup, the instructions for ScaleQuest-DSMath and ScaleQuest-Qwen2-Math were generated by DSMath-QGen and Qwen2-Math-QGen, respectively. We fixed the training data size at 400K and found that the mixed data resulted in the greatest improvement.",
                "position": 884
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Data Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18693/x6.png",
                "caption": "Figure 6:Overview of our filtering process.",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2410.18693/x7.png",
                "caption": "Figure 7:Topic distribution of our generated dataset.",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2410.18693/x7.png",
                "caption": "Figure 7:Topic distribution of our generated dataset.",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2410.18693/x8.png",
                "caption": "Figure 8:t-SNE plot of our dataset, with GSM8K, MATH, and NuminaMath.",
                "position": 1897
            }
        ]
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    }
]