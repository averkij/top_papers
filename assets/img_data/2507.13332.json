[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13332/x1.png",
                "caption": "Figure 1:Construction process of TAIL.(a)Schematic diagram of a Turing machine executing a program.(b)The basic principle of TAIL. Visualization of attention in the Transformer layers shows that the model has learned to imitate the behavior of a Turing machine.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Turing Machine Imitation Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13332/x2.png",
                "caption": "Figure 2:An overview of TAIL.(a) Core Modules of TAILimitate a Turing Machine, containing a Linear Transition of Atomic State with Memory Fetcher of previous reasoning results.(b) CoT generated by TAIL: the solution to a 0/1 knapsack problem using a dynamic programming algorithm.(c) Conventional CoTconsists of oversized subtasks, shortcut learning, and irregular expansion.",
                "position": 263
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13332/x3.png",
                "caption": "Figure 3:Length generalization across all 18 tasks in comparison with Qwen2.5-7B Instruct and DeepSeek-R1. It can be observed that the model demonstrates stronger generalization in reasoning over longer sequences, outperforming DeepSeek-R1 on the majority of tasks.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2507.13332/x4.png",
                "caption": "Figure 4:Comparison of fine-tuned Qwen2.5-7B with TAIL core module and the base model. For each algorithm, we select a representative task. After fine-tuning, model demonstrates length generalization on sequences that are 5 to 10 times longer than those in training.",
                "position": 578
            }
        ]
    },
    {
        "header": "6Disscusion",
        "images": []
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask Introduction to Dataset",
        "images": []
    },
    {
        "header": "Appendix BChain-of-Thought Format Examples",
        "images": []
    },
    {
        "header": "Appendix CData Proportion Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13332/x5.png",
                "caption": "Figure C1:Mixed scale experiments with data on 18 tasks. We find that adding a small amount of long data to most unsaturated tasks achieves fast performance gains.",
                "position": 1756
            }
        ]
    },
    {
        "header": "Appendix DTask Length Range",
        "images": []
    },
    {
        "header": "Appendix EBaseline Data Construction",
        "images": []
    },
    {
        "header": "Appendix FAttention visualization of Memory Fetcher",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13332/extracted/6632040/figures/attention.drawio.png",
                "caption": "Figure F1:Ablation study on attention visualization of Memory Fetcher.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2507.13332/x6.png",
                "caption": "Figure G1:Generalization performance between tasks within a single algorithm (e.g., DP and Greedy).Pretrainrepresents Qwen 2.5-7B as the basis for subsequent SFT. For each task,SFT(Rest)indicates training using data from other tasks within the algorithm,SFT(Task)indicates training using data from this task, andSFT(All)indicates training using data from all tasks within this algorithm.▼▼\\blacktriangledown▼indicates that this piece of data has a label accuracy of less than 5%.",
                "position": 2279
            }
        ]
    },
    {
        "header": "Appendix GTask Generalization Results",
        "images": []
    }
]