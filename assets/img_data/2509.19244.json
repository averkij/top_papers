[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19244/x1.png",
                "caption": "Figure 1:We propose Lavida-O,a unified masked diffusion model capable of multi-modal understanding and generation.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19244/x2.png",
                "caption": "Figure 2:Overall Pipeline of Lavida-O.Given an input image and text prompt, we first concatenate the image semantic embeddingCiC_{i}, image VQ embeddingCvC_{v}, and text prompt embeddingCpC_{p}to form the conditioning embeddingCC. The combined embedding is then passed to the model alongside the partially masked sequenceXtX_{t}. The model then predicts the fully-unmasked sequenceX0X_{0}.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x3.png",
                "caption": "Figure 3:Design of Elastic MoT.Elastic-MoT introduces two major modifications to standard MoT. First, the generation branch has a smaller hidden size. Second, given anNN-layer model, we only allow text and image modalities to interact in the firstMMlayers. These two designs allow us to flexibly load only a portion of parameters depending on tasks, improving the efficiency.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x4.png",
                "caption": "(a)Modality-Aware Masking",
                "position": 291
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x4.png",
                "caption": "(a)Modality-Aware Masking",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x5.png",
                "caption": "(b)Stratified Sampling",
                "position": 299
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19244/x6.png",
                "caption": "Figure 5:Training and Inference Speed of Lavida-O.We compare the end-to-end inference latency of Lavida-O on three tasks, as well as pretraining efficiency measured by per-step latency.",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x7.png",
                "caption": "Figure 6:Qualitative examples of text-to-image generation.We provide additional examples of text-to-image generation outputs on diverse prompts.",
                "position": 1025
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x8.png",
                "caption": "Figure 7:Qualitative examples of image editing.We provide additional examples of image editing outputs on diverse instructions.",
                "position": 1028
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19244/x9.png",
                "caption": "Figure 8:Activated parameters of Lavida-O under different task settings.Elastic-MoT Design allow Lavida-O to dynamically loads its parameters depending on the tasks. For understanding-only tasks, we only load the 8B generation branch. For text-to-image generation tasks, we load the firstM=16M=16layers of the understanding branch, which consists of 4B parameters, and the full 2.4B generation branch. For interleaved tasks, we load all 2.4B+8B parameters.",
                "position": 2229
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x10.png",
                "caption": "Figure 9:Training and inference with modality-aware masking.We visualize the sampling process with modality-aware masking on the left and the training process on the right. During the training, the loss is applied on eitherX0X_{0}orX0′X_{0}^{\\prime}depending on the value ofttwith respect totexp.t_{\\text{exp}}.",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x11.png",
                "caption": "Figure 10:Effect of Universal Text Conditioning.On the left side, we visualize the text format used in Universal Text Conditioning. On the right side, we visualize generation results under different choices of universal text conditioning.",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x12.png",
                "caption": "Figure 11:Visualization of different sampling processes.We compare the unmasking order of the stratified sampler, Halton sampler, and uniform random sampler. Uniform random sampler produces the least desirable spatial pattern, with many unmasked tokens clustered together. Halton sampler is less ideal than stratified sampler because it does\nnot guarantee perfectly stratified coverage. For example, when the number of unmasked tokens is 4, the upper-right quadrant remains unoccupied.",
                "position": 2342
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x13.png",
                "caption": "Figure 12:Coordinate Quantization. We normalize bounding box coordinates into the range [0,1] and discretize them into 1025 bins. This ensures that each bounding box is represented by exactly 4 tokens, allowing efficient parallel decoding of multiple bounding boxes in a single step.",
                "position": 2411
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x14.png",
                "caption": "Figure 13:Interleaved Generation with Planning and Reflection.We provide visual examples of interleaved generation, including text-to-image generation with planning (Top), text-to-image generation with reflection (Middle), and image editing with planning (Bottom). We always enable planning during the reflection process. The layout traces is omitted in the middle figure for clarity and better presentation.",
                "position": 2421
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiment Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19244/x15.png",
                "caption": "Figure 14:Effect of truncated initialization.Validation loss comparison of truncated initialization vs. training from scratch during Stage 2. Truncated initialization converges faster and achieves lower loss.",
                "position": 2750
            },
            {
                "img": "https://arxiv.org/html/2509.19244/x16.png",
                "caption": "Figure 15:Speed–quality tradeoff on generation, grounding, and reasoning.Latency (s/sample) and benchmark scores are shown. For MJHQ: FID (lower is better). For RefCOCO: Precision@0.5 (higher is better). For MathVista: accuracy (higher is better). On MathVista, the maximum generation length is capped at 256 tokens.",
                "position": 3102
            }
        ]
    },
    {
        "header": "Appendix CCompute Cost",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "Appendix EBoarder Impact",
        "images": []
    }
]