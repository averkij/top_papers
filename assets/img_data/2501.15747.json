[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15747/x1.png",
                "caption": "Figure 1:IndicMMLU-Pro Dataset Construction and Evaluation Pipeline. The diagram illustrates the end-to-end process of creating and validating the IndicMMLU-Pro dataset across nine Indic languages. Starting with the English MMLU-Pro dataset, content is translated using IndicTrans2 (1B parameters) and undergoes rigorous quality assurance through back-translation and multiple metric evaluations (chrF++, BLEU, METEOR, TER, and SacreBLEU). Only translations meeting quality thresholds proceed to the final dataset. The workflow also shows the comprehensive evaluation process including expert proofreading involving 13 reviewers who assess semantic accuracy, fluency, and linguistic style. This systematic approach ensures the creation of a high-quality, multilingual benchmark dataset that maintains the integrity of the original MMLU-Pro while adapting to the linguistic nuances of Indic languages.",
                "position": 162
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15747/x2.png",
                "caption": "Figure 2:The original text sample, its Hindi translation, and the corresponding back-translated text",
                "position": 907
            },
            {
                "img": "https://arxiv.org/html/2501.15747/x3.png",
                "caption": "Figure 3:The original text sample, its Gujarati translation, and the corresponding back-translated text",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2501.15747/x4.png",
                "caption": "Figure 4:The original text sample, its Tamil translation, and the corresponding back-translated text",
                "position": 921
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Key Findings and Implications of IndicMMLU-Pro",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.15747/x5.png",
                "caption": "Figure 5:Model Accuracy Across Different Languages",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2501.15747/x6.png",
                "caption": "Figure 6:Average Question and Option Lengths in # Words and # Tokens",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2501.15747/x7.png",
                "caption": "Figure 7:Additional examples showcasing the machine translation workflow, including the original text samples, their Hindi translations, and the corresponding back-translated texts.",
                "position": 1977
            },
            {
                "img": "https://arxiv.org/html/2501.15747/x8.png",
                "caption": "Figure 8:Additional examples showcasing the machine translation workflow, including the original text samples, their Gujarati translations, and the corresponding back-translated texts.",
                "position": 1980
            },
            {
                "img": "https://arxiv.org/html/2501.15747/x9.png",
                "caption": "Figure 9:Additional examples showcasing the machine translation workflow, including the original text samples, their Tamil translations, and the corresponding back-translated texts.",
                "position": 1983
            }
        ]
    },
    {
        "header": "8Additional Data Samples",
        "images": []
    }
]