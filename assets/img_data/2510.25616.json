[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25616/figures/method_1.png",
                "caption": "Figure 1.Visual alignment method overview. Mid-level VLA features are projected onto a normalized sphere and aligned with teacher embeddings, preserving visual semantics and improving OOD generalization. Bottom plots show comparison with standard SFT across three generalization axes on the Simpler-based benchmark(Liu et al.,2025).",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2510.25616/figures/scheme1.png",
                "caption": "Figure 2.Overview of the proposed method.\n(a, b) Training pipeline with visual alignment loss – no extra overhead, only precomputed teacher features and a lightweight regularization term during SFT. (c) Conceptual illustration of the loss landscape for VL tasks: the core idea is to optimize the model with respect to the action objective while preserving performance on VL understanding.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2510.25616/figures/vl_think.png",
                "caption": "Figure 3.VL-Think Task Suite examples.Each panel illustrates a pick-and-place episode where the agent must place an object on the board matching the instructed concept (e.g., color, number, symbol, or category).",
                "position": 201
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": []
    },
    {
        "header": "3.Preliminaries",
        "images": []
    },
    {
        "header": "4.VL-Think Task Suite",
        "images": []
    },
    {
        "header": "5.VL representations analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25616/x1.png",
                "caption": "Figure 4.Attention map comparison: the strongest and most semantically grounded attention appears around middle layers. OpenVLA fine-tuned with our proposed method (OpenVLA Align) maintains object-aligned focus in attention maps, while default OpenVLA SFT shows diffused and noisy patterns, indicating loss of visual-language grounding (for more results see AppendixFigure 6).",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2510.25616/figures/tsne1.png",
                "caption": "Figure 5.t-SNE visualization of token embeddings for Qwen2.5-VL, PrismaticVLM, and OpenVLA. While PrismaticVLM and Qwen2.5-VL maintains well-separated clusters for target objects, OpenVLA shows huge overlap across classes, indicating that action fine-tuning causes representations collapse.",
                "position": 352
            }
        ]
    },
    {
        "header": "6.Method",
        "images": []
    },
    {
        "header": "7.Experiments",
        "images": []
    },
    {
        "header": "8.Ablations",
        "images": []
    },
    {
        "header": "9.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25616/x2.png",
                "caption": "Figure 6.Attention maps across middle layers Qwen2.5-VL, OpenVLA SFT, and OpenVLA Align.\nThe proposed alignment method restores sharp, object-centered attention patterns, improving visual grounding degraded by standard fine-tuning. Question:”Do you see hamburger—baguette?”.",
                "position": 2918
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]