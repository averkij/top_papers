[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/aurora_teaser.png",
                "caption": "Figure 1:We introduce Perception Tokens, intermediate reasoning tokens that allow MLMs to go beyond using language in reasoning. With it, we developAurora, a framework that trains multimodal language models to leverage visual perception tokens, allowing them to use depth estimation and bounding box predictions while reasoning.",
                "position": 132
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/main_figure.png",
                "caption": "Figure 2:We demonstrate relative depth estimation and counting questions where LLaVA fails. In contrast, by learning to utilize visual perception tokens as intermediate reasoning steps, LLaVA-Aurorasuccessfully complete these tasks requiring perceptual understanding.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03548/x1.png",
                "caption": "Figure 3:The overallAuroratraining framework. We first learn visual perception tokens using VQVAE. We then finetune MLMs with a multi-task training approach where we distill intrinsic image representations (e.g., depth map) into MLMs by training them to decode the visual tokens as intermediate reasoning steps towards completing the tasks.",
                "position": 201
            }
        ]
    },
    {
        "header": "3Perception Tokens & Aurora",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/depth_vis.png",
                "caption": "Figure 4:Depth maps generated by Aurora are imperfect but resemble the ground-truths from Depth Anything[51].",
                "position": 1409
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Ablation study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/recons.png",
                "caption": "Figure 5:Qualitative comparison of predicted depth maps with and without reconstruction loss.",
                "position": 2562
            }
        ]
    },
    {
        "header": "7Cross-task generalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/depthdata.png",
                "caption": "Figure 6:Examples of sub-datasets for the depth task: (1) depth generation, (2) Chain-of-Thought reasoning, and (3) direct labeling.",
                "position": 2665
            },
            {
                "img": "https://arxiv.org/html/2412.03548/extracted/6053340/figures/files/countdata.png",
                "caption": "Figure 7:Examples of sub-datasets for the counting task: (1) bounding box prediction, (2) Chain-of-Thought reasoning, and (3) direct labeling.",
                "position": 2668
            }
        ]
    },
    {
        "header": "8Implementation details",
        "images": []
    }
]