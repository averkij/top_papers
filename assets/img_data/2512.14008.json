[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14008/x1.png",
                "caption": "Figure 1:We propose Sparse-LaViDa, a novel modeling technique for unified multimodal masked discrete diffusion models. Sparse-LaViDa achieves substantial speedup across a wide range of tasks, including text-to-image generation, image editing, and visual math reasoning, compared with the baseline LaViDa-O.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x2.png",
                "caption": "Figure 2:Overall design of Sparse-LaViDa.Left:Vanilla MDMs materialize all masked tokens and support arbitrary-order decoding (top-down). Unlike AR models, they have bidirectional context and naturally support tasks such as image generation and inpainting.Middle:Block Diffusion truncates redundant masked tokens from the right but imposes a left-to-right generation order using a block-causal attention mask, losing many benefits of MDMs.Right:Sparse-LaViDa is an alternative parameterization of vanilla MDMs. It preserves all benefits of standard MDMs while achieving the efficiency gains of Block Diffusion by allowing mask truncation at arbitrary positions. Special register tokens serve as compressed representations of truncated tokens.",
                "position": 110
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14008/x3.png",
                "caption": "(a)Inference with Sparse Parameterization",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x3.png",
                "caption": "(a)Inference with Sparse Parameterization",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x4.png",
                "caption": "(b)Inference Attention Mask of Sparse-LaViDa",
                "position": 136
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14008/x5.png",
                "caption": "Figure 4:Illustration of Sparse Representation of Masked Sequence.Instead of materializing all masked tokens, a partially masked sequence can be uniquely represented by non-mask tokens, their locations, and the number of total tokens in the original sequence.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x6.png",
                "caption": "Figure 5:Step-Causal Mask.We employ a step-causal attention mask during training to match the inference behavior of Sparse-LaViDa. Consider a sequence containing prompt tokensP0P_{0}–P3P_{3}and answer tokensX0X_{0}–X15X_{15}, where some tokens are clean and others are masked (color-coded in blue and gray). During inference, prompt tokens and clean tokens are sequentially added to the KV cache. To simulate this behavior during training, we assign block number 0 to the prompt, and block numbers 1–3 to clean tokens, such that each token may only attend to tokens in its own block or previous blocks. The bottom figure shows tokens sorted by block number.At inference, the model only observes a subset of masked tokens. To mimic this behavior in training, we assign block numbers 4–5 to masked tokens (e.g.,X10,X12X_{10},X_{12}in block 4 andX7,X8X_{7},X_{8}in block 5). Each block is accompanied by a corresponding register token (R4,R5R_{4},R_{5}). We apply an attention mask such that tokens in one masked block cannot attend to tokens in another masked block, but may attend to all clean and prompt tokens. This simulates inference paths0→1→2→3→40\\to 1\\to 2\\to 3\\to 4and0→1→2→3→50\\to 1\\to 2\\to 3\\to 5within a single training step.",
                "position": 237
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14008/x7.png",
                "caption": "Figure 6:Qualitative results.Unlike semi-AR approaches like Block Diffusion, Sparse-LaViDa supports tasks requiring bidirectional context, such as inpainting/outpainting, parallel grounding, and constrained captioning. In text generation examples, colored regions denote masked tokens initialized for infilling.",
                "position": 922
            }
        ]
    },
    {
        "header": "5Conclusion and Future works.",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "7Additional Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14008/x8.png",
                "caption": "Figure 7:Sparse-Representation of Partially Masked Sequence.Given a partially masked sequence, we can partition its tokens into two subsetsAAandBB, whereAAconsists of clean tokens andBBconsists of masked tokens. The standard MDM (Left) need to pass all|A|+|B||A|+|B|tokens. By contrast, Sparse-LaViDa (Middle and Right) only need to pass|A|+|C|+m|A|+|C|+mtokens, whereCCis a subset ofBBandmmis the number of register tokens (set to 3) in this case.",
                "position": 1215
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x9.png",
                "caption": "(a)Inference with Sparse Parameterization",
                "position": 1238
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x9.png",
                "caption": "(a)Inference with Sparse Parameterization",
                "position": 1241
            },
            {
                "img": "https://arxiv.org/html/2512.14008/x10.png",
                "caption": "(b)Design of Step-Causal Attention Mask",
                "position": 1246
            }
        ]
    },
    {
        "header": "8Additional Experiment Details and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14008/x11.png",
                "caption": "Figure 9:Qualitative Comparisons of Sparse-LaViDa and LaViDa-O baseline.We show qualitative results of text-to-image generation and image editing results of Sparse-LaViDa and LaViDa-O baseline.Sparse-LaViDa achieves a speedup of1.95×1.95\\timeson text-to-image generation and a speedup of2.83×2.83\\timeson image editing, while maintaining comparable visual quality",
                "position": 1584
            }
        ]
    },
    {
        "header": "9Limitations",
        "images": []
    }
]