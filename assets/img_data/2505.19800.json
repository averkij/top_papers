[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19800/x1.png",
                "caption": "Figure 1:Sample metadata extracted from a dummy paper with highlighted attributes.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2505.19800/x2.png",
                "caption": "Figure 2:MOLE pipeline. The paper text and Schema are used as input, and the output is the extracted metadata content.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19800/x3.png",
                "caption": "Figure 3:Latex vs. PDF vs. Docling input formats results across all models.",
                "position": 565
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19800/x4.png",
                "caption": "Figure 4:Few-shot results with 0, 1, 3, and 5 -shot examples using the Gemini 2.5 Pro model.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2505.19800/x5.png",
                "caption": "Figure 5:Browsing vs. no Browsing for all models in our evaluation benchmarks.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2505.19800/x6.png",
                "caption": "Figure 6:The effect of changing the context length on the results of all models. 1: full context, 1/2: half the context, 1/4: quarter of the context.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2505.19800/x7.png",
                "caption": "Figure 7:Results across 6 different metadata attributes (Link, Volume, License, Collection Style, Domain, and Tasks).",
                "position": 788
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": []
    },
    {
        "header": "Appendix BCosts",
        "images": []
    },
    {
        "header": "Appendix CErrors",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19800/x8.png",
                "caption": "Figure 8:Distribution of the number of errors per model. This graph only shows the models with at least one error.",
                "position": 2134
            }
        ]
    },
    {
        "header": "Appendix DInput Format Processing Time",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19800/x9.png",
                "caption": "Figure 9:Average processing time comparison between different input formats. LaTeX source processing (0.08s) is most efficient, followed by pdfplumber-based PDF extraction (2.03s), while Docling structured parsing (72.31s) requires substantially more computation time.",
                "position": 2144
            }
        ]
    },
    {
        "header": "Appendix EModelâ€™s Access",
        "images": []
    },
    {
        "header": "Appendix FSynthetic Template Generation",
        "images": []
    },
    {
        "header": "Appendix GValidation Groups",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19800/extracted/6475988/figures/MOLeEval-larger.png",
                "caption": "Figure 11:Schema validation groups and their associated attributes for the Arabic metadata.",
                "position": 2287
            }
        ]
    },
    {
        "header": "Appendix HIn-paper Annotations",
        "images": []
    },
    {
        "header": "Appendix ISystem Prompt",
        "images": []
    },
    {
        "header": "Appendix JEvaluation metrics",
        "images": []
    },
    {
        "header": "Appendix KIndividual Attribute Evaluation",
        "images": []
    },
    {
        "header": "Appendix LSchema",
        "images": []
    }
]