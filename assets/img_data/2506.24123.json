[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24123/x1.png",
                "caption": "Figure 1.Photorealistic text image customization resultsproduced by our proposedCalligrapher, which allows users to perform customization with diverse stylized images and text prompts.\nThe input and reference images are shown in the lower left corner of the generated results,\nrespectively for the setting of self-reference and cross-reference text image customization.",
                "position": 117
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24123/x2.png",
                "caption": "Figure 2.Motivation and technical differentiationof our approach. Existing typography design methods face critical limitations:\n(1) Standard font libraries prioritize accessibility but require extensive manual adjustments for integration into diverse backgrounds, resulting in inflexible and unrealistic outputs.\n(2) Neural generative models automate typography but often fail to capture precise font style nuances, especially when relying on textual descriptions.\nIn contrast, the proposed method addresses these challenges by enabling fully automated typography generation with precise style control and various kinds of references, including non-text images.",
                "position": 150
            }
        ]
    },
    {
        "header": "2.Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24123/x3.png",
                "caption": "Figure 3.Self-distillation pipelinefor style-oriented typography dataset construction and model training.\nWe emulate natural language processing practices by leveraging pre-trained text-to-image generative models and large language models to synthesize stylized text images, paired with reference prompts and masks. This generates self-supervised training pairs for robust style learning without manual annotation.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2506.24123/x4.png",
                "caption": "Figure 4.Training framework of¬†Calligrapher, demonstrating the integration of localized style injection and diffusion-based learning.\nThe framework processes masked images through a Variational Auto-Encoder (VAE) to obtain latent representations, concatenated with mask and noise latents.\nA style encoder comprising a visual encoder, Qformer, and linear layers is designed to extract style-related features from the reference style image, while text embeddings (e.g.,‚Äúgic‚Äùin the case) modulate the denoising transformer.\nIn the denoising block, style attention predicted from the style features replaces the original cross-attention, injecting style embeddings (K‚Ñ∞,V‚Ñ∞subscriptùêæ‚Ñ∞subscriptùëâ‚Ñ∞K_{\\mathcal{E}},V_{\\mathcal{E}}italic_K start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT) with the denoiser‚Äôs queryQùëÑQitalic_Qto enable granular typographic control in the latent space.\nThe model is optimized under the flow-matching learning objective with the self-distillation typography dataset.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2506.24123/x5.png",
                "caption": "Figure 5.Qualitative results of¬†Calligrapher under various settings.We demonstrate text customization results respectively under settings of (a) self-reference, (b) cross-reference, and (c) non-text reference. Reference-based image generation results are also incorporated in (d).",
                "position": 188
            }
        ]
    },
    {
        "header": "3.Methodology",
        "images": []
    },
    {
        "header": "4.Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24123/x6.png",
                "caption": "Figure 6.Qualitative comparisons on self-reference customization.¬†Calligrapher achieves better performance in terms of style sync and quality.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2506.24123/x7.png",
                "caption": "Figure 7.Ablation studieson the self-distillation (left) and in-context generation (right) to validate their effectiveness.",
                "position": 397
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.24123/x8.png",
                "caption": "Figure 8.Self-reference text image customization results.",
                "position": 1501
            },
            {
                "img": "https://arxiv.org/html/2506.24123/x9.png",
                "caption": "Figure 9.Cross-style customization based on (a) text reference and (b) non-text reference images.",
                "position": 1504
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]