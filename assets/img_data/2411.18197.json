[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18197/x1.png",
                "caption": "",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18197/x2.png",
                "caption": "Figure 2:Pipeline of the proposed framework.Given an input 3D character, we produce high-quality blend weights, bones, and pose-to-rest transformation for it, so that any animation is within easy reach.\nFirst, we coarsely localize the joints with a pre-trained lite version of this framework, which enables a finer shape representation.\nThen the shape is encoded into a neural field with a particle-based autoencoder. The decoding process involves spatial and learnable queries for different animation assets.\nFinally, the structure-aware modeling of bones is proposed to better align the predictions with skeleton topology priors.",
                "position": 389
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18197/x3.png",
                "caption": "Figure 3:Pipeline of the proposed structure-aware transformer.The per-bone shape-aware embedding is first added with its parent bone’s latent, which is encoded from the autoregressive outputs (in inference) or the ground-truth values (in training). The summation is then fused with the ancestral bones’ features via the masked causal attention. Eventually, bone attributes are decoded from the output shape- and structure-aware embeddings.\nIn inference, the whole process follows the paradigm of next-child-bone prediction.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x4.png",
                "caption": "Figure 4:Comparison with Meshy[meshy]and Tripo[tripo].We feed them the same image as reference and compare the performance based on their generated 3D models respectively.\nThe blend weights of two joints,i.e., Left Shoulder and Right Leg, are visualized.\nGiven that these baselines can only apply preset motions and their rest-pose models cannot be exported, we apply a similar “running” sequence to all the methods for fair comparison.\nThe T-pose models predicted by our method are also included as the front-view animating results.",
                "position": 547
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18197/x5.png",
                "caption": "Figure 5:Comparison with RigNet[xu2020rignet].We visualize the blend weights of selected joints and manually deform them to assess the impact of rigging quality on skinning results.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x6.png",
                "caption": "Figure 6:Comparison with TADA[liao2024tada]and HumanGaussian[liu2024humangaussian](HG).We use the generated meshes from TADA and 3D Gaussians from HG for comparison.\nNote that the skeletons of these two baselines are identical to the shape-specific SMPL[loper2015smpl]templates (without bone tail), with their weights interpolated from the template meshes.\nZoom in to better view the details.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x7.png",
                "caption": "Figure 7:Results of more cases to demonstrate the advantage of our method.The detailed explanations can be found in the supplementary material.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x8.png",
                "caption": "Figure 8:Visualizations of some ablative experiments.We show the effectiveness of the proposed modules and design choices by visualizing the predicted bones, blend weights, and pose transformations.",
                "position": 666
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S1Formulation of Low-Rank Dynamics",
        "images": []
    },
    {
        "header": "S2Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18197/x9.png",
                "caption": "Figure S1:The coarse (upper) and fine (lower) stages of training our framework.In the coarse stage, the input shape is uniformly sampled and only the bone positions are predicted. We apply data augmentation to the inputs via random 3D rotations, so that the coarse model is generalizable to global transformations of in-the-wild cases with an acceptable accuracy.\nIn the fine stage, we apply canonical transformation and hierarchical sampling to the shapes in advance based on the ground-truth bone positions.\nThen during inference, a 3D character is fed into the coarse framework to get its bone positions, which guide the establishment of coarse-to-fine shape representation later in the fine framework.\nNote that the body prior loss (Sec.3.4) is directly applied to the bone positions. As for pose prediction, we take the ground-truth bones as a proxy and use the predicted pose to transform them, thereby indirectly affecting the pose optimization.",
                "position": 1657
            }
        ]
    },
    {
        "header": "S3Data Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.18197/x10.png",
                "caption": "Figure S2:Some samples from the collected Mixamo[mixamo]dataset.The dataset contains bipedal humanoids with different shapes, ranging from realistic humans to cartoon or fantasy creatures. Each character is preprocessed to be animatable by any of the motion sequences.\nThe proposed framework is trained on this dataset.",
                "position": 1721
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x11.png",
                "caption": "Figure S3:Some samples of the anime characters with additional accessories for the extra-bone fine-tuning.These characters are all manually created using Vroid Studio[vroid]and then preprocessed to be compatible with the standard skeleton definition of Mixamo[mixamo].",
                "position": 1741
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x12.png",
                "caption": "Figure S4:Additional Comparison with generative 3D methods,i.e., Meshy[meshy]and Tripo[tripo].We feed them the same image as reference and compare the performance based on their generated 3D models respectively.\nThe blend weights of two joints,i.e., Left Shoulder and Right Leg, are visualized.\nThe T-pose models predicted by our method are included as the front-view animating results.\nZoom in to better view the details.",
                "position": 1747
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x13.png",
                "caption": "Figure S5:Additional comparison with template-based avatar generation methods,i.e., TADA[liao2024tada]and HumanGaussian[liu2024humangaussian](HG).We use the generated meshes from TADA and 3D Gaussians from HG for comparison.\nNote that the skeletons of these two baselines are identical to the shape-specific SMPL[loper2015smpl]templates (without bone tail), with their weights interpolated from the template meshes.\nZoom in to better view the details.",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x14.png",
                "caption": "Figure S6:Additional comparison with auto-rigging methods,i.e., RigNet[xu2020rignet], TARig[ma2023tarig], and Neural Blend Shapes[li2021learning](Neural BS).We visualize the blend weights of selected joints and manually deform them to assess the impact of rigging quality on skinning results.\n*: Neural Blend Shapes only support T-pose inputs, so for the non-rest cases (lower two), we feed it the T-pose meshes transformed by our pose-to-rest predictions.",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x15.png",
                "caption": "Figure S7:Comparison with commercial auto-rigging software,i.e., Mixamo[mixamo]and Anything World[anythingworld].Note that these two tools can only deal with simple input poses (T- or A-pose is recommended) and often raise errors when faced with complex ones.",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x16.png",
                "caption": "Figure S8:Qualitative comparison with RigNet[xu2020rignet]and TARig[ma2023tarig]on cases from the test split of “ModelsResource-RigNetv1” dataset[xu2019predicting].While both baselines are exactly trained on this dataset and ours are not, we still achieve the best performance.",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x17.png",
                "caption": "Figure S9:Results of more cases to demonstrate the advantage of our method.(a) Fine-grained control of fingers; (b) Capacity of abnormal shapes; (c) Complex input poses; (d) Efficiency for high polygon models; (e) Support of asymmetric inputs; (f) Adaptation to non-existing bones; (g) & (h): Extension to extra bones (e.g., long ears and tails).",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x18.png",
                "caption": "Figure S10:Qualitative analysis of our geometry-aware attention module and its injecting method.The proposed attention-based injection can benefit from normal information without any side effects.",
                "position": 1839
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x19.png",
                "caption": "Figure S11:Visualization of the attention score of our geometry-aware attention module.These per-sampled-point values are extracted from the first attention head (out of 8 heads in total).\nThe brighter color (yellower) indicates more attention to normals rather than coordinates.\nWe also use green bounding boxes to label some clusters where high-attention-score points are densely distributed.\nIt can be observed that the module adaptively learns to rely more on normals in regions like the inner thigh since coordinates become less discriminative there.",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2411.18197/x20.png",
                "caption": "Figure S12:Illustration of the limitations of SMPL-based rigging.While SMPL provides a good template for skeletons and weights, it lacks the flexibility to handle exaggerated body shapes.",
                "position": 1854
            }
        ]
    },
    {
        "header": "S4More Results",
        "images": []
    }
]