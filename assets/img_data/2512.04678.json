[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04678/x1.png",
                "caption": "",
                "position": 139
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04678/x2.png",
                "caption": "Figure 2:Comparison of EMA Sink with Existing Methods.Long video generation models typically extrapolate beyond their training sequence length during inference. (a) Window Attention caches only recent tokens for efficient inference but suffers performance degradation. (b) Sliding Window with attention sinks retains initial tokens for stable attention computation and recent tokens for extrapolation. However, discarding intermediate frames causes over-reliance on the first frame, leading to “frame copying” and stiff transitions. (c) EMA Sink preserves full history through exponential moving average (EMA) updates of all historical frames, maintaining stable and consistent performance in long video extrapolation without increasing computational cost.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2512.04678/x3.png",
                "caption": "Figure 3:Pipeline of Reward Forcing.In a streaming text-to-video generation, noisy tokens in the current stream are first projected to produce new key-value pairs (green blocks), which are appended to the KV cache for attention computation. When the current KV cache reaches its maximum attention window size, sink tokens initialized from start frames (yellow blocks) are updated via exponential moving average using evicted tokens (pink blocks). During training, hallucinated tokens are decoded into videos to compute a reward score via a reward function. This score is then used to weight the distribution matching gradient from the teacher model.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2512.04678/x4.png",
                "caption": "Figure 4:Qualitative comparison on dynamic complexity of long video generation.Reward Forcing excels in both text alignment and motion dynamics while baselines exhibit diminished dynamics and weaker alignment.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2512.04678/x5.png",
                "caption": "Figure 5:Qualitative comparison on long-range temporal consistency.Reward Forcing maintains superior coherence over long-horizon, while baselines suffer from noticeable quality degradation and inconsistency over time.",
                "position": 344
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04678/x6.png",
                "caption": "Figure 6:(Left)Ablation study on our proposed module, showing qualitative improvement.(Right)Top: Reward Forcing training leads to a steady rise in the dynamic score.Bottom: The plot of attention size versus FPS underscores the source of our inference efficiency.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2512.04678/x7.png",
                "caption": "Figure 7:Interactive video generation.Reward Forcing supports real-time prompt interaction with seamless transitions.",
                "position": 941
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S1: More Video Results",
        "images": []
    },
    {
        "header": "S2: User Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04678/x8.png",
                "caption": "Figure 8:User study instruction screenshots.",
                "position": 2292
            }
        ]
    },
    {
        "header": "S3: More Quantitative Results and Details",
        "images": []
    },
    {
        "header": "S4: More Implementation details",
        "images": []
    },
    {
        "header": "S5: Further Related Works",
        "images": []
    },
    {
        "header": "S6: Discussion and Future Work",
        "images": []
    },
    {
        "header": "S7: Border Social Impact",
        "images": []
    }
]