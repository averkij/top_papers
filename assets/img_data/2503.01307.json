[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01307/x1.png",
                "caption": "Figure 1:Comparative analysis of Qwen-2.5-3B and Llama-3.2-3B models with RL on Countdown.(Top) (a) Performance scores on Countdown for both models (b) Evolution of response lengths throughout RL training.\n(Middle) Emergence of specific reasoning characteristics as a function of training steps for Qwen-2.5-3B (left) and Llama-3.2-3B (right).\n(Bottom-Left) (a) Countdown performance when base models are primed with a synthetic dataset of desired reasoning behaviors; (b) Differential impact of RL on reasoning behaviors of the primed Llama3.2-3B: amplification of backtracking and verification contrasted with suppression of backward chaining and subgoal setting.\n(Bottom-Right) Comparative efficacy of teaching reasoning behaviors through fine-tuning on a curated OpenWebMath dataset, demonstrating that Llama’s reasoning capabilities can be improved to match Qwen’s through targeted training.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Identifying and Engineering Self-Improving Behavior",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01307/x2.png",
                "caption": "Figure 2:The effects of priming with different cognitive behaviors.(a, b) Performance comparison on the Countdown task between Llama-3.2-3B, Qwen-2.5-3B and their primed variants, illustrating the influence of reasoning behavior priming on scores.\n(c, d) Response length analysis for standard and primed Llama and Qwen models, showing how priming influences reasoning length.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2503.01307/x3.png",
                "caption": "Figure 3:Analysis of four key reasoning behaviors with Llama-3.2-3B, Qwen-2.5-3B, and primed versions of Llama-3.2-3B.Plots show mean frequency of (a) solution verification steps, (b) problem-solving backtracking instances, (c) explicit subgoal setting, and (d) backward chaining reasoning approaches across different tasks.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2503.01307/x4.png",
                "caption": "Figure 4:Exploration of different reasoning behaviors in base models. An analysis with Qwen2.5-3B, Llama3.2-3B, and Llama3.1-70B on Countdown.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2503.01307/",
                "caption": "Figure 5:Impact of Empty Chain-of-Thought Priming on Model Performance and Response CharacteristicsComparative analysis of (a) performance scores on the Countdown task and (b) response length distributions across different model configurations: baseline models (Llama3.2-3B, Qwen2.5-3B), unprimed conditions, length-matched empty CoT priming, and all-strategies-primed Llama3.2-3B.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2503.01307/x6.png",
                "caption": "Figure 6:Effects of Incorrect Chain-of-Thought Priming on Model Performance and Output CharacteristicsEvaluation of (a) Countdown task performance scores and (b) response length distributions comparing four conditions: Llama3.2-3B and Qwen2.5-3B base models, Llama3.2-3B with correct all-strategy priming, and Llama3.2-3B primed with incorrect reasoning examples.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2503.01307/x7.png",
                "caption": "Figure 7:Behaviors present in Math Pretraining Datasets. An analysis of the behaviors present in 200,000 randomly sampled documents of OpenWebMath and FineMath. We measure the average count of the behaviors in each document.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2503.01307/x8.png",
                "caption": "Figure 8:Impact of curated pretraining on model performance and behavior.(a) Comparison of performance scores across base models (Llama-3.2-3B, Qwen-2.5-3B) and Llama variants with curated pretraining versus behavior-minimized control. (b) Evolution of response lengths during training for each model configuration. (c) Emergence and development of specific cognitive behaviors in the curated pretraining model over training steps.",
                "position": 284
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Generation",
        "images": []
    },
    {
        "header": "Appendix BPriming",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01307/x9.png",
                "caption": "Figure 9:Scores of the priming datasets generated with Claude.Analyzing the average scores of Claude when instructed to solve Countdown employing different cognitive behaviors.",
                "position": 962
            }
        ]
    },
    {
        "header": "Appendix CReinforcement Learning",
        "images": []
    },
    {
        "header": "Appendix DMetrics.",
        "images": []
    },
    {
        "header": "Appendix EPretraining Data Interventions.",
        "images": []
    }
]