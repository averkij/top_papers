[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x1.png",
                "caption": "",
                "position": 70
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x2.png",
                "caption": "Figure 2:Comparing ourone-stepSwiftEdit withfew-stepandmulti-step diffusionediting methods in terms of background preservation (PSNR), editing semantics (CLIP score), and runtime. Our method delivers lightning-fast text-guided editing while achieving competitive results.",
                "position": 86
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x3.png",
                "caption": "Figure 3:Proposed two-stage training for our one-step inversion framework. In stage 1, we warms up our inversion network on synthetic data generated by SwiftBrushv2. At stage 2, we shift our focus to real images, enabling our inversion framework to instantly invert any input images without additional fine-tuning or retraining.",
                "position": 158
            }
        ]
    },
    {
        "header": "4Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x4.png",
                "caption": "Figure 4:Comparison of inverted noise predicted by our inversion network when trained without and with stage 2 regularization loss.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x5.png",
                "caption": "(a)Self-guided editing mask extraction. Given source and editing prompts, our inversion network predicts two different noise maps, highlighting the editing regionsMùëÄMitalic_M.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x5.png",
                "caption": "(a)Self-guided editing mask extraction. Given source and editing prompts, our inversion network predicts two different noise maps, highlighting the editing regionsMùëÄMitalic_M.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x6.png",
                "caption": "(b)Effect of global scale and our edit-aware scale. Comparison of edited results between varying global image condition scalesùê±subscriptùë†ùê±s_{{\\bf x}}italic_s start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPTwith our ARaM.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x7.png",
                "caption": "(c)Effect of editing strength scale. Visualization of edited results when varying mask-based text-alignment scalesysubscriptùë†ùë¶s_{y}italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT.",
                "position": 348
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x8.png",
                "caption": "Figure 6:Comparative edited results. The first column shows the source image, while source and edit prompts are noted under each row.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x9.png",
                "caption": "Figure 7:User Study.",
                "position": 571
            }
        ]
    },
    {
        "header": "6Ablation Study",
        "images": []
    },
    {
        "header": "7Conclusion and Discussion",
        "images": []
    },
    {
        "header": "8Derivation of the Regularization Loss in Stage 2",
        "images": []
    },
    {
        "header": "9Additional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x10.png",
                "caption": "Figure 8:Qualitative results when combining our inversion framework with other one-step text-to-image generation models.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x11.png",
                "caption": "(a)Varyingseditsubscriptùë†edits_{\\text{edit}}italic_s start_POSTSUBSCRIPT edit end_POSTSUBSCRIPTscale at different levels ofsnon-editsubscriptùë†non-edits_{\\text{non-edit}}italic_s start_POSTSUBSCRIPT non-edit end_POSTSUBSCRIPTwith defaultsy=2subscriptùë†ùë¶2s_{y}=2italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 2.",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x11.png",
                "caption": "(a)Varyingseditsubscriptùë†edits_{\\text{edit}}italic_s start_POSTSUBSCRIPT edit end_POSTSUBSCRIPTscale at different levels ofsnon-editsubscriptùë†non-edits_{\\text{non-edit}}italic_s start_POSTSUBSCRIPT non-edit end_POSTSUBSCRIPTwith defaultsy=2subscriptùë†ùë¶2s_{y}=2italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT = 2.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x12.png",
                "caption": "(b)Varyingsysubscriptùë†ùë¶s_{y}italic_s start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPTscale at different levels ofsnon-editsubscriptùë†non-edits_{\\text{non-edit}}italic_s start_POSTSUBSCRIPT non-edit end_POSTSUBSCRIPTwith defaultsedit=0subscriptùë†edit0s_{\\text{edit}}=0italic_s start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT = 0.",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x13.png",
                "caption": "Figure 10:Visualization of our extracted mask along with edited results using guided text described under each image row.",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x14.png",
                "caption": "Figure 11:Edit images with flexible prompting.SwiftEdit achieves satisfactory reconstructed and edited results with flexible source and edit prompt input (denoted under each image).",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x15.png",
                "caption": "Figure 12:Face identity and expression editing via simple prompts. Given a portrait input image, SwiftEdit can perform a variety of facial identities along with expression editing scenarios guided by simple text within just0.23seconds.",
                "position": 882
            }
        ]
    },
    {
        "header": "10More Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04301/x16.png",
                "caption": "Figure 13:Comparative results on the PieBench benchmark",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x17.png",
                "caption": "Figure 14:Comparative results on the PieBench benchmark",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2412.04301/x18.png",
                "caption": "Figure 15:Comparative results on the PieBench benchmark",
                "position": 906
            }
        ]
    },
    {
        "header": "11Societal Impacts",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]