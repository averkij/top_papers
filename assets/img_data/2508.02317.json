[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02317/x1.png",
                "caption": "Figure 1:Comparison between VeOmni and Existing Training Frameworks.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VeOmni: Scalable Omni-Modal Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02317/x2.png",
                "caption": "Figure 2:Composite Architecture for Omni-Modal LLMs. The architecture consists of three fully decoupled modules: Encoder, Foundation, and Decoder.",
                "position": 219
            },
            {
                "img": "https://arxiv.org/html/2508.02317/x3.png",
                "caption": "Figure 3:Overview of VeOmniâ€™s n-D Parallelism Design.The figure on the left illustrates the data flow of VeOmni during the training of omni-modal LLMs. The encoder for each modality processes its respective input, and then scatters the features to the corresponding ranks through an all-to-all communication operation.\nThe right figure shows the 3D parallelism system of VeOmni. Attention blocks apply HSDP with data sharded size 2 and data replicate size 2, input apply Ulysses with sequence parallel size 2, Mixture-of-Experts blocks apply expert parallel and FSDP with experts parallel size 2 and data sharded size 2.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02317/x4.png",
                "caption": "Figure 4:2D Parallelism (FSDP+SP) on Qwen2-VL 7B with 8 GPUs.The maximum sequence length varies from 8K to 256K tokens, with supported sequence parallel sizes ranging from 1 to 4.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2508.02317/x5.png",
                "caption": "Figure 5:2D Parallelism (FSDP+SP) on Qwen2-VL 72B with 128 GPUs.The maximum sequence length varies from 8K to 96K tokens, with supported sequence parallel sizes ranging from 1 to 8.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2508.02317/x6.png",
                "caption": "Figure 6:(a) 3D Parallelism (FSDP+SP+EP) on a 30B Qwen3MoE-based omni-modal LLM with 128 GPUs. The maximum sequence length varies from 16K to 192K tokens, with supported sequence parallel sizes ranging from 1 to 4 and expert parallel sizes ranging from 1 to 4. (b) Convergence Study on three Distinct omni-modal LLMs.",
                "position": 418
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix BAPI Design",
        "images": []
    }
]