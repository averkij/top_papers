[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11190/x1.png",
                "caption": "Figure 1:TheMini-Omni2model architecture.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11190/extracted/5978079/figures/jpg2/sample2.jpg",
                "caption": "Figure 2:Mini-Omni2now supports streaming speech responses for image, audio and text inputs.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Mini-Omni2",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11190/x2.png",
                "caption": "Figure 3:Schematic diagram of multi-layer tokens for input and output of the main task model ofMini-Omni2.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2410.11190/extracted/5978079/figures/jpg2/4_vocab.jpg",
                "caption": "Figure 4:The special multi-layer vocabulary construction ofMini-Omni2.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2410.11190/x3.png",
                "caption": "Figure 5:Mini-Omni2â€™s three-stage training phases",
                "position": 212
            }
        ]
    },
    {
        "header": "4Data and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11190/x4.png",
                "caption": "Figure 6:Use cases ofMini-Omni2",
                "position": 487
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]