[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22120/x1.png",
                "caption": "Figure 1:Illustration of different paradigms.Within each colored box, the top row illustrates the training stage and the bottom row shows the inference stage.\nPrior approaches are limited by shape-rigid inference-time tools and domain-specific solutions that generalize poorly.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22120/x2.png",
                "caption": "Figure 2:Overview of the Bi-directional Perceptual Shaping (BiPS) framework.BiPS employs a two-stage training curriculum built on the GRPO framework. Stage 1 (Consistency Stage) minimizes the KL-divergence (Lc​o​n​sL_{cons}) between the original policy (πθ\\pi_{\\theta}) and the policy on an evidence-preserving view (π~θ\\tilde{\\pi}_{\\theta}). Stage 2 (Separation Stage) maximizes the KL-divergence (Ls​e​pL_{sep}) between the original policy (πθ\\pi_{\\theta}) and the policy on an evidence-ablated view, forcing the model to ground its reasoning in visual evidence.",
                "position": 170
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22120/x3.png",
                "caption": "Figure 3:Overview of our data generation pipeline.This pipeline programmatically edits chart source code to generate the paired Evidence-Preserving (IpresI_{\\mathrm{{pres}}}) and Evidence-Ablated (IablI_{\\mathrm{{abl}}}) views used for bi-directional training.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22120/figures/icons/chart.png",
                "caption": "Table 1:Evaluation on chart understanding and general perception & reasoning benchmarks.Models withare chart-specialized; models withare math-specialized;†denotes our models.Bluenumbers denote chart-related data, andrednumbers denote math-related or perception-related data.Avg.is the arithmetic mean over available metrics. Models trained onMathVision(marked with⋆) omit their scores on MathVision.\nBest scores are inbold, and second-best scores areunderlined.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2512.22120/figures/icons/math.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2512.22120/x4.png",
                "caption": "Figure 4:Accuracy on CharXiv with respect to the weighting coefficients of the consistencyα\\alphaand separationβ\\betaconstraints.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2512.22120/x5.png",
                "caption": "Figure 5:Case study on ChartXiv comparing Qwen2.5-VL-7B and our BiPS-Chart. BiPS yields more visually grounded answers.",
                "position": 840
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Implementation Details",
        "images": []
    },
    {
        "header": "7Additional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22120/x6.png",
                "caption": "Figure 6:Cross-domain case on visual counting.The baseline fails due to incomplete object reasoning, whereas BiPS correctly tracks and subtracts objects to obtain the right answer.",
                "position": 1039
            }
        ]
    },
    {
        "header": "8Prompts",
        "images": []
    }
]