[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13744/x1.png",
                "caption": "Figure 1:Comparison of models evaluated on both ProcessBench(Zheng et al.,2024a)and our Hard2Verify benchmark. Past benchmarks do not sufficiently evaluate in the frontier-level math settings that Hard2Verify does; On the same error identification task, Qwen2.5-Math-PRM-72B performance drops from ProcessBench state-of-the-art at 78.3 to 37.3 on Hard2Verify.",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13744/x2.png",
                "caption": "Figure 2:Breakdown of correct vs. incorrect steps (left) and responses (right) by model. We consider a response incorrect ifanystep in the response is labeled incorrect.",
                "position": 254
            }
        ]
    },
    {
        "header": "3The Hard2Verify Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13744/x3.png",
                "caption": "Figure 3:Count of correct (left) and incorrect (right) labels by model solution step. Models tend to begin solutions correctly, but typically start to get derailed after a few steps.",
                "position": 310
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13744/x4.png",
                "caption": "Figure 4:Weaker models are unable to find mistakes, eventually consideringallsteps correct: TNR tends toward 0 while TPR tends towards 1.",
                "position": 930
            }
        ]
    },
    {
        "header": "5Additional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13744/x5.png",
                "caption": "Figure 5:Top: Scaling inference-time compute sequentially leads to higher performance in GPT-5 and gpt-oss models, with large gains for gpt-oss-20B (59.69→\\rightarrow70.93) and 120B (61.46→\\rightarrow74.64) in terms of step-level Balanced F1.Bottom: Parallel decoding has little effect on step-level F1 performance for gpt-oss-20B, failing to bridge the gap vs. gpt-oss-20B at high-reasoning effort.",
                "position": 939
            },
            {
                "img": "https://arxiv.org/html/2510.13744/x6.png",
                "caption": "",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2510.13744/x7.png",
                "caption": "Figure 6:Verifier TPR and TNR based on response generator model. For strong verifiers (GPT-5, Gemini 2.5 Pro), TPR varies based on generator, with GPT-5 exhibiting the most stable performance on a per-generator basis. Claude Sonnet 4 generates the easiest to catch mistakes across all verifiers, whereas Gemini 2.5 Pro produces the hardest to catch mistakes, as measured by TNR.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2510.13744/x8.png",
                "caption": "Figure 7:Each generators evaluates self-produced responses, and the fraction of steps correctly solved vs. fraction of steps correctly verified for a given question is plotted. In general, we find that models are more successful in catching mistakes than generating error-free responses.",
                "position": 977
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed Dataset Sources",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13744/x9.png",
                "caption": "Figure 8:Response-LevelandErrorIDtasks follow similar trends in TPR and TNR, with weaker verifiers unable to identify mistakes.",
                "position": 1869
            },
            {
                "img": "https://arxiv.org/html/2510.13744/x10.png",
                "caption": "",
                "position": 1873
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": []
    },
    {
        "header": "Appendix CPrompts for Generation and Evaluation",
        "images": []
    },
    {
        "header": "Appendix DAnnotation details",
        "images": []
    },
    {
        "header": "Appendix EEvaluated baselines",
        "images": []
    }
]