[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21754/x1.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2601.21754/x2.png",
                "caption": "Figure 1:Exploration and Distillation Stage will either directly award the language models the skills, such as the performance on Sokoban-Box2 (below) that leads to direct convergence (0.0 to 0.45), or indirectly teach relevant knowledge that will be later activated via Evolving Stage, such as the performance on Sudoku (above figure, from 0.0, to 0.29, then to 0.97).",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2601.21754/x3.png",
                "caption": "Figure 2:Overview of the SCOUT framework. The pipeline consists of three stages: (1) Exploration Stage: Lightweight scouts efficiently capture environmental dynamics to generate expert trajectories; (2) Distillation Stage: These trajectories are textualized to \"warm-up\" the LLM via supervised fine-tuning; (3) Evolving Stage: The LLM further refines its reasoning and decision making capabilities through multi-turn PPO.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Sub-Scale Collaboration On Unseen Tasks",
        "images": []
    },
    {
        "header": "3Experiment Setup",
        "images": []
    },
    {
        "header": "4Experimental Results and Findings",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21754/x4.png",
                "caption": "Figure 3:Comparison of task performance during sequential RL. While the Sequential RL (Left) exhibits some performance degradation on previously learned tasks, SCOUT (Right) successfully preserves historical task knowledge (e.g., Bandit, FrozenLake) while adapting to new environments (e.g., Sudoku), achieving a near optimal multi-task agent.",
                "position": 1889
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotation",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CRelated Works",
        "images": []
    },
    {
        "header": "Appendix DExperiments",
        "images": []
    },
    {
        "header": "Appendix EExtra Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21754/x5.png",
                "caption": "Figure 4:Scout-DQN detailed performance on 6 unseen tasks.",
                "position": 2920
            },
            {
                "img": "https://arxiv.org/html/2601.21754/x6.png",
                "caption": "Figure 5:Scout-PPO detailed performance on 6 unseen tasks.",
                "position": 2926
            }
        ]
    },
    {
        "header": "Appendix FUsed Prompts",
        "images": []
    },
    {
        "header": "Appendix GOther Details",
        "images": []
    }
]