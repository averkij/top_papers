[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11821/extracted/6447946/figs/rl_agent.png",
                "caption": "Figure 1:Overview of the multi-turn LLM agent pipeline and comparison of different advantage estimation methods. The agent interacts with the tool environment across multiple steps: reasoning, tool use, and answer generation, receiving both turn-level and outcome-level rewards. GRPO is used as a representative algorithm to illustrate the different advantage estimation strategies. GRPO-OR and GRPO-MR serve as baselines with trajectory-level advantage estimation, while MT-GRPO is our proposed variant with fine-grained turn-level advantage estimation.",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multi-Turn Tool-Calling LLM Agent System",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11821/x1.png",
                "caption": "Figure 2:Curves for different training reward components during training with various algorithms (MT-GRPO, GRPO-OR, and GRPO-MR). Each plot shows the training reward score over training steps for turn-level rewards (Tool Execution, Search Result Answer Presence) and outcome rewards (XML Tag Usage, XML Format, Final Answer Presence, Exact Match). Dotted lines represent the average reward across 10 runs, while solid lines show trends smoothed using the Exponential Moving Average (EMA).",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x1.png",
                "caption": "",
                "position": 569
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x2.png",
                "caption": "",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x3.png",
                "caption": "",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x4.png",
                "caption": "",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x5.png",
                "caption": "",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x6.png",
                "caption": "",
                "position": 591
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASystem Prompt",
        "images": []
    },
    {
        "header": "Appendix BRollout Examples",
        "images": []
    },
    {
        "header": "Appendix CTurn-Level Advantage Estimation for other RL Algorithms",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.11821/x7.png",
                "caption": "Figure 3:Curves for different training reward components during training using GRPO-OR, where shaded regions represent the range between the maximum and minimum values across 10 runs.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x7.png",
                "caption": "",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x8.png",
                "caption": "",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x9.png",
                "caption": "",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x10.png",
                "caption": "",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x11.png",
                "caption": "",
                "position": 1842
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x12.png",
                "caption": "",
                "position": 1846
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x13.png",
                "caption": "Figure 4:Curves for different training reward components during training using GRPO-MR, where shaded regions represent the range between the maximum and minimum values across 10 runs.",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x13.png",
                "caption": "",
                "position": 1855
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x14.png",
                "caption": "",
                "position": 1859
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x15.png",
                "caption": "",
                "position": 1864
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x16.png",
                "caption": "",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x17.png",
                "caption": "",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x18.png",
                "caption": "",
                "position": 1877
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x19.png",
                "caption": "Figure 5:Curves for different training reward components during training using MT-GRPO, where shaded regions represent the range between the maximum and minimum values across 10 runs.",
                "position": 1883
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x19.png",
                "caption": "",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x20.png",
                "caption": "",
                "position": 1890
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x21.png",
                "caption": "",
                "position": 1895
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x22.png",
                "caption": "",
                "position": 1899
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x23.png",
                "caption": "",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2505.11821/x24.png",
                "caption": "",
                "position": 1908
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experiment Results",
        "images": []
    }
]