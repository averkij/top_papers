[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19202/figures/krux.pdf",
                "caption": "Figure 1:KRUXpipeline. Starting from the upper left, we prompt an LLM (one of base, DeepSeek-R1, Base-Math, Base-STEM, and Base-BOTH) with a question fromSciReasas knowledge source, collect the output and reasoning traces, and feed the reasoning traces to DeepSeek-R1 as the extractor to generate knowledge ingredients (KIs). We then evaluate the tested model with KI-augmented questions, which allows us to study three key research questions (RQ1, RQ2, RQ3) regarding LLMs’ knowledge and reasoning capabilities in scientific problem-solving.",
                "position": 54
            },
            {
                "img": "https://arxiv.org/html/2508.19202/figures/scireas_cost_scatter.pdf",
                "caption": "Figure 2:Frontier reasoning models’ performance evaluated onSciReas.\nThe X-axis shows the cost per 1k instances in USD.\nDifferent reasoning settings on the same model can result in distinct costs and performance, but the margins vary depending on the models.",
                "position": 82
            }
        ]
    }
]