[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/wildcard.drawio.png",
                "caption": "Figure 1:Two samples from MMLU-Pro (left) and its WiCkeD variant (right), whereHydrogenandCentrifugalwere removed. Correct answers in bold. Llama-3.1 8B correctly answers both original questions but fails on the WiCkeD variant for the second question. The probability distribution of the model for each answer is also shown.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/sba_example.drawio.png",
                "caption": "Figure 2:Applying WiCkeD on a single best answer (SBA) example (best answer D, second best answer A) would lead to an incoherent WiCkeD variant (incorrectly havingNone of the aboveas the gold correct answer instead of A). We thus copy SBA examples verbatim, see §3.2for details.",
                "position": 210
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetecting Single Best Answer examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/piechart.png",
                "caption": "Figure 3:The changes in models’ answers of the original benchmarks and the WiCkeD variant using chain-of-thoughts.",
                "position": 1152
            }
        ]
    },
    {
        "header": "Appendix BInstruct vs Base Models on Chain of Thought",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/mmlu_computer_science.png",
                "caption": "Figure 4:Examples from the MMLU computer science task using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results.",
                "position": 1190
            },
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/allenai_arc.png",
                "caption": "Figure 5:Examples from the AllenAi Arc challenge using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results. The first few-shot example does not includeNone of the aboveoption because it was classified as SBA question.",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/csqa.png",
                "caption": "Figure 6:Examples from the Common-sense QA using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results. The first few-shot example does not includeNone of the aboveoption because it was classified as SBA question.",
                "position": 1196
            },
            {
                "img": "https://arxiv.org/html/2502.18316/extracted/6232320/latex/imgs/mmlu_redux.png",
                "caption": "Figure 7:Examples from the MMLU-Redux using WiCkeD. We show 3-shot for brevity, but 5-shot was actually used in the experiments for the main results.",
                "position": 1199
            }
        ]
    },
    {
        "header": "Appendix CMultiple Choice Prompting",
        "images": []
    }
]