[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07685/x1.png",
                "caption": "Figure 1:Overview ofResearchRubricsand its evaluation pipeline.",
                "position": 195
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Overview ofResearchRubrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07685/x2.png",
                "caption": "Figure 2:The three-stage pipeline for creating and refining prompts and rubrics. An initial draft by Expert 1 is iteratively improved with Expert 2 before a final review and adjustment by Expert 3.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x3.png",
                "caption": "Figure 3:Distribution of task domains in our collected data.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x4.png",
                "caption": "(a)Distribution of task complexity dimensions inResearchRubrics.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x4.png",
                "caption": "(a)Distribution of task complexity dimensions inResearchRubrics.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x5.png",
                "caption": "(b)Distribution of rubric criteria categories. Implicit and explicit criteria dominate the benchmark.",
                "position": 747
            }
        ]
    },
    {
        "header": "4Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07685/x6.png",
                "caption": "Figure 5:Rubric-axis failure rates across Deep Research agents.Dark bars represent ternary grading; light bars show binary grading. Implicit reasoning and synthesis show markedly higher failure rates compared to communication quality and references. The pattern holds across all three systems, indicating architectural rather than implementation limitations.",
                "position": 1083
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x7.png",
                "caption": "Figure 6:Performance across Conceptual Breadth, Logical Nesting, and Exploration (Ternary Evaluation)",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x8.png",
                "caption": "Figure 7:Comparison of length vs. score across token count for the ternary setting.",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x9.png",
                "caption": "Figure 8:Failure rate stratification by criterion importance.Mandatory criteria show systematically higher failure rates across most dimensions, with the notable exception of implicit reasoning, where optional criteria failures dominate. This inversion suggests implicit requirements primarily distinguish excellent from merely sufficient responses. Dark bars represent ternary grading; light bars show binary grading.",
                "position": 1119
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix BExtended Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07685/x10.png",
                "caption": "Figure 9:How many evaluation axes does each task cover?Distribution of the number of rubric axes per prompt. Most tasks require 4 to 5 distinct dimensions of quality simultaneously, encouraging balanced capabilities rather than single-axis optimization.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x11.png",
                "caption": "Figure 10:Number of rubric criteria per task.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x12.png",
                "caption": "Figure 11:Axis mix by domain.Stacked proportions of the six rubric axes across domains.",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x13.png",
                "caption": "Figure 12:Performance across Conceptual Breadth, Logical Nesting, and Exploration (Binary Evaluation)",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x14.png",
                "caption": "Figure 13:Performance across Conceptual Breadth, Logical Nesting, and Exploration (Ternary Evaluation)",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x15.png",
                "caption": "Figure 14:Heatmap of failure contribution by rubric axis across domains.",
                "position": 1660
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x16.png",
                "caption": "(a)Mismatch by category (binary).",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x16.png",
                "caption": "(a)Mismatch by category (binary).",
                "position": 1673
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x17.png",
                "caption": "(b)Mismatch by category (ternary).",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x18.png",
                "caption": "(c)Mismatch by importance (binary).",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x19.png",
                "caption": "(d)Mismatch by importance (ternary).",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x20.png",
                "caption": "(e)Mismatch rate by category (binary).",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x21.png",
                "caption": "(f)Mismatch rate by category (ternary).",
                "position": 1700
            }
        ]
    },
    {
        "header": "Appendix CPrompt and Response Length Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07685/x22.png",
                "caption": "Figure 16:Distribution of prompt word counts across all 101 tasks.\nThe distribution is right-skewed, with a mean of 87.6 words and a median of 68 words.",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x23.png",
                "caption": "Figure 17:Prompt word count by task complexity dimensions (Conceptual Breadth, Logical Nesting, and Exploration).\nLonger prompts are consistently associated with higher complexity levels.",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x24.png",
                "caption": "(a)Length vs. score (tokens; binary).",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x24.png",
                "caption": "(a)Length vs. score (tokens; binary).",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x25.png",
                "caption": "(b)Length vs. score (tokens; ternary).",
                "position": 2066
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x26.png",
                "caption": "(c)Length vs. score (words; binary).",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2511.07685/x27.png",
                "caption": "(d)Length vs. score (words; ternary).",
                "position": 2078
            }
        ]
    },
    {
        "header": "Appendix DSupplementary Figures and Tables",
        "images": []
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]