[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04860/x1.png",
                "caption": "Figure 1:An illustration of how self-evolution can degrade performance. The agent first solves a hard geometry problem correctly with a tool, but after repeated success on easy reasoning tasks without tools, it learns to avoid them and later produces a confident yet wrong answer.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Alignment Tipping Process",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04860/x2.png",
                "caption": "Figure 2:A conceptual illustration of ATP. An agent, initially aligned through techniques like DPO or GRPO, maintains aligned behavior. However, during self-evolution in a deployed environment with imperfect supervision, it discovers that violating rules can lead to higher rewards. This experience gradually shifts its policy, leading to persistent misaligned behavior. ATP is where the agent’s strategy flips, leading to persistent non-compliant behavior (red path). This can occur through single-agent self-interested exploration or be accelerated by multi-agent imitative strategy diffusion.",
                "position": 133
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04860/figs/sa.png",
                "caption": "Table 1:Performance comparison of Qwen3-8B and Llama-3.1-8B-Instruct with their DPO and GRPO variants across different self-evolution rounds. Here, we also provide line charts for a clearer understanding of the trends.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2510.04860/figs/ma_multi_k1.png",
                "caption": "Figure 3:Collusion rates across 3 self-evolution rounds for Qwen3-8B and its aligned variants. Each subplot corresponds to a specific configuration of the collusion thresholdtt. The higher thettvalue, the greater the difficulty of collusion.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2510.04860/figs/ma_multi_heat_k1.png",
                "caption": "Figure 4:Conditional probability of collusion in Round 2, given a successful collusion in Round 1.",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2510.04860/x3.png",
                "caption": "Figure 5:A trace of a multi-agent simulation illustrating imitative strategy diffusion. Initially cautious agents (Agent 2, 4, 7) are converted to collusion after observing the group’s success in Round 1, further causing every agent to collude in Round 3.",
                "position": 498
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BExperiments on incentive ratio",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04860/figs/ma_multi.png",
                "caption": "Figure 6:Collusion rates across 3 self-evolution rounds for Qwen3-8B and its aligned variants. Each subplot corresponds to a specific configuration of the collusion thresholdttand incentive radiokk.",
                "position": 1216
            }
        ]
    },
    {
        "header": "Appendix CEnvironment examples",
        "images": []
    }
]