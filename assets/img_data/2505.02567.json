[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02567/x1.png",
                "caption": "Figure 1:Timeline of Publicly Available and Unavailable Unified Multimodal Models. The models are categorized by their release years, from 2023 to 2025. Models underlined in the diagram representany-to-any multimodal models, capable of handling inputs or outputs beyond text and image, such as audio, video, and speech. The timeline highlights the rapid growth in this field.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02567/x2.png",
                "caption": "Figure 2:Architecture of multimodal understanding models, containing multimodal encoders, a connector, and a LLM. The multimodal encoders transform images, audio, or videos into features, which are processed by the connector as the input of LLM. The architectures of theconnector can be broadly categorized by three types: projection-based, query-based, and fusion-based connectors.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2505.02567/x3.png",
                "caption": "Figure 3:Illustration of diffusion-based text-to-image generation models, where various conditions beyond text are introduced to steer the outcomes. The image generation is formulated as a pair of Markov chains: a forward process that gradually corrupts input data by adding Gaussian noise, and a reverse process that learns a parameterized distribution to iteratively denoise back to the input data.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2505.02567/x4.png",
                "caption": "Figure 4:Illustration of core components in autoregressive models, including the autoregression sequence modeling and discrete vector quantization. Exiting autoregressive models can be roughly divided into three types: Next-Pixel Prediction flattens the image into a pixel sequence, Next-Token Prediction converts the image into a token sequence via a visual tokenizer, and Next-Multiple-Tokens Prediction outputs multiple tokens in an autoregressive step.",
                "position": 261
            }
        ]
    },
    {
        "header": "3Unified Multimodal Models for Understanding and Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02567/x5.png",
                "caption": "Figure 5:Classification of Unified Multimodal Understanding and Generation Models. The models are divided into three main categories based on their backbone architecture: Diffusion, MLLM (AR), and MLLM (AR + Diffusion). Each category is further subdivided according to the encoding strategy employed, including Pixel Encoding, Semantic Encoding, Learnable Query Encoding, and Hybrid Encoding. We illustrate the architectural variations within these categories and their corresponding encoder-decoder configurations.",
                "position": 314
            }
        ]
    },
    {
        "header": "4Datasets on Unified Models",
        "images": []
    },
    {
        "header": "5Benchmarks",
        "images": []
    },
    {
        "header": "6Challenges and Opportunities on Unified Models",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]