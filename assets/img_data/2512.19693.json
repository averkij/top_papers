[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19693/figs/Gallery_Icon.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2512.19693/x1.png",
                "caption": "",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19693/x2.png",
                "caption": "Figure 2:Frequency energy distribution.Normalized energye​(k)e(k)across frequency bands for diverse tokenizers.\nDINOv2 and CLIP focus on low-frequency (semantic) content, while SD-VAE retains more high-frequency energy, capturing finer details.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2512.19693/x3.png",
                "caption": "Figure 3:Overall architecture of our proposed Unified Autoencoding (UAE).The input image is separately encoded by both a pretrainedSemantic Encoder(e.g., DINOv2) and the trainableUnified Encoder.\nThe unified encoder is initialized from the semantic encoder and optimized under two complementary objectives:\nasemantic-wise lossthat aligns low-frequency components decomposed from the semantic encoder’s representations,\nand apixel-wise reconstruction lossthat enforces visual fidelity via thePixel Decoderby adaptively dilating the high-frequency components.\nThe decoder employs spectral transform blocks to refine residual-frequency content and produce the reconstructed image.\nThis joint optimization harmonizes semantic structure and pixel detail within a single latent space.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2512.19693/x4.png",
                "caption": "Figure 4:Retrieval results via frequency filtering.Text–Image retrieval (R@5) remains stable under low-pass filtering but degrades sharply under high-pass filtering,\nconfirming that semantic alignment primarily resides in low-frequency components.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19693/x5.png",
                "caption": "Figure 5:Qualitative comparison of reconstruction fidelity across autoencoding paradigms.We visualize reconstructed samples from representative methods, including SD-VAE[rombach2022ldm], RAE[zheng2025rae], and our proposed UAE.\nEach row corresponds to reconstructions from a fixed source set spanning text, human, object, and artistic domains.\nUAE produces the most consistent and semantically faithful reconstructions, preserving both high-frequency details (e.g., texture and edge sharpness) and global structure (e.g., layout and color harmony), while reducing the blurring and semantic drift observed in SD-VAE and RAE. (The detail comparisons are denoted in theyellowboxes.)",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2512.19693/x6.png",
                "caption": "Figure 6:t-SNE visualization of semantic embeddings.We compare the feature distributions from the DINOv2 encoder (left) and the band-0 (low-frequency) component of UAE (right).\nThe two plots exhibit similar global structures and class separability, indicating that UAE effectively preserves the semantic organization of the original encoder while introducing a unified latent space that remains compatible with frequency-based factorization.",
                "position": 744
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]