[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x1.png",
                "caption": "Figure 1:Reflective planning.Our method uses a VLM to propose actions and a diffusion dynamics model to imagine the future state of executing the plan. The imagined future helps the VLM reflect the initial plan and propose better action.",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2502.16707/x2.png",
                "caption": "Figure 2:Training data generation.Training data for the reflection mechanism is collected by relabeling the rollouts. For each timestep, two training examples are generated: (Q1, A1) for action proposal and (Q2, A2) for reflection.HùêªHitalic_His the imagination horizon, andh‚Ñéhitalic_his the history length.at‚àósuperscriptsubscriptùëéùë°a_{t}^{*}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPTis the action label given by the expert policy.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries and Problem Statement",
        "images": []
    },
    {
        "header": "4Reflective Planning with Vision Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x3.png",
                "caption": "Figure 3:Architecture of Diffusion Dynamics Model,which consists of a latent encoder, text encoder, Diffusion UNet and latent decoder. The latent encoder and text encoder are frozen during training, while Diffusion UNet and latent decoder are finetuned on our task data.ùí©ùí©\\mathcal{N}caligraphic_N: random noise.",
                "position": 319
            }
        ]
    },
    {
        "header": "5Multi-Stage Robotic Manipulation Planning Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x4.png",
                "caption": "Figure 4:Filmstrip of our method solving a complicated assembly task.Frames are indexed by timestep. The goal image is in the top-left corner (with a green border). Each frame is the observation after executing the action (in black) above it. The other action in gray is the original action proposed by the VLM if it is revised after reflection. We highlight the reflection process at timestep 15, where the VLM first proposes an action to pick up the purple brick, but after reflection, it chooses to pick up the yellow brick instead as the generated future state (red-bordered image) shows little progress towards the goal.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2502.16707/x5.png",
                "caption": "Figure 5:Task examples.(a) Generated multi-stage manipulation tasks with interlocking pieces. Top: initial configurations. Bottom: goal configurations. See App.Bfor more examples. (b) The graph shows the dependencies between the objects in the blue assembly board on the left. Each node represents an object, and each directed edge indicates the predecessor object should be assembled before the successor object.",
                "position": 389
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x6.png",
                "caption": "Figure 6:Performance of our method and baselines.Success rate (%) on 100 tasks. For the zero-shot test of state-of-the-art VLMs and MCTS, the experiments were conducted once; for other methods, the results are the average of five seeds.",
                "position": 405
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATask generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x7.png",
                "caption": "Figure 7:Example of task generation.(a) Voxel representation of the board. (b) Generating a base board. (c) Generating a red brick. (d) Generating another blue brick. (e) Critical voxels (highlighted in purple) at the intersection of the two bricks. (f) Handling intersection by assigning the critical voxels to the red brick. (g) Explosion view of the board consisting of three interlocking pieces.",
                "position": 1351
            }
        ]
    },
    {
        "header": "Appendix BSamples of generated tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x8.png",
                "caption": "Figure 8:Samples of generated tasks.We procedurally generate a variety of multi-stage manipulation tasks, ranging from simple peg insertion to complex assembly tasks that contains multiple interlocking pieces. Top: initial configurations. Bottom: goal configurations.",
                "position": 1358
            }
        ]
    },
    {
        "header": "Appendix CExpert policy",
        "images": []
    },
    {
        "header": "Appendix DTraining details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/x9.png",
                "caption": "Figure 9:Architecture of our VLM.The model consists of a vision encoder and an LLM. We also add Low-Rank Adaptation (LoRA)(Hu et¬†al.,2022)layers to LLM for efficient adaptation. The input sequence contains interleaved images and text, where images are encoded into latent embeddings with a shared vision encoder. Finally, the concatenation of text and image embeddings are fed into VLM for multimodal reasoning.",
                "position": 1529
            }
        ]
    },
    {
        "header": "Appendix EPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gemini-2-success.png",
                "caption": "Figure 10:Success cases of zero-shot VLMs.Top: Gemini-2.0; Middle: Gemini-2.0-Thinking; Bottom: GPT-4o.",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gemini-2-think-success.png",
                "caption": "",
                "position": 1755
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gpt4o-success.png",
                "caption": "",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gpto1-success.png",
                "caption": "Figure 11:Success cases of zero-shot VLMs (GPT-o1).",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gemini-2.png",
                "caption": "Figure 12:Failure case of Gemini-2.0.",
                "position": 1764
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gemini-2-think.png",
                "caption": "Figure 13:Failure case of Gemini-2.0-Thinking.",
                "position": 1767
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gpt4o.png",
                "caption": "Figure 14:Failure case of GPT-4o.",
                "position": 1770
            },
            {
                "img": "https://arxiv.org/html/2502.16707/extracted/6226102/figs/zero_shot_results/gpto1.png",
                "caption": "Figure 15:Failure case of GPT-o1.",
                "position": 1773
            },
            {
                "img": "https://arxiv.org/html/2502.16707/x10.png",
                "caption": "Figure 16:Examples of Diffusion Dynamic Models.",
                "position": 1776
            }
        ]
    },
    {
        "header": "Appendix FBaseline details",
        "images": []
    }
]