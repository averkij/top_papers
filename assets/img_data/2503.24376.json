[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24376/x1.png",
                "caption": "Figure 1:(a) Overview of SEED-Bench-R1 (SBR), which systematically evaluates post-training methods for MLLMs in video understanding. SBR features a three-level evaluation hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with training data containing easily verifiable ground-truth answers, toassess generalization across different levels. These tasks necessitate bothperception and reasoningto tackle complex real-world challenges. (b) Notably, an MLLM trained using reinforcement learning via GRPO outperforms both the base model and supervised fine-tuning (SFT) model, particularly in out-of-distribution scenarios (e.g., levels 2–3). Additionally, this RL-trained model exhibits strong generalization capabilities across general video understanding benchmarks (e.g., LongVideoBench).",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24376/x2.png",
                "caption": "Figure 2:Example questions from the three-level evaluation hierarchy in SEED-Bench-R1’s validation set, including in-distribution, cross-environment, and cross-environment-task scenarios.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SEED-Bench-R1",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24376/x3.png",
                "caption": "Figure 3:The variation curves of completion length and accuracy reward w.r.t. RL training steps. While the reward value generally increases during RL, the completion length of the MLLM does not show a significant increase.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2503.24376/x4.png",
                "caption": "",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2503.24376/x5.png",
                "caption": "Figure 4:Comparison of model responses to a Level-1 question from SEED-Bench-R1. The visual input includes 16 sampled frames from a video (showing task progress) and a final observation image. Attention maps (output-to-visual tokens) are shown for each model: the base (Qwen2-VL-7B), GRPO fine-tuned, and SFT fine-tuned versions. The base and SFT modelsexhibited illusory perceptions(red text), while the GRPO modelattended more accurately to visual regions—e.g., correctly identifying cream cheese in the pot (green box) and suggesting the next step (discarding the empty yogurt container). The SFT model’s attention was ineffective (red box), and the base model’s attention was dispersed, impairing judgment.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2503.24376/x6.png",
                "caption": "Figure 5:Comparison of model responses to a Level-2 (out-of-distribution, cross-environment) question from SEED-Bench-R1. The GRPO fine-tuned modeldemonstrates more accurate attentionto hand movement (highlighted in the green box). Interestingly, while the GRPO fine-tuned model produces similar incorrect reasoning steps as the SFT-trained model (red text), it ultimately outputs the correct answer by disregarding the flawed semantic reasoning. This suggests that GRPO, with its outcome-supervised reward signal, primarilyenhances visual perceptionbut may compromise the logical coherence and semantic accuracy of the model’s reasoning process.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2503.24376/x7.png",
                "caption": "Figure 6:Comparison of model responses to a Level-3 (out-of-distribution, cross-environment-task) question from SEED-Bench-R1.\nThe GRPO-tuned model attends effectively to the entire video, while the SFT-tuned model ignores the middle segments. SFT also favors superficial reasoning (e.g., templated phrases like “The person has already… and is holding…”), often mismatching visual observations, which is also evidenced in Figure4. This suggests GRPO trains models tosearch visual space adaptively, whereas SFTencourages memorized reasoning patterns.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2503.24376/x8.png",
                "caption": "Figure 7:Failure Case 1. The model post-trained using GRPO-based RLlacks commonsense reasoning—it fails to recognize that the tap must be turned on before washing the mango.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2503.24376/x9.png",
                "caption": "Figure 8:Failure Case 2. The GRPO-based RL post-trained modeloverlooks key visual cues—it fails to detect the removal of leek ends due to the limited frame sampling rate and low frame resolution. Additionally, the final answer exhibits semantic inconsistency with the reasoning steps.",
                "position": 484
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]