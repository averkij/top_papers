[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x1.png",
                "caption": "(a)",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2410.02052/x1.png",
                "caption": "(a)",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2410.02052/x2.png",
                "caption": "(b)",
                "position": 194
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x3.png",
                "caption": "Figure 2:Overview of anR-MCTSAgent. We omit value function reflection for brevity.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2410.02052/x4.png",
                "caption": "Figure 3:Given a trajectory fromR-MCTS,Imitation Learningremoves intermediate search trees and directly trains GPT-4o to learn from the final executed actions;Exploratory Learningflattens tree traversals into a single trajectory and trains GPT-4o to explore, evaluate, and backtrack.",
                "position": 488
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x5.png",
                "caption": "Figure 4:After Exploratory Learning, GPT-4o demonstrates the ability to explore, evaluate, and backtrack without augmenting with search algorithms.",
                "position": 763
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x6.png",
                "caption": "Figure 5:R-MCTSMADMAD{}_{\\texttt{MAD}}start_FLOATSUBSCRIPT MAD end_FLOATSUBSCRIPTerror analysis. Sub-error types are omitted for brevity (seeSectionC.1).",
                "position": 928
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x7.png",
                "caption": "Figure 6:In-context reflection-improvement loop for anR-MCTS agent. For brevity, we only present policy reflection, as value reflection is similar.Left: after a complete episode, the agent 1) uses search tree statistics to select the most erroneous actiona~tsubscript~ùëéùë°\\tilde{a}_{t}over~ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfromœÑùúè\\tauitalic_œÑto reflect on; 2) prompts the VLM to generate a reflection by contrasting what itexpectsto achieveo^t+1subscript^ùëúùë°1\\hat{o}_{t+1}over^ start_ARG italic_o end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTand whatactuallyhappensot+1subscriptùëúùë°1o_{t+1}italic_o start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT; 3) embeds the reflection in a vector database.Right: to generate an action in a new task, the agent 1) retrievesmùëömitalic_mmost relevant reflections from the database; 2) improves its policy (and value function) using in-context learning.",
                "position": 2122
            }
        ]
    },
    {
        "header": "Appendix BAdditional Details about VWA",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x8.png",
                "caption": "Figure 7:Error analysis forR-MCTSMADMAD{}_{\\texttt{MAD}}start_FLOATSUBSCRIPT MAD end_FLOATSUBSCRIPTafter manually inspecting the search trees for 80 randomly sampled tasks from the Classfieds environment.",
                "position": 2255
            }
        ]
    },
    {
        "header": "Appendix CAdditional Analysis Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02052/x9.png",
                "caption": "Figure 8:Example Set-of-Mark augmented webpage used as inputs for VLM agents.",
                "position": 2305
            }
        ]
    },
    {
        "header": "Appendix DAdditional Prompting Details",
        "images": []
    }
]