[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13984/x1.png",
                "caption": "Figure 1:Given a single input image, our framework separates content and style, enabling flexible recontextualization and stylization to generate new images across diverse contexts.",
                "position": 76
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13984/x2.png",
                "caption": "Figure 2:Overview of our CSD-VAR.During optimization (left), a content-style promptùê≤ùê≤\\mathbf{y}bold_y,\"A photo of a <ycsubscriptùë¶ùëêy_{c}italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT> object in <yssubscriptùë¶ùë†y_{s}italic_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT> style\", is encoded into text embeddingsùêûùêû\\mathbf{e}bold_e. The rectified style embeddingessubscriptùëíùë†e_{s}italic_e start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPTreduces content leakage, while ground-truth scale-wise tokens from VQ-VAE are interpolated for next-scale prediction. Augmented K-V memories are prepended at specific scales before feeding into the autoregressive transformer. The model is trained with scale-wise cross-entropy losses, alternating optimization of content and style embeddings.\nAt inference (right), style or content K-V memories are prepended based on the prompt before predicting tokens.",
                "position": 151
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13984/x3.png",
                "caption": "Figure 3:Analysis of style-related scores across different scales.",
                "position": 203
            }
        ]
    },
    {
        "header": "4Our Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13984/x4.png",
                "caption": "Figure 4:Style embedding rectification and examples",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x5.png",
                "caption": "(a)Distribution of content and style categories in CSD-100.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x5.png",
                "caption": "(a)Distribution of content and style categories in CSD-100.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x6.png",
                "caption": "(b)Samples from our CSD-100 dataset.",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x7.png",
                "caption": "Figure 6:Qualitative comparison with prior approaches on our CSD-100 dataset.",
                "position": 383
            }
        ]
    },
    {
        "header": "5Our CSD-100 Dataset",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13984/x8.png",
                "caption": "Figure 7:User preference study on key criteria for content-style decomposition with 100 participants.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x9.png",
                "caption": "Figure 8:Illustrative effectiveness of each component.",
                "position": 615
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.13984/x10.png",
                "caption": "Figure 9:List of 50 prompts used in our experiments for evaluating content-style decomposition.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x11.png",
                "caption": "Figure 10:Our user study interface",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2507.13984/extracted/6633851/figures/csd100_full.jpeg",
                "caption": "Figure 11:Visualization of the full CSD-100 dataset, showcasing its diverse content and style pairings",
                "position": 1638
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x12.png",
                "caption": "Figure 12:Overview of the validation dataset, consisting of 35 curated concepts sourced from existing personalization benchmarks.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2507.13984/extracted/6633851/figures/4_dataset_analysis.png",
                "caption": "Figure 13:Distribution of content and style in the CSD-100 dataset",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2507.13984/x13.png",
                "caption": "Figure 14:Additional qualitative results from our CSD-VAR",
                "position": 1659
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]