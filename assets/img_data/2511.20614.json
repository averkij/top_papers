[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20614/x1.png",
                "caption": "Figure 1:Visual illustrations. (a) illustrates that we first conduct customized generation using GPT-4o[gpt], and then apply different methods for the post editing. Edit method[qwenimage]struggle to achieve fine-grained consistent generation, while images processed by super-resolution methods[guo2024refir]often exhibit noticeable detail inaccuracies. In contrast, our proposed ImageCritic corrects local details to ensure text and logo consistency while maintaining accurate spatial alignment, significantly improving the overall coherence of generated images. (b) We further apply our method to customized results generated by both state-of-the-art closed-source[nanobanana]and open-source models[qwenimage,uno]. After performing our correction, the fine-grained details of the generated images align precisely with those of the original objects, demonstrating the superior performance of our approach.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x2.png",
                "caption": "Figure 2:Data curation pipeline.(a) illustrates the complete pipeline of our approach, which involves generating customized images using existing state-of-the-art models, applying VLM-based filtering, and performing degradation. (b) shows local regions from our dataset, where the target patch aligns well with the input patch, and the degraded patch effectively simulates fine-grained inconsistencies in text and logo areas commonly seen between the input patch and the generated patch.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Data Curation",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20614/x3.png",
                "caption": "Figure 3:Overview of the proposed ImageCritic. We propose ImageCritic, which employs a Detail Encoder and an Attention Alignment Loss to enable the model to localize regions requiring restoration, thereby achieving high-quality and consistent image correcting. Furthermore, we develop a fully automated agent framework that supports both local patch restoration and multi-round correcting processes.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x4.png",
                "caption": "Figure 4:Attention visualization.We separately extract the noise attention maps with respect to the reference image and the input image to be corrected, denoted asMRM_{R}andMIM_{I}, respectively. The first row shows the results of the LoRA-finetuned base model, while the second row presents the results after applying the attention alignment loss. The first two columns correspond to the attention map of the double stream layer, and the last two columns correspond to the single stream layer. It can be observed that the attention alignment loss effectively promotes attention disentanglement.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x5.png",
                "caption": "Figure 5:Effect of the detail encoder.We find that when the input image exhibits structural differences from the reference image, the model fails to correctly identify the intended reference object, leading to inconsistent generation results.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x6.png",
                "caption": "Figure 6:Visual Results.We present the results obtained by applying our method to the generated outputs of existing state-of-the-art customized generation models, including both open-source and closed-source variants. As shown, our method substantially enhances the consistency of the generated images.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x7.png",
                "caption": "Figure 7:Robust Generalization.As shown in the clock’s numerals and hands and the robot’s panels, our proposed ImageCritic effectively preserves style and illumination consistency rather than relying on naive copy-paste patterns, achieving detailed and coherent corrections across varying viewpoints, cross-category objects, and diverse stylization settings.",
                "position": 387
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Appendix AData Curation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20614/x8.png",
                "caption": "Figure 8:Data curation details.We present a comprehensive account of our dataset construction methodology, encompassing data generation, data annotation, and data augmentation procedures.",
                "position": 840
            }
        ]
    },
    {
        "header": "Appendix BImage Correction Comparison",
        "images": []
    },
    {
        "header": "Appendix CDataset Comparison",
        "images": []
    },
    {
        "header": "Appendix DAdditional Visual Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20614/x9.png",
                "caption": "Figure 9:Comparisons with multimodal and editing model. We evaluated existing multimodal and editing methods. As shown, current models exhibit issues in preserving global coherence or performing localized corrections, which affects their practical applicability. In contrast, our approach maintains local consistency in the generated content while ensuring overall background coherence.",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x10.png",
                "caption": "Figure 10:Comparion with dataset.Comparison of data samples from our Critic-10k dataset, the Subjects-200K[ominicontrol], and the UNO-1M[uno]dataset.",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x11.png",
                "caption": "Figure 11:Additional visual result.We present the visual results of our proposed ImageCritic under multilingual, multi-view, and multi-scene settings. By applying an OCR[ocr]model to the generated images, we observe that after correction with our method, all recognized text perfectly matches the reference images. This demonstrates that our approach achieves precise and comprehensive detail correction without disrupting the overall structural or contextual integrity of the images.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x12.png",
                "caption": "Figure 12:Additional visual result.We present the visual results of our proposed ImageCritic under multilingual, multi-view, and multi-scene settings. By applying an OCR[ocr]model to the generated images, we observe that after correction with our method, all recognized text perfectly matches the reference images. This demonstrates that our approach achieves precise and comprehensive detail correction without disrupting the overall structural or contextual integrity of the images.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x13.png",
                "caption": "Figure 13:Additional visual result.We present the visual results of our proposed ImageCritic under multilingual, multi-view, and multi-scene settings. By applying an OCR[ocr]model to the generated images, we observe that after correction with our method, all recognized text perfectly matches the reference images. This demonstrates that our approach achieves precise and comprehensive detail correction without disrupting the overall structural or contextual integrity of the images.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2511.20614/x14.png",
                "caption": "Figure 14:Illustration of a multi-agent image correcting workflow.The system performs localized detection, reference matching, and iterative region-level corrections driven by user feedback, progressively correcting the generated image until it is accepted.",
                "position": 968
            }
        ]
    },
    {
        "header": "Appendix EAgent Chain Details",
        "images": []
    }
]