[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17259/assets/logofrappe.png",
                "caption": "",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2602.17259/x1.png",
                "caption": "",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2602.17259/x2.png",
                "caption": "",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2602.17259/x3.png",
                "caption": "",
                "position": 156
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17259/x4.png",
                "caption": "Figure 1:We demonstrate thatFRAPPEsignificantly outperforms the state-of-the-art models in both simulated and real-world complex scenarios, and it can effectively leverage data from different levels of the training data pyramid.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2602.17259/x5.png",
                "caption": "Figure 2:Overview of training and inference.During the training phase, the model progressively learns to align with the representation spaces of multiple visual foundation models simultaneously. The model is trained through a two-stage training process to extends to parallel processing of multiple input streams while aligning diverse visual representations. Similarly, parallel inference is implemented during the inference stage.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Simulation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17259/figure/allTolora.png",
                "caption": "Figure 3:Experiments on different parameter scales on 4 tasks on the RoboTwin 2.0.The 130M backbone model fine-tuned using either LoRA or full-parameter fine-tuning under theFRAPPEconsistently outperforms the naively fine-tuned RDT-130M across all tasks and remains competitive when compared with the naively fine-tuned RDT-1B. Especially in theStack Bowls Two-Hardtask, the improvement is significant.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2602.17259/x6.png",
                "caption": "Figure 4:Real-world experiment results in seen and unseen scenarios.We evaluate ourFRAPPEand prior SOTA VLAs on 4 representative tasks, each with different axes of generalization. Seen means the settings were included in the training data, while Unseen refers to new task settings that the model did not encounter during training.",
                "position": 842
            }
        ]
    },
    {
        "header": "5Real-world Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17259/x7.png",
                "caption": "Figure 5:Long-horizon Performance.Each data point represents the success rate of completing up to and including that corresponding subtask.",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2602.17259/x8.png",
                "caption": "Figure 6:Leveraging human egocentric data without action labels.(a).Experimental targets, including 5 objects that are easy to pick up, and 5 objects that are difficult to pick up.(b).Multi-task results ofFRAPPEtrained onRobot (task)and+ Ego (web)across 8 target object categories. Here, “multi-task” denotes that one model is used to be trained and tested on multiple target objects.(c).The results ofFRAPPEtrained onRobot (task),+ Ego (web)and+ Ego (task) + Ego (web)data mixture, respecitvely.",
                "position": 931
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Hyperparameter Settings",
        "images": []
    },
    {
        "header": "8Human Egocentric Co-Training Details",
        "images": []
    }
]