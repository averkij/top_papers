[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Experiment Setup",
        "images": []
    },
    {
        "header": "4Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.11343/x1.png",
                "caption": "Figure 1:The learning dynamics of RAFT and RAFT++, initialized from Qwen2.5-Math-7B-base (left) and LLaMA-3.2-3B-instruct (right). The y-axis is the average@16 accuracy, that is further averaged on MATH500, Minerva Math, and Olympiad Bench. We also plot the best model of GRPO, PPO, and Iterative DPO for reference.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x2.png",
                "caption": "",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x3.png",
                "caption": "Figure 2:Left: the training reward curves of RAFT, RAFT++, RAFT++ without clipping (i.e., RAFT and importance sampling), and GRPO, initialized from Qwen2.5-Math-7B-base. Right: the training reward curves of RAFT++ and RAFT++ enhanced by clip higher trick, initialized from LLaMA-3.2-3B-instruct. We transform the original reward using(1+r)/21ùëü2(1+r)/2( 1 + italic_r ) / 2so that the resulting value corresponds to the accuracy on the training data. We also apply a moving average with a window size of20202020to smooth the curves.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x4.png",
                "caption": "",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x5.png",
                "caption": "Figure 3:The learning dynamics of RAFT++ and GRPO, initialized from Qwen2.5-Math-7B-base (first row) and LLaMA-3.2-3B-instruct (second row). We also plot the KL loss in the left column and the policy entropy loss in the right column.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x6.png",
                "caption": "",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x7.png",
                "caption": "",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x8.png",
                "caption": "",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x9.png",
                "caption": "Figure 4:Ablation study on the components of GRPO and Reinforce-type algorithms with LLaMA-3.2-3B-instruct. We compare GRPO with other Reinforce-based variants to isolate the effects of removing incorrect samples, correct samples, and applying normalization. Removing incorrect samples (‚ÄúRemove all wrong‚Äù) provides the largest gain in reward, highlighting their harmful impact. In contrast, the reward of removing correct samples is still not satisfactory. Mean-zero normalization increases KL loss and destabilizes training. Normalizing by standard deviation shows minimal additional benefit. The variant ‚ÄúReinforce + Remove both‚Äù achieves a good balance between reward, KL stability, and entropy regularization. We transform the original reward using(1+r)/21ùëü2(1+r)/2( 1 + italic_r ) / 2so that the resulting value corresponds to the accuracy on the training data. We also apply a moving average with a window size of20202020to smooth the curves.",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x10.png",
                "caption": "",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x11.png",
                "caption": "",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x12.png",
                "caption": "",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x13.png",
                "caption": "",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2504.11343/x14.png",
                "caption": "",
                "position": 598
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]