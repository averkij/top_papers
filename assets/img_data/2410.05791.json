[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05791/x1.png",
                "caption": "Figure 1.Our paper (a) collects the first large-scale 3D hand motion dataset of piano playing, accompanied by synchronized audio and key pressing events; (b) proposes a method that can control a physically simulated hand to play novel pieces ‘unheard’ from the training set.",
                "position": 171
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05791/x2.png",
                "caption": "Figure 2.Overview of our pipeline to reconstruct motion data from multi-view videos. We (a) shoot 4K videos from 5 different views at 59.94 FPS using RGB camera; (b) detect 2D keypoints of the hands from each view; (c) triangulate the 2D keypoints into 3D hand skeletons with calibrated camera intrinsics and extrinsics; (d) fit the skeleton onto MANO hand meshes(Romero et al.,2017); and (e) run IK with ground-truth MIDI as end effector goals to refine the finger placements for correct key pressing.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2410.05791/extracted/5909197/figs/setup_blurred.png",
                "caption": "Figure 3.Data capture setup. Five GoPro cameras are placed around the piano to provide multi-view recordings of elite pianists’ performances.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2410.05791/x3.png",
                "caption": "Figure 4.Examples of some piano skills in our dataset, including scales, octaves, and arpeggio. The trajectory of each fingertip is visualized. The green keys show the pressed keys through the trajectory.",
                "position": 261
            }
        ]
    },
    {
        "header": "3.Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05791/x4.png",
                "caption": "Figure 5.Overview of our method to physically simulate piano performance from a given sheet music. We use MIDI to retrieve motion data from the collected motion dataset and as input to a diffusion model for generating piano performance motions. These two sets of motions are combined into a reference motion ensemble. Utilizing the reference motions, we then employ two discriminator ensembles and three critics, which consider imitation and goal rewards, respectively, to train a control policy via reinforcement learning.",
                "position": 338
            }
        ]
    },
    {
        "header": "4.Play Piano with Physically Simulated Hands",
        "images": []
    },
    {
        "header": "5.Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05791/x5.png",
                "caption": "Figure 6.F1 scores of the diffusion model and our policy on the 14 test pieces. RL policies have a significant improvement over diffusion-generated motions across all 14 pieces.",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2410.05791/x6.png",
                "caption": "Figure 7.Results of our model accompanied with generated diffusion motions. The trajectory of each fingertip is visualized. The simulated hands can correctly press all the target keys while tying to follow the diffusion-generated motion. The policy can handle chords (abc), double notes (d), and large wrist movements (ef) naturally and accurately.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2410.05791/x7.png",
                "caption": "Figure 8.Comparison between motions generated by the diffusion model and our full model with reinforcement learning. Our full model fixes the imprecise key-pressing issue of the diffusion model. In the left demo, by imitating retrieved motions, the control policy learns to use the ring finger to press two keys at the same. This pose is not provided by the diffusion-generated motions.",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2410.05791/x8.png",
                "caption": "Figure 9.Learning performance of our full model and the ablative models. In all the tested cases, our model shows better performance compared to the ablative models.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2410.05791/x9.png",
                "caption": "Figure 10.Comparisons between our full model and the ablative models. Without diffusion guidance, the ablative modelsRL,RL+Retr, andRL+Wholehave excessive or unnatural movements because no motions corresponding to the given piece are provided during training. Without retrieved motions from the dataset (RL+Diff), the model tends to overfit the imprecise motions generated by the diffusion models, resulting in lower accuracy.",
                "position": 630
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Capture Details",
        "images": []
    },
    {
        "header": "Appendix BHyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05791/",
                "caption": "Figure S1.Comparison between\nthe synthesized motions and motions in our dataset when facing the same target notes. Our control policy could take diverse key-pressing poses by leveraging motions synthesized by the diffusion models (a), or imitating poses existing in our dataset with the similar (b) or different (c) fingering strategy.",
                "position": 1664
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    },
    {
        "header": "Appendix DRepertoire",
        "images": []
    }
]