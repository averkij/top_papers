[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06273/x1.png",
                "caption": "Figure 1:Illustration of (a) Audio-Visual Speech Romanizer (AV-Romanizer): It is pre-trained to learn language-agnostic representations by predicting romanized text through CTC loss. (b) Cascaded Zero-shot AVSR: By providing instructions along with the predicted Roman text from the proposed AV-Romanizer, diverse LLMs can be employed to predict graphemes from the predicted Roman text. Please note that, as the AV-Romanizer has learned to generate pronunciation information (Roman text) from the input speech, it can convert even unseen languages into Roman text. Since LLMs already contain information about the target language (key assumption of this paper), they can then convert this Roman text into the target language.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2503.06273/x2.png",
                "caption": "Figure 2:Illustration of the proposed multi-tasks to train Zero-AVSR. (a) Task 1: Alignment between the AV Romanizer and a LLM. By using the paired audio-visual speech inputs and ground truth roman text, we align the audio-visual speech representations into LLM space (i.e., text). (b) Task 2: Learning to de-romanize. By leveraging abundant text-only data, we fine-tune the LLM to de-romanize input roman text into language-specific graphemes. This approach enables us to cover a wider range of languages, as text data is more readily available than audio-visual speech data for diverse languages.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06273/x3.png",
                "caption": "Table 6:Ablation study on the impact of using data from the same language family (Romance) on zero-shot speech recognition.",
                "position": 1119
            },
            {
                "img": "https://arxiv.org/html/2503.06273/x3.png",
                "caption": "Figure 3:The performances of Cascaded Zero-AVSR and Zero-AVSR using audio-only (A) and audio-visual (AV) inputs under different SNR noise levels.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2503.06273/x4.png",
                "caption": "Figure 4:Examples of prediction results from the Cascaded Zero-AVSR on an unseen language, Japanese, on out-of-domain data.",
                "position": 1472
            }
        ]
    },
    {
        "header": "6Detailed Information of MARC",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]