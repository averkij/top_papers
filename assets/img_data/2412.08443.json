[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x1.png",
                "caption": "Figure 1:Performance of Open-Source Models on the OpenCompass Leaderboard(Contributors,2023).POINTS1.5 ranks first among all models under 10B in size, even outperforming models several times larger. The size of each bubble represents the model size.",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x2.png",
                "caption": "Figure 2:POINTS1.5 shows great potential to solve challenging real world problems.",
                "position": 95
            }
        ]
    },
    {
        "header": "2Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x3.png",
                "caption": "Figure 3:POINTS1.5 uses the converntional LLaVA-style architecture, consisting of a vision encoder, a MLP projector and a LLM.",
                "position": 126
            }
        ]
    },
    {
        "header": "3Bilingual Support",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x4.png",
                "caption": "Figure 4:The chat template during pre-training in POINTS1.0 (above) and POINTS1.5 (below)",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x5.png",
                "caption": "Figure 5:Prompts used in the chat template during pre-training stage.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x6.png",
                "caption": "Figure 6:Prompts to create the Chinese OCR datasets.",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x7.png",
                "caption": "Figure 7:Distribution of visual instruction tuning data in POINTS1.5.The left figure shows the distribution across different categories, and the right figure shows the distribution between English and Chinese.",
                "position": 253
            }
        ]
    },
    {
        "header": "4Visual Instruction Tuning Set Filtering",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x8.png",
                "caption": "Figure 8:Procedure to filter out samples containing grammatical errors (a) and distribution between grammatically correct samples and samples containing grammatical errors (b).",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x9.png",
                "caption": "Figure 9:Questions can be answered without referring to the image.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x10.png",
                "caption": "Figure 10:Data samples containing grammatical errors (marked withred) in visual instruction tuning set.",
                "position": 284
            }
        ]
    },
    {
        "header": "5Training and Model Strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x11.png",
                "caption": "Figure 11:Unfreezing the vision encoder during pre-training degrades the performance.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x12.png",
                "caption": "Figure 12:We envision that extending a large language model with additional modalities using LLaVA-style architecture should follow the three-stage procedure illustrated in this figure. The three icons on the left denote the status of each module during the three stages. From left to right, they are: stage-1, stage-2, stage-3.",
                "position": 361
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08443/x13.png",
                "caption": "Figure 13:OCR and reasoning ability.",
                "position": 1423
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x14.png",
                "caption": "Figure 14:Complex Chinese OCR",
                "position": 1426
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x15.png",
                "caption": "Figure 15:Complex OCR",
                "position": 1429
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x16.png",
                "caption": "Figure 16:Summarize key points from an image.",
                "position": 1432
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x17.png",
                "caption": "Figure 17:Latex formula extraction",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x18.png",
                "caption": "Figure 18:Mathematical problem solving",
                "position": 1438
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x19.png",
                "caption": "Figure 19:Image translation",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x20.png",
                "caption": "Figure 20:Object identification.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2412.08443/x21.png",
                "caption": "Figure 21:Key information extraction and reasoning.",
                "position": 1447
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]