[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06673/x1.png",
                "caption": "Figure 1:(Left) A typical visual latent generative framework that incorporates external semantics. (Top Right) Leading autoregressive models exhibit steep performance drops when CFG is disabled (VAR results fromChen et al. [5]). (Bottom Right) Generation quality degrades when the external SSL model DINO is removed (CFG is disabled).",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2510.06673/x2.png",
                "caption": "Figure 2:Illustration of Heptapod’s next 2D distribution prediction framework. The model operates on a sequence of visual tokens from a simple reconstruction-focused tokenizer. The Transformer autoregressively predicts the distributions over remaining positions in the 2D grid in parallel for every input tokens. The loss is then computed across all these future positions, treating the prefix as the visible context (like MAE’s unmasked patches) and the remaining grid as targets. This forces the model to develop a holistic representation, bridging the gap between 1D language modeling and 2D spatial understanding.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3On the Nature of Visual Tokens and Their Predictive Modeling",
        "images": []
    },
    {
        "header": "4Next 2D Distribution Prediction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06673/x3.png",
                "caption": "Figure 3:(Left)Spatial correlations in VQ-VAE vs. semantic tokenizers. For three reference tokens (87th, 138th and 203rd, bounded by red lines), we compute cosine similarity to all other tokens in the grid.(Right)Attention maps of the final layer in autoregressive Transformer trained with each tokenizer. Under VQ-VAE, attention concentrates on spatial neighbors (local interpolation), while semantic tokens yield attention on spatially distant yet semantically related regions (long-range dependencies). Following DiGIT[60], semantic tokens are obtained by K-Means on DINO hidden states. Additional examples are provided in Appendix9.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2510.06673/x4.png",
                "caption": "Figure 4:(Left)The text language modeling with next 1D distribution prediction.(Middle)Vanilla image language modeling with next 1D distribution prediction. The 2D sptial postion is left-shifted in the input to specify the next target.(Right)Our next 2D distribution prediction. The 2D spatial positions are not shifted. The model must be prepared to predict any future position.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2510.06673/x5.png",
                "caption": "Figure 5:Architectural variants of the 2D prediction head.(Left)The global prediction head employs a bidirectional transformer to model full-image spatial dependencies.(Right)The local prediction head stacks cross attention and bidirectional attention to capture 2D spatial dependencies within a local chunk.",
                "position": 240
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06673/x6.png",
                "caption": "Figure 6:Training convergence for Heptapod-L with a discrete VQ tokenizer vs. a continuous VAE tokenizer. The VQ-based model converges faster initially, while the VAE-based model ultimately attains superior generative performance. The dotted line indicates each tokenizer’s rFID.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2510.06673/x7.png",
                "caption": "Figure 7:Effect of window size and supervision density on generation performance for Heptapod-B.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2510.06673/x8.png",
                "caption": "Figure 8:Effect of supervision density under a global prediction window (w=256w=256). Increasing the number of tokens per sequence for loss computation (densitynn) consistently improves FID and IS. Results are obtained with Heptapod-B and CFG is disabled.",
                "position": 586
            }
        ]
    },
    {
        "header": "6Discussion and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06673/figures/grid.png",
                "caption": "Figure 9:Example results generated by Heptapod-H.",
                "position": 1561
            }
        ]
    },
    {
        "header": "8Model Configuration",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06673/figures/similarity_grid_digit.png",
                "caption": "Figure 10:Visualization of semantic tokens with cosine similarity.",
                "position": 1655
            },
            {
                "img": "https://arxiv.org/html/2510.06673/figures/similarity_grid_vq.png",
                "caption": "Figure 11:Visualization of VQ-VAE tokens with cosine similarity.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2510.06673/figures/stitched_attention_digit.png",
                "caption": "Figure 12:Visualization of attention maps with semantic tokenizer.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2510.06673/figures/stitched_attention_vq.png",
                "caption": "Figure 13:Visualization of attention maps with VQ-VAE tokenizer.",
                "position": 1664
            }
        ]
    },
    {
        "header": "9Visualization of Tokenizer and Attention",
        "images": []
    }
]