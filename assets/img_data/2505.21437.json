[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21437/x1.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21437/x2.png",
                "caption": "Figure 2:Pipeline overview.(a) Given the initial human pose, object pose, and text, we first generate the articulated object trajectory and the corresponding end-effector trajectories via two conditional diffusion models.\n(b) We then optimize the latent noise inputs of three decoupled diffusion models by propagating gradients through the kinematic chain, guided by end-effector tracking, penetration, and regularization losses.\nFinally, we forward the optimized noise through the diffusion models to synthesize coherent whole-body motion aligned with the generated object motion.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2505.21437/x3.png",
                "caption": "Figure 3:The illustration of the end-effector BPS.(a) is the object BPS[132].\n(b) is the proposed end-effector BPS representation.\nGray points denote the basis points; pink/yellow are two object parts; blue indicates a fingertip.\nOnly one end-effector and64646464basis points are visualized for simplicity.",
                "position": 289
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21437/x4.png",
                "caption": "Figure 4:Qualitative comparison.Given the text “A person uses the ketchup.”, our method generates the whole-body motion with better hand-object contact compared to baselines.",
                "position": 779
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AIntroduction",
        "images": []
    },
    {
        "header": "Appendix BAdditional Related Work",
        "images": []
    },
    {
        "header": "Appendix CMethod Details",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details",
        "images": []
    },
    {
        "header": "Appendix EMore Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21437/x5.png",
                "caption": "Figure 5:Generalization to different object geometry.We train the object motion and end-effector trajectory models on hand-only data[138]with different object geometries.\nThese models are integrated into our framework to provide optimization targets, enabling realistic whole-body motion synthesis for unseen object geometry.",
                "position": 3072
            },
            {
                "img": "https://arxiv.org/html/2505.21437/x6.png",
                "caption": "Figure 6:Object motion control.Our method could generate coherent whole-body motion with the object motion keyframe.\nThe blue object indicates the object motion keyframe.",
                "position": 3088
            },
            {
                "img": "https://arxiv.org/html/2505.21437/x7.png",
                "caption": "Figure 7:Simultaneous locomotion and manipulation.Our method enables the human to manipulate objects while simultaneously a) walking forward, b) walking backward, c) walking to the right, and d) walking to the left.\nThe transparency of the meshes indicates time progression, where more transparent frames correspond to earlier frames.",
                "position": 3102
            },
            {
                "img": "https://arxiv.org/html/2505.21437/x8.png",
                "caption": "Figure 8:Deployment on simulated humanoids.We apply existing motion tracking techniques to deploy the generated motion to a simulated humanoid.\nThe articulated object is physically manipulated by the humanoid within the physics simulator[64].",
                "position": 3119
            },
            {
                "img": "https://arxiv.org/html/2505.21437/x9.png",
                "caption": "Figure 9:Generating whole-body motion from hand-only dataset.We use the fingertip and object trajectories from the dataset and assign them as the optimization targets.\nAfter the optimization, we could get the whole-body motion.",
                "position": 3134
            }
        ]
    },
    {
        "header": "Appendix FMore Discussions",
        "images": []
    }
]