[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06767/x1.jpg",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x2.jpg",
                "caption": "",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x3.jpg",
                "caption": "",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x4.jpg",
                "caption": "",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/rendering/gundam.jpg",
                "caption": "",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh/gundam_5.jpg",
                "caption": "",
                "position": 142
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4MAtCha Gaussians",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/rgb2.jpg",
                "caption": "Figure 2:Overview of MAtCha Gaussians.Given a few RGB images and their camera poses obtained using a sparse-view SfM method such as MASt3R-SfM[15], we first initialize charts using a pretrained monocular depth estimation model. Each chart is represented as a mesh equipped with a UV map, mapping a 2D plane to the 3D surface.\nWe then optimize our charts and enforce their alignment with input SfM data using two key components: (1) 1D depth encodings for quickly aligning the initial depth maps together, and (2) charts encodings for efficiently deforming the geometry while preserving surface details.\nOur aligned charts provide a sharp, dense and accurate estimate of the 3D scene, which can be further refined using input images and a Gaussian Splatting-based rendering pipeline. Our representation allows for reconstructing high-quality surface meshes within minutes, even in sparse-view scenarios.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/rgb1.jpg",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/rgb0.jpg",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/depth_2.jpg",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/depth_1.jpg",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/depth_0.jpg",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/initial_charts.jpg",
                "caption": "",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/aligned_rgb.jpg",
                "caption": "",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/refined_normal.jpg",
                "caption": "",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/gaussians.jpg",
                "caption": "",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/charts_encodings.png",
                "caption": "",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/depth_encodings.png",
                "caption": "",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/triangles.png",
                "caption": "",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/pipeline/gaussians_in_triangles.png",
                "caption": "",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x5.jpg",
                "caption": "Figure 3:Reconstruction with different numbers of input views.Our method can produce high-quality renderings (top) and surfaces (bottom) even with very sparse input views (3-10 views). The quality of our meshes is visually pleasing even in extreme sparse scenarios.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh/scan21_1.jpg",
                "caption": "",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x6.jpg",
                "caption": "",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh/scan37_1.jpg",
                "caption": "",
                "position": 728
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x7.jpg",
                "caption": "",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh/barn_5_views.jpg",
                "caption": "",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x8.jpg",
                "caption": "",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh/bonsai_5_views_0.jpg",
                "caption": "",
                "position": 740
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x9.jpg",
                "caption": "",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_extraction/adaptive_tetra.jpg",
                "caption": "",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x10.jpg",
                "caption": "",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh/kitchen_10_views_1.jpg",
                "caption": "",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_extraction/tsdf_multires.jpg",
                "caption": "Figure 4:Comparison between our two different mesh extraction methods: Multi-resolution TSDF fusion (left), and Adaptive tetrahedralization (right).We optimized MAtCha Gaussians representations with only 10 training images.\nContrary to vanilla TSDF fusion, our multi-resolution TSDF can reconstruct both foreground and background objects with a decent number of vertices. However, similarly to vanilla TSDF fusion, it produces eroded meshes with holes in the surface, as well as “disk-aliasing” artifacts. On the contrary, our adaptive tetrahedralization inspired by GOF[56]is able to reconstruct accurate and complete surfaces meshes (see top right image), with sharp and fine details (see bottom right image).",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_extraction/tsdf_multires2.jpg",
                "caption": "",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_extraction/adaptive_tetra2.jpg",
                "caption": "",
                "position": 793
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/Spurfies/spurfies.jpg",
                "caption": "Figure 5:Comparisons with Spurfies[39]and MVSplat[11]on an unbounded scene.Our method outperforms state-of-the-art approaches for surface reconstruction and feed-forward Gaussian splatting regression in sparse view scenarios.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/rendering/garden_5_views_1.jpg",
                "caption": "",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/MVSplat/garden_views5.jpg",
                "caption": "",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x11.jpg",
                "caption": "",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_comparison/caterpillar_2dgs.jpg",
                "caption": "Figure 6:Qualitative evaluation for surface reconstruction in a sparse-view scenario for unbounded scenes from Tanks&Temples[28], with 5 training images (top row) and 10 training images (bottom row).Contrary to the baselines, our approach is able to reconstruct accurate and complete surfaces meshes: It not only includes both foreground and background objects, but also recover sharper and finer details.",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_comparison/truck_2dgs.jpg",
                "caption": "",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_comparison/caterpillar_gof.jpg",
                "caption": "",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_comparison/truck_gof.jpg",
                "caption": "",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_comparison/caterpillar_ours.jpg",
                "caption": "",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/mesh_comparison/truck_ours.jpg",
                "caption": "",
                "position": 1293
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06767/x12.jpg",
                "caption": "Figure 7:Qualitative reconstruction results across different scenarios and numbers of input views.We show results on both bounded objects from DTU[1](first two rows, 3 views) and unbounded scenes from Tanks&Temples[28]and Mip-NeRF 360[3](middle and bottom rows). For each example, we show (from left to right): the rendered novel view, estimated depth map, surface normals, and the extracted mesh. For bounded objects (DTU), meshes are extracted directly from our manifold representation, while for unbounded scenes, we first refine free Gaussians around the manifold before mesh extraction. Note how our method maintains consistent quality across different scenarios, from small objects to large-scale scenes with complex backgrounds.",
                "position": 2406
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x13.jpg",
                "caption": "",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x14.jpg",
                "caption": "",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x15.jpg",
                "caption": "",
                "position": 2440
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x16.jpg",
                "caption": "",
                "position": 2441
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x17.jpg",
                "caption": "",
                "position": 2442
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x18.jpg",
                "caption": "",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x19.jpg",
                "caption": "",
                "position": 2449
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x20.jpg",
                "caption": "",
                "position": 2450
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x21.jpg",
                "caption": "",
                "position": 2451
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x22.jpg",
                "caption": "",
                "position": 2452
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x23.jpg",
                "caption": "",
                "position": 2453
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x24.jpg",
                "caption": "",
                "position": 2454
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x25.jpg",
                "caption": "",
                "position": 2455
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x26.jpg",
                "caption": "",
                "position": 2461
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x27.jpg",
                "caption": "",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x28.jpg",
                "caption": "",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x29.jpg",
                "caption": "",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x30.jpg",
                "caption": "",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x31.jpg",
                "caption": "",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x32.jpg",
                "caption": "",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x33.jpg",
                "caption": "",
                "position": 2473
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x34.jpg",
                "caption": "",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x35.jpg",
                "caption": "",
                "position": 2475
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x36.jpg",
                "caption": "",
                "position": 2476
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x37.jpg",
                "caption": "",
                "position": 2477
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x38.jpg",
                "caption": "",
                "position": 2478
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x39.jpg",
                "caption": "",
                "position": 2479
            },
            {
                "img": "https://arxiv.org/html/2412.06767/extracted/6051419/images/MVSplat360/garden_views5.jpg",
                "caption": "Figure 8:Qualitative comparisons of novel view synthesis with MVSplat360[12]on an unbounded scene with 5 training images.Our method can render more photorealistic images than the concurrent feed-forward novel view synthesis method in sparse view scenarios.",
                "position": 2493
            },
            {
                "img": "https://arxiv.org/html/2412.06767/x40.jpg",
                "caption": "",
                "position": 2503
            }
        ]
    },
    {
        "header": "8Additional Results and Details",
        "images": []
    }
]