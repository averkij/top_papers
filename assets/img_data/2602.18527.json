[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18527/x1.png",
                "caption": "Figure 1:Overview of theSpatialSceneQA61k dataset..Left:Example question-answer pairs demonstrating diverse spatial tasks, including sound source localization (azimuth/elevation), visual grounding (bounding boxes), and overlapping sound source identification.Right:The data synthesis pipeline leveragingHabitat-SimandSoundSpaces 2.0. The process consists of four stages: (1) selecting an HM3D scene, (2) sampling random source and receiver poses, (3) inserting 3D visual sound sources (e.g., speakers generated by Hunyuan3D-1.0), and (4) exporting synchronized RGB-D frames, FOA audio, and semantic and camera intrinsic/extrinsic metadata.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SpatialSceneQA",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18527/x2.png",
                "caption": "Figure 2:Comparisons between Classical IV and Neural IV.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2602.18527/x3.png",
                "caption": "Figure 3:Overview of the JAEGER Architecture.The framework processes RGB-D and FOA inputs.(1) Visual Stream:RGB features are fused with 3D-aware positional encodings derived from depth-projected Point Clouds.(2) Audio Stream:Semantic features are extracted from the omnidirectional channel (FOA W). For spatial cues, we compare Classical IV with Neural IV (N. IV). Specifically, IV derives features via STFT followed by fetching real parts and normalization, whereas Neural IV extracts geometric features from raw waveforms using channel-wise 1D-CNNs followed by an MLP.",
                "position": 437
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADiversity Analysis of Speaker Point Clouds",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18527/appendix_speaker_grid.png",
                "caption": "Figure 4:Visualization of the diversity in generated speaker point clouds. We display 32 randomly selected samples from the 120 generated instances. Despite using the same text prompt, varying the random seed results in distinct structural and morphological variations.",
                "position": 1233
            }
        ]
    },
    {
        "header": "Appendix BCoordinate System Definition",
        "images": []
    }
]