[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_1/fig1_celeba_fashion-row.png",
                "caption": "Figure 1:Generation with our distilled orderon the Fashion Product dataset(Left)and the Multimodal CelebA-HQ dataset(Right)with the corresponding generation order produced by our Ordered Autoregressive (OAR) model. The generation order is visualized through color intensity, progressing from yellow (early patches) to violet (later patches). Our learned order typically starts with simpler regions of the image before moving to more complex ones. For the Fashion Product dataset, this often means generating the white background first, while in the CelebA-HQ dataset, the model tends to begin with facial regions like the cheeks and chin, which are generally easier to generate.",
                "position": 117
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Perspective and Motivations",
        "images": []
    },
    {
        "header": "4Our Method: Ordered AR",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17069/x1.png",
                "caption": "Figure 2:Different Autoregressive (AR) models. (Top) A raster scan is the normal approach for autoregressive generation from top left to bottom-right. The input token contains the contentxisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTand the positionlisubscriptùëôùëñl_{i}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. (Middle) Any-given-order learns to generate tokens at any possible location. However, the position of the next token should be given as input in an additional positional embedding. (Bottom) Our method, Ordered Autoregressive, uses the any-given-order model but generates all possible positions and selects the most likely one (darker yellow) as the next generated token.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_samples/merged.png",
                "caption": "Figure 3:Examples of generation on the Fashion Products dataset.(Top)Generated images with raster AR mode.(Middle)Generated images with ordered AR model.(Bottom)Generation order, from yellow to violet. From these images, we see that our approach finds an order highly correlated with the image content, often resulting in better image quality.",
                "position": 383
            }
        ]
    },
    {
        "header": "5Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/celeba.png",
                "caption": "Figure 4:Examples of generation on the CelebA dataset. (Top) Generated images with raster AR mode. (Middle) Generated\nimages with ordered AR model. (Bottom) Generation order, from yellow to violet. On this dataset our model generates first the salient parts of a face, leaving hair and background at the end. Our model produces images with greater smoothness, rich context and more aligned with the text",
                "position": 631
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGeneration Architecture",
        "images": []
    },
    {
        "header": "Appendix BAdditional related work",
        "images": []
    },
    {
        "header": "Appendix CV-information",
        "images": []
    },
    {
        "header": "Appendix DDifferent Generation approaches",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_3/abs.png",
                "caption": "Figure 5:Generation order with absolute and relative positioning encoding.(Top)With absolute encoding the generation is very scattered.(Bottom)With relative positioning the generation is more localized. The average euclidean distance between the subsequently generated patches in case of absolute encoding is 5.78 whereas in case of relative encoding it is 4.34",
                "position": 1724
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_3/abs_arrow.png",
                "caption": "",
                "position": 1729
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_3/rel.png",
                "caption": "",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_3/rel_arrow.png",
                "caption": "",
                "position": 1733
            }
        ]
    },
    {
        "header": "Appendix EKey Value Caching",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/avg.png",
                "caption": "Figure 6:Average distance between generated patches for normal Fashion with white background and our modified version with a noisy background. Due to the different generation orders, the average distance is also different.",
                "position": 1832
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_5/white.png",
                "caption": "Figure 7:Generation order with different backgrounds.(Top)With an easy background, such as uniform white, the model will start generating from the background.(Bottom)With a more difficult background, such as random white noise, the model will start generating from the object.",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_5/white-order.png",
                "caption": "",
                "position": 1845
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_5/noise.png",
                "caption": "",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_5/noise-orde.png",
                "caption": "",
                "position": 1849
            }
        ]
    },
    {
        "header": "Appendix FThe effect of background",
        "images": []
    }
]