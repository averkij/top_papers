[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03590/x1.png",
                "caption": "Figure 1:Pareto frontier showing accuracy versus total API cost (log scale) on the MedQA benchmark (1273 questions total). We compare o1-preview (Sep 2024), GPT-4o (Aug 2024), and GPT-4 Turbo (Nov 2023) with various run-time steering strategies.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2411.03590/x2.png",
                "caption": "(a)",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2411.03590/x2.png",
                "caption": "(a)",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2411.03590/x3.png",
                "caption": "(b)",
                "position": 159
            }
        ]
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03590/x4.png",
                "caption": "Figure 3:Visual illustration of Medprompt components and additive contributions to performance on MedQA. The prompting strategy combineskùëòkitalic_kNN-based few-shot example selection, GPT-4‚Äìgenerated chain-of-thought prompting, and answer-choice shuffled ensembling.Relative contributions of each component are shown at the bottom. Figure from[NLZ+23b]",
                "position": 197
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03590/x5.png",
                "caption": "Figure 4:JMLE-2024: National medical licensing exam held in\nJapan in February 2024",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2411.03590/x6.png",
                "caption": "Figure 5:Comparison of prompting techniques on MedQA with the o1-preview model. Error bars indicate one standard deviation from three independent samples.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2411.03590/extracted/5980332/images/experiment_slope_chart_nomedmcqa.png",
                "caption": "Figure 6:Tests of different prompting strategies across benchmark datasets. Ensembling outputs consistently yielded strong boosts in performance. Writing a custom-tailored prompt that describes the task in greater detail (Figures12,13) had a marginal positive effect. Use of few-shot prompting was generally neutral or negative. Detailed results from all experiments are presented in Section8.1.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2411.03590/extracted/5980332/images/reasoning_token_slope_chart.png",
                "caption": "Figure 7:Effect of two prompting strategies which elicit variable length reasoning chains across benchmark datasets. Model accuracy tends to trend upwards when more reasoning tokens are used. Reasoning token count is returned from the OpenAI API. Exact prompt specifications are displayed in Figures10and11.",
                "position": 523
            }
        ]
    },
    {
        "header": "5Directions with LLM Run-Time Strategies",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03590/extracted/5980332/images/performance_heatmap_greenred.png",
                "caption": "Figure 8:Heatmap showing absolute accuracy and relative performance over baseline zero-shot prompt (in parenthesis) across all benchmark datasets. As shown in Figure6, ensembling strategies consistently helped performance, while few-shot produced mixed results.",
                "position": 1489
            }
        ]
    },
    {
        "header": "8Appendix",
        "images": []
    }
]