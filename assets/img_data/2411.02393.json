[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02393/x1.png",
                "caption": "Figure 1:Adaptive Length Image Tokenizationmaps an image to multiple variable-length representations through a recurrent token allocation process,enabling task-specific sampling. We learn the tokenizer via image reconstruction as a self-supervised objective. While a compressed representation can be optimized for specific tasks (e.g., fewer tokens for ‚Äúdog‚Äù, ‚Äúleaf‚Äù, ‚Äúgrass‚Äù may suffice for a VLM task), reconstruction objective supports learning a universal, task-agnostic tokenizer.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x2.png",
                "caption": "Figure 2:Adaptive Length Image Tokenizer (ALIT):Given an image, we first convert it into 2D image tokens before applying the 2D‚Üí‚Üí\\rightarrow‚Üí1D latent distillation. ALIT recurrently distills 2D image tokens into variable 1D latent tokens, with each iteration addingnew latent tokensand processing them with theexisting 2D image tokensand theold latent tokens. Training focuses on reconstructing 2D image tokens through reverse distillation from latent 1D to masked 2D tokens. Based on token-reconstruction quality, we can optionally mask specific 2D tokens from further processing, enabling dynamic halting per token.Recurrent processing with Adaptive Memoryleads tocompressible representations, flexible tokenization & specialized tokens focusing on objects/parts.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02393/x3.png",
                "caption": "Figure 3:Reconstruction Analysis on ImageNet-100:Our approach outperforms all baselines in terms of reconstruction loss (right). Comparing Row-1 (high complexity) and Row-2 (low complexity) demonstrates the effectiveness of adaptive tokenization. Even with fewer tokens, our reconstructions maintain reasonable global alignment with ground truth, with an expected loss in detail.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x4.png",
                "caption": "Figure 4:Compression vs. Information Entropy Hypothesis on the Out-of-Distribution People-Art Dataset:Adaptive tokenization enables analysis of the Low-Complexity Art Hypothesis by examining token requirements for images of varying complexity. The plot on the right clearly shows that as (human-annotated)image complexity increases, so does the need for more computational tokens.More complex images have higher L1 reconstruction loss at fewer token count.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Adaptive Length Image Tokenization",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02393/x5.png",
                "caption": "Figure 5:Analysing Dataset Representation Capacity: We vary tokens per image using differentToken Selection Criteria (TSC)222Token-Selection Criteria (TSC) Example‚Äì Selecting per-image tokens using TSC = Top-X Classification means ‚Äì identifying the minimum number of tokens for each image such that the corresponding reconstructed image is correctly classified (by comparing against GT label) among the Top X predictions by ResNet-18.‚Äì Best Top-X Classification Accuracy (Left) and Depth Error<<<Threshold (Right). We use GT class/depth maps for computing TSC classification/depth errors. We then evaluate Classification Accuracy (Task of Interest, TOI) on the dataset reconstructed using different TSCs. X-axis = TSC, Y-axis = Dataset Token Count, Marker-Color = TOI Perf.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Not all images are worth the same representation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02393/x6.png",
                "caption": "Figure 6:TSC‚ÄìTOI Alignment determines Dataset Representation Capacity: We vary tokens per image via‚ÄúReconstruction Loss<<<Threshold‚Äù as automatic Token Selection Criteria (TSC)and evaluate multiple Tasks of Interest (TOI) on the reconstructions. Since TSC determines dataset token count,strong TSC‚ÄìTOI alignment enables desired TOI performance at compressed representations ‚Äì‚Äì 60% of max-tokens (selected via Recon. TSC) achieve similar perf. on all TOIs as max tokens perf., supporting adaptive-tokenization.Fig.14/15show classification / depth TSC plots.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x7.png",
                "caption": "Figure 7:Tokens vs. Model Strength:Stronger models outperform weaker ones across all token counts but are more sensitive to fewer-token reconstructions, showing a sharper perf. drop (‚àºsimilar-to\\sim‚àºcolor) at fewer tokens. In contrast,weaker models (lower GT perf.) can manage with fewer tokens.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x8.png",
                "caption": "Figure 8:Visualizing Latent Token Attention Maps:The learned latent tokens effectively bind to distinct objects, suggesting potential for object discovery as future research. This contrasts with 2D tokenizers, where tokens are strongly biased to predefined patches (also see Appendix Fig.13).",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x9.png",
                "caption": "Figure 9:Role of Recurrence on Latent Tokens:Credited to increasing number of tokens in each iteration, the recurrent update on existing latent tokens makes them focus on sparser & more localized regions, leading to improved alignment w/ GT segmentation over iterations (see Tab.2).",
                "position": 314
            }
        ]
    },
    {
        "header": "5Further Experiments & Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02393/x10.png",
                "caption": "Figure 10:(Plots 1-3)Ablating FID improvements w/ larger n/w, longer training, larger datasets, and multi-stage GAN training.(Plot 4)Ablating quantized vs continuous 2D and 1D tokenizers.",
                "position": 500
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.02393/x11.png",
                "caption": "Figure 11:Compression vs. Entropy Hypothesis on the Places Dataset:Adaptive tokenization enables analysis of Low-Complexity Art Hypothesis by examining token requirements for images of varying complexity. The plot on the right clearly shows that as image complexity increases, so does the need for more computational tokens. These reconstructions use Imagenet100-trained model.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x12.png",
                "caption": "Figure 12:Compression vs. Information Entropy Hypothesis on the MSCOCO Objects Dataset:Adaptive tokenization enables analysis of Low-Complexity Art Hypothesis by examining token requirements for images of varying complexity. The plot on the right clearly shows that as image complexity increases, so does the need for more computational tokens. These reconstructions use Imagenet100-trained model.",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2411.02393/extracted/5969434/images/token_visualization_dense_32.jpg",
                "caption": "Figure 13:Recurrent processing of Latent Tokens on COCO Validation Set:Recurrent processing with increasing computational tokens leads to specialization of attention. We showcase all the first 32 tokens after the1s‚Å¢tsuperscript1ùë†ùë°1^{st}1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT&8t‚Å¢hsuperscript8ùë°‚Ñé8^{th}8 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTiterations (out of total 256 tokens at the8t‚Å¢hsuperscript8ùë°‚Ñé8^{th}8 start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPTiteration). (Left) A broader comparison b/w iteration 1 vs 8 attention maps highlight emerging sparsity in attention.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x13.png",
                "caption": "Figure 14:Dataset Representation Analysis as factor of TSC-TOI Alignment: We vary tokens per image using‚ÄúBest Top-X Classification‚Äù Accuracy as the Token Selection Criteria (TSC)and evaluate different Tasks of Interest (TOI) on the reconstructions. We sample tokens per image such that each reconstructed image is classified correctly under Top-X Classification criteria, and then evaluate different downstream tasks of interests (TOIs) on the reconstructed dataset.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x14.png",
                "caption": "Figure 15:Dataset Representation as a factor of TSC-TOI Alignment: We vary tokens per image using‚ÄúDepth Loss<<<some Threshold‚Äù as the Token Selection Criteria (TSC)and evaluate different Tasks of Interest (TOI) on the reconstructions. Being a dense task, depth error thresholding at several continuous thresholds provide more granular and diverse support to dataset representation compression and TOI performance, compared to Top-X Classification (X being discrete integer).",
                "position": 1148
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x15.png",
                "caption": "Figure 16:Token Visualization over Multiple Recurrent Iteration:Recurrent processing leads to sparse attention & specialization of tokens to localised objects, parts etc. For example ‚Äì recurrence led to near-perfect bed segmentation (row 1), bicycle-sign segmentation (row 2), improved human segmentation (row 3), zebra-instance segmentation (row 4), and potted-plants segmentation (row 5).",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x16.png",
                "caption": "Figure 17:Effect of Model Size on Image Reconstruction Quality.Increasing model size enhances reconstruction quality. This visualization uses VQGAN as the base tokenizer (discrete 2D) distilled into discrete 1D latent tokens. With 256 tokens, the reconstruction quality closely aligns with VQGAN results and approaches ground truth, even for complex images, supporting the use of adaptive representations. (Zoom in for details.)",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2411.02393/x17.png",
                "caption": "Figure 18:Impact of Discrete vs. Continuous Tokenization on Image Reconstruction.Continuous 1D latent representations yield the best reconstructions overall. The advantage of using VAE as the base tokenizer (third column of Cont-2D + Cont-1D) becomes more significant at higher token counts. VQGAN (discrete 2D) combined with continuous 1D (second column) also performs well, especially at lower token counts, possibly due to the learning ease of the discrete cross-entropy training framework. Note: ALIT-S is used in this visualization, and reconstructions may improve with a larger model. (Zoom in for details.)",
                "position": 1455
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]