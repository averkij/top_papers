[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09614/x1.png",
                "caption": "Figure 1:LoCoBench Pipeline Architecture. Our systematic 5-phase pipeline transforms high-level specifications into a comprehensive evaluation benchmark. Phase 1 generates 1,000 diverse project specifications across 10 programming languages and 36 domains. Phase 2 creates complete codebases with realistic multi-file architectures, generating over 50K files with 15M lines of code. Phase 3 transforms codebases into 8,000 evaluation scenarios across 8 long-context task categories, with systematic context scaling from 10K to 1M tokens. Phase 4 ensures quality through automated compilation checks, quality metrics validation, and bias detection. Phase 5 evaluates LLMs using 17 comprehensive metrics across 4 evaluation dimensions.",
                "position": 212
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LoCoBench Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09614/x2.png",
                "caption": "Figure 2:LoCoBench Coverage Overview.Left:Programming language distribution showing equal representation (10% each) across 10 languages spanning diverse paradigms from systems programming (C, C++, Rust) to web development (JavaScript, TypeScript, PHP) to enterprise applications (Java, C#) to modern languages (Go, Python).Right:Hierarchical domain organization with 36 sub-categories grouped into 10 main categories, ensuring comprehensive coverage across web applications, API services, data systems, ML/AI systems, desktop applications, mobile applications, system infrastructure, financial technology, gaming & simulation, and blockchain systems.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x3.png",
                "caption": "",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x4.png",
                "caption": "Figure 3:Additional uniqueness factors in LoCoBench. Three independent factors provide comprehensive evaluation coverage:Left:10 architecture patterns including modern paradigms (microservices, serverless, event-driven) and traditional approaches (monolithic, layered, MVC), ensuring evaluation across diverse software architectures.Center:8 project themes spanning business applications, educational tools, healthcare systems, entertainment platforms, productivity software, social applications, utilities, and creative tools.Right:4 complexity levels (Easy, Medium, Hard, Expert) with equal 25% distribution, providing systematic difficulty progression from basic long-context tasks to enterprise-scale challenges.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x5.png",
                "caption": "Figure 4:LoCoBenchâ€™s evaluation projects analysis.Top rowshows distribution of lines of code (left) and file counts (right) across all evaluation projects.Bottom row:Programming language breakdown displaying lines of code distribution (left) and file count distribution (right) across 10 programming languages.",
                "position": 722
            }
        ]
    },
    {
        "header": "4Evaluation Metrics",
        "images": []
    },
    {
        "header": "5Experiments, Results and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09614/x6.png",
                "caption": "Figure 5:Overall performance comparison of GPT-5, Gemini-2.5-Pro, and Claude-Sonnet-4 across 10 LoCoBench dimensions. Gemini-2.5-Pro demonstrates superior performance on many aspects, particularly on cross-file refactoring, long-context utilization, integration tests, and multi-session development capabilities, while GPT-5 excels in architectural understanding. Claude-Sonnet-4 shows balanced performance with particular strength in code comprehension.",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x7.png",
                "caption": "Figure 6:Model ranking and long-context utilization comparison. Left chart shows LoCoBench Score (LCBS) rankings. Right chart displays long-context utilization performance.",
                "position": 1311
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x8.png",
                "caption": "Figure 7:Programming language performance heatmap showing model performance across 10 programming languages. Languages are ordered by difficulty from easiest to hardest.",
                "position": 1333
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x9.png",
                "caption": "Figure 8:Task category performance analysis. Top chart shows performance distribution across all models for each task category, with individual model performance overlaid. Bottom chart displays task difficulty patterns and model performance trends across different software engineering tasks.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x10.png",
                "caption": "Figure 9:Context length and difficulty impact analysis. Upper left shows performance distribution by difficulty level. Upper right displays individual model performance trends across difficulty levels. Lower left presents model consistency versus specialization patterns. Lower right analyzes performance versus consistency relationships.",
                "position": 1380
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x11.png",
                "caption": "Figure 10:Domain specialization and performance analysis. Top chart shows model performance trends across 10 application domains. Lower left displays domain difficulty spectrum from easiest to hardest. Lower right presents model consistency analysis comparing average performance with domain variation patterns.",
                "position": 1408
            },
            {
                "img": "https://arxiv.org/html/2509.09614/x12.png",
                "caption": "Figure 11:Architecture pattern performance analysis. Top chart shows model performance trajectories across 10 architecture patterns. Lower left presents complexity versus performance relationship with bubble sizes indicating performance variation. Lower right displays coupling/cohesion analysis across different architectural coupling levels.",
                "position": 1436
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Related Work",
        "images": []
    },
    {
        "header": "Appendix BLoCoBench Pipeline Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": []
    }
]