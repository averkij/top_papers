[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x1.png",
                "caption": "Figure 1:Our method performs lightweight test-time optimization(≈\\approx0.419 s/image)without requiring any dataset-level training.It generalizes seamlessly across domains while maintaining consistent reconstruction quality for every image. (All examples are randomly selected, without cherry-picking.)",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x2.png",
                "caption": "Figure 2:Comparison of dataset-level training and our test-time optimization (TTO). (a) Dataset-level methods (FeatUp, LoftUp, JAFAR, AnyUp) require paired training data and handle only 2D feature maps. (b) Our Upsample Anything performs TTO using only one HR image and generalizes to feature, depth, segmentation, and even 3D features.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x3.png",
                "caption": "Figure 3:Overview of Upsample Anything.Given a high-resolution imageIh​rI_{hr}, we downsample it toIl​rI_{lr}and optimize GSJBU to reconstructIh​rI_{hr}, learning per-pixel anisotropic kernels{σx,σy,θ,σr}\\{\\sigma_{x},\\sigma_{y},\\theta,\\sigma_{r}\\}via test-time optimization (TTO).\nThe learned kernels are then applied to foundation featuresFl​rF_{lr}for rendering the high-resolution featuresFh​rF_{hr}, achieving pixel-wise anisotropic joint bilateral upsampling.",
                "position": 214
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methods",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x4.png",
                "caption": "Figure 4:Depth upsampling results on Middlebury (top) and NYUv2 (bottom). 32×32 low-resolution depth maps were upsampled to high resolution using different methods.\nWhile Upsample Anything produces sharper and more detailed edges, it still achieves lower RMSE (0.237) than bilinear (0.159) on low-resolution maps.\nHowever, in high-resolution depth prediction, Upsample Anything outperforms both qualitatively and quantitatively.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2511.16301/x5.png",
                "caption": "Figure 5:Comparison across different resolutions. Qualitative results of AnyUp (previous SOTA) and our Upsample Anything on varying input resolutions.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2511.16301/x6.png",
                "caption": "Figure 6:Visual comparison across different backbones. Given the same 224×224 input, feature maps have varying spatial resolutions (7×7 for ConvNeXt, 14×14 for CLIP and DINOv1, 16×16 for DINOv2/v3). Upsample Anything produces sharper edges, richer textures, and more distinct feature clustering than AnyUP across all backbones, demonstrating its strong adaptability through test-time optimization.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2511.16301/x7.png",
                "caption": "Figure 7:Visualization of feature similarity between a reference and a target image. The feature vector is obtained by averaging the reference features within the reference mask, and cosine similarity is then computed against all feature locations in the target image.",
                "position": 748
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Semantic Segmentation on Cityscapes",
        "images": []
    },
    {
        "header": "8Details of Probabilistic Map Upsampling in Table 1.",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x8.png",
                "caption": "Figure 8:Comparison of segmentation pipelines. (a) Conventional segmentation pipeline with a Vision Foundation Model encoder and task-specific decoders such as DPT, UPerNet, SegFormer, or Mask2Former. (b) Feature upsampling pipeline using pretrained upsamplers such as FeatUP, LoftUP, JAFAR, or AnyUp, operating on feature maps. (c) Our proposed Upsample Anything, which performs test-time optimization and handles both feature and segmentation upsampling without additional training.",
                "position": 1093
            }
        ]
    },
    {
        "header": "9Details of Depth Estimation in Table 2.",
        "images": []
    },
    {
        "header": "10From 2D Low-Resolution Feature Maps to 3D High-Resolution Feature Volumes",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x9.png",
                "caption": "Figure 9:Visualization of our 3D feature upsampling results. The first two columns show the RGB image and its corresponding depth map. The remaining panels depict representative depth slices from the reconstructed 3D high-resolution feature volume obtained using our Upsample Anything. Each slice is visualized via PCA projection into RGB space. Notice that the recovered 3D feature layers exhibit smooth transitions along the depth axis while preserving fine object boundaries and geometric continuity.",
                "position": 1173
            },
            {
                "img": "https://arxiv.org/html/2511.16301/x10.png",
                "caption": "Figure 10:Visualization of learned Gaussian blobs.\n(a) shows the original image and (b) displays the Gaussian blobs overlaid on the low-resolution input.\nThe blobs reveal locally coherent directions and magnitudes, indicating that the learned kernels adapt to the underlying structure of the scene.",
                "position": 1187
            },
            {
                "img": "https://arxiv.org/html/2511.16301/x11.png",
                "caption": "Figure 11:Visualization of thesegment-then-upsamplepipeline.\nThe segmentation logits are first generated at low resolution and then upsampled by16×16\\timesusing our method.\nThe results exhibit remarkably sharp object boundaries and preserve semantic coherence, highlighting the effectiveness of our upsampling approach.",
                "position": 1194
            }
        ]
    },
    {
        "header": "11Gaussian Blob Visualization",
        "images": []
    },
    {
        "header": "12Segment-then-Upsample Pipeline Visualization Results",
        "images": []
    },
    {
        "header": "13Formal Relation Between Joint Bilateral Upsampling and Gaussian Splatting",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16301/x12.png",
                "caption": "Figure 12:Qualitative comparison under low-SNR and noise corruption. From left to right: RGB input, low-resolution feature, upsampled feature by AnyUp, and ours (Upsample Anything). From top to bottom: clean image, 10% noise, and 20% noise. AnyUp remains stable under noise, while our TTO-based method overfits to noisy pixels, revealing its limitation when directly optimizing on corrupted inputs.",
                "position": 1495
            }
        ]
    },
    {
        "header": "14Hyperparameter Table",
        "images": []
    },
    {
        "header": "15Limitation under Low-SNR or Corrupted Inputs",
        "images": []
    }
]