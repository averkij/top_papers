[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24711/x1.png",
                "caption": "Figure 1:(a) We randomly sample 1k intermediate-layer tokens from 110 ImageNet classes for 10-cluster k-means clustering (differentiated by color).\nWith class names/labels as inputs, LLM tokens form compact, well-separated clusters with high semantic density, whereas visual tokens are diffuse. This disparity is quantified by the ratio of inter- to intra-class distance (19.283≫0.74819.283\\gg 0.748).\n(b) We measure inter-expert diversity using singular value decomposition on\neach MoE layer’s expert weight matrices and computing the mean similarity of the subspaces spanned by their top-k left singular vectors(Hu et al.,2021).\nIncorporating routing guidance (Ours) enhances expert diversity.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24711/x2.png",
                "caption": "Figure 2:Overview of ProMoE architecture.The input tokens are split by conditional routing into unconditional and conditional subsets. Unconditional image tokens are processed by unconditional experts. Conditional image tokens are assigned by prototypical routing with learnable prototypes.\nThe routing contrastive learning explicitly enhances semantic guidance in prototypical routing.",
                "position": 189
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4ProMoE",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24711/x3.png",
                "caption": "Figure 3:Comparisons and scaling results across diverse settings.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2510.24711/x4.png",
                "caption": "Figure 4:Samples generated by ProMoE-XL-Flowafter 2M iterations with cfg=4.0.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2510.24711/x5.png",
                "caption": "Figure 5:Training loss curve comparisons.",
                "position": 721
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix BImplementation Algorithms",
        "images": []
    },
    {
        "header": "Appendix CMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24711/x6.png",
                "caption": "Figure 6:t-SNE visualization results of ProMoE and DiT-MoE on expert allocation (token assignment).Each color corresponds to a single expert.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2510.24711/x7.png",
                "caption": "Figure 7:Comparison with dense model and MoE SOTAs on Inception Score.",
                "position": 1913
            }
        ]
    },
    {
        "header": "Appendix DMore Results on Scaling Behavior",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24711/x8.png",
                "caption": "Figure 8:More scaling results on model size.",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2510.24711/x9.png",
                "caption": "Figure 9:More scaling results on the number of experts.",
                "position": 2140
            }
        ]
    },
    {
        "header": "Appendix EMore Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24711/x10.png",
                "caption": "Figure 10:More t-SNE visualization results of Llama-3 8Bon different layers.",
                "position": 2390
            },
            {
                "img": "https://arxiv.org/html/2510.24711/x11.png",
                "caption": "Figure 11:More t-SNE visualization results of DiT-XL/2on different layers and diffusion timesteps.",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2510.24711/x12.png",
                "caption": "Figure 12:More samples generated by ProMoE-XL-Flowafter 2M iterations with cfg=4.0.",
                "position": 2400
            }
        ]
    },
    {
        "header": "Appendix FUsage of Large Language Models (LLMs)",
        "images": []
    }
]