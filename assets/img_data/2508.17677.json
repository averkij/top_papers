[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17677/x1.png",
                "caption": "Figure 1:Performance Comparisons of our TiKMiX versus SOTA Data Mixing Strategies for Pre-training a 1B Parameter Language Model with 1T Tokens.",
                "position": 104
            }
        ]
    },
    {
        "header": "Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17677/",
                "caption": "Figure 2:The process involves periodically measuring domain contributions via Group Influence and adjusting the data mixture to maximize learning efficiency.",
                "position": 148
            }
        ]
    },
    {
        "header": "Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17677/x3.png",
                "caption": "Figure 3:The impact of different pre-training data domains on the validation set as training progresses.",
                "position": 168
            }
        ]
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17677/x4.png",
                "caption": "Figure 4:Analysis of the Group Influence and actual performance on the benchmark.",
                "position": 411
            }
        ]
    },
    {
        "header": "Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17677/x5.png",
                "caption": "Figure 5:The impact of domains on a 1B model’s performance on the ARC benchmark as training progresses.",
                "position": 1615
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x6.png",
                "caption": "Figure 6:The impact of domains on a 1B model’s performance on the HELLASWAG benchmark as training progresses.",
                "position": 1618
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x7.png",
                "caption": "Figure 7:The impact of domains on a 1B model’s performance on the MMLU benchmark as training progresses.",
                "position": 1621
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x8.png",
                "caption": "Figure 8:The impact of domains on a 1B model’s performance on the TRIVIAQA benchmark as training progresses.",
                "position": 1624
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x9.png",
                "caption": "Figure 9:The impact of domains on a 7B model’s performance on the ARC benchmark as training progresses.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x10.png",
                "caption": "Figure 10:The impact of domains on a 7B model’s performance on the HELLASWAG benchmark as training progresses.",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x11.png",
                "caption": "Figure 11:The impact of domains on a 7B model’s performance on the MMLU benchmark as training progresses.",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x12.png",
                "caption": "Figure 12:The impact of domains on a 7B model’s performance on the TRIVIAQA benchmark as training progresses.",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2508.17677/x13.png",
                "caption": "Figure 13:A Group Influence-based Analysis of Data Mixing Effects on Various Benchmarks.",
                "position": 1639
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]