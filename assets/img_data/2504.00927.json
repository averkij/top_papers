[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background on multi-head attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00927/x1.png",
                "caption": "Figure 1:Multi-Token Attention (MTA)on the right, compared to standard attention on the left. In MTA, within each head we apply a key-query convolution on the attention scores and, after softmax normalization, head convolution across groups of heads. Additionally, we apply a group normalization with depth-dependent scaling before final concatenation.",
                "position": 178
            }
        ]
    },
    {
        "header": "3Multi-Token Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00927/x2.png",
                "caption": "Figure 2:MTA applies key-query and head convolution over attention values.",
                "position": 211
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00927/x3.png",
                "caption": "Figure 3:Kernel pattern (left) and corresponding attention map (right), which has the highest attention scores on the targeted needle ”The magic number of San Francisco is 8”.\nThis kernel amplifies attention if a query token sequence matches a key sequence –\nuseful for searching for related sentences that match the current sentence.",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x4.png",
                "caption": "",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x5.png",
                "caption": "Figure 4:(left)Average accuracy on QA1-5 tasks in BabiLong. The models are all pretrained with 2K context and then finetuned with 4K context. Distraction text length varies from 0K (no distraction) to 4K tokens. MTA consistently outperforms the baseline models.(right)Ablation on the number of MTA layers with key-query convolutions (head convolution is applied to all layers). We report average validation perplexity on SlimPajama.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x6.png",
                "caption": "",
                "position": 895
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BMTA Architecture",
        "images": []
    },
    {
        "header": "Appendix CToy task details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00927/x7.png",
                "caption": "Figure 5:Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K).",
                "position": 1921
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x8.png",
                "caption": "",
                "position": 1924
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x9.png",
                "caption": "",
                "position": 1925
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x10.png",
                "caption": "",
                "position": 1927
            },
            {
                "img": "https://arxiv.org/html/2504.00927/x11.png",
                "caption": "",
                "position": 1928
            }
        ]
    },
    {
        "header": "Appendix DLanguage model training details",
        "images": []
    },
    {
        "header": "Appendix EBabiLong details",
        "images": []
    },
    {
        "header": "Appendix FComplexity evaluation",
        "images": []
    },
    {
        "header": "Appendix GMulti-needle evaluation results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/2_needles.png",
                "caption": "Figure 6:Needle-in-a-Haystack accuracy across different models. Models trained with MTA are better at predicting needles hidden deep in the context.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/4_needles.png",
                "caption": "",
                "position": 2090
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/6_needles.png",
                "caption": "",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/kernels_l3.png",
                "caption": "Figure 7:Key-query kernel patterns across heads at 3rd layer of the Transformer model with MTA.",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/kernels_l7.png",
                "caption": "Figure 8:Key-query kernel patterns across heads at 7th layer of the Transformer model with MTA.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/kernels_l11.png",
                "caption": "Figure 9:Key-query kernel patterns across heads at 11th layer of the Transformer model with MTA.",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/kernels_l15.png",
                "caption": "Figure 10:Key-query kernel patterns across heads at 15th layer of the Transformer model with MTA.",
                "position": 2112
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/kernels_l19.png",
                "caption": "Figure 11:Key-query kernel patterns across heads at 19th layer of the Transformer model with MTA.",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/kernels_l23.png",
                "caption": "Figure 12:Key-query kernel patterns across heads at 23rd layer of the Transformer model with MTA.",
                "position": 2118
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/head_kernel.png",
                "caption": "Figure 13:Head kernel patterns across first three layers of the Transformer model with MTA.",
                "position": 2121
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/diag_norm.png",
                "caption": "Figure 14:Average of the diagonal kernel values across head pairs for Multi-Token Attention models with (left) and without (right) group normalization. Values for layers with key-query convolution are colored in orange.",
                "position": 2124
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/diag_wonorm.png",
                "caption": "",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/ratio_norm.png",
                "caption": "Figure 15:Average of the ratio between non-diagonal and diagonal kernel values across head pairs for Multi-Token Attention models with (left) and without (right) group normalization. Values for layers with key-query convolution are colored in orange.",
                "position": 2131
            },
            {
                "img": "https://arxiv.org/html/2504.00927/extracted/6327991/figures/ratio_wonorm.png",
                "caption": "",
                "position": 2134
            }
        ]
    },
    {
        "header": "Appendix HMTA kernel patterns.",
        "images": []
    }
]