[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03006/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03006/x2.png",
                "caption": "Figure 2:Comparison betweenGeneration-Then-Predictionand ourJoint Generationapproach. Given the generated RGB in (a), (b) and (c) show the predicted alpha (top) and the composited result (bottom). In (d), the top shows the jointly generated alpha.",
                "position": 93
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03006/x3.png",
                "caption": "Figure 3:Pipeline of TransPixar.Our method is organized as follows: (1)Left: we extend the input of DiT to include new alpha tokens; (2)Top Center: we initialize alpha tokens with our positional encoding; (3)Bottom Center: we insert a partial LoRA and adjust attention computation during training and inference.",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x4.png",
                "caption": "Figure 4:Positional Encoding Design for RGBA Generation.Assigning alpha tokens the same positional encoding as RGB yields similar results, resulting in faster convergence after 1000 iterations compared to standard encoding strategies.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x5.png",
                "caption": "Figure 5:Attention Rectification.(a) Eliminating all attention from alpha as a key preserves 100% RGB generation but leads to poor alignment. (b) Retaining all attention significantly degrades quality, causing a lack of motion in bicycles. (c) Our method achieves an effective balance.",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x6.png",
                "caption": "Figure 6:Applications.Top: Text-to-Video with Transparency.Bottom: Image-to-Video generation with transparency.\n.",
                "position": 408
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03006/x7.png",
                "caption": "Figure 7:Comparison with Generation-then-Prediction Pipelines.Our method demonstrates superior alignment.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x8.png",
                "caption": "Figure 8:Comparison with Joint Generation Pipelines.Top: LayerDiffusion + AnimateDiff;Bottom: Ours. Our method achieves better alignment and generates corresponding motion described by prompts.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x9.png",
                "caption": "Figure 9:Alternative Designs for Joint Generation with DiT. Sequence extension (b) represents our method.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x10.png",
                "caption": "Figure 10:Ablation Study.(a) Ours; (b) Ours without RGB-attend-to-Alpha; (c) Ours with Text-attend-to-alpha; (d) Batch Extension Strategy; (e) Latent Dimension Extension Strategy. Our method maintains high-quality motion generation (e.g., butterflies waving their wings) while achieving good alignment.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2501.03006/x11.png",
                "caption": "Figure 11:Quantitative Evaluation. Our approach achieves a good balance between alignment (low flow difference) and preserving generative quality (low FVD).",
                "position": 517
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Comparisons with Video Matting",
        "images": []
    },
    {
        "header": "8Data Preprocessing",
        "images": []
    },
    {
        "header": "9Optical Flow Difference",
        "images": []
    },
    {
        "header": "10Video Results",
        "images": []
    },
    {
        "header": "11Additional Visual Results",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]