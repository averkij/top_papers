[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24988/x1.png",
                "caption": "Figure 1:RQ1 & RQ2 overview.(Left) Self- vs. cross-model correctness prediction across Qwen and Llama:\naccuracies are comparable for each predictor model, suggesting no inherent advantage to a modelpredicting its ownoutputs. (Right) Historical information improves calibration:\n(a) training on multiple model’s histories learns generalizable strategies for correctness prediction;\n(b) predictive power comes from phrasing of output, CM’s world knowledge, and matching performance to question type. Each stage generalizes, and most prominently strategies for applying world knowledge;\n(c) History injected with post-hoc calibration and in-context learning helps improve correctness without finetuning. The GCM combines insights from RQs to achieve high accuracy and extremely low calibration error for the correctness prediction of multiple models, outperforming the logits of equally-sized and larger models.\nWe include a mapping for results in this figure to experimental settings in AppendixA.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Methods and Experimental Setup",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.24988/x2.png",
                "caption": "Figure 2:Do LLMs possess special self-knowledge of their correctness?We compare correctness prediction inanswerful(with responses) andanswerless(without responses) settings. Qwen2.5-7B beats Llama3.1-8B when responses are included, while both perform similarly without them, indicating that dataset signals and world knowledge drive performance, not privileged self-knowledge.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2509.24988/x3.png",
                "caption": "Figure 3:Conditioning factorsablation.\nGCMs and SCMs across conditioning settings in RQ2b (Section3.3). More metrics:Table24.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2509.24988/x3.png",
                "caption": "Figure 3:Conditioning factorsablation.\nGCMs and SCMs across conditioning settings in RQ2b (Section3.3). More metrics:Table24.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2509.24988/x4.png",
                "caption": "Figure 4:Risk-Coverage Curvesfor Selective Prediction,lowerAURC curves are better.",
                "position": 840
            }
        ]
    },
    {
        "header": "4Related Work.",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFigure 1 Experimental Settings",
        "images": []
    },
    {
        "header": "Appendix BDiscussion",
        "images": []
    },
    {
        "header": "Appendix COptimal Batch Size Leads to Negligible Calibration Error",
        "images": []
    },
    {
        "header": "Appendix DAdditional Details on Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix EFurther results showing that models have little special information about their own abilities",
        "images": []
    },
    {
        "header": "Appendix FAdditional Evaluations for GCMs vs SCMs",
        "images": []
    },
    {
        "header": "Appendix GUnlimited Training Time Ablation",
        "images": []
    },
    {
        "header": "Appendix HFurther Discussion on Posthoc Calibration",
        "images": []
    },
    {
        "header": "Appendix IConditioning Factors Ablations",
        "images": []
    },
    {
        "header": "Appendix JRelated Work",
        "images": []
    }
]