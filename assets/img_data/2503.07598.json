[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07598/x1.png",
                "caption": "Figure 1:Task categories covered byVACE.Four basic tasks can be combined to create a vast number of possibilities.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07598/x2.png",
                "caption": "Figure 2:Overview ofVACEFramework.Frames and masks are tokenized through Concept Decoupling, Context Latent Encode and Context Embedder. To achieve training with VCU as input, we employ two strategies, (a) Fully Fine-tuning and (b) Context Adapter Tuning. The latter converges faster and supports pluggable features.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Datasets",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07598/x3.png",
                "caption": "Figure 3:Visualization results of compositional tasks.VACEcreatively enables reference-, move-, animate-, swap-, and expand-anything.",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2503.07598/extracted/6268010/figures/ablation_1.png",
                "caption": "(a)Base structure setting.",
                "position": 953
            },
            {
                "img": "https://arxiv.org/html/2503.07598/extracted/6268010/figures/ablation_1.png",
                "caption": "(a)Base structure setting.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2503.07598/extracted/6268010/figures/ablation_2.png",
                "caption": "(b)Hyperparameter settings.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2503.07598/extracted/6268010/figures/ablation_3.png",
                "caption": "(c)Context adapter configurations.",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2503.07598/extracted/6268010/figures/ablation_4.png",
                "caption": "(d)Concept decouple setting.",
                "position": 972
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07598/x4.png",
                "caption": "Figure 5:More visualization resultsof Wan-T2V-basedVACEframework.",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2503.07598/x5.png",
                "caption": "Figure 6:More visualization resultsof Wan-T2V-basedVACEframework.",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2503.07598/x6.png",
                "caption": "Figure 7:Qualitative comparisonson various tasks based on LTX-Video-basedVACEframework.",
                "position": 2220
            }
        ]
    },
    {
        "header": "Appendix CDiscussion",
        "images": []
    }
]