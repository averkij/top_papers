[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13581/x1.png",
                "caption": "Figure 1:Illustration of the tokenization process of ActionPiece. Each action is represented as an unordered feature set.\nThis figure presents two possible tokenized sequences. The same action can be tokenized into different tokens depending on the surrounding context. A detailed case study can be found inSection4.5.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13581/x2.png",
                "caption": "Figure 2:Illustration of how weights of co-occurring token pairs are counted during vocabulary construction. In this example, two adjacent sets in the sequence are considered: one with4444tokens (represented as‚óã‚óã\\bigcirc‚óã) and another with3333tokens (represented as‚ñ°‚ñ°\\square‚ñ°). Token pairs are counted within a single set (<‚óã,‚óã><\\bigcirc,\\bigcirc>< ‚óã , ‚óã >and<‚ñ°,‚ñ°><\\square,\\square>< ‚ñ° , ‚ñ° >) and across the two adjacent sets (<‚óã,‚ñ°><\\bigcirc,\\square>< ‚óã , ‚ñ° >).",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2502.13581/x3.png",
                "caption": "Figure 3:Illustration of how the linked list, which maintains the action sequence, is updated when merging two tokens into a new token. Three cases are considered: (1) both tokens are in the same action node; (2) the tokens are in two adjacent action nodes; (3) one token is in an action node, while the other is in an intermediate node.",
                "position": 414
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13581/x4.png",
                "caption": "Figure 4:Analysis of recommendation performance (NDCG@10,‚Üë‚Üë\\uparrow‚Üë) and average tokenized sequence length (NSL,‚Üì‚Üì\\downarrow‚Üì) w.r.t. vocabulary size across three datasets.\n‚ÄúN/A‚Äù indicates that ActionPiece is not applied,i.e.,action sequences are represented solely by initial tokens.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2502.13581/x5.png",
                "caption": "Figure 5:Analysis of token utilization rate (%) during model training w.r.t. segmentation strategy.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2502.13581/x6.png",
                "caption": "Figure 6:Analysis of performance (NDCG@10,‚Üë‚Üë\\uparrow‚Üë) w.r.t. the number of ensembled segmentsqùëûqitalic_qduring model inference.",
                "position": 859
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotations",
        "images": []
    },
    {
        "header": "Appendix BAlgorithmic Details",
        "images": []
    },
    {
        "header": "Appendix CEfficient Vocabulary Construction Implementation",
        "images": []
    },
    {
        "header": "Appendix DDiscussion: Comparison Between ActionPiece and BPE",
        "images": []
    },
    {
        "header": "Appendix EDatasets",
        "images": []
    },
    {
        "header": "Appendix FBaselines",
        "images": []
    },
    {
        "header": "Appendix GImplementation Details",
        "images": []
    },
    {
        "header": "Appendix HReproduction",
        "images": []
    }
]