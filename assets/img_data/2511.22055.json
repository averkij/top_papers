[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22055/x1.png",
                "caption": "Figure 1:Overview of diverse dental-specialized corpus. (a) Eight types of widely used dental imaging modalities. (b) Introduction of our proposed TRACE-CoT reasoning pattern that enhances the reliability of MLLM’s response. (c) The composition of the training corpus for OralGPT-Omni. The bar chart shows the distribution of various dental modalities.",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x2.png",
                "caption": "Figure 2:(a) The dental imaging data curation and TRACE-CoT data generation pipeline. It involves curating diverse imaging modalities from public datasets and dental hospitals. TRACE-CoT data is then generated using GPT, Wikipedia, and various annotations. Finally, the data is split into a training set and a benchmark, with professional dentists assessing the training samples and a thorough manual correction conducted on the benchmark. (b) Results from two dentists evaluating the quality of 300 TRACE-CoT data from the training set.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x3.png",
                "caption": "Figure 3:There are four stages for training OralGPT-Omni, and only the first stage is training in the single modality.",
                "position": 187
            }
        ]
    },
    {
        "header": "2OralGPT-Omni",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22055/x4.png",
                "caption": "Figure 4:The difficulty-aware data selection strategy for RLT.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x5.png",
                "caption": "Figure 5:(a) The distribution of the MMOral-Uni benchmark, spanning five dental imaging modalities and covering five tasks. (b) Performance comparison on the MMOral-Uni benchmark. (c) Performance comparison on the MMOral-OPG benchmark.",
                "position": 259
            }
        ]
    },
    {
        "header": "3MMOral-Uni Benchmark",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22055/x6.png",
                "caption": "Table 1:Results on the MMOral-Uni for various existing LVLMs across five dental imaging modalities. The abbreviations are defined as follows: II - Intraoral Image, PA - Periapical Radiograph, CE - Cephalometric Radiograph, PI - Pathological Image, TP - Treatment Planning, IV - Intraoral Video. The best-performing model in each category is highlightedin-bold, while the second-best isunderlined.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x6.png",
                "caption": "Table 2:Performance on the MMOral-OPG benchmark for various LVLMs on open-ended VQA tasks. The best-performing model is highlightedin-bold, while the second-best isunderlined.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x6.png",
                "caption": "Figure 6:Three modalities of case studies on the MMOral-Uni benchmark are presented, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 2062
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "1Related Works",
        "images": []
    },
    {
        "header": "2Details on Training Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22055/figs/link.png",
                "caption": "Table 4:Comprehensive list of dental-specific data sources used in the training phase, including corresponding abnormalities and tasks.",
                "position": 2755
            }
        ]
    },
    {
        "header": "3Implementation Details of Model Training",
        "images": []
    },
    {
        "header": "4MMOral-Uni Benchmark",
        "images": []
    },
    {
        "header": "5LLM as the judges for MMOral-Omni: A Feasibility Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22055/figs/suppl_llm-as-judge-stability.png",
                "caption": "Figure 7:The means and standard deviations of each category on 5 repeated evaluations across four LVLMs’ predictions.",
                "position": 3572
            },
            {
                "img": "https://arxiv.org/html/2511.22055/figs/link.png",
                "caption": "Table 7:The category, visual appearance, and relevant dental knowledge utilized in prompts for the TRACE-CoT data construction pipeline across multiple datasets.",
                "position": 3696
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x7.png",
                "caption": "Figure 8:The word cloud maps for training datasets used in the DKI, DCA, and SFT stages, respectively.",
                "position": 4189
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x8.png",
                "caption": "Figure 9:The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for image-level diagnosis of intraoral images.",
                "position": 4192
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x9.png",
                "caption": "Figure 10:The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for region-level diagnosis of intraoral images.",
                "position": 4195
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x10.png",
                "caption": "Figure 11:The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for diagnosis of periapical radiographs.",
                "position": 4266
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x11.png",
                "caption": "Figure 12:The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for diagnosis of pathological images.",
                "position": 4657
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x12.png",
                "caption": "Figure 13:The prompt for GPT-5-mini to generate the TRACE-CoT reasoning chains for comprehension of intraoral videos.",
                "position": 4660
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x13.png",
                "caption": "Figure 14:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4663
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x14.png",
                "caption": "Figure 15:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4667
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x15.png",
                "caption": "Figure 16:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4671
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x16.png",
                "caption": "Figure 17:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4675
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x17.png",
                "caption": "Figure 18:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4679
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x18.png",
                "caption": "Figure 19:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4683
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x19.png",
                "caption": "Figure 20:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4687
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x20.png",
                "caption": "Figure 21:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4691
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x21.png",
                "caption": "Figure 22:Data example used in the training stage. It contains multimodal information, including the image, question, and ground truth.",
                "position": 4695
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x22.png",
                "caption": "Figure 23:The prompt for computing the TRACE-based reward using GPT-5-nano.",
                "position": 4699
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x23.png",
                "caption": "Figure 24:The prompt for computing the answer reward using GPT-5-nano.",
                "position": 4702
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x24.png",
                "caption": "Figure 25:Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists.",
                "position": 4705
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x25.png",
                "caption": "Figure 26:Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists.",
                "position": 4708
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x26.png",
                "caption": "Figure 27:Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists.",
                "position": 4711
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x27.png",
                "caption": "Figure 28:Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists.",
                "position": 4714
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x28.png",
                "caption": "Figure 29:Some examples in our MMOral-Uni benchmark. Each example contains the image, question, category, and corresponding ground truth validated by experienced dentists.",
                "position": 4717
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x29.png",
                "caption": "Figure 30:Case study on the intraoral image modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4765
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x30.png",
                "caption": "Figure 31:Case study on the intraoral image modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4768
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x31.png",
                "caption": "Figure 32:Case study on the periapical X-ray modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4771
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x32.png",
                "caption": "Figure 33:Case study and clinical validity on the intraoral image modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4774
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x33.png",
                "caption": "Figure 34:Case study and clinical validity on the periapical X-ray modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4777
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x34.png",
                "caption": "Figure 35:Case study and clinical validity on the histopathological image modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4780
            },
            {
                "img": "https://arxiv.org/html/2511.22055/x35.png",
                "caption": "Figure 36:Case study and clinical validity on the cephalometric radiograph modality, with correct responses highlighted ingreenand obvious incorrect responses highlighted inred.",
                "position": 4783
            }
        ]
    },
    {
        "header": "6Case Study and Clinical Validity of OralGPT-Omni",
        "images": []
    }
]