[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example3_cut.png",
                "caption": "Figure 1:An example from InfographicsVQA.Question:What is the resource and expertise level needed for Pinterest?Answer:high.The LLM is expected to generate the answer together with the corresponding grounded bounding boxes that can support its answer, which requires deeper spatial understanding and reasoning and sometimes instruction-following abilities.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3TRIG: Text-Rich Image Grounding",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/sample.png",
                "caption": "Figure 2:Text-rich document image examples from different source datasets.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/main_figure.png",
                "caption": "Figure 3:Main Constriction Pipeline. The pipeline contains 4 steps: Preprocessing, Generation, Correction, and Human Evaluation. The benchmark data will go through all of these 4 steps, and the training data will go through the previous 3 steps.",
                "position": 254
            }
        ]
    },
    {
        "header": "4Proposed Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/embedding_illustrate.png",
                "caption": "Figure 4:The illustrative pipeline of ourEmbedding-Based Method. The example is from ChartQA.Question:Is the percentage value of “STEM” segment 52?Answer:Yes.The visualization of ground-truth bounding boxes is presented on the left in green, and the visualization of the generated grounding area (patches) by our embedding-based method is presented on the right in red.\nThe input image will be processed into32×32323232\\times 3232 × 32patches before sending it into the MLLM and obtaining the image patch embeddings. After obtaining both the image patch embeddings and text token embeddings, a similarity vector with the length of the number of image patches is generated. The higher scores represent the alignment between image and text, whose position will be selected as the grounding patches. For simplicity, the embedding merge and 2-level selection mechanisms are not presented in this figure.",
                "position": 338
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": []
    },
    {
        "header": "6Further Discussions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Information",
        "images": []
    },
    {
        "header": "Appendix BTraining Configurations",
        "images": []
    },
    {
        "header": "Appendix CDetailed Evaluation Settings and Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/eval.png",
                "caption": "Figure 5:Illustrations on Evaluation Settings.In evaluation setting 1, no OCR model is used, representing the hardest scenario. While in settings 2 & 3, an additional OCR model is utilized to facilitate LLM on grounding information generation. The “Instruction” in the prompt describes the requirement of generating grounded bounding boxes and defines the desired format.",
                "position": 2853
            }
        ]
    },
    {
        "header": "Appendix DDetailed Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/ablation_detailed.png",
                "caption": "Figure 6:The visualization of detailed ablation studies of our embedding-based method on OCR-free grounding setting for the 2-level selection mechanism. The utilization of the 2-level selection mechanism avoids the performance decline when more patches are selected, making the performances more stable and getting better performances.",
                "position": 3597
            }
        ]
    },
    {
        "header": "Appendix EGeneration & Evaluation Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/evaluation_prompt.png",
                "caption": "Figure 9:The evaluation prompts for different evaluation settings.",
                "position": 3658
            },
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_chart.png",
                "caption": "Figure 10:Benchmark data examples from ChatQA. The grounded bounding boxes have already been visualized in the original image for better illustration.",
                "position": 3671
            },
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_doc.png",
                "caption": "Figure 11:Benchmark data examples from DocVQA. The grounded bounding boxes have already been visualized in the original image for better illustration.",
                "position": 3676
            },
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_info.png",
                "caption": "Figure 12:Benchmark data examples from InfographicsVQA. The grounded bounding boxes have already been visualized in the original image for better illustration.",
                "position": 3681
            },
            {
                "img": "https://arxiv.org/html/2504.04974/extracted/6341787/figures/example_trins.png",
                "caption": "Figure 13:Benchmark data examples from TRINS. The grounded bounding boxes have already been visualized in the original image for better illustration.",
                "position": 3686
            }
        ]
    },
    {
        "header": "Appendix FBenchmark Data Examples",
        "images": []
    }
]