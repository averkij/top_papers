[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06176/x1.png",
                "caption": "",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2411.06176/x2.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2411.06176/x3.png",
                "caption": "",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2411.06176/x4.png",
                "caption": "",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06176/extracted/5987183/assets/documodal-data-pie.png",
                "caption": "Figure 1:Data distribution of document topics\nin our M-LongDoc benchmark.",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2411.06176/extracted/5987183/assets/documodal-data-pie.png",
                "caption": "Figure 1:Data distribution of document topics\nin our M-LongDoc benchmark.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2411.06176/x5.png",
                "caption": "Figure 3:Example questions in different multimodal document question answering benchmarks. For illustration, we include content from the relevant page in the original document.\nThe example question from M-LongDoc is more complex than those from other benchmarks, as it requires an explanatory answer rather than an extraction of a short text span. Furthermore, it requires the model to understand the semantics of both image and text.\nPlease note that in our benchmark setting, the model is provided with all page contents from the document, and not only the relevant page.",
                "position": 274
            }
        ]
    },
    {
        "header": "2M-LongDoc Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06176/x6.png",
                "caption": "Figure 4:Overview of our data construction process with question verification stages. For brevity, we shorten the checklist prompts and include the full details in AppendixA.1.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2411.06176/x7.png",
                "caption": "Figure 5:Our automated evaluation framework to assess the correctness of open-ended solutions for multimodal question answering. The full evaluation guide is included in AppendixA.3.",
                "position": 351
            }
        ]
    },
    {
        "header": "3Retrieval-Aware Multimodal Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06176/x8.png",
                "caption": "Figure 6:Our retrieval-aware multimodal tuning framework to enhance the ability of models to identify and utilize pertinent content in multimodal documents. At training time, the model is provided with more relevant pages retrieved from the document, which may contain both the gold evidence page and multiple ‘distractor’ pages.",
                "position": 584
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06176/extracted/5987183/assets/two_charts_understanding_example.png",
                "caption": "Table 6:An example of a challenging question from M-LongDoc that requires the model to compare the trends of two charts in a document.",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2411.06176/extracted/5987183/assets/retrieve_aware_tuning_better_example.png",
                "caption": "Table 7:Sample answers generated by Qwen2-VL and Qwen2-VL w/ Retrieval-aware Tuning, respectively.",
                "position": 1824
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]