[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02857/x1.png",
                "caption": "",
                "position": 72
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02857/x2.png",
                "caption": "Figure 2:Comparison between AnyI2V and previous methods, DragAnything[49], DragNUWA[53], and MOFA[28]. ‚ÄòFirst Frame*‚Äô indicates that the condition images for previous methods are generated using AnyI2V to ensure a more consistent and fair comparison.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02857/x3.png",
                "caption": "Figure 3:Overview of Our Pipeline:\nOur pipeline begins by performing DDIM inversion on the conditional image. To do this, we remove the temporal module (i.e., temporal self-attention) from the 3D UNet and then extract features from its spatial blocks at timesteptŒ±subscriptùë°ùõºt_{\\alpha}italic_t start_POSTSUBSCRIPT italic_Œ± end_POSTSUBSCRIPT. Next, we optimize the latent representation by substituting the features from the first frame back into the U-Net. This optimization is constrained to a specific region by an auto-generated semantic mask (detailed inFig.6) and is only performed for timestepst‚Ä≤‚â§tŒ≥superscriptùë°‚Ä≤subscriptùë°ùõæt^{\\prime}\\leq t_{\\gamma}italic_t start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚â§ italic_t start_POSTSUBSCRIPT italic_Œ≥ end_POSTSUBSCRIPT.",
                "position": 135
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02857/x4.png",
                "caption": "Figure 4:The study examines the influence of injecting different features. Each feature is injected independently. The input text prompt is ‚ÄúA sculpture of a horse in the musuem.‚Äù",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x5.png",
                "caption": "Figure 5:This figure compares different PCA-reduced features in terms of their temporal consistency and entity representation. The results reveal several key observations: (1) Features of attention map exhibit low temporal consistency. (2) Features of residual hidden state fail to treat the object as a coherent entity. (3) The attention query provides both high temporal consistency and strong entity representation.",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x6.png",
                "caption": "Figure 6:The process of aligning the object across frames by optimizing the latent noise while incorporating semantic masks.",
                "position": 295
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02857/x7.png",
                "caption": "Figure 7:This picture demonstrates AnyI2V‚Äôs ability to control diverse conditions. AnyI2V can not only handle modalities that ControlNet does not support but also effectively control mixed modalities, which previously required additional training by other methods.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x8.png",
                "caption": "Figure 8:The line chart in the figure demonstrates the ablation study on the impact of different PCA reduction dimensions on FID, FVD, and ObjMC, while the histogram presents the ablation study on optimization targets for ObjMC.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x9.png",
                "caption": "Figure 9:The ablation study examines feature injection across time steps. Visual results are from the first output frames.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x9.png",
                "caption": "Figure 9:The ablation study examines feature injection across time steps. Visual results are from the first output frames.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x10.png",
                "caption": "Figure 10:The ablation study examines operations on the residual hidden state. Visual results are from the first output frames.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2507.02857/x11.png",
                "caption": "Figure 11:The generalization test results of AnyI2V on other backbones, including Lavie[44]and VideoCrafter2[5].",
                "position": 501
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]