[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20715/x1.png",
                "caption": "Figure 1:Performance of MUSEG-7B on varioustemporal grounding(Charades-STA, THUMOS14 and THUMOS15) and broadertime-sensitive video understanding(E.T. Bench Subset) tasks.",
                "position": 124
            },
            {
                "img": "https://arxiv.org/html/2505.20715/x2.png",
                "caption": "Figure 2:An example comparing our MUSEG-7B with previous models. MUSEG-7B performs more precise, timestamp-aware reasoning by leveraging multiple key temporal cues to derive the correct answer.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20715/x3.png",
                "caption": "Figure 3:Overview of MUSEG. (a) Our proposed segment matching reward (up) and timestamp reward (down). (b) RL-based training process with phased rewards of MUSEG.",
                "position": 185
            }
        ]
    },
    {
        "header": "3Preliminaries: Reward Design in GRPO",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20715/x4.png",
                "caption": "Figure 4:Cases of MUSEG-7B on multi-segment grounding (in domain) and referred action recognition (out of domain) tasks.",
                "position": 739
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20715/x5.png",
                "caption": "Figure 5:Segment matching reward (a) w/o local matching, (b) w/ local matching (sequential), and (c) w/ local matching (maximum). (d) Evolution of numbers of predicted segments during training process. For all the plots, we only consider queries whose groundtruths are more than one segments.",
                "position": 818
            }
        ]
    },
    {
        "header": "6Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20715/x6.png",
                "caption": "Figure 6:(a) Model performance with different training recipes. For the setting of phased rewards recipe, we train models with timestamp reward for 300 steps when total steps are 600 and 700, for 400 steps when total steps are 800 and 900. (b) Model performance when we vary number of steps with timestamp reward, keeping total steps to be 900. For all the experiments, we report average score of Charades-STA, THUMOS14 and THUMOS15 as in-domain score, and average score of E.T. Bench (Subset) as out-of-domain score.",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2505.20715/x7.png",
                "caption": "Figure 7:Rewards with different training recipes. We also report timestamp reward during training.",
                "position": 943
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BBaselines",
        "images": []
    }
]