[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19304/figures/gen_ppl_mdlm.png",
                "caption": "Figure 1:Unconditional gen PPL measured with GPT-2 Large (sentence entropy in parentheses).",
                "position": 119
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Loopholing Discrete Diffusion Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19304/x1.png",
                "caption": "Figure 2:Architectural comparison of standard discrete diffusion models and the Loopholing Discrete Diffusion Models (LDDMs).(a)The standard architecture of discrete diffusion.(b)During inference, LDDMs propagate the continuous latent representationùê°s\\mathbf{h}_{s}to the subsequent step, creating a deterministic pathway that preserves rich contextual information.(c)During training, LDDMs employ a self-conditioning strategy: a first pass generates a pseudo-contextùê°0\\mathbf{h}^{0}, which is then used to condition the second pass.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.19304/x2.png",
                "caption": "Figure 3:Illustration of thesampling wallin Masked Diffusion Models (MDMs), which induces two distinct failure modes. (1)Steps without Progress: Fixing on a single token can cause the input sequence to remain static across multiple denoising steps, leading to significant computational inefficiency. (2)Excessive Oscillations: Sampling a low-probability token (e.g., ‚Äúloud‚Äù) can trigger excessive oscillations in subsequent steps.",
                "position": 316
            }
        ]
    },
    {
        "header": "4Discussion: Why Loopholing Works",
        "images": []
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.19304/x3.png",
                "caption": "Figure 4:(a)Unconditional generative perplexity measured using GPT-2 Large, with values in parentheses indicating the sentence entropy of the generated samples.(b)Evaluation of generation quality for consistency and naturalness using G-eval framework, rated by GPT-4.1 on a 0-10 scale.(c)The optimal value of the self-conditioning rate (pp) that yields the lowest zero-shot perplexity for each dataset.",
                "position": 522
            },
            {
                "img": "https://arxiv.org/html/2510.19304/x4.png",
                "caption": "Figure 5:(a)Generative perplexity across varying latent propagation steps.(b)KL divergence (log-scale) between the predicted token distribution at each stepttand the distribution from 20 steps prior (t‚àí20t{-}20) during the generation.(c)Entropy of the predicted token distributions throughout the generation.",
                "position": 594
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUse of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BUniform Diffusion Models (UDMs)",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    },
    {
        "header": "Appendix EImplementation Pseudo-Code",
        "images": []
    },
    {
        "header": "Appendix FSamples",
        "images": []
    }
]