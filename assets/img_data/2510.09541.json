[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09541/x1.png",
                "caption": "Figure 1:Test accuracy of SPG and baseline methods on four mathematical and logical reasoning benchmarks. All methods are evaluated with a generation length of 256 in 128 denoising steps. Full results are provided inTableÀú1.",
                "position": 236
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09541/iclr2026/images/main.png",
                "caption": "Figure 2:The training process of SPG for MDLM.Left:From a promptùíÑ\\bm{c}, we generate responses{ùíôj}j=1g\\{\\bm{x}^{j}\\}_{j=1}^{g}. We then maximize a lower bound on the likelihoodœÄùúΩ‚Äã(ùíôj‚à£ùíÑ)\\pi_{\\bm{\\theta}}(\\bm{x}^{j}\\mid\\bm{c})for high-reward responses while minimizing an upper bound for low-reward ones.Right:The upper/lower bound of likelihood is estimated via Monte Carlo using a block-wise masking strategy, where a random block is selected for masking, with earlier blocks kept clean and later blocks fully masked.\nThe example shows a sequence of length 9 with a block size of 3, where the current generation block is highlighted in yellow.",
                "position": 251
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Sandwiched Policy Gradient with Evidence Bounds",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09541/x2.png",
                "caption": "Figure 3:Reward dynamics of SPG w/ Mixture during RL training, compared withD1,WD1, and UniGRPO. SPG consistently leads to faster convergence and higher reward level. We report mean and standard deviation over a rolling window of 50 steps.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2510.09541/x3.png",
                "caption": "Figure 4:Reward dynamics of different log-likelihood estimation methods for negative advantage traces on Sudoku. SPG w/ Mixture leads to both fast convergence and high rewards.",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2510.09541/x4.png",
                "caption": "Figure 5:(a)-(d): ablations on the effect ofŒ≤\\betain the upper bound;(e)-(f): ablations on the mixture coefficientœâ\\omega. The best performedŒ≤‚â•1\\beta\\geq 1andœâ‚àà[0,1]\\omega\\in[0,1]are marked by triangle in each setting.",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2510.09541/x5.png",
                "caption": "Figure 6:Ablations on inference strategies, including different combinations of decoding orders (i.e., semi-autoregressive (semi-AR) decoding with varying block sizes and full sequence decoding) and unmasking approaches (i.e., confidence-based and random unmasking). We set generation length to 256 and report the average accuracy across four benchmarks. SPG consistently outperforms all baselines by a large margin across different inference strategies.",
                "position": 907
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABasics of dLLMs",
        "images": []
    },
    {
        "header": "Appendix BEvidence Upper Bound for dLLMs",
        "images": []
    },
    {
        "header": "Appendix CAdditional Analysis on Upper and Lower Bounds",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09541/x6.png",
                "caption": "Figure 7:Dynamics of the gradient norm of models trained with different log-likelihood estimation methods. SPG w/ Mixture achieves lower gradient norm and more stable optimization. We report mean and standard deviation over a rolling window of 50 steps.",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2510.09541/iclr2026/images/elbo_3d.png",
                "caption": "Figure 8:Landscapes of‚àí‚ÑíELBO-\\mathcal{L}_{\\text{ELBO}}and‚àí‚Ñí~EUBO-\\tilde{\\mathcal{L}}_{\\text{EUBO}}for0<a,b<10<a,b<1.‚àí‚Ñí~EUBO-\\tilde{\\mathcal{L}}_{\\text{EUBO}}is flatter among low value regions while sharper among high value regions, making it more suitable for log-likelihood minimization; vice versa for‚àí‚ÑíELBO-\\mathcal{L}_{\\text{ELBO}}.",
                "position": 2301
            },
            {
                "img": "https://arxiv.org/html/2510.09541/iclr2026/images/eubo_3d.png",
                "caption": "",
                "position": 2304
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09541/x7.png",
                "caption": "Figure 9:Dynamics of the effective generation length of SPG during RL training, compared withD1,WD1, and UniGRPO. SPG leads to concise solutions with better token efficiency. We report mean and standard deviation over a rolling window of 50 steps.",
                "position": 2706
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]