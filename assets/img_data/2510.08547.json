[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x1.png",
                "caption": "Figure 1:R2RGen is a simulator-free data generation framework. Given one human-collected demonstration, R2RGen directly parses and edits both pointcloud observations and action trajectories in a shared 3D space.\nR2RGen achieves strong spatial generalization on diverse complex tasks.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x2.png",
                "caption": "Figure 2:Pre-processing results. The 3D scene is parsed into complete objects, environment and robot’s arm. The trajectory is parsed into interleaved motion and skill segments.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2510.08547/x3.png",
                "caption": "Figure 3:The pipeline ofR2RGen. Given processed source demonstration, we backtrack skills and apply group-wise augmentation to maintain the spatial relationships among target objects, where a fixed object set is maintained to judge whether the augmentation is applicable. Then motion planning is performed to generate trajectories that connect adjacent skills. After augmentation, we perform camera-aware processing to make the pointclouds follow distribution of RGB-D camera. The solid arrows indicate the processing flow, while the dashed arrows indicate the updating of fixed object set.",
                "position": 192
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x4.png",
                "caption": "Figure 4:Visualization of our real-world tasks. We show the start and end moments of each task.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2510.08547/x5.png",
                "caption": "Figure 5:Effects of the number of generated demonstrations and source demonstrations on the final performance of R2RGen.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2510.08547/x6.png",
                "caption": "Figure 6:Two implementations ofFilloperation, i.e., shrinking and expanding.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2510.08547/x7.png",
                "caption": "Figure 7:Extension on appearance generalization. The spatial generalization of R2RGen can serve as a foundation to achieve other kinds of generalization with much less data.",
                "position": 496
            }
        ]
    },
    {
        "header": "5Concluding Remark",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x8.png",
                "caption": "Figure 8:The annotation UI. The users first segment all relevant objects in the first frame. Then they clickPlay,StoporRollbackto capture key frames for skill / motion annotation.",
                "position": 1211
            }
        ]
    },
    {
        "header": "Appendix BHardware Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x9.png",
                "caption": "Figure 9:Robot platform overview. We employ two robot platforms: (a) single-arm UR5e system and (b) dual-arm Mobile Aloha system.",
                "position": 1364
            }
        ]
    },
    {
        "header": "Appendix CTasks and Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x10.png",
                "caption": "Figure 10:Protocol for evaluating spatial generalization. We evaluate policies on different robot’s viewpoints, object locations and rotations.\nBlack crosses indicate seen locations (if human demonstrations are sufficient to cover) and red ones denote unseen locations during training.",
                "position": 1487
            }
        ]
    },
    {
        "header": "Appendix DApplication",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08547/x11.png",
                "caption": "Figure 11:Visualization of mobile manipulation results. The policy trained with R2RGen successfully generalizes to different camera views with only one human-collected demonstration.",
                "position": 1536
            }
        ]
    },
    {
        "header": "Appendix EUsage of LLM",
        "images": []
    }
]