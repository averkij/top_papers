[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16428/x1.png",
                "caption": "Figure 1:Illustration of XAttention:XAttention optimizes attention through a three-step process: (Left) Strided Antidiagonal Scoring: Each block (8√ó\\times√ó8 in this example) is scored by summing values along its strided antidiagonals (stride = 4), with red lines generally indicating higher summed values and blue lines lower values. (Middle) Block Selection: High-scoring blocks are selected based on these evaluations. (Right) Block Sparse Attention: Attention is computed only on the selected blocks (red blocks on the right), achieving substantial computational savings. This example uses a sequence length of 24.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16428/x2.png",
                "caption": "Figure 2:XAttention‚Äôs antidiagonal pattern intersects both vertical and slash patterns within a block, enabling efficient detection of these patterns and guiding effective sparse attention computation.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16428/x3.png",
                "caption": "Figure 3:Qualitative comparison of video generation results on the VBench benchmark using the first prompt in the VBench dataset. Rows show frames from videos generated using: (1) Full Attention (baseline), (2) XAttention with no warmup and (œÑùúè\\tauitalic_œÑ= 0.95), (3) XAttention with 5 warmup steps and (œÑùúè\\tauitalic_œÑ= 0.9), and (4) XAttention with 5 warmup steps and (œÑùúè\\tauitalic_œÑ= 0.95). XAttention with warmup achieves high visual fidelity to the full attention baseline.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2503.16428/x4.png",
                "caption": "Figure 4:Speedup comparison of attention methods across context lengths, relative to FlashInfer‚Äôs implementation of FlashAttention. XAttention consistently outperforms other sparse attention methods, achieving up to 13.5x speedup at 256K tokens.",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2503.16428/x5.png",
                "caption": "Figure 5:Breakdown of prefill attention time. Xattention significantly reduces pattern selection time while maintaining density, achieving substantial acceleration compared to existing methods.",
                "position": 808
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]