[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07409/x1.png",
                "caption": "",
                "position": 63
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07409/x2.png",
                "caption": "Figure 2:Pipeline Overview.Given a single-view image of any general object, DIMO firstdistills rich motion priorsfrom video models (Sec.3.1). We then represent each motion as structured neural key point trajectories (Sec.3.2.1). During training, we embed each motion sequence into a latent code in motion latent space andjointly model diverse motion patternsusing a shared motion decoder (Sec.3.2.2). The decoded key point transformations are used to drive canonical 3DGS for 4D optimization with only photometric losses (Sec.3.2.3).",
                "position": 122
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07409/x3.png",
                "caption": "Figure 3:Qualitative Results.During inference, DIMO caninstantlygeneratediverse3D motions and photorealistic 4D contents ina single forward passby sampling from latent space. We render three motions for each case under two views at two novel timestamps.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2511.07409/x4.png",
                "caption": "Figure 4:Visual Comparison on 3D Motion Generation.DIMO can generate diverse and high-fidelity 3D motions, whereas the baseline fails to produce noticeable motions (DG4D-generated kangaroo is a thin slice so the back side appears as a mirror image of the front).",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2511.07409/x5.png",
                "caption": "Figure 5:Visual Comparison on Image-to-4D.DIMO can generate high-quality 4D contents for both synthetic and in-the-wild objects.",
                "position": 331
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.07409/x6.png",
                "caption": "Figure 6:3D Motion Interpolation.We generate new motion by linearly interpolating between two motions sampled from latent space.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2511.07409/x6.png",
                "caption": "Figure 6:3D Motion Interpolation.We generate new motion by linearly interpolating between two motions sampled from latent space.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2511.07409/x7.png",
                "caption": "Figure 7:Language-Guided Motion Generation.We project language into latent code and enable feed-forward 3D motion generation.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2511.07409/x8.png",
                "caption": "Figure 8:Test Motion Reconstruction.The first row is the input video of the test motion, and the rest are our reconstruction results.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Limitations & Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]