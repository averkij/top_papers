[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14161/x1.png",
                "caption": "",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/desktop.png",
                "caption": "",
                "position": 173
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14161/x2.png",
                "caption": "Figure 1:An overview of TheAgentCompany benchmark. It features a reproducible and self-hosted environment, simulated colleagues to test agent communication capabilities, checkpoint and execution-based evaluation, and a set of 175 diverse, realistic and professional tasks in a software engineering company setting.",
                "position": 188
            },
            {
                "img": "https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/browser.png",
                "caption": "Table 1:Comparison of different AI agent benchmarks.Interface: the interface agent has access to;is web browser,is desktop,is API usage,is Python script,is chat platform,is bash terminal.Supported Tasks: tasks in the benchmark,∗*∗indicate tasks with no association with real-world occupations;SErefers to software engineering,HRis human resources,PMis project manager.Checkpoint-based evaluation: if tasks are evaluated at intermediate checkpoints and assigned partial scores.Interact with NPC Agents: If the agent can interact with other NPC agents during task-solving.",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/api.png",
                "caption": "",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/python.png",
                "caption": "",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/social.png",
                "caption": "",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2412.14161/extracted/6080186/figs/icon/terminal-black.png",
                "caption": "",
                "position": 217
            }
        ]
    },
    {
        "header": "2Benchmark Desiderata and Comparison to Other Benchmarks",
        "images": []
    },
    {
        "header": "3TheAgentCompany Environment Setup",
        "images": []
    },
    {
        "header": "4Task Structure",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14161/x3.png",
                "caption": "Figure 2:Example TheAgentCompany workflow illustrating an agent managing a sprint for theRisingWaveproject. The task involves identifying and moving unfinished issues to next sprint cycle, notifying assignees of those issues, running a code coverage script, uploading summarized report to OwnCloud, and incorporating feedback on report from a simulated project manager.",
                "position": 797
            }
        ]
    },
    {
        "header": "5Task Creation",
        "images": []
    },
    {
        "header": "6Baseline Agent",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14161/x4.png",
                "caption": "Figure 3:Overview of OpenHands’ default CodeAct + Browsing agent architecture, the baseline agent used throughout the experiments.",
                "position": 886
            }
        ]
    },
    {
        "header": "7Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14161/x5.png",
                "caption": "(a)Success rate across platforms",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2412.14161/x5.png",
                "caption": "(a)Success rate across platforms",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2412.14161/x6.png",
                "caption": "(b)Success rate across task categories",
                "position": 1435
            }
        ]
    },
    {
        "header": "8Implications and Future Directions",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore TheAgentCompany Environment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14161/x7.png",
                "caption": "Figure 5:Simulated Colleague Communication Example 1 – The agent is tasked with collecting required equipment while adhering to the department’s budget. After calculating that the requested items exceed the budget, the agent negotiates with the simulated colleague to reduce the request, showcasing its ability of effective communication.",
                "position": 2764
            },
            {
                "img": "https://arxiv.org/html/2412.14161/x8.png",
                "caption": "Figure 6:Simulated Colleague Communication Example 2 – The agent is tasked with writing a job description for a new graduate software engineering position. To fulfill the task, the agent communicates with simulated Project Manager to gather requirements. The agent requests the job description template, minimum and preferred qualifications, and the ideal salary range. This interaction evaluates the agent’s ability to gather information systematically and clarify task-related requirements through effective communication.",
                "position": 2767
            },
            {
                "img": "https://arxiv.org/html/2412.14161/x9.png",
                "caption": "Figure 7:Simulated Colleague Communication Example 3 - The agent is tasked with scheduling a meeting between NPCs Emily Zhou and Liu Qiang based on their availability. Emily is available on Wednesday and Thursday, while Liu is only available on Thursday. The agent identifies Thursday as the common free day and successfully proposes a mid-morning slot at 10:30 AM, which both participants confirm. This example highlights the agent’s ability to manage multi-turn conversations, effectively going back and forth between participants to align schedules and finalize a meeting time.",
                "position": 2770
            }
        ]
    },
    {
        "header": "Appendix BAgent-Simulated Colleagues Conversation Examples",
        "images": []
    }
]