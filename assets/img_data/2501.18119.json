[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/intro_f.png",
                "caption": "Figure 1:Illustration of different strategies to integrate KGs with LLMs. (a) The direct method utilizes (sampled) graph structures and semantic text as inputs. (b) Our method for seamlessly integrating KGs with LLMs using learned quantized and discrete codes.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/fb_sta.png",
                "caption": "Figure 2:The statistics of 2-hop sampled neighbors and needed tokens (by LLaMA2) for entities in FB15k-237.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Quantized Representation for KGs",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/arc_f.png",
                "caption": "Figure 3:The overall architecture of our study. (a) is for SSQR learning. (b) is for instruction tuning for KG tasks, where the learned quantized representations serve as features.\nIconsandrepresent the status of the module during training, indicating if it is frozen or being updated, respectively.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/icon/snow.png",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/icon/fire.png",
                "caption": "",
                "position": 165
            }
        ]
    },
    {
        "header": "3Tuning LLMs with SSQR",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/x1.png",
                "caption": "(a)Original text embedding.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x1.png",
                "caption": "(a)Original text embedding.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x2.png",
                "caption": "(b)SSQR.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x3.png",
                "caption": "(c)SSQR w/o GCN.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x4.png",
                "caption": "(d)SSQR w/o semantics.",
                "position": 384
            }
        ]
    },
    {
        "header": "4Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/x5.png",
                "caption": "(a)WN18RR dataset.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x5.png",
                "caption": "(a)WN18RR dataset.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x6.png",
                "caption": "(b)FB15k-237 dataset.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x7.png",
                "caption": "(a)WN18RR dataset.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x7.png",
                "caption": "(a)WN18RR dataset.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x8.png",
                "caption": "(b)FB15k-237 dataset.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/llm_bar2.png",
                "caption": "Figure 7:The impacts of quantized representation for KG link prediction task using LLMs on FB15k-237.",
                "position": 1018
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/x9.png",
                "caption": "Figure 8:Token embedding virtualization in LLMs (WN18RR dataset), where red and blue dots are real word tokens and code tokens, respectively.",
                "position": 1042
            }
        ]
    },
    {
        "header": "6Conclusion and Potential Impacts",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AStatistics of WN18RR Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/wn_sta.png",
                "caption": "Figure 9:The statistics of 2-hop sampled neighbors and needed tokens (by LLaMA2) for entities in WN18RR.",
                "position": 1782
            }
        ]
    },
    {
        "header": "Appendix BBaselines",
        "images": []
    },
    {
        "header": "Appendix CExperimental Details",
        "images": []
    },
    {
        "header": "Appendix DEntropy and Jaccard Distance",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/train_zhexian.png",
                "caption": "Figure 10:The training process of SSQR, where the Hits@1 metric is used to show the model performance.",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2501.18119/extracted/6164369/fig/llm_bar1.png",
                "caption": "Figure 11:The impacts of quantized representation for KG link prediction task using LLMs on WN18RR.",
                "position": 2042
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x10.png",
                "caption": "(a)Original text embedding.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x10.png",
                "caption": "(a)Original text embedding.",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x11.png",
                "caption": "(b)SSQR.",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x12.png",
                "caption": "(c)SSQR w/o GCN.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x13.png",
                "caption": "(d)SSQR w/o semantics.",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2501.18119/x14.png",
                "caption": "Figure 13:Token embedding virtualization in LLMs (FB15k-237 dataset), where red and blue dots are real word tokens and code tokens, respectively.",
                "position": 2080
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experimental Analysis",
        "images": []
    }
]