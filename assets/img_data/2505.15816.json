[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15816/extracted/6463361/Figures/acc_cost_revised.png",
                "caption": "Figure 1:ProxyV retains or increases the fine-grained benchmark performance while effectively reducing the computational cost. ProxyV-L12 and ProxyV-L16 denote applying ProxyV from layers 12 and 16, respectively.",
                "position": 75
            }
        ]
    },
    {
        "header": "2Discover Computation-level Redundancy",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15816/extracted/6463361/Figures/mask_ratio_revised.png",
                "caption": "Figure 2:The relativeScorefinesubscriptScorefine\\rm{Score_{fine}}roman_Score start_POSTSUBSCRIPT roman_fine end_POSTSUBSCRIPTwith different vision attention masking ratios for different LLMs. The computation redundancy begins in the middle to rear part of the LLMs as masking the vision attention does not affect the performance.",
                "position": 126
            }
        ]
    },
    {
        "header": "3A Better Solution with Proxy Vision Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15816/extracted/6463361/Figures/pipeline.png",
                "caption": "Figure 3:Left: the vanilla LMM structure where full vision tokens cause significant computation. Right: the overall pipeline of the proposed ProxyV algorithm. The full vision tokens are first downsampled to obtain a much smaller version that works as proxy vision tokens. The proxy vision tokens participate in the original operations in the decoder layer including the self-attention and the FFNs to obtain useful information at a much lower cost. After this, each original vision token is guided by its spatially corresponding proxy vision token for an update through a lightweight MLP.",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2505.15816/extracted/6463361/Figures/nonspatial.png",
                "caption": "Figure 4:The illustration of the non-spatial ProxyV. Upper part: proxy vision tokens are generated as a weighted combination of full vision tokens through a simple attention operation. Lower part: The previous attention score is reused to splat the proxy vision tokens into guidance for the full vision tokens update. The softmax operations are skipped in the figure.",
                "position": 621
            }
        ]
    },
    {
        "header": "4Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15816/extracted/6463361/Figures/comparison_revised.png",
                "caption": "Figure 5:Cases where token reduction methods fail. Left: Token reduction methods fail to extract the complete dense information accurately. Right: Token reduction methods fail to retain critical visual information when the image contains diverse and dense visual details. In these cases, ProxyV retains all the visual information and successfully extracts the important visual details.",
                "position": 652
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation on More Benchmarks",
        "images": []
    },
    {
        "header": "Appendix BFine-grained Benchmark Details and Full Results",
        "images": []
    }
]