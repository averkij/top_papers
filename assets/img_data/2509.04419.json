[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04419/x1.png",
                "caption": "Figure 1:Illustration of the Unified Policy Gradient Estimator. The “∇\\nabla” in the background of the Likelihood Gradient part refers to the calculation of the gradient with respect to theπθ\\pi_{\\theta}.",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3A Unified View on Post-Training Algorithms",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04419/x2.png",
                "caption": "Figure 2:Pass@kperformance ofHPTagainst baselines on Qwen2.5-Math-7B. The evaluation spans 3 benchmarks, withPass@kvalues estimated via bootstrap sampling from a set of20482048generated solutions per problem.",
                "position": 1210
            },
            {
                "img": "https://arxiv.org/html/2509.04419/x3.png",
                "caption": "Figure 3:GRPO training dynamics of SFT→\\rightarrowGRPO on Qwen2.5-Math-1.5B across 50 training epochs. We visualize the model’s per-question sampling accuracy throughout the training process.",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2509.04419/x4.png",
                "caption": "Figure 4:Performance difference (HPTv.s. SFT→\\rightarrowGRPO) on Qwen2.5-Math-1.5B across 50 training epochs.\nA diverging color scale indicates the advantage: red forHPT, blue for SFT→\\rightarrowGRPO, and white for no difference.",
                "position": 1412
            },
            {
                "img": "https://arxiv.org/html/2509.04419/x5.png",
                "caption": "Figure 5:Validation performance comparisons on Qwen2.5-Math-1.5B across benchmarks.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2509.04419/x6.png",
                "caption": "Figure 6:Dynamic offline data ratio dynamics during training.\nThe offline data ratio is calculated as the proportion of offline training samples relative to the total training data at each step.",
                "position": 1452
            },
            {
                "img": "https://arxiv.org/html/2509.04419/x7.png",
                "caption": "Figure 7:Comparisons of training dynamics across different methods: (left) The entropy measures the diversity of model outputs, indicating exploration behavior; (right) The response length tracks the average length of generated responses.",
                "position": 1469
            },
            {
                "img": "https://arxiv.org/html/2509.04419/x8.png",
                "caption": "Figure 8:Training reward (left) and offline data ratio (right) comparisons across different gate settings on Qwen2.5-Math-1.5B.",
                "position": 1561
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGradient Derivation for Classical Algorithms",
        "images": []
    },
    {
        "header": "Appendix BAdditional Theoretical Details for Section3.2",
        "images": []
    }
]