[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x1.png",
                "caption": "",
                "position": 152
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x2.png",
                "caption": "Figure 2:Demonstration of MIGE as a unified framework for processing multimodal instructions and conditional inputs across diverse tasks and scenarios.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x3.png",
                "caption": "Figure 3:Overall framework of MIGE.MIGE consists of two components: a multimodal encoder for processing multimodal instructions and a transformer-based diffusion model for modeling input-output relationships. The encoder incorporates a feature fusion mechanism to integrate visual and semantic features from reference image.",
                "position": 229
            }
        ]
    },
    {
        "header": "3Unified Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x4.png",
                "caption": "((a))",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x4.png",
                "caption": "((a))",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x5.png",
                "caption": "((b))",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x6.png",
                "caption": "Figure 5:Qualitative comparison for subject-driven image generation (top rows) and instruction-based image editing (bottom rows).We compare the universal model and task-specific models on the two tasks, respectively. The prompts listed in the figure are used for MIGE and vary according to the usage of each model.",
                "position": 318
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x7.png",
                "caption": "Figure 6:Qualitative results on the benchmark for the subject addition and subject replacement.The upper section compares subject addition results, while the lower section compares subject replacement. During testing, the <imagehere> placeholder in the multimodal instruction is replaced according to the image sequence. MIGE demonstrates flexibility in editing and excels in subject preservation ability and input-output consistency.",
                "position": 1151
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x8.png",
                "caption": "Figure 7:The pipeline of benchmark construction.",
                "position": 2103
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x9.png",
                "caption": "Figure 8:The annotation interface for benchmarks is built with Gradio.",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x10.png",
                "caption": "Figure 9:Subject addition examples in our benchmark.",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2502.21291/x11.png",
                "caption": "Figure 10:Subject replacement examples in our benchmark.",
                "position": 2112
            }
        ]
    },
    {
        "header": "Appendix BDataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x12.png",
                "caption": "Figure 11:Composition of training data.",
                "position": 2123
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.21291/x13.png",
                "caption": "Figure 12:Qualitative results of subject-driven image generation (top) , instruction-based image editing (middle), and instruction-based subject-driven image editing (bottom).",
                "position": 2178
            }
        ]
    },
    {
        "header": "Appendix EPrompts",
        "images": []
    }
]