[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10563/extracted/5993817/assets/mega-bench.png",
                "caption": "",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2410.10563/extracted/5993817/assets/teaser_v2.png",
                "caption": "Figure 1:MEGA-Benchcontains 505 multimodal tasks with diverse data sources, input/output formats, and skill requirements. The taxonomy tree guides and calibrates the annotation process.",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MEGA-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10563/x1.png",
                "caption": "Figure 2:MEGA-Bench’s four keyword dimensions and the task-level statistics. The diversity along various dimensions enables fine-grained capability analysis.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x2.png",
                "caption": "Figure 3:The annotation process ofMEGA-Bench. We first propose a “draft” taxonomy tree, and then distribute the meta-nodes to different annotators. We allow the annotators to gradually refine the tree structure as they add new tasks. Each task consists of many examples and has a shared task-level instruction, several per-example questions, and several per-question ground truth answers.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x3.png",
                "caption": "Figure 4:Representative examples forMEGA-Bench’s diverse output formats and the corresponding customized metrics. The outputs are thereal responsesfrom GPT-4o(OpenAI,2024a). We implement robust parsing to extract the final answer from raw responses.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x3.png",
                "caption": "",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x3.png",
                "caption": "",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x4.png",
                "caption": "",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x5.png",
                "caption": "",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x6.png",
                "caption": "",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x7.png",
                "caption": "",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x7.png",
                "caption": "",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x8.png",
                "caption": "",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x9.png",
                "caption": "",
                "position": 273
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x10.png",
                "caption": "",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10563/x11.png",
                "caption": "Figure 5:Fine-grained breakdown analysis of flagship models on four dimensions. From top-left to bottom-right: input format, output format, skills, and application.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x12.png",
                "caption": "Figure 6:Fine-grained analysis of efficiency models on input format (left) and application (right).",
                "position": 881
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x13.png",
                "caption": "Figure 7:(Left) The bootstrap distribution of benchmark scores of a subset of models as we gradually increase the bootstrap sample size of the number of examples per task. We use the results with the CoT prompting. Claude 3.5 Sonnet (1022) and Gemini 1.5 Pro (002) are used in this figure.\n(Right) The task-wise error distribution of GPT-4o (0513) over a subset of 255 Core tasks.",
                "position": 902
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x13.png",
                "caption": "",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x14.png",
                "caption": "",
                "position": 909
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASingle-image Setting: Results and Analyses",
        "images": []
    },
    {
        "header": "Appendix BDetails of Annotation Protocols",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10563/x15.png",
                "caption": "Figure 8:The structure of our task annotation format, which helps coordinate all task annotators and standardize the annotation format.",
                "position": 3810
            },
            {
                "img": "https://arxiv.org/html/2410.10563/extracted/5993817/assets/annotation_tool.png",
                "caption": "Figure 9:A screenshot of our GUI annotation tool.",
                "position": 3816
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x16.png",
                "caption": "Figure 10:Illustrations of our task visualization page.",
                "position": 3868
            }
        ]
    },
    {
        "header": "Appendix CTaxonomy Tree and Multi-dimensional Keywords",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10563/extracted/5993817/tex/figures/annotation_format/Non-CoT.png",
                "caption": "Figure 11:The prompt template structure without Chain-of-Thought (CoT).",
                "position": 4042
            },
            {
                "img": "https://arxiv.org/html/2410.10563/extracted/5993817/tex/figures/annotation_format/CoT.png",
                "caption": "Figure 12:The prompt template structure for the Chain-of-Thought (CoT) setting",
                "position": 4058
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x17.png",
                "caption": "Figure 13:The prompt template structure for LLM-Assisted Metrics",
                "position": 4248
            }
        ]
    },
    {
        "header": "Appendix EComplete Multi-dimensional Breakdown Results",
        "images": []
    },
    {
        "header": "Appendix FDetailed Inspection of Model Behaviours onMEGA-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10563/x18.png",
                "caption": "Figure 14:A sample error case of Coding (subfield: Code Debugging). Source:WebBack to List of Figures",
                "position": 6143
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x19.png",
                "caption": "Figure 15:A sample error case of Coding (subfield: Code Understanding).Source:WebBack to List of Figures",
                "position": 6149
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x20.png",
                "caption": "Figure 16:A sample error case of Coding (subfield: Code Understanding).Source:WebBack to List of Figures",
                "position": 6156
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x21.png",
                "caption": "Figure 17:A sample error case of Information Extraction (subfield: Multimodel QA).Source: MVBench(Li et al.,2024e)and STAR(Wu et al.,2024)Back to List of Figures",
                "position": 6162
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x22.png",
                "caption": "Figure 18:A sample error case of Information Extraction (subfield: Detailed Manual Understanding).Source:WebBack to List of Figures",
                "position": 6170
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x23.png",
                "caption": "Figure 19:A sample error case of Information Extraction (subfield: Search by Attribute without Calculate).Source:WebBack to List of Figures",
                "position": 6176
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x24.png",
                "caption": "Figure 20:A sample error case of Knowledge (subfield: World Knowledge).Source: BIOSCAN-1M(Gharaee et al.,2024)Back to List of Figures",
                "position": 6182
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x25.png",
                "caption": "Figure 21:A sample error case of Knowledge (subfield: World Knowledge). Source: WebBack to List of Figures",
                "position": 6189
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x26.png",
                "caption": "Figure 22:A sample error case of Knowledge (subfield: Art). Source: WebBack to List of Figures",
                "position": 6195
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x27.png",
                "caption": "Figure 23:A sample error case of Mathematics (subfield: Graph Theory).Source:WebBack to List of Figures",
                "position": 6201
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x28.png",
                "caption": "Figure 24:A sample error case of Metrics (subfield: Generated Image Eval).Source:Motion Guidance(Geng & Owens,2024)Back to List of Figures",
                "position": 6208
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x29.png",
                "caption": "Figure 25:A sample error case of Metrics (subfield: Generated Image Eval).Source: EASI-Tex(Perla et al.,2024)Back to List of Figures",
                "position": 6215
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x30.png",
                "caption": "Figure 26:A sample error case of Planning: (subfield: Puzzles and Games).Source: WebBack to List of Figures",
                "position": 6222
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x31.png",
                "caption": "Figure 27:A sample error case of Planning (subfield: Puzzles and Games).Source:WebBack to List of Figures",
                "position": 6229
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x32.png",
                "caption": "Figure 28:A sample error case of Planning (subfield: Reordering).Source:Perception Test(Patraucean et al.,2024)Back to List of Figures",
                "position": 6236
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x33.png",
                "caption": "Figure 29:A sample error case of Perception (subfield: Multimodal Constrained Captioning).Source: WebBack to List of Figures",
                "position": 6243
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x34.png",
                "caption": "Figure 30:A sample error case of Perception (subfield: Visual Recognition).Source:WebBack to List of Figures",
                "position": 6252
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x35.png",
                "caption": "Figure 31:A sample error case of Perception (subfield: Visual Recognition).Source: COCO(Lin et al.,2014)Back to List of Figures",
                "position": 6259
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x36.png",
                "caption": "Figure 32:A sample error case of Perception (subfield: Visual Recognition).Source: CelebA(Liu et al.,2015)Back to List of Figures",
                "position": 6266
            },
            {
                "img": "https://arxiv.org/html/2410.10563/x37.png",
                "caption": "Figure 33:A sample error case of Science (subfield: STEM).Source: SciBench(Wang et al.,2023b)Back to List of Figures",
                "position": 6273
            }
        ]
    },
    {
        "header": "Appendix GDetailed Task Information",
        "images": []
    },
    {
        "header": "Appendix HAuthor Contribution Statement",
        "images": []
    }
]