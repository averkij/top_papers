[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x1.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x2.png",
                "caption": "Figure 2:Overview of Memory-V2V.(a) From an external cache of previously edited videos, only the top-kkmost relevant videos are retrieved and used as memory inputs to ensure cross-iteration consistency.\n(b) Dynamic tokenizers allocate more tokens to highly relevant videos‚Äîpreserving fine details while maintaining an efficient overall token budget.\n(c) Adaptive token merging reduces latency and FLOPs by compressing less informative frames based on their attention-based responsiveness to the target query.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Memory-V2V",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x3.png",
                "caption": "Figure 3:Comparison of different memory encoders on two-turn novel view synthesis.The red-colored box depicts the novel region which are expected to be consistent betweenùíô1{\\boldsymbol{x}}_{1}andùíô2{\\boldsymbol{x}}_{2}.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x4.png",
                "caption": "Figure 4:Long-video editing as multi-turn video editing with Memory-V2V.(a) We extend target videos from existing video editing dataset for Memory-V2V training.\n(b) During inference, the external cacheŒ©\\Omegastores the editing history as source‚Äìtarget video pairs.\nAt theii-th editing turn, relevant memory videos are retrieved based on the similarity between source video segments.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x5.png",
                "caption": "Figure 5:Qualitative results for multi-turn video novel view synthesis.\nCompared to baselines, Memory-V2V (Ours) generates videos from new camera trajectories while maintaining consistency across all previously generated novel regions (e.g., red and blue areas).",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x6.png",
                "caption": "Figure 6:Text-guided long video editing results by iterative multi-turn editing.\nIn contrast to (iterative) LucyEdit[1], Memory-V2V (Ours) consistently adds the same ‚Äòhat‚Äô to a woman (left) and transforms the dark door into the same ‚Äòwhite door‚Äô (right) across all segments.",
                "position": 372
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x7.png",
                "caption": "Figure 7:Ablation on video retrieval and token merging.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x8.png",
                "caption": "Figure 8:Comparison between merging and discarding tokens.",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x9.png",
                "caption": "Figure 9:Computational cost analysis.Adaptive token merging reduces FLOPs and latency by over 30%, with efficiency gains further increasing as the number of memory videos grows.",
                "position": 846
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix APreliminary: Rectified Flow Matching",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x10.png",
                "caption": "Figure 10:Additional comparison of different memory encoders on two-turn novel view synthesis.The red-colored box depicts the novel region which are expected to be consistent betweenùíô1{\\boldsymbol{x}}_{1}andùíô2{\\boldsymbol{x}}_{2}.",
                "position": 930
            }
        ]
    },
    {
        "header": "Appendix BContext Encoder Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x11.png",
                "caption": "Figure 11:Reconstruction results using CUT3R statess.",
                "position": 975
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x12.png",
                "caption": "Figure 12:Novel view synthesis results using LVSM statezz.",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x13.png",
                "caption": "Figure 13:Sphere sampling and Field-of-View visualization.",
                "position": 1025
            }
        ]
    },
    {
        "header": "Appendix CVideo Retrieval Algorithm Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x14.png",
                "caption": "Figure 14:Example video retrieval results using Alg.1.",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x15.png",
                "caption": "Figure 15:Example video retrieval results using Alg2.",
                "position": 1088
            }
        ]
    },
    {
        "header": "Appendix DAdditional Training and Inference Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x16.png",
                "caption": "Figure 16:RoPE assignments to mitigate train-inference gap.",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x17.png",
                "caption": "Figure 17:Computational cost analysis.Our method reduces FLOPs and latency by over 90%, with efficiency gains further increasing as the number of memory videos grows.",
                "position": 1581
            }
        ]
    },
    {
        "header": "Appendix EAdditional Computational Analysis",
        "images": []
    },
    {
        "header": "Appendix FAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x18.png",
                "caption": "Figure 18:Failure case.Memory-V2V exhibits difficulties when the input long video contains multiple shots with large scene transitions.",
                "position": 1601
            }
        ]
    },
    {
        "header": "Appendix GAdditional Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.16296/x19.png",
                "caption": "Figure 19:Additional qualitative results for multi-turn video novel view synthesis.Refer to our project page for video results.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2601.16296/x20.png",
                "caption": "Figure 20:Additional qualitative results for multi-turn video novel view synthesis.Refer to our project page for video results.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2601.16296/figs/suppl/suppl_qual_edit.png",
                "caption": "Figure 21:Additional qualitative results for text-guided long video editing.Refer to our project page for video results.",
                "position": 1634
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]