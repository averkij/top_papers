[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIPrelimnaries",
        "images": []
    },
    {
        "header": "IVDepth Foundation Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18923/x1.png",
                "caption": "Figure 2:Overview of the self-supervised pretraining of DeFM.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x2.png",
                "caption": "Figure 3:Visualization of different depth input normalization methods. Log normalization effectively captures the overall depth range while preserving fine-grained structure in near-field regions (b, d, f). In contrast, standard metric normalization yields weaker contrast and poorly separated gradients (c, e). In our representation, we stack columns (b), (d), and (f) to form a 3-channel normalized depth input, which preserves metric depth while maintaining robustness across diverse domains.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x3.png",
                "caption": "Figure 4:Overview of distilling the DeFM-L/14 teacher into CNN backbones. A BiFPN module is added on top of the CNN encoder to produce dense spatial features, which are supervised using the teacher’s spatial tokens. The teacher’s class token provides global supervision to the CNN’s pooled feature representation.",
                "position": 817
            }
        ]
    },
    {
        "header": "VExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18923/x4.png",
                "caption": "Figure 5:PCA visualization of the patch features obtained from the DeFM-L/14 encoder when processing depth images of various cups captured by different sensors. The first three PCA components are mapped to the RGB color channels for visualization. Notice the feature consistency of the cup handle (visualized in yellow) across all images, demonstrating that DeFM learns a useful prior for a robotic grasping task. The background is removed by thresholding the first PCA component.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x5.png",
                "caption": "Figure 6:Qualitative results of semantic segmentation on different datasets.",
                "position": 1290
            }
        ]
    },
    {
        "header": "VIRobotic Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18923/x6.png",
                "caption": "Figure 7:(a) Shows the training environment, while (b-d) represent the test environments for embodiment-aware navigation.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x7.png",
                "caption": "Figure 8:Split of navigation failure count (↓\\downarrow) for embodiment aware navigation evaluation across the test environments.",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x8.png",
                "caption": "Figure 9:Real world deployments using a Unitree B2W robot in diverse environments- (A) Indoor, (B) Urban, (C) Park, (D) Construction Site.",
                "position": 1716
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x9.png",
                "caption": "Figure 10:Real world deployment in a park environment with a goal point at around90m90\\text{\\,}\\mathrm{m}.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2601.18923/",
                "caption": "Figure 11:Rollout of DeFM model for dexterous grasping using the Kuka-Allegro setup. Bottom: The simulated noisy depth images used during training.",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2601.18923/figures/dextrah/depth-3/08_t0035.png",
                "caption": "",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2601.18923/figures/dextrah/depth-3/08_t0090.png",
                "caption": "",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2601.18923/figures/dextrah/depth-3/08_t0150.png",
                "caption": "",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x13.png",
                "caption": "Figure 12:Depth images input to the dexterous grasping policy. During training, images are augmented with speckles, dropout, and stick noise. For evaluations, we also consider depth images augmented with the Kinect noise model[108].",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x14.png",
                "caption": "Figure 13:PCA visualization for real-world images from different scenes of the DROID dataset[97].",
                "position": 1839
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x15.png",
                "caption": "Figure 14:ANYmal climbing a ladder, shown together with noisy depth image (top row) from onboard camera.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2601.18923/x16.png",
                "caption": "Figure 15:PCA visualization for real-world images collected during deployment of ANYmal ladder-climbing in the wild.",
                "position": 1868
            }
        ]
    },
    {
        "header": "VIIConclusion and Future Work",
        "images": []
    },
    {
        "header": "ACKNOWLEDGMENT",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]