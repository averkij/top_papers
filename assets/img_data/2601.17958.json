[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17958/github.png",
                "caption": "",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17958/tensorLens_1.png",
                "caption": "Figure 1:Transformers are re-formulated as data-controlled linear operators, characterized by an input-dependent high-order attention tensorùíØ\\mathcal{T}. This formulation enables a unified self-attention representation that captures the entire Transformer architecture, including sub-components such as FFN layers, normalization, embedding layers, and residual connections.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": []
    },
    {
        "header": "3Method: TensorLens",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17958/tensorLensMethod_1.png",
                "caption": "Figure 2:Method:A schematic visualization of our method, where each sub-component of the transformer architecture, including self-attention, LayerNorm, FFNs, input and output embedding layers, and the residual connection (which is omitted here for simplicity), is formulated as a data-control linear operator represented by high-order tensor in‚ÑùL√óD√óL√óD\\mathbb{R}^{L\\times D\\times L\\times D}. These tensors are composed into per-block tensorsùíØ(n)\\mathcal{T}^{(n)}for each layern‚àà[N]n\\in[N], which are then used to construct the final linear operator representing the entire Transformer.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2601.17958/x1.png",
                "caption": "Figure 3:Perturbation Tests in Vision:Effect of perturbations on final hidden representations of DeiT-Base. Measured by the mean squared error between the last hidden-state of the [CLS] token in the original and perturbed input (higher is better).",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2601.17958/x2.png",
                "caption": "Figure 4:Perturbation Tests in NLP:Effect of token perturbations on final hidden representations of BERT-Base.",
                "position": 603
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17958/x3.png",
                "caption": "(a)Bias",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2601.17958/x3.png",
                "caption": "(a)Bias",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2601.17958/x4.png",
                "caption": "(b)Common Sense",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2601.17958/x5.png",
                "caption": "(c)Factual",
                "position": 641
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Perturbation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.17958/x6.png",
                "caption": "Figure 6:Perturbation Tests in NLP:Effect of token perturbations on final hidden representations of RoBERTa-Base. Measured by the mean squared\nerror between the last hidden-state of the [CLS] token in the original and perturbed input (higher is better)",
                "position": 1256
            },
            {
                "img": "https://arxiv.org/html/2601.17958/x7.png",
                "caption": "Figure 7:Perturbation Tests in Vision:Effect of perturbations on final hidden representations of DeiT-Small.",
                "position": 1260
            }
        ]
    },
    {
        "header": "Appendix BTensor Derivation with Biases",
        "images": []
    },
    {
        "header": "Appendix CMemory-Efficient Tensor Computation",
        "images": []
    },
    {
        "header": "Appendix DRelation Decoding Experiment",
        "images": []
    },
    {
        "header": "Appendix ETensor Approximation Error Bound",
        "images": []
    }
]