[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09641/x1.png",
                "caption": "Figure 1:(a) Our SANA-Sprint accelerate the inference speed for generating 1024√ó\\times√ó1024 images, achieving a remarkable speedup from FULX-Schnell‚Äôs 1.94 seconds to only 0.03 seconds. This represents a 64√ó\\times√óimprovement over the current state-of-the-art step-distilled model, FLUX-Schnell, as measured with a batch size of 1 on an NVIDIA A100 GPU. The ratio is calculated based on Transformer latency.\n(b) Additionally, our model demonstrates efficient GPU memory usage during training, outperforming other distillation methods in terms of memory cost. The GPU memory is measured using official code, 1024√ó\\times√ó1024 images and on a single A100 GPU.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09641/x2.png",
                "caption": "Figure 2:Training paradigm of SANA-Sprint.In SANA-Sprint, we use the student model for synthetic data generation¬†(x0^^subscriptùë•0\\hat{x_{0}}over^ start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG) andJVPcalculation, and we use the teacher model for velocity¬†(d‚Å¢x/d‚Å¢tdùë•dùë°\\mathrm{d}x/\\mathrm{d}troman_d italic_x / roman_d italic_t) compute and its feature for the GAN loss, which allows us train sCM and GAN together and have only one training model purely in the latent space. Details of training objective andTrigFlow Transformationare inEq.9,Eq.11and Sec.3.1.",
                "position": 238
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09641/x3.png",
                "caption": "Figure 3:Efficient Distillation via QK Normalization, Dense Timestep Embedding, and Training-free Schedule Transformation.(a) We compare gradient norms and visualizations with/without QK Normalization, showing its stabilizing effect.\n(b) Gradient norm curves for timestep scales (0‚àºsimilar-to\\sim‚àº1 vs. 0‚àºsimilar-to\\sim‚àº1000) highlight impacts on stability and stability and quality.\n(c) PCA-based similarity analysis of timestep embeddings.\n(d) Image results after 5,000 iterations of fine-tuning with (left) and without (right) the proposed schedule transfer (Sec.3.1).",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x4.png",
                "caption": "Figure 4:Visual comparison among SANA-Sprint and selected competing methods in different inference steps.‚Ä†¬†indicates that distinct models are required for different inference steps, and time below the method name is the latency of 4 steps tested on A100 GPU. SANA-Sprint produces images with superior realism and text alignment in all inference steps with the fastest speed.",
                "position": 546
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09641/x5.png",
                "caption": "Figure 5:Visual comparison among SANA-Sprint with different inference steps and the teacher model SANA.SANA-Sprint can generate high-quality images with one or two steps and the images can be better when increasing steps.",
                "position": 1225
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix APseudo Code for Training-Free Transformation from Flow to Trigflow",
        "images": []
    },
    {
        "header": "Appendix BTransformation Algorithm",
        "images": []
    },
    {
        "header": "Appendix CTraining Algorithm of SANA-Sprint",
        "images": []
    },
    {
        "header": "Appendix DProof of Proposition 3.1",
        "images": []
    },
    {
        "header": "Appendix EFull Related Work",
        "images": []
    },
    {
        "header": "Appendix FMore Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09641/x6.png",
                "caption": "Figure 6:Inference timesteps search.This figure illustrates the performance of timesteps search for achieving optimal results during inference with 0.6B and 1.6B models. The subplots compare FID¬†(top row) and CLIP-Score¬†(bottom row) across different timesteps for 1-step, 2-step, and 4-step inference settings. The x-axis represents the timestep being searched at the current step; for multi-step settings (e.g., 4 steps), the timesteps for earlier steps are fixed to their previously optimized values.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x6.png",
                "caption": "Figure 6:Inference timesteps search.This figure illustrates the performance of timesteps search for achieving optimal results during inference with 0.6B and 1.6B models. The subplots compare FID¬†(top row) and CLIP-Score¬†(bottom row) across different timesteps for 1-step, 2-step, and 4-step inference settings. The x-axis represents the timestep being searched at the current step; for multi-step settings (e.g., 4 steps), the timesteps for earlier steps are fixed to their previously optimized values.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x7.png",
                "caption": "Figure 7:Controlling the sCM noise distribution.This figure compares FID and CLIP-Score across different noise distribution settings over 40k training steps in sCM-only experiments. The green curve(Pmean,Pstd)=(0.0,1.6)subscriptùëÉmeansubscriptùëÉstd0.01.6(P_{\\text{mean}},P_{\\text{std}})=(0.0,1.6)( italic_P start_POSTSUBSCRIPT mean end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT std end_POSTSUBSCRIPT ) = ( 0.0 , 1.6 )demonstrates optimal performance, achieving stable training dynamics and superior generation quality.",
                "position": 1927
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x8.png",
                "caption": "Figure 8:Controlling the LADD noise distribution.We vary the parameters of a logit-normal distribution for biasing the sampling of the LADD teacher noise level. When biasing towards very high noise levels (m = 0.4, s = 2), we observe unstable training.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x8.png",
                "caption": "Figure 8:Controlling the LADD noise distribution.We vary the parameters of a logit-normal distribution for biasing the sampling of the LADD teacher noise level. When biasing towards very high noise levels (m = 0.4, s = 2), we observe unstable training.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x9.png",
                "caption": "Figure 9:Visualization of SANA-Sprint-ControlNet‚Äôs capabilities.The model outputs high-quality images of 1024√ó\\times√ó1024 pixels in only2 stepsand0.3 secondson an NVIDIA H100 GPU. The process involves processing the input image (first column) to extract a scribe graph, which, along with a prompt, generates an image (second column). The blended image (third column) highlights precise boundary alignment and control, demonstrating the model‚Äôs robust control capabilities.",
                "position": 2022
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x10.png",
                "caption": "Figure 10:ControlNet Demo: Hand-Crafted Scribble to Stunning Image.Left: A hand-crafted scribble created with a brush.Right: The result generated by the Sana-Sprint-ControlNet model, strictly following the scribble and prompt.Inference Latency: The model achieves remarkable speed, generating the 1024√ó\\times√ó1024 images in only1 stepand0.25 secondson H100 GPU, as shown in the right red box. This demo showcases the model‚Äôs exceptional control and efficiency, adhering closely to the user‚Äôs input while producing visually appealing results.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2503.09641/x11.png",
                "caption": "Figure 11:Generated images with SANA-Sprint.The model outputs high-quality images of 1024√ó\\times√ó1024 pixels in2 stepsand0.24 secondson an NVIDIA A100 GPU, showcasing comprehensive generation capabilities with high-fidelity details and accurate text rendering, handling diverse scenarios with robust image quality.",
                "position": 2037
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]