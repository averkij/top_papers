[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10106/x1.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIEgoHumanoid Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10106/x2.png",
                "caption": "Figure 2:Hardware setup for data collection.Humans and the\nG1 humanoid robot are equipped with an integrated VR-based system for portable usage and agile development. The same camera captures egocentric recordings. The VR headset and trackers provide coarse human poses, while the VR controller is employed to teleoperate the robot.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2602.10106/x3.png",
                "caption": "Figure 3:Pipeline of human-to-humanoid alignment.(a) View Alignment: Egocentric images are transformed to approximate robot viewpoints by reprojecting estimated depth points and generative inpainting to fill in blank holes.(b) Action Alignment: We employ relative end-effector poses to unify the upper body action space, and discrete commands for lower-body locomotion.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2602.10106/x4.png",
                "caption": "Figure 4:Humanoid loco-manipulation tasks for evaluation.We design four tasks which span varying levels of difficulty for large-space movement and dexterous manipulation.\nRobots are teleoperated in laboratories as the source domain (Top).\nHuman-centric scenes\noccur\nin human demonstrations only(Middle), and set as testbeds for generalization evaluation (Bottom).",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2602.10106/x5.png",
                "caption": "Figure 5:Performance of human-robot data co-training withEgoHumanoid.Our pipeline achieves unanimous improvements over robot-only baselines across in-domain and generalized environments. The boost is\namplified in robots’ unseen settings.",
                "position": 329
            }
        ]
    },
    {
        "header": "IVEvaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10106/x6.png",
                "caption": "Figure 6:Failure mode analysis.For each setting—(a) Robot-only, (b) Human-only, and (c) Co-training—we log all failed episodes. The Sankey diagrams break failures down into locomotion vs. manipulation errors and finer causes.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2602.10106/x7.png",
                "caption": "Figure 7:Scaling human demonstrations with various training-time data sampling strategies.More human demonstrations, from robot-only (0 Human Demos) to 3×\\timesrobot data (300 Human Demos), principally yield improved loco-manipulation performance in generalized scenarios. The optimal sampling ratio depends on task characteristics: manipulation-heavy tasks favor higher robot data ratios, while navigation-dominant tasks benefit from more human data.",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2602.10106/x8.png",
                "caption": "Figure 8:View alignment ablation.View alignment is essential for effective human-robot co-training, especially for tasks involving variable object heights such as Toy Transfer.",
                "position": 714
            }
        ]
    },
    {
        "header": "VDiscussions",
        "images": []
    },
    {
        "header": "VIConclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10106/x9.png",
                "caption": "Figure 9:Qualitative results of the view alignment pipeline.For each task, we show the original human egocentric image, the MoGe-estimated depth map, the reprojected image after point cloud transformation to the robot camera frame, and the final inpainted result. The pipeline produces a consistent downward viewpoint shift that approximates the robot’s lower camera height, with inpainting filling disoccluded regions to yield complete RGB observations.",
                "position": 2339
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]