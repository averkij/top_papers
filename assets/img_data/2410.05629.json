[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05629/x1.png",
                "caption": "Figure 1:Comparing regular in-context learning to vector in-context learning.(a) In regular ICL, textual demonstrations are given as context during LLM inference.\n(b) In Vector-ICL, the input space is extended across multiple modalities.\nThe input data is first encoded as embeddings, then transformed into continuous vectors which represent as box tokens (□□\\Box□) via embedding projection.\nDuring inference, we provide box tokens in prompts as demonstrations for ICL.\nWe consider box tokens representing text, numerical data, brain fMRI, time series, and graphs in this study.",
                "position": 205
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05629/x2.png",
                "caption": "Figure 2:Pretraining and finetuning the projectors.Vector-ICL requires updating the parameters of a lightweight projector while keeping the encoder and decoder parameters fixed.\nThe encoder first compresses the input into single token embeddings, and then the projector will project it to the aligned representation space for LLMs’ later use.\n(a) Pretraining the projector on a general language modeling corpus (or a modality-to-text dataset) enables Vector-ICL.\n(b) Task-specific fine-tuning makes Vector-ICL outperform few-shot ICL on natural language tasks, as well as with domain-specific models on non-language tasks.",
                "position": 248
            }
        ]
    },
    {
        "header": "3Method: vector context via embedding projection",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05629/x3.png",
                "caption": "Figure 3:Main results: LLMs can perform Vector-ICL (↑↑\\uparrow↑= better).We show that training the embedding projector with a simple next-token prediction objective enables Vector-ICL.\nFine-tuning the projector on downstream tasks further improves the use of continuous context, surpassing the performance of few-shot ICL and domain-specific baseline models.\nThe study begins with text reconstruction to assess LLMs’ ability to interpret box token embeddings, followed by function regression to evaluate reasoning capabilities.\nWe then demonstrate Vector-ICL’s effectiveness and applicability across various downstream tasks, including text classification, summarization, time-series classification, graph classification, and brain fMRI decoding & classification.\nResults in each panel are averaged over different encoders and LLMs for the diverse tasks we study; error bars show 95% confidence intervals.",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experimental setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/nvd.png",
                "caption": "Table 1:Overview of the tasks, datasets, encoders, large language models, and prompts used in the experiments.Each task utilizes encoders and LLMs to perform functions across multiple modalities. The table highlights the diversity of models and configurations applied to each task.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/sfr.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/stella.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/gtr.png",
                "caption": "",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/llama.png",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/circled-m.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/crown.png",
                "caption": "",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/yi.png",
                "caption": "",
                "position": 417
            }
        ]
    },
    {
        "header": "5Results: unlocking versatile applications across modalities",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05629/x4.png",
                "caption": "(a)",
                "position": 879
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x4.png",
                "caption": "(a)",
                "position": 887
            },
            {
                "img": "https://arxiv.org/html/2410.05629/extracted/5903061/img/digits_projector_heatmap.png",
                "caption": "(b)",
                "position": 896
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x5.png",
                "caption": "(c)",
                "position": 905
            }
        ]
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05629/x6.png",
                "caption": "Figure 5:Text Reconstruction",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x6.png",
                "caption": "",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x7.png",
                "caption": "",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x8.png",
                "caption": "Figure 6:Function Regression - 10 digits (upper) and 3 Digits (lower)",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x8.png",
                "caption": "",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x9.png",
                "caption": "",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x10.png",
                "caption": "Figure 7:Text Classification - Pretrained Projectors",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x11.png",
                "caption": "Figure 8:Text Classification - Finetuned Projectors",
                "position": 2077
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x12.png",
                "caption": "Figure 9:Text Classification - Few-shot ICL",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x13.png",
                "caption": "Figure 10:Text Summarization - Pretrained Projectors",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2410.05629/x14.png",
                "caption": "Figure 11:Text Summarization - Finetuned Projectors",
                "position": 2086
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]