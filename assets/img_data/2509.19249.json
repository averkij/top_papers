[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19249/x1.png",
                "caption": "Figure 1:Scaling law of RLPT performance on various benchmarks with respect to training tokens.",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Reinforcement Learning on Pre-Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19249/x2.png",
                "caption": "Figure 2:Overview of RLPT. Raw data from internet corpora is processed into training samples of the form(s<i,si,si+1)(s_{<i},s_{i},s_{i+1}). During the reinforcement pre-training stage, the policy LLM predictss^i\\hat{s}_{i}conditioned ons<is_{<i}(ASR) or on(s<i,si+1)(s_{<i},s_{i+1})(MSR). The prediction is then compared withsis_{i}to compute the reward.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19249/x3.png",
                "caption": "Figure 3:Comparative scaling properties of RLVR and RLPT++RLVR.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2509.19249/x4.png",
                "caption": "Figure 4:Comparison between Strict Reward and Prefix Reward: (a) Training Reward, (b) Response Length, (c) Validation Performance (Pass@11).",
                "position": 574
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]