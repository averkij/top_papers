[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x1.png",
                "caption": "Figure 1:Multimodal embedding optimization via Embedder-Guided Reinforcement Learning (EG-RL). (a) Frameworks evolution. (b) Reasoning enhancement with RL-optimized evidential Traceability CoT (T-CoT). (c) Comparison of multi-task performance.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x2.png",
                "caption": "Figure 2:Overview of the proposed data synthesis and EG-RL framework. (a) Data Construction generates T-CoT annotations for query-positive pairs, filters and splits the dataset to enable contrastive and reinforcement learning, laying the groundwork for reasoning-aware embedding. (b) Embedder-Guided Reinforcement Learning finetunes the MLLM with a process-outcome reward function, encouraging T-CoT trajectories that yield more discriminative and beneficial generative embeddings.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x3.png",
                "caption": "Table 1:Comparison of performance between baselines and our method on MMEB-V2. CLS: classification, QA: question answering, RET: retrieval, GD: grounding, MRET: moment retrieval, VDR: ViDoRe, VR: VisRAG, OOD: out-of-domain. The highest and second-highest values are highlighted inboldandunderline.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x3.png",
                "caption": "Figure 3:Example visualization of our reasoning-driven embedding framework on multimodal retrieval tasks. The figure shows the evidential Traceability CoT reasoning process for video and visual document retrieval.",
                "position": 851
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x4.png",
                "caption": "Figure 4:Similarity differenceΔ​s=sim​(query,top1)−sim​(query,top2)\\Delta s=\\text{sim}(\\text{query},\\text{top1})-\\text{sim}(\\text{query},\\text{top2})before and after EG-RL.\nHere,sim​(⋅,⋅)\\text{sim}(\\cdot,\\cdot)denotes cosine similarity of normalized embeddings,top1is the most similar positive candidate andtop2the second-most similar.\nThis metric quantifies the model’s discriminative ability over similar candidates on multimodal datasets.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x5.png",
                "caption": "Figure 5:Relationship between traceable evidence counts and retrieval metrics across datasets.\nHit@1 is employed for Image and Video;\nNDCG@5 is used for VisDoc.\nBounding box counts are shown for Image and VisDoc, while keyframe counts for Video.",
                "position": 1162
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix 0.AAdditional details",
        "images": []
    },
    {
        "header": "Appendix 0.BTraining Details",
        "images": []
    },
    {
        "header": "Appendix 0.CDetailed Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x6.png",
                "caption": "Table 5:Statistics of Initial Sampling and CoT-guided Filtering.",
                "position": 1297
            }
        ]
    },
    {
        "header": "Appendix 0.DDetailed Scores of MMEB-V2",
        "images": []
    },
    {
        "header": "Appendix 0.EDetailed Scores of MMEB-V1",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x6.png",
                "caption": "Table 9:Results on the MMEB-V1 benchmark (consisting of 36 image embedding tasks). IND and OOD denote the in-distribution and out-of-distribution datasets, respectively. The highest and second-highest values are highlighted inboldandunderline.",
                "position": 2797
            }
        ]
    },
    {
        "header": "Appendix 0.FPrompt for synthesizing multimodal chain of thought",
        "images": []
    },
    {
        "header": "Appendix 0.GComprehensive Performance Characterization on Video Retrieval",
        "images": []
    },
    {
        "header": "Appendix 0.HTraining Trajectory Dynamics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x6.png",
                "caption": "Figure 6:Key RL-phase metrics of Embedder-Guided RL (entropy, response length, reward).",
                "position": 4596
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x7.png",
                "caption": "Figure 7:Contrastive training loss and gradient norm of 2B and 4B scale Embed-RL models.",
                "position": 4605
            }
        ]
    },
    {
        "header": "Appendix 0.IEfficiency and Latency",
        "images": []
    },
    {
        "header": "Appendix 0.JLimitations",
        "images": []
    },
    {
        "header": "Appendix 0.KExploratory Perspectives",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13823/x8.png",
                "caption": "Figure 8:Example 1 of T-CoT Before and After EG-RL.",
                "position": 4642
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x9.png",
                "caption": "Figure 9:Example 2 of T-CoT Before and After EG-RL.",
                "position": 4645
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x10.png",
                "caption": "Figure 10:Example 3 of T-CoT Before and After EG-RL.",
                "position": 4648
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x11.png",
                "caption": "Figure 11:Example 4 of T-CoT Before and After EG-RL.",
                "position": 4651
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x12.png",
                "caption": "Figure 12:Example 5 of T-CoT Before and After EG-RL.",
                "position": 4654
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x13.png",
                "caption": "Figure 13:Example 6 of T-CoT Before and After EG-RL.",
                "position": 4657
            },
            {
                "img": "https://arxiv.org/html/2602.13823/x14.png",
                "caption": "Figure 14:Example 7 of T-CoT Before and After EG-RL.",
                "position": 4660
            }
        ]
    },
    {
        "header": "Appendix 0.LComparative Examples of T-CoT Before and After Embedder-Guided RL",
        "images": []
    }
]