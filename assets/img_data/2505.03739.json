[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03739/x1.png",
                "caption": "(a)The visualization of the attention maps between text tokens and audio tokens.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2505.03739/x1.png",
                "caption": "(a)The visualization of the attention maps between text tokens and audio tokens.",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2505.03739/x2.png",
                "caption": "(b)The transcription results of the generated audio into text under different attention masks.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03739/x3.png",
                "caption": "Figure 2:Architecture overview. (a) VITA-Audio is an end-to-end large speech model equipped with 10 light-weight Multiple Cross-modal Token Prediction (MCTP) modules that enable speech generation with extremely low latency. As shown inFig.1, we observe that the hidden states of certain text tokens in the LLM backbone contain sufficient semantic information for generating the corresponding audio tokens, which means that it is unnecessary to attend to additional text tokens when generating audio. Thus, we propose to utilize a set of light-weight MCTP modules to model the mapping from LLM hidden states to the audio tokens. (b) The details of the MCTP modules. Our MCTP module has a light-weight architecture, which enables it to finish one forward pass within0.00240.00240.00240.0024seconds (11111111% of the LLM backbone). The MCTP module is capable of generating 10 audio tokens from the LLM hidden states and the text embedding, and the generated audio tokens can be decoded by the audio decoder directly. The utilization of MCTP modules enables VITA-Audio to generate audio responses in one LLM forward pass, which achieves extremely fast generation speed.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2505.03739/x4.png",
                "caption": "Figure 3:Training pipeline of VITA-Audio.\nThe first stage (Audio-Text Alignment) enhances the LLM by extending its audio modeling capability through large-scale speech pre-training.\nThe second stage (Single MCTP module Training) connects an MCTP module with the LLM to predict one subsequent token based on the input tokens and the LLMâ€™s hidden states.\nThe third stage (Multiple MCTP Modules Training) increases the number of MCTP modules in the model to predict more tokens in each model forward.\nThe last stage (Supervised Fine-tuning) provides the speech-to-speech capability to the model by optimizing it on the large-scale speech QA dataset.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2505.03739/x5.png",
                "caption": "Figure 4:The four text-audio interleaved inference modes are illustrated as follows:\n1) Turbo:\nAs the fastest inference mode, it generates1111token by the main model and10101010additional tokens via MCTP in each forward pass.\nTo ensure that a valid audio chunk is decoded after the first forward pass, the first generated11111111tokens are split into1111text token and10101010audio tokens.\nThen, the Turbo mode iteratively generates4444text tokens and10101010audio tokens in the following forward.\n2) Boost:\nTo enhance the quality of text tokens, Boost mode follows the text-audio cyclic pattern of Turbo mode, with the main model generating every text token and MCTP generating every audio token.\n3) Balance:\nTo keep a balanced text-audio ratio,i.e.,1:2:121:21 : 2, the balance mode further changes the text-audio cyclic pattern of the Boost mode.\nSpecifically, the balance mode sequentially generates1111text token from the main model,4444audio tokens (2222tag tokens mark the beginning and end of audios, and2222common tokens denote the audio content) from MCTP,3333text tokens from the main model,8888text tokens (2222tag tokens mark the beginning and end of audios, and6666common tokens denote the audio content) from MCTP, and then iteratively generates4444text tokens from the main model and10101010audio tokens (2222tag tokens mark the beginning and end of audios, and8888common tokens denote the audio content) from MCTP.\n4) Vanilla:\nAs the slowest inference mode, Vanilla mode follows the text-audio cyclic pattern of Balance mode, with the main model generating every token.",
                "position": 597
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03739/x6.png",
                "caption": "Figure 5:Token generation speed curves of four text-audio interleaved modes.",
                "position": 1349
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]