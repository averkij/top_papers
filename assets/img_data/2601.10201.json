[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10201/x1.png",
                "caption": "Figure 1:PRL workflow demonstration. For each prompt and response trajectory(x,a)(x,a)witha=[a1,a2,⋯,aL]a=[a^{1},a^{2},\\cdots,a^{L}], we could split the reasoning response into several intermediate steps (by fixed length, newline symbol, etc.) and calculate the process reward as the entropy ratio between the current policyπω\\pi_{\\omega}and reference policyπ0\\pi_{0}. The final process reward for each stepiiis the combination of ① the entropy ratio∑j=1plog⁡πω​(aj|x,a(j−1))π0​(aj|x,a(j−1))\\sum_{j=1}^{p}\\log\\frac{\\pi_{\\omega}(a^{j}|x,a^{(j-1)})}{\\pi_{0}(a^{j}|x,a^{(j-1)})}and ② the final outcome rewardr∗​(x,a)r^{*}(x,a).",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Processing Reward Learning from KL Regularized RL",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10201/x2.png",
                "caption": "Figure 2:The training dynamics of KL loss and entropy loss with Qwen2.5-Math-7B as the base model under different configurations.",
                "position": 658
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix BMore Experiments Details",
        "images": []
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]