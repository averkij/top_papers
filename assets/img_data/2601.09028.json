[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09028/x1.png",
                "caption": "Figure 1.Comparison between the existing decoding LLMs that use their default probability distribution and our proposed approach that modifies the distribution by leveraging external explicit relevance signals.",
                "position": 190
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.OpenDecoder",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09028/x2.png",
                "caption": "Figure 2.The framework ofOpenDecoder, including Searching External Information with top-k retrieved documents, Indicators Construction based on the retrieved documents with various types of quality scores, teaching the model to leverage external explicit quality indicators for the Decoding Computation of LLM by modulating internal attention score computation and applying Robust Training, and finally obtaining the reshaped token probability distribution during content generation.",
                "position": 264
            }
        ]
    },
    {
        "header": "4.Experimental Setup",
        "images": []
    },
    {
        "header": "5.Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09028/x3.png",
                "caption": "Figure 3.Performance of aggregating various scores as guidance features across different evaluation settings and datasets.",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2601.09028/x4.png",
                "caption": "Figure 4.Performance of normalizing scores features with various approaches across different evaluation settings and datasets.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2601.09028/x5.png",
                "caption": "Figure 5.The performance of using various top-k retrieved documents in the normal evaluation setting.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2601.09028/x6.png",
                "caption": "Figure 6.Comparison between SFT andOpenDecoderof scaling model size across five datasets in the noisy evaluation setting.",
                "position": 1068
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADatasets Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09028/x7.png",
                "caption": "Figure 7.Comparison between SFT andOpenDecoderof scaling model size across five datasets in the normal evaluation setting.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2601.09028/x8.png",
                "caption": "Figure 8.Comparison between SFT andOpenDecoderof scaling model size across five datasets in the extreme noisy evaluation setting.",
                "position": 1908
            }
        ]
    },
    {
        "header": "Appendix BBaseline Details",
        "images": []
    },
    {
        "header": "Appendix CMore Results on Model Scaling",
        "images": []
    },
    {
        "header": "Appendix DDiscussion on Time and Space Efficiency",
        "images": []
    }
]