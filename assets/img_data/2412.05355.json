[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/extracted/6051329/figs/logo.png",
                "caption": "",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2412.05355/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/x2.png",
                "caption": "Figure 2:Our intuition.Visualization of motion characteristicsℳ⁢(z)ℳ𝑧\\mathcal{M}(z)caligraphic_M ( italic_z )extracted from early-timestep conditional scores.(Left)Multiple object motion representation showing the simultaneous movement of two objects.(Right)Combined object and camera motion representation demonstrating how our method captures both local object motion and global camera movement patterns. The visualizations are obtained from the conditional score maps∇ztlog⁡pt⁢(z|y)subscript∇subscript𝑧𝑡subscript𝑝𝑡conditional𝑧𝑦\\nabla_{z_{t}}\\log p_{t}(z|y)∇ start_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z | italic_y )at early timestepst≪Tmuch-less-than𝑡𝑇t\\ll Titalic_t ≪ italic_T.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/x3.png",
                "caption": "Figure 3:Method Overview.Framework of our Mixture of Score Guidance (MSG) for zero-shot motion transfer in diffusion models.Left:Reference motion extraction stage captures motion characteristicsM⁢(z)𝑀𝑧M(z)italic_M ( italic_z )from early-timestep conditional scores∇zlog⁡p⁢(z(1)|y(1))subscript∇𝑧𝑝conditionalsuperscript𝑧1superscript𝑦1\\nabla_{z}\\log p(z^{(1)}|y^{(1)})∇ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT roman_log italic_p ( italic_z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT | italic_y start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT )and∇zlog⁡p⁢(z(2)|y(2))subscript∇𝑧𝑝conditionalsuperscript𝑧2superscript𝑦2\\nabla_{z}\\log p(z^{(2)}|y^{(2)})∇ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT roman_log italic_p ( italic_z start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT | italic_y start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT ).Middle:Motion transfer combines content and motion scores through our MSG formulationsMSG⁢(zt,zt∗)=∇zlog⁡pt⁢(z|y)+wMSG⁢(∇zlog⁡pt⁢(z∗|y∗)−∇zlog⁡pt⁢(z))subscript𝑠MSGsubscript𝑧𝑡superscriptsubscript𝑧𝑡subscript∇𝑧subscript𝑝𝑡conditional𝑧𝑦subscript𝑤MSGsubscript∇𝑧subscript𝑝𝑡conditionalsuperscript𝑧superscript𝑦subscript∇𝑧subscript𝑝𝑡𝑧s_{\\text{MSG}}(z_{t},z_{t}^{*})=\\nabla_{z}\\log p_{t}(z|y)+w_{\\text{MSG}}(%\n\\nabla_{z}\\log p_{t}(z^{*}|y^{*})-\\nabla_{z}\\log p_{t}(z))italic_s start_POSTSUBSCRIPT MSG end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) = ∇ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z | italic_y ) + italic_w start_POSTSUBSCRIPT MSG end_POSTSUBSCRIPT ( ∇ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT | italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) - ∇ start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z ) ).Right:MSG path redirection mechanism showing attention-guided dynamics that enable stable motion transfer by exploring the correct motion manifold while preserving content through modified Langevin dynamics governed by our mixture of potential energiesUMSG⁢(zt)=Ucontent⁢(zt)+wMSG⁢[Umotion⁢(zt,zt∗)−Uprior⁢(zt)]subscript𝑈MSGsubscript𝑧𝑡subscript𝑈contentsubscript𝑧𝑡subscript𝑤MSGdelimited-[]subscript𝑈motionsubscript𝑧𝑡superscriptsubscript𝑧𝑡subscript𝑈priorsubscript𝑧𝑡U_{\\text{MSG}}(z_{t})=U_{\\text{content}}(z_{t})+w_{\\text{MSG}}[U_{\\text{motion%\n}}(z_{t},z_{t}^{*})-U_{\\text{prior}}(z_{t})]italic_U start_POSTSUBSCRIPT MSG end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_U start_POSTSUBSCRIPT content end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_w start_POSTSUBSCRIPT MSG end_POSTSUBSCRIPT [ italic_U start_POSTSUBSCRIPT motion end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) - italic_U start_POSTSUBSCRIPT prior end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ].",
                "position": 186
            }
        ]
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/x4.png",
                "caption": "Figure 4:Qualitative results demonstrating our method’s ability to preserve motion priors while generating novel content from text prompts.(Left)Single-object motion transfer where complex motions like mechanical movements, horseback riding sequences are accurately preserved in the generated outputs.(Right)Multi-object scenarios where our method successfully maintains the original motion dynamics while generating diverse subjects. Please refer to the Supplementary Material for full videos and additional examples.",
                "position": 365
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Qualitative Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/x5.png",
                "caption": "Figure 5:Qualitative comparison of motion transfer capabilities.We compareMotionShop(bottom row) with existing methods (VMC, DMT, MD, MI) on three challenging scenarios. Left: Single object motion transfer of a robot-driven motorcycle in a desert scene. Middle: Multiple object motion transfer involving miniature medieval knights, demonstrating the ability to preserve interactions between objects. Right: Camera motion transfer capturing the dynamic perspective of a raindrop on a leaf. Our method demonstrates superior motion-text alignment across all three motion transfer categories.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2412.05355/x6.png",
                "caption": "Figure 6:Trade-off Analysis between Text Similarity and Motion Fidelity.Comparison of our method against baselines shows superior performance in both metrics, with our approach (green star) achieving higher motion fidelity (0.913) while maintaining competitive text similarity (0.314).",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2412.05355/x7.png",
                "caption": "Figure 7:Camera Motion Transfer Results Across Diverse Scenarios.Each row shows the camera trajectory (left) and corresponding input-output image sequences. Our method can transfer camera motions while maintaining spatial consistency, as demonstrated in various cases: a steampunk clockwork butterfly animation, a raindrop on a leaf, an eagle soaring through mountain peaks, and dominos falling on a rail track. The colored trajectories represent the camera path through 3D space, with different colors indicating temporal progression.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2412.05355/x8.png",
                "caption": "Figure 8:Ablation study on strength and timestep parameters.Left: We analyze the effect of noise addition in the motion extraction stage, where strength=0.7 achieves optimal motion representation - lower values (0.6) result in weak motion transfer while higher values (0.8) lead to over-stylization. Right: Impact of applying Mixture of Score guidance at different timestep ratios of total 50 timesteps on motion transfer quality.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2412.05355/x9.png",
                "caption": "Figure 9:Comparison of different guidance mechanisms.Comparing our Mixture of Score Guidance (MSG) against Classifier-Free Guidance (CFG, baseline without reference) and Unconditional Score Guidance (USG, using reference video’s unconditional score).",
                "position": 515
            }
        ]
    },
    {
        "header": "7Qualitative Comparisons",
        "images": []
    },
    {
        "header": "8Quantitative Experiments",
        "images": []
    },
    {
        "header": "9Discussion on Quantitative Experiments",
        "images": []
    },
    {
        "header": "10MotionBench Dataset",
        "images": []
    },
    {
        "header": "11Limitation and Societal Impact.",
        "images": []
    },
    {
        "header": "12Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/x10.png",
                "caption": "Figure 10:Type of Questions.We ask 3 different questions for Text Alignment, Motion Fidelity and Temporal Consistency.",
                "position": 1121
            }
        ]
    },
    {
        "header": "AUser Study Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05355/x11.png",
                "caption": "Figure 11:User Study Interface.Given a reference video we ask for 3 different type of questions with 5 different options including DMT, MI, MD, VMC and MotionShop results.",
                "position": 1328
            }
        ]
    },
    {
        "header": "BMotionBench: A Comprehensive Motion Transfer Dataset",
        "images": []
    }
]