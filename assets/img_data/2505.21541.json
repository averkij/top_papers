[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21541/x1.png",
                "caption": "Figure 1:We propose a novel generative task, Layer-Wise Decomposition of Alpha-Composited Images, to recover constituent layers from single overlapped images under the condition of semi-transparent or transparent layer non-linear occlusion.\nWe introduce the AlphaBlend dataset, the first large-scale dataset for transparent and semi-transparent layer decomposition to support six real-world subtasks. (a) shows generation results on alpha layer removal (I–II), semi-transparent and transparent layer separation (III–IV), and complex non-linear alpha-blend decomposition (V–VI). (b) highlights the dataset’s broad coverage across categoriese.g.,flare, fog, glassware, X-ray contraband.",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset and Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21541/x2.png",
                "caption": "Figure 2:The comparison of conventional inpainting methods with our proposed DiffDecompose. The conventional inpainting approach (blue background) relies on predefined object masks and predicts missing regions, often causing semantic errors in transparent scenes. In contrast, DiffDecompose (green background) conditions on the full composited image and jointly predicts foreground and background via layer-level decomposition. This formulation removes the need for explicit masks and enables more accurate separation in the presence of transparency and complex blending.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x3.png",
                "caption": "Figure 3:The DiffDecompose framework comprises two steps: (1) VAE encodes the source image into a condition token and concatenates it with a noisy latent token, controlling the generation of layer decomposition. (2) In-Context Decomposition constructs an image-conditioned base model to decompose multiple layers. Among them, the LPEC clones the position encoding to ensure the alignment between different layers, and MMAttention are introduced to process the multi-modulation features.",
                "position": 232
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21541/x4.png",
                "caption": "Figure 4:Our DiffDecompose shows impressive layer-level decomposition results of the image. It can solve the layer-level decomposition (i.e., Subtask II and Subtask III), the nonlinear alpha-blend layer removal and decomposition (i.e., Subtask I and Subtask VI), and the semi-transparent/transparent object-level decomposition (i.e., Subtask IV and Subtask VI), demonstrating its generalization and application in various scenarios.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x5.png",
                "caption": "Figure 5:Qualitative comparisons between our method and other methods.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x6.png",
                "caption": "Figure 6:The visualization of LPEC ablation. Full settings ensure the information alignment of different layers, while removals degrade performance.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x6.png",
                "caption": "Figure 6:The visualization of LPEC ablation. Full settings ensure the information alignment of different layers, while removals degrade performance.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x7.png",
                "caption": "Figure 7:The visualization of ICD ablation. It shows that only the full setting enables effective decomposition.",
                "position": 804
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlphaBlend dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21541/x8.png",
                "caption": "Figure 8:The presentation of the six subtasks’ dataset. Each foreground has its respective properties.",
                "position": 1875
            }
        ]
    },
    {
        "header": "Appendix BTraining Loss Function",
        "images": []
    },
    {
        "header": "Appendix CInference Algorithm",
        "images": []
    },
    {
        "header": "Appendix DMore Comparison Experiments and Generative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21541/x9.png",
                "caption": "Figure 9:The SDXL-inpainting results under different strengths. We input the prompt like \"Remove the light and make the scenario darker\" and \"Remove the foreground rain and fog\", it is difficult for the model to process the layer-level edition.",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x10.png",
                "caption": "Figure 10:Qualitative results of our and compared methods on the proposed AlphaBlend dataset and the publicly LOGO dataset. Inpainting models often replace the object by predicting new pixel information instead of removing it, which fails to generate a realistic background. Our method is the only one that effectively removes objects and fills the regions in a accurate manner.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x11.png",
                "caption": "Figure 11:Additional Qualitative Results for Subtasks I–III. Extended visualizations demonstrating DiffDecompose’s capability in removing complex semi-transparent artifacts, including lens flares, translucent occlusions, and semi-transparent watermarks. The results illustrate faithful layer-wise decomposition, preserving fine details and semantic consistency across diverse real-world scenarios.",
                "position": 2066
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x12.png",
                "caption": "Figure 12:Additional Qualitative Results for Subtasks IV–VI. Extended results showcasing DiffDecompose’s effectiveness in transparent glassware decomposition, X-ray contraband separation, and semi-transparent cell decomposition. The visualizations highlight the framework’s ability to disentangle complex overlapping layers, preserving structural details and semantic fidelity in challenging transparency scenarios.",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x13.png",
                "caption": "Figure 13:User study results. The voting results of DiffDecompose and the baseline method are compared on different subtasks, including removal effectiveness, background integrity, and result plausibility.",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x14.png",
                "caption": "Figure 14:A user study voting interface was provided to participants. We present the performance of five competing methods on the task of watermark removal, cell separation, and glassware removal. The participants can click the alphabet to choose which method is the best and accord with their requirements.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2505.21541/x15.png",
                "caption": "Figure 15:Results of failed decomposition by DiffDecompose.",
                "position": 2096
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Broader Impacts",
        "images": []
    }
]