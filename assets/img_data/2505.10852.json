[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10852/x1.png",
                "caption": "Figure 1:Three LLM responses to scientific knowledge questions and materials tool usage tasks (generating reasonable codes for using materials simulation tools).",
                "position": 145
            }
        ]
    },
    {
        "header": "2MatTools",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10852/x2.png",
                "caption": "Figure 2:Overview of MatTools. The upper half of the schematic represents the pipeline for constructing and applying the QA benchmark, whereRepoAgentextracts code snippets frompymatgenand Gemini-2.0-flash generates documentation.\nGemini (2.0-flash) createspymatgen_code_qaandpymatgen_doc_qabenchmarks using the code snippets and documentation.\nFor testing, LLMs answer questions by selecting one of the four options (A, B, C, or D).\nThe lower half of the schematic illustrates the real-world tool-usage benchmark, whereTree-sitter[30]extracts test functions from unit test files and GPT-4o generates question-property-validation triples.\nCode generated by different LLM-based systems is verified in a secure code sandbox.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x3.png",
                "caption": "Figure 3:Illustration of the five benchmarked LLM-based systems for real-world tool-usage.",
                "position": 273
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10852/x4.png",
                "caption": "Figure 4:Comparison of the performance of different LLMs on the real-world tool-usage benchmark. Error bars indicate standard deviation across three independent experiments. The displayed values represent the mean performance metrics from these trials.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x5.png",
                "caption": "Figure 5:Comparative performance analysis of a single RAG agent using different LLMs and retrieval sources on the real-world tool-usage benchmark.\nRetrieval sources include: (1) pymatgen codebase, (2) pymatgen official document split by recursively looking at characters, (3) LLM-generated document split based on semantic similarity, and (4) LLM-generated document split based on function and class.\nError bars indicate standard deviation across three independent experimental runs; displayed values represent mean performance metrics from these trials.",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x6.png",
                "caption": "Figure 6:Comparative performance analysis of advanced RAG agent systems on the real-world tool-usage benchmark. All systems used GPT-4o as the base model to generate code.",
                "position": 439
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10852/extracted/6444207/Figures/si/repoagent_method.png",
                "caption": "Figure 7:Three steps ofRepoAgent: global structure analysis, documentation generation, and documentation update. This figure is from[31]. We used the first two steps ofRepoAgentto extract the code segments and generate the documentation for the pymatgen software.",
                "position": 1112
            },
            {
                "img": "https://arxiv.org/html/2505.10852/extracted/6444207/Figures/si/repoagent_prompt_template.png",
                "caption": "Figure 8:Prompt template ofRepoAgent. This figure is from[31].",
                "position": 1115
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x7.png",
                "caption": "Figure 9:Example of extracted code segment and generated documentation forpymatgen.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x8.png",
                "caption": "Figure 10:Prompt template for generating the QA pairs forpymatgencode.",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x9.png",
                "caption": "Figure 11:Prompt template for generating the QA pairs ofpymatgendocumentation.",
                "position": 1129
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x10.png",
                "caption": "Figure 12:Prompt template for instructing the LLM to answer questions in the QA benchmark.",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x11.png",
                "caption": "Figure 13:One example of generated QA pairs and testing results from different models. The Darwin 1.5-7B model failed to follow the required format in its response, so we considered it as a failed attempt and marked the answer as incorrect.",
                "position": 1148
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x12.png",
                "caption": "Figure 14:One example of generated QA pairs and testing results from different models. Both the closed-source and materials chemistry LLMs failed to output correct answers. The ChemDFM-v1.5-8B model even generated garbled characters and Chinese text.",
                "position": 1151
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x13.png",
                "caption": "Figure 15:Prompt template for extracting the materials properties from the unit test code.",
                "position": 1252
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x14.png",
                "caption": "Figure 16:Prompt template for proposing the problem statement from the unit test code.",
                "position": 1255
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x15.png",
                "caption": "Figure 17:Prompt template for generating the verification code for the problem statement.",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x16.png",
                "caption": "Figure 18:Example of the problem statement.",
                "position": 1261
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x17.png",
                "caption": "Figure 19:Example of the property dictionary.",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x18.png",
                "caption": "Figure 20:Example of the verification code.",
                "position": 1267
            },
            {
                "img": "https://arxiv.org/html/2505.10852/extracted/6444207/Figures/si/pymatgen_api_overview.png",
                "caption": "Figure 21:Overview of the functions ofpymatgen-analysis-defects[26]package.",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x19.png",
                "caption": "Figure 22:Example of the wrong result for the problem statement shown in Figure18.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2505.10852/x20.png",
                "caption": "Figure 23:Example of the correct result for the problem statement shown in the figure.",
                "position": 2116
            }
        ]
    },
    {
        "header": "Appendix ATechnical appendices",
        "images": []
    }
]