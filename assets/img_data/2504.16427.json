[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16427/x1.png",
                "caption": "Figure 1:Overview of the MMLA benchmark. The left side shows examples from six evaluation dimensions and nine datasets. The right side displays three methods for evaluating both LLMs and MLLMs: (1) zero-shot inference (top right), which generates predictions from task-specific prompts; (2) supervised fine-tuning (middle right), which trains on each supervised task; and (3) instruction tuning (bottom right), which trains on multiple tasks simultaneously. Both (2) and (3) utilize LoRA to efficiently adapt foundation models.",
                "position": 68
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3MMLA Benchmark",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16427/x2.png",
                "caption": "Figure 2:Rank of foundation models after zero-shot inference.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2504.16427/x3.png",
                "caption": "Figure 3:Rank of foundation models after SFT and IT.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2504.16427/x4.png",
                "caption": "Figure 4:Fine-grained zero-shot inference and SFT performance (ACC). Within each bar, the light-colored lower segment corresponds to zero-shot inference performance, while the darker upper segment represents the additional gains from SFT. The performance of SOTA MML methods (if available) and GPT-4o are indicated with purple and green dashed lines, respectively.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2504.16427/x5.png",
                "caption": "Figure 5:Fine-grained performance (ACC) of instruction-tuned MLLMs and LLMs on each dataset across six dimensions. The performance of SOTA MML methods and humans are indicated with dashed lines, if available.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2504.16427/x6.png",
                "caption": "Figure 6:Scalability of Qwen2 and Qwen2-VL on the MMLA benchmark.",
                "position": 473
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALicense",
        "images": []
    },
    {
        "header": "Appendix BUsed Labels for Each Dataset",
        "images": []
    },
    {
        "header": "Appendix CAssurance of Annotation Quality",
        "images": []
    },
    {
        "header": "Appendix DUsed Prompts",
        "images": []
    },
    {
        "header": "Appendix EDetailed Experimental Results",
        "images": []
    }
]