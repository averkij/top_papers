[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18605/x1.png",
                "caption": "Figure 1:Understanding object orientation is essential for spatial reasoning. However, even advanced VLMs like GPT-4o and Gem- ini-1.5-pro are not yet able to resolve the basic orientation issue.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18605/extracted/6093642/annotation.png",
                "caption": "Figure 2:The orientation data collection pipeline is composed of three steps:1) Canonical 3D Model Filtering: This step removes any 3D objects in tilted poses.2) Orientation Annotating: An advanced 2D VLM is used to identify the front face from multiple orthogonal perspectives, with view symmetry employed to narrow the potential choices.3) Free-view Rendering: Rendering images from random and free viewpoints, and the object orientation is represented by the polarŒ∏ùúÉ\\thetaitalic_Œ∏, azimuthalœÜùúë\\varphiitalic_œÜand rotation angleŒ¥ùõø\\deltaitalic_Œ¥of the camera.",
                "position": 199
            }
        ]
    },
    {
        "header": "3Orientation Understanding in 2D VLMs",
        "images": []
    },
    {
        "header": "4Orientation Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18605/extracted/6093642/model.png",
                "caption": "Figure 3:Orient Anything consists of a simple visual encoder and multiple prediction heads. It is trained to judge\nif the object in the input image has a meaningful front face and fits the probability distribution of 3D orientation.",
                "position": 330
            }
        ]
    },
    {
        "header": "5Orient Anything",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18605/extracted/6093642/sigma.png",
                "caption": "Figure 4:Ablation study for hyper-parameterœÉŒ∏subscriptùúéùúÉ\\sigma_{\\theta}italic_œÉ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT,œÉœÜsubscriptùúéùúë\\sigma_{\\varphi}italic_œÉ start_POSTSUBSCRIPT italic_œÜ end_POSTSUBSCRIPTandœÉŒ¥subscriptùúéùõø\\sigma_{\\delta}italic_œÉ start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT.",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2412.18605/extracted/6093642/generation.png",
                "caption": "Figure 5:Generated images with given textual prompt (left two from DALL-E 3[5], right two from FLUX[21]). Accurate orientation estimation is helpful to confirm whether generated contents follow the given orientation or perspective condition.",
                "position": 785
            }
        ]
    },
    {
        "header": "7Applications",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Results on COCO Benchmark",
        "images": []
    },
    {
        "header": "Appendix BVisualization of Real-image Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CMore Visualizations of Images in The Wild",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18605/x2.png",
                "caption": "Figure 6:Qualitative results on COCO",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x3.png",
                "caption": "",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x4.png",
                "caption": "",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x5.png",
                "caption": "",
                "position": 2118
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x6.png",
                "caption": "Figure 7:Qualitative results on SUN RGB-D.",
                "position": 2121
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x7.png",
                "caption": "Figure 8:Qualitative results on KITTI and nuScenes.",
                "position": 2124
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x8.png",
                "caption": "Figure 9:Qualitative results on Objectron.",
                "position": 2127
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x9.png",
                "caption": "Figure 10:Qualitative results on ARKitScenes.",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x10.png",
                "caption": "Figure 11:More visualization of images in the wild.",
                "position": 2133
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x11.png",
                "caption": "",
                "position": 2137
            }
        ]
    },
    {
        "header": "Appendix DVisualization of Ori-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.18605/x12.png",
                "caption": "Figure 12:Visualization and qualitative comparison on theObject Direction Recognitiontask of Ori-Bench.",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x13.png",
                "caption": "",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x14.png",
                "caption": "",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x15.png",
                "caption": "Figure 13:Visualization and qualitative comparison on theSpatial Part Reasoningtask of Ori-Bench.",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x16.png",
                "caption": "",
                "position": 2165
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x17.png",
                "caption": "",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x18.png",
                "caption": "Figure 14:Visualization and qualitative comparison on theSpatial Relation Reasoningtask of Ori-Bench.",
                "position": 2171
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x19.png",
                "caption": "",
                "position": 2175
            },
            {
                "img": "https://arxiv.org/html/2412.18605/x20.png",
                "caption": "",
                "position": 2177
            }
        ]
    },
    {
        "header": "Appendix EOrient Anything for Orientation Understanding",
        "images": []
    },
    {
        "header": "Appendix FPrompts for VLMs",
        "images": []
    }
]