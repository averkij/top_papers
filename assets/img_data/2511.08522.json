[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08522/x1.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x2.png",
                "caption": "Figure 1:Comparison of OpenEvolve (with program-based reward), ShinkaEvolve (with program-based reward) and AlphaResearch (with program-based and peer-review reward). We run three agents on Packing Circles (n=26) problems. AlphaResearch achieves better performance than others.",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08522/x3.png",
                "caption": "Figure 2:The launch of AlphaResearch contains two steps. (1) Train reward models with real-world peer-reviewed records. (2) Prepare initial research proposals, initial programs and evalution program. AlphaResearch will refine the research proposals and programs autonomously.",
                "position": 179
            }
        ]
    },
    {
        "header": "2AlphaResearch",
        "images": []
    },
    {
        "header": "3AlphaResearchComp",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08522/x4.png",
                "caption": "Figure 3:Execution-based reward of AlphaResearch on packing circles (n=26) problem (left) and third autocorrelation inequality problem (right).",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x5.png",
                "caption": "",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x6.png",
                "caption": "Figure 4:The idea comparison between execution-only research agent and AlphaResearch where AlphaResearch-RM-7B are used.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x7.png",
                "caption": "Figure 5:Reward overview during the discovery process. Each action in AlphaResearch will obtain 3 kinds of reward: (1) idea scrapping due to lower RM score than threshold, (2) idea execution successes, and (3) idea execution fails.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x8.png",
                "caption": "Figure 6:The impact of real-world peer review environment on execution results. AlphaResearch-RM-7B filters 151 bad ideas where 108 ideas fail to executed and 43 successes.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x9.png",
                "caption": "Figure 7:We show an example of a formatted task of AlphaResearch.",
                "position": 711
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExamples",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.08522/x10.png",
                "caption": "Figure 8:New construction of AlphaResearch (right) improving the best known AlphaEvolve (right) bounds on packing circles to maximize their sum of radii. Left: 32 circles in a unit square with sum of radii≥\\geq2.9379. Right: 32 circles in a unit square with sum of radii≥\\geq2.9395",
                "position": 1282
            },
            {
                "img": "https://arxiv.org/html/2511.08522/x11.png",
                "caption": "",
                "position": 1285
            }
        ]
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    },
    {
        "header": "Appendix CThe Use of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix DCurated Problems and Human-Best Values",
        "images": []
    }
]