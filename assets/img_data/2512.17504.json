[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17504/x1.png",
                "caption": "Figure 2:InsertAnywhere is a two-stage VOI framework that uses 4D scene reconstruction to generate a user controllable and geometrically consistent mask, which then conditions a diffusion model fine-tuned on ROSE++ to synthesize illumination-aware object insertions.",
                "position": 163
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17504/x2.png",
                "caption": "Figure 3:ROSE++ dataset generation pipeline: reference images are produced by the VLM-based object retrieval process.[1]",
                "position": 283
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17504/x3.png",
                "caption": "Figure 4:Qualitative comparison between closed-source commercial video generative tools and ours. Best viewed when zoomed in.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x4.png",
                "caption": "Figure 5:Ablation results. We performed an ablation study by sequentially adding each of the proposed components. Best viewed when zoomed in.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x5.png",
                "caption": "Figure 6:Ablation result of finetuning with ROSE++ datasets. Best viewed when zoomed in.",
                "position": 505
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17504/x6.png",
                "caption": "Figure 7:Comparison before and after the application of Section3.2.3",
                "position": 1218
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x7.png",
                "caption": "Figure 8:Prompt for object retrieval in constructing the ROSE++ dataset",
                "position": 1275
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x8.png",
                "caption": "Figure 9:Kling / Pika-Pro prompt:“Using the context of the video, seamlessly place the image into the empty space of the cart.”",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x9.png",
                "caption": "Figure 10:Kling / Pika-Pro prompt:“Using the context of the video, seamlessly place the image into the empty space of the shoe rack.”",
                "position": 1296
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x10.png",
                "caption": "Figure 11:Kling / Pika-Pro prompt:Top:“Using the context of the video, seamlessly place the image into the empty space of the bed. Bottom:“Using the context of the video, seamlessly place the image into the empty space of the kitchen table.”",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x11.png",
                "caption": "Figure 12:Kling / Pika-Pro prompt:“Using the context of the video, seamlessly place the image next to the pillar.”",
                "position": 1303
            },
            {
                "img": "https://arxiv.org/html/2512.17504/x12.png",
                "caption": "Figure 13:Kling / Pika-Pro prompt:“Using the context of the video, seamlessly place the image into the empty space on the table in front of the mirror.”",
                "position": 1306
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]