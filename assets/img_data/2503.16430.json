[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16430/x1.png",
                "caption": "Figure 1:Comparison of different autoregressive visual generation approaches.(a) Traditional discrete tokenization incorporate quantization during training, resulting in tokenizer training instability and limited vocabulary size that restricts representational capacity. (b) Hybrid continuous AR models preserve rich visual information but need complex distribution modeling (diffusion or GMM) beyond standard categorical prediction. (c) Our approach bridges these paradigms by applying post-training quantization to pretrained continuous features, maintaining the high representational capacity of continuous tokens while enabling simple autoregressive modeling.",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2503.16430/x2.png",
                "caption": "Figure 2:Generated samples from TokenBridge.Class-conditional generation results on ImageNet[6]256√ó256 demonstrating fine details and textures across diverse categories including animals, food, objects, and scenes.",
                "position": 77
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16430/x3.png",
                "caption": "Figure 3:Illustration of our post-training quantization process.The top row shows the pretrained continuous VAE tokenizer, mapping an input image to continuous latent featuresùëø‚àà‚ÑùH√óW√óCùëøsuperscript‚Ñùùêªùëäùê∂{\\bm{X}}\\in\\mathbb{R}^{H\\times W\\times C}bold_italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W √ó italic_C end_POSTSUPERSCRIPTand reconstructing it through the decoder. Our post-training quantization process (middle) transforms these continuous features into discrete tokens by independently quantizing each channel dimension. The bottom-left shows how our approach preserves the original Gaussian-like distribution (purple curve) in discretized form (purple histogram). The right portion demonstrates the de-quantization process that maps indices back to continuous values for decoding.",
                "position": 128
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16430/x4.png",
                "caption": "Figure 4:Our autoregressive generation process.At the spatial level, our model autoregressively generates tokens conditioning on previous positions. For each spatial location (highlighted in pink), we apply dimension-wise sequential prediction to efficiently handle the large token space. This approach decomposes the modeling of each token into a series of smaller classification problems while preserving essential inter-dimensional dependencies.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16430/x5.png",
                "caption": "Figure 5:Reconstruction quality of typical continuous and discrete tokenizers.For discrete baselines, we use VQ from[35], and LFQ from[19]. Our method achieves reconstruction quality comparable to continuous VAE, preserving more fine details than traditional discrete tokenizers, especially in text and facial features. Zoom in for better comparison.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2503.16430/x6.png",
                "caption": "Figure 6:Reconstruction quality of different quantization granularities B.Visual comparison showing reconstructions at decreasing quantization levels.\nZoom in for better comparison.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2503.16430/x7.png",
                "caption": "Figure 7:Token Prediction Strategy.Comparison of dimension-wise token prediction approaches.Top: Parallel prediction produces blurry, inconsistent images.Bottom: Our autoregressive approach sequentially predicts token dimensions, generating coherent, high-quality images. This highlights the interdependence of token dimensions and they cannot be predicted independently.",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2503.16430/x8.png",
                "caption": "Figure 8:Generation guided by token confidence.Our discrete token approach enables confidence-guided generation, producing clean foreground objects against simple backgrounds by prioritizing high-confidence tokens. This provides a advantage over continuous tokens, which lack explicit token-level confidence scores.",
                "position": 909
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "AImplementation Details for TokenBridge",
        "images": []
    },
    {
        "header": "BSpeed Comparison of Token prediction",
        "images": []
    },
    {
        "header": "CLimitations and Broader Impacts",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16430/x9.png",
                "caption": "Figure 9:Additional image generation results of TokenBridge across different ImageNet[6]categories.",
                "position": 2003
            }
        ]
    },
    {
        "header": "DMore Visualization Results",
        "images": []
    }
]