[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09443/x1.png",
                "caption": "Figure 1:Florenz (middle) is a vision-language model trained on an incomplete dataset (left) that covers the tasks image captioning (blue) and multimodal machine translation (orange). The incompleteness concerns the availability of languages for the individual tasks, where captioning data is only available for English and German while En→→\\rightarrow→X translation is available in all languages. Florenz generalizes to the missing captioning-language pairs during inference with sufficient scale (right).",
                "position": 73
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09443/x2.png",
                "caption": "Figure 2:Dataset generation pipeline. Input is an image dataset with short captions or alt texts and a translation dataset with bitexts in English and the target language German. (1) Image and short caption are fed into a VLM to generate a detailed English description, which is (2) translated into the target language. (3) The image and (4) all English sentences of the translation dataset are embedded in a shared vector space. (5) Cosine similarity is calculated and (6) top-N matching pairs the most similar images and translations.",
                "position": 158
            }
        ]
    },
    {
        "header": "4Multimodal Multilingual Dataset Creation",
        "images": []
    },
    {
        "header": "5Florenz Model Family",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09443/x3.png",
                "caption": "Figure 3:Test cross-entropy loss (CE) for various training compute budgets (GMACs, Giga Multiply-Accumulate operations), showing the effects of different model sizes and seen training samples. We show results for the test splits (seeSec.4.1) for unseen captioning (UC) in Spanish (Es) and Chinese (Zh), seen translation (ST) in these languages, and seen captioning (SC) in English (En) and German (De). The models F-0.4B, F-1.0B, F-3.5B, and F-11.2B are trained for 500, 2k, 5k, and 10k steps, respectively, resulting in 0.5M to 10M seen samples with a batch size of 1024.Eq.1is fitted to the points on the Pareto frontier (gray staircase graph). Higher compute budgets improve CE for UC (left), ST (middle), and SC (right). This suggests that translation facilitates generalization in captioning.",
                "position": 355
            }
        ]
    },
    {
        "header": "6Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09443/x4.png",
                "caption": "Figure 4:Predicted test cross-entropy loss (CE) as function of model parameters in billion (B) with confidence intervals (CI) and prediction intervals (PI) for unseen captioning in Spanish and Chinese (UC, blue), seen translation in Spanish and Chinese (ST, orange), and seen captioning in English and German (SC, green). The number of seen training samples is fixed to 10M, respective measurements are shown as dots. Extrapolation is drawn dashed.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2503.09443/x5.png",
                "caption": "Figure 5:Effect of adding a prefix (Fr: ”La photo montre”, Es: ”La imagen muestra”, etc.) to the decoder input to unlock zero-shot captioning. Tested on the image captioning dataset XM3600[62]in the unseen languages Fr, Es, Ru, and Zh. The mean CIDEr[64]over unseen languages significantly improves with the prefix.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2503.09443/x6.png",
                "caption": "Figure 6:Scaling laws for fine-tuned models on different downstream tasks. First row: Multi30K translation to De and Fr measured in BLEU (Task 1; mean over Test2016, Test2017 and AmbiguousCOCO splits), CoMMuTE translation and disambiguation for En→→\\rightarrow→De and En→→\\rightarrow→{De, Fr, Ru, Zh} measured in BLEU and accuracy, respectively. Second row: Captioning tasks measured with CIDEr: COCO Karpathy (En), Multi30K (En, De) (Task 2, Test2016), and XM3600 for En, De and unseen languages (Fr, Es, Ru, Zh). UC and ST exhibit stronger scaling laws than tasks for languages already known to the pre-trained LLM (En) or those with complete task coverage (De).",
                "position": 571
            }
        ]
    },
    {
        "header": "7Supervised Fine-tuning",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFlorenz Model Overview",
        "images": []
    },
    {
        "header": "Appendix BEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09443/x7.png",
                "caption": "Figure 7:Distribution of caption and translation data with language coverage in our pre-training dataset.",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2503.09443/x8.png",
                "caption": "Figure 8:Distribution of caption and translation data with language coverage in our fine-tuning dataset.",
                "position": 3398
            }
        ]
    },
    {
        "header": "Appendix CDatasets",
        "images": []
    },
    {
        "header": "Appendix DDetailed Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09443/x9.png",
                "caption": "Figure 9:Captions generated with F-11.2B by prompting with theunderlined prefix. F-11.2B has not seen captioning data for Fr, Es, Ru, and Zh. English references are created with DeepL. Photo by Jeanette Dickson.",
                "position": 7449
            },
            {
                "img": "https://arxiv.org/html/2503.09443/x10.png",
                "caption": "Figure 10:Captions generated with F-11.2B by prompting with theunderlined prefix. F-11.2B has not seen captioning data for French (Fr), Spanish (Es), Russian (Ru), and Chinese (Zh). English references are created with DeepL. Photo by Reinaldo Simoes.",
                "position": 7452
            },
            {
                "img": "https://arxiv.org/html/2503.09443/x11.png",
                "caption": "Figure 11:Detailed caption generated with F-11.2B30K ftfor German (De). English reference is created with DeepL. Photo by Jeanette Dickson.",
                "position": 7455
            },
            {
                "img": "https://arxiv.org/html/2503.09443/x12.png",
                "caption": "Figure 12:Detailed caption generated with F-11.2B30K ftfor German (De). English reference is created with DeepL. Photo by Reinaldo Simoes.",
                "position": 7458
            }
        ]
    },
    {
        "header": "Appendix EQualitative Results",
        "images": []
    }
]