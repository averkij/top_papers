[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07917/x1.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2508.07917/x2.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2508.07917/x3.png",
                "caption": "",
                "position": 123
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07917/figures/f1.png",
                "caption": "Figure 1:Overview.MolmoActis an open action reasoning model that, given a user’s language instruction, reasons in space and autoregressively predicts three structured reasoning chains:Depth Perception Tokensfor sensing and reconstructing the 3D environment,Visual Reasoning Trace Tokensfor representing its planned trajectory in the scene, andAction Tokensfor generating the corresponding robot control commands. Each explainable reasoning chain can be independently decoded—yielding a depth map of the scene, a 2D trajectory overlay on the image plane, and executed actions in the physical world—providing explicit, spatially grounded reasoning at every stage.",
                "position": 129
            }
        ]
    },
    {
        "header": "2MolmoAct",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07917/figures/fig2.png",
                "caption": "Figure 2:Training process ofMolmoAct.The model training process consists of two stages:Pre-training(left) andPost-training, Mid-training & Inference(right). During pre-training, the vision–language backbone (Molmo) is trained on multimodal and robot reasoning data for diverse objectives, including discretized robot control, 2D pointing, trajectory drawing, open-vocabulary question answering, and perception token prediction. In post-training, the action reasoning model consumes multi-view camera images and either natural language instructions or visual trajectory inputs, generating perception tokens, visual reasoning trace tokens, and action tokens for execution.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/f3.png",
                "caption": "Figure 3:Distribution of data mixture in the overall pre-training mixture (left) and in the sampled subset used forMolmoActpre-training (right). The mixture contains primarily action reasoning data (38.7%), trajectory-conditioned data (38.7%), and multimodal web data (21.5%), with small fractions of auxiliary depth and trace data (0.5% each). The sampled subset increases the proportion of auxiliary data (7.5% each for depth and line) while reducing multimodal web data to 5%.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/Figure3.png",
                "caption": "Figure 4:Examples and verb distribution in theMolmoAct Dataset.Left: Sample robot manipulation tasks paired with natural language instructions, spanning diverse household activities such as closing a laptop, loading a plate, cleaning a toilet, and opening a microwave.\nRight: Log-scale distribution of the top verbs in the dataset, showing a long-tail pattern with “put,” “turn,” and “close” as the most frequent actions.",
                "position": 250
            }
        ]
    },
    {
        "header": "3Data Curation and Generation",
        "images": []
    },
    {
        "header": "4Training Recipe",
        "images": []
    },
    {
        "header": "5Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07917/figures/Figure5.png",
                "caption": "Figure 5:Real-world evaluation of OpenVLA,π0\\pi_{0}-FAST, andMolmoActon single-arm (left) and bimanual (right) Franka tasks. Bar plots report average task progression with standard error across 25 trials per task.MolmoActconsistently outperforms baselines, particularly on single-arm tasks such asWipe TableandTable Bussing, and maintains strong performance on bimanual tasks includingFold TowelandSet Table. Bottom row shows example task setups with corresponding natural language instructions.",
                "position": 800
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/figure6a.png",
                "caption": "(a)MolmoActgeneralizes beyond training distributions.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/figure6a.png",
                "caption": "(a)MolmoActgeneralizes beyond training distributions.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/f6b.png",
                "caption": "(b)MolmoAct Datasetimproves task performance.",
                "position": 843
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/f7.png",
                "caption": "Figure 7:Line steerability evaluation across models.Left:Elo ratings show thatMolmoActachieves the highest performance, surpassing Gemini-2.5-Flash, GPT-4o, and HAMSTER, with error bars indicating standard error.Right:Example qualitative results showing predicted visual traces overlaid on robot camera views.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/Fig8.png",
                "caption": "Figure 8:Language Instruction Evaluation.Left:Elo ratings for three models based on human votes in a head-to-head instruction-following evaluation.Right:Qualitative comparison of execution traces for the open-ended instruction “Put the redbull into the bowl.\"MolmoActaligns more closely with the intended instruction than other models.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2508.07917/figures/Figure9.png",
                "caption": "Figure 9:Steerability evaluation with open instructions and visual traces.Left:Success rates for different steering modes, showing thatMolmoActwith visual trace steering achieves the highest success rate (0.75), outperforming its open-instruction variant andπ0\\pi_{0}-FAST.Right:Example of the\"Pick up the bowl\"task: the model-predicted trajectory (yellow) is adjusted via a user-provided steering trajectory (cyan), resulting in the corrected task completion.",
                "position": 895
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]