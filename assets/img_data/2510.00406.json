[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00406/x1.png",
                "caption": "Figure 1:The Framework of VLA-RFT.A world model functions as a simulator that processes multi-rollout VLA action sequences to generate corresponding future states. By incorporating verified rewards through a GRPO optimization framework, we perform end-to-end updates of the VLA. Our approach achieves superior performance with remarkably fewer optimization steps—requiring only 0.4K iterations compared to 150K iterations for a strongly supervised baseline—demonstrating advantages in both standard and perturbed environments. Furthermore, the method exhibits enhanced execution-time robustness, characterized by reliable failure recovery and retry capabilities. For more details, please refer to ourwebpage.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00406/x2.png",
                "caption": "Figure 2:Training Paradigm of VLA-RFT.In the pre-training stage, both the world model and VLA policy are initialized, where the world model takes a 7-dimensional action input that is consistent in format with the VLA’s action output. In the reinforcement fine-tuning stage, the VLA generates action chunks based on an initial frame and language instruction, which are rolled out in the world model to predict future states. Verified rewards are then computed from the predicted states and used to optimize the VLA via GRPO Optimization.",
                "position": 137
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00406/x3.png",
                "caption": "Table 1:World model generation performance.Left: frame-level metrics across four suites (Spatial,Object,Goal,Long) and their averages—MSE (pixel error), PSNR (signal-to-noise ratio), SSIM (structural similarity), and LPIPS (perceptual distance).\nRight: qualitative results. Left column shows simulator sequences, right column shows world-model generations from the same initial frame and actions, illustrating consistent appearance and action-induced dynamics.",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2510.00406/x4.png",
                "caption": "Figure 3:Action distribution visualization of VLA-RFT and VLA-SFT.The plots show distributions alongXXandZZaction dimensions: the left plot corresponds to the RFT-trained policy, and the right plot to the SFT-only base policy.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2510.00406/x5.png",
                "caption": "Table 2:Performance under LIBERO Standard Suites.The table reports SR (%) across the four suites (Spatial,Object,Goal, andLong) and their average; the radar plot on the right provides a visual comparison of different model stages across tasks.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2510.00406/x6.png",
                "caption": "Figure 4:Illustration of perturbed task settings in LIBERO.We consider four perturbation types to evaluate out-of-distribution robustness:\n(Object Position) shifting the initial(x,y)(x,y)coordinates of the manipulated object;\n(Goal Position) displacing the target object in the(x,y)(x,y)plane;\n(Robot State) modifying the gripper’s vertical height and horizontal offset;\nand (Combination) applying all perturbations together.\nEach row shows the original setting (Origin), the perturbed variant (Disturb), and a side-by-side comparison (Contrast).",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2510.00406/figure/reward_funcv3.png",
                "caption": "Table 4:Reward design comparison on LIBERO.The left table reports the average success rates (SR, %) of the base policy (Base 15w) and its variants trained with three different verified reward types.\nThe right figure illustrates the corresponding reward function structures.",
                "position": 1114
            }
        ]
    },
    {
        "header": "5Conclusion & Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.00406/figure/world_model_generation.png",
                "caption": "Figure 5:Illustration of World Model Generation.The initial imageI0I_{0}and input action sequencea0:T−1a_{0:T-1}are first encoded into image and action tokens. These tokens are then fed into the world model to autoregressively predict the future state token sequence. Finally, decoders transform the generated image tokens into predicted future imagesI1,I2,…,ITI_{1},I_{2},\\dots,I_{T}.",
                "position": 2151
            },
            {
                "img": "https://arxiv.org/html/2510.00406/x7.png",
                "caption": "Figure 6:Comparison of original and disturbed scenarios.",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2510.00406/x8.png",
                "caption": "Figure 7:Comparison of base policy and VLA-RFT.",
                "position": 2954
            },
            {
                "img": "https://arxiv.org/html/2510.00406/figure/method_appendixv4.png",
                "caption": "Figure 8:Detailed Implementation of Method.",
                "position": 2959
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]