[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26604/sammaudio_v2.png",
                "caption": "Figure 1:Overview of SAGANetÂ control module. Given a video and its corresponding segmentation masks, the model combines global and local information streams. Gated Cross-Attention layers[1,26]are used to fuse global and local features extracted by Synchformer[18], with shared weights across both branches. Only the layers highlighted in orange are updated during training. The final audio is generated following the same procedure as in the base MMAudio model. For additional details on MMAudio, refer to[6].",
                "position": 248
            }
        ]
    },
    {
        "header": "4Segmented Music Solos Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26604/sms_v2.png",
                "caption": "Figure 2:Samples from Segmented Music Solos. The top row indicates the musical instrument label. The second row displays the first frames of the video and the corresponding mask stream. Last row displays the audio associated with the sample.",
                "position": 388
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]