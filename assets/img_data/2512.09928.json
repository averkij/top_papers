[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09928/x1.png",
                "caption": "Figure 1:(a-b) Comparison with existing methods: VLAs rely on instantaneous observations[kim2024openvla,kim2025fine](a-top), stack multiple past frames[liu2025towards,tianpredictive](a-second), or generate pixel-level subgoals[zhao2025cot,zhang2025up](a-third), suffering from redundancy, high inference cost, and weak structure. In contrast, HiF-VLA (a-bottom) jointly models Hindsight, Insight, and Foresight, expanding the temporal receptive field bidirectionally for compact, structured, and efficient reasoning. (c) HiF-VLA reduces inference latency and achieves state-of-the-art performance on LIBERO-Long and CALVIN ABC-D, significantly outperforming the baseline in real-world experiments.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09928/x2.png",
                "caption": "Figure 2:HiF-VLA Pipeline. (a) In Hindsight Prior Acquisition (seeSec.3.2), HiF-VLA encodes dense historical frame sequences into compact Motion Vector (MV) streams, forming structured hindsight primitives that capture temporal dynamics without pixel redundancy. (b) In Foresight Reasoning with Insight (seeSec.3.3), the VLM interprets the task instruction and current observation to infer plausible foresight motions and corresponding latent action tokens. (c) Finally, the Hindsight-Modulated Joint Expert (seeSec.3.4) fuses hindsight, foresight, and action representations within a unified latent space, producing temporally consistent and causally coherent action predictions.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09928/x3.png",
                "caption": "(a)Historical Frame Example",
                "position": 1306
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x3.png",
                "caption": "(a)Historical Frame Example",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x4.png",
                "caption": "(b)Inference Efficiency",
                "position": 1314
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x5.png",
                "caption": "(c)Effect of Hindsight",
                "position": 1319
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x6.png",
                "caption": "(a)Injecting VLM",
                "position": 1432
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x6.png",
                "caption": "(a)Injecting VLM",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x7.png",
                "caption": "(b)Conditioning Decoder",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x8.png",
                "caption": "(c)Results",
                "position": 1446
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x9.png",
                "caption": "Figure 5:Real-world long-horizon tasks. (a) We deploy our system on the AgileX Piper robotic arm equipped with an external scene camera (Intel RealSense D435) and a wrist-mounted camera. (b) We design three long-horizon tasks covering diverse primitives such as pick, put, cover, stack, and press, emphasizing temporal consistency in action generation.",
                "position": 1463
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6More Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09928/x10.png",
                "caption": "Figure 6:Architecture of the hindsight-modulated joint expert.",
                "position": 1532
            }
        ]
    },
    {
        "header": "7Comparison with Video-Generation VLAs",
        "images": []
    },
    {
        "header": "8More Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09928/x11.png",
                "caption": "Figure 7:Convergence of the foresight-motion L1 loss during training. The curve “w/o action prediction” corresponds to a variant where the action-prediction branch is removed and only the foresight-motion pathway is trained. The curve “w/ action prediction” represents the full HiF-VLA architecture, where both streams in the joint expert (foresight motion and action) are retained.",
                "position": 1747
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x12.png",
                "caption": "Figure 8:Example rollouts of three tasks in LIBERO-Long, illustrating the close alignment between the predicted foresight motion and the observed action execution over a short inference window.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x13.png",
                "caption": "(a)Place blocks on the plates.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x13.png",
                "caption": "(a)Place blocks on the plates.",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x14.png",
                "caption": "(b)Cover block and stack bowls.",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x15.png",
                "caption": "(c)Press buttons in order.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x16.png",
                "caption": "(a)Place blocks on the plates.",
                "position": 1828
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x16.png",
                "caption": "(a)Place blocks on the plates.",
                "position": 1831
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x17.png",
                "caption": "(b)Cover block and stack bowls.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2512.09928/x18.png",
                "caption": "(c)Press buttons in order.",
                "position": 1843
            }
        ]
    },
    {
        "header": "9Real-World Experiments",
        "images": []
    }
]