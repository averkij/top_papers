[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08646/x1.png",
                "caption": "",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08646/x2.png",
                "caption": "Figure 2:Comparison of context in the decoding process with existing models.For each text token, the black and blue arrows indicate the beginning and end of the utilized visual context, respectively. While existing models (top) use a fixed visual context when decoding, StreamChat (bottom) aligns the video and text streams temporally and dynamically updates its visual context based on the streaming video.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08646/x3.png",
                "caption": "Figure 3:The StreamChat architecture.We utilize cross-attention blocks to bridge the visual and text tokens and V-FFN blocks to update the visual tokens throughout the LLM’s forward process. Those two blocks’ outputs are scaled with a linear gate mechanism.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2412.08646/x4.png",
                "caption": "Figure 4:The parallel 3D-RoPE. For visual and text tokens at the same timestamp, they share the same temporal position.",
                "position": 168
            }
        ]
    },
    {
        "header": "3Experiment Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.08646/x5.png",
                "caption": "Figure 5:Comparison of StreamChat with leading video LMMs on streaming evaluation.We use StreamChat-7B/-14B as one of the candidate models and report the win/tie/loss rate against VILA or LLaVA-Video models. Our StreamChat models demonstrate stronger streaming interaction capabilities, and can even outperform LLaVA-Video-72B which uses a much larger base LLM.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2412.08646/x6.png",
                "caption": "Figure 6:Qualitative evaluation of StreamChat on streaming video.In the example shown, the questions are asked at the first second (top) and the 21st second (bottom), respectively.\nOur model can capture the dynamic video content and adapt its answer accordingly. In comparison, VILA and LLaVA-Video fail to follow the streaming video and exhibit factual errors (highlighted in red).",
                "position": 242
            }
        ]
    },
    {
        "header": "4Streaming Evaluation",
        "images": []
    },
    {
        "header": "5Benchmark Results",
        "images": []
    },
    {
        "header": "6Ablation Studies",
        "images": []
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]