[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19001/x1.png",
                "caption": "Figure 1:The Example of The GPT-OSS-20B Model.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x2.png",
                "caption": "Figure 2:Attention Heatmap of Reasoning Tokens.We use the Phi-4-Reasoning model(Abdin et al.,2025)to generate a reasoning trace for a sample GSM8K question(Cobbe et al.,2021).\nThe figure shows attention heatmaps from transformer layers 0, 30 and 39, with the first head (top row) and last head (bottom row).\nYellow indicates higher attention weights and blue indicates lower ones.\nIn shallow layers, contributions to the final answer are nearly uniform, while deeper layers and later heads highlight specific tokens with stronger influence.",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x3.png",
                "caption": "Figure 3:Total attention weight distribution to the final answer token</think>from different components of the reasoning trace.We visualize the total attention weight distribution of the Phi-4-Reasoning model\non a sample GSM8K question, using transformer layers11,3030, and4040.\nThe results show that a few reasoning traces contribute strongly to the final token</think>,\nwhile many traces have negligible influence, particularly in the layers3030and4040.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x4.png",
                "caption": "Figure 4:Theoretical Analysis of Reasoning Outlier Removal.We conduct a theoretical analysis with Phi-4-Reasoning model to demonstrate that removing reasoning outliers using theSoftmax1\\mathop{\\rm{Softmax}}_{1}function (FROST) can preserve or even enhance the model’s reasoning capacity.\nAs shown in the figure, the attention weight distribution before and after outlier removal indicates that the model’s focus on critical reasoning traces is maintained or improved, while the influence of outliers is significantly reduced.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x5.png",
                "caption": "Figure 5:Overview of theFROSTworkflowWe replace the vanillaSoftmax\\mathop{\\rm{Softmax}}layer with an outlier-removal layer based onSoftmax1\\mathop{\\rm{Softmax}}_{1}, followed by SFT to adapt model parameters to the new activation function. We observe that our method significantly reduces the number of low-attention sentences.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x6.png",
                "caption": "Figure 6:Attention Distribution of Each Activation Function.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x7.png",
                "caption": "Figure 7:Theoretical Analysis of Reasoning Outlier Removal in All Layers",
                "position": 2290
            },
            {
                "img": "https://arxiv.org/html/2601.19001/x8.png",
                "caption": "Figure 8:Attention Distribution ofSoftmax1\\mathop{\\rm{Softmax}}_{1}Across All Layers",
                "position": 2293
            },
            {
                "img": "https://arxiv.org/html/2601.19001/figs/all_layer_heatmap.png",
                "caption": "Figure 9:Extended Attention Heatmaps Across Additional Layers and Heads",
                "position": 2303
            }
        ]
    },
    {
        "header": "Supplementary Material",
        "images": []
    }
]