[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17803/x1.png",
                "caption": "Figure 1.(a) ConsistEditenables multi-round editing by allowing users to specify both the target region and the nature of the editing through prompts. Unlike existing methods, it can perform structure-preserving (hair, clothing folds) and shape-changing with identity-preserving edits in edited regions while keeping non-edited regions intact.(b)ConsistEdit handles multi-region edits in one pass and preserves both the edited structure and unedited content.(c)Our method enables smooth control over consistency strength in the edited region. In contrast, existing approaches lack smooth transitions and often alter non-edited areas.(d)Beyond image editing and rectified flow models, ConsistEdit generalizes well to all MM-DiT variants, including diffusion and video models.",
                "position": 164
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17803/x2.png",
                "caption": "Figure 2.Visualization of projectedQ,K,Vvision tokens in attention layers of the MM-DiT blocks at 15th sampling step of prompt ‚ÄúA standing horse.‚Äù",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x3.png",
                "caption": "Figure 3.(a)shows theConsistEditpipeline. Given a real image or videoùë∞s\\bm{I}_{s}and source text tokensùêès\\bm{\\mathrm{P}}_{s}, we first invert the source to obtain the vision tokensùíõT\\bm{z}^{T}, which is concatenated with the target prompt tokensùêèt‚Äãg\\bm{\\mathrm{P}}_{tg}and passed into the generation process to produce the edited image or videoùë∞t‚Äãg\\bm{I}_{tg}. During inference, a maskùë¥\\bm{M}generated by our extraction method delineates editing and non-editing regions. We apply structure and content fusion to enable prompt-aligned edits while preserving structural consistency within edited regions and maintaining content integrity elsewhere.(b)illustrates the mask-guided attention fusion for timesteps wheret>(1‚àíŒ±)‚ÄãTt>(1-\\alpha)T, which is applied exclusively to the vision parts, while the text parts remain unchanged.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x4.png",
                "caption": "Figure 4.Comparison ofVtoken swapping strategies for content consistency.\nSwapping vision-onlyVtokens leads to superior content consistency under high consistency strength settings, while maintaining comparable editing capability to original methods when the consistency strength is low.",
                "position": 361
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17803/x5.png",
                "caption": "Figure 5.Real image multi-round editing results. Starting from a real image, we first perform inversion to project it into the latent space. We then sequentially edit the clothing color, motion, and hair.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x6.png",
                "caption": "Figure 6.Qualitative comparison of methods on real image editing tasks.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x7.png",
                "caption": "Figure 7.Qualitative comparison of methods on structure-consistent editing tasks.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x8.png",
                "caption": "Figure 8.Qualitative comparison of methods on structure-inconsistent editing tasks.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x9.png",
                "caption": "Figure 9.Effect of consistency strength on structural consistency. High strength strictly enforces structural preservation, while low strength permits prompt-driven shape changes. Texture editing remains consistent, highlighting effective disentanglement.",
                "position": 644
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x10.png",
                "caption": "Figure 10.Qualitative comparison on consistency strength adjustment.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x11.png",
                "caption": "Figure 11.Ablation study on attention control for structure consistency. We compare (1) fixed seed results, (2) swapping allQandKtokens across all blocks, (3) swapping only the vision part ofQandKtokens in the last half of the blocks, (4) swapping only the vision part ofQandKtokens in all blocks, and (5) adding our non-editing region consistency module.",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x12.png",
                "caption": "Figure 12.Ablation study of non-edited region preservation. The edit prompt is ‚Äúa head‚Äù‚Üí\\to‚Äúa dog head‚Äù.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x13.png",
                "caption": "Figure 13.Examples of editing results with FLUX.",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x14.png",
                "caption": "Figure 14.Qualitative comparison of methods on video editing tasks. The edit prompt is ‚Äúgreen toy ship‚Äù‚Üí\\to‚Äúdark red toy ship‚Äù.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x15.png",
                "caption": "Figure 15.Examples of applications.",
                "position": 734
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17803/x16.png",
                "caption": "Figure 16.Examples of video editing.. The prompt is ‚Äúred SUV‚Äù‚Üí\\to‚Äúblue SUV‚Äù.",
                "position": 1390
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x17.png",
                "caption": "Figure 17.Examples of multi-region editing.",
                "position": 1393
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x18.png",
                "caption": "Figure 18.Additional qualitative comparison of methods on video editing tasks. The edit prompt is ‚Äúblue shorts‚Äù‚Üí\\to‚Äúgreen shorts‚Äù.",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x19.png",
                "caption": "Figure 19.Examples of real input image editing. The first row shows the source images, the second row presents the reconstructed images via inversion, and the third row displays the editing results based on the target prompts.",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x20.png",
                "caption": "Figure 20.Qualitative comparison of different consistency strength settings. ‚ÄúOurs‚Äù denotes the method proposed in the main paper, while ‚ÄúOurs*‚Äù refers to a modified version of our method.",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x21.png",
                "caption": "Figure 21.Additional qualitative comparison of methods on structure-consistent and structure-inconsistent editing tasks.",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2510.17803/x22.png",
                "caption": "Figure 22.Examples of typical failure cases.",
                "position": 1478
            }
        ]
    },
    {
        "header": "Appendix BResults and Analysis",
        "images": []
    }
]