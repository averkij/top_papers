[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04835/extracted/6047317/figs/front_fig_2.png",
                "caption": "Figure 1:Representation-Aligned Preference-based Learning (RAPL), is an observation-only method for learning visual robot rewards from significantly less human preference feedback.\n(center) Unlike traditional reinforcement learning from human feedback, RAPL focuses human feedback on fine-tuning pre-trained vision encoders to align with the end-user‚Äôs visual representation.\nThe aligned representation is used to construct an optimal transport-based visual reward for aligning the robot‚Äôs visuomotor policy. (left) Before alignment, the robot frequently picks up a bag of chips by squeezing the middle, risking damage to the contents. (right) After alignment with our RAPL reward, the robot adheres to the end-user‚Äôs preference and picks up the bag by its edges.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Setup",
        "images": []
    },
    {
        "header": "4RAPL: Representation-Aligned Preference-Based Learning",
        "images": []
    },
    {
        "header": "5Reinforcement Learning in Simulation with RAPL",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04835/extracted/6047317/figs/sim_env_all.png",
                "caption": "Figure 2:X-Magical & IsaacGym tasks.\nTop row are high-reward behaviors and bottom row are low-reward behaviors according to the human‚Äôs preferences.",
                "position": 508
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x1.png",
                "caption": "Figure 3:X-Magical.(left & right) examples of preferred and disliked videos for each task. (center) reward associated with each video under each method. RAPL‚Äôs predicted reward follows the GT pattern: low reward when the behavior are disliked and high reward when the behavior are preferred. RLHF and TCC assign high reward to disliked behavior (e.g., (D)).",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x2.png",
                "caption": "Figure 4:X-Magical.Policy evaluation success rate during policy learning. Colored lines are the mean and variance of the evaluation success rate. RAPL can match GT in theavoidingtask and outperforms baseline visual rewards ingroupingtask.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x3.png",
                "caption": "Figure 5:Manipulation: Reward Prediction.(center) Expert, preferred, and disliked video demo.\n(left) Reward of each video under each method. RAPL‚Äôs predicted reward follows the GT pattern. RLHF assigns high reward to disliked behavior.\n(right) OT coupling matrix for each visual representation. Columns are embedded frames of expert demo. Rows of top matrices are embedded frames of preferred demo; rows of bottom matrices are embedded frames of disliked demo. Peaks exactly along the diagonal indicate that the frames of the two videos are aligned in the latent space; uniform values in the matrix indicate that the two videos cannot be aligned (i.e., all frames are equally ‚Äúsimilar‚Äù to the next). RAPL exhibits the diagonal peaks for expert-and-preferred and uniform for expert-and-disliked, while baselines show diffused values no matter the videos being compared.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x4.png",
                "caption": "Figure 6:(Left)Manipulation: Qualitative RLHF Comparison.We visualize the attention map for RLHF-150 demos, RLHF-300 demos, and RAPL with 150 demos for both Franka and Kuka (cross-embodiment). Each entry shows two observations from respective demonstration set with the attention map overlaid. Bright yellow areas indicate image patches that contribute most to the final embedding; darker purple patches indicate less contribution.œïR‚Å¢L‚Å¢H‚Å¢F‚àí150subscriptitalic-œïùëÖùêøùêªùêπ150\\phi_{RLHF-150}italic_œï start_POSTSUBSCRIPT italic_R italic_L italic_H italic_F - 150 end_POSTSUBSCRIPTis biased towards paying attention to irrelevant areas that can induce spurious correlations; in contrast RAPL learns to focus on the task-relevant objects and the goal region.œïR‚Å¢L‚Å¢H‚Å¢F‚àí300subscriptitalic-œïùëÖùêøùêªùêπ300\\phi_{RLHF-300}italic_œï start_POSTSUBSCRIPT italic_R italic_L italic_H italic_F - 300 end_POSTSUBSCRIPT‚Äôs attention is slightly shifted to objects but still pays high attention to the robot embodiment. (Right)Manipulation: Quantitative RLHF Comparison.RAPLoutperformsRLHFby75%percent7575\\%75 %with50%percent5050\\%50 %less human preference data.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x5.png",
                "caption": "Figure 7:Manipulation: Policy Learning.Success rate during robot policy learning under each visual reward.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x6.png",
                "caption": "Figure 8:Manipulation: Cross-Embodiment Reward Transfer.(center) Expert video ofFrankarobot, preferred video ofKuka, and dislikedKukavideo.\n(left) Predicted reward under each method trained only onFrankavideo preferences. RAPL‚Äôs reward generalizes to theKukarobot and follows the GT pattern.\n(right) OT plan for each visual representation shown in the same style as inFigure¬†5.\nRAPL‚Äôs representation shows a diagonal OT plan for expert-and-preferred demos vs. a uniform for expert-and-disliked, while baselines show inconsistent plan patterns.",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x7.png",
                "caption": "Figure 9:X-Magical: Cross-Embodiment Reward Transfer.RAPLdiscriminates preferred and disliked videos of novel robots.",
                "position": 730
            },
            {
                "img": "https://arxiv.org/html/2412.04835/x8.png",
                "caption": "Figure 10:Cross-Embodiment Policy Learning.Policy evaluation success rate during policy learning. Colored lines are the mean and variance of the evaluation success rate. RAPL achieves a comparable success rate compared to GT with high learning efficiency, and outperforms all baselines.",
                "position": 755
            }
        ]
    },
    {
        "header": "6Aligning Diffusion Policies in the Real World with RAPL",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04835/x9.png",
                "caption": "Figure 11:Diffusion Policy Alignment Results. (Top) The pre-trained visuomotor policy exhibits undesired behaviors: grasping the interior of the cup (left), crushing the chips (middle), and making contact with the tines of the fork and dropping it out of the bowl (right). (Bottom) After alignment using RAPL rewards, the robot‚Äôs behaviors are aligned with the end-user‚Äôs preferences.",
                "position": 798
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]