[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x1.png",
                "caption": "(a)IsoFLOP surface over sparsity and total parameters",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x1.png",
                "caption": "(a)IsoFLOP surface over sparsity and total parameters",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x2.png",
                "caption": "(b)IsoFLOP surface over sparsity and active parameters",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x3.png",
                "caption": "Figure 2:IsoFLOP slices along Sparsity and Model Size (C=1⁢e⁢20𝐶1𝑒20C=1e20italic_C = 1 italic_e 20).We use fitted isoFLOP surfaces (Section2) to analyze how sparsity𝐒𝐒\\mathbf{S}bold_Sand model size𝐍𝐍\\mathbf{N}bold_Nimpact the loss𝐋𝐋\\mathbf{L}bold_Lfor a fixed compute budget. We identify optimal points by (a) fixing𝐍𝐍\\mathbf{N}bold_Nand varying𝐒𝐒\\mathbf{S}bold_S, (b) fixing𝐒𝐒\\mathbf{S}bold_Sand varying𝐍𝐍\\mathbf{N}bold_Nand (c) fixing𝐒𝐒\\mathbf{S}bold_Sand varying active parameters𝐍𝐚subscript𝐍𝐚\\mathbf{N_{a}}bold_N start_POSTSUBSCRIPT bold_a end_POSTSUBSCRIPT. Observe that (a) the optimal sparsityS𝑆Sitalic_Sincreases with increasing model sizeN𝑁Nitalic_Nand converges to 1 while (b) and (c) show that the optimal model sizeN𝑁Nitalic_Nand active parameter countNasubscript𝑁𝑎N_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTincrease and decrease respectively with increasing sparsity levels. (seeFigure9inSectionD.1for other total\ntraining compute budgets.)",
                "position": 192
            }
        ]
    },
    {
        "header": "2The Interplay between Model Parameters and Sparsity in MoEs",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x4.png",
                "caption": "Figure 3:Effect of compute budget on model size, number of active parameters and loss with sparsity.Across all compute budgets, we observe that (a) the optimal model sizeN𝑁Nitalic_Nincreases with sparsity, (b) the optimal number of active parametersNasubscript𝑁𝑎N_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTdecreases with sparsity, and (c) the lossL𝐿Litalic_Ldecreases with sparsity.",
                "position": 301
            }
        ]
    },
    {
        "header": "3Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x5.png",
                "caption": "Figure 4:Effect of training budgetC𝐶Citalic_Cand total parametersN𝑁Nitalic_Non MoE sparsity.Optimal MoE sparsityS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTchanges with respect to the total number of parametersN𝑁Nitalic_Nand the training budgetC𝐶Citalic_C. Thex𝑥xitalic_x-axis represents the total parametersN𝑁Nitalic_Non a logarithmic scale, and they𝑦yitalic_y-axis shows the optimal MoE sparsityS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT.",
                "position": 318
            }
        ]
    },
    {
        "header": "4Effect of MoE Sparsity on Downstream Task Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x6.png",
                "caption": "Figure 5:Effect of sparsity on downstream vs upstream performance.Downstream error shows a tight relationship with pretraining (“upstream”) loss across downstream tasks across all sparsity levels.",
                "position": 340
            }
        ]
    },
    {
        "header": "5Incorporating Sparsity into Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x7.png",
                "caption": "(a)Fit on data used to estimate coefficients.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x7.png",
                "caption": "(a)Fit on data used to estimate coefficients.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x8.png",
                "caption": "(b)Validating scaling law on held-out dataset.",
                "position": 403
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Appendix APreliminaries",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix CEstimating Mixture-of-Expert (MoE) FLOPs",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x9.png",
                "caption": "Figure 7:Accuracy of6⁢Na⁢D6subscript𝑁𝑎𝐷6N_{a}D6 italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_DFLOPs Estimator for MoEs.\nRatio of the MoE FLOPs estimator (Equation9) to the6⁢Na⁢D6subscript𝑁𝑎𝐷6N_{a}D6 italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_Destimator as a function of the total number of parameters, for a fixed context length ofD=2048𝐷2048D=2048italic_D = 2048, used in our experiments.",
                "position": 1111
            }
        ]
    },
    {
        "header": "Appendix DAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x10.png",
                "caption": "Figure 8:IsoFLOP surfaces over total parametersN𝑁Nitalic_N, MoE sparsityS𝑆Sitalic_S, and pretraining lossL𝐿Litalic_Lfor different compute budgets.\nThe rows correspond to IsoFLOP surface fitted using models trained with a budget of 3e19, 6e19, 1e20, 3e20, and 1e21.\nThe subplots on the left visualize IsoFLOP surfaces mapping total parametersN𝑁Nitalic_Nand sparsity levelS𝑆Sitalic_Sto pretraining lossL𝐿Litalic_L.\nThe subplots on the right correlate the ground-truth pretraining loss with the estimated pretraining loss on held-out data.\nTaken together, these results show that isoFLOP surfaces are accurate proxies for understanding how model size and MoE sparsity jointly impact pretraining loss.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x10.png",
                "caption": "",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x11.png",
                "caption": "",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x12.png",
                "caption": "",
                "position": 1154
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x13.png",
                "caption": "",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x14.png",
                "caption": "",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x15.png",
                "caption": "Figure 9:Optimal MoE configurations predictably change with training compute budget.Each row corresponds to an analysis of how optimal MoE sparsityS∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, total parametersN∗superscript𝑁N^{*}italic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, and active parametersNa∗subscriptsuperscript𝑁𝑎N^{*}_{a}italic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTchange for a given training budget.\nThe subplots on the left show that (a) increasing the training budget increases the model sizeN𝑁Nitalic_N(denoted with black dots) with the minimum pretraining loss and (b) for models smaller than a threshold (which increases with training budget), dense models (i.e.,0%percent00\\%0 %sparsity) fare better than sparse MoEs.\nThe subplots in the second and third panel show that (a) increasing MoE sparsity increases the optimal total parametersN∗superscript𝑁N^{*}italic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTand decreases the optimal active parametersNa∗subscriptsuperscript𝑁𝑎N^{*}_{a}italic_N start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT. In both cases, for a fixed sparsity level, increasing the budget shifts increases the optimal total and active parameters.",
                "position": 1175
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x15.png",
                "caption": "",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x16.png",
                "caption": "",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x17.png",
                "caption": "",
                "position": 1187
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x18.png",
                "caption": "",
                "position": 1191
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x19.png",
                "caption": "",
                "position": 1196
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x20.png",
                "caption": "Figure 10:Downstream task performance vs. upstream pre-training loss.Each subplot shows the relationship between upstream pre-training loss (x-axis) and downstream task performance (y-axis) for a specific task.\nSimilar to our results inSection4, we find that the MoE sparsity level does not change the relationship between upstream pre-training loss and downstream task performance.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x20.png",
                "caption": "",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x21.png",
                "caption": "",
                "position": 1294
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x22.png",
                "caption": "",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x23.png",
                "caption": "",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x24.png",
                "caption": "Figure 11:Effect of MoE sparsity on pretraining loss across different training compute budgets.\nAs sparsity increases, the validation loss decreases for all compute budgets, with larger budgets (darker lines) achieving lower losses at each sparsity level.\nThis trend is consistent with the findings fromSection3, demonstrating that increasing sparsity reduces the optimal pretraining loss across all compute budgets.",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x25.png",
                "caption": "Figure 12:Effect of MoE sparsity on optimal total and active parameters across different training compute budgets.Each row shows the change in total and active parameters as a function of sparsity level for fixed training budgets.\nIncreasing sparsity leads to an increase in the optimal total parameters while reducing the optimal active parameters, consistent with our findings inSection2(Figure2).\nLarger training compute budgets result in higher optimal (total and active) parameters across all sparsity levels.",
                "position": 1348
            }
        ]
    },
    {
        "header": "Appendix EIncorporating Sparsity into Scaling Laws",
        "images": []
    }
]