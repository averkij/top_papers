[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x1.png",
                "caption": "(a)IsoFLOP surface over sparsity and total parameters",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x1.png",
                "caption": "(a)IsoFLOP surface over sparsity and total parameters",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x2.png",
                "caption": "(b)IsoFLOP surface over sparsity and active parameters",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x3.png",
                "caption": "Figure 2:IsoFLOP slices along Sparsity and Model Size (C=1â¢eâ¢20ğ¶1ğ‘’20C=1e20italic_C = 1 italic_e 20).We use fitted isoFLOP surfaces (Section2) to analyze how sparsityğ’ğ’\\mathbf{S}bold_Sand model sizeğğ\\mathbf{N}bold_Nimpact the lossğ‹ğ‹\\mathbf{L}bold_Lfor a fixed compute budget. We identify optimal points by (a) fixingğğ\\mathbf{N}bold_Nand varyingğ’ğ’\\mathbf{S}bold_S, (b) fixingğ’ğ’\\mathbf{S}bold_Sand varyingğğ\\mathbf{N}bold_Nand (c) fixingğ’ğ’\\mathbf{S}bold_Sand varying active parametersğğšsubscriptğğš\\mathbf{N_{a}}bold_N start_POSTSUBSCRIPT bold_a end_POSTSUBSCRIPT. Observe that (a) the optimal sparsitySğ‘†Sitalic_Sincreases with increasing model sizeNğ‘Nitalic_Nand converges to 1 while (b) and (c) show that the optimal model sizeNğ‘Nitalic_Nand active parameter countNasubscriptğ‘ğ‘N_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTincrease and decrease respectively with increasing sparsity levels. (seeFigure9inSectionD.1for other total\ntraining compute budgets.)",
                "position": 192
            }
        ]
    },
    {
        "header": "2The Interplay between Model Parameters and Sparsity in MoEs",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x4.png",
                "caption": "Figure 3:Effect of compute budget on model size, number of active parameters and loss with sparsity.Across all compute budgets, we observe that (a) the optimal model sizeNğ‘Nitalic_Nincreases with sparsity, (b) the optimal number of active parametersNasubscriptğ‘ğ‘N_{a}italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTdecreases with sparsity, and (c) the lossLğ¿Litalic_Ldecreases with sparsity.",
                "position": 301
            }
        ]
    },
    {
        "header": "3Impact of Training Compute Budget on the Interaction between Model Parameters and Sparsity",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x5.png",
                "caption": "Figure 4:Effect of training budgetCğ¶Citalic_Cand total parametersNğ‘Nitalic_Non MoE sparsity.Optimal MoE sparsitySâˆ—superscriptğ‘†S^{*}italic_S start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPTchanges with respect to the total number of parametersNğ‘Nitalic_Nand the training budgetCğ¶Citalic_C. Thexğ‘¥xitalic_x-axis represents the total parametersNğ‘Nitalic_Non a logarithmic scale, and theyğ‘¦yitalic_y-axis shows the optimal MoE sparsitySâˆ—superscriptğ‘†S^{*}italic_S start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT.",
                "position": 318
            }
        ]
    },
    {
        "header": "4Effect of MoE Sparsity on Downstream Task Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x6.png",
                "caption": "Figure 5:Effect of sparsity on downstream vs upstream performance.Downstream error shows a tight relationship with pretraining (â€œupstreamâ€) loss across downstream tasks across all sparsity levels.",
                "position": 340
            }
        ]
    },
    {
        "header": "5Incorporating Sparsity into Scaling Laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x7.png",
                "caption": "(a)Fit on data used to estimate coefficients.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x7.png",
                "caption": "(a)Fit on data used to estimate coefficients.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x8.png",
                "caption": "(b)Validating scaling law on held-out dataset.",
                "position": 403
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Appendix APreliminaries",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix CEstimating Mixture-of-Expert (MoE) FLOPs",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x9.png",
                "caption": "Figure 7:Accuracy of6â¢Naâ¢D6subscriptğ‘ğ‘ğ·6N_{a}D6 italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_DFLOPs Estimator for MoEs.\nRatio of the MoE FLOPs estimator (Equation9) to the6â¢Naâ¢D6subscriptğ‘ğ‘ğ·6N_{a}D6 italic_N start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_Destimator as a function of the total number of parameters, for a fixed context length ofD=2048ğ·2048D=2048italic_D = 2048, used in our experiments.",
                "position": 1111
            }
        ]
    },
    {
        "header": "Appendix DAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12370/x10.png",
                "caption": "Figure 8:IsoFLOP surfaces over total parametersNğ‘Nitalic_N, MoE sparsitySğ‘†Sitalic_S, and pretraining lossLğ¿Litalic_Lfor different compute budgets.\nThe rows correspond to IsoFLOP surface fitted using models trained with a budget of 3e19, 6e19, 1e20, 3e20, and 1e21.\nThe subplots on the left visualize IsoFLOP surfaces mapping total parametersNğ‘Nitalic_Nand sparsity levelSğ‘†Sitalic_Sto pretraining lossLğ¿Litalic_L.\nThe subplots on the right correlate the ground-truth pretraining loss with the estimated pretraining loss on held-out data.\nTaken together, these results show that isoFLOP surfaces are accurate proxies for understanding how model size and MoE sparsity jointly impact pretraining loss.",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x10.png",
                "caption": "",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x11.png",
                "caption": "",
                "position": 1149
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x12.png",
                "caption": "",
                "position": 1154
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x13.png",
                "caption": "",
                "position": 1158
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x14.png",
                "caption": "",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x15.png",
                "caption": "Figure 9:Optimal MoE configurations predictably change with training compute budget.Each row corresponds to an analysis of how optimal MoE sparsitySâˆ—superscriptğ‘†S^{*}italic_S start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT, total parametersNâˆ—superscriptğ‘N^{*}italic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT, and active parametersNaâˆ—subscriptsuperscriptğ‘ğ‘N^{*}_{a}italic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTchange for a given training budget.\nThe subplots on the left show that (a) increasing the training budget increases the model sizeNğ‘Nitalic_N(denoted with black dots) with the minimum pretraining loss and (b) for models smaller than a threshold (which increases with training budget), dense models (i.e.,0%percent00\\%0 %sparsity) fare better than sparse MoEs.\nThe subplots in the second and third panel show that (a) increasing MoE sparsity increases the optimal total parametersNâˆ—superscriptğ‘N^{*}italic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPTand decreases the optimal active parametersNaâˆ—subscriptsuperscriptğ‘ğ‘N^{*}_{a}italic_N start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT. In both cases, for a fixed sparsity level, increasing the budget shifts increases the optimal total and active parameters.",
                "position": 1175
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x15.png",
                "caption": "",
                "position": 1178
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x16.png",
                "caption": "",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x17.png",
                "caption": "",
                "position": 1187
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x18.png",
                "caption": "",
                "position": 1191
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x19.png",
                "caption": "",
                "position": 1196
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x20.png",
                "caption": "Figure 10:Downstream task performance vs. upstream pre-training loss.Each subplot shows the relationship between upstream pre-training loss (x-axis) and downstream task performance (y-axis) for a specific task.\nSimilar to our results inSection4, we find that the MoE sparsity level does not change the relationship between upstream pre-training loss and downstream task performance.",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x20.png",
                "caption": "",
                "position": 1289
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x21.png",
                "caption": "",
                "position": 1294
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x22.png",
                "caption": "",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x23.png",
                "caption": "",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x24.png",
                "caption": "Figure 11:Effect of MoE sparsity on pretraining loss across different training compute budgets.\nAs sparsity increases, the validation loss decreases for all compute budgets, with larger budgets (darker lines) achieving lower losses at each sparsity level.\nThis trend is consistent with the findings fromSection3, demonstrating that increasing sparsity reduces the optimal pretraining loss across all compute budgets.",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2501.12370/x25.png",
                "caption": "Figure 12:Effect of MoE sparsity on optimal total and active parameters across different training compute budgets.Each row shows the change in total and active parameters as a function of sparsity level for fixed training budgets.\nIncreasing sparsity leads to an increase in the optimal total parameters while reducing the optimal active parameters, consistent with our findings inSection2(Figure2).\nLarger training compute budgets result in higher optimal (total and active) parameters across all sparsity levels.",
                "position": 1348
            }
        ]
    },
    {
        "header": "Appendix EIncorporating Sparsity into Scaling Laws",
        "images": []
    }
]