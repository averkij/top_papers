[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24521/x1.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24521/extracted/6496516/images/motivation.jpg",
                "caption": "Figure 2:Our key insights lie in (a) pre-trained video diffusion models capture accurate inter-frame correspondence (the same patches in different frames highlight in the attention maps), (b) the correspondence can be specified by applying identical positional encodings onto different frames, and (c) geometric properties within a shared global coordinate system naturally exhibit alignment across frames.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24521/x2.png",
                "caption": "Figure 3:Method overview.Our method targets at predicting geometric properties that are defined in the global coordinate system, where “radius” represents the distance from a 3D points to the origin (left). We efficiently adapt a pre-trained video diffusion model that inherently encodes inter-frame correspondence into a consistent geometry estimation model (right), where we process both rgb sequence and geometry sequence through our proposed SPE-Transformer.",
                "position": 185
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24521/x3.png",
                "caption": "Figure 4:Qualitative comparisons on normal estimation.Our method produces more consistent normals, remains robust to highlight reflections, and generates results closest to the GT. It effectively removes noise from the GT, delivering smooth predictions.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2505.24521/x4.png",
                "caption": "Figure 5:Qualitative comparisons on radius estimation.Our method achieves both accuracy and consistency, producing results closest to the ground truth. The entire video is normalized for consistent visualization.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2505.24521/x5.png",
                "caption": "Figure 6:Comparison to Dust3R.Our method demonstrates better inter-frame consistency in multi-frame sequence reconstruction. Please refer to the videos in Supp for clearer comparisons.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2505.24521/x6.png",
                "caption": "Figure 7:Comparison to E2E.Our method predicts normals of better consistency with more details preserved .",
                "position": 512
            }
        ]
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "S1Outline",
        "images": []
    },
    {
        "header": "S2Multi-view Data Processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24521/extracted/6496516/images_supp/data_group.png",
                "caption": "Figure S1:Given a query image, we use its depth and camera parameters to project it into other viewpoints. We then compute the overlap pixel ratio with the corresponding image to determine their relevance.",
                "position": 1543
            }
        ]
    },
    {
        "header": "S3Coordinate Frame Definition",
        "images": []
    },
    {
        "header": "S4Ablation Study Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24521/extracted/6496516/images_supp/ablation_multi_attr.png",
                "caption": "Figure S2:After the same number of fine-tuning steps, our method aligns RGB with geometric properties more quickly and produces more consistent predictions. This demonstrates that our proposed approach preserves the original prior as much as possible without disruption.",
                "position": 1603
            },
            {
                "img": "https://arxiv.org/html/2505.24521/extracted/6496516/images_supp/ablation_condition.png",
                "caption": "Figure S3:After the same number of fine-tuning steps, our method aligns RGB with geometric properties more quickly and produces more consistent predictions. This demonstrates that our proposed approach preserves the original prior as much as possible without disruption.",
                "position": 1606
            },
            {
                "img": "https://arxiv.org/html/2505.24521/extracted/6496516/images_supp/ablation_multistep.png",
                "caption": "Figure S4:The figure illustrates a comparison between the results of multi-step and single-step approaches. The yellow boxes indicate areas where the multi-step method achieves sharper results, while the red boxes highlight regions where the multi-step approach exhibits prediction errors.",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2505.24521/x7.png",
                "caption": "Figure S5:We show more visual comparisons of predicted normal on scannetpp dataset. The inconsistency is marked in red rectangles. It can be seen that ours achieve the most consistent visual effect while E2E and StableNormal provide inconsistent results, and VDA provides erroneous and inconsistent normals.",
                "position": 1751
            },
            {
                "img": "https://arxiv.org/html/2505.24521/x8.png",
                "caption": "Figure S6:We show more visual comparisons of predicted radius on scannetpp dataset. Compared with other depth estimation methods, our approach produces more consistent and accurate geometry estimation.",
                "position": 1754
            }
        ]
    },
    {
        "header": "S5More Results",
        "images": []
    }
]