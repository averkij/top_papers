[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08511/x1.png",
                "caption": "",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2512.08511/x2.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08511/x3.png",
                "caption": "Figure 2:Thinking-with-images-through-self-calling.A visual language model termedmain agentdecomposes a complex visual query into simple, atomic subtasks handled by the model’s virtual replicas (“subagents”). Each subagent solves a localized atomic subtask (grounding, captioning, OCR,etc.) and returns textual outputs which are aggregated by the main agent to produce the final answer.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08511/x4.png",
                "caption": "Figure 3:Diagram of sCoT.During training, the main agent receives a task given by users and decomposes it into subtasks and invokes parameter-sharing subagents iteratively through pure-language CoT steps. This self-calling mechanism allows the model to simulate tool-using behaviors purely in the language domain. Then, the resulting CoT responses are used to calculate rewards, which includes three aspects: accuracy (RaccR_{\\text{acc}}), format (RformatR_{\\text{format}}), and tool-usage (RtoolR_{\\text{tool}}). Finally, GRPO is used to incentivize sCoT.",
                "position": 196
            }
        ]
    },
    {
        "header": "3Preliminary: Thinking with Images",
        "images": []
    },
    {
        "header": "4The Proposed Approach",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08511/x5.png",
                "caption": "Table 2:Comparison on High-Rosolution Benchmarks. The mark * indicates reproduced results and†\\daggermeans results are evaluated using a Qwen2.5-7B-Instruct serving as a judge, while DeepEyes uses Qwen2.5-72B-Instruct.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2512.08511/x5.png",
                "caption": "Figure 4:Training Dynamics. Training process could also be divided into three stages.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2512.08511/x6.png",
                "caption": "Figure 5:Training dynamics w.r.t. existence of constraints on tool calling protocol.",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2512.08511/x7.png",
                "caption": "Figure 6:Reward hacking occurs at step 20 when the ordering constraint is missing.",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2512.08511/x8.png",
                "caption": "Figure 7:Ablation study on impact of training data.",
                "position": 939
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    }
]