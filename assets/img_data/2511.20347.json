[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Soft Adaptive Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20347/x1.png",
                "caption": "Figure 1:Comparison of policy-update objectives under positive advantage. The left panel shows the surrogate objective value; the right panel shows the corresponding gradient weightwi,t​(θ)w_{i,t}(\\theta)as a function of the policy ratiori,t​(θ)r_{i,t}(\\theta).",
                "position": 173
            }
        ]
    },
    {
        "header": "4A Gating‑Function Perspective on SAPO’s Connections to GRPO and GSPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20347/x2.png",
                "caption": "Figure 2:Empirical validation of assumptions (A1)–(A2) on the MoE model (Qwen3-30B-A3B). Left: histogram of token importance ratiosri,t​(θ)r_{i,t}(\\theta). Middle: histogram of per-sequence log-ratio varianceVari​(θ)\\mathrm{Var}_{i}(\\theta). Right: scatter ofVari​(θ)\\mathrm{Var}_{i}(\\theta)versusDi​(θ)D_{i}(\\theta).",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x3.png",
                "caption": "",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x4.png",
                "caption": "",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x5.png",
                "caption": "Figure 3:Empirical validation of assumptions (A1)–(A2) on the dense model (Qwen3-4B). Left: histogram of token importance ratiosri,t​(θ)r_{i,t}(\\theta). Middle: histogram of per-sequence log-ratio varianceVari​(θ)\\mathrm{Var}_{i}(\\theta). Right: scatter ofVari​(θ)\\mathrm{Var}_{i}(\\theta)versusDi​(θ)D_{i}(\\theta).",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x6.png",
                "caption": "",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x7.png",
                "caption": "",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x8.png",
                "caption": "Figure 4:Training reward and validation performance of a cold-start model fine-tuned from Qwen3-30B-A3B-Base under different RL algorithms. SAPO exhibits consistently stable learning and achieves higher final performance compared with GSPO and GRPO-R2, both of which experience early training collapse.",
                "position": 560
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20347/x9.png",
                "caption": "Figure 5:Training reward and validation performance of a cold-start model fine-tuned from Qwen3-30B-A3B-Base using SAPO with different temperature settings. Using a higher temperature for negative tokens (τneg>τpos\\tau_{\\text{neg}}>\\tau_{\\text{pos}}) leads to the most stable training dynamics, whereas settingτneg<τpos\\tau_{\\text{neg}}<\\tau_{\\text{pos}}causes significant instability.",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2511.20347/x10.png",
                "caption": "Figure 6:Training reward and validation performance of Qwen3-VL-30B-A3B from a preliminary cold-start initialization, showing that SAPO achieves consistent improvements and outperforms GSPO and GRPO‑R2 under the same compute budget.",
                "position": 598
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]