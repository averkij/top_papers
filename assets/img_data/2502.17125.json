[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17125/extracted/6228702/files/demo.png",
                "caption": "Figure 1:A web demo of our application built in Streamlit666streamlit.io. It features three input fields: question, context, and answer. The output shows the highlighted hallucinated spans.",
                "position": 95
            }
        ]
    },
    {
        "header": "3Data",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17125/extracted/6228702/files/architecture.png",
                "caption": "Figure 2:The architecture of LettuceDetect. The figure illustrates an example of a Question, Context, and Answer triplet as input to our architecture. After the tokenization step, the tokens are fed into LettuceDetect for token-level classification. Tokens from both the question and the context are masked (indicated by the red line) for loss calculations. In the output of LettuceDetect, we provide probabilities for each answer token. If the output type is span-level, we aggregate subsequent tokens that are hallucinated for the span-level output.",
                "position": 250
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]