[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16972/x1.png",
                "caption": "Figure 1:Pipeline ofSpeech Back-Translation.\nThe main objective is to augment limited training data (≤\\leq≤100 hours) for low-resource languages by synthesizing extensive amounts of speech (>>>10,000 hours). Starting from a multilingual TTS model pre-trained with high-resource languages, we fine-tune it on a small set of seed data, then generate synthetic speech by conditioning the fine-tuned model on a large textual corpus and diverse audio prompts.",
                "position": 181
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Approach: Speech Back-Translation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16972/x2.png",
                "caption": "Figure 2:XTTS inference speedmeasured on a single NVIDIA V100-32GB GPU.\n“DS” refers to DeepSpeed-Inference while “Batch” refers to batch inference.\nFor batch inference, we set batch size to be 16.",
                "position": 377
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16972/x3.png",
                "caption": "Figure 3:Comparison of dataset sizes across seven languages(log-scale y-axis).\nLanguages are categorized by resource availability in the Whisper dataset: (a) high-resource, (b) mid and low-resource groups.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2505.16972/x4.png",
                "caption": "Figure 4:Whisper’s performance improves consistently with larger models and more training data.We train five sizes of Whisper models with up to 160,000 hours of data and conduct evaluation on Common Voice 16. We report averaged WER across seven languages.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2505.16972/x5.png",
                "caption": "Figure 5:Impact of training data quantity and epochs on Vietnamese TTS quality.The purple dashed line shows the WER of natural speech from Fleurs.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2505.16972/x6.png",
                "caption": "Figure 6:Relationship between TTS quality and ASR performance.Higher TTS intelligibility correlates with greater ASR improvement.",
                "position": 674
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInference Optimization Details",
        "images": []
    },
    {
        "header": "Appendix BXTTS vs ChatTTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16972/x7.png",
                "caption": "Figure 7:Comparison of Whisper-medium ASR performance on in-domain (CV16) and out-of-domain (Fleurs) test sets, as well as TTS quality, when training with synthetic Chinese and English speech generated by XTTS and ChatTTS.",
                "position": 1678
            }
        ]
    },
    {
        "header": "Appendix CAudio Prompt Details",
        "images": []
    },
    {
        "header": "Appendix DTextual Data Details",
        "images": []
    },
    {
        "header": "Appendix ETraining Details",
        "images": []
    },
    {
        "header": "Appendix FRelated Work",
        "images": []
    },
    {
        "header": "Appendix G500K-Hour Training Data Statistics",
        "images": []
    },
    {
        "header": "Appendix HAdditional 500K-Hour Scaling Results",
        "images": []
    }
]