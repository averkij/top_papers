[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07041/extracted/5913948/SmallSetfinal_no_evnew.jpg",
                "caption": "Figure 1:Repetition Helps (Left):Performance as a function of repetition for a fixed training budget (600600600600M).GCD (blue). Models trained on smaller datasets, repeated30303030times, perform much better than models trained on one to four epochs.Multiplication mod 67 (red). Models trained for1111to4444epochs do not learn. Learning â€œemergesâ€ when models are trained on smaller data budgets, with increased repetition.Two-set training (Right):For a fixed data budget, splitting the data into tworandomsubsets and increasing the training frequency of one greatly improves performance.GCD (left): repeating 50k examples 3000 times for a training budget of 600M brings performance from 37 to 69 on 100M.Modular multiplication (right): Models trained on 600M single-use examples do not learn. With25252525M examples repeated18181818times, and150150150150M single use examples, accuracy is92%percent9292\\%92 %, with2.52.52.52.5M examples repeated60606060times, and450450450450M single-use, accuracy is68%percent6868\\%68 %. Smooth distributions of repetition over the training set achieve70%percent7070\\%70 %accuracy.",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2410.07041/extracted/5913948/TwoSetnew3.jpg",
                "caption": "",
                "position": 98
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Experimental settings and baselines",
        "images": []
    },
    {
        "header": "4Repetition Helps",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07041/x1.png",
                "caption": "Figure 2:GCD problem:(Left) GCD accuracy for different data and training budgets (average of 5 models). (Right) Test loss of models as a function of training budget, for fixed data budgets.",
                "position": 205
            }
        ]
    },
    {
        "header": "5Two-set training",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07041/x2.png",
                "caption": "Figure 3:Two-set training for the GCD problem:Number of correctly predicted GCD as a function ofSğ‘†Sitalic_Sandpğ‘pitalic_p. Each measurement is the average of6666models. Data budget100100100100M, training budget600600600600M. Note the high performance for very small setsSğ‘†Sitalic_Sof sizes50505050,75757575,100100100100,150150150150and200200200200thousand, withp=0.25ğ‘0.25p=0.25italic_p = 0.25andp=0.5ğ‘0.5p=0.5italic_p = 0.5.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2410.07041/x3.png",
                "caption": "Figure 4:Two-set versus single-set training for the GCD problem:Number of correct GCD as a function of training budget(up to600600600600M) for data budgets of10101010M (left),25252525M (center), and50505050M (right). Two-set training withp=0.25ğ‘0.25p=0.25italic_p = 0.25andS=50,000ğ‘†50000S=50,000italic_S = 50 , 000(top 6 curves) versus single-set training (lower6666curves). See Figure9in AppendixBfor extended TB with DB of50505050M.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2410.07041/x4.png",
                "caption": "Figure 5:Two-set training for Modular Multiplication:Accuracy as a function of small set sizeSğ‘†Sitalic_Sandpğ‘pitalic_p, each averaged over6666models. Data budget100100100100M (left) and unlimited (right), training budget600600600600M. Note: the bottom right of the left graph correspond to single-set10101010M-models: forp=0.1ğ‘0.1p=0.1italic_p = 0.1andS=10ğ‘†10S=10italic_S = 10M, the small and large set are selected with the same probability.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2410.07041/x5.png",
                "caption": "",
                "position": 345
            }
        ]
    },
    {
        "header": "6Ablations and variations",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALearning dynamics and overfitting in math transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07041/x6.png",
                "caption": "Figure 6:Learning curves for eigenvalue computation of 5x5 matrices:Accuracy, train and test loss, for10101010models trained on a data budget of200,000200000200,000200 , 000, as a function of training budget (TB). The curves represent different seeds. Note theinitial phase, common to all curves, up to a sharp transition of test loss atâˆ¼2similar-toabsent2\\sim 2âˆ¼ 2M TB. At this point the dark curves begin to overfit (test loss increases) while the light curves undergo another drop in test loss that initiates thelearning phase.",
                "position": 1271
            }
        ]
    },
    {
        "header": "Appendix BAdditional figures and experiments for modular multiplication",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07041/x7.png",
                "caption": "Figure 7:Learning curves for modular multiplication:Test accuracy for different model initializations. We see a clear step-like learning curve with a plateau just above50%percent5050\\%50 %accuracy before jumping to near perfect accuracy.",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2410.07041/x8.png",
                "caption": "Figure 8:Two-set training for the GCD problem forâˆ\\inftyâˆ-models:Number of correctly predicted GCD as a function of small set sizeSğ‘†Sitalic_Sandpğ‘pitalic_p, each averaged over6666models. Data budgetandtraining budget equal600600600600M (âˆ\\inftyâˆ-models). Note the high performance for very small setsSğ‘†Sitalic_Sof sizes between50505050and200200200200thousand, withp=0.25ğ‘0.25p=0.25italic_p = 0.25andp=0.5ğ‘0.5p=0.5italic_p = 0.5compared to â€œstandardâ€ training with the same data budget, predicting25252525GCD correctly (see Section4).",
                "position": 1294
            },
            {
                "img": "https://arxiv.org/html/2410.07041/x9.png",
                "caption": "Figure 9:Two-set versus single-set training for the GCD problem:Number of correctly predicted (test) GCD as a function of training budget (up to1111B) and data budget of50505050M Two-set training withp=0.25ğ‘0.25p=0.25italic_p = 0.25and|S|=50,000ğ‘†50000|S|=50,000| italic_S | = 50 , 000(top 6 curves) versus single-set training (lower6666curves). With enough TB, single-set training achieves comparable performance with two-set training.",
                "position": 1297
            }
        ]
    },
    {
        "header": "Appendix CAblation results",
        "images": []
    }
]