[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11136/x1.png",
                "caption": "Figure 1:JAFAR upsamples features from any foundation vision encoder to any image resolution, using the input image as high-resolution guidance. It generates sharp, boundary-aligned feature maps and serves as a versatile drop-in module for a variety of downstream tasks—including semantic segmentation, open-vocabulary segmentation, depth estimation, CAM evaluation, and bird’s-eye-view segmentation—consistently enhancing performance.",
                "position": 118
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3JAFAR",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11136/x2.png",
                "caption": "Figure 2:Overview of JAFAR.To construct the upsampling kernel, queries and keys are derived from a shared image representation. Queries are downsampled to match the target output resolution, while keys are downsampled to align with the spatial resolution of the vision encoder’s features. Keys are then semantically enriched via SFT modulation to promote semantic alignment between queries and keys. The resulting kernel is then used to interpolate features from the foundation vision encoder.",
                "position": 210
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11136/x3.png",
                "caption": "Figure 3:PCA Feature Visualization.DINOv2 ViT-S/14 features at32×32323232\\times 3232 × 32resolution from the ImageNet validation set are upsampled to448×448448448448\\times 448448 × 448. Baseline methods—whether training-free, task-dependent, or task-agnostic—introduce varying levels of blurriness and artifacts. Besides being task-agnostic, JAFAR produces sharp, content-aware feature maps with fewer artifacts.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2506.11136/x4.png",
                "caption": "Figure 4:Visual Comparison of Upsampler Outputs in Downstream Tasks.JAFAR-upsampled features produce sharper outputs that align more accurately with object boundaries across various downstream tasks respectively class activations maps, semantic segmentation and depth estimation.",
                "position": 535
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AClass Activation Maps Evaluation",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details on Baselines",
        "images": []
    },
    {
        "header": "Appendix CAdditional Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11136/x5.png",
                "caption": "Figure 5:PCA Feature Visualization.DINOv2 ViT-S/14 features at32×32323232\\times 3232 × 32resolution from the ImageNet validation set are upsampled to448×448448448448\\times 448448 × 448.",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2506.11136/x6.png",
                "caption": "Figure 6:Class Activation Maps comparison.",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2506.11136/x7.png",
                "caption": "Figure 7:Depth Estimation Visualization.",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2506.11136/x8.png",
                "caption": "Figure 8:Semantic Segmentation Visualization.",
                "position": 1893
            }
        ]
    },
    {
        "header": "Appendix DAdditionnal Comparison With Task-Agnostic Baselines",
        "images": []
    },
    {
        "header": "Appendix EInference Time",
        "images": []
    }
]