[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/x1.png",
                "caption": "Figure 1:Overview of robotic manipulation models classified by instruction type and input. Our RoboOmni integrates cross-modal contextual instruction for end-to-end multimodal interaction and action execution.",
                "position": 169
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3OmniAction Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/x2.png",
                "caption": "Figure 2:Overview of OmniAction Dataset Construction Pipelines and Examples.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x3.png",
                "caption": "Figure 3:Distribution of contextual instruction types in OmniAction.",
                "position": 256
            }
        ]
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/x4.png",
                "caption": "Figure 4:The framework ofRoboOmni, a Perceiver-Thinker-Talker-Executor architecture that unifies vision, text, and audio in a shared token space to generate actions and speech.",
                "position": 318
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/x5.png",
                "caption": "Table 2:Performance comparison on OmniAction-LIBERO-Real.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x5.png",
                "caption": "Figure 5:Demonstration of success cases of RoboOmni on the real-world WidowX 250S robot arm.",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x6.png",
                "caption": "(a)Intent recognition capability",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x6.png",
                "caption": "(a)Intent recognition capability",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x7.png",
                "caption": "(b)Qualitative comparison of interaction capabilities.",
                "position": 1008
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x8.png",
                "caption": "Figure 7:Training efficiency comparison between OmniAction-pretrained + SFT vs. from-scratch SFT on OmniAction-LIBERO (Spatial).",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x9.png",
                "caption": "Figure 8:Comparison between end-to-end RoboOmni and cascaded plannerâ€“controller pipelines across six contextual instruction types.",
                "position": 1028
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x10.png",
                "caption": "Figure 9:Per-inference latency comparing cascaded pipelines and RoboOmni.",
                "position": 1040
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of OmniAction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/x11.png",
                "caption": "(a)Distribution of speaker timbres across six demographic categories.",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x11.png",
                "caption": "(a)Distribution of speaker timbres across six demographic categories.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x12.png",
                "caption": "(b)Distribution of audio segment lengths across contextual instruction types.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x13.png",
                "caption": "(a)Common Dataset Skills",
                "position": 1957
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x13.png",
                "caption": "(a)Common Dataset Skills",
                "position": 1960
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x14.png",
                "caption": "(b)Common Dataset Objects",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/events.png",
                "caption": "",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/emotion.png",
                "caption": "",
                "position": 2006
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/identity.png",
                "caption": "",
                "position": 2033
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/overlap.png",
                "caption": "",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/two.png",
                "caption": "",
                "position": 2087
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/three.png",
                "caption": "",
                "position": 2113
            }
        ]
    },
    {
        "header": "Appendix BReal-World Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/x15.png",
                "caption": "Figure 12:Demonstration of success cases of RoboOmni on theIdentity CuesandNon-verbal.",
                "position": 2211
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x16.png",
                "caption": "Figure 13:Demonstration of success cases of RoboOmni on theDyadic DialogueandTriadic Dialogue.",
                "position": 2214
            },
            {
                "img": "https://arxiv.org/html/2510.23763/x17.png",
                "caption": "Figure 14:Comparison of interaction capabilities across four models and six instruction types.",
                "position": 2224
            }
        ]
    },
    {
        "header": "Appendix CPrompt Template",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/D-01.png",
                "caption": "",
                "position": 2582
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/D-02.png",
                "caption": "",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/D-03.png",
                "caption": "",
                "position": 2638
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/D-04.png",
                "caption": "",
                "position": 2665
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/D-05.png",
                "caption": "",
                "position": 2691
            },
            {
                "img": "https://arxiv.org/html/2510.23763/imgs/case_study/D-06.png",
                "caption": "",
                "position": 2717
            }
        ]
    },
    {
        "header": "Appendix DOmniAction-LIBERO",
        "images": []
    }
]