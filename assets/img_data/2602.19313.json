[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19313/x1.png",
                "caption": "Figure 1:Result highlights.TOPRewardenables effective zero-shot estimation of task progress across diverse and challenging real-world manipulation tasks, without task-specific training. By bootstrapping on a range of vision–language model backbones,TOPRewardprovides a temporally consistent visual reward signal that supports multiple downstream applications, including success detection, policy improvement, and evaluation on our in-house benchmark,ManiRewardBench.",
                "position": 112
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TOPReward",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/method_example.png",
                "caption": "Figure 2:Qualitative example of “Fold the Towel”:Instruction-conditioned progress estimation on a real trajectory. The curve showsTOPReward’s predicted completion value over time, with annotated values at selected frames corresponding to semantic subtasks.",
                "position": 216
            }
        ]
    },
    {
        "header": "4ManiRewardBench: a benchmark for reward modeling in robotic manipulation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/voc_by_model_bar_by_group_2.png",
                "caption": "Figure 3:VOC comparison across datasets.Mean dataset-level VOC for GVL (0-shot) andTOPRewardacross two evaluation sets: OXE (39 datasets, 20 episodes each) andManiRewardBench(4 datasets, 113 tasks, 497 episodes). Error bars denote standard deviation across datasets within each evaluation set.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/linear_progress_examples1x4_3.png",
                "caption": "Figure 4:Progress traces for ManiRewardBench.Example progress traces predicted byTOPReward(orange) compared to stage-aware ground-truth completion (dashed) fromManiRewardBench, computed from annotated subtask boundaries. We also overlay Gemini-GVL (blue) on the same episodes when available.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/voc_failure_mode.png",
                "caption": "Figure 5:Illustrative example of the VOC failure mode.Because VOC depends only on the rank order of predicted values (not the absolute completion level), trajectories that rise and then plateau at different final completion levels can all score highly (≥0.85\\geq 0.85). As a result, VOC may not distinguish a well-ordered but incomplete (early-plateau) trajectory from a complete trajectory.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2602.19313/x2.png",
                "caption": "Figure 6:The six real-world single-arm SO-100 manipulation tasks used for advantage-weighted behavior cloning evaluation.",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2602.19313/x3.png",
                "caption": "Figure 7:Qualitative comparison on “Place doll in box.”The pretrained policy and behavior cloning (BC) both fail, while TOP-AWR, fine-tuned with advantage weights fromTOPReward, succeeds consistently. Frames are uniformly sampled from evaluation rollouts.",
                "position": 435
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlternative Reward Formulation",
        "images": []
    },
    {
        "header": "Appendix BWhy theTrueToken?",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/top_token_mass_comparison.png",
                "caption": "Figure 8:Top 10 tokens by absolute difference in mean final-step token probability between successful and failed trajectories. The affirmative tokenTrueshows the largest separation, motivating its use as the completion token inTOPReward. Left: mean token probability by group; right: absolute difference in mean token probability.",
                "position": 1164
            }
        ]
    },
    {
        "header": "Appendix CDataset-level breakdown",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/voc_overall_grid_horizontal.png",
                "caption": "Figure 9:Per-episode VOC distributions, broken down by evaluation set (ManiRewardBenchvs Open X-Embodiment) and model backbone.",
                "position": 1723
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/delta_voc_histogram.png",
                "caption": "Figure 10:Distribution of dataset-levelΔ\\DeltaVOC==VOC(TOPReward)−-VOC(GVL), shown separately for each model backbone. Positive values indicateTOPRewardoutperforms GVL; the dashed line marks the per-model mean.",
                "position": 1726
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/Single_yam.png",
                "caption": "Figure 11:Counts of different example tasks in the single-arm YAM dataset.",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/Bimanual_yam.png",
                "caption": "Figure 12:Counts of different example tasks in the bimanual YAM dataset.",
                "position": 1947
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/s0-100.png",
                "caption": "Figure 13:Counts of different example tasks in the SO-100 dataset.",
                "position": 1953
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/Franka.png",
                "caption": "Figure 14:Counts of different example tasks in the Franka dataset.",
                "position": 1960
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/frequency_of_verbs.png",
                "caption": "Figure 15:Frequency of verbs",
                "position": 1966
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/Tool1.png",
                "caption": "Figure 16:Screenshot of the Annotation Tool.",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/Tool1.png",
                "caption": "",
                "position": 1982
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/Tool2.png",
                "caption": "",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_1.png",
                "caption": "start_second: 0.0",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_1.png",
                "caption": "start_second: 0.0",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_1.png",
                "caption": "start_second: 0.0",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_1.2.png",
                "caption": "end_second: 3.9",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_2.png",
                "caption": "start_second: 4.0",
                "position": 2012
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_2.png",
                "caption": "start_second: 4.0",
                "position": 2015
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_2.2.png",
                "caption": "end_second: 6.4",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_3.png",
                "caption": "start_second: 6.5",
                "position": 2030
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_3.png",
                "caption": "start_second: 6.5",
                "position": 2033
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_3.2.png",
                "caption": "end_second: 9.5",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_4.png",
                "caption": "start_second: 9.6",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_4.png",
                "caption": "start_second: 9.6",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2602.19313/Figure/fold_the_towel_4.2.png",
                "caption": "end_second: 11.4",
                "position": 2055
            }
        ]
    },
    {
        "header": "Appendix DAdditional details ofManiRewardBench",
        "images": []
    }
]