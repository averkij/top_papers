[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11407/x1.png",
                "caption": "Figure 1:Visualization of the hybrid attention mechanism when the window size for local attention (sliding window attention) is set to 3. This figure illustrates how context tokens are processed across layers under the hybrid attention mechanism, highlighting the interaction between local and global attention.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2507.11407/x2.png",
                "caption": "Figure 2:Visualization of repositioning layer normalization. The LayerNorm is applied after input queries and keys, and it is performed after attention output again. The type of normalization is RMSNorm.",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2507.11407/x3.png",
                "caption": "Figure 3:The post-training pipeline of the EXAONE 4.0. The pipeline consists of five stages, which include supervised fine-tuning (SFT), reinforcement learning (RL), and preference learning.",
                "position": 376
            }
        ]
    },
    {
        "header": "3Evaluation",
        "images": []
    },
    {
        "header": "4Limitations",
        "images": []
    },
    {
        "header": "5Deployment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AContributors",
        "images": []
    },
    {
        "header": "Appendix BModel License",
        "images": []
    },
    {
        "header": "Appendix CBaseline models",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.11407/x4.png",
                "caption": "Figure 4:Performance of various models across sixHELMETtask categories,Recall,RAG,Passage Re-ranking,ICL,LongQA, andSummarization, at different context lengths (8K to 128K tokens). Darker cells indicate higher accuracy. Missing entries (N/A) denote models that do not support the corresponding input length or task.",
                "position": 2928
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]