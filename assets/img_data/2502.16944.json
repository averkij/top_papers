[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16944/x1.png",
                "caption": "Figure 1:Overview of Decoupled Value Policy Optimization (DVPO) and PPO in RLHF. DVPO eliminates the need for a reward model and decouples policy and value learning during policy optimization. In contrast, PPO requires training a reward model before policy optimization. DVPO instead trains a global value model using the same offline data as the reward model. During policy training, no additional ground-truth rewards are obtained.",
                "position": 172
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": []
    },
    {
        "header": "5Experiment results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16944/x2.png",
                "caption": "Figure 2:Results of the model on the Ultrafeedback held-out testset. We employed GPT4o as a judge to assess the quality of model-generated responses. Performance is measured using the win rate, whereLeftrepresents DVPO, and Right represents the baseline model for comparison.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2502.16944/x3.png",
                "caption": "Figure 3:Learning curve of the policy model during the RL stage under the Base setting. DVPO demonstrates faster and more stable convergence compared to other methods.",
                "position": 652
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BGVM case study",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16944/x4.png",
                "caption": "Figure 4:An example of the supervisory signal provided by a Global Value Model (GVM). The GVM is capable of providing token-level feedback. In this example, the GVM assigns alower valueto the incorrect response (response2:“is an island”) and ahigher valueto the critical token“not”in the correct response (response1: “not an island”).",
                "position": 1358
            }
        ]
    },
    {
        "header": "Appendix CGVM performance",
        "images": []
    },
    {
        "header": "Appendix DGPT4 evaluation Prompt",
        "images": []
    }
]