[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x1.png",
                "caption": "",
                "position": 531
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x2.png",
                "caption": "Figure 2:Geometric shortcut.We select a point on the sofa arm and compute pairwise similarity with other points. The similarity heatmap reveals that CSC[38]collapses to surface normals, and MSC[88]overfits to point height. In contrast, our Sonata can extract higher-level concepts, as can be seen by the high similarity between all sofa arms highlighted inred.",
                "position": 561
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Pilot Study and Design Principle",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x3.png",
                "caption": "Figure 3:The geometric shortcut is unique to 3D.When comparing the information contained in 2D image and 3D point cloud data after removing the input feature (indicated by color), it is evident that in images all information is within the input feature. Whereas point clouds retain geometric information in point positions, which is directly utilized by operators. This characteristic leads to what we term geometric shortcuts in 3D SSL.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2503.16429/x4.png",
                "caption": "Figure 4:What is learned by the hierarchical backbone?We visualize PCA embeddings from different stages of a hierarchical encoder and decoder, trained forsemantic segmentation. The encoder captures diverse and dispersed feature patterns, indicating a broad range of information. Notably, as the point cloud becomes coarser, accessible geometric information within point coordinates becomes increasingly global. In contrast, the decoder’s representations are more uniform and structured, suggesting a focus on refining features for task-specific outputs.",
                "position": 607
            }
        ]
    },
    {
        "header": "4Point Self-distillation with Sonata",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x5.png",
                "caption": "Figure 5:Self-distillation framework of Sonata.(1) Local views (bottom left) and global views (right) are generated with dedicated spatial and photometric augmentations, while masked views are created by randomly masking out grid-based patches from the global views (top left). (2) Embeddings from local and masked views are extracted by the student, with global views processed by the teacher (top). (3) Points from local and masked views are matched with corresponding points in the global views based on their original spatial distance, allowing for the distillation of embeddings from global views to local and masked views (bottom).",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2503.16429/x6.png",
                "caption": "Figure 6:The roadmap.We evolve Mask Scene Contrast[88]into our Sonata by modernizing self-supervised learning with self-distillation, addressing the geometric shortcut, and scaling up training. Our designs are validated through progressive ablation with linear and decoder probing on ScanNet semantic segmentation[23]. Starting with 23k training data (a combination of ScanNet and Structured3D[109]) and a 39M PTv3 model[89], we ultimately scale up to 140k assets (Tab.2) and a 108M PTv3 model.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2503.16429/x7.png",
                "caption": "Figure 7:Zero-shot comparison with DINOv2.We compare the PCA visualizations of DINOv2, Sonata, and their combined feature representation. DINOv2 excels at capturing photometric details, while Sonata better distinguishes spatial information. The combined model demonstrates improved coherence and detail, showcasing the complementary strengths of both models.",
                "position": 889
            }
        ]
    },
    {
        "header": "5Main Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x8.png",
                "caption": "Figure 8:Zero-shot representation across scenes.We provide PCA-mapped colors and dense matching (with five representative points marked with×\\times×) on a house-scale point cloud from HM3D[68], comprising 2 floors and 12 rooms (left:floor 1,right:floor 2). The visualization demonstrates that Sonata consistently delivers semantically rich and informative representations across diverse indoor environments.",
                "position": 2768
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x9.png",
                "caption": "Figure 9:View generation.Top: we generate global crops using random crops with a crop ratio ranging from 40% to 100% of the minimal number of the raw point cloud size and216superscript2162^{16}2 start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT, combined with random photometric and spatial augmentations. Photometric augmentation is shared among all global views, while spatial augmentation is applied independently to each global view to balance the challenges posed by spatial and photometric features. The first global view is designated as the principal view, and the center of the subsequent global view is restricted to fall within the principal view.Bottom:Local views are generated with a similar pipeline as global views but with a crop ratio restricted to 5% to 40%. All augmentations are applied independently to each local view. Additionally, masked views are obtained by applying random patch masks to the global views.",
                "position": 2812
            }
        ]
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x10.png",
                "caption": "Figure 10:Point self-distillation loss items.The pair-wise point self-distillation between masked views and global views, and between local views and the principal global view. We evenly weight the loss terms for the 8 point self-distillation pairs.",
                "position": 3001
            }
        ]
    },
    {
        "header": "Appendix BAdditional Properties",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16429/x11.png",
                "caption": "Figure 11:Surface reconstruction.Scene surface is reconstructed with SDF regression from frozen Sonata features, demonstrating strong geometric priors and cross-domain generalization.",
                "position": 3082
            }
        ]
    },
    {
        "header": "Appendix CAdditional Comparision",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]