[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00358/x1.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00358/x2.png",
                "caption": "Figure 1:Instead of using pre-determined domains (e.g., by task type), we find that it is often better to first repartition the data into finer-grained, semantically related domains. Optimizing the proportions of these new semantic domains can significantly improve training performance.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x2.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x3.png",
                "caption": "",
                "position": 187
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3R&B: Regroup and Balance Data Mixtures",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00358/x4.png",
                "caption": "Figure 2:Top Row: Across various data settings, we find that there is a ‚Äúsweet spot‚Äù in the number of domains used for data mixing, indicated by the green star. The optimal number of groups varies significantly with the dataset, which motivates the need for compute-efficient data mixing.Bottom Row: We find that silhouette score often correlates with model performance, suggesting that it is possible to predict data mixing performance based on clustering metrics.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x4.png",
                "caption": "",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x5.png",
                "caption": "",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x6.png",
                "caption": "",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x7.png",
                "caption": "",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x8.png",
                "caption": "",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x9.png",
                "caption": "",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x10.png",
                "caption": "",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x11.png",
                "caption": "",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2505.00358/x12.png",
                "caption": "",
                "position": 428
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00358/x13.png",
                "caption": "Figure 3:Left: Regrouping skills before applying data mixing strategies can yield substantial improvements.Underlinedvalues indicate where regrouping beats the original grouping for that method and dataset.Highlightedvalues (with brown background) indicate the best overall performance for each dataset. Note that we do not applyBalance to the original categorization ofSup-NatInsttest, as we assume that training data and validation data are bucketed into the samemùëömitalic_mgroups. Right: Loss curve on the Sup-NatInst dataset.",
                "position": 741
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ANotation",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Results",
        "images": []
    },
    {
        "header": "Appendix CAlgorithm details",
        "images": []
    },
    {
        "header": "Appendix DCompute cost models for online data mixing",
        "images": []
    },
    {
        "header": "Appendix EImplementation Details",
        "images": []
    },
    {
        "header": "Appendix FExperimental Details",
        "images": []
    },
    {
        "header": "Appendix GExtended Training Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00358/x14.png",
                "caption": "Figure 4:Left: Training loss curves for Dolly-15k trained for 40,000 steps with different data mixing methods using the original category partitioning. Right: Average test loss on Dolly-15k after 40,000 training steps using original category partitioning.Highlightedvalues (with brown background) indicate the best overall performance.",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2505.00358/extracted/6403770/figure/clip_proportions.png",
                "caption": "Figure 5:Domain weight evolution during training. Our method dynamically adjusts the importance of each domain throughout the training process, with Domains 1 and 5 eventually receiving the highest weights while Domains 0, 2, 3, 7, 8, and 9 are downweighted over time.",
                "position": 2658
            },
            {
                "img": "https://arxiv.org/html/2505.00358/extracted/6403770/figure/eval_proportions.png",
                "caption": "Figure 6:Comparison between domain proportions in training versus evaluation data (KL Divergence: 1.04). Our method strategically reweights domain distributions during training to optimize performance, notably increasing the representation of Domains 1 and 5 while reducing emphasis on Domains 2, 3, 4, 7, 8, and 9 compared to their evaluation proportions.",
                "position": 2661
            }
        ]
    },
    {
        "header": "Appendix HClustering Interpretation",
        "images": []
    }
]