[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17826/x1.png",
                "caption": "",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2505.17826/x2.png",
                "caption": "Figure 1:The design ofTrinity-RFT.",
                "position": 143
            }
        ]
    },
    {
        "header": "1Vision ofTrinity-RFT",
        "images": []
    },
    {
        "header": "2Key Features",
        "images": []
    },
    {
        "header": "3Design and Implementations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17826/x3.png",
                "caption": "Figure 2:An illustration of data flow inTrinity-RFT.",
                "position": 417
            },
            {
                "img": "https://arxiv.org/html/2505.17826/extracted/6471401/figs/wandb_screencut.png",
                "caption": "Figure 3:A snapshot of the monitor implemented inTrinity-RFT.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2505.17826/extracted/6471401/figs/config_manager_beginner.jpg",
                "caption": "(a)The “beginner” mode.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2505.17826/extracted/6471401/figs/config_manager_beginner.jpg",
                "caption": "(a)The “beginner” mode.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2505.17826/extracted/6471401/figs/config_manager_expert.jpg",
                "caption": "(b)The “expert” mode.",
                "position": 564
            }
        ]
    },
    {
        "header": "4Examples and Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17826/x4.png",
                "caption": "Figure 5:An interactive interface for human preference annotation.",
                "position": 1051
            },
            {
                "img": "https://arxiv.org/html/2505.17826/x5.png",
                "caption": "(a)",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2505.17826/x5.png",
                "caption": "(a)",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2505.17826/x6.jpg",
                "caption": "(b)",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2505.17826/x7.jpg",
                "caption": "(c)",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2505.17826/x8.jpg",
                "caption": "(d)",
                "position": 1114
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOff-Policy Reinforcement Learning Algorithms",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details for Data Pipelines",
        "images": []
    }
]