[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MLRC-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09702/x1.png",
                "caption": "Figure 1:Overview of ourMLRC-Benchand the evaluation pipeline.",
                "position": 722
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09702/x2.png",
                "caption": "Figure 2:We measure Pass@kùëòkitalic_kas we scale the number of trials and ideas, running MLAB for eight trials per idea. The total inference-time computes are equivalent among these points:k=4ùëò4k=4italic_k = 4for one-idea line,k=2ùëò2k=2italic_k = 2for two-idea line,k=1ùëò1k=1italic_k = 1for four-idea line, andk=4ùëò4k=4italic_k = 4for the remaining lines. For results breakdown on each task, please refer to Figure9in AppendixB. Our results indicate that 1) providing high-quality ideas‚Äîespecially human-generated ones‚Äîsignificantly boosts an agent‚Äôs success rate across multiple attempts, 2) while varying the balance between idea exploration and exploitation under a fixed budget yields similar outcomes due to diminishing returns from repeated trials.",
                "position": 1112
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x3.png",
                "caption": "Figure 3:Radar plots of objective and subjective evaluations for agent-generated solutions across seven research tasks. Each dimension is normalized on a 1‚Äì5 scale, where higher values indicate better performance.Objectivemetrics includeeffectiveness,efficiency, andsimplicity, which are highlighted inbold. The rest aresubjectivemetrics, assessed by prompting an LLM as a judge. Notably, more effective solutions identified by agents tend to be more complex and time-consuming (e.g., in backdoor trigger recovery task). Additionally, overlapping scores in subjective dimensions suggest that LLM-based evaluation struggles to distinguish the research capabilities of different models.",
                "position": 1122
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x4.png",
                "caption": "Figure 4:Correlation heatmap between objective (x-axis) and subjective (y-axis) metrics for agent-generated solutions across all tasks. In this setting, code is included when prompting the LLM to evaluate subjective dimensions. No strong correlation is observed, suggesting that LLM-judged subjective metrics may not reliably indicate real-world impact.",
                "position": 1127
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x5.png",
                "caption": "Figure 5:We track the percentages of changes of performance, runtime, and lines of code compared to baseline across iterative refinement of implementations within a trial of LLM-based MLAB agent on the development set. Performance improvement is the higher the better, while increased runtime and lines of code are the lower the better. For results breakdown on each task, please refer to Figure10and11in AppendixB.",
                "position": 1130
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x6.png",
                "caption": "Figure 6:Stage distribution across each step, annotated using GPT-4o and grouped into seven distinct stages to illustrate shifts in task focus and activity over the course of all tasks.",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x7.png",
                "caption": "Figure 7:We perform a cost-effectiveness analysis of various setups. On the x-axis, we plot API cost, where lower is better, and on the y-axis, we show reletive improvement to human (Section3.3), where higher is better. Among the settings evaluated, Llama 3.1 405B with the MLAB scaffolding emerges as a Pareto-optimal setting that balances cost and performance improvement.",
                "position": 1194
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Description of Research Competitions",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09702/x8.png",
                "caption": "",
                "position": 2233
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x9.png",
                "caption": "Figure 9:For each task, we measure Pass@kùëòkitalic_kas we scale the number of trials and ideas, running MLAB for eight trials per idea. Pass@kùëòkitalic_kis the probability that\nat least one of k trials converges to a successful implementation, defined as the agent closes at least 5% of the gap\nbetween baseline and top human participant scores.",
                "position": 2401
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x9.png",
                "caption": "",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x10.png",
                "caption": "",
                "position": 2408
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x11.png",
                "caption": "",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x12.png",
                "caption": "",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x13.png",
                "caption": "",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x14.png",
                "caption": "",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x15.png",
                "caption": "",
                "position": 2431
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x16.png",
                "caption": "Figure 10:For each task, we track the percentages of changes of performance, runtime, and lines of code compared to baseline across iterative refinement of implementations within a trial of LLM-based MLAB agent.",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x16.png",
                "caption": "",
                "position": 2442
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x17.png",
                "caption": "",
                "position": 2447
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x18.png",
                "caption": "",
                "position": 2452
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x19.png",
                "caption": "",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x20.png",
                "caption": "Figure 11:(Cont‚Äôd) For each task, we track the percentages of changes of performance, runtime, and lines of code compared to baseline across iterative refinement of implementations within a trial of LLM-based MLAB agent.",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x20.png",
                "caption": "",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x21.png",
                "caption": "",
                "position": 2471
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x22.png",
                "caption": "",
                "position": 2476
            }
        ]
    },
    {
        "header": "Appendix CPrompts for LLM-as-a-Judge",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09702/x23.png",
                "caption": "Figure 12:Distribution of environment and non-environment errors across different MLRC-Bench tasks. Each task is represented with two pie charts: one for errors related to the environment (e.g., submission issues, argument mismatches) and another for non-environment errors (e.g., runtime failures, memory issues) .",
                "position": 2551
            }
        ]
    },
    {
        "header": "Appendix DAgent Trace Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.09702/x24.png",
                "caption": "Figure 13:Error response distribution across tasks. For each task, errors are classified asFixed(fully resolved),Attempted(partially addressed), orDidn‚Äôt attempt(unresolved). These labels were assigned by GPT-4o-mini after evaluating each error along with all its subsequent steps (action, reflection, thought, and observation).",
                "position": 2651
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x25.png",
                "caption": "Figure 14:Solve rate and average steps taken for resolving various error types. The top chart shows the proportion of errors successfully resolved (Solve Rate), annotated with the total number of instances per error type. The bottom chart illustrates the average number of steps required to achieve resolution, only errors which were fixed were used to calculate average steps.",
                "position": 2676
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x26.png",
                "caption": "Figure 15:Distribution of Actions Taken Across Steps. This visualization depicts how frequently different types of actions were taken at each step by the agent.",
                "position": 2692
            },
            {
                "img": "https://arxiv.org/html/2504.09702/x27.png",
                "caption": "Figure 16:Combined stage timelines across multiple tasks. Each timeline represents an individual run, with block widths proportional to the time spent in each stage. The total duration of each run is shown on the right.",
                "position": 2705
            }
        ]
    },
    {
        "header": "Appendix EAgent Code Samples",
        "images": []
    },
    {
        "header": "Appendix FPrompt for Stage Annotation",
        "images": []
    },
    {
        "header": "Appendix GPrompt for Error Response Annotation",
        "images": []
    }
]