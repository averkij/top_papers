[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x1.png",
                "caption": "Figure 1:Pippo generates high-resolution, multi-view, studio-quality images from a single photo.In each sample, the left-most image is the input, followed by novel generated views of unseen subjects. First and second rows show generations from Full-body and Face-only photos captured in-the-wild using a mobile phone. Third row shows generation from a Head-only studio image. Last row illustrates Pippo‚Äôs capability to faithfully blend observed and generated content, alongside the corresponding ground truth.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x2.png",
                "caption": "Figure 2:Pipeline overview. This is an illustration of how we train our model. (Left) we use data from a studio capture and train our multi-view diffusion model (right). We condition on a full reference photo and a cropped face, as well as the target view cameras and 2D projected spatial anchor indicating head position and orientation. Our diffusion model also takes in noisy target views and a timestep in order to predict the denoised views (top). In practice, we apply a segmentation mask around the person.",
                "position": 187
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x3.png",
                "caption": "Figure 3:DiT and ControlMLP Block. Our DiT block (left) loosely follows[62],\nwith a AdaIn-based timestep modulation.\nWe apply attention and MLP blocks in parallel[13],\nand jointly apply self-attention to the noisy generated and identity conditioning tokens.\nControlMLP block (right) is used to provide lightweight spatially-aligned conditioning -\nPl√ºcker and Spatial Anchor.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2502.07785/x4.png",
                "caption": "Figure 4:Entropy vs Growth Factor (Œ≥ùõæ\\gammaitalic_Œ≥) for varying number of views (tokens) (Sec.3.4). We present the entropy results (Y-axis) from our Attention Biasing technique inspired from[35]for varying number of tokens (individual line plots), and across different scaling growth factorŒ≥ùõæ\\gammaitalic_Œ≥introduced in Eq.¬†(6) (X-axis). On X-axis, \"No scaling\" refers to the default attention formulation[82]andŒ≥=1.0ùõæ1.0\\gamma=1.0italic_Œ≥ = 1.0refers previous work[35]formulation. Empirically, we find that a slightly higher value ofŒ≥=1.4ùõæ1.4\\gamma=1.4italic_Œ≥ = 1.4leads to best visuals.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2502.07785/x5.png",
                "caption": "Figure 5:Generations under varying strengths of growth factorŒ≥ùõæ\\gammaitalic_Œ≥(Sec.3.4). On each row we show the generated views across vanilla attention[82](No scaling), prior work[35]and our formulation Eq.¬†(6). It can be seen that growth factor (Œ≥ùõæ\\gammaitalic_Œ≥) greater than 1.0 is crucial to mitigate the entropy buildup. We show only 10 views per row subsampled evenly from 60 views generated at512√ó512512512512\\times 512512 √ó 512resolution. The model was trained to jointly denoised only 12 views (Ni=5‚àóNtsubscriptùëÅùëñ5subscriptùëÅùë°N_{i}=5*N_{t}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 5 ‚àó italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).",
                "position": 547
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x6.png",
                "caption": "Figure 6:Visual comparison with state-of-the-art methods. We visually compare Pippo with state-of-the-art baselines DiffPortrait3D[24](head-only) and SiTH[29](full-body) generation.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2502.07785/x7.png",
                "caption": "Figure 7:High Resolution Multi-view Generation of Unseen subjects.Pippo enables generation of high-resolution 1K images given only a single image as input (left most in each block, separated with a dashed line). First row LHS shows generation from mobile captured photo, while the RHS shows unseen studio subject. Second row shows mobile captured face-only generations. Third and fourth row shows unseen studio subjects. Last two rows demonstrate simultaneous generation of 10 novel views given unseen studio subject.",
                "position": 1267
            },
            {
                "img": "https://arxiv.org/html/2502.07785/x8.png",
                "caption": "Figure 8:Pippo can handle occluded inputs.We show Pippo‚Äôs generations given incomplete input images ‚Äì such as partially or fully occluded faces, unobserved t-shirt designs on test split of Full-body dataset. We show corresponding ground truth separated with blue dotted line ‚Äì it can be seen that Pippo faithfully follows the known content while auto-completing unseen segments.",
                "position": 1270
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6DiT Architecture and Training",
        "images": []
    },
    {
        "header": "7Diffusion Model and Inference Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x9.png",
                "caption": "Figure 9:Generations under varying strengths of growth factorŒ≥ùõæ\\gammaitalic_Œ≥(Sec.3.4). On each row we show the generated views across vanilla attention[82](No scaling), prior work[35]and our formulation Eq.¬†(6). Growth factor (Œ≥ùõæ\\gammaitalic_Œ≥) greater than 1.0 helps mitigate the entropy buildup, however increasingŒ≥ùõæ\\gammaitalic_Œ≥beyond1.61.61.61.6leads to oversaturation artifacts (somewhat akin to high CFG scale). We show only 6 views per row subsampled evenly from 60 views generated at512√ó512512512512\\times 512512 √ó 512resolution. The model was trained to jointly denoised only 12 views (Ni=5‚àóNtsubscriptùëÅùëñ5subscriptùëÅùë°N_{i}=5*N_{t}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 5 ‚àó italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT).",
                "position": 2785
            }
        ]
    },
    {
        "header": "8Humans-3B¬†: Filtering and Stats",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x10.png",
                "caption": "Figure 10:Statistics of our curated Humans-3B¬†dataset. We bucket these attributes in bins along X-axis and plot their respective sizes normalized between[0,1]01[0,1][ 0 , 1 ]on Y-axis. Filtering with these statistics enable us to retain images with detected-person confidence and image quality.",
                "position": 2857
            }
        ]
    },
    {
        "header": "9Webpage Visuals",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x11.png",
                "caption": "Figure 11:Qualitiative and ablation visuals from pretrained model. Consistent with quantitative\nevaluation, visual quality of generated images improves with using human-centric filtering, and\nimage-conditioned models generate samples which are visually closer to the domain (casual iPhone captures).\nThere is also an obvious boost in quality due to higher resolution.",
                "position": 2867
            }
        ]
    },
    {
        "header": "10Why the name Pippo?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/extracted/6191266/img/pippo.png",
                "caption": "",
                "position": 2877
            }
        ]
    },
    {
        "header": "11Visuals from Pretrained Model (P1 )",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07785/x12.png",
                "caption": "Figure 12:Ablation with Missing or Inconsistent Spatial Anchors.We run inference on the Head-only P1111@512 model with missing spatial anchor, or when it is inconsistent with input head pose rotated downwards towards the floor by 90‚àò.",
                "position": 2906
            },
            {
                "img": "https://arxiv.org/html/2502.07785/x13.png",
                "caption": "Figure 13:Pippo can handle extreme facial expressions. We show generations where reference image comes from unseen Head-only subjects showing extreme facial expression alongside ground truth. We put similar visuals from our Face-only model on webpage.",
                "position": 2925
            },
            {
                "img": "https://arxiv.org/html/2502.07785/x14.png",
                "caption": "",
                "position": 2930
            }
        ]
    },
    {
        "header": "12Frequently Asked Questions",
        "images": []
    }
]