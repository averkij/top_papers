[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/x1.png",
                "caption": "Figure 1:We showUniDisc‚Äôs ability to jointly inpaint image & text pairs. We do not explicitly optimize for this objective but it is intrinsic toUniDisc‚Äôs unified diffusion objective.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3UniDisc: Unified Discrete Diffusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/x2.png",
                "caption": "Figure 2:UniDiscis a unified multimodal discrete diffusion model that can jointly process and generate text and images. Each modality is converted into a sequence of discrete tokens and we jointly denoise, supervising with a weighted cross-entropy loss. At inference time we begin with a set of [MASK] tokens and iteratively unmask tokens.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/icml_scaling_v3.png",
                "caption": "Figure 3:Scaling Analysis for AR andUniDiscmodels: (Left) IsoFLOP curves forUniDisc, plotting varying model size for a fixed FLOP budget. (Right) Estimating optimal parameter size for each budget - minima of fitted parabola, we plot scaling laws for both AR andUniDisc. We find 13.2x more compute is required forUniDiscto achieve the same overall loss as AR.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/icml_scaling_nar_ar_v3.png",
                "caption": "",
                "position": 327
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/x3.png",
                "caption": "Figure 4:Conditional generation results for both FID and CLIP metrics, across a range of CFG values.We find that AR is more sensitive to the CFG weighting, with a narrower optimal range.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x4.png",
                "caption": "",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x5.png",
                "caption": "Figure 5:Inference Comparisons forUniDiscand AR baseline: (a) Chameleon Perplexity (Text+Image) vs. Time - we perform similar to best AR method, (b) Chameleon Perplexity vs. Entropy -UniDischas high diversity and low perplexity, while AR has significantly lower diversity, (c) Image FID vs. NFE, showing image generation saturates quickly with NFE (‚âà32absent32\\approx 32‚âà 32), (d) GPT2 Generative Text Perplexity vs. NFE showing text generation benefits from more sampling steps (diminishing).",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x6.png",
                "caption": "Figure 6:Uniform Concept Generation: We perform joint generation given only masked text input (left). We use a language conditioned segmentation model and find thatUniDiscgenerates uniformly inconcept space(right).",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x7.png",
                "caption": "Figure 8:Zero-shot Image Editing:UniDisccan take corrupted and mismatched image/text pairs (left) and produce an aligned, high-quality pair (right), using the model‚Äôs own likelihood as a scoring function.",
                "position": 504
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUniDisc",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/x8.png",
                "caption": "Figure 9:Conditional generation results for both FID and CLIP metrics, across a range of CFG values.We find that AR is more sensitive to the CFG weighting, with a narrower optimal range.",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x9.png",
                "caption": "",
                "position": 1633
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x10.png",
                "caption": "Figure 10:Unconditional multimodal generation results forUniDiscand AR baseline at 115M parameters - both models perform similarly.",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x10.png",
                "caption": "Figure 10:Unconditional multimodal generation results forUniDiscand AR baseline at 115M parameters - both models perform similarly.",
                "position": 1638
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x10.png",
                "caption": "Figure 12:We compareUniDiscwith an AR model fine-tuned for joint inpainting and evaluate on a subset of DataComp1B.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/datacomp_joint_retrieval.png",
                "caption": "Figure 13:Joint Retrieval Accuracy on DataComp1B.We outperform AR given the task of retrieving one correct image-text pair out of 16 possible pairs, implying better learnt representations.",
                "position": 1792
            }
        ]
    },
    {
        "header": "Appendix CScaling Experiment Details",
        "images": []
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    },
    {
        "header": "Appendix EFine-tuning An Autoregressive model for Discrete Diffusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/lm1b_finetune_v2.png",
                "caption": "Figure 14:Fine-tuning a pre-trained 270M parameter AR model on LM1B.",
                "position": 1928
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/lm1b_finetune_v2.png",
                "caption": "Figure 14:Fine-tuning a pre-trained 270M parameter AR model on LM1B.",
                "position": 1931
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/large_scale_loss.png",
                "caption": "Figure 15:Training Loss Curve vs. Tokens on our 1.4B model.",
                "position": 1936
            }
        ]
    },
    {
        "header": "Appendix FAblations",
        "images": []
    },
    {
        "header": "Appendix GLarge Scale Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/x11.png",
                "caption": "Figure 16:UniDisc‚Äôs ability to generate an image, given unseen text as input.",
                "position": 2045
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x12.png",
                "caption": "Figure 17:UniDisc‚Äôs ability to generate text (captioning), given unseen image as input.",
                "position": 2048
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x13.png",
                "caption": "Figure 18:Zero-shot text-conditioned inpainting.UniDiscinpaints a masked region given a user-provided text prompt.",
                "position": 2051
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x14.png",
                "caption": "Figure 19:Zero-shot multimodal inpainting.UniDiscjointly inpaints in both image and text spaces.",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x15.png",
                "caption": "Figure 20:Zero-shot multimodal editing. We provide acleanimage and text pair andUniDiscautomatically enhances both the image and text. In the final row, we fix the text and allow only the image to change.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x16.png",
                "caption": "Figure 21:We show howUniDiscjointly infills both image and text.argmax‚Å°pŒ∏‚Å¢(x0‚à£xt)argmaxsubscriptùëùùúÉconditionalsubscriptùë•0subscriptùë•ùë°\\operatorname{argmax}p_{\\theta}(x_{0}\\mid x_{t})roman_argmax italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à£ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )",
                "position": 2079
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x17.png",
                "caption": "Figure 22:We demonstrate thatUniDiscuniformly generates all concepts at once.",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x18.png",
                "caption": "Figure 23:We trainUniDiscon 512x512 resolution images but demonstrate zero-shot inpainting at 1024x1024.",
                "position": 2093
            }
        ]
    },
    {
        "header": "Appendix HLarge Scale Quantitative Comparisons",
        "images": []
    },
    {
        "header": "Appendix IUnderstanding the effect of Classifier Free Guidance (CFG)",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/cfg_dist_vs_pct_tokens.png",
                "caption": "Figure 24:L2 distance between unconditional and conditional logits on currently masked tokens as sampling steps increase.",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/cfg_dist_vs_pct_tokens.png",
                "caption": "Figure 24:L2 distance between unconditional and conditional logits on currently masked tokens as sampling steps increase.",
                "position": 2277
            },
            {
                "img": "https://arxiv.org/html/2503.20853/extracted/6312801/images/cfg_v1.png",
                "caption": "Figure 26:We show the effect of classifier-free guidance from left-to-right, starting withw=0ùë§0w=0italic_w = 0, and increasing linearly tow=8ùë§8w=8italic_w = 8on the right, where output logits arelcfg=(1+w)‚Å¢lcond+w‚àóluncondsubscriptùëôcfg1ùë§subscriptùëôcondùë§subscriptùëôuncondl_{\\operatorname{cfg}}=(1+w)l_{\\operatorname{cond}}+w*l_{\\operatorname{uncond}}italic_l start_POSTSUBSCRIPT roman_cfg end_POSTSUBSCRIPT = ( 1 + italic_w ) italic_l start_POSTSUBSCRIPT roman_cond end_POSTSUBSCRIPT + italic_w ‚àó italic_l start_POSTSUBSCRIPT roman_uncond end_POSTSUBSCRIPT.Caption: \"crab meditating, surfboard, orange sun setting, rainbow clouds, zen beach\"",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2503.20853/x19.png",
                "caption": "Figure 27:Generative Perplexity vs. Time with various models and sampling strategies.",
                "position": 2333
            }
        ]
    },
    {
        "header": "Appendix JInference: Generation time vs. batch size",
        "images": []
    }
]