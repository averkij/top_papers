[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25189/x1.png",
                "caption": "Figure 1:An illustration of how InfoAgent leverage search and browse tools to solve information-seeking problems (left). InfoAgent achieves advanced results on several deep research benchmarks (right).",
                "position": 167
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3InfoAgent Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25189/x2.png",
                "caption": "Figure 2:Pipeline for synthesizing multi-entity search questions. (1)Tree Constructionconverts Wikipedia entities into fuzzy-annotated tree structures with configurable branching. (2)QA Generationsamples sub-trees and produces questions.",
                "position": 212
            },
            {
                "img": "https://arxiv.org/html/2509.25189/x3.png",
                "caption": "Figure 3:Tool call distribution analysis on randomly sampled subsets (n=800n{=}800per dataset). Our dataset exhibits both more tool calls and broader distribution compared to ASearcher and DeepDive.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2509.25189/x4.png",
                "caption": "Figure 4:Workflow of our search tool. The search function receives a query and obtain initial results from the search engines. Then it crawls the corresponding web pages and extracts a concise document (≤\\leq128 tokens) for each page, using BM25 filter, Embedding&Reranker Models, and LLM. The browse function receives a URL and extract a longer chunk (≤\\leq2048 tokens) from the web page, using the same extraction pipeline with the search function, but without LLM for final generation.",
                "position": 323
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25189/x5.png",
                "caption": "Figure 5:Left:Average reward (accuracy) per trajectory during the RL training process.Middle:Average tool calls per trajectory. Calls of search function and browse function are counted together.Right:Average ratio of repeated query per trajectory, which is the ratio of # unique queries and # all queries in the trajectory.",
                "position": 560
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Synthesis details",
        "images": []
    },
    {
        "header": "Appendix BDetails of Search Tool",
        "images": []
    },
    {
        "header": "Appendix CMore Details about Training and Evaluation",
        "images": []
    },
    {
        "header": "Appendix DReinforcement Learning with Process Reward",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25189/x6.png",
                "caption": "Figure 6:Distribution of recall rates calculated by different methods",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2509.25189/x7.png",
                "caption": "Figure 7:Training Curve with different weight of bonus",
                "position": 1893
            }
        ]
    },
    {
        "header": "Appendix ECase Study",
        "images": []
    },
    {
        "header": "Appendix FUse of LLM",
        "images": []
    }
]