[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00404/figures/anisotropy.png",
                "caption": "Figure 1:Singular value spectra of the final FeedForward module in pretrained models (Qwen2.5-7B, Qwen3-32B, Qwen2.5-72B, DeepSeek-R1-671B). A small fraction of singular values dominates the spectrum (1.9%, 2.2%, 2.1%, 2.4%), with the share remaining relatively stable across model scales, indicating consistent anisotropy.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/gradient_alignment_two_modules.png",
                "caption": "Figure 2:Gradient singular alignment|ai|=|ùêÆi‚ä§‚ÄãùêÜ‚ÄãùêØi||a_{i}|=|\\mathbf{u}_{i}^{\\top}\\mathbf{G}\\,\\mathbf{v}_{i}|for two representative weight matrices from a middle layer of a 1B-parameter GPT-2 model: the attention key projection (left) and the first feed-forward linear transformation (right). Curves are colored by training step. Across training,|ai||a_{i}|decreases systematically withœÉi\\sigma_{i}, indicating that gradient energy is concentrated on dominant singular values. This preferential allocation accelerates the growth of large singular values, reinforcing their dominance in the spectrum.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/tensor_analysis_9.png",
                "caption": "Figure 3:Analysis of weight, activation, and gradient matrices from a 1B GPT-2 model at 10k steps. The top row shows singular value distributions, and the bottom row displays numerical distributions (log-log scale, frequency-based), overlaid with dashed histograms of rank-1 components (ùêÆi‚ÄãœÉi‚ÄãùêØiT\\mathbf{u}_{i}\\sigma_{i}\\mathbf{v}_{i}^{T}fori=0,16,128,1024i=0,16,128,1024). Large singular values dominate, driving wide numerical distributions.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/fp4_bias_analysis_2.png",
                "caption": "Figure 4:Analysis of the first linear mapping matrix from the deepest feed-forward network layer of a 1B GPT-2 model at 10k steps. (A) Comparison of value distributions before and after quantization, revealing significant loss of small-value information due to clipping to zero. (B) Relative error in singular values, showing larger singular values are less affected by quantization, while smaller ones exhibit greater changes. (C) Absolute cosine similarity of left singular vectors, indicating better directional preservation for larger singular values.",
                "position": 263
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00404/figures/scale_invariant_2.png",
                "caption": "Figure 5:Analysis of the first linear mapping matrix from the deepest feed-forward network layer of a 1B GPT-2 model at 10k steps. The left panel illustrates that the broad distribution of the original parameter matrix results from the superposition of various singular components, with larger singular values predominantly influencing the high-value region and corresponding sub-distributions exhibiting wider spreads. The right panel demonstrates that, when singular values are extracted as scaling factors, the value distributions of all singular components except the largest are narrow and exhibit similar Gaussian-like shapes.",
                "position": 291
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00404/figures/fp8-loss-1.png",
                "caption": "Figure 6:Training loss comparison for 1B-parameter GPT-2 under FP8 quantization. The direct FP8 baseline shows a persistent loss gap relative to the full-precision FP32 baseline, while Metis based FP8 methods (Metis forward full rank SVD (a) and Metis forward 1% rank SVD (b)) closely track the FP32 loss trajectory throughout training, effectively eliminating the degradation observed in the direct FP8 case.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/fp8-loss-1.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/fp8-loss-2.png",
                "caption": "",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/0p13b-loss.png",
                "caption": "Figure 7:For different GPT-2 : (a) 130M GPT-2 training loss curves. (b) 1B GPT-2 training loss curves.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/0p13b-loss.png",
                "caption": "",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/1p1b-loss.png",
                "caption": "",
                "position": 562
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.00404/figures/UV1.png",
                "caption": "Figure 8:(a): Singular value distribution of left and right singular vector matrices during training, showing reduced anisotropy at convergence compared to the original parameter matrix. (b): Numerical distribution of the left and right singular matrices, demonstrating a significantly narrower range than the original matrix.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/UV1.png",
                "caption": "",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2509.00404/figures/UV2.png",
                "caption": "",
                "position": 1336
            }
        ]
    },
    {
        "header": "Appendix AIsotropy Trend in singular space",
        "images": []
    }
]