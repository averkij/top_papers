[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12044/x1.png",
                "caption": "Figure 1:Qualitative comparison of training-free semantic segmentation methods.We compare ITACLIP with SCLIP[60]and NACLIP[24]using images from the COCO-Stuff[8]dataset. Additional visualizations are included in the Appendix.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12044/x2.png",
                "caption": "Figure 2:Overview of ITACLIP.Our method integrates image, text, and architectural enhancements to produce a more accurate segmentation map. We apply various data augmentation techniques, then process both the original and augmented images through a modified image encoder to obtain image embeddings. We also utilize an LLM to generate auxiliary texts (e.g., definitions or synonyms) for each original class name. TheŒªùúÜ\\lambdaitalic_ŒªandŒ±ùõº\\alphaitalic_Œ±symbols denote the image engineering and auxiliary text coefficients used in weighted summations, respectively.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12044/x3.png",
                "caption": "Figure 3:Visualization of attention maps from various layers for a selected patch.The red rectangle indicates the position of the randomly selected patch. Note that we use CLIP-ViT-B/16 as our visual backbone, with Layer 12 serving as the final layer.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.12044/x4.png",
                "caption": "(a)We employ the illustrated prompt to generate definitions.",
                "position": 1802
            },
            {
                "img": "https://arxiv.org/html/2411.12044/x4.png",
                "caption": "(a)We employ the illustrated prompt to generate definitions.",
                "position": 1805
            },
            {
                "img": "https://arxiv.org/html/2411.12044/x5.png",
                "caption": "(b)We employ the illustrated prompt to generate synonyms.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2411.12044/x6.png",
                "caption": "Figure 5:Qualitative comparison of training-free semantic segmentation methods.We compare ITACLIP with SCLIP[60]and NACLIP[24]using images from the Pascal VOC[19], Pascal Context[46], and COCO-Object[39]datasets. ITACLIP consistently outperforms the other approaches. GT denotes the ground truth of the image.",
                "position": 1914
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]