[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14922/x1.png",
                "caption": "Figure 1:Applying SIFT to DeepSeek-R1 demonstrates highly competitive reasoning performance on AIME2024, AIME2025, and MATH-500 (pass@1 accuracy). The results for o1-mini and o3-mini on AIME are referenced fromYe et al. (2025).",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x2.png",
                "caption": "Figure 2:An example of a query and its Sticker.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14922/x3.png",
                "caption": "Figure 3:Factual drift occurs during (i) Sticker generation and (ii) prediction generation from Sticker.",
                "position": 163
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14922/x4.png",
                "caption": "Figure 4:Self-verification occurs during DeepSeek-R1’s reasoning, where the model revisiting the query, focusing on key information, and paraphrasing it.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x5.png",
                "caption": "Figure 5:Four core operations in SIFT: (i) Sticker Generation (SG), (ii) Consensus Prediction (CP), (iii) Forward Optimization (FO), (iv) Inverse Generation (IG).",
                "position": 210
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14922/x6.png",
                "caption": "Figure 6:Comparison of SIFT and traditional Zero-shot CoT across multiple models and datasets.\nWe divide SIFT into three stages: Stage 1 only uses SG & CP, while Stage 2 and Stage 3 optimize the Sticker through forward (+FO) and inverse (+IG) direction, respectively.\nThe bidirectional arrows in the figure highlight the performance gap between Zero-shot CoT and the complete SIFT (i.e., Stage 3).\nWe see that in nearly all scenarios, SIFT leads to a significant performance improvement.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x7.png",
                "caption": "Figure 7:Iterative optimization results for SIFT.\nThe performance improves as the number of tokens per sample increases across different stages.\nSignificant gains are observed in the first repeats of Stage 2 and Stage 3.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x8.png",
                "caption": "Figure 8:Venn diagrams illustrating the accuracy of predictions obtained from the “Only Sticker” and “Query & Sticker” representations at each stage.\nThe percentages represent the accuracy where both methods correctly predict the same outcomes.\nFrom Stage 1 to Stage 2, the accuracy increases by 6.14%, and from Stage 2 to Stage 3, it increases by 4.85%.\nThe results show the significant impact of Forward Optimization (FO) and Inverse Generation (IG) in improving prediction alignment from the two representations.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x9.png",
                "caption": "Figure 9:Comparison of SIFT and standard Self-Consistency (SC) in terms of accuracy versus average tokens per sample. The solid lines represent the output tokens used by SC (blue) and SIFT (red), while the dashed lines indicate the total tokens consumed. The “*” symbol in the legend denotes that the total tokens for SIFT fluctuate due to the additional formatting and example constraints used during inference. SIFT achieves comparable accuracy to SC while using significantly fewer output tokens, demonstrating its efficiency.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x10.png",
                "caption": "Figure 10:Comparison of SIFT-Consistency and Self-Consistency across different numbers of sampled responses per query. SIFT-Consistency consistently outperforms Self-Consistency.",
                "position": 622
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14922/x11.png",
                "caption": "Figure 11:SIFT performance on DeepSeek-R1 with increasing average token count.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x12.png",
                "caption": "Figure 12:Prompt format for generating a Sticker inversely from the prediction.",
                "position": 1127
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x13.png",
                "caption": "Figure 13:Prompt format for generating predictions.",
                "position": 1137
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x14.png",
                "caption": "Figure 14:Prompt format for generating a Sticker from the query.",
                "position": 1140
            },
            {
                "img": "https://arxiv.org/html/2502.14922/x15.png",
                "caption": "Figure 15:Prompt format for forward optimization of the Sticker.",
                "position": 1143
            }
        ]
    },
    {
        "header": "Appendix BPrompting for SIFT",
        "images": []
    }
]