[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21111/x1.png",
                "caption": "Figure 1:Passive Visual Reasoning (top) fails with partial views. Active Visual Reasoning (AVR, bottom) actively interacts to gather information for a correct answer.",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Active Visual Reasoning",
        "images": []
    },
    {
        "header": "4CLEVR-AVR Benchmark and AVR-152k Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21111/x2.png",
                "caption": "Figure 2:Top: CLEVR-AVR Simulator Benchmark, showing distributions of question types, action space, scenes, and examples. Bottom: Higher-order Markov Decision Process (MDP) paradigm for Active Visual Reasoning (AVR).",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x3.png",
                "caption": "Figure 3:AVR-152k Dataset Construction. Left: Workflows for AVR-Caption and AVR-Embodied Reasoning (perception, temporal reasoning). Right: AVR-Core details including its sequential annotation, CoT supervision, and quality verification.",
                "position": 255
            }
        ]
    },
    {
        "header": "5PhysVLM-AVR Model",
        "images": []
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21111/x4.png",
                "caption": "Figure 4:Results for Embodied and Visual Reasoning Tasks.",
                "position": 572
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore details of CLEVR-AVR benchmark",
        "images": []
    },
    {
        "header": "Appendix BMore details of AVR-152k dataset",
        "images": []
    },
    {
        "header": "Appendix CMore details of Model and Training",
        "images": []
    },
    {
        "header": "Appendix DBaseline Model Input Prompt for CLEVR-AVR Experiments",
        "images": []
    },
    {
        "header": "Appendix ECode Availability",
        "images": []
    },
    {
        "header": "Appendix FEthical Considerations and Usage Restrictions",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.21111/x5.png",
                "caption": "Figure A-1:Examples of the three scene types in the CLEVR-AVR benchmark.",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x6.png",
                "caption": "Figure A-2:Question template settings for different question types in the CLEVR-AVR benchmark.",
                "position": 1400
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x7.png",
                "caption": "Figure A-3:Distribution of occlusion and stacking quantities across the three scene types in the CLEVR-AVR benchmark.",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x8.png",
                "caption": "Figure A-4:System prompt for generating AVR-Caption data.",
                "position": 1408
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x9.png",
                "caption": "Figure A-5:Data instance of AVR-Caption.",
                "position": 1412
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x10.png",
                "caption": "Figure A-6:Prompt for DeepSeek-R1 reasoning in AVR-Embodied Reasoning data.",
                "position": 1416
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x11.png",
                "caption": "Figure A-7:Prompt for DeepSeek-V3 to refine Chain-of-Thought (CoT) reasoning in AVR-Embodied Reasoning data.",
                "position": 1420
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x12.png",
                "caption": "Figure A-8:Data instance of AVR-Embodied Reasoning.",
                "position": 1424
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x13.png",
                "caption": "Figure A-9:Prompt for Gemini to refine Chain-of-Thought annotations in AVR-Core data, where {question}, {options}, {answer}, {visual_reasoning}, {hypothesis}, {gain_prediction}, and {planning} are all derived from expert human annotations.",
                "position": 1428
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x14.png",
                "caption": "Figure A-10:Data instance of AVR-Embodied Reasoning. Step 0 of the active visual reasoning process, which make a action decision to get more information.",
                "position": 1432
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x15.png",
                "caption": "Figure A-11:Data instance of AVR-Embodied Reasoning. Step 1 of the active visual reasoning process, which make a action decision to get more information.",
                "position": 1437
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x16.png",
                "caption": "Figure A-12:Data instance of AVR-Embodied Reasoning. Step 2 of the active visual reasoning process, which getting the final answer.",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x17.png",
                "caption": "Figure A-13:Training configuration details for PhysVLM-AVR-3B and AVR-Qwen2.5-VL-7B.",
                "position": 1445
            },
            {
                "img": "https://arxiv.org/html/2510.21111/x18.png",
                "caption": "Figure A-14:Model architecture of PhysVLM-AVR.",
                "position": 1450
            }
        ]
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]