[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24717/x1.png",
                "caption": "Figure 1:Illustration of different image/video generation paradigms.Discrete-space approaches such as AR and MDM adopt non-refinable local generation, where produced tokens are fixed once generated. In contrast, URSA introduces iterative global refinement, conceptually aligning discrete methods with continuous-space approaches, and substantially narrowing their performance gap.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24717/x2.png",
                "caption": "Figure 2:Global refinement via token distance in embedding space.Starting from categorical noisex0x_{0}(left), our framework refines data based on token distance to get target datax1x_{1}(right), enabling hierarchical structure generation from global semantics to fine details.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24717/x3.png",
                "caption": "Figure 3:Sampling performance across inference steps.Using the Cosmos tokenizer, we evaluate the image samples at 256×\\times256 (∼\\sim1K tokens)\nand the video samples at 25×\\times384×\\times240 (∼\\sim10K tokens).",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x4.png",
                "caption": "",
                "position": 1391
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x5.png",
                "caption": "",
                "position": 1392
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x6.png",
                "caption": "",
                "position": 1393
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x7.png",
                "caption": "Figure 4:Sampling performance of different paths.We evaluate the image samples at 256×\\times256.",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x8.png",
                "caption": "",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x9.png",
                "caption": "",
                "position": 1423
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x10.png",
                "caption": "Figure 5:Sampling performance of different model sizes.All models are trained for the same epoch count as in the main experiments and evaluated on 256×\\times256 images and 25×\\times384×\\times240 videos.",
                "position": 1436
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x11.png",
                "caption": "",
                "position": 1441
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x12.png",
                "caption": "Figure 6:Model metrics across training iterations.We sample 256×\\times256 images for evaluation.",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x13.png",
                "caption": "",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x14.png",
                "caption": "",
                "position": 1458
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x15.png",
                "caption": "",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x16.png",
                "caption": "Figure 7:Timestep shifting across SNR schedules.We sample 25×\\times384×\\times240 videos for evaluation.",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x17.png",
                "caption": "",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x18.png",
                "caption": "",
                "position": 1483
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining and Sampling Details",
        "images": []
    },
    {
        "header": "Appendix BVideo extrapolation experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.24717/x19.png",
                "caption": "Figure 8:Zero-shot video extrapolation.We extend the 4-second text-to-video result to 40 seconds.",
                "position": 3011
            },
            {
                "img": "https://arxiv.org/html/2510.24717/x20.png",
                "caption": "Figure 9:Zero-shot start-end frame control.The start-end frames are rendered with transparency.",
                "position": 3024
            }
        ]
    },
    {
        "header": "Appendix CStart-End frame control experiments",
        "images": []
    }
]