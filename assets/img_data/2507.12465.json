[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x1.png",
                "caption": "Figure 1:Visualizations of our PhysXNet for phsycial 3D generation. 3D assets in our dataset have fine-grained physical property annotations, including1)absolute scale,2)material,3)affordance,4)kinematics, and5)function descriptions(basic, functional, and kinematical descriptions).",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3PhysXNet Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x2.png",
                "caption": "Figure 2:Top: Definition of properties in PhysXNet .By defining and annotating properties across three categories, common physical quantities can be systematically calculated to enable physical simulations.Bottom: Overview of our human-in-the-loop annotation pipeline.We utilize GPT-4o to gather foundational raw data, which is subsequently verified through human oversight. The kinematic parameters are then rigorously determined and finalized through human review.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2507.12465/x3.png",
                "caption": "Figure 3:Statistics and distribution of PhysXNet and PhysXNet-XL. (a) Distribution histogram of part number in PhysXNet.\n(b) Dimensional distribution analysis in PhysXNet, showing physical measurements (length/width/height) frequency.\n(c) Proportional composition of kinematic states and material, including density, Young’s modulus, and Poisson’s ratio distribution in PhysXNet, visualized through sectoral ratios.\n(d) Tag frequency statistics for prevalent object labels in PhysXNet-XL.\n(e) Component-Category distribution of procedurally generated 3D objects in PhysXNet-XL.",
                "position": 332
            }
        ]
    },
    {
        "header": "4PhysXGen Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x4.png",
                "caption": "Figure 4:The architecture of PhysXGen framework.PhysXGen features a two-stage architecture comprising: a physical 3D VAE framework for latent space learning, and a physics-aware generative process for structured latent. The former focuses on establishing a compressed yet information-rich latent representation that encodes physical properties, while the latter specializes in generating physical latents.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x5.png",
                "caption": "Figure 5:Visualization of the generated results.Given a single image as the prompt, our PhysXGen can generate the physical-grounded 3D assets.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2507.12465/x6.png",
                "caption": "Figure 6:Qualitative comparison of different methods.We employ MAE-based similarity assessment between generated results and ground-truth forabsolute scale,material, andkinematics, whileaffordanceandfunction descriptionare evaluated through affordance ranking and cosine similarity score with inputted prompt, respectively.",
                "position": 556
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x7.png",
                "caption": "Figure 7:Qualitative comparison of different annotation setting.",
                "position": 1085
            }
        ]
    },
    {
        "header": "Appendix BDetails of human-in-the-loop annotation pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x8.png",
                "caption": "Figure 8:Workflow of our procedural generation method.Leveraging procedural generation within PhysXNet, we automatically generate over 6 million physically plausible 3D assets, forming an extended dataset denoted as PhysXNet-XL.",
                "position": 1183
            }
        ]
    },
    {
        "header": "Appendix CProcedural generation in PhysXNet-XL",
        "images": []
    },
    {
        "header": "Appendix DMore experimental results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12465/x9.png",
                "caption": "Figure 9:Qualitative comparison of different methods.Compared with the existing method, our method achieves robust performance in generating physical 3D assets.",
                "position": 1204
            },
            {
                "img": "https://arxiv.org/html/2507.12465/x10.png",
                "caption": "Figure 10:Qualitative comparison of different architectures.",
                "position": 1207
            },
            {
                "img": "https://arxiv.org/html/2507.12465/x11.png",
                "caption": "Figure 11:Error distribution of different physical properties.",
                "position": 1220
            }
        ]
    },
    {
        "header": "Appendix EFurther analysis on challenges in physical-grounded 3D generation",
        "images": []
    }
]