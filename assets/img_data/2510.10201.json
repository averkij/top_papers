[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10201/x1.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2510.10201/icon/internet.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2510.10201/x2.png",
                "caption": "",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2510.10201/Figs/performance.png",
                "caption": "",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2510.10201/Figs/RLFR.png",
                "caption": "Figure 2:Policy optimized with RLVR prone to overlook potential valuable explorations in reasoning trajectories.\nTo beyond binary verification, auxiliary signals are used for reward shaping of process tokens, involving token entropy and likelihood collected from logit space, where self-policy rewarding risks are non-negligible.\nAlternatively, we show that the latent space is much underexplored yet highly expressive and a well established flow field can be a sound environment for yielding flow reward from velocity deviations and extending RLVR with latent reward utilization.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10201/x3.png",
                "caption": "Figure 3:Distribution of trajectory tokens in LLM reasoning.(a) Distribution of trajectory tokens in latent space (up) and logit space (down).We perform 256 rollouts for prompt randomly sampled from MATHHendrycks et al. (2021). The latent distribution show progressively expressive signals on tail trajectory tokens, as continuously interacting with preceding tokens for context compression. In contrast, neither the logit distribution nor the(b) & (c) textual clouds of reasoning trajectoriesreveal any distinguishable signals, highlighting the potential of latent space for reward utilization.",
                "position": 205
            }
        ]
    },
    {
        "header": "3RLFR",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10201/Figs/wordcloud.png",
                "caption": "Table 3:Case study of reward tokens in training progress. “+” means positive flow reward and “-” means negative flow reward.",
                "position": 917
            },
            {
                "img": "https://arxiv.org/html/2510.10201/Figs/wordcloud.png",
                "caption": "Figure 4:Textual cloud of offline start dataset.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2510.10201/Figs/timesteps.jpg",
                "caption": "Figure 5:Results on different timesteps for flow reward and debiasing effect.",
                "position": 1128
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10201/Figs/training_logs.png",
                "caption": "Figure 6:Training logs ofRLVRandRLFRon Qwen2.5VL-3B.",
                "position": 2182
            }
        ]
    },
    {
        "header": "Appendix BExtended Analysis of Latent Space",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10201/x4.png",
                "caption": "Figure 7:Distribution of reasoning trajectory tokens in latent space across different layer percentiles of the Qwen2.5-Base-7B, which consistently show expressive signals on tail trajectory tokens,\nhighlighting the broader potential of latent space for reward signal collection.",
                "position": 2196
            }
        ]
    },
    {
        "header": "Appendix CTheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]