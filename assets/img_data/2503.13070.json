[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13070/x1.png",
                "caption": "Figure 1:Samples are taken from the corresponding papers of Diff-Instruct++ and RG-LCM. It can be observed that there exist certain artifacts in samples, e.g., repeated text/objects in the background. We hypothesize this comes fromreward hacking.",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x2.png",
                "caption": "Figure 2:4-step samples at 1024 resolution generated byR0. TheR0here is trained fromSD3-mediumpurely by reward maximization.",
                "position": 84
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x3.png",
                "caption": "Figure 3:Log Gradient norm curve of DI++ through training process. We use the best configuration reported in DI++(Luo,2024).",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13070/x4.png",
                "caption": "Figure 4:Comparison withReferee can play(Liu et¬†al.,2024). The baseline samples are taken from their paper. It can be seen that our method has significantly better visual quality.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Rewards are enough",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13070/x5.png",
                "caption": "Figure 5:Samples diffusion models with 100 NFE and 7.5 CFG by varying theŒ∑ùúÇ\\etaitalic_Œ∑in sampling. The samples are generated from the same initial noise. The prompt is ‚ÄúA pikachu, best quality‚Äù.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x6.png",
                "caption": "Figure 6:Cosine Similarity between DI gradient and Reward gradient.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x7.png",
                "caption": "Figure 7:The prior distillation-based reward maximization methods collapse when the reward is chosen to be HPS v2.1. In contrast, our R0 still works well, benefiting from the proposed effective regularization technique.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x8.png",
                "caption": "Figure 8:Training progress of various metrics over iterations. It can be seen that the normalized gradient shows better performance. This is evaluated on 1k prompts from HPS benchmark.",
                "position": 318
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x9.png",
                "caption": "Figure 9:Four-step samples generated by R0. We observed that the quality of the image monotonically increases with the gradual increment of the reward count.",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x10.png",
                "caption": "Figure 10:Thecomplementary effectof different reward. We do not use randomŒ∑ùúÇ\\etaitalic_Œ∑sampling and set small weight regularization in training here to highlight the complementary effect between rewards.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x11.png",
                "caption": "Figure 11:The effect of regularization loss. Images are from the same initial noise.",
                "position": 390
            }
        ]
    },
    {
        "header": "4Photo-Realistic Generation From Rewards",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13070/x12.png",
                "caption": "Figure 12:Comparison on the convergence speed between R0 and R0+. This is evaluated on 1k prompts from HPS benchmark.",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x13.png",
                "caption": "Figure 13:Path comparison between R0 and R0+. The prompt is ‚ÄúTwo dogs, best quality‚Äù.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x14.png",
                "caption": "Figure 14:The effect of high-resolution guidance loss. Images are from the same initial noise.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x15.png",
                "caption": "Figure 15:Qualitative comparisons of R0 against distillation-based methods and diffusion base models on SD-v1.5 backbone. All images are generated by the same initial noise. We surprisingly observe that our proposed R0 has better image quality and text-image alignment compared to prior distillation-based reward maximization methods in 4-step text-to-image generation.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2503.13070/x16.png",
                "caption": "Figure 16:Qualitative comparison against competing methods and applications in downstream tasks.",
                "position": 718
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]