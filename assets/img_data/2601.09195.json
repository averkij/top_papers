[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x1.png",
                "caption": "Figure 1:Breaking the trade-off between training cost and semantic diversity.While Multi-reference SFT offers semantic richness at prohibitive data and computational costs, standard SFT is efficient but semantically limited.\nProFit achieves the best of both: by focusing supervision on high-value tokens, it captures core semantic integrity without sacrificing the efficiency of single-reference training.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries and Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x2.png",
                "caption": "Figure 2:Performance comparison on diverse benchmarks.While multi-reference training (SFT w/ 3 ans) offers sporadic gains, it suffers from optimization instability and stagnation on complex tasks.\nIn contrast,ProFitachieves superior and robust performance across all metrics by selectively extracting high-value signals from a single reference.",
                "position": 283
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x3.png",
                "caption": "Figure 3:Probability density estimation of semantic tokens.\nWe categorize tokens into semanticallyCoreandTrivialgroups.\nWhilecore tokensare heavily concentrated in high-confidence zones,trivial tokensexhibit a significantlong-tail distribution, disproportionately dominating the low-probability spectrum.\na hypothesis test confirms this significant distributional difference (p=1×10−6p=1\\times 10^{-6}).",
                "position": 344
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x4.png",
                "caption": "Figure 4:Ablation study on the probability thresholdτ\\tau.\nThedashed linerepresents the performance of the standard SFT baseline.\n(a) Training exclusively on low-probability tokens (p​(yt∗)<τp(y_{t}^{*})<\\tau) results in performance consistently below the baseline, indicating that non-core expressions are insufficient for constructing effective reasoning chains.\n(b) Conversely, the proposed strategy (p​(yt∗)>τp(y_{t}^{*})>\\tau), which masks low-probability noise, consistently outperforms the baseline across all tasks, validating the effectiveness of focusing on core logic.",
                "position": 741
            }
        ]
    },
    {
        "header": "6Extensive Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x5.png",
                "caption": "Figure 5:Average performance variation across different LoRA ranks (r∈{4,…,1024}r\\in\\{4,\\dots,1024\\}). The dashed lines represent the baseline performance of full-parameter fine-tuning for each corresponding setting. While core tokens (p​(yt∗)>0.1p(y_{t}^{*})>0.1) exhibit monotonic improvement driven by capacity, non-core tokens (p​(yt∗)<0.1p(y_{t}^{*})<0.1) and standard SFT show a U-shaped trend, revealing optimization interference at medium ranks.",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2601.09195/x6.png",
                "caption": "Figure 6:Average performance trajectory across training epochs. ProFit (p>τp>\\tau) demonstrates rapid convergence and a superior performance ceiling, whereas focusing on low-probability tokens (p<τp<\\tau) results in training instability and limited capacity.",
                "position": 830
            },
            {
                "img": "https://arxiv.org/html/2601.09195/x7.png",
                "caption": "Figure 7:Performance trajectories on MATH-500, OlympiadBench, and Minerva datasets. ProFit combines high initial performance with continuous learning capability, ultimately achieving the best Pass@4 and Avg@4 scores across all benchmarks.",
                "position": 833
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AProof of Theorem1",
        "images": []
    },
    {
        "header": "Appendix BHyper parameters",
        "images": []
    },
    {
        "header": "Appendix CDetailed Analysis of LoRA Rank across Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x8.png",
                "caption": "Figure 8:Detailed performance comparison across different LoRA ranks for individual datasets.\nThe trends corroborate our hypothesis: core tokens (p>0.1p>0.1) benefit from increased capacity, while non-core tokens (p<0.1p<0.1) induce optimization interference, particularly at high ranks.",
                "position": 1642
            }
        ]
    },
    {
        "header": "Appendix DDetailed Analysis of Training Dynamics across Epochs",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x9.png",
                "caption": "Figure 9:Performance evolution across training epochs for individual datasets.\nProFit (p>τp>\\tau) exhibits rapid convergence and stability, whereas training on low-probability tokens (p<τp<\\tau) suffers from instability and overfitting to non-core expressions.",
                "position": 1689
            }
        ]
    },
    {
        "header": "Appendix ECase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x10.png",
                "caption": "Figure 10:Training Dynamics in RL Stage. We compare the KL Divergence, Entropy, and Response Length of models initialized with Base, DFT, and ProFit strategies. ProFit demonstrates superior stability (low KL), confident convergence (low Entropy), and evolves deeper reasoning capabilities (highest Response Length).",
                "position": 1723
            }
        ]
    },
    {
        "header": "Appendix FBenchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09195/x11.png",
                "caption": "Figure 11:Visualization of token probability",
                "position": 1951
            }
        ]
    },
    {
        "header": "Appendix GTraining Dynamics Analysis in RL Stage",
        "images": []
    }
]