[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13389/extracted/6077587/images/teaser.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13389/extracted/6077587/images/method/depth_completion_pipeline.png",
                "caption": "Figure 2:Overview ofMarigold-DCinference scheme for depth completion.Our method extends the existing Marigold architecture (above the dashed line) by incorporating task-specific guidance mechanisms (below the line).\nStarting from the current depth latent variableùê≥t(0‚Å¢p‚Å¢t)subscriptsuperscriptùê≥0ùëùùë°ùë°\\mathbf{z}^{(0pt)}_{t}bold_z start_POSTSUPERSCRIPT ( 0 italic_p italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, our method calculates a ‚Äúpreview‚Äù of the final denoised depth map via the Tweedie formula.\nThis preview is then decoded and scaled using the learnable scale parametera^^ùëé\\hat{a}over^ start_ARG italic_a end_ARGand shift parameterb^^ùëè\\hat{b}over^ start_ARG italic_b end_ARG.\nWe backpropagate the loss (red arrows) between the ‚Äúpreview‚Äù and the sparse depth and adjust the latent simultaneously with the scale and shift.\nFinally, we execute a scheduler step to proceed to the next denoising iteration.",
                "position": 253
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.13389/",
                "caption": "Figure 3:Qualitative comparisonof all benchmarked methods on samples from four datasets.\nNon-generative methods struggle with dataset-specific biases, such as input resolution lock or variations in guidance sparsity.Marigold-DCdemonstrates high-quality metric depth densification with strong generalization capabilities.\nThe sampling patterns and noise characteristics vary across datasets.\nBlack regions indicate missing depth measurements.\nArrows suggest key areas of interest.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2412.13389/",
                "caption": "Figure 4:Comparison between vanilla Marigold and guided predictionson iBims-1[33]samples.\nWith guidance from sparse points, the scene geometry is correctly adjusted (1strow), and the depth of challenging protruding text can be recovered (2ndrow).",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2412.13389/",
                "caption": "Figure 5:Ablation of learning rate, number of inference steps, and ensemble size.Left: Empirically, we identify an optimal learning rate that balances the trade-off between guidance strength and optimization stability. Middle: Performance consistently improves with more denoising iterations, showing saturation beyond 50 steps. Right: A monotonic improvement is seen with increasing ensemble size, diminishing after 10 predictions per sample.",
                "position": 687
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]