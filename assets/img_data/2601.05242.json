[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05242/figs/teaser.png",
                "caption": "(a)An overview of our proposed GDPO",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2601.05242/figs/teaser.png",
                "caption": "(a)An overview of our proposed GDPO",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2601.05242/x1.png",
                "caption": "(b)Reward trends: GDPO vs. GRPO",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2GRPO’s propensity for reward signal collapse in multi-reward RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05242/figs/gdpo_grpo.png",
                "caption": "Figure 2:Comparison of GRPO and GDPO advantage computation in a two-binary-reward, two-rollout example. GRPO maps different reward combinations into only two distinct advantage groups, whereas GDPO normalizes each reward independently and retains three distinct groups of advantage values. We skip the batch-wise normalization calculation step in GDPO here for simplicity since it does not change the number of distinct advantage groups.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2601.05242/x2.png",
                "caption": "Figure 3:Comparison of the number of distinct advantage groups produced by GRPO, GRPO without standard deviation normalization (GRPO w/o std), and GDPO. As the number of rollouts (left) or rewards (right) grows, GDPO consistently preserve a substantially larger number of distinct advantage groups compared to GRPO and GRPO w/o std. This results in advantage estimations that provide more expressive training signals.",
                "position": 279
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05242/x3.png",
                "caption": "Figure 4:Median and IQR reward curves across five runs of Qwen2.5-1.5B on the tool-calling task for GDPO, GRPO, and GRPO w/o std. GDPO consistently converges to higher correctness and format rewards, while GRPO w/o std matches correctness gains but fails to converge on the format reward.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2601.05242/x4.png",
                "caption": "Figure 5:Training behavior of GRPO and GDPO on DeepSeek-R1-1.5B across correctness reward, length reward, and maximum batch response length. Both methods rapidly maximize the length reward, briefly suppressing correctness, yet GDPO subsequently recovers it and surpasses GRPO. After roughly 400 steps, GRPO’s correctness score declines and its length-constraint violations increase, as reflected by rising maximum response lengths. In contrast, GDPO continues to improve correctness while steadily improving the control over response length.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2601.05242/x5.png",
                "caption": "Figure 6:Average accuracy and exceed-length ratios for GRPO/GDPO-trained DeepSeek-R1-7B models under varying length reward weights{1.0,0.75,0.5,0.25}\\{1.0,0.75,0.5,0.25\\}, with and without the conditioned length rewardℛ~length\\tilde{\\mathcal{R}}_{\\text{length}}, on mathematical reasoning tasks.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2601.05242/x6.png",
                "caption": "Figure 7:Training curves of GRPO and GDPO with the conditioned length rewardℛ~length\\tilde{\\mathcal{R}}_{\\text{length}}on DeepSeek-R1-7B across correctness reward, length reward.",
                "position": 756
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining stability issue of GDPO without batch-wise advantage normalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05242/x7.png",
                "caption": "Figure 8:Training stability of GDPO with and without batch-wise advantage normalization. Runs without normalization occasionally fail to converge.",
                "position": 1740
            }
        ]
    },
    {
        "header": "Appendix BToolRL Training Prompt Format",
        "images": []
    },
    {
        "header": "Appendix CTool Calling Reward Functions",
        "images": []
    },
    {
        "header": "Appendix DToolRL Hyperparameters Setting",
        "images": []
    },
    {
        "header": "Appendix EMath/Coding Reasoning Hyperparameters Setting",
        "images": []
    },
    {
        "header": "Appendix FTraining curves of GRPO and GDPO when training DeepSeek-R1-7B and Qwen3-4B-Instruct withℛlength\\mathcal{R}_{\\text{length}}andℛcorrect\\mathcal{R}_{\\text{correct}}on math reasoning data.",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05242/x8.png",
                "caption": "Figure 9:Training behavior of GRPO and GDPO when optimizing DeepSeek-R1-7B across correctness reward, length reward, and maximum batch response length on math reasoning data. We can see that GDPO maintains improving correctness and better adherence to length constraints over GRPO.",
                "position": 2062
            },
            {
                "img": "https://arxiv.org/html/2601.05242/x9.png",
                "caption": "Figure 10:Training behavior of GRPO and GDPO when optimizing Qwen3-4B-Instruct across correctness reward, length reward, and maximum batch response length on math reasoning data. We can see that GDPO maintains improving correctness and better adherence to length constraints over GRPO.",
                "position": 2065
            }
        ]
    },
    {
        "header": "Appendix GComparison of GRPO/GDPO finetuned DeepSeek-R1-7B models under varying length reward weights{1.0,0.75,0.5,0.25}\\{1.0,0.75,0.5,0.25\\}with and without the conditioned length rewardℛ~length\\tilde{\\mathcal{R}}_{\\text{length}}on math reasoning tasks",
        "images": []
    }
]