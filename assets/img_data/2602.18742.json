[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18742/x1.png",
                "caption": "Figure 1:Overview of RoboCurate. (1) We generate diverse neural trajectory by applying image-to-image (I2I) model for scene diversity and video-to-video (V2V) model for appearance diversity, respectively. (2) We then filter neural trajectory using simulator-replay consistency, retaining only those for which a classifier predicts the motion in the generated video matches the simulator rollout.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18742/assets/i2i_orig_first.png",
                "caption": "Figure 2:Examples of neural trajectory.(Top): original videos, (Bottom): visually augmented neural trajectory. The two bottom-left frames indicate a video whose initial frame is edited by I2I model, while the two bottom-right frames indicate a video processed with V2V transfer.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/i2i_orig_second.png",
                "caption": "",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/v2v_orig_first.png",
                "caption": "",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/v2v_orig_second.png",
                "caption": "",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/i2i_gen_first.png",
                "caption": "",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/i2i_gen_second.png",
                "caption": "",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/v2v_gen_first.png",
                "caption": "",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/v2v_gen_second.png",
                "caption": "",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2602.18742/x2.png",
                "caption": "Figure 3:Examples of negative pairs for attentive probe training.We construct negative pairs from real-world dataset by inducing temporal shifts or sampling video from different episodes.",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2602.18742/x3.png",
                "caption": "Figure 4:An overview of experimental design for RoboCurate.We conduct two-phase experiments: (1) pre-training on real data and neural trajectory followed by fine-tuning on simulation data, and (2) co-finetuning on real data and neural trajectory.",
                "position": 356
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18742/assets/gr1_tabletop.png",
                "caption": "Figure 5:Visualization of benchmarks.We visualize our benchmark settings (from left to right): (1) GR-1 Tabletop(nvidia2025gr00tn1openfoundation), (2) DexMimicGen(jiang2025dexmimicgen)with bimanual Panda arms with dexterous hands, (3) DexMimicGen with GR-1 humanoid, and (4) a real-robot benchmark on dexterous-hand humanoid robot ALLEX.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/dmg_bimanual_panda.png",
                "caption": "",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/dmg_gr1_frontview.png",
                "caption": "",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2602.18742/assets/allex.jpg",
                "caption": "",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2602.18742/x4.png",
                "caption": "Figure 6:Aggregation of RoboCurate performance.Comparison of VLA performance across different pre-training or co-finetuning settings on GR-1 Tabletop(nvidia2025gr00tn1openfoundation), DexMimicGen(jiang2025dexmimicgen), and real-world benchmark. RoboCurate shows strong performance across all settings.",
                "position": 792
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BNeural Trajectory Generation",
        "images": []
    },
    {
        "header": "Appendix CPrompt for VLM",
        "images": []
    }
]