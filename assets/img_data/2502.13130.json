[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_logo.png",
                "caption": "",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x1.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/intro_fig.png",
                "caption": "Figure 2:A multimodal AI agent should be capable of mutimodal understanding and action-prediction towards a given goal.",
                "position": 184
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multimodal Agentic Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/som_fig.png",
                "caption": "Figure 3:Set-of-Mark supervisions for action grounding on UI screenshot (left), robot manipulation (middle) and human video (right). All coordinates are normalized by image size (height, width) and then quantized into 256 bins. Images better viewed by zooming in.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/tom_fig.png",
                "caption": "Figure 4:Trace-of-Mark supervisions for robot manipulation (left) and human action (right). Same coordinate normalization and quantization is used as SoM. Images show the future traces to predict.",
                "position": 333
            }
        ]
    },
    {
        "header": "4Multimodal Agentic Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/tom_fig5.png",
                "caption": "Figure 5:An illustration of Alg.2to handle videos with camera motions for SoM/ToM generation.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x2.png",
                "caption": "Figure 6:Overview of Pretraining Data Sources.A diverse collection of datasets including instructional videos (orange), robotics manipulation (green), UI navigation (pink), and multimodal understanding (blue). Note that we count the size of each dataset by the number of image samples. For video and robotics data, we extract the images from the short clips and trajectories, respectively.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x3.png",
                "caption": "",
                "position": 566
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_pt_v3.png",
                "caption": "Figure 7:Magma pretraining pipeline. For all training data, texts are tokenized into tokens, while images and videos from different domains are encoded by a shared vision encoder. The resulted discrete and continuous tokens are then fed into a LLM to generate the outputs in verbal, spatial and action types. Our proposed method reconcile the multimodal understanding and action prediction tasks.",
                "position": 601
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13130/x4.png",
                "caption": "Figure 8:SimplerEnv performance comparison on Google Robots and Bridge.Magma(OXE) represents our model trained solely on Open-X-Embodiment (OXE)[22], whileMagmais our pretrained model. Results for each task are averaged across visual matching and variant aggregation scenarios.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x5.png",
                "caption": "Figure 9:Few-shot finetuning and generalization performance on real robot.On a WidowX robot, we evaluateMagmaon 4 tasks including diverse everyday object manipulation.",
                "position": 1451
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/images/magma_libero.png",
                "caption": "Figure 10:Few-shot finetuning results on the LIBEROsimulation benchmark, using 10 trajectories per task for fine-tuning.",
                "position": 1454
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_spatial_visualizations_v2.png",
                "caption": "Figure 11:Spatial evaluation predictions.Spatial reasoning questions are challenging even for GPT-4o butMagmacan answer relatively well despite relying on much fewer pretraining data.",
                "position": 1569
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APretraining and Finetuning",
        "images": []
    },
    {
        "header": "Appendix BDatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13130/x6.png",
                "caption": "Figure 12:Training samples in our Magma-PT-UI.It covers a wide range of action grounding and UI understanding tasks including: (a) Given the bounding box or point coordinates as the query, assistant should return the natural language description or the content. (b) Given the natural language or the exact content as the query, assistant should return the value of the bounding box coordinates.. (c) Given the natural language as the query, assistant should return the value of the point coordinate. (d) Widget captioning. (e) UI summarization.",
                "position": 3915
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x7.png",
                "caption": "Figure 13:Action distributions in three types of action-oriented pretraining datasets. (a) UI Navigation; (b) Robotic Manipulation; (c) Instructional Videos.",
                "position": 3935
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x8.png",
                "caption": "Figure 14:Real robot setup.Magma is deployed on a WidowX 250 robot arm to perform a sequence of kitchen manipulation tasks including object pick-place and soft manipulation.",
                "position": 3962
            },
            {
                "img": "https://arxiv.org/html/2502.13130/x9.png",
                "caption": "Figure 15:Examples for mobile UI navigation sample. We prompt the model with two tasks: “What’s the weather like in Tokyo” and “Install app ‘Instagram’”. The model take actions sequentially given the new observation and history action information.",
                "position": 4263
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_hotdog.png",
                "caption": "aRobot policy rollout for task “Put the sausage to hotdog” for OpenVLA model. (Failure)",
                "position": 4270
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_hotdog.png",
                "caption": "aRobot policy rollout for task “Put the sausage to hotdog” for OpenVLA model. (Failure)",
                "position": 4273
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/openvla_mushroom.png",
                "caption": "bRobot policy rollout for task “Pick up the mushroom to the pot” for OpenVLA model. (Failure)",
                "position": 4279
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_hotdog.png",
                "caption": "cRobot policy rollout for task “Put the sausage to hotdog” for Magma model. (Success)",
                "position": 4285
            },
            {
                "img": "https://arxiv.org/html/2502.13130/extracted/6213812/figures/magma_mushroom.png",
                "caption": "dRobot policy rollout for task “Pick up the mushroom to the pot” for Magma model. (Success)",
                "position": 4291
            }
        ]
    },
    {
        "header": "Appendix CQualitative Analysis",
        "images": []
    }
]