[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04040/x1.png",
                "caption": "Figure 1:RELICis an interactive video world model that allows users to freely explore virtual scenes initialized from an arbitrary first-frame image in real time. Built as a 14B-parameter autoregressive model,RELICgenerates videos at 480×832 resolution, 16 FPS, for up to 20 seconds, exhibiting consistent long-horizon spatial memory.",
                "position": 176
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Data Curation for Interactive Video World Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04040/x2.png",
                "caption": "(a)Video duration distribution of our curated UE dataset.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x2.png",
                "caption": "(a)Video duration distribution of our curated UE dataset.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x3.png",
                "caption": "(b)Action distribution of our curated UE dataset",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x4.png",
                "caption": "Figure 3:The data curation pipeline inRELIC.Given a set of 3D scenes, we manually collect thousands of camera trajectories and generate high-quality video-action-text triplets through a series of data filtering, captioning, and balancing steps.",
                "position": 396
            }
        ]
    },
    {
        "header": "4RELICWorld Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04040/x5.png",
                "caption": "Figure 4:Model Pipeline.Starting from an input image and a sequence of noisy video latents, our DiT-based architecture generates a 20-second video conditioned on text, action labels, and camera poses. Each DiT block integrates YaRN-RoPE, SDPA with QK-Norm, and cross-attention to conditioning tokens. Camera and action information are embedded through dedicated encoders, and conditioning is injected throughout the denoising process to produce spatially consistent and action-aligned video frames.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x6.png",
                "caption": "Figure 5:ODE initialization.We convert a bidirectional video diffusion model into a causal generator by initializing the student on a set of ODE trajectories obtained from the teacher. To achieve this, we adopt a hybrid forcing strategy that combines teacher forcing and diffusion forcing (mask shown on the right).",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x7.png",
                "caption": "Figure 6:Long video distillation with replayed back-propagation.Given a 20-second bidirectional teacher, we distill it into a fast autoregressive student model via self-forcing(huang2025self). We achieve memory-efficient distillation via replayed back-propagation. Specifically, (a) we first let the student model generate the full sequence via self-rollout with gradients disabled. (b) Then we compute and cache the DMD score difference maps over the entire predicted sequence using the critic and teacher models. (c) We re-run the student forward pass with auto-differentiation enabled and back-propagate the corresponding cached score difference maps to accumulate parameter gradients. Parameters are\nupdated once after the full replay.",
                "position": 699
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04040/x8.png",
                "caption": "Figure 7:RELICgenerates high-quality, diverse, and controllable videos while maintaining strong spatial consistency over long horizons. The figure showcases its generalization across varied artistic styles, its ability to follow adjustable camera velocities, and its support for complex multi-key control.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x9.png",
                "caption": "Figure 8:Qualitative comparison of action control.In the “gallery” example (top row), Hunyuan-GameCraft pans the camera left instead of rotating it upward (i.e.Tile Up).\nMatrix-Game-2.0 nominally follows the intended action, but it introduces a large black region at the top of the frame.\nIn the “bedroom” example (bottom row),\nHunyuan-GameCraft rotates the camera instead of translating left.\nMatrix-Game-2.0 drifts left instead of executing the commanded leftward rotation.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x10.png",
                "caption": "Figure 9:Qualitative comparison of memory.Previous methods, such as Hunyuan-GameCraft(li2025hunyuangame)and Matrix-Game-2.0(he2025matrixgame2), forget the bench on the right-hand side in this case quickly.",
                "position": 931
            },
            {
                "img": "https://arxiv.org/html/2512.04040/x11.png",
                "caption": "Figure 10:Qualitative comparison with Marble from World Labs(worldlabs). We compare with a commercial solution, Marble.\nSince the final product from Marble is a Gaussian splatting(kerbl3Dgaussians)rendered result, it inevitably introduces artifacts such as Gaussian floaters.\nOur methodRELIC, instead, generates clean results.",
                "position": 935
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    }
]