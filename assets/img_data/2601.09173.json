[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Distinctness of Stability and Similarity",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x1.png",
                "caption": "Figure 1:Stability and similarity are independent dimensions of representational geometry.(a) Spectral Sensitivity:CKA (red) collapses after removing just the single top principal component, while Shesha (blue) retains sensitivity to the spectral tail. CKA measures dominant variance; Shesha measures full manifold geometry.(b) Universality:Across 2,463 encoder configurations spanning seven domains, Shesha and CKA show negligible net correlation (ρ=−0.01\\rho=-0.01, 95% CI[−0.06,+0.03][-0.06,+0.03]), confirming they capture distinct geometric properties.(c) Regime Analysis:Aggregate near-zero correlation emerges from opposing effects: random transformations yield positive correlation (ρ=+0.76\\rho=+0.76), while PCA compression yields negative correlation (ρ=−0.47\\rho=-0.47). These cancel in aggregate, revealing that Shesha specifically detects compression-induced damage invisible to CKA.",
                "position": 832
            }
        ]
    },
    {
        "header": "3Discovered Phenomena",
        "images": []
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Code Availability",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Outline",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6The Shesha Metric",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x2.png",
                "caption": "Figure 2:Metric Convergence.Shesha estimates remain stable as sample size increases from 400 to 1600 across representative architectures. The flat trajectories confirm rapid convergence and numerical reliability at modest sample sizes.",
                "position": 3056
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x3.png",
                "caption": "Figure 3:Model Leaderboard.Ranking of 15 architectures by Shesha score (feature split). Bar segments show contributions from CIFAR-10 (teal) and CIFAR-100 (blue). Modern architectures with attention or dense connectivity achieve higher geometric stability.",
                "position": 3177
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x4.png",
                "caption": "Figure 4:Seed Stability.Comparison of Shesha scores computed with two different random seeds (Seed A==100 vs. Seed B==200). Points align closely with the diagonal identity line, indicating high reproducibility across random initializations.",
                "position": 3671
            }
        ]
    },
    {
        "header": "7Evaluating Distinction of Stabiltiy and Similarity: Extended Methods and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x5.png",
                "caption": "Figure 5:Spectral Sensitivity Analysis.We measure metric responses as the topkkprincipal components are progressively removed from a power-law representation.(A)Shesha degrades gracefully while all similarity metrics (CKA, PWCKA, Procrustes) collapse after removing just 1 PC.(B)Comparison with whitened Shesha shows high correlation (ρ=0.999\\rho=0.999), though whitening reduces baseline stability.(C)Shesha robustness across preprocessing conditions (raw, centered, normalized, whitened).(D)CKA behavior across preprocessing; notably, whitening causes CKA to recover sensitivity by equalizing the spectrum.",
                "position": 3826
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x6.png",
                "caption": "Figure 6:Metric Dissociation.A scatter plot of Shesha vs. Debiased CKA using balanced sampling across four stability/similarity quadrants. The presence of distinct clusters, particularly the High Stability/Low Similarity quadrant (blue), confirms that stability is mathematically distinct from similarity. The low correlation (ρ=0.20\\rho=0.20) indicates that Shesha measures intrinsic geometric consistency largely independent of pairwise alignment.",
                "position": 4076
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x7.png",
                "caption": "Figure 7:Construct Validity: Ground Truth Recovery.Shesha scores plotted against parametrically controlled stability levels (signal-to-noise ratio) in synthetic representations. The metric shows a near-perfect monotonic response (ρ=0.990\\rho=0.990) to the underlying ground truth, confirming high sensitivity to geometric consistency.",
                "position": 4109
            }
        ]
    },
    {
        "header": "8Steering: Extended Methods and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x8.png",
                "caption": "Figure 9:Task-aligned stability is required for real-world control.Comparison of supervised Shesha (task-aligned geometric consistency) versus\nunsupervised stability (feature-partition consistency) in predicting steering\neffectiveness. A significant gap: unsupervised stability predicts\nsteering in the synthetic setting (ρ=0.77\\rho=0.77) where the data manifold is fully aligned with the task structure but completely fails in the real-world\n(SST-2:ρ=0.10\\rho=0.10; MNLI:ρ=0.35\\rho=0.35). This indicates that for semantic control,\nintrinsic representational rigidity is insufficient. Stability must be aligned\nwith the task manifold for reliable linear intervention.",
                "position": 5064
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x9.png",
                "caption": "Figure 10:Negative controls validate methodology.(A)Shuffled label control: Supervised Shesha computed with true labels\n(dark) versus randomly permuted labels (light). The complete collapse of Shesha\nunder label shuffling (0.60→−0.0010.60\\to-0.001for Synthetic,0.23→−0.0010.23\\to-0.001for SST-2,0.02→−0.0010.02\\to-0.001for MNLI; allp<10−10p<10^{-10}) confirms that the metric captures\ngenuine task-relevant structure rather than spurious geometric patterns.(B)Random direction control: Steering effect using the true probe direction\n(dark) versus averaged effect over 20 random unit vectors (light). True directions\nproduce10.8×10.8\\times(Synthetic),2.7×2.7\\times(SST-2), and1.3×1.3\\times(MNLI) larger\neffects than random, which confirms direction-specific controllability. The decreasing\nratio reflects narrowing steering margins as task complexity increases.",
                "position": 5237
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x10.png",
                "caption": "Figure 11:Model characteristics associated with steerability.Top 5 (colored) and bottom 5 (gray) models ranked by steering effectiveness\n(max_drop) for SST-2 (left) and MNLI (right). Consistent patterns emerge across\ntasks: the most steerable models are from the BGE, E5, and GTE families, all\ntrained with supervised contrastive objectives. The least steerable models are\nunsupervised variants (unsup-simcse,e5-base-unsupervised) and\nretrieval-specialized models (multi-qa-*). This suggests that supervised\ncontrastive training produces representations with the geometric stability\nrequired for reliable linear intervention, while unsupervised or task-misaligned\ntraining yields brittle geometries that fracture under steering.",
                "position": 5408
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x11.png",
                "caption": "Figure 12:Geometric stability predicts linear steerability across all experimental settings.Scatter plots show supervised Shesha (computed on held-out Set A) versus steering\neffectiveness (max_drop, evaluated on disjoint Set B) for each model.(A)Synthetic sentiment data (n=69n=69models):ρ=0.894\\rho=0.894,p<10−24p<10^{-24}.(B)SST-2 binary sentiment (n=35n=35models):ρ=0.962\\rho=0.962,p<10−19p<10^{-19}.(C)MNLI ternary NLI (n=35n=35models):ρ=0.956\\rho=0.956,p<10−18p<10^{-18}.\nDashed lines show linear fits. The near-perfect correlations establish Shesha as a\nstate-of-the-art predictor of controllability, effective across both synthetic and\nnaturalistic settings.",
                "position": 5426
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x12.png",
                "caption": "Figure 13:Shesha captures unique variance beyond class separability.Comparison of raw Spearman correlations (solid bars) and partial correlations\ncontrolling for Fisher discriminant and silhouette score (hatched bars).\nWhile Shesha and Fisher show similar raw correlations with steering effectiveness,\nShesha maintains large partial correlations (ρ=0.62\\rho=0.62-0.760.76, allp<0.001p<0.001)\nafter controlling for separability. This demonstrates that geometric\nconsistency (the reliability of class structure under perturbation) captures\na distinct mechanism enabling control that static separability metrics miss.\nSeparability may be necessary for steering, but stability is what guarantees it.",
                "position": 5487
            }
        ]
    },
    {
        "header": "9Visual Perception Architecture: Extended Methods and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x13.png",
                "caption": "Figure 14:Correlation Structure Across All Datasets.Spearman correlation heatmaps for all six datasets.",
                "position": 5777
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x14.png",
                "caption": "Figure 15:Shesha-Var vs. Shesha-FS Across Datasets.DINOv2 models (red) cluster in the high-Var/low-FS region; CLIP models (blue) maintain high stability.",
                "position": 5780
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x15.png",
                "caption": "Figure 16:Metric Distributions Across Datasets.Violin plots comparing distributions.",
                "position": 5841
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x16.png",
                "caption": "Figure 17:Cross-Dataset Correlation Comparison.Spearman correlations between metric pairs across all datasets.",
                "position": 5969
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x17.png",
                "caption": "Figure 18:Architecture Family Performance Across Datasets.Mean Shesha-Var (left) and Shesha-FS (right) by family.",
                "position": 6039
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x18.png",
                "caption": "Figure 19:Cross-Dataset Rank Stability.Scatter plots comparing Shesha-FS rankings between dataset pairs.",
                "position": 6118
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x19.png",
                "caption": "Figure 20:Geometric Stability Heatmap: Family×\\timesDataset.Red = unstable, green = stable.",
                "position": 6125
            }
        ]
    },
    {
        "header": "10Representational Drift Detection: Extended Methods and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x20.png",
                "caption": "Figure 21:Post-training drift varies substantially across model families.Geometric drift between base and instruction-tuned model pairs, measured by Shesha and CKA, aggregated by model family (23 pairs total).\nThe Shesha/CKA ratio ranges from 1.1×\\times(BLOOM) to 5.2×\\times(Llama), indicating that Shesha consistently detects greater representational reorganization than CKA.\nFamilies with larger ratios exhibit more distributed geometric changes that CKA’s top-principal-component weighting underestimates.\nThis pattern is consistent across model scales within each family.",
                "position": 7571
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x21.png",
                "caption": "Figure 22:Drift magnitude varies by prompt type.Mean geometric drift across 23 base-instruct pairs, stratified by prompt category.\nFactual and descriptive prompts induce the largest Shesha/CKA ratios (2.37×\\timesand 2.28×\\times), while instruction prompts show the smallest ratio (1.44×\\times).\nThis pattern suggests that instruction tuning most strongly reshapes representations for instruction-following inputs (reducing the Shesha-CKA gap) while introducing greater reorganization for out-of-distribution prompt types where the tuning objective provides less direct supervision.",
                "position": 7578
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x22.png",
                "caption": "Figure 23:Metric response to Gaussian noise perturbation.Mean drift across 16 causal LMs as noise magnitude increases (σ∈[0,0.5]\\sigma\\in[0,0.5]).\nShesha exhibits the steepest response curve, reaching 71% drift atσ=0.5\\sigma=0.5compared to 43% for CKA and 42% for Procrustes.\nAt low noise levels (σ<0.1\\sigma<0.1), Procrustes shows elevated sensitivity relative to Shesha and CKA, foreshadowing the false alarm behavior characterized in Experiment 4.\nThe divergence between metrics grows with perturbation magnitude, with Shesha capturing approximately 1.7×\\timesmore drift than CKA at high noise levels.",
                "position": 7839
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x23.png",
                "caption": "Figure 24:Drift scales with LoRA initialization magnitude.Mean drift across 16 causal LMs as LoRA initialization scale increases (rank fixed at 8).\nAll metrics exhibit exponential growth with init scale, but Procrustes maintains a consistent offset above Shesha and CKA across the entire range.\nAt minimal perturbation (init scale =10−310^{-3}), Procrustes already registers detectable drift while Shesha and CKA remain near zero, consistent with Procrustes’s sensitivity to rigid geometric transformations that do not affect functional behavior.\nAt init scale =10−110^{-1}, Shesha reaches 44% drift compared to 33% for CKA, reflecting Shesha’s greater sensitivity to distributed representational changes.",
                "position": 7993
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x24.png",
                "caption": "Figure 25:Predictive validity of geometric drift metrics.Scatter plots showing the relationship between drift magnitude and functional degradation (accuracy drop) across 26 sentence embedding models under Gaussian noise perturbation (σ∈[0.01,0.5]\\sigma\\in[0.01,0.5]).\nAll three metrics exhibit strong correlation with accuracy loss: Shesha (ρ=0.927\\rho=0.927), CKA (ρ=0.937\\rho=0.937), and Procrustes (ρ=0.935\\rho=0.935).\nEach point represents one model at one noise level; dashed lines show linear fits.\nThe consistently high correlation that confirms the validity of this measurement from an empirical standpoint, and serves as the groundwork for using geometric drift as a reliable proxy for functional degradation as a basis to assess performance of models over time.",
                "position": 8115
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x25.png",
                "caption": "Figure 26:Drift trajectories across noise levels.Evolution of Shesha, CKA, Procrustes, and accuracy drop as Gaussian noise magnitude increases (σ∈[0,0.5]\\sigma\\in[0,0.5]) for four representative sentence embedding models.\nThe horizontal dashed line indicates a 5% detection threshold.\nProcrustes consistently exceeds this threshold at lower noise levels than Shesha or CKA, illustrating its heightened sensitivity to geometric perturbations.\nAll metrics track accuracy degradation, but Procrustes’s early activation in low-noise regimes where accuracy remains stable contributes to its elevated false alarm rate.",
                "position": 8271
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x26.png",
                "caption": "Figure 27:Shesha provides earlier warning than CKA.(Left) Distribution of which metric first exceeded the 5% detection threshold across 26 sentence embedding models. Shesha detected drift earlier in 73% of cases (19/26), with the remaining 27% tied; CKA never detected first.\n(Right) Among the 19 cases where metrics diverged, Shesha achieved a 100% win rate.\nThis early warning advantage stems from Shesha’s equal weighting of all pairwise relationships, allowing it to detect subtle structural reorganization before it manifests in the dominant principal components that drive CKA.",
                "position": 8278
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x27.png",
                "caption": "Figure 28:ROC analysis for drift detection on the LoRA perturbation benchmark. All metrics achieve high overall performance (AUC>0.98>0.98), but Shesha provides superior sensitivity at low false alarm rates. At the operationally relevant 5% FPR threshold (vertical line), Shesha maintains 90.2% sensitivity compared to 85.4% for Procrustes, confirming that Shesha’s earlier detection reflects genuine signal rather than noise susceptibility. The ground truth is defined as functional degradation>1%>1\\%accuracy drop.",
                "position": 8646
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x28.png",
                "caption": "Figure 29:False alarm analysis reveals Procrustes oversensitivity.(a) In the stable regime (accuracy drop<1%<1\\%), Procrustes triggers false alarms in 44% of cases compared to only 7.3% for Shesha and CKA (a 6×\\timesdifference).\n(b) At minimal perturbation where functional performance is unchanged, Procrustes reports 1.50% drift versus 0.04% for Shesha (a 37×\\timesinflation).\nThis demonstrates that Procrustes detects rigid geometric transformations that do not affect model behavior, making it unsuitable as a primary monitoring metric despite comparable predictive validity.\nShesha achieves the optimal balance between sensitivity and specificity.",
                "position": 8649
            }
        ]
    },
    {
        "header": "11Transfer Learning and the Limits of Unsupervised Stability",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x29.png",
                "caption": "Figure 30:Transfer experiments summary. (A) Experiment 1 results for Label-RDM Alignment. (B) Experiment 2 results for Label-RDM Alignment. (C) Metric comparison across experiments. (D) Correlation of stability in Experiment 1 across levels ofkk.",
                "position": 9060
            }
        ]
    },
    {
        "header": "12CRISPR Perturbation Magnitudes: Extended Methods and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x30.png",
                "caption": "Figure 31:Geometric stability tracks perturbation magnitude across CRISPR modalities.Stability vs. effect magnitude for four independent datasets spanning CRISPRa, CRISPRi, and pooled screens. Each point represents one perturbation; color indicates local density. Spearman correlations range fromρ=0.746\\rho=0.746[0.641, 0.827] (Dixit) toρ=0.985\\rho=0.985[0.939, 0.997] (Papalexi). Dashed lines show linear fits. The consistency across datasets, modalities, and cell types suggests a universal geometric relationship between effect size and directional coherence.",
                "position": 10240
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x31.png",
                "caption": "Figure 32:Stability-magnitude correlation is robust across distance metrics.Bar chart showing Spearman correlations with 95% bootstrap CIs (error bars) for three distance computation methods: Euclidean (standardL2L_{2}in PCA space), Whitened (Mahalanobis-scaled coordinates), and k-NN (local control centroids). All methods achieve strong correlations (ρ>0.74\\rho>0.74) across all datasets. Notably, whitening substantially improves the Dixit correlation fromρ=0.75\\rho=0.75toρ=0.97\\rho=0.97, suggesting residual covariance structure in PCA space attenuates the relationship in that dataset.",
                "position": 10396
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x32.png",
                "caption": "Figure 33:Combinatorial perturbations exhibit higher geometric stability than single-gene perturbations.Violin plots showing stability distributions for single-gene versus combinatorial (multi-gene) perturbations in Norman et al. (CRISPRa) and Dixit et al. (CRISPRi).\nCombinatorial perturbations show significantly higher stability in both datasets (Mann-WhitneyUU,p<10−9p<10^{-9}), suggesting that multi-target interventions produce more coherent transcriptional responses.\nIndividual perturbations shown as points.",
                "position": 10988
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x33.png",
                "caption": "Figure 34:Discordant perturbations reveal regulatory specificity.Stability versus magnitude for all perturbations in Norman et al. (2019), with CEBP family members (CEBPA, CEBPB, CEBPE) and KLF1 combinations highlighted. CEBP perturbations clusterbelowthe trend line (lower stability relative to their high magnitude), consistent with CEBPA’s known role as a pleiotropic master regulator. KLF1 perturbations clusterabovethe trend line (higher stability relative to magnitude), consistent with its lineage-specific role in erythroid development. Dashed line shows linear fit to all perturbations.",
                "position": 11060
            }
        ]
    },
    {
        "header": "13Neuroscience Drift and Behavior: Extended Methods and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.09173/x34.png",
                "caption": "Figure 37:Geometric stability predicts neural-behavioral coupling.Each point represents one brain area in one session (n=228n=228).\nGeometric stability (Shesha) correlates significantly with trial-by-trial\nneural-behavioral coupling (ρ=0.18\\rho=0.18,p=0.005p=0.005), indicating that\nregions with more stable representational geometry show tighter correspondence\nbetween neural state magnitude and behavioral outcome. Points are colored\nby brain region. Black line shows linear regression with 95% confidence band.",
                "position": 11736
            },
            {
                "img": "https://arxiv.org/html/2601.09173/x35.png",
                "caption": "Figure 38:Regional hierarchy of geometric vs. temporal stability.(A)Geometric stability (Shesha) is highest in action-related regions\n(Striatum, Motor) and lowest in Hippocampus.(B)Temporal stability (centroid similarity) shows an opposing pattern,\nwith sensory regions (Thalamus, Visual) most stable and Striatum least stable.\nThis dissociation indicates that geometric and temporal stability capture\nindependent dimensions of neural population dynamics.\nError bars show 95% bootstrap confidence intervals.\nRegions are ordered by geometric stability in both panels.",
                "position": 11883
            }
        ]
    },
    {
        "header": "14Broader Impact",
        "images": []
    }
]