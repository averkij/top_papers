[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/teaser.png",
                "caption": "Figure 1.CAST brings diverse 3D scenes to life from a single image, where the relations between objects shaped by their physical roles and interactions come together to form a cohesive and immersive virtual environment.",
                "position": 200
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/pipeline1.png",
                "caption": "Figure 2.Overview of the proposed pipeline. The input RGB image is processed through scene analysis to extract key information, followed by pose-aware generation to create initial 3D models. Physical constraint refinement ensures realistic interactions and spatial relations, yielding a high-quality, mesh-based 3D scene.",
                "position": 259
            }
        ]
    },
    {
        "header": "3.Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/pipeline2.png",
                "caption": "Figure 3.Network design of our alignment generation model (Sec.4.2), occlusion-aware object generation model (Sec.4.1), and an illustrative figure of the texture generation model.",
                "position": 325
            }
        ]
    },
    {
        "header": "4.Perceptive 3D Instance Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/sec5.png",
                "caption": "Figure 4.Physics-aware correction via constraint graph mapped from fine-grained relation graph. Top: Floating surfboard grounded on the van. Bottom: Penetrating guitar and cooler separated.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/gallery_small.png",
                "caption": "Figure 5.Bringing the vibrant diversity of the real world into the virtual realm, this collection reimagines open-vocabulary scenes as immersive digital environments, capturing the richness and depth of each unique setting. For each scene, the images display as follows: the top-left shows the input image, the top-center displays the rendered geometry, and the right presents the rendered image with realistic textures.",
                "position": 501
            }
        ]
    },
    {
        "header": "5.Physics-Aware Correction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/compare.png",
                "caption": "Figure 6.Qualitative comparisons of CAST with state-of-the-art single-image scene reconstruction methods. From left to right: Input image, CAST, ACDC, and Gen3DSR. Top to bottom: random open vocabulary dataset (rows 1–3), Gen3DSR input (rows 4–5), ACDC input (rows 6–7).",
                "position": 652
            }
        ]
    },
    {
        "header": "6.Result",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/Ablation_occlusion.png",
                "caption": "Figure 7.We evaluate the generation performance with and without the occlusion-aware generation module. The RGB and normal renderings of the object highlight the significance of this module in ensuring the completeness and high quality of the generated object.",
                "position": 795
            },
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/Ablation_pcd.png",
                "caption": "Figure 8.A stack of books with varying lengths and widths directly generated as a single complex object, demonstrating how point cloud conditioning enhances the preservation of scale, dimensions, and local details compared to traditional methods.",
                "position": 815
            },
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/Ablation_pose.png",
                "caption": "Figure 9.Comparative evaluation of pose estimation methods. Our pose alignment module demonstrates superior alignment accuracy compared to Iterative Closest Point (ICP) and differentiable rendering (DR).",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/Ablation_simulator.png",
                "caption": "Figure 10.Comparison of scene reconstruction with and without relational graph constraints. By integrating relational graph constraints, our method ensures both physical plausibility and accurate alignment with the intended scene, maintaining correct spatial relations.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/application.png",
                "caption": "Figure 11.CAST enables realistic physics-based animations, immersive game environments, and efficient real-to-simulation transitions, driving innovation across various fields.",
                "position": 914
            }
        ]
    },
    {
        "header": "7.Conclusions",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12894/extracted/6434240/figs/failure.png",
                "caption": "Figure 12.In some scenes, transparent glass, textiles, and fabrics are difficult to express, as the mesh struggles to represent them realistically.",
                "position": 926
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGPT-4v prompt",
        "images": []
    }
]