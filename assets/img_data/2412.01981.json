[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01981/x1.png",
                "caption": "Figure 1:The x-axis indicates the FLOPs required to collect the data and train the model,\nand y axis the accuracies of best-of-64 performance.\nThe accuracy is averaged over the best-of-64 accuracies of Mistral-7B-Instruct-v0.2(Jiang et al.,2023), Llama-3.1-8B-Instruct, and Llama-3.1-70B-Instruct(Meta,2024)on MATH(Hendrycks et al.,2021).\nDifferent dots on the same line indicates models trained with the same approach but on different scales of data.\nThe top-left zone is desirable in this figure, as it suggests a model can achieve higher performance with less development overhead.\nOur implicit PRM is much cheaper to train while presenting the best performance under the same budget.",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2ORMs vs. PRMs: Dilemma of Performance and Expense",
        "images": []
    },
    {
        "header": "3Implicit PRMs For Free Through Reward Parameterization",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01981/x2.png",
                "caption": "Figure 2:Overhead of developing different PRMs, in terms of FLOPs during data collection and training.\nThe X axis indicates the number of responses per instruction which determines the scale of training data, and the Y axis is the number of FLOPs. Our implicit PRM always consumes the least FLOPs compared to baselines, with CE being 38.6×\\times×to 38.8×\\times×more efficient than Math-Shepherd across different dataset scales.",
                "position": 703
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01981/x3.png",
                "caption": "Figure 3:Results with majority voting. We present the averaged best-of-N accuracy across three testsets.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2412.01981/x4.png",
                "caption": "(a)Implicit PRM (DPO).",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2412.01981/x4.png",
                "caption": "(a)Implicit PRM (DPO).",
                "position": 748
            },
            {
                "img": "https://arxiv.org/html/2412.01981/x5.png",
                "caption": "(b)Implicit PRM (CE).",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2412.01981/x6.png",
                "caption": "(a)Implicit PRM (DPO).",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2412.01981/x6.png",
                "caption": "(a)Implicit PRM (DPO).",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2412.01981/x7.png",
                "caption": "(b)Implicit PRM (CE). Note that one repsonse per instruction is the extreme case of the unpaired setup.",
                "position": 770
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition",
        "images": []
    }
]