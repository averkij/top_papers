[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17691/x1.png",
                "caption": "Figure 1:Scaling laws for predictingQuantization-inducedDegradation (QiD, denoted asΔq⁢L⁢o⁢s⁢ssubscriptΔ𝑞𝐿𝑜𝑠𝑠\\Delta_{q}Lossroman_Δ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s) in 7B, 70B, and 405B models trained on up to 100 trillion (1014superscript101410^{14}10 start_POSTSUPERSCRIPT 14 end_POSTSUPERSCRIPT) tokens. While low-bit quantization yields acceptable QiD for undertrained LLMs (trained with≤1012absentsuperscript1012\\leq 10^{12}≤ 10 start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPTtokens), it becomes undesirable when applied to fully trained LLMs (e.g., trained with 100 trillion tokens, a milestone expected to be reached in the next few years), particularly for smaller models. Note that thegray areasin this figure indicate levels of QiD that degrade the model’s predictions to a level worse than random guessing.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x1.png",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x2.png",
                "caption": "",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x3.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17691/x4.png",
                "caption": "Figure 2:Performance of LLMs after low-bit quantization at different sizes and training levels. It is obvious that the models which are smaller or trained with more tokens suffer from greater quantization-induced degradation.",
                "position": 111
            }
        ]
    },
    {
        "header": "2Preliminary: Scaling Laws for Large Language Models",
        "images": []
    },
    {
        "header": "3Scaling Laws for Low-bit Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17691/x5.png",
                "caption": "Figure 3:The fitted scaling law of QiD with respect to the number of training tokens in the form of Eq (5), whereβ𝛽\\betaitalic_βis fitted to be 0.5316.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x6.png",
                "caption": "Figure 4:The fitted scaling law of QiD with respect to the model size (i.e., the number of non-embedding parameters) in the form of Eq (6), whereα𝛼\\alphaitalic_αis fitted to be 0.2276.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x7.png",
                "caption": "Figure 5:The fitted scaling law of QiD with respect to the bit width in the form of Eq (7), whereγ𝛾\\gammaitalic_γis fitted to be 5.4812.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x8.png",
                "caption": "Figure 6:The unified scaling law we fit based on Eq (8) with the GPTQ-quantized LLMs from the Pythia suite:Δq⁢L⁢o⁢s⁢s⁢(N,D,P)=0.017⁢D0.5251/(N0.2261⋅P5.4967)subscriptΔ𝑞𝐿𝑜𝑠𝑠𝑁𝐷𝑃0.017superscript𝐷0.5251⋅superscript𝑁0.2261superscript𝑃5.4967\\Delta_{q}Loss(N,D,P)=0.017D^{0.5251}/(N^{0.2261}\\cdot P^{5.4967})roman_Δ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s ( italic_N , italic_D , italic_P ) = 0.017 italic_D start_POSTSUPERSCRIPT 0.5251 end_POSTSUPERSCRIPT / ( italic_N start_POSTSUPERSCRIPT 0.2261 end_POSTSUPERSCRIPT ⋅ italic_P start_POSTSUPERSCRIPT 5.4967 end_POSTSUPERSCRIPT )",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x9.png",
                "caption": "Figure 7:We can predict the performance of a quantized LLM asL⁢o⁢s⁢sq=L⁢o⁢s⁢s16-bit+Δq⁢L⁢o⁢s⁢s𝐿𝑜𝑠subscript𝑠𝑞𝐿𝑜𝑠subscript𝑠16-bitsubscriptΔ𝑞𝐿𝑜𝑠𝑠Loss_{q}=Loss_{\\textrm{16-bit}}+\\Delta_{q}Lossitalic_L italic_o italic_s italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT 16-bit end_POSTSUBSCRIPT + roman_Δ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s, whereL⁢o⁢s⁢s16-bit𝐿𝑜𝑠subscript𝑠16-bitLoss_{\\textrm{16-bit}}italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT 16-bit end_POSTSUBSCRIPTcan be predicted by the conventional LLM’s scaling law which is fitted based on the function form of Eq (3) with the LLMs in the Pythia suite asL⁢o⁢s⁢s16-bit=[(4.74⁢e19/N)(0.045/0.399)+7.63⁢e10/D]0.399𝐿𝑜𝑠subscript𝑠16-bitsuperscriptdelimited-[]superscript4.74superscript𝑒19𝑁0.0450.3997.63superscript𝑒10𝐷0.399Loss_{\\textrm{16-bit}}=[(4.74e^{19}/N)^{(0.045/0.399)}+7.63e^{10}/D]^{0.399}italic_L italic_o italic_s italic_s start_POSTSUBSCRIPT 16-bit end_POSTSUBSCRIPT = [ ( 4.74 italic_e start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT / italic_N ) start_POSTSUPERSCRIPT ( 0.045 / 0.399 ) end_POSTSUPERSCRIPT + 7.63 italic_e start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT / italic_D ] start_POSTSUPERSCRIPT 0.399 end_POSTSUPERSCRIPT.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x10.png",
                "caption": "Figure 8:QiD results evaluated on RefinedWeb and Wikitext-2 with the 12B Pythia model.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x11.png",
                "caption": "Figure 9:QiD results and fitted scaling laws for different quantization methods. Note that the GPTQ function here differs slightly from that in Figure6, as it is fitted exclusively with 4-bit quantized Pythia checkpoints, whereas the function in Figure6is fitted using all quantized Pythia checkpoints.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x12.png",
                "caption": "Figure 10:Left:Scaling laws for low-bit quantization, fitted on the LLM checkpoints of the Spectra suite, which are all trained with 300B tokens;Right:ActualΔq⁢L⁢o⁢s⁢ssubscriptΔ𝑞𝐿𝑜𝑠𝑠\\Delta_{q}Lossroman_Δ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_sVSPredictedΔq⁢L⁢o⁢s⁢ssubscriptΔ𝑞𝐿𝑜𝑠𝑠\\Delta_{q}Lossroman_Δ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_sthat is computed based on the scaling laws fitted on Llama and Qwen.",
                "position": 373
            }
        ]
    },
    {
        "header": "4Discussion: Low-bit Quantization Favors Undertrained LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17691/x13.png",
                "caption": "Figure 11:Fully trained LLMs suffer from much greater QiD (i.e.,Δq⁢L⁢o⁢s⁢ssubscriptΔ𝑞𝐿𝑜𝑠𝑠\\Delta_{q}Lossroman_Δ start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_L italic_o italic_s italic_s) than undertrained LLMs.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x14.png",
                "caption": "Figure 12:Changes in model weights between adjacent checkpoints. Early (undertrained) checkpoints exhibit significant weight fluctuations during training, making the model relatively robust to weight variations. Therefore, small changes introduced by quantization have a limited impact on the model’s performance. In contrast, fully trained checkpoints demonstrate very little weight fluctuations during training. As a result, low-bit quantization is likely to push weightsbeyond the narrow range of recent variations, leading to performance degradation or even model collapse.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x15.png",
                "caption": "Figure 13:The number of training tokens for the state-of-the-art 7B-scale LLMs increase by nearly50×50\\times50 ×over the past 4 years. According to this trend, it is expected that the future models will have much more training tokens.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x16.png",
                "caption": "Figure 14:Training losses of BitNet and its 16-bit counterparts show a trend similar to that of low-bit quantization – they tend to perform well when undertrained but struggle to match the performance of fully trained LLMs.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x16.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2411.17691/x17.png",
                "caption": "",
                "position": 537
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]