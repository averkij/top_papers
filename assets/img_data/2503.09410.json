[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09410/x1.png",
                "caption": "Figure 1:Advantage of Monte Carlo diffusion.Model-1 and Model-2 denote NG-RANSAC[6]trained on SIFT[24]and LoFTR[35], respectively. The green lines indicate inliers, and the red ones are outliers. As shown in the blue box, the models trained on specific patterns show limited generalization on out-of-distribution data, e.g., Model-2 trained on LoFTR performs poorly when tested on SIFT. In contrast, we propose a diffusion-based training mechanism where training data is agnostic to specific patterns through a Monte Carlo diffusion process. NG-RANSAC trained on diffused matches demonstrates better generalization across different initial matches.",
                "position": 73
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09410/x2.png",
                "caption": "Figure 2:Pipeline of the diffusion process.We leverage diffusion to simulate noisy data for training learning-based RANSAC. Given ground-truth matches𝐂gtsubscript𝐂gt\\mathbf{C}_{\\text{gt}}bold_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPTbetween two images, we randomly split them into two subsets𝐂gtasuperscriptsubscript𝐂gt𝑎\\mathbf{C}_{\\text{gt}}^{a}bold_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPTand𝐂gtbsuperscriptsubscript𝐂gt𝑏\\mathbf{C}_{\\text{gt}}^{b}bold_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT.𝐂gtasuperscriptsubscript𝐂gt𝑎\\mathbf{C}_{\\text{gt}}^{a}bold_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPTis processed by a Monte Carlo diffusion module with multi-stage randomization, generating multiple sets of noised matches at different timesteps. The final diffused matches are formed by combining𝐂gtbsuperscriptsubscript𝐂gt𝑏\\mathbf{C}_{\\text{gt}}^{b}bold_C start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPTas inliers with𝐂nbsuperscriptsubscript𝐂𝑛𝑏\\mathbf{C}_{n}^{b}bold_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPTsampled at timesteptisubscript𝑡𝑖t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTas outliers. The learning-based RANSAC is then trained on the resulting diffused matches.",
                "position": 111
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09410/x3.png",
                "caption": "Figure 3:Illustration of the multi-stage randomization module.We randomly sample the three hyperparameters, timestept𝑡titalic_t, diffusion ratior𝑟ritalic_r, and noise scales𝑠sitalic_s, in the diffusion mechanism. This multi-stage randomization introduces different sources of randomness into the noised matches, affecting the diffusion intensity, outlier ratio, and noise level, respectively. Invalid matches in the tentative set are replaced by randomly sampled matches, which ensures the validity of the final diffused matches.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2503.09410/x4.png",
                "caption": "Figure 4:Visualization of diffused matches.Given the same image pair, different values of diffusion ratio and noise scale result in significantly different diffused matches.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09410/x5.png",
                "caption": "Figure 5:Qualitative results.Init. Matches represent the initial correspondences generated by SIFT. Baseline and Ours indicate the pruned results using NG-RANSAC trained on LoFTR and diffused matches, respectively. The green and red lines denote inliers and outliers. The baseline shows limited generalization to SIFT, which serves as out-of-distribution data, leading to many outliers after the pruning. In contrast, our method achieves significantly better generalization, identifying more inliers.",
                "position": 312
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]