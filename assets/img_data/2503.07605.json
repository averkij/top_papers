[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07605/x1.png",
                "caption": "Figure 1:Visualization of hidden statesh‚Å¢(P)‚ÑéùëÉh(P)italic_h ( italic_P )from different tasks. Each point represents the activation of a hidden state in the model for a specific task. The clustering patterns illustrate how tasks with similar requirements tend to activate similar regions in the model.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2503.07605/x2.png",
                "caption": "Figure 2:Framework of the SEAP approach. Theleftside shows theMotivation Discoveryphase, where task-specific activation patterns are identified by analyzing hidden states and neuron activations extracted from the task corpus. Therightside illustrates theTraining-free Sparse Expert Activation Pruningprocess, consisting of five main steps described in Section2.1.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07605/extracted/6267883/figures/l2norms.png",
                "caption": "Figure 3:Heatmaps of dimension-wise average normalized‚Ñì‚Ñì\\ellroman_‚Ñì2 norms for different tasks. Each row corresponds to a layer or module, and each column represents a dimension in the hidden state space. The top and bottom parts of the figure show activation patterns from two randomly selected subsets of the same task. Consistent color patterns appear within tasks of the same type, while distinctly different tasks exhibit unique activation signatures, supporting our hypothesis that tasks selectively activate specific dimensions.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2503.07605/x3.png",
                "caption": "Figure 4:Illustration of how neurons are pruned based on importance scores.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2503.07605/extracted/6267883/figures/mmlu_rm_test.png",
                "caption": "Figure 5:Impact of pruning on MMLU performance at different layers and sparsity levels. Early layers are more sensitive to pruning.",
                "position": 522
            }
        ]
    },
    {
        "header": "3Experiment and Results Analysis",
        "images": []
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07605/extracted/6267883/figures/piqa_rm_test.png",
                "caption": "Figure 6:Impact of pruning on PIQA performance at different layers and sparsity levels. Deeper layers are more robust to pruning.",
                "position": 2060
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07605/extracted/6267883/figures/ppl.png",
                "caption": "Figure 7:Perplexity (PPL) results under different pruning ratios. A lower‚Üì perplexity indicates better performance.",
                "position": 2080
            }
        ]
    },
    {
        "header": "Appendix CGeneration Examples",
        "images": []
    }
]