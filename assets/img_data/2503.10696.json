[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10696/x1.png",
                "caption": "",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10696/x2.png",
                "caption": "Figure 2:Generation quality and efficiency comparisons between various visual generation methods. Data is collected from ImageNet256√ó256256256256\\times 256256 √ó 256dataset over models with parameters around 300M.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x3.png",
                "caption": "Figure 3:Comparisons of different autoregressive visual generation paradigm.The proposed NAR paradigm formulates the generation process as an outpainting procedure, progressively expanding the boundary of the decoded token region. This approach effectively preserves locality, as all tokens near the starting point are consistently decoded before the current token.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10696/x4.png",
                "caption": "Figure 4:Illustration of the dimension-oriented decoding heads.The horizontal head and the vertical head are responsible for predicting the next token in the row and column dimensions, respectively. Here,LùêøLitalic_Lis the number of Transformer blocks in the backbone network.",
                "position": 188
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10696/x5.png",
                "caption": "Figure 5:Proximity-aware attention masks for the NAR paradigm.‚ÄúSnùëõnitalic_n‚Äù denotes thenùëõnitalic_n-th generation step. Tokens generated within the same step are represented by the same color. To maintain the autoregressive property, a causal mask is applied between tokens across different generation steps (aligned with Figure3). Within each step, bidirectional attention is employed among the tokens to enhance consistency during parallel generation.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10696/x6.png",
                "caption": "Figure 6:Efficiency comparisons between vanilla AR, VAR and the proposed NAR paradigm for visual generation. With a batch size larger than 64, NAR achieves a lower FID with lower latency, lower memory usage and significantly higher throughput.",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x6.png",
                "caption": "",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x7.png",
                "caption": "",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x8.png",
                "caption": "",
                "position": 776
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10696/x9.png",
                "caption": "Figure A:Video generation sampleson UCF-101 dataset. Each row shows sampled frames from a 16-frame,128√ó128128128128\\times 128128 √ó 128resolution sequence generated by NAR-XL across various action categories.",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x10.png",
                "caption": "Figure B:Class-conditional image generation samplesproduced by NAR-XXL on ImageNet256√ó256256256256\\times 256256 √ó 256.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x11.png",
                "caption": "Figure C:Class-conditional image generation samplesproduced by NAR-XXL on ImageNet256√ó256256256256\\times 256256 √ó 256.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x12.png",
                "caption": "Figure D:256√ó256256256256\\times 256256 √ó 256text-guided image generation samplesproduced by LlamaGen-XL-Stage1 with next-token prediction paradigm and NAR-XL-Stage1 with next-neighbor prediction paradigm.",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2503.10696/x13.png",
                "caption": "Figure E:512√ó512512512512\\times 512512 √ó 512text-guided image generation samplesproduced by LlamaGen-XL-Stage2 with next-token prediction paradigm and NAR-XL-Stage2 with next-neighbor prediction paradigm. The text prompts are sampled from Parti prompts.",
                "position": 1709
            }
        ]
    },
    {
        "header": "Appendix AMore Visualizations",
        "images": []
    }
]