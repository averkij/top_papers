[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01583/x1.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01583/x2.png",
                "caption": "Figure 2:Overview of TEMPURA’s two-stage training pipeline.(a) Masked Event Prediction Reasoning:The model infers missing events by analyzing the masked video context, generating both a textual description and step-by-step causal explanations.(b) Video Event Segmentation and Temporal Dense Captioning:The model partitions an untrimmed video into non-overlapping events, each aligned with precise start/end timestamps and enriched with detailed captions, thereby reinforcing a structured understanding of temporal progressions.",
                "position": 305
            }
        ]
    },
    {
        "header": "4VER Data Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01583/x3.png",
                "caption": "Figure 3:Structured Training Data for Masked Event Prediction and Dense Event Caption",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2505.01583/x4.png",
                "caption": "Figure 4:VER Data Pipeline: The pipeline begins by filtering and categorizing a large video pool. GPT-4o then generates event captions with start/end times, followed by a temporal coherence check that discards invalid events. For valid events, a subset is masked to form a fill-in-the-blank task, and GPT-4o infers the missing segments—ultimately creating a dataset for video temporal understanding.",
                "position": 405
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01583/x5.png",
                "caption": "Figure 5:Our model can segment videos into more fine-grained events, capturing subtle transitions and short-duration activities. In contrast, the baseline model (QwenVL2.5) tends to generate coarser segments. This difference suggests that our approach is more adept at recognizing and differentiating fine-grained patterns within video sequences, leading to detailed and structured event representation.",
                "position": 736
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7VER Data Creation Pipeline and Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01583/x6.png",
                "caption": "Figure 6:Our mask event prediction data example and generation process.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2505.01583/x7.png",
                "caption": "Figure 7:Video Class Distribution",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2505.01583/x8.png",
                "caption": "Figure 8:Event Caption Distribution",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2505.01583/x9.png",
                "caption": "Figure 9:Video Frame Temporal Relevance Distribution",
                "position": 1702
            }
        ]
    },
    {
        "header": "8Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.01583/x10.png",
                "caption": "Figure 10:Comparison oflong video temporal grounding responseson a cooking tutorial video, generated by baseline models: Qwen2.5-VL-3B and Grounded Video LLM-Meta-Llama-3-8B-Instruct. Each of the baseline models are prompted to segment videos and describe each of the video segments in detail with correct start and end times. The text highlighted in red indicates incorrect determination of start and end times for frame descriptions.",
                "position": 1788
            },
            {
                "img": "https://arxiv.org/html/2505.01583/x11.png",
                "caption": "Figure 11:Comparison oflong video temporal grounding responseson a cooking tutorial video, generated by TEMPURA (our model) and baseline models: Grounded-Video-LLM-Phi and VideoQA. Each of the models are prompted to segment videos and describe each of the video segments in detail with correct start and end times. The text highlighted in red indicates incorrect determination of start/end times and video segment descriptions. The text in green indicates correct determination of start/end times and and video segment descriptions.",
                "position": 1791
            },
            {
                "img": "https://arxiv.org/html/2505.01583/x12.png",
                "caption": "Figure 12:Comparing TEMPURA (our model) and other baseline model (Qwen2-5-VL-3B, Grounded-Video-LLM-Meta-Llama-3-8B-Instruct, Grounded-Video-LLM-Phi, and VideoQA) abilities to generate detailed descriptions onfine-grained events on short videos. Each model is prompted to segment the video into fine-grained events and describe the events in detail with correct start/end timestamps. Red text indicates incorrect responses with incorrect start/end timestamps and/or poor descriptions of the event segment.",
                "position": 1794
            }
        ]
    },
    {
        "header": "9Qualitative Analysis",
        "images": []
    }
]