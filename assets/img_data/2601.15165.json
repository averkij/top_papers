[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15165/x1.png",
                "caption": "Figure 1:Less flexibility unlocks better reasoning potential.Left: We observe a counter-intuitive phenomenon where restricting dLLMs to standard Autoregressive (AR) order expands the reasoning solution space.Right: Motivated by this, we propose “JustGRPO”. By foregoing complex arbitrary-order adaptations and adopting standard GRPO, we effectively elicit the reasoning capability of dLLMs.",
                "position": 169
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15165/x2.png",
                "caption": "Figure 2:Confrontingvs.bypassing uncertainty.(a)AR orderpreserves reasoning space by forcing decisions at uncertain tokens.\n(b)Arbitrary orderbypasses uncertainty and resolves easier tokens first.\nOnce future context is established, the original forks collapse, prematurely narrowing the solution space.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15165/x3.png",
                "caption": "Figure 3:Reasoning potential measured by Pass@kk.While arbitrary order is competitive in single-shot settings (k=1k=1), it exhibits notably flatter scaling curves compared to AR Order.",
                "position": 303
            }
        ]
    },
    {
        "header": "3The Flexibility Trap",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15165/x4.png",
                "caption": "Figure 4:Solution space coveragemeasured by Pass@10241024. The reasoning traces generated by arbitrary order are largely a strict subset of those generated by AR Order.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2601.15165/x5.png",
                "caption": "Figure 5:Frequently bypassed tokensin arbitrary order, measured on MATH-500, are typically logical connectors and transition words.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2601.15165/x6.png",
                "caption": "Figure 6:Entropy degradation.While the global average entropy of Arbitrary Order remains comparable to AR (dashed lines), the entropy at logical forks drops significantly (blue bars).",
                "position": 364
            }
        ]
    },
    {
        "header": "4“Just GRPO” for Diffusion Language Models",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15165/x7.png",
                "caption": "Figure 7:JustGRPO preserves the parallel decoding capability of dLLMs.Interestingly, when compared to original instruct model, accuracy gains are larger with more parallel tokens, likely due to more robust reasoning capabilities after JustGRPO training.\nWe adopt training-free EB-samplerBen-Hamu et al. (2025)for parallel decoding.",
                "position": 707
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.15165/x8.png",
                "caption": "Figure 8:Pass@K comparison with different temperatures.",
                "position": 1536
            },
            {
                "img": "https://arxiv.org/html/2601.15165/x9.png",
                "caption": "Figure 9:(a) Sampling algorithm comparison.(b) Correlation between different sampling algorithms and AR in per-problem accuracy.\nWe compare different sampling algorithms in pass@k performance and correlation with AR in per-problem accuracy.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2601.15165/x10.png",
                "caption": "Figure 10:Pass@kkcomparison with different semi-autoregressive block sizesof arbitrary order (AO) generation.\nSmaller block sizes explicitly restrict the model’s order flexibility, leading to a more AR-like behavior and better pass@k performance.",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2601.15165/x11.png",
                "caption": "Figure 11:Entropy comparison results on more forking tokens.",
                "position": 1588
            },
            {
                "img": "https://arxiv.org/html/2601.15165/x12.png",
                "caption": "Figure 12:Training efficiency on GSM8K (Wall-clock Timevs.Accuracy). The approximation-based baseline (ESPO) suffers from early saturation. In contrast, JustGRPO (Ours) exhibits better accuracy-wall time trade-off despite the theoretical overhead of exact likelihood estimation. Moreover,Ours-Fast(gradient update on top-25% entropy tokens) illustrates the potential for further acceleration via simple engineering optimizations.\nWall-clock time is measured on 16×\\timesH100 GPUs.",
                "position": 1606
            }
        ]
    },
    {
        "header": "Appendix BMore Analysis Results",
        "images": []
    }
]