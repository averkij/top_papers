[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x1.png",
                "caption": "Figure 1:Comparison of static and interactive ways of evaluating Large Audio Models. In this work, we perform interactive evaluations to understand how LAMs are likely to be used and how they can be benchmarked.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Interactive Evaluation",
        "images": []
    },
    {
        "header": "4What Tasks Do Users Expect LAMs to Perform?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x2.png",
                "caption": "Figure 2:We identify four main topics in user queries — task execution, knowledge expansion, chat, advice seeking as well as sub-topics under each category through hierarchical clustering (left) and analyze the relative proportions of each query type (right).",
                "position": 310
            }
        ]
    },
    {
        "header": "5What Models are Best at these Tasks and Why?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x3.png",
                "caption": "Figure 3:Head-to-head model comparisons (left) and Bradley-Terry (right) Scores from our evaluation. For win rates, * indicates the difference between preferences is significant (P<0.05) by a pairwise bootstrap test. For Bradley-Terry scores, distributions are shown shown for 10,000 bootstraps.††\\dagger†denotes an ASR + LLM pipeline.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2502.15919/x4.png",
                "caption": "",
                "position": 328
            }
        ]
    },
    {
        "header": "6Which Benchmarks are the Best Proxies for User Preferences?",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x5.png",
                "caption": "Figure 4:Mixed-effect regression of benchmark performance differences as a predictor of user preferences across models. 15 other features were pre-screened using VIF thresholding (threshold=10.0) with ties removed. Model fitting was performed fixed effects for benchmarks and random effects for model identity. The model achieved conditional/marginalR2superscript𝑅2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTof 0.99/0.30.",
                "position": 681
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt",
        "images": []
    },
    {
        "header": "Appendix BCorrelation among Static Benchmarks",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x6.png",
                "caption": "Figure A.1:PCA Analysis of model performance on 20 static benchmarks.",
                "position": 1919
            }
        ]
    },
    {
        "header": "Appendix CInteractive Evaluation Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x7.png",
                "caption": "Figure A.2:Gradio interface of interactive evaluation. Users record their own speech and audio without constraints and receive responses from two LAM systems anonymously. They then provide a binary preference between the models, and are provided the option to provide qualitative feedback through either voice or text.",
                "position": 1929
            }
        ]
    },
    {
        "header": "Appendix DModel Ranking in Static and Interactive Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.15919/x8.png",
                "caption": "Figure A.3:Kendall tau rank distance between model rank in interactive evaluation and that on different static benchmarks.",
                "position": 1942
            },
            {
                "img": "https://arxiv.org/html/2502.15919/x9.png",
                "caption": "Figure A.4:Forest plot demonstrating the effect sizes of static benchmarks in a logistic regression without mixed-effects for predicting individual user preferences. The results are overall consistent with our mixed effects regression with the only shift being theβ𝛽\\betaitalic_βfor ASR tasks.",
                "position": 1952
            }
        ]
    },
    {
        "header": "Appendix ELogistic Regression",
        "images": []
    }
]