[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11997/x1.png",
                "caption": "Figure 1:Illustration of thereceptive fieldsinduced by pLSTM and related architectures (for a single layer). CNNs are locally restricted while ViTs have a global receptive field. Modern recurrent architectures, such as ViM, traverse the 2D grid sequentially. pLSTM effectively extends the receptive field via its combination of D-mode and P-mode.",
                "position": 185
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11997/x2.png",
                "caption": "Figure 2:Illustration of theArrow pointing task. The model has to classify whether an arrow is pointing towards a circle (top left).\nModels with global receptive fields, such as Vision Transformers (ViTs), can solve this task by leveraging directional information encoded via positional embeddings (top right), but they often struggle to generalize to higher resolutions. In contrast, pLSTMs can effectively solve this task in both diffusive distribution (D-mode, bottom left) and directed propagation (P-mode, bottom right) modes, enabling long-range reasoning and better scalability.",
                "position": 245
            }
        ]
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11997/x3.png",
                "caption": "Figure 3:Transition from input/forget/output gating in between nodes (and input/output) in (linear) RNNs towardsSource/Transition/Mark gatingbetween edges and input/output (bottom/top) in pLSTMs.",
                "position": 315
            }
        ]
    },
    {
        "header": "4pLSTM",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11997/x4.png",
                "caption": "Figure 4:Illustration ofpLSTMon general DAGs (left), and on 1D / 2D grids (right): In the top-left part, a general DAG is visualized, with SourcesSe‚Ä≤nsubscriptsuperscriptùëÜùëõsuperscriptùëí‚Ä≤S^{n}_{e^{\\prime}}italic_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, TransitionsTe‚Ä≤‚Å¢esubscriptùëásuperscriptùëí‚Ä≤ùëíT_{e^{\\prime}e}italic_T start_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT italic_e end_POSTSUBSCRIPT, and MarksMensubscriptsuperscriptùëÄùëõùëíM^{n}_{e}italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPTpopulating node-outgoing-edge, incoming-edge-outgoing-edge, and incoming-edge-node pairs. Due to the associative structure of these linear operators, they can be combined; examples are shown in the center left. The end-to-end (node-to-node) result is the Gating matrix. In the bottom left, the two stable modes are depicted: The P-mode, where the sum over absolute Transitions of an in-coming edge have to be limited by one, and the D-mode, where the line graph over edges is reduced to a multitree, requiring the remaining Transitions each to be limited in absolute to one. On the right, the application of this to regular 1D and 2D grids is shown. For 2D, the P-mode results in a directional propagation, whereas the D-mode results in an un-directed distribution (shown with decay here).",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2506.11997/x5.png",
                "caption": "Figure 5:Training curves for theArrow Pointing Extrapolation task, averaged over 5 seeds with 90% CI. ViT and EfficientNet can quickly match the training set (left), and EfficientNet reaches the best validation performance of all models on the samples of the same resolution as seen during training. pLSTM performs best on the extrapolation set (right) by a significant gap. While Mamba2D and 2DMamba should cover restricted modes of pLSTM, their learning shows high variance. ViL and Mamba2D do not extrapolate at all beyond random performance.",
                "position": 551
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.11997/x6.png",
                "caption": "Figure 6:Training curves for theArrow Pointing Extrapolation task, averaged over 5 seeds with 90% CI on different model ablations. P-mode by itself performs worse, D-mode by itself is not as general in interpolation, but performs better on extrapolation compared to other models. pLSTM does not rely on the positional embedding for strong performance.",
                "position": 583
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMethod Details",
        "images": []
    },
    {
        "header": "Appendix BFull Model Architecture",
        "images": []
    },
    {
        "header": "Appendix CTheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix DExperimental Results",
        "images": []
    }
]