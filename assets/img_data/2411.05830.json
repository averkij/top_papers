[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05830/x1.png",
                "caption": "Figure 1:Left:Modern LLMs often struggle with generating version-accurate code, highlighting the need for benchmarks that specifically assess their ability to handle versioning.Right:Cumulative year-over-year (YoY) version releases of popular Python-based machine learning libraries show a consistent upward trend, reflecting the rapid pace of development and version updates of code libraries and packages.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x2.png",
                "caption": "",
                "position": 111
            }
        ]
    },
    {
        "header": "2GitChameleonBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05830/x3.png",
                "caption": "(a)Number of unique versions per library inGitChameleon.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x3.png",
                "caption": "(a)Number of unique versions per library inGitChameleon.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x4.png",
                "caption": "(b)Number of samples by version release year.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x5.png",
                "caption": "(c)Number of samples by type of change.",
                "position": 321
            }
        ]
    },
    {
        "header": "3Empirical Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05830/x6.png",
                "caption": "Figure 3:Correlation ofGitChameleonwith the representative code benchmarks (HumanEval, EvalPlus, and BigCodeBench-Hard split). For each benchmark, the spearman correlation coefficient was 0.37, 0.50, and 0.36, respectively.",
                "position": 1187
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x7.png",
                "caption": "Figure 4:Instruct-tuned model performance breakdown by version release year (top) and type of change (bottom): We analyze model performance in terms ofpass @ 10for baseline and with error feedback generation across two dimensions: version release year and type of changes.\nDarker shaded bars represent values obtained via error feedback generation. Standard deviation is drawn as a black line, obtained from 5 random seeds. (Top) Many models perform poorly on 2022, and generally perform worse on more recent versions. (Bottom) All models perform very poorly at semantic changes, indicating an potential area for massive improvement. Most models perform well on function name changes and new feature (with the exception of Llama-3.2-3B, which is also the smallest model in this comparison).",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x8.png",
                "caption": "",
                "position": 1210
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x9.png",
                "caption": "Figure 5:Comparison of sample and model differences.The left panel shows the distribution of sample difficulty, measured by the frequency of pass@10 scores across all models and seeds. The right panel presents the same distribution, but averaged for specific models across their seeds. Interestingly, individual models tend to exhibit bimodal distributions, indicating they are either consistently good or bad at specific problems. However, the aggregate distribution is less bimodal, suggesting that different models excel at different problems.",
                "position": 1236
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x10.png",
                "caption": "",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x11.png",
                "caption": "Figure 6:Doc-prompting vs no doc (greedy decoding) instruct-tuned model performance on a subset of libraries.Doc-prompting refers to the prompting technique in which the prompt contains the documentation of the library function specific to the problem at hand. Darker shaded bars represent doc-prompting values, lighter shaded bars represent no doc values.",
                "position": 1254
            },
            {
                "img": "https://arxiv.org/html/2411.05830/x12.png",
                "caption": "Figure 7:Most frequent categories of errors obtained from executing unit tests on model generated code, averaged across models.The most common error by far isNameerrors, which represents a variable that was used but not defined. This suggests that models do not grasp the context in many cases. Furthermore,Name,Indentation,Attribute,Import,AssertionError, andModuleNotFounderrors were all reduced with error feedback, whileTimeOutandSyntaxerrors increased.",
                "position": 1264
            }
        ]
    },
    {
        "header": "4Related work",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]