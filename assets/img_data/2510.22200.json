[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22200/x1.png",
                "caption": "Figure 1:Examples onText-to-Video,Image-to-VideoandVideo-Continuationtasks.Video-Continuationsupports long video generation as well as interactive generation with multiple instructions. We unify these tasks with a single model.",
                "position": 204
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22200/images/DataCurationPipeline.png",
                "caption": "Figure 2:Overview of data curation pipeline. The data preprocessing stage extracts well-segmented video clips from raw source videos in the data pool. In the data annotation stage, each video clip is annotated with a variety of attributes, forming a comprehensive metadata database. This metadata database enables the convenient and flexible assembly of training datasets to support various training stages and objectives.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x2.png",
                "caption": "Figure 3:Overview of the video captioning workflow. The main content of each video is captured by a basic captioning model, and complemented by additional models that extract attributes such as cinematography and visual style. These elements are integrated to produce varied and informative captions, enhancing the quality and diversity of training data.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/distribution2.png",
                "caption": "Figure 4:We apply text embedding to video captions and perform clustering analysis. An LLM summarizes each cluster and assigns tags, enabling unsupervised categorization of the dataset.",
                "position": 388
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22200/images/omni_architecture.png",
                "caption": "Figure 5:Left: Unified transformer for multiple generation tasks. Our model simultaneously supportsText-to-Video,Image-to-Video(with a single conditioning frame), andVideo-Continuation(with multiple conditioning frames) tasks. The timestep configuration is consistent with the input, and the condition part are fixed to zero. Right: Block Causal Attention. In self-attention, the updates of the condition tokens are independent of the noisy tokens. In cross-attention, condition tokens do not participate in cross-attention computation.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x3.png",
                "caption": "Figure 6:Our GRPO method significantly improves the video generation quality.",
                "position": 494
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/grpo-ab-reweight.jpg",
                "caption": "(a)",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/grpo-ab-reweight.jpg",
                "caption": "(a)",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/grpo-ab-maxstd.png",
                "caption": "(b)",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/grpo-curves-5b6-font-l-trunc.jpg",
                "caption": "Figure 8:GRPO reward curves from the multi-reward training of LongCat-Video.",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x4.png",
                "caption": "Figure 9:Reward hacking with single reward. Our multi-reward training approach prevents reward hacking for any single reward by establishing a balance among multiple rewards. For instance, the motion reward counteracts the static tendency induced by HPSv3 hacking while still leveraging HPSv3 to enhance visual quality.",
                "position": 797
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/SR.png",
                "caption": "Figure 10:Comparison of native480​p480p, native720​p720p, andcoarse-to-fine720​p720pgeneration. The coarse-to-fine strategy produces texture details and quality that surpass those of the native720​p720pgeneration and can also correct local distortions.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2510.22200/images/2stage.png",
                "caption": "Figure 11:The coarse-to-fine generation processes forText-to-Video,Image-to-Video, andVideo-Continuationtasks. The green arrows indicate the low-resolution generation phase, while the orange arrows represent the refinement phase. Compared toText-to-Video,Image-to-VideoandVideo-Continuationinclude additional configuration for the condition.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x5.png",
                "caption": "Figure 12:Illustration of 3D block sparse attention for queryqiq_{i}and keys{kj}j=1T​H​W\\{k_{j}\\}_{j=1}^{THW}.(a)Partitionqiq_{i}and allkjk_{j}into non-overlapping 3D blocks of sizet×h×wt\\times h\\times w. The block containingqiq_{i}is identified, and a similarity score is computed between this query block and each key block using their average values.(b)Select the top-rrkey blocks with the highest similarity scores.(c)Compute the standard attention betweenqiq_{i}and all keys within the selectedrrkey blocks.",
                "position": 1083
            }
        ]
    },
    {
        "header": "4Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22200/x6.png",
                "caption": "Figure 13:Overview of training process.",
                "position": 1133
            }
        ]
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22200/x7.png",
                "caption": "Figure 14:Text-to-Video MOS evaluation results on our internal benchmark.",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x8.png",
                "caption": "Figure 15:Text-to-Video GSB evaluation results on our internal benchmark.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x9.png",
                "caption": "Figure 16:Image-to-Video MOS evaluation results on our internal benchmark.",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x10.png",
                "caption": "Figure 17:Results onText-to-Videogeneration.",
                "position": 1663
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x11.png",
                "caption": "Figure 18:Results onImage-to-Video. As shown in the top row, given the same initial image, LongCat-Video accurately responds to instructions for various actions.",
                "position": 1671
            },
            {
                "img": "https://arxiv.org/html/2510.22200/x12.png",
                "caption": "Figure 19:Results onVideo-Continuation. LongCat-Video supports minutes-long video generation without quality degradation, as well as interactive video generation with changing instructions for each clip.",
                "position": 1679
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "7Contributors and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22200/x13.png",
                "caption": "Figure 20:MQ Reward model validation loss curve",
                "position": 3251
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]