[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01126/x1.png",
                "caption": "",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01126/x2.png",
                "caption": "Figure 2:Overview of a denoising step in UniEgoMotion. The input noisy motionùëø1:Nt\\boldsymbol{X}^{t}_{1:N}bold_italic_X start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_N end_POSTSUBSCRIPTis denoised using a transformer decoder network conditioned on the ego-device trajectoryùëª1:N\\boldsymbol{T}_{1:N}bold_italic_T start_POSTSUBSCRIPT 1 : italic_N end_POSTSUBSCRIPTand egocentric imagesùë∞1:N\\boldsymbol{I}_{1:N}bold_italic_I start_POSTSUBSCRIPT 1 : italic_N end_POSTSUBSCRIPT. A robust image encoder is used to extract fine-grained scene context from the images. During training, conditioning inputs are randomly replaced with learnable mask tokens to simulate three tasks: egocentric reconstruction, forecasting, and generation. During inference, the learned mask tokens are used in place of any missing conditioning input, allowing a single model to perform all three tasks consistently.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2508.01126/x3.png",
                "caption": "Figure 3:Qualitative comparison of Egocentric Reconstruction. The input egocentric images are shown on the left, with the corresponding ego-device trajectory visualized alongside the predictions. Baseline methods exhibit floating motion, floor penetration, and inaccurate joint localization, whereas UniEgoMotion generates reconstructions that closely align with the ground truth.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2508.01126/x4.png",
                "caption": "Figure 4:Qualitative comparison of Egocentric Forecasting for predicting future motion using the first 2 seconds of egocentric video and trajectory input. The LSTM baseline predicts an average future motion and suffers from foot sliding, while the Two-stage baseline produces damped motion. In contrast, our model successfully predicts complex motions, such as squatting down to repair a bike tire (top), performing a salsa dance (middle), and executing a dribbling drill around a dome cone (bottom).",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2508.01126/x5.png",
                "caption": "Figure 5:Qualitative comparison of Egocentric Motion Generation from a single egocentric image input.\nCompared to the LSTM and Two-stage baseline, our model leverages the fine-grained image features for more accurate motion generation, demonstrating soccer juggling (top), a basketball shooting drill (middle), and interaction with the lower cabinet on theleftside of the person.",
                "position": 834
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion & Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01126/x6.png",
                "caption": "Figure 6:Qualitative comparison of Egocentric Reconstruction, with absolute vertex errors color-coded. The input egocentric images are shown on the left, with the corresponding ego-device trajectory visualized alongside the predictions.",
                "position": 2257
            }
        ]
    },
    {
        "header": "Appendix AQualitative Comparison",
        "images": []
    },
    {
        "header": "Appendix BBaselines",
        "images": []
    },
    {
        "header": "Appendix CAblation on Conditioning Inputs",
        "images": []
    },
    {
        "header": "Appendix DWhy Not Text Conditioning",
        "images": []
    },
    {
        "header": "Appendix ETraining Details",
        "images": []
    },
    {
        "header": "Appendix FMotion Representation",
        "images": []
    },
    {
        "header": "Appendix GEE4D-Motion Dataset",
        "images": []
    }
]