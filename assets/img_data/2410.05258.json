[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05258/x1.png",
                "caption": "Figure 1:Transformer often over-attends to irrelevant context (i.e., attention noise).DiffTransformer amplifies attention to answer spans and cancels noise, enhancing the capability of context modeling.",
                "position": 90
            }
        ]
    },
    {
        "header": "2Differential Transformer",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05258/x2.png",
                "caption": "Figure 2:Multi-head differential attention.\nEach head takes the difference between twosoftmaxsoftmax\\mathrm{softmax}roman_softmaxattention maps to cancel out attention noise.位\\lambdaitalic_位is a learnable scalar that is initialized to位initsubscriptinit\\lambda_{\\text{init}}italic_位 start_POSTSUBSCRIPT init end_POSTSUBSCRIPT.GroupNormGroupNorm\\operatorname{GroupNorm}roman_GroupNormapplies normalization to each head independently.\nA fixed multiplier(1位init)1subscriptinit(1-\\lambda_{\\text{init}})( 1 - italic_位 start_POSTSUBSCRIPT init end_POSTSUBSCRIPT )is used afterGroupNormGroupNorm\\operatorname{GroupNorm}roman_GroupNorm, which aligns the gradient flow with Transformer.\nThe code implementation is available athttps://aka.ms/Diff-Transformer.",
                "position": 132
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05258/x3.png",
                "caption": "(a)Scaling model size ranging from 830M to 13B.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x3.png",
                "caption": "(a)Scaling model size ranging from 830M to 13B.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x4.png",
                "caption": "(b)Scaling number of training tokens for 3B models.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x5.png",
                "caption": "Figure 4:Cumulative average negative log-likelihood (lower is better) on book data.DiffTransformer leverages long context more effectively.",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x6.png",
                "caption": "(a)Transformer.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x6.png",
                "caption": "(a)Transformer.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x7.png",
                "caption": "(b)DiffTransformer.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x8.png",
                "caption": "(a)TREC with 6 classes.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x8.png",
                "caption": "(a)TREC with 6 classes.",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x9.png",
                "caption": "(b)TREC-fine with 50 classes.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x10.png",
                "caption": "(c)Banking-77 with 77 classes.",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x11.png",
                "caption": "(d)Clinic-150 with 150 classes.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x12.png",
                "caption": "(a)Examples are randomly arranged.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x12.png",
                "caption": "(a)Examples are randomly arranged.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x13.png",
                "caption": "(b)Examples are arranged alternately by class.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x14.png",
                "caption": "Figure 8:Zero-shot accuracy on the HellaSwag[12]dataset. We quantize the attention logits from 16 bits (i.e., unquantized) to 8 bits, 6 bits, and 4 bits.",
                "position": 895
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation of Differential Attention",
        "images": []
    },
    {
        "header": "Appendix BLanguage Modeling Evaluation",
        "images": []
    },
    {
        "header": "Appendix CHyperparameters forSection3.1",
        "images": []
    },
    {
        "header": "Appendix DHyperparameters forSection3.2",
        "images": []
    },
    {
        "header": "Appendix ERobustness of In-Context Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.05258/x15.png",
                "caption": "(a)TREC with 6 classes.",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x15.png",
                "caption": "(a)TREC with 6 classes.",
                "position": 2125
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x16.png",
                "caption": "(b)TREC-fine with 50 classes.",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x17.png",
                "caption": "(c)Banking-77 with 77 classes.",
                "position": 2136
            },
            {
                "img": "https://arxiv.org/html/2410.05258/x18.png",
                "caption": "(d)Clinic-150 with 150 classes.",
                "position": 2141
            }
        ]
    },
    {
        "header": "Appendix FGradient Flow ofDiffTransformer",
        "images": []
    }
]