[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11134/x1.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x2.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2511.11134/assets/huggingface_logo.png",
                "caption": "",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x3.png",
                "caption": "Figure 1:The paradigm shift to generative reasoning. Conventional benchmarks evaluate (a) Understanding or (b) Generation in isolation. GGBench introduces (c) integrated Understanding& Generation evaluation, requiring generative reasoning from Unified Multimodal Models.",
                "position": 212
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11134/x4.png",
                "caption": "Figure 2:GGBench’s step-by-step evaluation. Beyond traditional text-image pairs, GGBench provides executable code for each construction step, allowing for precise and automated verification.",
                "position": 225
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The GGBench Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11134/x5.png",
                "caption": "Figure 3:Overview of the GGBench data construction pipeline.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x6.png",
                "caption": "Figure 4:Difficulty distribution and category composition in GGBench. The inner ring shows the proportion of difficulty levels (Easy/Medium/Hard), while the outer ring presents category shares within each difficulty band, reflecting progressive complexity across reasoning types.",
                "position": 466
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11134/x7.png",
                "caption": "Table 4:Main results on GGBench. Higher is better (↑) except for LPIPS (↓).",
                "position": 714
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x7.png",
                "caption": "Figure 5:VLM-I scores across eight construction categories in GGBench. Each cell reflects the average multimodal reasoning quality for a model-category pair.",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x8.png",
                "caption": "Figure 6:VLM-I scores across geometric task types: Analytic Construction (AC), Geometric Transformation Construction (GTC), and Straightedge-and-Compass Construction (SCC). Higher values indicate better performance.",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x9.png",
                "caption": "Figure 7:VLM-I performance across difficulty levels on GGBench. Bars representEasy,Medium,Hard, and overall scores. VLM-I captures both intermediate reasoning quality and final visual correctness.",
                "position": 1138
            }
        ]
    },
    {
        "header": "5Error Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11134/x10.png",
                "caption": "Figure 8:The common error analysis.",
                "position": 1147
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Data Curation Prompts",
        "images": []
    },
    {
        "header": "8Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.11134/x11.png",
                "caption": "Figure 19:Correlation between VLM-I and human evaluation.",
                "position": 1796
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x12.png",
                "caption": "Figure 20:Example of an easy-level task.",
                "position": 1879
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x13.png",
                "caption": "Figure 21:Example of a medium-level task.",
                "position": 1882
            },
            {
                "img": "https://arxiv.org/html/2511.11134/x14.png",
                "caption": "Figure 22:Example of a hard-level task.",
                "position": 1885
            }
        ]
    },
    {
        "header": "9Case Study",
        "images": []
    }
]