[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15268/x1.png",
                "caption": "Figure 1:The LLM-judge pipeline introduces new potential confounds in evaluation, compared to standard benchmarks.We diagram the LLM-judge pipeline for alignment benchmarking and observe that it is more complex than that of most standard benchmarks; (a) it replaces an explainable, deterministic metric with an opaque LLM-judge. (b) it does not attempt to establish any verifiable ground truth. (c) it contains a relatively small number of questions covering an very wide range of topics, resulting in limited coverage of any particular knowledge domain. (d) it introduces novel confounds in the form of the judging template (explicit bias) and the judgeâ€™s unstated internal preferences (implicit bias).",
                "position": 173
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3LLM-Judge Preference Benchmarks",
        "images": []
    },
    {
        "header": "4Implicit bias in LLM-Judges",
        "images": []
    },
    {
        "header": "5SOS-Bench",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15268/x2.png",
                "caption": "Figure 2:More is more in alignment.In the SFT stage of post-training, the size of the dataset, rather than the method used to curate the data, is the strongest predictor of alignment. We report average normalized accuracy on the y axis, and dataset size (in 1000s) on the X axis. The shaded region represents 95% confidence intervals.",
                "position": 522
            }
        ]
    },
    {
        "header": "7Recommendations",
        "images": []
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Impact / Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Training Details",
        "images": []
    },
    {
        "header": "Appendix BCompute and Resources",
        "images": []
    },
    {
        "header": "Appendix CMethod Comparison",
        "images": []
    },
    {
        "header": "Appendix DSOS-BenchBenchmark Datasets",
        "images": []
    },
    {
        "header": "Appendix EAdditional two-stage post-training results",
        "images": []
    },
    {
        "header": "Appendix FTemplates",
        "images": []
    },
    {
        "header": "Appendix GAltered Model Responses",
        "images": []
    },
    {
        "header": "Appendix HGenerative Prompts",
        "images": []
    },
    {
        "header": "Appendix IAblation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15268/x3.png",
                "caption": "Figure 3:More is more in alignment.ipso facto",
                "position": 3282
            }
        ]
    },
    {
        "header": "Appendix JLLM-judges are robust to common authority bias hacks",
        "images": []
    }
]