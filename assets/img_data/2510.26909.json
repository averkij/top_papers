[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IINTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26909/simulation.png",
                "caption": "TABLE I:An overview of vision language navigation and VLM datasets.",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2510.26909/automatic.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2510.26909/manual.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2510.26909/human.png",
                "caption": "",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2510.26909/anymal.png",
                "caption": "",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2510.26909/wheeled_robot.png",
                "caption": "",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2510.26909/bicycle.png",
                "caption": "",
                "position": 441
            }
        ]
    },
    {
        "header": "IIRELATED WORK",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26909/x1.png",
                "caption": "Figure 2:Left:Geographic distribution of image sources, with the inner circle denoting countries and the outer circle specifying cities or regions.\nImages originating from the GrandTour Dataset[25]are explicitly marked in the outer circle.Right:Distribution of scenarios by setting (urban vs. rural), environment type (natural vs. structured), lighting, and weather.",
                "position": 527
            }
        ]
    },
    {
        "header": "IIINAVITRACE BENCHMARK",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26909/x2.png",
                "caption": "Figure 3:Left:Comparison between penalty cost masks based on Mask2Former and manual segmentation. These masks are used to punish traces crossing unsafe or irrelevant areas.Right:We show that the score function aligns with human preference by calculating the correlation between the score ranking and a pairwise ranking created by a human.",
                "position": 759
            }
        ]
    },
    {
        "header": "IVEXPERIMENTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26909/x3.png",
                "caption": "Figure 4:Left:Ranking of VLMs, the uninformed baseline Straight Forward, and human expert performance split into each embodiment. Note that a higher score is better.Right:Performance per task category for the same models.",
                "position": 906
            },
            {
                "img": "https://arxiv.org/html/2510.26909/x4.png",
                "caption": "Figure 5:Example predictions by the models Gemini 2.5 Pro, GPT-5, Qwen 3 VL, and o3.",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2510.26909/x5.png",
                "caption": "Figure 6:Example of o3’s reasoning with the prediction in pink on the left and the steps on the right. The model reasons correctly but is unable to predict a corresponding trace.",
                "position": 1046
            }
        ]
    },
    {
        "header": "VCONCLUSION",
        "images": []
    },
    {
        "header": "VILIMITATIONS",
        "images": []
    },
    {
        "header": "ACKNOWLEDGMENT",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]