[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x1.png",
                "caption": "Figure 1:Overview.We propose a pipeline that generates high-quality aerial images along with their labels. Our method outperforms baseline detectors trained on source images and open-set detectors directly inferred on target images.",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x2.png",
                "caption": "Figure 2:Overview of our pipeline.It consists of two stages. First, we finetune Stable Diffusion and synthesize both source and target domain images. Second, we automatically label synthetic target domain images via cross-attention maps.",
                "position": 225
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x3.png",
                "caption": "Figure 3:Image samples from our datasets.(left) LINZ sample, (right) UGRC, (green markers) small vehicle location annotations. For more examples, check the Supplementary Material.",
                "position": 354
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x4.png",
                "caption": "Figure 4:Cross-attention maps for different tokens.We analyze the effectiveness of utilizing multi-channel cross-attention maps by assessing the label quality synthetic UGRC images. (a) Synthetic UGRC images. (b) Labels generated using only the cross-attention map of the word “car”. (c) Labels generated by using multi-channel heatmaps. (d) Multi-channel cross-attention maps. (e) Cross-attention maps of the word “car”. (f) Cross-attention maps of token [V1\\text{V}_{1}V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT], which is designed to capture the concept of cars. (g) Cross-attention maps of token [V3\\text{V}_{3}V start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT] for background concept, which are further inverted for better comparison. In (b) and (c), bounding boxes with dotted lines denote the predicted pseudo-bounding box labels while dots denote predicted vehicle centers. In (e), (f), and (g), grayscale cross-attention maps are displayed as color heatmaps to highlight the intensity difference.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x5.png",
                "caption": "Figure 5:Quantitative comparison with varying thresholds.We report the AP50result.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x6.png",
                "caption": "Figure 6:Comparison of our method with different thresholds for label quality.Blue bounding boxes represent pseudo labels generated using fixed thresholds, while red bounding boxes correspond to labels obtained through refinement. The blue and red dots indicate the predicted car centers. (a) to (e) show the results for thresholds of 0.3, 0.4, 0.5, 0.6, and 0.7, respectively.",
                "position": 898
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AMethods",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x7.png",
                "caption": "Figure 7:Vehicles belonging to the object classsmall vehicle.",
                "position": 1983
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x8.png",
                "caption": "(a)Selwyn (New Zealand)",
                "position": 1987
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x8.png",
                "caption": "(a)Selwyn (New Zealand)",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x9.png",
                "caption": "(b)Utah (USA)",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x10.png",
                "caption": "Figure 9:Examples of images from our real-world datasets.(first row) LINZ images containing small vehicles;\n(second row) LINZ images without vehicles;\n(third row) UGRC images containing small vehicles;\n(fourth row) UGRC images without vehicles;",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x11.png",
                "caption": "Figure 10:Comparison between cross-dataset generalization and within-dataset performance.The purple bars represent the model trained on the LINZ dataset and evaluated on the UGRC dataset, while the pink bars correspond to both training and testing conducted on the UGRC dataset. We report theAP50\\text{AP}_{50}AP start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPTresult.",
                "position": 2017
            }
        ]
    },
    {
        "header": "BDatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x12.png",
                "caption": "(a)",
                "position": 2030
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x12.png",
                "caption": "(a)",
                "position": 2033
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x13.png",
                "caption": "(b)",
                "position": 2039
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x14.png",
                "caption": "(c)",
                "position": 2045
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x15.png",
                "caption": "(d)",
                "position": 2051
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x16.png",
                "caption": "Figure 12:Two popular VLLMs (BLIP2 and Kosmos2) tested as zero-shot image car presence classifier. The severe imbalance of the positive and negative classes causes the high levels of accuracy, which is deceptive. F1 score, Precision and Recall metrics clearly show that the classification quality is less than ideal.",
                "position": 2060
            }
        ]
    },
    {
        "header": "CLimitation of Foundation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20976/x17.png",
                "caption": "Figure 13:Failure cases of Open-set detectors. (a) Detection results of Grounding-DINO. (b) Detection results of Omdet-Turbo. (c) Detection results of OWLV2. (d) Detection results of Owlvit. The blue bounding boxes with dotted lines denote the predicted pseudo bounding box labels while the dots denote the predicted car centers.",
                "position": 2204
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x18.png",
                "caption": "Figure 14:Illustration of how we obtain the 42.36 px bounding box size. (a) The black bounding boxes denote the ground truth pseudo bounding box labels while the red bounding boxes denote the predicted pseudo bounding box labels. The dots denote the corresponding centers.Δ​x\\Delta xroman_Δ italic_xandΔ​y\\Delta yroman_Δ italic_ydenote the x-and y-coordinate difference between the ground truth center and the predicted center. (b) Isocontour of Intersection of Union (IoU). The yellow arc is14\\frac{1}{4}divide start_ARG 1 end_ARG start_ARG 4 end_ARGof the decision circle with a 12 px radius while the black curve represents the isocontour whereIoU=0.5\\text{IoU}=0.5IoU = 0.5. (c) An example of a decision circle with a radius of 12 px centered at the car’s center with the corresponding 42.36 px pseudo-bounding box.",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2507.20976/x19.png",
                "caption": "Figure 15:Failure cases of diffusion models. (a) Images generated by pre-trained Stable Diffusion V1.4. (b) Images generated by pre-trained Stable UnCLIP[43]with canny edge maps, semantic segmentation maps, and average CLIP image embedding difference between the LINZ and the UGRC dataset as conditions. From left to right: Real images from LINZ dataset, edge maps, semantic segmentation maps, and synthetic images. (c) Images generated by GLIGEN. Left: Real images from the LINZ dataset. Right: Synthetic UGRC images. The bounding boxes that have the same color in left and right images correspond to the same location.",
                "position": 2330
            }
        ]
    },
    {
        "header": "DMore Implementation Details",
        "images": []
    },
    {
        "header": "EMore Ablation Studies",
        "images": []
    }
]