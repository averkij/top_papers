[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26800/x1.png",
                "caption": "Figure 1:We presentOmniX, a versatile and unified framework that repurposes pre-trained 2D flow matching models for panoramic perception, generation, and completion. This framework enables the construction of immersive, photorealistic, and graphics-compatible 3D scenes, suitable for physically-based rendering (PBR), relighting, and physical dynamics simulation.",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26800/figures/3.panox.png",
                "caption": "Figure 2:A preview of the proposed PanoX dataset, providing high-quality panoramic rendered images with rich pixel-aligned annotations, including distance, world normal, albedo, roughness, and metallic. The dataset is collected from both indoor and outdoor scenes.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2510.26800/x2.png",
                "caption": "Figure 3:OmniX pipeline for panoramic generation and perception.Built on a pre-trained 2D flow matching model with flexible, modality-specific adapters, OmniX is capable of performing a wide range of panoramic vision tasks including generation, perception, and completion.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2510.26800/x3.png",
                "caption": "Figure 4:Different cross-modal adapter structuresfor multiple condition inputs{ùêúi|i=0,1,‚Ä¶}\\{\\mathbf{c}^{i}~|~i=0,1,...\\}and multiple target outputs{ùê≥^1j|j=0,1,‚Ä¶}\\{\\mathbf{\\hat{z}}_{1}^{j}~|~j=0,1,...\\}. Specifically, (a)Shared-Branchconcatenates different inputs along the channel dimension; (b)Shared-Adapteris equivalent to token-wise concatenation; (c)Separate-Adapterlearns specific adapter weights for each type of input.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2510.26800/x4.png",
                "caption": "Figure 5:Occlusion-aware mask sampling.Based on the panoramic distance map and a randomly sampled 3D displacement, we can estimate the occluded regions by ray intersection. These regions are used as masks for training panoramic completion and guided panoramic perception models.",
                "position": 367
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26800/x5.png",
                "caption": "Figure 6:Qualitative evaluation of OmniX on panoramic intrinsic decompositioncompared to state-of-the-art methods: IID(Kocsis et¬†al.,2024), and DiffusionRenderer(Liang et¬†al.,2025).",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2510.26800/x6.png",
                "caption": "Figure 7:Qualitative evaluation of OmniX on panoramic geometry estimationcompared to state-of-the-art geometry estimation methods: DepthAnyCamera(Guo et¬†al.,2025), DepthAnywhere(Wang & Liu,2024), OmniData-v2(Kar et¬†al.,2022), MGNet(Zhu et¬†al.,2022), DiffusionRenderer(Liang et¬†al.,2025), and MoGe(Wang et¬†al.,2025). Our approach demonstrates a notable advantage in capturing fine image details, thanks to the proposed cross-modal adapter structure that effectively leverages 2D generative priors to enhance visual perception.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2510.26800/x7.png",
                "caption": "Figure 8:Demonstrations of the graphics-compatible 3D scenes created with OmniX, ready for free exploration, PBR-based relighting, and physical dynamics simulation.",
                "position": 753
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AIn-the-wild Panoramic Perception",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26800/x8.png",
                "caption": "Figure 9:Panoramic perception results of OmniX on in-the-wild images. Our method demonstrates excellent generalization performance on unseen images.",
                "position": 1613
            }
        ]
    },
    {
        "header": "Appendix BResults on Panoramic Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26800/x9.png",
                "caption": "Figure 10:Panorama generation results of OmniX given a single image input. Note that the input single-view image is generated by Flux.1-dev(Labs,2025).",
                "position": 1623
            }
        ]
    },
    {
        "header": "Appendix CResults on Panoramic Completion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.26800/x10.png",
                "caption": "Figure 11:Panorama completion and guided panoramic perception results of OmniX. Given masked inputs and corresponding masks, OmniX is able to generate accurate and locally coherent results for masked areas. For guided panoramic perception, extra RGB references are input to ensure that the prediction results are consistent with RGB references.",
                "position": 1633
            }
        ]
    },
    {
        "header": "Appendix DMore Ablation Analysis and Discussion",
        "images": []
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    }
]