[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08189/x1.png",
                "caption": "Figure 1:Timeline of Representative Visual Reinforcement Learning Models.The figure presents a chronological overview of key Visual RL models from 2023 to 2025, organized into four tracks: Multimodal LLM, Visual Generation, Unified Models, and VLA Models.",
                "position": 226
            }
        ]
    },
    {
        "header": "2Preliminary: Reinforcement Learning in LLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08189/x2.png",
                "caption": "Figure 2:Three Alignment Paradigms for Reinforcement Learning.(a) RLHF learns a reward model from human preference data and optimizes the policy via PPO.\n(b) DPO removes the reward model and directly optimizes a contrastive objective against a frozen reference.\n(c) RLVR replaces subjective preferences with deterministic verifiable signals and trains the policy using GRPO.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2508.08189/x3.png",
                "caption": "Figure 3:Two Representative Policy Optimization Algorithms for LLM.PPO (a) uses a learned value modelVψV_{\\psi}for advantage estimation and injects the KL penalty at each token.\nGRPO (b) removes the value model, computes group-normalized advantagesA^i,t\\hat{A}_{i,t}acrossGGcontinuations, and applies an explicit prompt-level KL penalty.",
                "position": 713
            }
        ]
    },
    {
        "header": "3Reinforcement Learning in Vision",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.08189/x4.png",
                "caption": "Figure 5:Three reward paradigms for RL-based image generation.(a) Human-Centric Preference Optimization: aligns outputs with human aesthetic scores (HPS(Wu et al.,2023b), ImageReward(Xu et al.,2023)); (b) Multimodal Reasoning-Based Evaluation: scores images via multimodal reasoning consistency (UnifiedReward(Wang et al.,2025h), PARM(Guo et al.,2025d)); (c) Metric-Driven Objective Optimization: minimizes task-specific quantitative metrics such as FID and IoU.",
                "position": 1175
            },
            {
                "img": "https://arxiv.org/html/2508.08189/x5.png",
                "caption": "Figure 6:Metric Granularity in Visual RL.(a) Set-level metricℳset\\mathcal{M}_{\\mathrm{set}}: one score over the whole prompt set, used for final evaluation (e.g.,FID).\n(b) Sample-level metricℳsamp\\mathcal{M}_{\\mathrm{samp}}: per-output rewards that train the policy (RLHF, DPO).\n(c) State-level metricℳstatet\\mathcal{M}_{\\mathrm{state}}^{t}: training-time signals like KL or length drift, used to monitor stability.\nNotation:pip_{i},ygeniy^{i}_{\\text{gen}},ygtiy^{i}_{\\text{gt}}denote the prompt, the generated output, and ground truth, respectively.πθ0\\pi_{\\theta_{0}}andπθt\\pi_{\\theta_{t}}refer to the0-th andtt-th policy model.Rϕ​(⋅)R_{\\phi}(\\cdot)denotes the reward model.",
                "position": 1310
            }
        ]
    },
    {
        "header": "4Metrics and Benchmarks",
        "images": []
    },
    {
        "header": "5Challenges and Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]