[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/x1.png",
                "caption": "Figure 1:An example use case of our model; information retrieval on specific aspects of a scene.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2503.00329/x2.png",
                "caption": "Figure 2:An overview of our training regime. We use negative mining to augment our pretraining dataset withalmost plausibletext negatives for each image query. In our instruction finetuning stage, we craft multiple instructions from each image. We use multiple for captions for same imageas negatives, the model must use the natural language instruction to choose the best (positive) text candidate for the query.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3ABC",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/x3.png",
                "caption": "Figure 3:A sample from our pretraining dataset. The positive caption(green)is the best caption for the image. The mined negatives(red)are relevant but not the best choice.",
                "position": 205
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/x4.png",
                "caption": "Figure 4:(Left)Pixel distributions of benchmarks.(Right)Scaling the number of pixels (tokens) in the vision encoder.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2503.00329/x5.png",
                "caption": "",
                "position": 818
            },
            {
                "img": "https://arxiv.org/html/2503.00329/x6.png",
                "caption": "Figure 5:Visualization of temperature (œÑùúè\\tauitalic_œÑ) and its effects on training dynamics.(Left)The effect of temperature initializations on gradient norm while training our negative mining model.(Middle)Temperature behavior during pretraining, without and without mined negatives.(Right)Validation accuracy during pretraining, with and without mined negatives.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2503.00329/x7.png",
                "caption": "",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2503.00329/x8.png",
                "caption": "",
                "position": 882
            }
        ]
    },
    {
        "header": "5Discussion and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/x9.png",
                "caption": "Figure 6:An example of a poorly constructed task. The candidate pool has no relevant negatives, and therefore the image isn‚Äôt needed to solve the task.",
                "position": 1082
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInstruction Prompt Design",
        "images": []
    },
    {
        "header": "Appendix BArchitecture Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/x10.png",
                "caption": "Figure 7:(Left)Causal vs Bidirectional attention.(Right)LoRA adapter rank ablation.",
                "position": 1988
            },
            {
                "img": "https://arxiv.org/html/2503.00329/x11.png",
                "caption": "",
                "position": 1997
            }
        ]
    },
    {
        "header": "Appendix CScaling Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/x12.png",
                "caption": "",
                "position": 2012
            }
        ]
    },
    {
        "header": "Appendix DComparison to VLM2Vec",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.00329/extracted/6243274/ctrlbench/img1.jpg",
                "caption": "",
                "position": 2114
            },
            {
                "img": "https://arxiv.org/html/2503.00329/extracted/6243274/ctrlbench/img1.jpg",
                "caption": "",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2503.00329/extracted/6243274/ctrlbench/img2.jpg",
                "caption": "",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2503.00329/extracted/6243274/ctrlbench/img3.jpg",
                "caption": "",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2503.00329/extracted/6243274/ctrlbench/img3.jpg",
                "caption": "",
                "position": 2205
            },
            {
                "img": "https://arxiv.org/html/2503.00329/extracted/6243274/ctrlbench/img4.jpg",
                "caption": "",
                "position": 2247
            }
        ]
    },
    {
        "header": "Appendix ECtrlBench",
        "images": []
    }
]