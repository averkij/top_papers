[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20563/x1.png",
                "caption": "",
                "position": 68
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20563/x2.png",
                "caption": "Figure 2:Method Overview: We represent 3D humans in loose clothing as temporally consistent 4D neural fields (Sec.3.1). Central to our approach is a flexible motion representation that captures fine-grained clothing deformations as well as limb motions, while effectively utilizing domain-specific priors such as 3D human body pose (Sec.3.2). We perform video-specific optimization that fits this model to dense image-based priors via differentiable rendering (Sec.3.3). After optimization, our neural implicit surface can be extracted into a time-consistent mesh via marching cubes, or converted into explicit 3D Gaussians for high-fidelity interactive rendering (Sec.3.4).",
                "position": 209
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20563/x3.png",
                "caption": "Figure 3:Visualization of two-layer deformation. The body and clothing deformation layers each contribute separate types of motion. In this sequence, the clothing Gaussians deform the woman’s dress to be larger, while the body Gaussians move her right arm forward. During forward warping, we start from the canonical shape (left), and first apply the forward warp described by clothing Gaussians, then the forward warp described by body Gaussians. The same process happens in reverse during backward warping.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.20563/x4.png",
                "caption": "Figure 4:3D reconstruction results on DNA-Rendering.We demonstrate DressRecon’s ability to reconstruct challenging sequences with large cloth deformation. DressRecon’s predictions align well with the image evidence, even in the presence of rapid clothing and object deformations. Vid2Avatar often outputs spurious shape artifacts and is unable to reconstruct challenging structures, such as the white cloth (row 2), brown brush (row 3), and detailed sleeves (row 4). BANMo and RAC produce hollow cellos on the first row, and tend to output over-smoothed surfaces for the other cases. ECON produces highly detailed textures, but it performs the worst numerically (Tab.LABEL:tab:chamfer_dna_rendering) as the outputs often have an incorrect overall shape (e.g. Row 1). We encourage readers to view the video results on the supplementary webpage.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2409.20563/x5.png",
                "caption": "Figure 5:3D reconstruction results on ActorsHQ.DressRecon is on par with Vid2Avatar for standard clothing (Rows 2 and 4), and higher fidelity than Vid2Avatar for loose clothing (Rows 1 and 3). Vid2Avatar’s reconstructed skirts often contain shape artifacts. We attribute DressRecon’s improved performance to its flexible shape and deformation representation, which is capable of representing non-standard geometry and deformation.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2409.20563/x6.png",
                "caption": "Figure 6:Qualitative ablation of numerical normals. We show the difference between optimizing with numerical and analytical normals. Using analytical normals causes training to be unstable, resulting in a flat shape with no surface detail. The quality of surface details is reduced when normal loss is disabled (Tab.5).",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2409.20563/x7.png",
                "caption": "Figure 7:RGB rendering results on DNA-Rendering.For each sequence, we show DressRecon’s and Vid2Avatar’s renderings at both the input view and a 90-degree novel view. DressRecon’s renderings are shown with and without 3D Gaussian refinement. We find (similar to Tab.LABEL:tab:psnr_dna_rendering) that refinement significantly improves the textures, especially the flowers on the yellow dancer’s sleeve. Vid2Avatar’s renderings are less detailed, and fail to accurately depict structures that substantially deviate from the body, such as the cello and white stool.",
                "position": 516
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Video Results",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    }
]