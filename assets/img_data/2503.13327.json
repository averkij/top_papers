[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13327/x1.png",
                "caption": "Figure 1:Edit Transferaims to learn a transformation from a given source‚Äìtarget editing example, and apply the edit to a query image.\nOur framework can effectively transfer both (b) single and (c) compositional non-rigid edits via proposed visual relation in-context learning.",
                "position": 62
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13327/x2.png",
                "caption": "Figure 2:Comparisons with existing editing paradigms.(a) Existing TIE methods[20,21,22,23,24,25]rely solely on text prompts to edit images, making them ineffective for complex non-rigid transformations that are difficult to describe accurately.\n(b) Existing RIE methods[6,7,8,9,10,11,12,13,14,15]incorporate visual guidance via a reference image but primarily focus on appearance transfer, failing in non-rigid pose modifications.\n(c) In contrast, our proposed Edit Transfer learns and applies the transformation observed in editing examples to a query image, effectively handling intricate non-rigid edits.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13327/x3.png",
                "caption": "Figure 3:Visual relation in-context learning for Edit Transfer.(a) We arrange in-context examples in a four-panel layout:\nthe top row (an editing pair(‚Ñês,‚Ñêt)subscript‚Ñêùë†subscript‚Ñêùë°(\\mathcal{I}_{s},\\mathcal{I}_{t})( caligraphic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )) and the bottom row (the query pair(‚Ñê^s,‚Ñê^t)subscript^‚Ñêùë†subscript^‚Ñêùë°(\\mathcal{\\hat{I}}_{s},\\mathcal{\\hat{I}}_{t})( over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )).\nOur goal is to\nto learn the transformation from‚Ñês‚Üí‚Ñêt‚Üísubscript‚Ñêùë†subscript‚Ñêùë°\\mathcal{I}_{s}\\to\\mathcal{I}_{t}caligraphic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚Üí caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and apply it to the bottom-left image‚Ñê^ssubscript^‚Ñêùë†\\hat{\\mathcal{I}}_{s}over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, producing the target‚Ñê^tsubscript^‚Ñêùë°\\hat{\\mathcal{I}}_{t}over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, in the bottom-right. (b) We fine-tune a lightweight LoRA in the MMA to better capture visual relations.\nNoise addition and removal are applied only toztsubscriptùëßùë°z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, while the conditional tokenscTsubscriptùëêùëác_{T}italic_c start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT( derived from(‚Ñês,‚Ñêt,‚Ñê^s)subscript‚Ñêùë†subscript‚Ñêùë°subscript^‚Ñêùë†(\\mathcal{I}_{s},\\mathcal{I}_{t},\\hat{\\mathcal{I}}_{s})( caligraphic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )) remain noise-free.\n(c) Finally, we cast Edit Transfer as an image generation task by initializing the bottom-right latent tokenzTsubscriptùëßùëáz_{T}italic_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTwith random noise and concatenating it with the clean tokenscIsubscriptùëêùêºc_{I}italic_c start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT.\nLeveraging the enhanced in-context capability of the fine-tuned DiT blocks, the model generates‚Ñêtsubscript‚Ñêùë°\\mathcal{I}_{t}caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, effectively transferring the same edits from the top row to the bottom-left image.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13327/x4.png",
                "caption": "Figure 4:Edit Transferexhibits impressive versatility to transfer visual exemplar pairs‚Äôedit into the requested source image, delivering high-quality (a) single-edit transformations as well as (b) effective compositional edits that seamlessly combine multiple modifications.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x5.png",
                "caption": "Figure 5:Qualitative comparisons.Compared with TIE and RIE methods, our method consistently outperforms in various non-rigid editing tasks.\nWe provide the detailed text prompt of TIE methods in ¬†SectionB.1.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x6.png",
                "caption": "Figure 6:Results of user study and VLM evaluation.We compare our Edit Transfer (IV) with P2P[20](I), RF-Solver-Edit[23](II) and MimicBrush[13](III).\n(a) The values show the proportion of users who prefer our method over the others.\n(b) The values represent the average scores given to each method by GPT-4o[31].",
                "position": 449
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13327/x7.png",
                "caption": "Figure 7:Influence of dataset scale.(a)\nSetting the number of training samples per editing type toNc=2subscriptùëÅùëê2N_{c}=2italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 2is sufficient for learning effective non-rigid edits, even when the total number of editing typesNT=10subscriptùëÅùëá10N_{T}=10italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 10.\n(b) IncreasingNT=21subscriptùëÅùëá21N_{T}=21italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = 21further improves the model‚Äôs ability to capture subtle local edits and enhances its generalization to cases where the editing example and query image are spatially misaligned.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x8.png",
                "caption": "Figure 8:Oursvs.w/o fine-tuning.Without fine-tuning, Flux can only capture some of the pose information identified inItsubscriptùêºùë°I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTregardless of the relation betweenIs‚ÜíIt‚Üísubscriptùêºùë†subscriptùêºùë°I_{s}\\to I_{t}italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚Üí italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandI^ssubscript^ùêºùë†\\hat{I}_{s}over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT.\nIn contrast, with our few-shot fine-tuning, the model effectively learns the visual relation from example pairs and applied toI^ssubscript^ùêºùë†\\hat{I}_{s}over^ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT.",
                "position": 511
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x9.png",
                "caption": "Figure 9:Investigating the alignment between text and visual example pairs.When the text prompt and visual demonstrations convey different semantics, the generated images‚Ñê^tsubscript^‚Ñêùë°\\hat{\\mathcal{I}}_{t}over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTtend to (a)(b) exhibit mixed semantics from both sources, and either (c) follow the text or (d) the visual demonstrations.\nNote that theredlabel indicates misalignment, whilegreenlabel indicates alignment.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x10.png",
                "caption": "Figure 10:Generalization performance of Edit Transfer.Our model demonstrates remarkable generalization by: (b) Generating novel pose variations within a given editing type, even if such variations were unseen during training; (c) Flexibly combining different editing types; (d) Transferring its capabilities across other species.",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x11.png",
                "caption": "Figure 11:Limitations.Our method struggles with low-level properties transfere.g.color.",
                "position": 591
            }
        ]
    },
    {
        "header": "5Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BDetails of Comparisons with Baselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13327/x12.png",
                "caption": "Figure 12:Image samples of each editing type in the dataset.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x13.png",
                "caption": "Figure 16:Additional experimental results of single edit transfer.",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x14.png",
                "caption": "Figure 17:Additional experimental results of single edit transfer.",
                "position": 1273
            },
            {
                "img": "https://arxiv.org/html/2503.13327/x15.png",
                "caption": "Figure 18:Additional experimental results of compositional edit transfer.",
                "position": 1276
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]