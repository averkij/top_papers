[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24213/x1.png",
                "caption": "Figure 1:Samples from our progression of video generation models and additionally included image datasets.We present 4 frames from timestampst‚àà{0,10,20,30}ùë°0102030t\\in\\{0,10,20,30\\}italic_t ‚àà { 0 , 10 , 20 , 30 }of a randomly sampled video from each of our generated datasets, and UCF101 (left to right).",
                "position": 90
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Pre-training Video Models without Natural Videos",
        "images": []
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24213/x2.png",
                "caption": "Figure 2:Action recognition accuracy on UCF101.We present the UCF101 classification accuracy of the progression of models{Mi}subscriptùëÄùëñ\\{M_{i}\\}{ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, after fine-tuning each of them on UCF101. The accuracy increases along the progression.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2410.24213/x3.png",
                "caption": "Figure 3:Distribution Shift results on UCF101-P(Schiappa et¬†al.,2023)(ViT-B)The last model in our progression outperforms pre-training on natural videos for 11 out of 14 corruption datasets.",
                "position": 239
            }
        ]
    },
    {
        "header": "5Datasets Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.24213/x4.png",
                "caption": "Figure 4:Dataset properties compared to downstream performance.We compare the downstream classification accuracy on UCF101 after fine-tuning to frame and video properties of all the dataset variants we used in our analysis (see datasets list inSectionA.1).",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2410.24213/x5.png",
                "caption": "Figure 5:Feature visualizations for pre-trained models.We present the 3 principal components of the attention keys of the last encoder layer, for allMisubscriptùëÄùëñM_{i}italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTas the three color channels. Different object parts start to appear as the datasets progress.",
                "position": 502
            }
        ]
    },
    {
        "header": "6Limitations and discussion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]