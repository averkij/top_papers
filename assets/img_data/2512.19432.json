[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19432/logo/tongyi.jpg",
                "caption": "",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x1.png",
                "caption": "Figure 1:Compared to AndroidWorld, MobileWorld exhibits lower SOTA success rates, longer task horizons, more cross-application tasks, and sharp performance drops for recent models.",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19432/x2.png",
                "caption": "Figure 2:Beyond traditional GUI-only tasks, MobileWorld includes agent–user interaction tasks and MCP-augmented tasks, each with distinct deterministic evaluation strategies.Left: An example of an agent–user interaction task, in which the agent must proactively request clarification from a simulated user when encountering incomplete information. A GPT-4.1–based simulated user agent is then triggered to provide the requested information, which is embedded in its system prompt. Task completion is verified through the application’s callback cache.Right: An example of an MCP-augmented task, where the agent is initialized with a list of GitHub MCP tools and selects the appropriate tool to retrieve README content from a GitHub repository before completing the task via GUI operations. Task completion is verified through backend database inspection.",
                "position": 196
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3MobileWorld",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19432/x3.png",
                "caption": "Figure 3:The system architecture of MobileWorld consists of two main components.Left: the host machine is where GUI agents receive task instructions and optionally interact with users for clarification, then choose between GUI actions or MCP tool calls to complete tasks.Right: the docker environment contains an isolated Android ecosystem with emulators, self-hosted app backends, and an evaluator that verifies task completion through text matching, backend database, local storage, and app callbacks.",
                "position": 690
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x4.png",
                "caption": "Figure 4:Scenario Distribution. The benchmark predominantly features third-party applications (95%), with system apps comprising the remaining 5% of tasks.",
                "position": 949
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x4.png",
                "caption": "Figure 4:Scenario Distribution. The benchmark predominantly features third-party applications (95%), with system apps comprising the remaining 5% of tasks.",
                "position": 951
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19432/x5.png",
                "caption": "Table 6:Success rate (%) comparison of state-of-the-art mobile GUI agent models on MobileWorld under maximum 50 steps. We report overall SR and breakdown by task category: GUI-Only tasks, agent-user interaction tasks, and MCP-augmented tasks. The number of tasks is indicated in the corresponding column.Boldindicates the best result andunderlineindicates the second best.",
                "position": 1177
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x5.png",
                "caption": "Table 7:Detailed metrics comparison on MobileWorld under maximum 50 steps. We report Success Rate (SR), Average Completion Steps (Steps), Average User Queries (Queries), User Interaction Quality (UIQ), and Average MCP Calls (MCP).Boldindicates the best result andunderlineindicates the second best.",
                "position": 1311
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x5.png",
                "caption": "Figure 5:Comparison of completion steps between AndroidWorld and MobileWorld.",
                "position": 1476
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x6.png",
                "caption": "Figure 6:Hallucination without user clarification: A representative failure case showing how the mobile GUI agent hallucinates actions when faced with ambiguous scenarios that require user clarification.",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x7.png",
                "caption": "Figure 7:Context overflow from MCP tool responses: A failure case demonstrating ineffective MCP tool integration due to context management issues, where tool responses exceed the context window capacity.",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x8.png",
                "caption": "Figure 8:Lack of long-term memory: A representative case illustrating insufficient memory mechanisms for tracking multi-step operations, leading to failure in maintaining state across sequential actions.",
                "position": 1491
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix APlanner-Executor Agentic Framework Details",
        "images": []
    },
    {
        "header": "Appendix BAPP Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19432/x9.png",
                "caption": "Figure 9:Task Completion Task Example",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x10.png",
                "caption": "Figure 10:Information Retrieval Task Example",
                "position": 2107
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x11.png",
                "caption": "Figure 11:Agent-User Interaction Task Example",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2512.19432/x12.png",
                "caption": "Figure 12:MCP-Augmented Task Example",
                "position": 2128
            }
        ]
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]