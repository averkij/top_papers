[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/model-architecture-and-3d-modality.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/task-teaser.png",
                "caption": "Figure 2:3D task exampleswith Kyvo‚Äôs unified autoregressive framework using a structured 3D modality. (1)3D shape and scene reconstruction:From a single input image, Kyvo reconstructs individual objects with accurate geometry and spatial relationships. (2)3D object recognition:Given an input image, Kyvo identifies objects and predicts their 3D positions in real-world scenes. (3)Shape and scene rendering:Kyvo generates semantically consistent images from structured 3D scene inputs. (4)Instruction-Following:Given an image, 3D scene and text instruction, Kyvo produces coherent modifications to both image and the 3D representation.",
                "position": 168
            }
        ]
    },
    {
        "header": "3Building our model token-by-token",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/granularity-rendering.png",
                "caption": "Figure 3:Effect of Granularity.A0.050.050.050.05granularity more accurately captures object locations and shapes.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2506.08002/x1.png",
                "caption": "Figure 4:Hybrid number encodings are robust across training data scales.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/center-token-reordering-visual.png",
                "caption": "(a)",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/center-token-reordering-visual.png",
                "caption": "(a)",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/center-token-reordering-plot.png",
                "caption": "",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/center-token-reordering-rendering.png",
                "caption": "(b)",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/objaworld-rendering-singlerow.png",
                "caption": "(a)",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/objaworld-rendering-singlerow.png",
                "caption": "(a)",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/ood.png",
                "caption": "(b)",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/task-decomposition-objaworld.png",
                "caption": "(c)",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/shape-and-scene-reconstruction.png",
                "caption": "(a)",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/shape-and-scene-reconstruction.png",
                "caption": "(a)",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/shape-and-scene-rendering.png",
                "caption": "(b)",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures/scaling-laws-rendering-opengeometry.png",
                "caption": "Figure 8:Qualitative Scaling Behavior.Training on limited data produces amorphous color blobs lacking semantic coherence. Increasing training data progressively improves geometric precision and spatial relationships.",
                "position": 561
            }
        ]
    },
    {
        "header": "4Conclusion & Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset details",
        "images": []
    },
    {
        "header": "Appendix BAdditional qualitative examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/app-rendering-examples-clevr.png",
                "caption": "Figure 9:Rendering examples for CLEVR.Example image generations for the rendering task on CLEVR. The model takes a 3D scene as input and produces a corresponding image. Additionally, we show the ground-truth image rendered using Blender.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/app-rendering-examples-objaworld.png",
                "caption": "Figure 10:Rendering examples for ObjaWorld.Example image generations for the rendering task on ObjaWorld with complex shapes. The model takes a 3D scene as input and produces a corresponding image. Additionally, we show the ground-truth image rendered using Blender.",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/appendix-scene-to-images-examples.png",
                "caption": "Figure 11:Rendering examples for ObjaWorld with explicit shape encodings.Example image generations for the rendering task on ObjaWorld. The model takes a 3D scene with embedded shape encodings as input and produces a corresponding image. Additionally, we show the ground-truth image rendered using Blender.",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/app-recognition-examples-clevr.png",
                "caption": "Figure 12:Recognition examples for ObjaWorld.Two example predictions from the recognition task on ObjaWorld. The colored numbers indicate object matching between the predicted and ground-truth scenes, based on the criteria for Jaccard Index as defined in Algorithm1. Note that the fourth number in the list is the azimuth pose value, this format of prediction saves sequence length.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/appendix-images-to-scenes-examples-1.png",
                "caption": "Figure 13:Unified shape and scene reconstruction examples.Given a single input image, Kyvo predicts shape sequences and reconstructs individual objects (bottle, cheeseburger,etc.) along with their 3D locations and poses via our structured 3D modality, effectively reconstructing the 3D scene with consistent spatial relations between the objects, visualized using Blender.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/appendix-images-to-scenes-examples-2.png",
                "caption": "",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/app-qa-examples-clevr.png",
                "caption": "Figure 14:Question-answering examples for CLEVR.Example cases from the question-answering task on CLEVR. The model takes an image, a 3D scene, and a question as input to generate the corresponding answer.",
                "position": 1723
            }
        ]
    },
    {
        "header": "Appendix CEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/accuracy_comparison-effect-of-tau.png",
                "caption": "Figure 15:Effect ofœÑùúè\\tauitalic_œÑ.The plot shows the impact ofœÑùúè\\tauitalic_œÑon the Jaccard Index for models trained with increasing amounts of training data on CLEVR for the recognition task. The drop in Jaccard Index with decreasingœÑùúè\\tauitalic_œÑis more pronounced for models trained on smaller datasets. Higher-performing models demonstrate greater robustness to changes inœÑùúè\\tauitalic_œÑ.",
                "position": 1887
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/human-evals-ui.png",
                "caption": "Figure 16:Human Evaluation User Interface.A snapshot of the user interface used for human evaluation of generated images. This example is taken from an experiment analyzing the effect of center-token reordering and weighted loss, comparing four models. The results of this experiment are presented in Table 2 of the main paper.",
                "position": 1906
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/ssim-visual.png",
                "caption": "Figure 17:Object-level nuances are challenging for image metrics, like SSIM and L2-loss, to capture, prompting the need for human evaluation. The predicted image is incorrect in both cases but differs only subtly from the groundtruth.",
                "position": 1912
            }
        ]
    },
    {
        "header": "Appendix DAdditional implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/DomainSpecificVQGANArchitecture.png",
                "caption": "Figure 18:VQGAN Architecture.We show the detailed architecture of the VQGAN model that we use to train a domain-specific codebook. We show the output shapes for an input shape (1111,3333,256256256256,256256256256).",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/gt_image_step0100000.jpg",
                "caption": "(a)Groundtruth",
                "position": 2303
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/gt_image_step0100000.jpg",
                "caption": "(a)Groundtruth",
                "position": 2306
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/ref_image_step0100000.jpg",
                "caption": "(b)Reconstructions w/ Trellis",
                "position": 2311
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/threshold_image_1.0_step0100000.jpg",
                "caption": "(c)Reconstructions w/ 3D VQ-VAE",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/codebook-usage.png",
                "caption": "Figure 20:Codebook usage.Histogram of usage counts (log scale) for the8192819281928192latent codes on5000500050005000assets used for training (unused codes are omitted). The heavy-tailed distribution reveals a compact core of frequently reused codes that capture ubiquitous 3D primitives, while the long tail represents rarer geometric variations. In total,‚âà70%absentpercent70\\approx 70\\%‚âà 70 %of the vocabulary is exercised at least once, indicating that the codebook is neither under- nor over-parameterised for the diversity of real-world assets.",
                "position": 2326
            }
        ]
    },
    {
        "header": "Appendix EAdditional experiments and observations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/llm-failure.png",
                "caption": "Figure 21:Failure of modern-day LLMs on rendering task.Each row depicts one test scene described by our 3D structured modality; columns show the ground-truth Blender render (GT) and images produced by GPT4o, Gemini 2.0 Flash, and Kyvo (ours). GPT4o and Gemini frequently hallucinate extra objects, omit specified ones, or displace shapes, violating fundamental xyz and relational constraints.",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/llm-failure-llamav.png",
                "caption": "Figure 22:Failure of Llama3.2-V on recognition task.For six randomly selected scenes, we render (from left to right) the ground-truth scene, the recognition prediction scene by Llama 3.2-Vision, and the predicted scene by Kyvo (ours). Llama 3.2-Vision frequently collapses objects toward the centre or misplaces them entirely, failing to recover the true spatial layout even in this simple synthetic setting. Moreover, it also misidentifies some objects in the scene.",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/max_value_histogram-objectron.png",
                "caption": "(a)Objectron",
                "position": 2419
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/max_value_histogram-objectron.png",
                "caption": "(a)Objectron",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/max_value_histogram-arkitscenes.png",
                "caption": "(b)ARKitScenes",
                "position": 2427
            },
            {
                "img": "https://arxiv.org/html/2506.08002/extracted/6526522/figures-appendix/instruction-following-failure.png",
                "caption": "Figure 24:Instruction-following failure cases for the image modality on CLEVR.. As observed, the images generated by the model do not accurately reflect the intended modifications based on the input image and text instruction. On the other hand, the output 3D scenes are correct, meaning that our Kyvo accurately modified them based on the instructions. This suggests that a better avenue for predicting instruction-modified images is by task decomposition: first predict the modified 3D scene and then render the final image.",
                "position": 2447
            }
        ]
    },
    {
        "header": "Appendix FFailure cases",
        "images": []
    }
]