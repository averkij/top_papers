[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14059/extracted/5945072/figures/fin-robot.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14059/x1.png",
                "caption": "Figure 1:Overview framework of theUCFE Benchmark.",
                "position": 164
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4User-Centric Financial Expertise Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14059/x2.png",
                "caption": "Figure 2:The visualization displays the top 25 most common root verbs (inner circle) and their top 4 associated direct noun objects (outer circle) extracted from the provided texts.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x3.png",
                "caption": "Figure 3:Distribution of test and evaluation input lengths for the datasets.",
                "position": 333
            }
        ]
    },
    {
        "header": "5UCFE Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14059/x4.png",
                "caption": "Figure 4:The evaluation pipeline of theUCFE Benchmarkinvolves the following steps: ① selecting the model and task, ② generating dialogues between the user and AI assistant via a user simulator, ③ creating evaluation prompts based on source information to assess model performance, ④ pairwise comparison of dialogue outputs by evaluators, aligned with human expert judgments, and ⑤ computing Elo scores based on win-loss outcomes.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x5.png",
                "caption": "Figure 5:Comparison of model performance onUCFE benchmarkacross three evaluators.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x6.png",
                "caption": "Figure 6:Comparison of average dialogue rounds and total tokens across different models in few shot tasks.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x7.png",
                "caption": "Figure 7:Correlation between human Elo scores and Claude-3.5-Sonnet Elo scores.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x8.png",
                "caption": "Figure 8:Overall Elo scores of various models plotted against model parameters (in billions).",
                "position": 847
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "Ethical Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AQuestionnaire",
        "images": []
    },
    {
        "header": "Appendix BUCFE Dataset Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14059/x9.png",
                "caption": "Figure 11:Geographical Distribution of Survey Respondents",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x10.png",
                "caption": "Figure 12:Primary Source of Financial Information extracted from the survey",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x11.png",
                "caption": "Figure 13:Results of whether preferring generation answers or predefined options",
                "position": 1695
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x12.png",
                "caption": "Figure 14:Win counts heatmap for all tasks. The heatmap illustrates the total number of wins where the target model outperforms the base model across all head-to-head comparisons.",
                "position": 1853
            }
        ]
    },
    {
        "header": "Appendix CHuman Expert Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.14059/x13.png",
                "caption": "Figure 15:Human evaluation pipeline.",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2410.14059/x14.png",
                "caption": "Figure 16:UI interface for human expert evaluation",
                "position": 1866
            }
        ]
    },
    {
        "header": "Appendix DMore Experiment Results",
        "images": []
    },
    {
        "header": "Appendix EPrompt",
        "images": []
    }
]