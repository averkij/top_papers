[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17637/x1.png",
                "caption": "Figure 1:(a) Overview of MIA-DPO.We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization.(b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks.",
                "position": 182
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17637/x2.png",
                "caption": "Figure 2:Examples of Multi-Image Hallucinations.Top:Sequence Confusionthat the model is confused about the order in which the images should be referenced.Bottom:Element Interference. The model incorrectly identified the attributes due to visual element interference across different images.Attention valuesillustrate how the modelâ€™s focus was dispersed across different images, resulting in the hallucination response.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2410.17637/x3.png",
                "caption": "Figure 3:MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2410.17637/x4.png",
                "caption": "Figure 4:Multi-Images DPO Data Format.To address multi-image hallucinations mentioned in Fig.2, we construct our multi-image prompts in three formats: (a) Sequence. (b) Grid Collage. (c) Pic-in-Pic.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2410.17637/x5.png",
                "caption": "Figure 5:Attention Ratio Statistic.We analyze the attention ratios distribution for different image counts across various data types, and use dashed lines to indicate the thresholds for each data set.",
                "position": 387
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17637/x6.png",
                "caption": "Figure 6:Attention Difference Before and After DPO.We present the attention distribution in the intermediate layers for the original LLaVA-v1.5 (top row), MIA-DPO + LLaVA-v1.5 (second row), and the difference value (bottom row), respectively.",
                "position": 1053
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Experiments",
        "images": []
    },
    {
        "header": "Appendix BModel and Data Sources",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.17637/x7.png",
                "caption": "Figure 7:Sequence Image Data Cases.The image displays several examples of Sequence data.",
                "position": 2398
            },
            {
                "img": "https://arxiv.org/html/2410.17637/x8.png",
                "caption": "Figure 8:Grid Collage Data Cases with Two Images.We present some examples of Grid Collage Data, which consists of images created by stitching together 2 to 9 pictures. Here, we showcase examples of images that combine two pictures.",
                "position": 2401
            },
            {
                "img": "https://arxiv.org/html/2410.17637/x9.png",
                "caption": "Figure 9:Pic-in-Pic Image Data Cases.The image displays several examples of Pic-in-Pic data.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2410.17637/x10.png",
                "caption": "Figure 10:Attention Observation.We studied the multi-layer attention of LVLMs and found that the attention of images is most pronounced in the middle layer.",
                "position": 2407
            }
        ]
    },
    {
        "header": "Appendix CMIA-DPO Data Cases",
        "images": []
    },
    {
        "header": "Appendix DMore Observation",
        "images": []
    }
]