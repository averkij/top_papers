[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22238/x1.png",
                "caption": "",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2512.22238/x2.png",
                "caption": "Figure 2:Illustrating training dynamics ofMasters, where we represent how (a) mask ratio is controlled during distillation, and (b) its averaged performance log of student (InternVL3.5-8B[wang2025internvl3]) for evaluation benchmarks inTab.˜1. In addition, we show (c) the effect of RL under naive and mask-progressive distillation. Note that asterisk (*) represents the combined distillation of mid-size and large teacher.",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22238/x3.png",
                "caption": "Figure 3:Overview of mask-progressive distillation where teacher is masked with a decreasing masking ratio (0.20, 0.15, 0.10, 0.05, 0), gradually restoring its full capacity. At each masking stage for teacher, student is updated using two rewards: accuracy rewardRaccR_{\\text{acc}}and distillation rewardRdistillR_{\\text{distill}}. This progressive distillation enables smooth and stable knowledge transfer to the student.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2512.22238/x4.png",
                "caption": "Figure 4:Depicting multiple responses generated by both the masked teacher and the student, where anaccuracy reward(RaccR_{\\text{acc}}) evaluates the binary correctness of each response, and adistillation reward(RdistillR_{\\text{distill}}) measures the ease of knowledge transfer based on divergence objective between teacher and student logits. Note that the rank labels (1st, 2nd, 3rd, etc.) inRdistillR_{\\text{distill}}indicate the relative magnitude of the divergence values, where the smallest divergence (1st) receives the highest reward (1.0) and the largest divergence the lowest reward (0.0).",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Masters",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22238/x5.png",
                "caption": "Figure 5:Comparing the performances by the number of generated responses from teacher and student.",
                "position": 1471
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22238/x6.png",
                "caption": "Figure 6:Comparing hours of inference time at one GPU across various models described inTab.˜3",
                "position": 2411
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Appendix ARelated Work of Efficient VLMs",
        "images": []
    },
    {
        "header": "Appendix BThe Objective of GRPO",
        "images": []
    },
    {
        "header": "Appendix CVisual Instruction Tuning Data",
        "images": []
    },
    {
        "header": "Appendix DAdditional Parsing Prompts for Accuracy Reward",
        "images": []
    }
]