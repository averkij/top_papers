[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2MMGiCDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x1.png",
                "caption": "Figure 1:Structured template(Left)and data example(Right)ofMMGiC.\nDifferent colored text indicatestemplate text,image placeholders,annotation placeholdersand multi-grained concept annotations, respectively.\nEach image‚Äìtext interleaved data sample will be tokenized into discrete tokens.",
                "position": 312
            }
        ]
    },
    {
        "header": "3Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x2.png",
                "caption": "Figure 2:Illustration of our general MLLM framework.\nOnly the LLM are loaded and partially fine-tuned during training.",
                "position": 374
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x3.png",
                "caption": "Figure 3:Comparison of generated captions by MLLMs pre-trained with different data recipes.MMGiC(C),MMGiC(CLD) andMMGiC(CLDR) denote the{0,2,30230,2,30 , 2 , 3}-th data recipes in Table1, respectively.\nThe bottom right of each example shows associated label‚Äìdescription pairs fromMMGiC.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2412.05939/x4.png",
                "caption": "Figure 4:Comparison of generated images by MLLMs pre-trained with different data recipes(Left)and image editing and multimodal in-context image synthesis examples(Right).",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2412.05939/x5.png",
                "caption": "Figure 5:Analysis on8888dimensions of SEED-Bench-IMG.\nLeft: the performance of MLLM-MMGiCtrained with different-grained concept annotations fromMMGiC.\nRight: corresponding case studies.\nCG, FG, and MG denote MLLMs trained with coarse-, fine-, and multi-grained concept annotations fromMMGiC, respectively.\n‚úÖ denote the ground truth; ‚ùé denote incorrect prediction(s).",
                "position": 1058
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABroader Impact",
        "images": []
    },
    {
        "header": "Appendix BLimitation and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x6.png",
                "caption": "Figure 6:Visualization of the original and reconstructed images by the visual tokenizer inherited from LaVIT(Jin et¬†al.,2023)used in this paper.\nText-rich and chart images cannot be well reconstructed.",
                "position": 3570
            }
        ]
    },
    {
        "header": "Appendix CMore Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x7.png",
                "caption": "Figure 7:Comparison of performance (CIDEr Score) and concept overlap (%) for different training data settings on image captioning datasets COCO and NoCaps.",
                "position": 3609
            },
            {
                "img": "https://arxiv.org/html/2412.05939/x8.png",
                "caption": "Figure 8:Detailed evaluation of MLLM-MMGiC&‚ÄâIC with different evaluation strategies on8888dimensions of SEED-Bench-IMG.",
                "position": 3766
            },
            {
                "img": "https://arxiv.org/html/2412.05939/x9.png",
                "caption": "Figure 9:Case study of MLLM-MMGiCtrained with different-grained concept annotations fromMMGiCon SEED-Bench-IMG.\nCG, FG, and MG denote MLLMs trained with coarse-, fine-, and multi-grained concept annotations fromMMGiC, respectively.\n‚úÖ denote the ground truth; ‚ùé denote incorrect prediction(s).\nThe bottom right of the first three examples show the associated label‚Äìdescription pairs fromMMGiC.",
                "position": 3784
            }
        ]
    },
    {
        "header": "Appendix DDiscussion",
        "images": []
    },
    {
        "header": "Appendix EExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x10.png",
                "caption": "Figure 10:A brief illustration of the hyperparameterm‚Å¢a‚Å¢s‚Å¢k‚Å¢_‚Å¢p‚Å¢r‚Å¢o‚Å¢bùëöùëéùë†ùëò_ùëùùëüùëúùëèmask\\_probitalic_m italic_a italic_s italic_k _ italic_p italic_r italic_o italic_bin the pre-training stage when the template is image-first.",
                "position": 4552
            }
        ]
    },
    {
        "header": "Appendix FDetails ofMMGiCDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x11.png",
                "caption": "Figure 11:Label frequency distribution of objects, attributes and relationships inMMGiC.",
                "position": 5127
            },
            {
                "img": "https://arxiv.org/html/2412.05939/x12.png",
                "caption": "",
                "position": 5131
            },
            {
                "img": "https://arxiv.org/html/2412.05939/x13.png",
                "caption": "",
                "position": 5133
            }
        ]
    },
    {
        "header": "Appendix GDetails of IC Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.05939/x14.png",
                "caption": "Figure 12:Comparison of noun-phrase statistics before and after filtering IC (not including aesthetic part).\nThe x-axis represents the unique noun-phrases ordered by frequency in the descending order.\nThe total number of unique noun-phrases are reported in the legend.",
                "position": 5195
            }
        ]
    },
    {
        "header": "Appendix HDetails of SFT Data",
        "images": []
    }
]