[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05638/x1.png",
                "caption": "",
                "position": 68
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05638/x2.png",
                "caption": "Figure 2:Overview of TrajectoryCrafter.Starting with a source video, whether casually captured or AI-generated, we first lift it into a dynamic point cloud via depth estimation. Users can then interactively render the point cloud with desired camera trajectories. Finally, the point cloud renders and the source video are jointly processed by our dual-stream conditional video diffusion model, yielding a high-fidelity video that precisely aligns with the specified trajectory and remains 4D consistent with the source video.",
                "position": 148
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05638/x3.png",
                "caption": "Figure 3:Ref-DiT Block.The text and view tokens are first processed through 3D attention, followed by a cross-attention that injects the detailed, yet mis-aligned, reference information into the view tokens, yielding refined view tokens for subsequent layers.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2503.05638/x4.png",
                "caption": "Figure 4:Double-reprojection.Given a target video, we lift it into a dynamic point cloud to render a novel viewùë∞‚Ä≤superscriptùë∞‚Ä≤\\bm{I}^{\\prime}bold_italic_I start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTvia a random view transformation. Thenùë∞‚Ä≤superscriptùë∞‚Ä≤\\bm{I}^{\\prime}bold_italic_I start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTis reprojected to the original camera pose, yieldingùë∞‚Ä≤‚Ä≤superscriptùë∞‚Ä≤‚Ä≤\\bm{I}^{\\prime\\prime}bold_italic_I start_POSTSUPERSCRIPT ‚Ä≤ ‚Ä≤ end_POSTSUPERSCRIPTthrough the inverse view transformation.ùë∞‚Ä≤‚Ä≤superscriptùë∞‚Ä≤‚Ä≤\\bm{I}^{\\prime\\prime}bold_italic_I start_POSTSUPERSCRIPT ‚Ä≤ ‚Ä≤ end_POSTSUPERSCRIPTcontains occlusions and aligns with the target video, simulating the point cloud renders.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2503.05638/x5.png",
                "caption": "Figure 5:Qualitative comparison of novel trajectory video synthesis.We compare our method with both reconstruction-based method, Shape-of-motion[78], and generative methods, GCD[75]and ViewCrafter[95]on the multi-view dataset, iphone[23].",
                "position": 265
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05638/x6.png",
                "caption": "Figure 6:Qualitative comparison on in-the-wild monocular videos.We show results of redirecting the camera trajectory as‚Äúzoom-in and orbit to the right‚Äùfrom the input videos, produced by our method and the generative baselines, GCD[75]and ViewCrafter[95].",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2503.05638/x7.png",
                "caption": "Figure 7:Effectiveness of Ref-DiT blocks.We compare our full model (w/ Ref-DiT) to two alternatives: a baseline without Ref-DiT (w/o Ref-DiT), and a variant that directly concatenates the source video with the point cloud renders (w/ Concat Condition).\nThe yellow box highlights the most prominent differences.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2503.05638/x8.png",
                "caption": "Figure 8:Ablation on the training data.We compare our model trained with mixed data to two alternatives: training without multi-view data and training without dynamic data.\nThe yellow box highlights the most prominent differences of occulusions, geometric distortions, and motion consistency.",
                "position": 658
            },
            {
                "img": "https://arxiv.org/html/2503.05638/x9.png",
                "caption": "Figure 9:Failure case.Due to inaccuracies in depth estimation, the generated novel trajectory videos may exhibit physically implausible behavior,e.g., the dog‚Äôs nose appears to pass through the glass of the door.",
                "position": 674
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]