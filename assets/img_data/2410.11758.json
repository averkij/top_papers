[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/x1.png",
                "caption": "Figure 1:Problem Formulation.We investigate building a generalist robotic foundation model from human motion videos without action labels.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LAPA: Latent Action Pretraining for general Action models",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/x2.png",
                "caption": "Figure 2:Overview of LAPA. (1) Latent Action Quantization: We first learn discrete latent actions in a fully unsupervised manner using the VQ-VAE objective. (2) Latent Pretraining: The VLM is trained to predict latent actions, essentially performing behavior cloning. After pretraining, we finetune the LAPA model on a small set of action-labeled trajectories to map the latent space to the end effector delta action space.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/x3.png",
                "caption": "(a)Language Table",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x3.png",
                "caption": "(a)Language Table",
                "position": 354
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x4.png",
                "caption": "(b)Simpler",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x5.png",
                "caption": "(c)Real",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x6.png",
                "caption": "Figure 4:SIMPLER Results.Avg. success rate (%) is shown across 4 tasks. Detailed results are in AppendixE.2.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x7.png",
                "caption": "Figure 5:Real-world Tabletop Manipulation Results.We evaluate on a total of 54 rollouts for each model encompassing unseen object combinations, unseen objects and unseen instructions. Average success rate (%) are shown. We provide detailed results depedning on the generalization type in Table12and individual results in AppendixE.3.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x8.png",
                "caption": "(a)SIMPLER Results",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x8.png",
                "caption": "(a)SIMPLER Results",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x9.png",
                "caption": "(b)Real-world Tabletop Manipulation Robot Results",
                "position": 588
            }
        ]
    },
    {
        "header": "5Ablation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/x10.png",
                "caption": "(a)Model Scaling",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x10.png",
                "caption": "(a)Model Scaling",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x11.png",
                "caption": "(b)Data Scaling (%)",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x12.png",
                "caption": "(c)Latent Action Seq",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x13.png",
                "caption": "(d)Latent Action Vocab",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x14.png",
                "caption": "Figure 8:Latent Action Analysis in Language Table.We condition the current observationx1subscriptùë•1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand quantized latent action to the decoder of the latent action quantization model. We observe that each latent action can be mapped into a semantic action. For example, latent action 0 corresponds to moving a bit left and forward.",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2410.11758/extracted/5929182/figures/langtable_analysis_map.png",
                "caption": "Figure 9:Correlation of latent actions with ground-truth actions.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x15.png",
                "caption": "Figure 10:Latent Action Analysis in Human Manipulation Videos.We condition the current observationx1subscriptùë•1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand quantized latent action to the decoder of the latent action quantization model. We observe that each latent action can be mapped into a semantic action including camera movements. For example, latent action [3,5,2,7] corresponds to moving the camera a bit down while [4,2,0,0] corresponds to moving the camera slightly up.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x16.png",
                "caption": "Figure 11:Latent Action Analysis in Multi-Embodiment Setting.We condition the current observationx1subscriptùë•1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand quantized latent action to the decoder of the latent action quantization model. We observe that each latent action can be mapped into a similar semantic action even though the embodiments are different. For example, latent action [1,1,3,2] corresponds to going down and left while [3,2,0,1] corresponds to going up a little bit.",
                "position": 675
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x17.png",
                "caption": "Figure 12:Closed loop rollout of LAPA.LAPA is conditioned on current imagex1subscriptùë•1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand language instruction of ‚Äòtake the broccoli out of the pot‚Äô. We generate rollout images by conditioning the decoder of Latent Action Quantization Model with latent actions generated by LAPA.",
                "position": 681
            }
        ]
    },
    {
        "header": "6Limitations and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALatent Action Quatization Model Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/x18.png",
                "caption": "Figure 13:Model architecture of our Latent Action Quantization Model.",
                "position": 1550
            }
        ]
    },
    {
        "header": "Appendix BDetails on Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/x19.png",
                "caption": "Figure 14:Real-world Tabletop Manipulation Examples.",
                "position": 1571
            }
        ]
    },
    {
        "header": "Appendix CBaseline Details",
        "images": []
    },
    {
        "header": "Appendix DExperimental Result Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.11758/extracted/5929182/figures/pair_test2.png",
                "caption": "(a)Win rate (%) disregarding ties.",
                "position": 1677
            },
            {
                "img": "https://arxiv.org/html/2410.11758/extracted/5929182/figures/pair_test2.png",
                "caption": "(a)Win rate (%) disregarding ties.",
                "position": 1680
            },
            {
                "img": "https://arxiv.org/html/2410.11758/extracted/5929182/figures/pair_test1.png",
                "caption": "(b)Win rate (%) with ties.",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x20.png",
                "caption": "(a)Latent Action Seq",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x20.png",
                "caption": "(a)Latent Action Seq",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x21.png",
                "caption": "(b)Latent Action Vocab",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2410.11758/x22.png",
                "caption": "Figure 17:Success and Failure Cases ofUniPi.(Top) Given the instruction of ‚Äòmove the green block away from the red cube and red pentagon‚Äô, the diffusion model ofUniPisuccessfully generates the plan. (Bottom) Given the instruction of ‚Äòput the blue moon toward the yellow block‚Äô, the diffusion model fails to generate the correct plan.",
                "position": 1727
            }
        ]
    },
    {
        "header": "Appendix EDetailed Experimental Results",
        "images": []
    }
]