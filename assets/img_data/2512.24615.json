[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24615/figs/logo.png",
                "caption": "",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Youtu-Agent Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig2_autogen.png",
                "caption": "Figure 1:Automated generation mechanism. Left: user input describing the desired agent. Middle: two generation paradigms—Workflow mode (top) follows a deterministic four-stage pipeline, while Meta-Agent mode (bottom) deploys an architect agent with flexible tool access. Right: the generated agent configuration ready for deployment.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig2_2_tfgrpo.png",
                "caption": "Figure 2:Training-free GRPO mechanism. Given only a few dozen of samples, the agent performs multiple rollouts per task. An LLM evaluator assesses group relative trajectory quality, and distilling contextual memory of experiential knowledge by comparing successful and failed trials. During online testing, such learned experiences are injected as “textual LoRA” to guide reasoning.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig_youtu-agent-rl.png",
                "caption": "Figure 3:An end-to-end RL training pipeline with Youtu-Agent. Left: Data flow in the RL training framework[2]. Middle: Connector between RL framework and Agent framework[3]. Right: Data flow in the Youtu-Agent inference framework.",
                "position": 324
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig4_benchmark_webwalkerqa.png",
                "caption": "Figure 4:Performance comparison on WebWalkerQA, including both training-free and trained agent approaches.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2512.24615/x1.png",
                "caption": "Figure 5:During the learning stage of Training-free GRPO, performance improves steadily while tool usage becomes more efficient.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/efficiency-time1.png",
                "caption": "(a)Time of an iteration step.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/efficiency-time1.png",
                "caption": "(a)Time of an iteration step.",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/efficiency-time2.png",
                "caption": "(b)Time of the rollout generation part.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_1_ppo_kl.png",
                "caption": "(a)PPO KL Divergence",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_1_ppo_kl.png",
                "caption": "(a)PPO KL Divergence",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_2_pg_frac.png",
                "caption": "(b)Policy Gradient Clip Fraction",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_3_grad_norm.png",
                "caption": "(c)Gradient Norm",
                "position": 765
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_4_entropy_loss.png",
                "caption": "(d)Entropy Loss",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_5_critic_score.png",
                "caption": "(e)Critic Score",
                "position": 776
            },
            {
                "img": "https://arxiv.org/html/2512.24615/figs/fig5_6_val_acc.png",
                "caption": "(f)Validation Accuracy",
                "position": 781
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Application",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24615/x2.png",
                "caption": "Figure 8:Demonstrations of the Tip application.",
                "position": 877
            }
        ]
    },
    {
        "header": "Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]