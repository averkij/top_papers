[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17481/x1.png",
                "caption": "Figure 1:Learnable N:M sparsity for Large Language Models.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17481/x2.png",
                "caption": "Figure 2:This work introduces learnable semi-structured sparsity for LLMs. MaskLLM models mask selection as a distribution learning problem, enabling the creation of accurate masks through end-to-end training on large-scale datasets. The learned and general mask can be further transferred to downstream tasks or domains, achieving lossless compression.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x3.png",
                "caption": "Figure 3:Drawing a random mask from the learnable distribution with Gumbel Softmax. Each consecutive M parameters are associated with a learnable distribution for candidate masks. All illustrated computations, including Gumbel Softmax, and the weighted averaging are differentiable.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17481/x4.png",
                "caption": "Figure 4:Consumed samples vs. PPL on LLaMA-2 7B. MaskLLM requires 128 samples for the prior and outperforms SparseGPT after 1280 samples.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x5.png",
                "caption": "(a)The mask difference between adjacent steps",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x5.png",
                "caption": "(a)The mask difference between adjacent steps",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x6.png",
                "caption": "(b)The Maximum probability of mask distribution",
                "position": 792
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AImplementation Details",
        "images": []
    },
    {
        "header": "BHyper-parameters",
        "images": []
    },
    {
        "header": "CMask Learning with the C4 Dataset",
        "images": []
    },
    {
        "header": "D2:4 Results on Llama-3 8B",
        "images": []
    },
    {
        "header": "EComparison to More Pruning Methods for LLMs",
        "images": []
    },
    {
        "header": "FSparse Weight Regularization",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17481/x7.png",
                "caption": "(a)Relative norm of pruned weights on GPT-3 2B.",
                "position": 2183
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x7.png",
                "caption": "(a)Relative norm of pruned weights on GPT-3 2B.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x8.png",
                "caption": "(b)Relative norm of pruned weights on LLaMA2 7B.",
                "position": 2191
            }
        ]
    },
    {
        "header": "GLayer Sensitivity",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17481/x9.png",
                "caption": "Figure 7:Layer Sensitivity of LLaMA-2 7B",
                "position": 2208
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x9.png",
                "caption": "Figure 7:Layer Sensitivity of LLaMA-2 7B",
                "position": 2211
            }
        ]
    },
    {
        "header": "HThroughput of 2:4 LLaMA-2 7B.",
        "images": []
    },
    {
        "header": "IMask Difference",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17481/x10.png",
                "caption": "(a)Mask Difference on LLaMA-2 7B with regularization",
                "position": 2343
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x10.png",
                "caption": "(a)Mask Difference on LLaMA-2 7B with regularization",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x11.png",
                "caption": "(b)Mask Difference on GPT-3 2B with regularization",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2409.17481/x12.png",
                "caption": "(c)Mask difference on LLaMA-2 7B without regularization.",
                "position": 2358
            }
        ]
    },
    {
        "header": "JLimitations.",
        "images": []
    },
    {
        "header": "KBroader Impacts.",
        "images": []
    }
]