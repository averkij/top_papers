[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17945/x1.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17945/x2.png",
                "caption": "Figure 2:Left: MARVEL annotation pipeline for 3D assets. Our pipeline starts with human metadata[18,17]and rendered multi-view images to create detailed visual descriptions using InternVL-2[13]. These containobject names,shapes,textures,colors, andenvironments. Qwen2[85]then processes these descriptions into five hierarchical levels, progressively compressing different aspects of the 3D assets.Right: Our Text-to-3D pipeline finetunes SD 3.5[21,3]with this dataset and uses pretrained SF3D[7]to generate a textured mesh in 15s .",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x3.png",
                "caption": "Figure 3:Qualitative Annotation Comparison: From top to bottom Cap3D[53], 3DTopia[28], Kabra[34], MARVEL (Level-4) annotations and GPT-4[60]evaluation. MARVEL consistently provides the most comprehensive and precise annotations, capturing intricate details such as object names, color, structure, and specific attributes.Redis for wrong captions.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17945/x4.png",
                "caption": "Figure 4:Visual results of high fidelity TT3D generation. Left to right, the reconstructed 3D assets from Shap-E[33], DreamFusion[62], Lucid-Dreamer[43], HIFA[91]and MARVEL-FX3D.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x5.png",
                "caption": "Figure 5:MARVEL uses human-generated metadata from source datasets to create detailed, accurate captions (e.g., names of the lunar craters, detection of human footprints) and reduce hallucinations. Without metadata, VLMs like GPT-4[60]and InternVL2[15]generate vague or speculative descriptions.",
                "position": 677
            }
        ]
    },
    {
        "header": "5Limitation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17945/x6.png",
                "caption": "",
                "position": 1905
            }
        ]
    },
    {
        "header": "8Additional Details on Captioning Process",
        "images": []
    },
    {
        "header": "9Additional details on MARVEL annotations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17945/x7.png",
                "caption": "Figure 7:Effect of including human metadata, highlighting improvements in descriptive accuracy and contextual relevance compared to outputs generated without metadata, even when using state-of-the-art models like GPT-4[60]and InternVL2[13]. Metadata inclusion helps reduce hallucinations and enhances domain-specific understanding.",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x8.png",
                "caption": "Figure 8:Failure cases of the MARVEL annotation pipeline. From left to right, the examples illustrate errors such as counting mistakes, object misidentification, and challenges with ambiguous views.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x9.png",
                "caption": "Figure 9:Qualitative comparison of 3D annotations across baselines[53,28,34]and the proposed MARVEL-40404040M+ forautomotive (cars, planes, etc) and CAD models. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted inred, while important captions are highlighted ingreen.",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x10.png",
                "caption": "Figure 10:Qualitative comparison of 3D annotations across baselines[53,28,34]and the proposed MARVEL-40404040M+ forpopular anime, movie, and cartoon characters. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted inred, while important captions are highlighted ingreen.",
                "position": 2037
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x11.png",
                "caption": "Figure 11:Qualitative comparison of 3D annotations across baselines[53,28,34]and the proposed MARVEL-40404040M+ forbiological objects, including animals, plants, and molecular models. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted inred, while important captions are highlighted ingreen.",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x12.png",
                "caption": "Figure 12:Qualitative comparison of 3D annotations across baselines[53,28,34]and the proposed MARVEL-40404040M+ fordiverse items including daily objects, essentials. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted inred, while important captions are highlighted ingreen.",
                "position": 2043
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x13.png",
                "caption": "Figure 13:Qualitative comparison of 3D annotations across baselines[53,28,34]and the proposed MARVEL-40404040M+ forhistorical elements including statues, places, memorials, etc. Incorrect captions are highlighted inred, while important captions are highlighted ingreen.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x14.png",
                "caption": "Figure 14:Qualitative comparison of 3D annotations across baselines[53,28,34]and the proposed MARVEL-40404040M+ fordiverse scenes including digital elevation maps, places, realistic or animated scenes. Incorrect captions are inred, while important captions are ingreen.",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x15.png",
                "caption": "Figure 15:Multi-level annotation examples of MARVEL for the Objaverse[18]dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x16.png",
                "caption": "Figure 16:Multi-level annotation examples of MARVEL for the Omni-Object[80]dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x17.png",
                "caption": "Figure 17:Multi-level annotation examples of MARVEL for the ShapeNet[10]dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2058
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x18.png",
                "caption": "Figure 18:Multi-level annotation examples of MARVEL for the Toys4K dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2061
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x19.png",
                "caption": "Figure 19:Multi-level annotation examples of MARVEL for the ABO (Amazon Berkeley Objects)[16]dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2064
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x20.png",
                "caption": "Figure 20:Multi-level annotation examples of MARVEL for the GSO (Google Scanned Objects)[20]dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2067
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x21.png",
                "caption": "Figure 21:Multi-level annotation examples of MARVEL for the Pix3D[74]dataset. Words corresponding toObject and Componentsare highlighted in violet,Shape and Geometryin green,Texture and Materialsin orange,Colorsin blue, andContextual Environmentin purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions.",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x22.png",
                "caption": "Figure 22:Qualitative Results for high fidelity TT3D generation on unseen prompts. From left to right, 3D models generated using Shap-E[33], DreamFusion[62], LucidDreamer[43], HiFA[91]and MARVEL-FX3D (ours).",
                "position": 2073
            },
            {
                "img": "https://arxiv.org/html/2411.17945/x23.png",
                "caption": "Figure 23:Qualitative Results for high fidelity TT3D generation on unseen prompts. From left to right, 3D models generated using Shap-E[33], DreamFusion[62], LucidDreamer[43], HiFA[91]and MARVEL-FX3D (ours).",
                "position": 2076
            }
        ]
    },
    {
        "header": "10Additional results of MARVEL-FX3D",
        "images": []
    }
]