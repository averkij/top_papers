[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10716/x1.png",
                "caption": "Figure 1:Our self-supervisedWildRayZerlearns to render static novel views fromdynamicimages without any 3D or GT mask supervision. It extends the state-of-the-art self-supervised large view synthesis model RayZer to dynamic environments by adding a learned motion mask estimator and a masked 3D scene encoder.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dynamic RealEstate10K",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10716/x2.png",
                "caption": "Figure 2:WildRayZer self-supervised learning framework.(a) Training.WildRayZer takes unposed, uncalibrated multi-viewdynamicimagesℐ\\mathcal{I}and predicts per-view camera parameters (intrinsics and relative poses), which are converted into pixel-aligned Plücker ray mapsℛ\\mathcal{R}. A camera-only static renderer explains the rigid background; residuals between renderingsℐ^B\\hat{\\mathcal{I}}_{B}and targetsℐB\\mathcal{I}_{B}highlight dynamic regions, which are sharpened by our pseudo-motion mask constructor (seeSection4.2andFigure3). We distill a motion estimator from these pseudo-masks and use it to gate dynamic image tokens before scene encoding; the same pseudo-masks also gate dynamic pixels in the photometric rendering loss.(b) Inference.Given dynamic input viewsℐ\\mathcal{I}, the model predicts camera parameters, motion masks, and a static scene representation in a single feed-forward pass. The motion estimator operates on the input views to mask dynamic tokens, and the renderer synthesizes transient-free novel views given the inferred scene representation and a target camera.",
                "position": 449
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10716/x3.png",
                "caption": "Figure 3:Pseudo Motion Mask Pipeline.We fuse SSIM- and DINO-based dissimilarity into a saliency map, cluster DINO patch features to vote for dynamic patches, then refine the coarse patch mask to pixel resolution via morphological smoothing, small-component removal, and GrabCut[65].",
                "position": 518
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10716/x4.png",
                "caption": "Figure 4:Qualitative Comparisons.Qualitative results on DRE10K-Mask (top two rows) and DRE10K-iPhone (bottom row). Compared to baselines, our method (1) more cleanly removes transient objects, (2) better handles cross-view completion (compare with RayZer + SAV baseline), and (3) better preserves global scene geometry (e.g., kitchens) and fine details (e.g., plants). SLS denotes Splotless-Splats[66].",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2601.10716/x5.png",
                "caption": "Figure 5:Qualitative Results.(1) First row: D-RE10K (no ground-truth novel views). (2) Second row: D-RE10K-iPhone. (3) Third and fourth rows: additional NVS results on DAVIS[54], where ground truth is also unavailable, demonstrating that WildRayZer generalizes to outdoor scenes and can mask unseen transient objects.",
                "position": 1605
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10716/x6.png",
                "caption": "Figure 6:Examples of Copy–paste mask augmentation.We inject synthetic transient objects (e.g., animals, household items, vehicles) into static RE10K scenes\nto simulate dynamic elements in otherwise static environments.",
                "position": 3313
            }
        ]
    },
    {
        "header": "Appendix BBenchmarking Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10716/x7.png",
                "caption": "Figure 7:Motion Masks Comparisons.We present motion masks comparisons on D-RE10K (row 1) and D-RE10K-iPhone (row 2) across Co-segmentation[1], MegaSAM[36], Segment Any Video[21], VideoCutler[84]and WildRayzer’s motion mask predictions.",
                "position": 3402
            },
            {
                "img": "https://arxiv.org/html/2601.10716/x8.png",
                "caption": "Figure 8:Failure Cases.Each row contains one failure case of WildRayzer. Motion mask only highlights moving parts dispite highlight television, it only highlights part of human body in row 1. Moreover, motion mask may miss small places such as feet in row 2. Row 3 and 4 show failure case when transient object is too big and predicted masks are smaller.",
                "position": 3430
            },
            {
                "img": "https://arxiv.org/html/2601.10716/x9.png",
                "caption": "Figure 9:Additional qualitative results.We show 12 extra examples to illustrate WildRayZer’s behavior across datasets.\nThe first three rows are from D-RE10K, the next two rows demonstrate generalization to the unseen DAVIS dataset[55], and the last row shows additional real-world results on D-RE10K-iPhone.",
                "position": 3443
            }
        ]
    },
    {
        "header": "Appendix CAdditional Visualizations",
        "images": []
    }
]