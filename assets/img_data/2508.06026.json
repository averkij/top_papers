[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06026/x1.png",
                "caption": "Figure 1:Comparison of response preference dynamics between Self-Rewarding and Temporal Self-Rewarding (Temporal SR) frameworks across iterations. We track: (1) the score difference (chosen - rejected) evaluated by GPT-4o-mini (the scoring prompt is the same as that used in Temporal Self-Rewarding, detailed in Appendix A) and (2) similarity between chosen and rejected responses(cosin similarity calculated through the last layerâ€™s features). With similar computational budgets (4 vs. 2 iterations), Self-Rewarding shows rapid degradation with score gap shrinking 9 times and rapid similarity improvement between chosen and rejected responses of Llama3.1-8B, indicating a progressive narrowing of the quality gap between chosen and rejected samples. Our Temporal approach effectively mitigates this quality convergence.",
                "position": 125
            }
        ]
    },
    {
        "header": "Methodology",
        "images": []
    },
    {
        "header": "Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06026/x2.png",
                "caption": "Figure 2:The performance of Self-Rewarding and Temporal Self-Rewarding(Temporal SR) using different Judge, evaluated by AlpacaEval 2.0 Win Rate and Arena-Hard-v0.1 Score. Base model is Llama3.1-8B. This figure illustrates the best model of all iterations in each baseline, detailed results of all iterations can be seen in Appendix E.",
                "position": 861
            }
        ]
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06026/x3.png",
                "caption": "Figure 3:User Prompt of Judging in Self-Rewarding paradigm.",
                "position": 1122
            },
            {
                "img": "https://arxiv.org/html/2508.06026/x4.png",
                "caption": "Figure 4:System Prompt of Judging in Self-Rewarding paradigm.",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2508.06026/x5.png",
                "caption": "Figure 5:AlpacaEval win rate breakdown for instruction categories of Qwen2.5-7B and Mistral-7B. Temporal Self-Rewarding models give gains across nearly all topics than Self-Rewarding and SFT intial.",
                "position": 1997
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]