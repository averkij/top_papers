[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07232/x1.png",
                "caption": "Figure 1:Given an input image (left in each pair), either real (top row) or generated (mid row), along with a simple textual prompt describing an object to be addedAdd-itseamlessly adds the object to the image in a natural way.Add-itallows the step-by-step creation of complex scenes without the need for optimization or pre-training.",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07232/x2.png",
                "caption": "Figure 2:Architecture outline:Given a tuple of source noiseXs⁢o⁢u⁢r⁢c⁢eTsuperscriptsubscript𝑋𝑠𝑜𝑢𝑟𝑐𝑒𝑇X_{source}^{T}italic_X start_POSTSUBSCRIPT italic_s italic_o italic_u italic_r italic_c italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, target noiseXt⁢a⁢r⁢g⁢e⁢tTsuperscriptsubscript𝑋𝑡𝑎𝑟𝑔𝑒𝑡𝑇X_{target}^{T}italic_X start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, and a text promptPt⁢a⁢r⁢g⁢e⁢tsubscript𝑃𝑡𝑎𝑟𝑔𝑒𝑡P_{target}italic_P start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT, we first apply Structure Transfer to inject the source image’s structure into the target image. We then extend the self-attention blocks so thatXt⁢a⁢r⁢g⁢e⁢tTsuperscriptsubscript𝑋𝑡𝑎𝑟𝑔𝑒𝑡𝑇X_{target}^{T}italic_X start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTpulls keys and values from bothPt⁢a⁢r⁢g⁢e⁢tsubscript𝑃𝑡𝑎𝑟𝑔𝑒𝑡P_{target}italic_P start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPTandXs⁢o⁢u⁢r⁢c⁢eTsuperscriptsubscript𝑋𝑠𝑜𝑢𝑟𝑐𝑒𝑇X_{source}^{T}italic_X start_POSTSUBSCRIPT italic_s italic_o italic_u italic_r italic_c italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, with each source weighted separately. Finally, we use Subject Guided Latent Blending to retain fine details from the source image.",
                "position": 167
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07232/x3.png",
                "caption": "Figure 3:User Study results evaluated on the real images\nfrom the Emu Edit Benchmark.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x3.png",
                "caption": "Figure 3:User Study results evaluated on the real images\nfrom the Emu Edit Benchmark.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x4.png",
                "caption": "Figure 4:User Study results evaluated on the generated images from the ImageAdditingBenchmark.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x5.png",
                "caption": "Figure 5:Qualitative Results from the Emu-Edit Benchmark. Unlike other methods, which fail to place the object in a plausible location, our method successfully achieves realistic object insertion.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x6.png",
                "caption": "Figure 6:Qualitative Results from theAdditingBenchmark. While Prompt-to-Prompt fails to align with the source image, and SDEdit fails to align with the prompt, our method offersAdditingthat adheres to both prompt and source image.",
                "position": 654
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07232/x7.png",
                "caption": "Figure 7:(A)Affordance and Object Inclusion scores across weight scale values, with our automatic weight scale achieving a good balance between the two.(B)Visualization of the prompt token attention spread across different sources, model blocks, and weight scales, averaged over multiple examples from a small validation set.(C)A representative example demonstrating the effect of varying target weight scales.",
                "position": 699
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x8.png",
                "caption": "Figure 8:Ablation over various steps for applying the Structure Transfer mechanism. Applying it too early misaligns the generated images with the source image’s structure while applying it too late causes the output image to neglect the object. Our chosen step strikes a balance between both.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x9.png",
                "caption": "Figure 9:Images generated byAdd-itwith and without the latent blending step, along with the resulting affordance map. The latent blending block helps align fine details from the source image, such as removing the girl’s glasses or adjusting the shadows of the bicycles.",
                "position": 705
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07232/x10.png",
                "caption": "Figure 10:Add-itmay fail to add a subject that already exists in the source image. When prompted to add another dog to the image,Add-itgenerates the same dog instead, though it successfully adds a person behind the dog.",
                "position": 734
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.07232/x11.png",
                "caption": "Figure 11:Step-by-Step Generation:Add-itcan generate images incrementally, allowing it to better adapt to user preferences at each step.",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x12.png",
                "caption": "Figure 12:Qualitative results of our method on theAdditingAffordance Benchmark show that our method successfully adds objects naturally and in plausible locations.",
                "position": 1394
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x13.png",
                "caption": "Figure 13:Our method can operate on non-photorealistic images.",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x14.png",
                "caption": "Figure 14:Our method generates different outputs when given different starting noises. All the outputs remain plausible.",
                "position": 1400
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x15.png",
                "caption": "Figure 15:Positional Encoding Analysis: shifting the positional encoding of the source image results in a corresponding shift in the object’s location in the generated image.",
                "position": 1403
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x16.png",
                "caption": "Figure 16:Failure cases:Add-itmay fail generating the added object in the right location (sunglasses), it can be biased to replace existing object in the scene (Pikachu) and it can struggle with complicated scenes (woman cooking).",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x17.png",
                "caption": "Figure 17:Visual examples from theAdditingAffordance Benchmark. Each image is annotated with bounding boxes highlighting the plausible areas where the object can be added.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2411.07232/x18.png",
                "caption": "Figure 19:One trial of theAdditinguser study.",
                "position": 1470
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]