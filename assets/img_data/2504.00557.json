[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00557/x1.png",
                "caption": "Figure 1:Comparison of LVLM architectures.(a) Self-attention-only models process both image and text embeddings in all attention layers. (b) Cross-attention-based models use image features exclusively for KV operations in cross-attention layers, enabling efficient multimodal integration.",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x2.png",
                "caption": "Figure 2:Proposed method.Image features are pruned in the first cross-attention block using a criterion derived from attention weights. The features serve as inputs for the keys and values in subsequent cross-attention layers, with the compressed keys and values stored in the KV cache (blue-shaded area).",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x3.png",
                "caption": "Figure 3:KV cache memory.(a) As batch size increases, the KV cache volume from image features grows. (b) As the language token count grows, the KV cache size in cross-attention still dominates that of self-attention, up to a certain number of tokens.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Cross-attention Redundancy",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00557/x4.png",
                "caption": "(a)Attention weights at the first cross-attention layer (x-axis: index of image features).",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x4.png",
                "caption": "(a)Attention weights at the first cross-attention layer (x-axis: index of image features).",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x5.png",
                "caption": "(b)Cross-attention weight patterns across different layers (x-axis: index of image features; y-axis: index of text query features).",
                "position": 148
            }
        ]
    },
    {
        "header": "3Trimming Visual Features in Cross-Attention-Based LVLMs",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00557/x6.png",
                "caption": "Figure 5:Results under different compression ratios.Even with up to 50% reduction of visual features, our method retains the performance of the original model.",
                "position": 916
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x7.png",
                "caption": "Figure 6:Visualization of compression.Purple patches indicate features trimmed by our method.",
                "position": 919
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00557/x8.png",
                "caption": "Figure 7:Additional results of cross-attention weights.(Left) Images utilized for the extraction of attention weights. (Right) Cross-attention weight patterns of different layers from corresponding image (x-axis: the index of image features; y-axis: the index of text query features).",
                "position": 1334
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x9.png",
                "caption": "Figure 8:Visualization of our token pruning algorithm.The 2D grid represents image token indices (x-axis) and attention heads (y-axis). The final compression ratio is determined by the fraction of tokens not selected as salient.",
                "position": 1337
            },
            {
                "img": "https://arxiv.org/html/2504.00557/x10.png",
                "caption": "Figure 9:Visualization of computational cost reduction.The heatmap illustrating the theoretical FLOPs reduction ratio is presented with varying KV cache budgets and input sequence lengths.",
                "position": 1350
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]