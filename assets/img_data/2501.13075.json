[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13075/x1.png",
                "caption": "Figure 1.Interlocking principles enabling evolution’s robustness to Knightian Uncertainty.(a) Evolution happens within a search space that is open-ended enough such that a vast array of complex adaptations can be encoded, e.g. the human brain, multicellularity, developmental systems as a whole, and photosynthesis. (b) Diversification pressure in evolution continually creates new behaviors and adaptations from the set of open-ended possibilities, which implicitly can be seen as bets about how the organism and its lineage can persist into the future. (c) Because organisms form part of the environment of other organisms, novel behaviors and adaptations in one lineage create novel unforeseen situations for other organisms as an externality, e.g. the high branches of a tree provide a novel situation a giraffe can exploit. (d) Organisms unable to persist across the uncertainty created by other organisms are filtered away, in effect invalidating their bets about how to persist through KU; the image shows a coelacanth, a fish that has persisted for 400 million years. In concert, these factors can be seen as a form of open-ended generation and falsification of bets about how to deal with KU. We believe there may be ways to adapt these principles to ML research (see discussion in Section5).",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2501.13075/extracted/6148994/images/04png.png",
                "caption": "Figure 2.Two Strategies for Dealing with an Open World.This figure describes two possible strategies for coping with an open changing world. In (a)diversify-and-filter, a process continually refreshes and adapts its diverse hypotheses about how to persist through the open-ended future. Such hypotheses are filtered through empirical success at tackling later unanticipated problems. Evolution, market competition, and science can be seen to largely operate through this paradigm. There is no explicit formalism, although robustness implicitly relies on the Lindy effect(Taleb,2014), i.e. an adaptable solution long-tested by time is more likely than an untested one to persist yet longer.\nIn (b)anticipate-and-train, diverse problems are first collected, and augmented through human anticipation about what novel situations might later arise. Then, a single policy is trained to solve the problems to convergence, and that policy is then deployed into a changing world. Much of current ML adopts this paradigm; although the closed-world formalism adopted in training mismatches the open world it is deployed into, the hope is that generalization will enable sufficient robustness to unforeseen challenges. One conclusion is that nothing precludes machine learning from more deeply integrating diversify-and-filter approaches into its methods(Lehman and Stanley,2010; Kumar et al.,2020; Jaderberg et al.,2017; Lee et al.,2023). Another conclusion is that diversify-and-filter leverages the temporal structure of when novel problems arise, and forces agents to directly grapple with the issue of KU (if they do not, they are discarded).",
                "position": 261
            }
        ]
    },
    {
        "header": "2.Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13075/extracted/6148994/images/03png.png",
                "caption": "Figure 3.Optimizing for known unknowns can exacerbate risk from Knightian uncertainty.An optimization formalism that makes closed-world assumptions will indeed improve an agent’s performance on the situations an experimenter anticipates. However, if such aclosed-worldoptimizer aggressively trains anopen-worldagent, the agent may perversely becomemore brittleto Knightian uncertainty, as it is incentivized to internalize the closed-world assumptions as true.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2501.13075/extracted/6148994/images/02png.png",
                "caption": "Figure 4.Neural network generalization is not a general cure for Knightian Uncertainty.Imagine as part of a larger reinforcement learning policy, an agent decides whether to eat certain mushrooms, which can either be deadly or edible, and can be separated through features learned in training that correspond to the cap size of the mushroom and its thickness. (a) In a closed world, it is safe to assume that the distribution of mushrooms encountered during training (red×\\times×’s and green+++’s) reflects that encountered during testing, and the (b) NN decision boundary on whether to eat or not eat the mushroom learned through training will likely reflect this assumption. However, in an open world, not all mushroom varieties are known, the policy might be deployed in a slightly different ecosystem, or a new variety of mushroom might evolve or be bred. If encountering the unanticipated mushroom (question mark symbol) at the center of (a), it is likely rational for an open-world agent to forgo eating it, given its novelty and the risk of death. The claim is that simple generalization from what is known does not address Knightian uncertainty.",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2501.13075/extracted/6148994/images/05png.png",
                "caption": "Figure 5.Typical metalearning setups do not incentivize learning how to solve unforeseen tasks.This figure offers a caricature of optimal behavior under a typical meta-RL formalism, where an agent is trained across a fixed distribution of problems; this setup is similar to e.g.(Duan et al.,2016). In meta-RL, it is common for an agent to be exposed many times to training tasks covering all major necessary task-relevant skills. Thus the agent is incentivized to learnin trainingall qualitative skills needed to solve the tasks; after many iterations of training, there need be no significant remaining surprise for the agent when solving new tasks drawn from an IID test distribution (which is the formal goal of the algorithm). At completion of training, an optimal agent’s behavior is sketched as: (1) it encounters a task drawn from the IID test distribution, which is ambiguous (as this characterizes the need for metalearning); (2) the agent takes actions that optimally disambiguate the sampled task; and (3) having identified the task, which it has encountered many similar variants of before in training, the agent executes its previously-learned optimal solution. In practice, optimal behavior will entail mixing steps (2) and (3) together, but nowhere in this process does optimality under the formalism require the generalized ability to learn how to learn. The conclusion is that if then deployed into a changing world where it encounters an unknown unknown, the agent may struggle to handle it gracefully.",
                "position": 389
            }
        ]
    },
    {
        "header": "3.Biological Evolution and Knightian Uncertainty",
        "images": []
    },
    {
        "header": "4.Reinforcement Learning’s Formalisms Limit Robustness to Knightian Uncertainty",
        "images": []
    },
    {
        "header": "5.Discussion",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]