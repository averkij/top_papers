[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11858/x1.png",
                "caption": "Figure 1:Average scores across multimodal perception benchmarks.\nZwZ-4B/7B/8B demonstrate competitive performance compared with current SOTA MLLMs (e.g., Gemini-3-Flash, Kimi-K2.5, Qwen3-VL-235B).",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x2.png",
                "caption": "Figure 2:Zooming without Zooming.“Thinking-with-Images” models rely on iterative tool-based cropping and re-encoding at inference, incurring high latency. OurRegion-to-Image Distillationperforms zooming only during training to synthesize region-grounded supervision on the full image, enabling single-pass fine-grained perception without test-time tool use.",
                "position": 166
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Zooming without Zooming",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11858/x3.png",
                "caption": "Figure 3:Overview ofRegion-to-Image Distillation. We synthesize fine-grained VQA pairs on zoomed-in micro-crops using strong teachers with consensus filtering, then distill them to the full image via box-overlay grounding and an augmented prompt, enabling improved single-pass inference without test-time zooming.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x4.png",
                "caption": "Figure 4:Category distribution across six fine-grained dimensions of our benchmark (left) andZoomBenchdata statistics: distribution of image resolutions (middle) and crop-to-image area ratios (right).",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x5.png",
                "caption": "",
                "position": 481
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11858/x6.png",
                "caption": "Table 4:We compare our models (single forward pass) with agentic models on several perception benchmarks. The best results are highlighted inbold, and the second-best areunderlined.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x6.png",
                "caption": "Figure 5:Benchmark accuracy versus inference speed. Our models achieve higher accuracy than base models and agentic baselines while retaining single-pass efficiency.",
                "position": 1227
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x7.png",
                "caption": "Figure 6:Generalization to real-world tasks. ZwZ-8B consistently improves over Qwen3-VL-8B on AIGC detection and GUI agent benchmarks.",
                "position": 1432
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x7.png",
                "caption": "Figure 6:Generalization to real-world tasks. ZwZ-8B consistently improves over Qwen3-VL-8B on AIGC detection and GUI agent benchmarks.",
                "position": 1435
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x8.png",
                "caption": "Figure 7:Dual-view evaluation. ZwZ-8B exhibits strong performance on both “Global-View” and “Regional-View” with the smallest zooming gap.",
                "position": 1440
            }
        ]
    },
    {
        "header": "5Deeper Analysis on ZoomBench",
        "images": []
    },
    {
        "header": "6Discussion and Future Direction",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Implementation Details of Our Method",
        "images": []
    },
    {
        "header": "9Implementation Details of Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11858/x9.png",
                "caption": "Figure 8:The training reward of Qwen3-VL-8B.",
                "position": 4064
            }
        ]
    },
    {
        "header": "10Prompts.",
        "images": []
    },
    {
        "header": "11Case Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11858/x10.png",
                "caption": "",
                "position": 4180
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x11.png",
                "caption": "",
                "position": 4200
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x12.png",
                "caption": "",
                "position": 4220
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x13.png",
                "caption": "",
                "position": 4247
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x14.png",
                "caption": "",
                "position": 4267
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x15.png",
                "caption": "",
                "position": 4287
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x16.png",
                "caption": "",
                "position": 4307
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x17.png",
                "caption": "",
                "position": 4327
            },
            {
                "img": "https://arxiv.org/html/2602.11858/x18.png",
                "caption": "",
                "position": 4347
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0002_original.png",
                "caption": "",
                "position": 4505
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0002_layer24_basemodel.png",
                "caption": "",
                "position": 4508
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0002_layer24.png",
                "caption": "",
                "position": 4511
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0010_original.png",
                "caption": "",
                "position": 4546
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0010_layer24_basemodel.png",
                "caption": "",
                "position": 4549
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0010_layer24.png",
                "caption": "",
                "position": 4552
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0012_original.png",
                "caption": "",
                "position": 4587
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0012_layer24_basemodel.png",
                "caption": "",
                "position": 4590
            },
            {
                "img": "https://arxiv.org/html/2602.11858/figs/sample_0012_layer24.png",
                "caption": "",
                "position": 4593
            }
        ]
    },
    {
        "header": "12Relative Attention Map Computation",
        "images": []
    }
]