[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00994/x1.png",
                "caption": "Figure 1:Experimental setup overview and key results on sequence classification (610M model size, 40% MLM ratio). The upper plot shows downstream performance as a function of the CLMâ€“to-MLM step ratio during pretraining (configurations outperforming MLM-only are highlighted in yellow). The lower plot illustrates the effect of applying MLM CPT to models initially trained with either MLM or CLM (Base).",
                "position": 123
            }
        ]
    },
    {
        "header": "2Experimental Setup",
        "images": []
    },
    {
        "header": "3Pretraining with CLM or MLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00994/x2.png",
                "caption": "Figure 2:MLM vs. CLM downstream performance, averaged across tasks and reported for all model sizes. For MLM, results correspond to a 40% masking ratio.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2507.00994/x3.png",
                "caption": "Figure 3:Task-wise downstream performance across different masking ratios for all model sizes.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2507.00994/x4.png",
                "caption": "Figure 4:Downstream performance as a function of pretraining steps for CLM and MLM objectives. Results are reported for 610M models, with a 40% masking ratio for MLM.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2507.00994/x5.png",
                "caption": "Figure 5:Impact of the fine-tuning learning rate on MLM- vs. CLM-pretrained models. Error bars indicate the standard deviation of metric scores across all seeds and learning rates between 1e-5 and 1e-4. Results are shown for 610M-parameter models, with a 40% masking ratio for MLM.",
                "position": 334
            }
        ]
    },
    {
        "header": "4Two-Stage CLM+MLM Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00994/x6.png",
                "caption": "Figure 6:Impact of two-stage CLM+MLM pretraining on downstream performance under different training budgets (12,000, 22,000, and 42,000 steps). The x-axis shows the percentage of CLM steps allocated in the first phase. Experiments are conducted on 610M models, with a 40% masking ratio during MLM training.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2507.00994/x7.png",
                "caption": "Figure 7:Comparison of downstream performance variability across different masking ratios (20%, 30%, 40%, 50%) for CLM and CLM+MLM pretraining configurations. Error bars indicate the standard deviation across fine-tuning seeds and masking ratios. Results are reported for 610M models, using 42,000 training steps for MLM and a schedule of 40,000 CLM steps followed by 2,000 MLM steps for CLM+MLM.",
                "position": 377
            }
        ]
    },
    {
        "header": "5Continued Pretraining from CLM and MLM Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00994/x8.png",
                "caption": "Figure 8:Impact of performing MLM CPT on either CLM- or MLM-pretrained models (denoted as Base). CPT is conducted for 22,000 steps on 610M models with a 40% masking ratio, following 42,000 steps of initial pretraining.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2507.00994/x9.png",
                "caption": "Figure 9:MLM loss curves for CLM- and MLM-pretrained models across the 3 CPT compute budgets (2,000, 12,000, and 22,000 steps). Results are reported for 610M models, with MLM using a 40% masking ratio.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2507.00994/x10.png",
                "caption": "Figure 10:Downstream performance as a function of CPT length for CLM- and MLM-pretrained models, reported for settings of 2,000, 12,000, and 22,000 training steps. Experiments use 610M models with a 40% MLM ratio.",
                "position": 405
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Setup Details",
        "images": []
    },
    {
        "header": "Appendix BDetails on Evaluation Datasets",
        "images": []
    },
    {
        "header": "Appendix CDetailed Results",
        "images": []
    }
]