[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17115/x1.png",
                "caption": "",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x2.png",
                "caption": "",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x3.png",
                "caption": "Figure 1:Training FLOPs v.s. average downstream performance. Although these corpora have gone through expert-crafted rules, applyingProXstill yields significant improvements over these baseline models trained with original data corpus. Moreover, with much less training FLOPs,\nmodel trained onProXcurated data show comparable performance with existing models.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x4.png",
                "caption": "",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x5.png",
                "caption": "Figure 2:An overview ofProXframework: (1) we adapt a base language model to perform data refinement; (2)ProXrefining models are able to generate complex programs for each document, including document level filtering and more fine-grained chunk level refining; (3) APython executor will execute the programs with the docs, producing the refined high-quality corpora.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2409.17115/extracted/5876943/pics/python.png",
                "caption": "",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x6.png",
                "caption": "Figure 3:The illustration of the model adaptation inProX. We employ powerful LLMs (Llama-3) to annotate random seed documents with valid programs, and use thisdoc-programpairs to fine-tune a small base model, obtaining the refining model suitable for fine-grained data refining tasks.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x7.png",
                "caption": "Figure 4:Downstream zero-shot performance w.r.t. different training steps: first0.50.50.50.5K, then evenly from2.52.52.52.5K to12.512.512.512.5K. Rule: the best performingFineWebrule in Table2.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x7.png",
                "caption": "Figure 4:Downstream zero-shot performance w.r.t. different training steps: first0.50.50.50.5K, then evenly from2.52.52.52.5K to12.512.512.512.5K. Rule: the best performingFineWebrule in Table2.",
                "position": 812
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x8.png",
                "caption": "Table 4:Refining model’s performance on valid set and token retention ratio of original corpus.",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x8.png",
                "caption": "Table 4:Refining model’s performance on valid set and token retention ratio of original corpus.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x9.png",
                "caption": "Figure 6:Performance of original data andProXcurated data trained models across different datasets using≈50absent50\\approx 50≈ 50B tokens and comparison with existing models trained using different techniques.\nInst-LM representsInstructionLM-1.3B; Cosmo representsCosmo-1.8B; S-Llama representsShearedLlama-1.3B.",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x10.png",
                "caption": "RedPajama-V2",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x10.png",
                "caption": "RedPajama-V2",
                "position": 1533
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x11.png",
                "caption": "C4",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x12.png",
                "caption": "FineWeb",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x13.png",
                "caption": "OpenWebMath",
                "position": 1548
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x14.png",
                "caption": "Figure 8:FLOPs comparison for comparable downstream performance with/withoutProXrefining: 0.3B(Avg.Perf = 40.5), 0.7B (41.6), and 1.7B (42.9).222The train FLOPs for the base model (approximately5.3×10195.3superscript10195.3\\times 10^{19}5.3 × 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT) used to create the refining model are excluded. This is because any pre-trained LLM can theoretically serve as the base for refinement. This also reflectsProX’s flexibility.",
                "position": 1566
            },
            {
                "img": "https://arxiv.org/html/2409.17115/x15.png",
                "caption": "Figure 13:Visualization of dynamic performance on ten benchmarks.",
                "position": 5139
            }
        ]
    },
    {
        "header": "",
        "images": []
    }
]