[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12992/x1.png",
                "caption": "Figure 1:Pass@1 accuracy versus maximum token budget for DeepSeek-R1-Distill-Qwen-1.5B (1st row) and 7B (2nd row) on reasoning benchmarks. Solid blue lines show the original full chain-of-thought (CoT) sampling, while dashed orange lines show our truncated CoT + response approach. Across all benchmarks, truncating the CoT (and generating the final answer) achieves equal or better accuracy with substantially fewer tokens, demonstrating that full CoT is unnecessary and that truncating CoT can save a large amount of computation without sacrificing performance.",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x2.png",
                "caption": "Figure 2:(a) Comparison of sampling strategies for reasoning LLMs.\nTop: Sampling Trajectories‚Äìmultiple complete reasoning chains are sampled independently from the model.\nMiddle: Sampling Responses‚Äìa single reasoning chain is used to generate diverse final responses.\nBottom: Fractured Sampling‚Äìour proposed method samples across both multiple reasoning trajectories and intermediate reasoning steps, enabling fine-grained control over diversity and computation. (b) Token statistics across tasks and models. Bars represent the average token count per sample, broken down into reasoning steps (blue) and final solutions (orange). The thinking process dominates the overall cost.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x3.png",
                "caption": "",
                "position": 161
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Fractured sampling for long chain-of-thought reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12992/x4.png",
                "caption": "Figure 3:Correlation matrices of binary failure indicators across intermediate reasoning depths (positionsHùêªHitalic_H) under fractured sampling for five benchmarks. Each cell shows the Pearson correlation coefficient between failure events at two depth positions; green denotes positively correlated failures (synchronized error modes), while pink denotes negatively correlated failures (diverse error modes) that fractured sampling exploits to boost overall success.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x5.png",
                "caption": "",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x6.png",
                "caption": "",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x7.png",
                "caption": "",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x8.png",
                "caption": "",
                "position": 448
            }
        ]
    },
    {
        "header": "4Empirical results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12992/x9.png",
                "caption": "Figure 4:Pass@‚Å¢kPass@ùëò\\text{Pass@}kPass@ italic_kperformance versus total token budget for three sampling schemes on five benchmarks. In each subplot, we compare:m(green dotted)‚Äìsampling only the final solution;n(blue solid)‚Äìsampling full reasoning trajectories;H(orange dashed)‚Äìfractured sampling across all intermediate steps.\nRows correspond to DeepSeek-R1-Distill-Qwen-1.5B, 7B and DeepSeek-R1 models.\nFractured sampling (H) consistently yields higherpass@‚Å¢kpass@ùëò\\text{pass@}kpass@ italic_kat a given token budget. Please refer to FigureB.1for DeepScaleR and Qwen3 with a similar pattern.",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x10.png",
                "caption": "Figure 5:Pass@‚Å¢kPass@ùëò\\text{Pass@}kPass@ italic_kperformance versus total token budget for four sampling schemes on five benchmarks. In each subplot, we compare:H=1, m=1‚Äìonly sampling full reasoning trajectory;H=1, m=4‚Äìsampling both full reasoning trajectories and the final solution;H=16, m=1‚Äìboth full reasoning trajectory sampling and fractured sampling across all intermediate steps;H=16, m=4‚Äìsampling all three dimensions.nùëõnitalic_nis in [1, 2, 4, 8, 16] for the five points (from left to right) on each line. Rows correspond to DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1 models. Please refer to FigureB.2for DeepScaleR and Qwen3 with a similar pattern.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x11.png",
                "caption": "Figure 6:Accuracy versus the position of fractured CoT. We split the whole reasoning trajectory into 16 intermediate steps. We can observe: (1) Even with a116116\\frac{1}{16}divide start_ARG 1 end_ARG start_ARG 16 end_ARGreasoning trajectory, the accuracy is still decent, especially for GPQA; (2) More reasoning tokens lead to higher accuracy.",
                "position": 719
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of the Diversity Lower Bound",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12992/x12.png",
                "caption": "Figure B.1:Pass@‚Å¢kPass@ùëò\\text{Pass@}kPass@ italic_kperformance versus total token budget for three sampling schemes on five benchmarks. In each subplot, we compare:m(green dotted)‚Äìsampling only the final solution;n(blue solid)‚Äìsampling full reasoning trajectories;H(orange dashed)‚Äìfractured sampling across all intermediate steps.\nRows correspond to DeepScaleR-1.5B-Preview and Qwen3-1.7B models models.\nFractured sampling (H) consistently yields higherpass@‚Å¢kpass@ùëò\\text{pass@}kpass@ italic_kat a given token budget.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2505.12992/x13.png",
                "caption": "Figure B.2:Pass@‚Å¢kPass@ùëò\\text{Pass@}kPass@ italic_kperformance versus total token budget for four sampling schemes on five benchmarks. In each subplot, we compare:H=1, m=1‚Äìonly sampling full reasoning trajectory;H=1, m=4‚Äìsampling both full reasoning trajectories and the final solution;H=16, m=1‚Äìboth full reasoning trajectory sampling and fractured sampling across all intermediate steps;H=16, m=4‚Äìsampling all three dimensions.nùëõnitalic_nis in [1, 2, 4, 8, 16] for the five points (from left to right) on each line. Rows correspond to DeepScaleR-1.5B-Preview and Qwen3-1.7B models. MATH500 L5 is saturated here, resulting in a less efficient gain from dimensionsHùêªHitalic_Handmùëömitalic_m.",
                "position": 1697
            }
        ]
    },
    {
        "header": "Appendix BMore results",
        "images": []
    }
]