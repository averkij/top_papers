[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13884/images/finerweb_approach.png",
                "caption": "Figure 1:Three-stage pipeline for constructingFiNERweb: We (1) use a multilingual LLM to generate preference data for identifying high-quality NER examples, (2) train a regression model to filter the FineWeb2 corpus, and (3) annotate the filtered passages with a multilingual LLM.",
                "position": 97
            }
        ]
    },
    {
        "header": "2FiNERweb",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13884/images/preference_classifier_cm.png",
                "caption": "Figure 2:Confusion matrices for regression models reported inTable˜1showing the distribution of prediction errors. GPT-4o mini annotations yield more balanced predictions along the diagonal, whereas Gemma3 annotations rarely give the highest score. Most errors occur close to the diagonal, indicating models avoid severe misclassifications.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2512.13884/images/typescript_stacked_normalized_horizontal.png",
                "caption": "Figure 3:Distribution of scripts inFiNERweb: Around 50% of the languages use the Latin script, while the dataset covers a long tail of 25 different scripts in total.",
                "position": 398
            }
        ]
    },
    {
        "header": "3Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13884/images/hallucination_analysis.png",
                "caption": "Figure 4:Quality evaluation with an LLM judge. Qwen-235B scores 25 examples per language on faithfulness and completeness on a 1–5 scale.",
                "position": 572
            },
            {
                "img": "https://arxiv.org/html/2512.13884/images/label_similarity_distribution.png",
                "caption": "Figure 5:Pairwise cosine similarity distributions of label prompts embedded with MiniLM. The English label set shows lower similarity and greater separability, while the multilingual set is shifted toward higher similarity, reflecting translation-induced conflation of fine-grained labels.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2512.13884/images/confidence_distribution.png",
                "caption": "Figure 6:Distribution of model confidence scores for gold spans obtained via k-fold cross validation. We partition the dataset into five subsets and collect the confidence scores for each gold span of the held-out set. We can see annotations inFiNERwebfollow a long-tail distribution.",
                "position": 655
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Overview",
        "images": []
    },
    {
        "header": "Appendix BEntity Extraction Algorithm",
        "images": []
    },
    {
        "header": "Appendix CSelected Multilingual Examples",
        "images": []
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    }
]