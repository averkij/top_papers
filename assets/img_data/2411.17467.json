[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17467/x1.png",
                "caption": "Figure 1:Point-MAE-Zero.(a)Our synthetic 3D point clouds are generated by sampling, compositing, and augmenting simple primitives with procedural 3D programs[34].(b)We use Point-MAE[42]as our pretraining framework to learn 3D representation from synthetic 3D shapes, dubbedPoint-MAE-Zerowhere ‚ÄúZero‚Äù underscores that we do not use any human-made 3D shapes.(c)We evaluate Point-MAE-Zero in various 3D shape understanding tasks.",
                "position": 171
            }
        ]
    },
    {
        "header": "3Learning from Procedural 3D Programs",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17467/x2.png",
                "caption": "Figure 2:Masked Point Cloud Completion.This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes.Left: Ground truth 3D point clouds and masked inputs with a 60% mask ratio.Middle: Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE[19].Right: Point cloud reconstructions without any guidance points.\nTheL2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTChamfer distance (lower is better) between the predicted 3D point clouds and the ground truth is displayed below each reconstruction.",
                "position": 802
            },
            {
                "img": "https://arxiv.org/html/2411.17467/x3.png",
                "caption": "Figure 3:Impact of 3D Shape Complexity on Performance.Left:Examples of procedurally generated 3D shapes with increasing complexity, used for pretraining. Textures are shown for illustration purposes only; in practice, only the surface points are used.Right:Comparison of pretraining masked point reconstruction loss (Eqn.1)[19]and downstream classification accuracy on the ScanObjectNN dataset[25]. Each row in Point-MAE-Zero represents an incrementally compounded effect of increasing shape complexity and augmentation, with the highest accuracy achieved using shape augmentation.",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2411.17467/x4.png",
                "caption": "Figure 4:Impact of pretraining dataset size.We report the classification accuracy (%) on the PB-T50-RS subset of ScanObjectNN[25]as a function of the pretraining dataset size.",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2411.17467/x5.png",
                "caption": "Figure 5:Learning curves in downstream tasks.We present validation accuracy (top row) and training curves (bottom row) in object classification tasks on ScanObjectNN (left column) and ModelNet40 (right column).",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2411.17467/x6.png",
                "caption": "Figure 6:t-SNE visualization of 3D shape representations.(a)displays the distribution of representations from transformer encoders with different initialization strategies: randomly initialized (Scratch), pre-trained with Point-MAE on ShapeNet (Point-MAE-SN), and pre-trained on procedurally generated 3D shapes (Point-MAE-Zero).(b)shows the distribution of shape representations after fine-tuning on the target tasks, which include object classification on ModelNet40 (top row) and ScanObjectNN (bottom row). Each point represents a 3D shape while the color denotes the semantic categories.",
                "position": 1040
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining-from-Scratch Baseline",
        "images": []
    },
    {
        "header": "Appendix BMore Rigorous Evaluation",
        "images": []
    },
    {
        "header": "Appendix CLinear Probing",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17467/x7.png",
                "caption": "Figure 7:Masked Point Cloud Completion.This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes.Left: Ground truth 3D point clouds and masked inputs with a 60% mask ratio.Middle: Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE[19].Right: Point cloud reconstructions without any guidance points.\nTheL2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTChamfer distance (lower is better) between the predicted 3D point clouds and the ground truth is displayed below each reconstruction.",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2411.17467/x8.png",
                "caption": "Figure 8:t-SNE Visualization.We visualize features extracted by Point-MAE-Zero (left) and Point-MAE-SN (right) for three primitive shapes ‚Äî Ellipsoid, Cube, and Cylinder.",
                "position": 2133
            }
        ]
    },
    {
        "header": "Appendix DAdditional Visualization",
        "images": []
    }
]