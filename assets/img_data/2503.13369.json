[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/hface.png",
                "caption": "",
                "position": 211
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/visual_abstract_bk.png",
                "caption": "Figure 1:The key benefit of utilizing sighted user feedback lies in theirassessments, which are based on solid visual grounding. The compiled assessments prove an effective training substance for steering VLMs towards more accessible descriptions. Dataset use and the subsequent validation are described in Sec.4. A complete list of use cases is provided in AppendixA.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/dimensions_assignment.png",
                "caption": "Figure 2:The qualities assessed by their respective groups.",
                "position": 627
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TheSightationDataset",
        "images": []
    },
    {
        "header": "4Performance Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/together.png",
                "caption": "Figure 3:Tuning VLMs onSightationenhanced various qualities of the diagram descriptions, evaluated by BLV educators, and shown here as normalized ratings averaged in each aspect. The capability of the dataset is most strongly pronounced with the 2B variant, shown above. Full results across 4 models and 22 metrics are reported in TablesE.1,E.1,11, and12.",
                "position": 798
            }
        ]
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOur Complete Dataset Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/interleaved_evaluation.png",
                "caption": "Figure 4:Percentage distribution of the quality of question-answer pairs in AI2D andSightationVQA",
                "position": 1851
            }
        ]
    },
    {
        "header": "Appendix BFurther Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/x1.png",
                "caption": "Figure 5:Less can be more for BLV users. Our approach streamlines details to highlight the core information while emphasizing key details to increase information density and maximize information efficiency per unit length.",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix CDetails on the Annotations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/wins_average.png",
                "caption": "Figure 6:Win rates bym‚Å¢o‚Å¢d‚Å¢e‚Å¢lùëöùëúùëëùëíùëômodelitalic_m italic_o italic_d italic_e italic_l.",
                "position": 2653
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/best_sentence_distribution_ag.png",
                "caption": "Figure 7:Descriptions generated byGPT-4o mini",
                "position": 2683
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/best_sentence_distribution_bg.png",
                "caption": "",
                "position": 2686
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/best_sentence_distribution_aq.png",
                "caption": "Figure 8:Descriptions generated byQwen2-VL",
                "position": 2690
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/best_sentence_distribution_bq.png",
                "caption": "",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/best_sentence_length_word.png",
                "caption": "Figure 9:boxplot of best sentence length",
                "position": 2703
            }
        ]
    },
    {
        "header": "Appendix DRetrieval Dataset Construction",
        "images": []
    },
    {
        "header": "Appendix EDetailed Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png",
                "caption": "Table 9:The full evaluation on descriptions by GPT. Nature of Context values are not in bold because it is a categorical variable.",
                "position": 2740
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png",
                "caption": "Table 10:The full evaluation on descriptions by the 72B model. Due to limited recruiting, BLV annotators were not given this set.",
                "position": 2922
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png",
                "caption": "Table 13:The smaller model outperforms a larger variant across many metrics. It is also important to note that the VLM judgments align better with sighted educators than with BLV educators. Further analysis is found in Section5. This tendency is especially strong with the pairwise comparison between 72B- and 7B-generated descriptions. Nature of Context values are not in bold because it is a categorical variable.",
                "position": 3576
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png",
                "caption": "Table 14:The 2B model performs on par with the 7B variant. Again, VLM judgments align better with sighted educators than with BLV educators. Further analysis is found in Section5. Nature of Context values are not in bold because it is a categorical variable.",
                "position": 3739
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png",
                "caption": "Table 15:A 2B model fine-tuned onSightationCompletionsoutperforms a 3B model tuned on a larger dataset. Note thatChartGemmais not meant for conversational use. Hence, for a fair comparison, we didnotenter our guided generation prompt and instead input only the brief request ‚ÄúGenerate a caption‚Äù to both models.",
                "position": 3902
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/spider_retrieval_blip2coco_vs_blip2ours.png",
                "caption": "Figure 10:Retrieval performance was measured with 2-way cross validation. On our test set (Left), the COCO-tunedBLIP-2generalizes poorly, whereas on the COCO test set (Right), theSightationRetrieval-tunedBLIP-2performs on par with the COCO-tunedBLIP-2.",
                "position": 4078
            }
        ]
    },
    {
        "header": "Appendix FAnnotator Demographics and Interviews",
        "images": []
    },
    {
        "header": "Appendix GPrompts",
        "images": []
    },
    {
        "header": "Appendix HFine-tuning Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/label_studio_sample.png",
                "caption": "",
                "position": 5466
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/label_studio_sample2.png",
                "caption": "",
                "position": 5492
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/label_studio_sample3.png",
                "caption": "",
                "position": 5593
            },
            {
                "img": "https://arxiv.org/html/2503.13369/extracted/6287606/figures/label_studio_sample4.png",
                "caption": "",
                "position": 5628
            }
        ]
    },
    {
        "header": "Appendix IGuidelines",
        "images": []
    }
]