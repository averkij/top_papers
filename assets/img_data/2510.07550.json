[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07550/figures/teas.jpg",
                "caption": "",
                "position": 158
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07550/figures/method-reved.jpg",
                "caption": "Figure 2:Overview of our proposed TRAVL framework.Given input video frames, we apply a vision encoder followed by trajectory-aware masked self-attention, which integrates spatial and temporal context using patch trajectories tracked by CoTracker. The enriched features are projected into the language model’s embedding space. Only the trajectory attention and vision-to-language projector are fine-tuned; the vision encoder and language model are kept frozen.",
                "position": 209
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07550/figures/training-samp.jpg",
                "caption": "Figure 3:Fine-tuning data pipeline.Our dataset is built in three stages:Stage 1 (Plausible Captioning):GPT-4o generates initial captions for real (plausible) videos, verified by human reviewers.Stage 2 (Feedback-Augmented Captioning):Human annotators provide short temporal feedback for each implausible video, which is combined with the original real caption to create a complete description using GPT-4o.Stage 3 (QA Generation):Based on the final caption, GPT-4o produces temporally grounded question-answer pairs per video.\nThis pipeline enables fine-grained supervision across a controlled set of plausible and implausible variants.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/implausibench-sample.jpg",
                "caption": "Figure 4:Example from ImplausiBench.For each scenario, we include both a real (plausible) and a generated (implausible) video that share the same initial scene and visual style. Each pair is annotated with a shared multiple-choice question containing three plausible, three implausible, and one “None of the above” option. The correct answer depends on which version of the video is shown—ensuring that models must ground their predictions in visual-temporal evidence rather than language alone.",
                "position": 402
            }
        ]
    },
    {
        "header": "4ImplausiBench",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AResults in Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07550/figures/impossible-1.jpg",
                "caption": "",
                "position": 1632
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/imposible-2.jpg",
                "caption": "",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/implausibench-chair.jpg",
                "caption": "",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/implausibench-match.jpg",
                "caption": "",
                "position": 1638
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/implausibench-magnet.jpg",
                "caption": "",
                "position": 1640
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/implausibench-paint.jpg",
                "caption": "Figure 5:Qualitative examples from TRAVL.The first two pages show frames from Impossible Videos, while the remaining illustrate plausible and implausible variants from ImplausiBench. These examples were selected to showcase representative successes (check mark) and failures (cross) across different models, as identified through manual inspection.",
                "position": 1642
            }
        ]
    },
    {
        "header": "Appendix BFine-Tuning Dataset",
        "images": []
    },
    {
        "header": "Appendix CImplausiBench Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07550/figures/t1.jpg",
                "caption": "Figure 6:Qualitative examples from ImplausiBench.Each row shows a real video (left) and its implausible counterpart (right).\nPairs share a seven-option MCQ designed to prevent language-only shortcuts.",
                "position": 1856
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/t2.jpg",
                "caption": "",
                "position": 1860
            },
            {
                "img": "https://arxiv.org/html/2510.07550/figures/t3.jpg",
                "caption": "",
                "position": 1862
            }
        ]
    },
    {
        "header": "Appendix DTRAVL Details and Model Specifications",
        "images": []
    },
    {
        "header": "Appendix EObservations",
        "images": []
    }
]