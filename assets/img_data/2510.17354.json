[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17354/x1.png",
                "caption": "Figure 1.An illustration of the input patterns of “mixed-modal” content in the URAG scenario.††:",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2510.17354/x2.png",
                "caption": "Figure 2.The proposed four-step automatedNyxQAconstruction pipeline.††:",
                "position": 175
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17354/x3.png",
                "caption": "Figure 3.Overview of theNyxarchitecture and its training paradigm.††:",
                "position": 238
            }
        ]
    },
    {
        "header": "4.Main Experiments",
        "images": []
    },
    {
        "header": "5.Quantitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17354/x4.png",
                "caption": "Figure 4.Impact of training data scale onNyxQAaccuracy when trainingNyxwith varying sample sizes.††:",
                "position": 1201
            },
            {
                "img": "https://arxiv.org/html/2510.17354/x5.png",
                "caption": "Figure 5.Impact of (a) the number of in-context documents and (b) feedback-based retriever fine-tuning on downstream generation performance. Results are shown onNyxQAusing InternVL3 models of varying sizes, respectively.††:",
                "position": 1204
            },
            {
                "img": "https://arxiv.org/html/2510.17354/x6.png",
                "caption": "Figure 6.Comparison of retrieval and answer correctness distributions onNyxQAfor mmE5,Nyx-pretrained, andNyx.††:",
                "position": 1207
            },
            {
                "img": "https://arxiv.org/html/2510.17354/x7.png",
                "caption": "Figure 7.Case study on MMQA. The top-1 retrieved documents by mmE5,Nyx-pretrained, andNyxare shown together with the corresponding answers produced by VLM.††:",
                "position": 1210
            }
        ]
    },
    {
        "header": "6.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BBaseline Retriever Models",
        "images": []
    },
    {
        "header": "Appendix CDataset Details",
        "images": []
    },
    {
        "header": "Appendix DDetails for Raw QA Generation",
        "images": []
    }
]