[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20619/x1.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20619/x2.png",
                "caption": "Figure 2:Illustration of the proposed SemanticGen.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20619/x3.png",
                "caption": "Figure 3:Overview of SemanticGen.(a) We train a semantic generator to fit the compressed semantic representation distribution of off-the-shelf semantic encoders. (b) We optimized a latent diffusion model for denoising video VAE latents conditioned on their semantic representations. (c) During inference, we integrate the semantic generator and VAE latent generator to achieve high-quality T2V generation.",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x4.png",
                "caption": "Figure 4:Video generation conditioned on semantic features extracted from a reference video.Row 1:The reference video.Rows 2â€“4:Reconstructions based on semantic representations (Sem. Rep.) with dimensions of 2048, 64, and 8, respectively.Row 5:T2V generation results without semantic representations.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x5.png",
                "caption": "Figure 5:Implementation of Swin-Attention.When generating long videos, we apply full attention to model the semantic representations and use shifted-window attention[swin]to map them into the VAE space. Thebluesquares indicate VAE latents, while theyellowsquares denote semantic representations.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x6.png",
                "caption": "Figure 6:Comparison with state-of-the-art methods on short video generation.It shows that SemanticGen generates high-quality videos that adhere to the text prompts and are comparable to strong baselines.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x7.png",
                "caption": "Figure 7:Comparison with state-of-the-art methods on long video generation.It demonstrates that SemanticGen generates videos with long-term consistency and significantly alleviates the drifting issues.",
                "position": 285
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20619/x8.png",
                "caption": "Figure 8:Qualitative ablation on semantic space compression.Row 1:SemanticGen without compression;Row 2:Compress the semantic space using an MLP with 64 output channels;Row 3:Compress the semantic space using an MLP with 8 output channels.",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x9.png",
                "caption": "Figure 9:Ablation on the representation space.We visualize the generation results of learning on the semantic space and the compressed VAE latent space with the same training steps.",
                "position": 602
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20619/x10.png",
                "caption": "Figure 10:Overview of the base text-to-video generation model.",
                "position": 667
            }
        ]
    },
    {
        "header": "Appendix AIntroduction of the Base Text-to-Video Generation Model",
        "images": []
    },
    {
        "header": "Appendix BMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20619/x11.png",
                "caption": "Figure 11:More synthesized results of SemanticGen.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x12.png",
                "caption": "Figure 12:More synthesized results of SemanticGen.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x13.png",
                "caption": "Figure 13:Comparison with additional baselines.",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2512.20619/x14.png",
                "caption": "Figure 14:Visualization of failure cases.",
                "position": 711
            }
        ]
    },
    {
        "header": "Appendix CFuture Work",
        "images": []
    }
]