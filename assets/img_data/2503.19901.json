[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19901/x1.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19901/x2.png",
                "caption": "Figure 2:TokenHSI consists of two stages: (left) foundational skill learning and (right) policy adaptation. Through multi-task policy training, the proposed framework learns versatile interaction skills in a single transformer network. Theses learned skills can be flexibly adapted to more challenging HSI tasks by training the lightweight modules,e.g.,𝕋n⁢e⁢wsuperscript𝕋𝑛𝑒𝑤\\mathbb{T}^{new}blackboard_T start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT,𝕋csuperscript𝕋𝑐\\mathbb{T}^{c}blackboard_T start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, andξ𝔸={ξ0𝔸,ξ1𝔸}superscript𝜉𝔸subscriptsuperscript𝜉𝔸0subscriptsuperscript𝜉𝔸1\\xi^{\\mathbb{A}}=\\{\\xi^{\\mathbb{A}}_{0},\\xi^{\\mathbb{A}}_{1}\\}italic_ξ start_POSTSUPERSCRIPT blackboard_A end_POSTSUPERSCRIPT = { italic_ξ start_POSTSUPERSCRIPT blackboard_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_ξ start_POSTSUPERSCRIPT blackboard_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT }.",
                "position": 184
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19901/x3.png",
                "caption": "Figure 3:Learning curves comparing the efficiency on skill composition tasks using TokenHSI, policies trained from scratch[79], CML[108], and its improved version CML (dual). Colored regions denote mean values±plus-or-minus\\pm±a standard deviation based on3333models initialized with different random seeds.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2503.19901/",
                "caption": "Figure 4:Through policy adaptation, TokenHSI can generalize learned foundational skills to more challenging scene interaction tasks.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2503.19901/x5.png",
                "caption": "Figure 5:Learning curves comparing the efficiency on object shape variation tasks using TokenHSI, full fine-tuning of pre-trained policies, and AdaptNet[109].",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2503.19901/x6.png",
                "caption": "Figure 6:Learning curves comparing the efficiency on terrain shape variation tasks using TokenHSI, Scratch[79], and AdaptNet[109]. We ablate the adapter layers during training.",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2503.19901/x7.png",
                "caption": "Figure 7:Long-horizon task completion by sequentially executing (a) pre-trained skills and (b) adapted skills by our approach.",
                "position": 590
            }
        ]
    },
    {
        "header": "5Discussion and Limitations",
        "images": []
    },
    {
        "header": "6Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ASimulated Character",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19901/x8.png",
                "caption": "Figure A:Different simulated character models. Building on (a) AMP’s model, we devise two improved versions: (b) and (c), which are used for tasks on flat ground and tasks on stairs terrain, respectively.",
                "position": 2367
            }
        ]
    },
    {
        "header": "BTasks",
        "images": []
    },
    {
        "header": "CImplementation Details of CML",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19901/x9.png",
                "caption": "Figure B:Learning curves comparing the efficiency on long-horizon task completion using TokenHSI, Scratch[79], and iterative fine-tuning of multiple pre-trained specialist policies, namely Finetune.",
                "position": 3049
            }
        ]
    },
    {
        "header": "DQuantitative Evaluation on Long-horizon Task Completion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19901/x10.png",
                "caption": "Figure C:Qualitative results of new skills learned by our policy adaptation. (a) We first learn two out-of-domain interaction skills,i.e., pushing down a large object and walking to a target location while lifting up a box using two hands. (b) We then combine the new lifting skill with previously learned sitting and path-following skills. These results demonstrate the good extensibility of our transformer policy.",
                "position": 3065
            }
        ]
    },
    {
        "header": "EExtensibility",
        "images": []
    }
]