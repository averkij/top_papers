[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00918/x1.png",
                "caption": "Figure 1:The detailed design of LiBMoE, which comprises three major modules. First, the MoE module implements various MoE algorithms. Second, the training modules handles the training process and supports various configurations. Lastly, the evaluation module supports almost 100 zero-shot benchmarks and a wide-range of metrics.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LibMoE",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00918/x2.png",
                "caption": "Figure 2:Overview of the LibMoE architecture and training process. In the first stage of Dense Training, only the MLP is trained to improve alignment. In the second stage, all parameters are trained. During MoE Training, the feed-forward networks (FFNs) of the Vision Encoder (VE) and MLP Connector are used to initialize the experts within the MoE framework, and all parameters continue to be trained.",
                "position": 207
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00918/x3.png",
                "caption": "Figure 3:Comparison of the performance of different MoE algorithms over time. The experiments are conducted on theLLaVa-332K datasetand the CLIP + Phi3 model.",
                "position": 1024
            },
            {
                "img": "https://arxiv.org/html/2411.00918/x4.png",
                "caption": "Figure 4:Impact of Training Data Percentage on Expert Selection.",
                "position": 1045
            },
            {
                "img": "https://arxiv.org/html/2411.00918/x5.png",
                "caption": "Figure 5:Entropy analysis of expert selection frequency across subtasks in MoE algorithms. The entropy values indicate the tendency of different routers to consistently select specific experts for given subtasks.",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2411.00918/x6.png",
                "caption": "Figure 6:Measured confidence levels of various MoE algorithms across tasks. Entropy was computed for each sample and then averaged within each task to illustrate differences in confidence across MoE algorithms. For the Cosine-R and Perturbed Cosine-R algorithms, values on the x-axis (denoted by∗) were scaled to enhance visualization of subtle entropy variations. The scaled entropy values are calculated using the transformation(entropy−1.999)×10000entropy1.99910000(\\text{entropy}-1.999)\\times 10000( entropy - 1.999 ) × 10000.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2411.00918/x7.png",
                "caption": "Figure 7:Expert selection across layers on different tasks in the MME benchmarks. The model uses SigLIP as the vision encoder and Phi 3.5 as the LLM. This figure highlights the distinct expert selection behavior observed in the vision encoder layers.",
                "position": 1170
            },
            {
                "img": "https://arxiv.org/html/2411.00918/x8.png",
                "caption": "Figure 8:Comparison of the average entropy of the frequency distribution of selected experts across subtasks using different vision encoders: Siglip and CLIP.",
                "position": 1205
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe performance of different MoE algorithms over time.",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00918/x9.png",
                "caption": "Figure 9:Comparison of the performance of different MoE algorithms across 11 benchmarks over time. The experiments were conducted using the LLaVa-332K dataset and the CLIP + Phi3 model.",
                "position": 2004
            }
        ]
    },
    {
        "header": "Appendix BTraining Times Details",
        "images": []
    }
]