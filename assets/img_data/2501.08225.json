[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08225/x1.png",
                "caption": "",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08225/x2.png",
                "caption": "Figure 2:Overview of FramePainter.Reformulating image editing as an image-to-video generation task, FramePainter takes a source image and an editing instruction as the first frame and control guidance, and produces a two-frame video comprising of reconstructed and target images.\nTo improve visual consistency of two images involving large motion, matching attention is proposed to enlarge the receptive field and encourage dense correspondence between target and source image tokens.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08225/x3.png",
                "caption": "Figure 3:Collected samples from videos.We present three types of editing signals from top to bottom: drawing sketches, click points, and dragging regions.",
                "position": 179
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08225/x4.png",
                "caption": "Figure 4:Qualitative comparisons across different visual editing instructions.Compared to the baselines, FramePainter not only achieves more coherent and plausible editing results, but also automatically polishes the edited images to meet real-world dynamics,e.g., remove duplicate tail and adjust car door in mirror (highlighted inred box).\nWe note that LightningDrag and DragDiffusion require users to provide additional masks, whereas FramePainter does not.",
                "position": 499
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08225/x5.png",
                "caption": "Figure 5:Emerging capabilities of FramePainter.Although FramePainter is trained on image pairs from real-world videos, it demonstrates several emerging capabilities as a convenient tool:(i)Supporting highly intuitive and simplified instructions.(ii)Offering precise control over complex editing signals.(iii)Generalizing well to out-of-domain cases, such as shape transformation.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2501.08225/x6.png",
                "caption": "Figure 6:Qualitative ablation study on the effectiveness of matching attention.Matching attention obtains plausible edited results with fine-grained visual consistency.\nIn contrast, temporal attention fails to handle editing signals involving large edited areas, while cross-frame attention struggles to precisely capture appearance.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2501.08225/x7.png",
                "caption": "Figure 7:Qualitative ablation study on source image reconstruction.Compared to w/o reconstruction, reconstruction source image in diffusion loss can better preserve its color and texture and produce more visually consistent edited image.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2501.08225/x8.png",
                "caption": "Figure 8:Visualization of attention weights and dense correspondence.The attention map is computed between the selected target image token (i.e.,redquery point) and all source image tokens.\nAmong all source image tokens, the token with the highest similarity is marked as the matching point.\nWe only visualize the tokens of foreground objects for simplicity.",
                "position": 588
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details of Different Visual Editing Instructions.",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08225/x9.png",
                "caption": "Figure 9:More visualization examples of FramePainter.This figure presents both a wide range of scenarios, including in-domain (e.g., change the position of cat ear) and out-of-domain cases (e.g., enlarge the dear horn in hat).",
                "position": 1659
            },
            {
                "img": "https://arxiv.org/html/2501.08225/x10.png",
                "caption": "Figure 10:More qualitative comparisons in sketch images.",
                "position": 1665
            },
            {
                "img": "https://arxiv.org/html/2501.08225/x11.png",
                "caption": "Figure 11:More qualitative comparisons in coarsely edited images.",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2501.08225/x12.png",
                "caption": "Figure 12:More qualitative comparisons in dragging points.We compare FramePainter with both encoder-based (i.e., LightningDrag) and optimization-based methods (i.e., DragDiffusion, SDE-Drag, and DiffEditor).\nDuring inference, DragDiffusion and SDE-Drag require to finetune additional LoRAs to preserve the visual appearance of source images.",
                "position": 1675
            }
        ]
    },
    {
        "header": "Appendix BMore Visualizations and Comparisons.",
        "images": []
    }
]