[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x1.png",
                "caption": "Figure 1:SpatialTree.Inspired by cognitive science, our proposed SpatialTree organizes spatial intelligence into a four-layer hierarchy (L1-L4). Rooted in foundational multi-modal capabilities (L0), the tree progressively branches from Basic perception (L1) to agentic competence (L4).",
                "position": 167
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x2.png",
                "caption": "(a)",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x2.png",
                "caption": "(a)",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x3.png",
                "caption": "(b)",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x4.png",
                "caption": "(c)",
                "position": 285
            }
        ]
    },
    {
        "header": "3The SpatialTree Taxonomy",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x5.png",
                "caption": "Figure 3:Benchmark Data Engines.Level-specific engines process data and construct QAs.",
                "position": 373
            }
        ]
    },
    {
        "header": "4Instantiating the SpatialTree Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x6.png",
                "caption": "Figure 4:Distribution of Benchmark data and Evaluation Metrics.We analyze the metric usage across 41 tasks in our benchmark. The evaluation relies primarily on multiple-choice questions (70.7%), complemented by task-specific numeric metrics (e.g., cognitive map accuracy) and LLM-as-a-Judge protocols.",
                "position": 403
            }
        ]
    },
    {
        "header": "5A Hierarchical Analysis of Spatial Capabilities",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x7.png",
                "caption": "Table 1:Our-Bench.Dark grayindicates the best result among all models andlight grayindicates the best result among open-source models. NT denotes the non-thinking model.Avgis computed using the weights in brackets [â‹…\\cdot].",
                "position": 429
            }
        ]
    },
    {
        "header": "6Exploring Ability Dependencies and Hierarchical Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x7.png",
                "caption": "Figure 5:Inter-Capability Dependencies via Pearson Correlation.(A) Correlation matrix among higher-level capabilities (L3 and L4); (B) Correlation matrix among foundational L1 capabilities; (C) Salient low-level abilities influencing higher-level tasks.",
                "position": 761
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x8.png",
                "caption": "Figure 6:Demonstration of Capability Transfer after Distance SFT.(Top)The model is trained on distance QAs, such as object depth sorting and comparison, just using data from synthetic and indoor scenes.(Middle)This learned capability transfers in a zero-shot manner to complex reasoning tasks in unseen, in-the-wild scenes, achieving a36.0%performance gain over the baseline.(Bottom)Furthermore, the skill exhibits cross-level transfer, enabling the model to perform a robotic arm manipulation task with a27.1%performance gain.",
                "position": 986
            }
        ]
    },
    {
        "header": "7Conclusion and Future works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AVisualization of Data Sources",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/figures/datasource_new.png",
                "caption": "Figure A:Construction of SpatialTree-Bench.We build our benchmark by reorganizing various existing datasets and mapping them to our capability tree, whereSpatialPlus, a complementary dataset are introduced to ensure the capability coverage.",
                "position": 2324
            }
        ]
    },
    {
        "header": "BEvaluation Metrics Details",
        "images": []
    },
    {
        "header": "CSpatialPlus: Complementary Data Annotations for SpatialTree",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x9.png",
                "caption": "Figure B:Orientation Annotations.The left side is the gravity field estimated from GeoCalib[49], while the right side is from OrientAnything.",
                "position": 2364
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x10.png",
                "caption": "Figure C:Prompt templatefor Orientation Estimation.",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x11.png",
                "caption": "(a)",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x11.png",
                "caption": "(a)",
                "position": 2435
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x12.png",
                "caption": "(b)",
                "position": 2440
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x13.png",
                "caption": "Figure E:Prompt of navigation.",
                "position": 2683
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x14.png",
                "caption": "Figure F:Prompt for Goal-Driven Manipulation with 7D Action Representation.",
                "position": 2719
            }
        ]
    },
    {
        "header": "DAbility Transfer via Prompting",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x15.png",
                "caption": "Figure G:Correspondence Prompting for Navigation.The correspondence prompt guides Gemini2.5-pro to navigate and move more accurately within 3D environments.",
                "position": 2744
            }
        ]
    },
    {
        "header": "EBenchmark Metric Aggregation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20617/x16.png",
                "caption": "Figure H:An illustration of the hierarchical weighting scheme for metric aggregation with in theSpatialTree. Each node represents a capability layer, with the assigned weight used for the bottom-up calculation of the final score. The weighting prioritizes foundational perceptual abilities (L1) as they are prerequisites for higher-level cognitive tasks.",
                "position": 2759
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x17.png",
                "caption": "Table B:Examples ofL1.Geometry.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2512.20617/figures/case/geometry-dist.jpg",
                "caption": "",
                "position": 2821
            },
            {
                "img": "https://arxiv.org/html/2512.20617/figures/case/geometry-shape.png",
                "caption": "",
                "position": 2855
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x18.png",
                "caption": "Table C:Examples ofL1.Relation.",
                "position": 2888
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x19.png",
                "caption": "",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x20.png",
                "caption": "Table D:Examples ofL1.Orientation.",
                "position": 2967
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x21.png",
                "caption": "",
                "position": 3015
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x22.png",
                "caption": "Table E:Examples ofL1.Motion.",
                "position": 3048
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x23.png",
                "caption": "",
                "position": 3092
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x24.png",
                "caption": "Table F:Examples ofL1.Localization.",
                "position": 3126
            },
            {
                "img": "https://arxiv.org/html/2512.20617/figures/case/L2_pers.jpg",
                "caption": "Table G:Examples ofL2.Understanding.",
                "position": 3175
            },
            {
                "img": "https://arxiv.org/html/2512.20617/figures/case/L2aff.jpg",
                "caption": "",
                "position": 3224
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x25.png",
                "caption": "Table H:Examples ofL2.Memory.",
                "position": 3258
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x26.png",
                "caption": "",
                "position": 3307
            },
            {
                "img": "https://arxiv.org/html/2512.20617/figures/case/L3_caus_dyn.jpg",
                "caption": "Table I:Examples ofL3.Causal Reasoning.",
                "position": 3340
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x27.png",
                "caption": "",
                "position": 3389
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x28.png",
                "caption": "",
                "position": 3421
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x29.png",
                "caption": "Table J:Examples ofL3.Sequential Planning.",
                "position": 3454
            },
            {
                "img": "https://arxiv.org/html/2512.20617/figures/case/L3_seq_ope.jpg",
                "caption": "",
                "position": 3498
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x30.png",
                "caption": "Table K:Examples ofL4 Agentic Competence.",
                "position": 3536
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x31.png",
                "caption": "",
                "position": 3591
            },
            {
                "img": "https://arxiv.org/html/2512.20617/x32.png",
                "caption": "Table L:Examples ofL4 Agentic Competence(Continued).",
                "position": 3628
            }
        ]
    },
    {
        "header": "FMore Visualizations for QAs in SpatialTree Bench",
        "images": []
    }
]