[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18966/x1.png",
                "caption": "Figure 1:Showcase of the versatile abilities of theUSOmodel. Prompts are inTableËœ4.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x2.png",
                "caption": "Figure 2:Illustration of our motivation. By jointly disentangling content and style across tasks, we unify style-driven and subject-driven generation within a single framework.",
                "position": 125
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18966/x3.png",
                "caption": "Figure 3:Illustration of our proposed cross-task triplet curation framework, which systematically generates layout-preserved and layout-shifted triplets.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x4.png",
                "caption": "Figure 4:Illustration of the training framework ofUSO. USO unifies subject-driven and style-driven generation in two stages: Stage 1 aligns SigLIP embeddings via style-alignment training to yield a style-capable model; Stage 2 disentangles the conditional encoders and trains on triplets to enable the joint conditional generation. Finally, a style-reward learning paradigm supervises both stages to yield a stronger unified model.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x5.png",
                "caption": "Figure 5:Qualitative comparison with different methods on subject-driven generation.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x6.png",
                "caption": "Figure 6:Qualitative comparison with different methods on style-driven generation.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x7.png",
                "caption": "Figure 7:Qualitative comparison with different methods on identity-driven generation.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x8.png",
                "caption": "Figure 8:Qualitative comparison with different methods on style-subject-driven generation.",
                "position": 383
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18966/x9.png",
                "caption": "Figure 9:Radar charts of user evaluation of methods for subject-driven and style-driven generation on different dimensions.",
                "position": 601
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18966/x10.png",
                "caption": "Figure 10:Ablation study of SRL. Theblueboxes denote content reference and thepurpleboxes denote style reference. Prompts are \"A toy with a mountain in the background.\", \"The man on the beach.\", \"The woman is skateboarding on the street.\", \"A beautiful woman.\", \"A beautiful woman.\", \"The woman gave an impassioned speech on the podium.\" from left to right.",
                "position": 622
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x11.png",
                "caption": "Figure 13:Ablation study of USO. Zoom in for details.",
                "position": 743
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "USO: Unified Style and Subject-Driven Generation viaDisentangled and Reward Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.18966/x12.png",
                "caption": "Figure 14:Examples of USO-Bench.",
                "position": 1330
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x13.png",
                "caption": "Figure 15:More results on subject-driven generation.",
                "position": 1591
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x14.png",
                "caption": "Figure 16:More results on subject-driven generation.",
                "position": 1594
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x15.png",
                "caption": "Figure 17:More results on identity-driven generation.",
                "position": 1597
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x16.png",
                "caption": "Figure 18:More results on identity-driven generation.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x17.png",
                "caption": "Figure 19:More results on style-driven generation.",
                "position": 1603
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x18.png",
                "caption": "Figure 20:More results on style-driven generation.",
                "position": 1606
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x19.png",
                "caption": "Figure 21:More results on style-subject-driven generation. We set prompt to empty for layout-preserved generation.",
                "position": 1609
            },
            {
                "img": "https://arxiv.org/html/2508.18966/x20.png",
                "caption": "Figure 22:More results on style-subject-driven generation. USO supports any subject combined with any style in any scenario.",
                "position": 1612
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]