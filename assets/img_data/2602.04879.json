[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04879/figs/moe_prob_ratio_tv_1.png",
                "caption": "Figure 2:The plots show numerical differences between a training and an inference engine for Qwen3-30B-A3B-Base with identical parameters.(Left)The probability ratio (used in PPO) is highly volatile for low-probability tokens.(Right)In contrast, the TV divergence (used in DPPO) is more stable.\nThis highlights a key flaw of PPOâ€™s clipping mechanism: it over-penalizes low-probability tokens, which can slow down learning; and under-penalizes high-probability tokens, which can permit large, destabilizing updates.",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Trust Region Under LLM Regime",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04879/x1.png",
                "caption": "Figure 3:DPPO variants achieve stable training while controlling the training-inference mismatch at a low level. In contrast, methods without a trust region (PG-IS, CISPO) or with a misspecified one (MiniRL) suffer from growing mismatch and eventual collapse.",
                "position": 486
            }
        ]
    },
    {
        "header": "5Analysis on Training Stability",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04879/x2.png",
                "caption": "Figure 4:Switching the stable DPPO-KL to a decoupled objective causes the mismatch to grow and performance to collapse, confirming that the trust region must be anchored to the rollout policy.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x3.png",
                "caption": "Figure 5:Isolating the source of instability. The solid curves are training rewards, while the dashed lines are the percentage ofbad updates. Starting with the unstable PG-IS, applying a minimal mask that only blocks large-divergence bad updates on negative samples is sufficient to stabilize training, indicating these bad updates are the primary cause of training instability.",
                "position": 574
            }
        ]
    },
    {
        "header": "6Analysis on Training Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04879/x4.png",
                "caption": "Figure 6:Analysis of relaxing trust regions for low-probability tokens. (Left) Training reward curves. (Middle) Rollout probability of clipped tokens. (Right) Entropy of clipped tokens.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x5.png",
                "caption": "Figure 7:Analysis of trust region relaxation direction. (Left) Training reward curves. (Right) Policy entropy.",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x6.png",
                "caption": "Figure 8:Evolution of AIME24 and AIME25 Avg@32 scores during RL training using Qwen3-30B-A3B-Base.\nThe first and third panels correspond to the same experiment without rollout router replay (w/o R3), while the second and fourth panels correspond to the same experiment with rollout router replay (w/ R3).",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x7.png",
                "caption": "Figure 9:Evolution of AIME24 and AIME25 scores during RL training using Qwen3-30B-A3B (left) and Qwen3-8B-Base (right).",
                "position": 632
            }
        ]
    },
    {
        "header": "7Scaling Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04879/x8.png",
                "caption": "Figure 10:Evolution of AIME24 and AIME25 scores for baselines and DPPO with binary/Top-K (K=20) TV/KL approximation under the same setting as MoE Base w/o R3.",
                "position": 661
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BTrust Region in LLMs",
        "images": []
    },
    {
        "header": "Appendix CApproximations as Lower Bounds of True Divergence",
        "images": []
    },
    {
        "header": "Appendix DMore Details for Stability Analysis",
        "images": []
    },
    {
        "header": "Appendix ECharacterizing Clipped Tokens",
        "images": []
    },
    {
        "header": "Appendix FMore Details for Scaling Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04879/x9.png",
                "caption": "Figure 11:Evolution of metrics forMoE Base w/o R3experiment (based on Qwen3-30B-A3B-Base, without rollout router replay).",
                "position": 2023
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x10.png",
                "caption": "Figure 12:Evolution of metrics forMoE Base w/ R3experiment (based on Qwen3-30B-A3B-Base, with rollout router replay).",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x11.png",
                "caption": "Figure 13:Evolution of metrics forMoE Thinkingexperiment (based on Qwen3-30B-A3B).",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x12.png",
                "caption": "Figure 14:Evolution of metrics forDense Baseexperiment (based on Qwen3-8B-Base).",
                "position": 2047
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x13.png",
                "caption": "Figure 15:Evolution of metrics forMoE Base w/ LoRAexperiment (based on Qwen3-30B-A3B-Base, with LoRA).",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x14.png",
                "caption": "Figure 16:Evolution of metrics for baselines, DPPO with binary TV/KL approximation, and DPPO with Top-K (K=20) approximation under the same setting as MoE Base w/o R3.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2602.04879/x15.png",
                "caption": "Figure 17:Learning curve comparison of using ratio (PPO-Ratio) and TV divergence (DPPO-Binary-TV) for the trust region clipping.",
                "position": 2066
            }
        ]
    },
    {
        "header": "Appendix GMore Empirical Results",
        "images": []
    }
]