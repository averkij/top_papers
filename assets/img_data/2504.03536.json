[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03536/x1.png",
                "caption": "",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03536/x2.png",
                "caption": "Figure 2:Overall framework of the proposedHumanDreamer-X. The process begins by initializing a coarse 3DGS avatar using a reference image. A rendered video serves as a guide, providing geometric and appearance priors. Subsequently,HumanFixerperforms video restoration, wherein an attention modulation is employed to enhance video consistency. Throughout this process, the restored video is used to continuously update the 3DGS model, ultimately resulting in a refined 3DGS avatar.",
                "position": 156
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03536/x3.png",
                "caption": "Figure 3:The creation of the dataset for trainingHumanFixer. First, we use Blender to render scans and obtain the ground truth video. Next, we employ the frontal image and its corresponding SMPL prior to reconstruct a coarse 3DGS model, followed by rendering multi-view videos. This process yields paired video data for training.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2504.03536/x4.png",
                "caption": "Figure 4:Attention weights visualization. The left and right sides show the head 0 attention weights at the temporal self-attention stage for training on cyclic videos without and with an attention modulation, respectively. Brighter colors indicate higher weights.",
                "position": 272
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03536/x5.png",
                "caption": "Figure 5:Comparison of generation with SOTA methods. Note that PSHumanâ€™s training dataset contains all of the CustomHumans. Best viewed with zoom-in.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2504.03536/x6.png",
                "caption": "Figure 6:Comparison of 3D reconstruction with SOTA methods. Best viewed with zoom-in.",
                "position": 537
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]