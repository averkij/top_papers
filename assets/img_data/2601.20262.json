[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20262/img/scope.png",
                "caption": "Figure 1:Layer reduction strategies and targeted structures in the VLA domain.\nPrevious methods primarily reduce backbone depth only or dynamically skip layers at inference time.\nIn contrast, we propose a systematic knowledge distillation framework that jointly reduces the transformer depth of both the VLM backbone and the action head, which is especially effective forπ\\pi-like architectures where the action head mirrors the VLM backbone to receive conditioning information from all layers.",
                "position": 77
            }
        ]
    },
    {
        "header": "1INTRODUCTION",
        "images": []
    },
    {
        "header": "2RELATED WORKS",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20262/img/computation.png",
                "caption": "Figure 2:CUDA inference time as a function of transformer depth and visual token count. Measurements are obtained using theπ0.5\\pi_{0.5}model trained on LIBERO, evaluated on an H100 GPU (left) and Jetson Orin (right).",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2601.20262/img/pi05_similarity_and_success_drop_edited.png",
                "caption": "Figure 3:(Top) Feature similarity trend along noise levelτ\\tau. (Bottom) Layer sensitivity analysis ofπ0.5\\pi_{0.5}on the LIBERO benchmark. The bar chart shows the decrease in average success rate caused by skipping individual layers.",
                "position": 214
            }
        ]
    },
    {
        "header": "4Why Layer Skipping Is Insufficient",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20262/img/sr-along-layer.png",
                "caption": "Figure 4:Success rate (%) on LIBERO (Spatial, Object, Goal, and 10) as a function of the number of skipped transformer layers.",
                "position": 235
            }
        ]
    },
    {
        "header": "5Knowledge Distillation for Shallow Layers",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20262/img/diagram.png",
                "caption": "Figure 5:Shallow-π\\pireduces the transformer depth of the VLM backbone and action head via knowledge distillation, using three loss terms to match ground-truth actions, teacher outputs, and intermediate cross-attention between the backbone and action head.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2601.20262/img/experiment-outline-new.png",
                "caption": "Figure 6:Experiment task suites for Shallow-π\\pi. The benchmarks include scenarios requiring complex manipulation under dynamic scenes, as well as tasks involving articulated robot platforms with coordinated hand and torso movements. All real-world evaluations are performed on edge devices (Jetson Thor and Orin).",
                "position": 418
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20262/img/experiment-setup.png",
                "caption": "Figure 7:Degrees of freedom (DoFs) and camera configurations (green circles) for the robot platforms.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2601.20262/img/rby1-tasks.png",
                "caption": "Figure 8:Skill sequences required by the tasks of the hand-typed robot (RB-Y1).",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2601.20262/img/jerkness_comparison.png",
                "caption": "Figure 9:(Top) Flow matching loss vs training step. (Bottom) Samples of chunk to compare the motion quality in the real-world training set.",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2601.20262/img/open-loop-histogram.png",
                "caption": "Figure 10:Histogram of the per-frame average translation of the right-arm end effector in the training dataset.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2601.20262/img/openloop-failure.png",
                "caption": "Figure 11:Experimental snapshots illustrating open-loop failures of the teacher model, where the student model achieves better performance by reacting to observations more rapidly.",
                "position": 811
            }
        ]
    },
    {
        "header": "7CONCLUSIONS",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]