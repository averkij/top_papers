[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16669/x1.png",
                "caption": "Figure 1:Video answer (our VANS) versus text-only answer (Gemini) on a procedural question. Video answer provides an intuitive and customized response by demonstrating the action directly, while text-only answer falls short in clarity.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2511.16669/x2.png",
                "caption": "Figure 2:Data curation pipeline of VANS-Data-100K, which processes raw videos through shot splitting, clip selection, and QA generation to produce high-quality data for both procedural and predictive Video-Next-Event Prediction.",
                "position": 96
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VANS-Data-100K",
        "images": []
    },
    {
        "header": "4VANS",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16669/x3.png",
                "caption": "Figure 3:Overall architecture of VANS.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2511.16669/x4.png",
                "caption": "Figure 4:Comparison of standard GRPO with Joint-GRPO. While standard GRPO optimizes a single model at a time, our Joint-GRPO coordinates their optimization under a joint reward function.",
                "position": 176
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16669/x5.png",
                "caption": "Figure 5:Visual comparison on VNEP. Captions are color-coded:green(correct),red(incorrect),blue(semantically correct but visually unfriendly). Yellow boxes highlight key regions. Baselines often fail in event prediction or visual consistency. Our SFT model improves reasoning but retains errors like semantic hallucination (predicting non-existent inreview in Case 1) and action misalignment (“adding cheese” yields pouring in Case 2). Joint-GRPO addresses both issues, enhancing model capability (correctly identifying document relationships and maintaining character appearance in Case 1) and fine-grained alignment (“sprinkle cheese” matching the GT “shower” in Case 2).",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2511.16669/x6.png",
                "caption": "Figure 6:Visualization Results of ablation studies. Key regions are highlighted with yellow boxes: the left example shows degradation in the “mask removal” action completion withoutrt​1r_{t1}; the right example illustrates the loss of visual consistency withoutrv​2r_{v2}and semantic alignment (leading to static frames) withoutrc​2r_{c2}.",
                "position": 843
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Appendix ADetails of VANS-Data-100K",
        "images": []
    },
    {
        "header": "Appendix BDetails of Joint-GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16669/x7.png",
                "caption": "Figure 7:Illustration of our Joint-GRPO reward design. Top: For a Stage-1 case, we simulate three reasoning samples during GRPO training. The text-only reward fails to penalize Sample 2’s visual inconsistency, while the video-only reward fails to penalize Sample 1’s semantic error. Bottom: For a Stage-2 case, the video-only reward fails to penalize Sample 1’s semantic inaccuracy, while the consistency-only reward fails to penalize Sample 2’s visual inconsistency.",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2511.16669/x8.png",
                "caption": "Figure 8:Training curves of Joint-GRPO: (a) format reward (rfr_{f}) in Stage 1; (b) text fidelity reward (rt​1r_{t1}) in Stage 1; (c) video fidelity reward (rv​1r_{v1}) in Stage 1; (d) total reward in Stage 1; (e) thinking length evolution in Stage 1; (f) video fidelity reward (rv​2r_{v2}) in Stage 2; (g) semantic alignment reward (rc​2r_{c2}) in Stage 2; (h) total reward in Stage 2.",
                "position": 1034
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16669/x9.png",
                "caption": "Figure 9:Visual comparison results on UI2V-Bench.",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2511.16669/x10.png",
                "caption": "Figure 10:Multi-Future Prediction Results.",
                "position": 1097
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]