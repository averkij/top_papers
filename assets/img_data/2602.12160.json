[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12160/x1.png",
                "caption": "Figure 1:Showcase ofDreamID-Omni.DreamID-Omniseamlessly unifies reference-based audio-video generation (R2AV), video editing (RV2AV), and audio-driven video animation (RA2V).",
                "position": 124
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12160/x2.png",
                "caption": "Figure 2:Overview ofDreamID-Omniframework.We integrate reference-based generation (R2AV), editing (RV2AV), and animation (RA2V) using a Symmetric Conditional DiT trained via a multi-task progressive training strategy. Structured Caption and Syn-RoPE ensure robust dual-level disentanglement in multi-person scenarios.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x3.png",
                "caption": "Figure 3:Qualitative comparisonwith state-of-the-art (SOTA) methods on R2AV. Please zoom in for more details.",
                "position": 371
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12160/x4.png",
                "caption": "Figure 4:Qualitative comparisonwith SOTA methods on RV2AV. Please zoom in for more details.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x5.png",
                "caption": "Figure 5:Qualitative comparisonwith SOTA methods on RA2V. Please zoom in for more details.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x6.png",
                "caption": "Figure 6:Qualitative results of our ablation studies.(a) Ablation on our dual-level disentanglement design.\n(b) Ablation on multi-task progressive training.",
                "position": 682
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12160/x7.png",
                "caption": "Figure 7:Data construction pipeline.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x8.png",
                "caption": "Figure 8:MLLM system prompt for Structured Caption.",
                "position": 878
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x9.png",
                "caption": "Figure 9:MLLM system prompt for speaker confusion detection.",
                "position": 888
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x10.png",
                "caption": "Figure 10:Morequalitative results on R2AV I",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x11.png",
                "caption": "Figure 11:Morequalitative results on R2AV II",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x12.png",
                "caption": "Figure 12:Morequalitative results on RV2AV",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2602.12160/x13.png",
                "caption": "Figure 13:Morequalitative results on RA2V",
                "position": 995
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]