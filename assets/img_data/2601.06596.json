[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06596/pics/main.png",
                "caption": "Figure 1:We propose a methodology based on factorial analysis to quantitatively diagnose how manipulative prompts exploit LLMs optimized for preference alignment, shifting responses from truth-oriented correction to user-appeasing agreement. Our analysis reveals a truth-deference trade-off, demonstrating that advanced models may be more vulnerable to Preference-Undermining Attacks (PUA). Tailored defenses are necessary to mitigate these vulnerabilities.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.06596/x1.png",
                "caption": "(a)Factuality Effect Coefficients",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2601.06596/x1.png",
                "caption": "(a)Factuality Effect Coefficients",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2601.06596/x2.png",
                "caption": "(b)Deference Effect Coefficients",
                "position": 763
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExample Prompts for PUA-Style Dialogue Factors",
        "images": []
    }
]