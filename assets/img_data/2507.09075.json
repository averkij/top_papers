[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09075/x1.png",
                "caption": "Figure 1:Demonstrating performance gains on LiveCodeBench, achieved through test-time scaling by generating 10 solutions per problem and employing self-critique for selecting the final output. Self-critique led to the greatest performance boost in our finetuned model,OCR-2-32B.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Development ofOpenCodeReasoning-IIand LiveCodeBench-C++",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09075/x2.png",
                "caption": "Figure 2:Overview of theOpenCodeReasoning-IIdevelopment stages.",
                "position": 187
            }
        ]
    },
    {
        "header": "3A Simple Test-time Scaling Approach via Self-Critique",
        "images": []
    },
    {
        "header": "4Main Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09075/x3.png",
                "caption": "Table 2:Performance comparison of reasoning models on LiveCodeBench. Highlighted rows show our finetuned models’ performances. Bold indicates the highest performance. Python results are averaged across 64 runs, and C++ results across 16 runs.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2507.09075/x3.png",
                "caption": "Table 3:Performance comparison of reasoning models under test-time scaling setup. Highlighted rows show our finetuned models’ performances. The pass@1 scores are averaged over 10 runs. The performance gains with self-critique are highlighted in blue and bold values indicate the largest gains.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2507.09075/x3.png",
                "caption": "Table 4:Accuracy comparison of reasoning models on self-critique. Highlighted rows show our finetuned models’ performances. Bold indicates the best performance.",
                "position": 439
            }
        ]
    },
    {
        "header": "5Ablation and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09075/x3.png",
                "caption": "Figure 3:Performance gap betweenpass@1,pass@1|select@k, andpass@kunder test-time scaling - large number of samples drawn fromOCR-2-32B.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2507.09075/x4.png",
                "caption": "Figure 4:Impact of scaling up data from 25k to 1.4M samples inOpenCodeReasoning-II.",
                "position": 601
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFinal Output Selection using Critique",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09075/x5.png",
                "caption": "Figure 5:Differences in pass@1 scores between randomly selecting the final output vs. choosing right solution with shortest critique thinking, using OCR-2-32B on LiveCodeBench-Python.",
                "position": 1508
            }
        ]
    },
    {
        "header": "Appendix BEvaluating Critique LLM Accuracy in Judging Code Solutions",
        "images": []
    }
]