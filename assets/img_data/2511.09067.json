[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09067/x1.png",
                "caption": "Figure 1:Multi-dimensional critique evaluation inMM-Critic. Basic critique includes binary correctness and textual feedback (Critique Accuracy,Critique Score); correction and comparative critique correspond toCorrection Critique ScoreandPreference Accuracy, respectively.",
                "position": 93
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3MM-CriticConstruction",
        "images": []
    },
    {
        "header": "4Evaluation Metric",
        "images": []
    },
    {
        "header": "5Evaluation and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09067/x2.png",
                "caption": "Figure 2:Scaling law onACCcritic\\mathrm{ACC_{critic}}across models. Note that the parameter sizes of all closed-source LMMs are estimated, as their exact values are not publicly available. However, the relative scale among them is preserved — for example, Gemini-2.5-flash is known to be smaller than Gemini-2.5-pro.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2511.09067/x3.png",
                "caption": "Figure 3:The distribution of critique scores across responses of different quality levels, where low-, medium-, and high-quality correspond to labeled response quality ranges of[0,4][0,4],[5,7][5,7], and[8,10][8,10], respectively.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2511.09067/x4.png",
                "caption": "Figure 4:The relationship between the average length of textual critiques and critique scores across models.",
                "position": 868
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset and LMMs Information.",
        "images": []
    },
    {
        "header": "Appendix BPrompts and Scoring Rubric Checklist",
        "images": []
    },
    {
        "header": "Appendix CExperimental Results",
        "images": []
    },
    {
        "header": "Appendix DAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.09067/x5.png",
                "caption": "Figure 10:An example of a visual mathematical reasoning task, where the response is clearly incorrect, demonstrates that the o4-mini model provides an accurate judgment along with a comprehensive textual critique. When evaluating its critique score, it is evident that the presence of the reference critique effectively guides the judge model to assign a high-quality score of 9, surpassing even the reference critique in some aspects.",
                "position": 4568
            },
            {
                "img": "https://arxiv.org/html/2511.09067/x6.png",
                "caption": "Figure 11:An example from the brand logo recognition and elaboration task, where the critique is generated by Qwen2.5-vl-32b. The model produces a detailed, step-by-step reasoning critique. When scored by the judge model, it explicitly explains its preference for lengthier, somewhat redundant reasoning—highlighted in bold red—demonstrating a bias toward richer textual justifications.",
                "position": 4571
            },
            {
                "img": "https://arxiv.org/html/2511.09067/x7.png",
                "caption": "Figure 12:An example from the StackOverflow debug QA task, where the critique is generated by claude-3.7-sonnet. The critique provides a complete analysis and exactly points out the original response’s error. Compared with the reference critique, the judge model accurately evaluates that Claude-3.7-sonnet, as a well-known pioneer model of coding, outperforms the reference critique.",
                "position": 4574
            },
            {
                "img": "https://arxiv.org/html/2511.09067/x8.png",
                "caption": "Figure 13:An example from the GUI agent application task, where the critique is generated by Genimi-2.5-pro. It can accurately tell the correctness of the model’s response, and the textual critique score is slightly below the anchored reference critique score (e.g., 8), where the judge model provides a reasonable explanation (marked in red).",
                "position": 4577
            },
            {
                "img": "https://arxiv.org/html/2511.09067/x9.png",
                "caption": "Figure 14:An example of comparative critique from the face keypoint detection task, where the critique is generated by Pixtral-large(24-11). The original responses are scored for their response quality scores by the annotator model (GPT-4o). As a (low, medium) pairwise comparison, it is easy to distinguish the better one with a high-performance model.",
                "position": 4580
            }
        ]
    },
    {
        "header": "Appendix ECase study",
        "images": []
    }
]