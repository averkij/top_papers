[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/teaser.jpg",
                "caption": "",
                "position": 185
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3HG-DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/x1.png",
                "caption": "Figure 2:Three-stage training of HG-DPO.It progressively enhances the model窶冱 human image generation capabilities.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/dataset_easy_stage_new.jpg",
                "caption": "Figure 3:DPO Dataset for the easy stage.In the upper figure,搨溟摧ｼsubscript搨溟摧ｼ\\mathcal{D}_{\\mathbb{E}}caligraphic_D start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPT, constructed with AI rather than human feedback, shows winning images with superior features over losing images. A user study in the lower figure confirms this outcome.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/baseline.jpg",
                "caption": "Figure 4:Qualitative comparison with the previous methods.HG-DPO generates high-quality human images with more realistic compositions and poses, providing superior text alignment compared to the prior methods.",
                "position": 522
            }
        ]
    },
    {
        "header": "4Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/progress.jpg",
                "caption": "Figure 5:Qualitative progress.ﾏｵb竅｢a竅｢s竅｢esubscriptitalic-ﾏｵ搗条搗酒搗搗箪\epsilon_{base}italic_ﾏｵ start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPTevolves as it progresses through each stage of the HG-DPO pipeline up to the hard stage.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/text_encoder.jpg",
                "caption": "Figure 6:Qualitative results by the enhanced text encoder.With the improved text encoder, Hard (ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT) + TE can achieve the enhanced image-text alignment, retaining the image quality ofﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT.",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/necessity.jpg",
                "caption": "Figure 7:Qualitative results illustrating the importance of each stage.To generate high-quality images like the one labeled as Hard (ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT), each stage of the HG-DPO pipeline is essential.",
                "position": 924
            }
        ]
    },
    {
        "header": "5Analysis on HG-DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/easy_stage_ablation.jpg",
                "caption": "Figure 8:Qualitative analysis of the easy stage.In the easy stage, only the model trained with both搨溟摧ｼsubscript搨溟摧ｼ\\mathcal{D}_{\\mathbb{E}}caligraphic_D start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPTand邃痴tatsubscript邃痴tat\\mathcal{L}_{\\textit{stat}}caligraphic_L start_POSTSUBSCRIPT stat end_POSTSUBSCRIPT, namelyN>2+邃痴tat搗2subscript邃痴tatN>2+\\mathcal{L}_{\\textit{stat}}italic_N > 2 + caligraphic_L start_POSTSUBSCRIPT stat end_POSTSUBSCRIPT, produces images without distortions in pose and color.",
                "position": 1118
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "AAdditional Results of HG-DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_teaser.jpg",
                "caption": "Figure 9:Qualitative results of HG-DPO.HG-DPO is capable of effectively generating high-quality human images that encompass a wide range of actions, appearances, group sizes, and backgrounds.",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_base_vs_hg_dpo.jpg",
                "caption": "Figure 10:Qualitative enhancements in text-to-image generation through HG-DPO.HG-DPO improves the base model窶冱 capability to generate human images with more realistic poses and anatomies that align more accurately with the given prompt.",
                "position": 1632
            }
        ]
    },
    {
        "header": "BAdditional Analysis on the Easy Stage",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/x2.png",
                "caption": "Figure 11:User studies comparing HG-DPO and baselines.HG-DPO demonstrates superior performance compared to the base model and previous approaches in human evaluation. Participants were tasked with choosing the image that exhibited higher realism and better alignment with the given prompt from the outputs of the two models. The detailed process for conducting the user study is described in SectionF.5.",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_pt2i_new.jpg",
                "caption": "Figure 12:Qualitative advancements achieved through in personalized text-to-image (PT2I) generation through HG-DPO.HG-DPO improves the base model窶冱 capability to generate human images with more realistic poses and anatomies that align more accurately with the given prompt, and these improvements extend to PT2I generation. As a result, we can produce high-quality images that accurately reflect the identity of the concept image shown in the bottom left.",
                "position": 1659
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_base_vs_easy.jpg",
                "caption": "Figure 13:Qualitative advancements achieved through the easy stage.We enhance the base model through the easy stage to generate images that better align with human preferences. Specifically, the model is improved to produce images with undistorted poses and anatomies and stronger alignment with the given prompts.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_dataset_with_pickscore_rank.jpg",
                "caption": "Figure 14:Visualization of the image pool.This figure shows the image pool with the size of 20 for the prompt in the leftmost column. The column labeled as 1st contains images with the highest PickScore, while the column labeled as 20th contains images with the 20th highest PickScore, i.e., the lowest PickScore, in the image pool. By selecting the image with the highest PickScore from this image pool as the winning image and the image with the 20th highest PickScore as the losing image, we magnify the semantic differences between the winning and losing images.",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_wo_stat_vs_w_stat.jpg",
                "caption": "Figure 15:Qualitative enhancements achieved with the statistics matching loss.The statistics matching loss effectively removes the color shift artifacts, leading to the generation of significantly more natural images.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_easy_vs_normal.jpg",
                "caption": "Figure 16:Qualitative advancements achieved through the normal stage.ﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPT, derived by refiningﾏｵ摧ｼsubscriptitalic-ﾏｵ摧ｼ\\epsilon_{\\mathbb{E}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPTthrough the normal stage, generates images with morerealistic compositions and posescompared toﾏｵ摧ｼsubscriptitalic-ﾏｵ摧ｼ\\epsilon_{\\mathbb{E}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPT.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_intermediate_domains.jpg",
                "caption": "Figure 17:Visualization of the intermediate domains.The images labeledt1subscript搗｡1t_{1}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTtotTsubscript搗｡搗t_{T}italic_t start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTare reconstructed from real images using theSDReconoperation. The image labeled generated image is produced via text-to-image generation based on the caption of the real image. As the labels progress towardtTsubscript搗｡搗t_{T}italic_t start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, SDRecon applies increasingly stronger noise to the real image, causing it to lose more of its original characteristics and resemble the generated image more closely. For the normal stage, we select four intermediate domains,t4subscript搗｡4t_{4}italic_t start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTtot7subscript搗｡7t_{7}italic_t start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT, as candidates for winning images, because they maintain the realistic pose of the real image while adopting the fine details typical of the generated image. The image with the highest PickScore among these candidates is chosen as the winning image.",
                "position": 1951
            }
        ]
    },
    {
        "header": "CAdditional Analysis on the Normal Stage",
        "images": []
    },
    {
        "header": "DAdditional Analysis on the Hard Stage",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_eyes_3.jpg",
                "caption": "Figure 18:Qualitative advancements achieved through the hard stage.ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generates finer details, especially more realistic depictions of theeyes, compared toﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTas shown in the red box.",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_gaze_1.jpg",
                "caption": "Figure 19:Qualitative advancements achieved through the hard stage.ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generates finer details, especially more realistic depictions of thegaze, compared toﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTas shown in the red box.",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_tooth_1.jpg",
                "caption": "Figure 20:Qualitative advancements achieved through the hard stage.ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generates finer details, especially more realistic depictions of thelips, compared toﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTas shown in the red box.",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_sharpness_1.jpg",
                "caption": "Figure 21:Qualitative advancements achieved through the hard stage.ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generatessharperimages with improved fine details, particularly exhibiting more vivid and realisticshading, compared toﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPT.",
                "position": 2027
            }
        ]
    },
    {
        "header": "ELimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/x3.png",
                "caption": "Figure 22:User study comparing a model trained up to the normal stage (ﾏｵ邃不ubscriptitalic-ﾏｵ邃表\epsilon_{\\mathbb{N}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPT) with one trained through the hard stage (ﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT).Participants were tasked with choosing the image that exhibited higher realism and better alignment with the given prompt from the outputs of the two models. The model trained through the hard stage achieves higher human evaluation scores due to its ability to generate finer details with greater realism compared to the model trained only up to the normal stage.",
                "position": 2121
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_hard_vs_hard_te.jpg",
                "caption": "Figure 23:Qualitative advancements achieved through the text encoder enhancement.By training the text encoder through the easy stage and incorporating it withﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPTduring inference, we achieve improved image-text alignment compared to usingﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPTalone. Moreover, the use of the enhanced text encoder does not compromise the image quality produced byﾏｵ邃行ubscriptitalic-ﾏｵ邃構\epsilon_{\\mathbb{H}}italic_ﾏｵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT.",
                "position": 2124
            }
        ]
    },
    {
        "header": "FImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_finger.jpg",
                "caption": "Figure 24:Qualitative results illustrating the limitations of HG-DPO.While HG-DPO significantly improves the base model in generating more realistic human images, it still struggles to accurately synthesize fingers.",
                "position": 2198
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_user_study_screenshot.jpg",
                "caption": "Figure 25:User study interface.We conduct the user study by providing a prompt and two images,\nasking users to choose the one that appeared more realistic considering the given prompt.",
                "position": 2224
            }
        ]
    },
    {
        "header": "GBroader Impacts",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]