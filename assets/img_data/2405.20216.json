[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/teaser.jpg",
                "caption": "",
                "position": 185
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3HG-DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/x1.png",
                "caption": "Figure 2:Three-stage training of HG-DPO.It progressively enhances the model’s human image generation capabilities.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/dataset_easy_stage_new.jpg",
                "caption": "Figure 3:DPO Dataset for the easy stage.In the upper figure,𝒟𝔼subscript𝒟𝔼\\mathcal{D}_{\\mathbb{E}}caligraphic_D start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPT, constructed with AI rather than human feedback, shows winning images with superior features over losing images. A user study in the lower figure confirms this outcome.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/baseline.jpg",
                "caption": "Figure 4:Qualitative comparison with the previous methods.HG-DPO generates high-quality human images with more realistic compositions and poses, providing superior text alignment compared to the prior methods.",
                "position": 522
            }
        ]
    },
    {
        "header": "4Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/progress.jpg",
                "caption": "Figure 5:Qualitative progress.ϵb⁢a⁢s⁢esubscriptitalic-ϵ𝑏𝑎𝑠𝑒\\epsilon_{base}italic_ϵ start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPTevolves as it progresses through each stage of the HG-DPO pipeline up to the hard stage.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/text_encoder.jpg",
                "caption": "Figure 6:Qualitative results by the enhanced text encoder.With the improved text encoder, Hard (ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT) + TE can achieve the enhanced image-text alignment, retaining the image quality ofϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT.",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/necessity.jpg",
                "caption": "Figure 7:Qualitative results illustrating the importance of each stage.To generate high-quality images like the one labeled as Hard (ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT), each stage of the HG-DPO pipeline is essential.",
                "position": 924
            }
        ]
    },
    {
        "header": "5Analysis on HG-DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/easy_stage_ablation.jpg",
                "caption": "Figure 8:Qualitative analysis of the easy stage.In the easy stage, only the model trained with both𝒟𝔼subscript𝒟𝔼\\mathcal{D}_{\\mathbb{E}}caligraphic_D start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPTandℒstatsubscriptℒstat\\mathcal{L}_{\\textit{stat}}caligraphic_L start_POSTSUBSCRIPT stat end_POSTSUBSCRIPT, namelyN>2+ℒstat𝑁2subscriptℒstatN>2+\\mathcal{L}_{\\textit{stat}}italic_N > 2 + caligraphic_L start_POSTSUBSCRIPT stat end_POSTSUBSCRIPT, produces images without distortions in pose and color.",
                "position": 1118
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "AAdditional Results of HG-DPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_teaser.jpg",
                "caption": "Figure 9:Qualitative results of HG-DPO.HG-DPO is capable of effectively generating high-quality human images that encompass a wide range of actions, appearances, group sizes, and backgrounds.",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_base_vs_hg_dpo.jpg",
                "caption": "Figure 10:Qualitative enhancements in text-to-image generation through HG-DPO.HG-DPO improves the base model’s capability to generate human images with more realistic poses and anatomies that align more accurately with the given prompt.",
                "position": 1632
            }
        ]
    },
    {
        "header": "BAdditional Analysis on the Easy Stage",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/x2.png",
                "caption": "Figure 11:User studies comparing HG-DPO and baselines.HG-DPO demonstrates superior performance compared to the base model and previous approaches in human evaluation. Participants were tasked with choosing the image that exhibited higher realism and better alignment with the given prompt from the outputs of the two models. The detailed process for conducting the user study is described in SectionF.5.",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_pt2i_new.jpg",
                "caption": "Figure 12:Qualitative advancements achieved through in personalized text-to-image (PT2I) generation through HG-DPO.HG-DPO improves the base model’s capability to generate human images with more realistic poses and anatomies that align more accurately with the given prompt, and these improvements extend to PT2I generation. As a result, we can produce high-quality images that accurately reflect the identity of the concept image shown in the bottom left.",
                "position": 1659
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_base_vs_easy.jpg",
                "caption": "Figure 13:Qualitative advancements achieved through the easy stage.We enhance the base model through the easy stage to generate images that better align with human preferences. Specifically, the model is improved to produce images with undistorted poses and anatomies and stronger alignment with the given prompts.",
                "position": 1719
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_dataset_with_pickscore_rank.jpg",
                "caption": "Figure 14:Visualization of the image pool.This figure shows the image pool with the size of 20 for the prompt in the leftmost column. The column labeled as 1st contains images with the highest PickScore, while the column labeled as 20th contains images with the 20th highest PickScore, i.e., the lowest PickScore, in the image pool. By selecting the image with the highest PickScore from this image pool as the winning image and the image with the 20th highest PickScore as the losing image, we magnify the semantic differences between the winning and losing images.",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_wo_stat_vs_w_stat.jpg",
                "caption": "Figure 15:Qualitative enhancements achieved with the statistics matching loss.The statistics matching loss effectively removes the color shift artifacts, leading to the generation of significantly more natural images.",
                "position": 1945
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_easy_vs_normal.jpg",
                "caption": "Figure 16:Qualitative advancements achieved through the normal stage.ϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPT, derived by refiningϵ𝔼subscriptitalic-ϵ𝔼\\epsilon_{\\mathbb{E}}italic_ϵ start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPTthrough the normal stage, generates images with morerealistic compositions and posescompared toϵ𝔼subscriptitalic-ϵ𝔼\\epsilon_{\\mathbb{E}}italic_ϵ start_POSTSUBSCRIPT blackboard_E end_POSTSUBSCRIPT.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_intermediate_domains.jpg",
                "caption": "Figure 17:Visualization of the intermediate domains.The images labeledt1subscript𝑡1t_{1}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTtotTsubscript𝑡𝑇t_{T}italic_t start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTare reconstructed from real images using theSDReconoperation. The image labeled generated image is produced via text-to-image generation based on the caption of the real image. As the labels progress towardtTsubscript𝑡𝑇t_{T}italic_t start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, SDRecon applies increasingly stronger noise to the real image, causing it to lose more of its original characteristics and resemble the generated image more closely. For the normal stage, we select four intermediate domains,t4subscript𝑡4t_{4}italic_t start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTtot7subscript𝑡7t_{7}italic_t start_POSTSUBSCRIPT 7 end_POSTSUBSCRIPT, as candidates for winning images, because they maintain the realistic pose of the real image while adopting the fine details typical of the generated image. The image with the highest PickScore among these candidates is chosen as the winning image.",
                "position": 1951
            }
        ]
    },
    {
        "header": "CAdditional Analysis on the Normal Stage",
        "images": []
    },
    {
        "header": "DAdditional Analysis on the Hard Stage",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_eyes_3.jpg",
                "caption": "Figure 18:Qualitative advancements achieved through the hard stage.ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generates finer details, especially more realistic depictions of theeyes, compared toϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTas shown in the red box.",
                "position": 2018
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_gaze_1.jpg",
                "caption": "Figure 19:Qualitative advancements achieved through the hard stage.ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generates finer details, especially more realistic depictions of thegaze, compared toϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTas shown in the red box.",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_tooth_1.jpg",
                "caption": "Figure 20:Qualitative advancements achieved through the hard stage.ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generates finer details, especially more realistic depictions of thelips, compared toϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTas shown in the red box.",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_nomal_vs_hard_sharpness_1.jpg",
                "caption": "Figure 21:Qualitative advancements achieved through the hard stage.ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT, derived by refiningϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPTthrough the hard stage, generatessharperimages with improved fine details, particularly exhibiting more vivid and realisticshading, compared toϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPT.",
                "position": 2027
            }
        ]
    },
    {
        "header": "ELimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/x3.png",
                "caption": "Figure 22:User study comparing a model trained up to the normal stage (ϵℕsubscriptitalic-ϵℕ\\epsilon_{\\mathbb{N}}italic_ϵ start_POSTSUBSCRIPT blackboard_N end_POSTSUBSCRIPT) with one trained through the hard stage (ϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT).Participants were tasked with choosing the image that exhibited higher realism and better alignment with the given prompt from the outputs of the two models. The model trained through the hard stage achieves higher human evaluation scores due to its ability to generate finer details with greater realism compared to the model trained only up to the normal stage.",
                "position": 2121
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_hard_vs_hard_te.jpg",
                "caption": "Figure 23:Qualitative advancements achieved through the text encoder enhancement.By training the text encoder through the easy stage and incorporating it withϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPTduring inference, we achieve improved image-text alignment compared to usingϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPTalone. Moreover, the use of the enhanced text encoder does not compromise the image quality produced byϵℍsubscriptitalic-ϵℍ\\epsilon_{\\mathbb{H}}italic_ϵ start_POSTSUBSCRIPT blackboard_H end_POSTSUBSCRIPT.",
                "position": 2124
            }
        ]
    },
    {
        "header": "FImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_finger.jpg",
                "caption": "Figure 24:Qualitative results illustrating the limitations of HG-DPO.While HG-DPO significantly improves the base model in generating more realistic human images, it still struggles to accurately synthesize fingers.",
                "position": 2198
            },
            {
                "img": "https://arxiv.org/html/2405.20216/extracted/6321445/img/sm_user_study_screenshot.jpg",
                "caption": "Figure 25:User study interface.We conduct the user study by providing a prompt and two images,\nasking users to choose the one that appeared more realistic considering the given prompt.",
                "position": 2224
            }
        ]
    },
    {
        "header": "GBroader Impacts",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]