[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11236/figure/bot5.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11236/x1.png",
                "caption": "Figure 1:Data cleaning and preprocessing pipeline to construct the UniACT-dataset.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2602.11236/x2.png",
                "caption": "Figure 2:Overview of the integrated UniACT-dataset,which contains more than six million trajectories in 9500+ hours with 20+ unique robot embodiments.",
                "position": 530
            }
        ]
    },
    {
        "header": "3The ABot-M0 Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11236/x3.png",
                "caption": "Figure 3:Model architecture of ABot-M0.We employ a two-component architecture consisting of a VLM and an action expert. In addition, we utilize action manifold learning to predict actions with two-stage training paradigm. We then carefully select VLM features and further introduce an optional 3D module and to enhance spatial reasoning.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2602.11236/x4.png",
                "caption": "Figure 4:Action Manifold. (a) We posit that a meaningful action sequence is a highly structured entity residing on a low-dimensional action manifold. The conventional prediction targets of noise or velocity are inherently high-dimensional and off-manifold, which increase the burden of model learning and lead to unreasonble action. (b) We propose to predict the action directly rather than velocity, which enables the model to focus on learning the intrinsic structure and semantics of actions.",
                "position": 661
            }
        ]
    },
    {
        "header": "4Pre-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11236/x5.png",
                "caption": "Figure 5:Embodiment distribution under different sampling strategies.Data are drawn from OXE[32], AgiBot-Beta[13], and RoboCoin[47]. The mixture receipe of single-arm data from OXE follows OpenVLA[20]and is fixed among these three strategies. We compare (a) Trajectory-Uniform, (b) Task-Uniform, and (c) Embodiment-Uniform sampling strategies and show the data distribution among different dataset and embodiments.",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2602.11236/figure/sampling_ratio.png",
                "caption": "Figure 6:Skill Sampling Characteristics.Skill sampling behavior under different bimanual sampling strategies: (a) rank–probability distribution, (b) Lorenz curves of skill sampling probabilities, and (c) Coverage@T, measuring the number of unique skills covered as sampling progresses.",
                "position": 713
            },
            {
                "img": "https://arxiv.org/html/2602.11236/figure/sampling_ratio.png",
                "caption": "Figure 6:Skill Sampling Characteristics.Skill sampling behavior under different bimanual sampling strategies: (a) rank–probability distribution, (b) Lorenz curves of skill sampling probabilities, and (c) Coverage@T, measuring the number of unique skills covered as sampling progresses.",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2602.11236/figure/embodiment_mae.png",
                "caption": "Figure 7:Validation Performance Across Bimanual Embodiments on RoboCoin.",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2602.11236/figure/dataset_mae.png",
                "caption": "Figure 8:Dataset-Wise Validation Performance Under Different Sampling Strategies.",
                "position": 743
            },
            {
                "img": "https://arxiv.org/html/2602.11236/figure/dataset_mae.png",
                "caption": "Figure 8:Dataset-Wise Validation Performance Under Different Sampling Strategies.",
                "position": 745
            }
        ]
    },
    {
        "header": "5Perception to Action",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11236/x6.png",
                "caption": "Figure 9:VLM Feature Interaction.Following pre-training on robotics data, we feed an action expert with either the VLM’s raw hidden features or an additional action query, extracted from its final, intermediate, or multiple layers.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2602.11236/x7.png",
                "caption": "Figure 10:3D Information Injection.Left: pipeline for fusing VLM and 3D features. Right: three fusion strategies.",
                "position": 857
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": []
    },
    {
        "header": "7Future Work & Conclusion",
        "images": []
    },
    {
        "header": "8Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "9Appendix",
        "images": []
    }
]