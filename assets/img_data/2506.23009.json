[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23009/x1.png",
                "caption": "",
                "position": 249
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23009/x2.png",
                "caption": "Figure 1:Data generation and model workflow in Musiğ•ğ•\\mathbb{X}blackboard_XQA. Music metadata is sampled and rendered into sheet images via MusiXTeX, with QA pairs generated from templates. The resulting data is used to train and evaluate MLLMs on visual music understanding tasks.",
                "position": 265
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Musiğ•ğ•\\mathbb{X}blackboard_XQAÂ Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23009/x3.png",
                "caption": "Figure 2:Data distribution of the Musiğ•ğ•\\mathbb{X}blackboard_XQAÂ dataset.Left: Boxplot showing the distribution of the number of notes and bars per image. Each box represents the inter-quartile range (IQR), covering the middle50%percent5050\\%50 %of the data. The horizontal line inside each box indicates the median document length, while the whiskers extend to the minimum and maximum values within1.51.51.51.5times the IQR.Right: Distribution of scales in the dataset. The inner circle groups enharmonic equivalent and relative scales according to the circle of fifths[35]. These scales share the same seven pitches but differ in accidentals (e.g., Câ™¯â™¯\\sharpâ™¯vs. Dâ™­â™­\\flatâ™­) or scale type (e.g., C major vs. A minor). The outer circle represents root of scales, with minor scales labeled using a lowercase â€™mâ€™.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2506.23009/x4.png",
                "caption": "Figure 3:Example of key elements in a music sheet.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2506.23009/x5.png",
                "caption": "Figure 4:Example of Visual Question Answering (VQA) tasks for music sheet understanding, covering OCR and OMR-based information extraction, layout understanding, and chord estimation.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2506.23009/x6.png",
                "caption": "Table 1:Quantitative comparison of six methods on the Musiğ•ğ•\\mathbb{X}blackboard_XQAÂ test split. The table includes two open-source models evaluated in a zero-shot setting, and the proprietary GPT-4o model under three inference variants: zero-shot, retrieval-augmented generation (RAG), and RAG with oracle OMR results. The two Phi-3-MusiX variants are finetuned on MusiXQA using JSON andkern+representations.",
                "position": 445
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.23009/x6.png",
                "caption": "Figure 5:Training loss and gradient norm curves for models trained withkern+and JSON formats.",
                "position": 668
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AScale Details",
        "images": []
    },
    {
        "header": "Appendix BSystem Prompts",
        "images": []
    }
]