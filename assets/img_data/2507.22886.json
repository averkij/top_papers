[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22886/x1.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22886/x2.png",
                "caption": "Figure 2:Comparison of (a) Ref-AVS[68]and (b) our proposed OmniAVS. In Ref-AVS, expressions mainly focus on surface-level sound properties, while OmniAVS demands deeper understanding and reasoning about sound content, enabling complex reasoning like identifying potential illness from coughing sounds.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmark: OmniAVS",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22886/x3.png",
                "caption": "Table 2:More dataset statistics.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2507.22886/x3.png",
                "caption": "Figure 3:Distribution of expression types. SI:sound+image.",
                "position": 426
            }
        ]
    },
    {
        "header": "4Method: OISA",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22886/x4.png",
                "caption": "Figure 4:Overview of the proposed methodOISA.\nVision/audio encoders and text embedding are omitted for clarity.",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2507.22886/x5.png",
                "caption": "Figure 5:Comparision of different mask decoder forms.",
                "position": 525
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22886/Figs/concat.png",
                "caption": "Table 3:Ablation study onaudio-visual fusion. AVI means Audio-Visual Interleaving. TVQA Split represents the performance evaluated on samples collected from TVQA[34]. These samples demand precise audio-visual alignment and feature a substantial amount of dialogue. See an example inFigure6(a).",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2507.22886/Figs/ws.png",
                "caption": "",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2507.22886/Figs/avi.png",
                "caption": "",
                "position": 609
            },
            {
                "img": "https://arxiv.org/html/2507.22886/Figs/avi+concat.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2507.22886/x6.png",
                "caption": "Figure 6:Success and failure cases of OISA-1B.",
                "position": 838
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]