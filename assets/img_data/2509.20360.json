[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20360/x1.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20360/x2.png",
                "caption": "Figure 2:Overview of EditVerse. We design a unified framework for image and video editing and generation, which processes text and vision inputs into a unified sequence.\nThe right part of the figure shows our positional embedding design.\nThis framework leverages full self-attention to facilitate robust in-context learning and effective knowledge transfer among modalities.",
                "position": 177
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x3.png",
                "caption": "Figure 3:Examples for the interleaved text and vision pattern. EditVerse is capable of processing image and video inputs and outputs of arbitrary resolution, duration, and sequential positions.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Data Pipeline",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20360/x4.png",
                "caption": "Figure 4:Examples from the proposed EditVerseBench. EditVerseBench includes200200editing pairs, evenly distributed across 20 editing categories as well as horizontal and vertical orientations.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x5.png",
                "caption": "Table 2:Quantitative comparison on EditVerseBench. For open-source research models, we compare two training-free methods (TokenFlow and STDF), one first-frame propagation method (Señorita-2M), and one instruction-guided video editing method (InsV2V). Best results are highlighted inbold. We also provide the results of a commercial model, Runway Aleph. While EditVerse lags Runway Aleph in generation quality due to base model differences, our proposed method EditVerse surpasses it in editing faithfulness (via VLM evaluation on editing quality), aligning better with human judgment that is further validated by user studies shown in Figure5.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x5.png",
                "caption": "Figure 5:User study on EditVerseBench.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x6.png",
                "caption": "Figure 6:Visualization of EditVerse and other video editing methods. EditVerse shows stronger context preservation and edit faithfulness. Complete comparisons are in the Appendix.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x7.png",
                "caption": "Figure 7:Compare EditVerse generated results with ground truth.Results show EditVerse can surpass ground-truth data quality by extracting knowledge from image and video generation data.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x8.png",
                "caption": "Figure 8:Visualization of ablation on training data.Image data plays a critical role.",
                "position": 686
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20360/x9.png",
                "caption": "Table 9:Quantitative comparison on V2VBench(Sun et al.,2024).Methods are grouped into three categories: (i) Network and Training Paradigm, (ii) Attention Feature Injection, and (iii) Diffusion Latent Manipulation. Local best are inbold. Global best areunderlined.",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x9.png",
                "caption": "Table 10:Detailed Statistics of the training datasets.We combine high-quality open-source datasets, internal datasets, and EditVerse datasets for unified training. This table presents the dataset name, sample counts, training ratios, and key details for each dataset.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2509.20360/x9.png",
                "caption": "Figure 9:Failure case examples of EditVerse. (a) The model fails to add object (treasure chest) at the correct position (at the man’s feet). (b) Generation of blurry artifacts within the edited region.",
                "position": 3334
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]