[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and related work",
        "images": []
    },
    {
        "header": "3Examining strong MIAs in realistic settings for pre-trained LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x1.png",
                "caption": "Figure 1:LiRA with different numbers of reference models.We attack a 140M-parameter model trained on 7M examples.\nAs reference models increase, LiRA‚Äôs performance improves (measured withROCROC\\mathrm{ROC}roman_ROCAUCAUC\\mathrm{AUC}roman_AUC).\nHowever, there are diminishing returns:AUCAUC\\mathrm{AUC}roman_AUCis effectively unchanged from 128 to 256 reference models.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x2.png",
                "caption": "(a)ROCROC\\mathrm{ROC}roman_ROCfor 6 Chinchilla-optimal models (1 epoch).",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x2.png",
                "caption": "(a)ROCROC\\mathrm{ROC}roman_ROCfor 6 Chinchilla-optimal models (1 epoch).",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x3.png",
                "caption": "(b)TPRTPR\\mathrm{TPR}roman_TPRat fixedFPRFPR\\mathrm{FPR}roman_FPRfor different model sizes.",
                "position": 394
            }
        ]
    },
    {
        "header": "4Investigating the limits of strong attacks",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x4.png",
                "caption": "(a)44M model, split dataset in half and train for 2 epochs, or train on the entire dataset for 1 epoch.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x4.png",
                "caption": "(a)44M model, split dataset in half and train for 2 epochs, or train on the entire dataset for 1 epoch.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x5.png",
                "caption": "(b)140M model, training for 10 epochs.",
                "position": 460
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x6.png",
                "caption": "(a)140M model, trained on various dataset sizes for a single epoch.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x6.png",
                "caption": "(a)140M model, trained on various dataset sizes for a single epoch.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x7.png",
                "caption": "(b)Various model sizes on a fixed-size dataset (223superscript2232^{23}2 start_POSTSUPERSCRIPT 23 end_POSTSUPERSCRIPTsamples) for a single epoch.",
                "position": 500
            }
        ]
    },
    {
        "header": "5Analyzing sample vulnerability to membership inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/extracted/6474347/figures/mia_score_over_train_steps.png",
                "caption": "(a)Evolution of per-sample MIA success for member samples over training steps.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2505.18773/extracted/6474347/figures/mia_score_over_train_steps.png",
                "caption": "(a)Evolution of per-sample MIA success for member samples over training steps.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x8.png",
                "caption": "(b)Token-length distributions for samples that are least and most vulnerable to MIA, and samples for which MIA has difficulty predicting membership.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x9.png",
                "caption": "Figure 6:We plot the 1,000 samples predicted most strongly as a member of training by LiRA for the Chinchilla-optimal 140M model trained for 1 epoch (which contains approx. 7M training samples). We plot samples that are members (713) of training inblueand non-members inorange(287). We plot the negative log-probability of (the first 100 tokens of) each of these samples from which we can derive extraction rates according to(n,p)ùëõùëù(n,p)( italic_n , italic_p )-discoverable extraction metric introduced byHayes et¬†al. (2025).",
                "position": 585
            }
        ]
    },
    {
        "header": "6Conclusion and future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparing membership inference attacks and signals",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x10.png",
                "caption": "Figure 7:Influence of signal type on MIA Performance.We plotROCROC\\mathrm{ROC}roman_ROCcurves that compare the efficacy of using model logits (AUCAUC\\mathrm{AUC}roman_AUC0.576) versus model loss (AUCAUC\\mathrm{AUC}roman_AUC0.678) as signals for membership inference with LiRA.\nThe results indicate that, in this setting, the loss provides a stronger signal for distinguishing members from non-members.",
                "position": 1589
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x11.png",
                "caption": "(a)LiRA",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x11.png",
                "caption": "(a)LiRA",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x12.png",
                "caption": "(b)RMIA",
                "position": 1630
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x13.png",
                "caption": "Figure 9:Comparing performance of LiRA and RMIA with an increasing number of reference models (c.f.Figure8).We plot MIAROCROC\\mathrm{ROC}roman_ROCAUCAUC\\mathrm{AUC}roman_AUCachieved by both attack methodologies for an increasing number of reference models.\nAs the number of reference models increases, LiRA‚Äôs performance continues to improve, while RMIA‚Äôs gains saturate, making LiRA the overall stronger attack.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x14.png",
                "caption": "Figure 10:Comparing performance of LiRA, RMIA (simple) and RMIA on a 10M parameter model trained for 1 epoch with a training set size of219superscript2192^{19}2 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT.",
                "position": 1666
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x15.png",
                "caption": "Figure 11:Performance of RMIA for increasing size of the population‚Ñ§‚Ñ§{\\mathbb{Z}}blackboard_Zon a 10M parameter model trained for 1 epoch with a training set size of219superscript2192^{19}2 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT.",
                "position": 1674
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x16.png",
                "caption": "(a)|‚Ñ§|=10,000‚Ñ§10000|{\\mathbb{Z}}|=10,000| blackboard_Z | = 10 , 000",
                "position": 1683
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x16.png",
                "caption": "(a)|‚Ñ§|=10,000‚Ñ§10000|{\\mathbb{Z}}|=10,000| blackboard_Z | = 10 , 000",
                "position": 1686
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x17.png",
                "caption": "(b)|‚Ñ§|=300,000‚Ñ§300000|{\\mathbb{Z}}|=300,000| blackboard_Z | = 300 , 000",
                "position": 1692
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x18.png",
                "caption": "(a)Offline",
                "position": 1754
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x18.png",
                "caption": "(a)Offline",
                "position": 1757
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x19.png",
                "caption": "(b)Online",
                "position": 1763
            }
        ]
    },
    {
        "header": "Appendix BMore experiments on Chinchilla-optimal models",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x20.png",
                "caption": "(a)Validation loss over time (1 epoch)",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x20.png",
                "caption": "(a)Validation loss over time (1 epoch)",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x21.png",
                "caption": "(b)LiRAROCROC\\mathrm{ROC}roman_ROCfor different learning rate schedules",
                "position": 1788
            }
        ]
    },
    {
        "header": "Appendix CAdditional experiments on LiRA limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x22.png",
                "caption": "Figure 15:The role of duplicates on MIA vulnerability.We observe no significant differences (particularly asFPRFPR\\mathrm{FPR}roman_FPRincreases) between models trained on C4 and de-duplicated C4.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x23.png",
                "caption": "(a)140M model with training set size of‚âà7‚Å¢Mabsent7ùëÄ\\approx 7M‚âà 7 italic_Mexamples for 10 epochs.",
                "position": 1837
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x23.png",
                "caption": "(a)140M model with training set size of‚âà7‚Å¢Mabsent7ùëÄ\\approx 7M‚âà 7 italic_Mexamples for 10 epochs.",
                "position": 1840
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x24.png",
                "caption": "(b)140M model with training set size of219‚âà500‚Å¢Ksuperscript219500ùêæ2^{19}\\approx 500K2 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT ‚âà 500 italic_Kexamples for 20 epochs.",
                "position": 1845
            }
        ]
    },
    {
        "header": "Appendix DDiscussion and other experimental results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x25.png",
                "caption": "(a)10M",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x25.png",
                "caption": "(a)10M",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x26.png",
                "caption": "(b)85M",
                "position": 1912
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x27.png",
                "caption": "(c)302M",
                "position": 1917
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x28.png",
                "caption": "(d)489M",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x29.png",
                "caption": "(e)604M",
                "position": 1928
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x30.png",
                "caption": "(f)1018M",
                "position": 1933
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x31.png",
                "caption": "(a)10M",
                "position": 1940
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x31.png",
                "caption": "(a)10M",
                "position": 1943
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x32.png",
                "caption": "(b)44M",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x33.png",
                "caption": "(c)85M",
                "position": 1953
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x34.png",
                "caption": "(d)140M",
                "position": 1959
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x35.png",
                "caption": "(e)302M",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x36.png",
                "caption": "(f)425M",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x37.png",
                "caption": "(g)489M",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x38.png",
                "caption": "(h)509M",
                "position": 1980
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x39.png",
                "caption": "(i)604M",
                "position": 1985
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x40.png",
                "caption": "(j)1018M",
                "position": 1991
            }
        ]
    },
    {
        "header": "Appendix EExperiment details",
        "images": []
    },
    {
        "header": "Appendix FMore per-example MIA results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x41.png",
                "caption": "(a)Mean TF-IDF scores across vulnerability categories",
                "position": 2276
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x41.png",
                "caption": "(a)Mean TF-IDF scores across vulnerability categories",
                "position": 2279
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x42.png",
                "caption": "(b)Unknown token (<unk>) counts across vulnerability categories",
                "position": 2284
            }
        ]
    },
    {
        "header": "Appendix GEvolution of losses over different model sizes",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x43.png",
                "caption": "Figure 20:For three different samples (referenced by their ID in the C4 dataset, and if they were a member or non-member of training), we plot the loss of the reference distributions and the loss of the sample of the target model (as a vertical red line). We plot this over different model sizes (yùë¶yitalic_y-axis).",
                "position": 2302
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x43.png",
                "caption": "",
                "position": 2322
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x44.png",
                "caption": "",
                "position": 2327
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x45.png",
                "caption": "",
                "position": 2331
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x46.png",
                "caption": "",
                "position": 2341
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x47.png",
                "caption": "",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x48.png",
                "caption": "",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x49.png",
                "caption": "",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x50.png",
                "caption": "",
                "position": 2363
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x51.png",
                "caption": "",
                "position": 2367
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x52.png",
                "caption": "",
                "position": 2377
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x53.png",
                "caption": "",
                "position": 2381
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x54.png",
                "caption": "",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x55.png",
                "caption": "",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x56.png",
                "caption": "",
                "position": 2399
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x57.png",
                "caption": "",
                "position": 2403
            }
        ]
    },
    {
        "header": "Appendix HComparing MIA over different number of reference models for all Chinchilla-optimally trained model sizes",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.18773/x58.png",
                "caption": "(a)10M",
                "position": 2423
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x58.png",
                "caption": "(a)10M",
                "position": 2426
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x59.png",
                "caption": "(b)85M",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x60.png",
                "caption": "(c)302M",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x61.png",
                "caption": "(d)489M",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x62.png",
                "caption": "(e)604M",
                "position": 2450
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x63.png",
                "caption": "(f)1018M",
                "position": 2456
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x64.png",
                "caption": "(a)Per-sample true positive probability for224superscript2242^{24}2 start_POSTSUPERSCRIPT 24 end_POSTSUPERSCRIPTsamples (mean and standard deviation computed over 64 target models). Ordered from smallest to largest.",
                "position": 2478
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x64.png",
                "caption": "(a)Per-sample true positive probability for224superscript2242^{24}2 start_POSTSUPERSCRIPT 24 end_POSTSUPERSCRIPTsamples (mean and standard deviation computed over 64 target models). Ordered from smallest to largest.",
                "position": 2481
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x65.png",
                "caption": "(b)Histogram of average per-sample true positive probabilities fromFigure22(a).",
                "position": 2486
            },
            {
                "img": "https://arxiv.org/html/2505.18773/x66.png",
                "caption": "(c)Histogram of standard deviation of per-sample true positive probabilities fromFigure22(a).",
                "position": 2491
            }
        ]
    },
    {
        "header": "Appendix IShowing variance in sample predictions",
        "images": []
    }
]