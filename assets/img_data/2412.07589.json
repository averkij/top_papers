[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The MangaZero Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x2.png",
                "caption": "Figure 2:We constructMangaZerothrough three steps: 1) Download manga pages from the internet. 2) Annotate manga panels autonomously with pre-trained models. 3) Human calibration for the character ID annotation.",
                "position": 280
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x3.png",
                "caption": "Figure 3:The architecture of DiffSensei. In the first stage, we train a multi-character customized manga image generation model with layout control. The dialog embedding is added to the noised latent after the first convolution layer. All the parameters in the U-Net and feature extractor are trained. In the second stage, we finetune LoRA and resampler weights of an MLLM to adapt the source character features corresponding to the text prompt. We use the model in the first stage as the image generator and freeze its weights.",
                "position": 289
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x4.png",
                "caption": "Figure 4:Qualitative comparison with baselines. Baselines followed by a ‚Äú*‚Äù use reference images as input rather than character images. Methods marked by ‚Äú‚Ä†‚Äù means re-trained with dialog embedding. Our model excels at preserving the characters while following the text prompt. Our DiffSensei successively generates highlighted details in panel captions. Better viewed with zoom-in.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x5.png",
                "caption": "Figure 5:Human preference study on MangaZero eval set.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x6.png",
                "caption": "Figure 6:Qualitative results. Character images in red boxes are from Manga109 (The rightmost example). Our DiffSensei can generate vivid manga pages in various scenarios. Better viewed with zoom-in. More results can be found in the appendix.",
                "position": 638
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Modelsfor Customized Manga GenerationSupplementary Material",
        "images": []
    },
    {
        "header": "Appendix AMore Qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x7.png",
                "caption": "Figure 7:A complete long manga story about Hinton, LeCun, and Bengio winning the Nobel Prize.",
                "position": 1497
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Comparison Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x8.png",
                "caption": "Figure 8:More qualitative comparisons with baselines. Baselines followed by a ‚Äú*‚Äù use reference images as input rather than character images. Methods marked by ‚Äú‚Ä†‚Äù means re-trained with dialog embedding.",
                "position": 1513
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x9.png",
                "caption": "Figure 9:Qualitative ablation of the proposed modules. CM is character masked attention injection. DM is dialog masked encoding. Magi means using Magi[30]image encoder. MLLM means using MLLM for stage 2 training.",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x10.png",
                "caption": "Figure 10:Qualitative ablation ofŒ≤ùõΩ\\betaitalic_Œ≤.",
                "position": 1673
            }
        ]
    },
    {
        "header": "Appendix ELimitations and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x11.png",
                "caption": "Figure 11:Failure cases.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x12.png",
                "caption": "(a)Covers of manga series.",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x12.png",
                "caption": "(a)Covers of manga series.",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x13.png",
                "caption": "(b)Examples of character and dialog annotations.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x14.png",
                "caption": "(c)Resolution distribution.",
                "position": 1720
            }
        ]
    },
    {
        "header": "Appendix FMangaZero Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07589/x15.png",
                "caption": "Figure 13:DiffSensei generated results with inputs (Part1).",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x16.png",
                "caption": "Figure 14:DiffSensei generated results with inputs (Part2).",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x17.png",
                "caption": "Figure 15:Manga pages generated by DiffSensei (Part 1).",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2412.07589/x18.png",
                "caption": "Figure 16:Manga pages generated by DiffSensei (Part 2).",
                "position": 1780
            }
        ]
    },
    {
        "header": "Appendix GBroader Impacts",
        "images": []
    }
]