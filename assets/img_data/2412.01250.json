[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x1.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x2.png",
                "caption": "Figure 2:Graphical depiction ofAIUTA: left shows its interaction cycle with the user, and right provides an exploded view of our method. â‘ Â The agent receives an initial instructionIğ¼Iitalic_I: â€œFind ac=<ğ‘c=<italic_c = <object category>>>â€. â‘¡Â At each timesteptğ‘¡titalic_t, a zero-shot policyÏ€ğœ‹\\piitalic_Ï€[44], comprising a frozen object detection module[17], selects the optimal actionatsubscriptğ‘ğ‘¡a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\nâ‘¢Â Upon detection, the agent performs the proposed AIUTA. Specifically, â‘£ the agent first obtains an initial scene description of observationOtsubscriptğ‘‚ğ‘¡O_{t}italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfrom a VLM. Then, aSelf-Questionermodule leverages an LLM to automatically generate attribute-specific questions to the VLM, acquiring more information and refining the scene description with reduced attribute-level uncertainty, producingSrâ¢eâ¢fâ¢iâ¢nâ¢eâ¢dsubscriptğ‘†ğ‘Ÿğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘S_{refined}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUBSCRIPT. â‘¤Â TheInteraction Triggermodule then evaluatesSrâ¢eâ¢fâ¢iâ¢nâ¢eâ¢dsubscriptğ‘†ğ‘Ÿğ‘’ğ‘“ğ‘–ğ‘›ğ‘’ğ‘‘S_{refined}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUBSCRIPTagainst the â€œfactsâ€ related to the target, to determine whether to terminate the navigation (if the agent believes it has located the target objectÂ â‘¥), or to posetemplate-free, natural-languagequestions to a humanÂ â‘¦, updating the â€œfactsâ€ based on the responseÂ â‘§.",
                "position": 189
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Collaborative Instance Navigation",
        "images": []
    },
    {
        "header": "4Proposed Method",
        "images": []
    },
    {
        "header": "5CoIN-Bench",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x3.png",
                "caption": "Figure 3:Ï„ğœ\\tauitalic_Ï„sensitivity results. For each method,30303030newÏ„ğœ\\tauitalic_Ï„values are sampled symmetrically around the optimal thresholdÏ„âˆ—superscriptğœ\\tau^{*}italic_Ï„ start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT.\nThexğ‘¥xitalic_x-axis shows the set size as a percentage of the original IDKVQA dataset size, while theyğ‘¦yitalic_y-axis displays the normalized ERÎ¦c=1subscriptÎ¦ğ‘1\\Phi_{c=1}roman_Î¦ start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT.",
                "position": 753
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "\\AlphAlphAdditional details of CoIN-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x4.png",
                "caption": "Figure 1:CoIN-Bench can be very challenging when only given the instance category to the agent. We highlight the target instance with red borders, while the distractor instances that exist in the same scene are marked with blue borders.",
                "position": 1481
            },
            {
                "img": "https://arxiv.org/html/2412.01250/x5.png",
                "caption": "Figure 2:We show the distribution of categories, categorized for each evaluation split.",
                "position": 1494
            }
        ]
    },
    {
        "header": "\\AlphAlphBaselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x6.png",
                "caption": "Figure 3:(a) Frontier map and (b) value map constructed by VLFM[44]. The blue dots in (a) (as well as the red dots in (b)) are the identified frontiers.",
                "position": 1528
            }
        ]
    },
    {
        "header": "\\AlphAlphIDKVQA dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x7.png",
                "caption": "Figure 4:Examples from IDKVQA, showing images and the questions generated by the LLM.",
                "position": 1668
            }
        ]
    },
    {
        "header": "\\AlphAlphPrompts",
        "images": []
    },
    {
        "header": "\\AlphAlphAlgorithm",
        "images": []
    }
]