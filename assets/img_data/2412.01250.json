[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x1.png",
                "caption": "",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x2.png",
                "caption": "Figure 2:Graphical depiction ofAIUTA: left shows its interaction cycle with the user, and right provides an exploded view of our method. ① The agent receives an initial instructionI𝐼Iitalic_I: “Find ac=<𝑐c=<italic_c = <object category>>>”. ② At each timestept𝑡titalic_t, a zero-shot policyπ𝜋\\piitalic_π[44], comprising a frozen object detection module[17], selects the optimal actionatsubscript𝑎𝑡a_{t}italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\n③ Upon detection, the agent performs the proposed AIUTA. Specifically, ④ the agent first obtains an initial scene description of observationOtsubscript𝑂𝑡O_{t}italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTfrom a VLM. Then, aSelf-Questionermodule leverages an LLM to automatically generate attribute-specific questions to the VLM, acquiring more information and refining the scene description with reduced attribute-level uncertainty, producingSr⁢e⁢f⁢i⁢n⁢e⁢dsubscript𝑆𝑟𝑒𝑓𝑖𝑛𝑒𝑑S_{refined}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUBSCRIPT. ⑤ TheInteraction Triggermodule then evaluatesSr⁢e⁢f⁢i⁢n⁢e⁢dsubscript𝑆𝑟𝑒𝑓𝑖𝑛𝑒𝑑S_{refined}italic_S start_POSTSUBSCRIPT italic_r italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUBSCRIPTagainst the “facts” related to the target, to determine whether to terminate the navigation (if the agent believes it has located the target object ⑥), or to posetemplate-free, natural-languagequestions to a human ⑦, updating the “facts” based on the response ⑧.",
                "position": 189
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Collaborative Instance Navigation",
        "images": []
    },
    {
        "header": "4Proposed Method",
        "images": []
    },
    {
        "header": "5CoIN-Bench",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x3.png",
                "caption": "Figure 3:τ𝜏\\tauitalic_τsensitivity results. For each method,30303030newτ𝜏\\tauitalic_τvalues are sampled symmetrically around the optimal thresholdτ∗superscript𝜏\\tau^{*}italic_τ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT.\nThex𝑥xitalic_x-axis shows the set size as a percentage of the original IDKVQA dataset size, while they𝑦yitalic_y-axis displays the normalized ERΦc=1subscriptΦ𝑐1\\Phi_{c=1}roman_Φ start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT.",
                "position": 753
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "\\AlphAlphAdditional details of CoIN-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x4.png",
                "caption": "Figure 1:CoIN-Bench can be very challenging when only given the instance category to the agent. We highlight the target instance with red borders, while the distractor instances that exist in the same scene are marked with blue borders.",
                "position": 1481
            },
            {
                "img": "https://arxiv.org/html/2412.01250/x5.png",
                "caption": "Figure 2:We show the distribution of categories, categorized for each evaluation split.",
                "position": 1494
            }
        ]
    },
    {
        "header": "\\AlphAlphBaselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x6.png",
                "caption": "Figure 3:(a) Frontier map and (b) value map constructed by VLFM[44]. The blue dots in (a) (as well as the red dots in (b)) are the identified frontiers.",
                "position": 1528
            }
        ]
    },
    {
        "header": "\\AlphAlphIDKVQA dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.01250/x7.png",
                "caption": "Figure 4:Examples from IDKVQA, showing images and the questions generated by the LLM.",
                "position": 1668
            }
        ]
    },
    {
        "header": "\\AlphAlphPrompts",
        "images": []
    },
    {
        "header": "\\AlphAlphAlgorithm",
        "images": []
    }
]