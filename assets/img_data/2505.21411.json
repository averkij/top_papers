[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21411/x1.png",
                "caption": "Figure 1:Illustration of the Mixture of Grouped Experts (MoGE) architecture. TheNùëÅNitalic_Ntotal experts are evenly partitioned intoMùëÄMitalic_Mnon-overlapping groups, with each group typically assigned to a distinct computational device. For each input token, initial gating scores for all experts are computed via a global softmax router. Subsequently, within each expert group, theK‚Ä≤superscriptùêæ‚Ä≤K^{\\prime}italic_K start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTexperts with the highest scores are chosen based on these initial scores. The scores corresponding to unselected experts are effectively set to zero. The final output is obtained by a weighted sum of the outputs from the activated experts and a shared expert.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Mixture of Grouped Experts",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21411/extracted/6487160/imgs/demo.png",
                "caption": "(a)The distribution of activated experts for a single token.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2505.21411/extracted/6487160/imgs/demo.png",
                "caption": "(a)The distribution of activated experts for a single token.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x2.png",
                "caption": "(b)Distribution of imbalance score.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x3.png",
                "caption": "Figure 3:Simulation results of candidate model configurations. Throughput under 100 ms and 50 ms TPOT constraints on Ascend 300I Duo and 800I A2 platforms, respectively, is recorded. All results are normalized relative to a randomly selected candidate.",
                "position": 360
            }
        ]
    },
    {
        "header": "3Model Training",
        "images": []
    },
    {
        "header": "4Infrastructure",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21411/x4.png",
                "caption": "Figure 4:Overview of the inference system optimization. AH2superscriptH2\\text{H}^{2}H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTP strategy is employed to achieve high-efficiency distributed parallel inference across different modules. Additionally, two key fused operators, MulAttention and SwiftGMM, are specifically designed for the Ascend platform to accelerate model inference.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x5.png",
                "caption": "Figure 5:Computation flow of the MulAttention operator. A large-packet KV transfer strategy is adopted to improve memory bandwidth utilization, as indicated by steps (1)(5). Furthermore, a dual-loop pipeline with a pingpong scheduler is introduced for KV processing to enhance MTE2 utilization, as indicated by steps (2)(3)(4) and (6)(7)(8).",
                "position": 722
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x6.png",
                "caption": "Figure 6:Overview of SwiftGMM. (a) A tiling cache strategy leverages historical profiling to predict optimal tiling parameters under dynamic workloads. (b) A dual-buffer mechanism overlaps data transfer and computation to maximize MTE2 utilization on the Ascend 300I Duo.",
                "position": 741
            }
        ]
    },
    {
        "header": "5Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21411/x7.png",
                "caption": "Figure 7:Expert specialization in Pangu Pro MoE¬†. Each subplot illustrates the token-to-expert distribution within a specific layer for a given task. Bars represent the proportion of tokens routed to individual experts, normalized by the total number of tokens. Pangu Pro MoE¬†employs 64 experts per layer, with 8 experts activated for each token, yielding an expected uniform distribution of 12.5%percent\\%%per expert‚Äîindicated by the gray dashed line. The observed distributions, however, deviate substantially from this uniform baseline, highlighting a pronounced degree of expert specialization. This specialization is indicative of effective expert differentiation and contributes to enhanced model training and overall performance.",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x8.png",
                "caption": "Figure 8:Expert co-activation across three layers (shallow, middle, and deep), evaluated on a random 0.5%percent\\%%subset of the C4 validation set. The first four expert groups, each containing four experts, are displayed with their expert IDs on the x- and y-axes. Color intensity reflects the co-activation scores between expert pairs.",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x9.png",
                "caption": "Figure 9:Intra-group expert distribution in Pangu Pro MoE¬†. the observed token distributions closely align with this theoretical baseline, demonstrating that Pangu Pro MoE¬† effectively maintains balanced utilization across experts within each group. Such balanced activation helps prevent expert underuse or oversaturation, thereby promoting stable training dynamics and maximizing the collective capacity of the expert pool",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2505.21411/x10.png",
                "caption": "Figure 10:Global expert distribution on the first MoE layer, evaluated using a random 0.5%percent\\%%subset of the C4 validation set. Pangu Pro MoE exhibits a more balanced expert utilization, with token proportions closer to the ideal 12.5%percent\\%%per expert.",
                "position": 1571
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContributions and Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix BCase Study",
        "images": []
    }
]