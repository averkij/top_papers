[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07785/x1.png",
                "caption": "Figure 1:Benchmark performance of GroveMoE-Inst‡and its counterparts. Our GroveMoE-Inst achieves performance comparable to open-source SOTA LLMs of similar or even larger scales.",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07785/x2.png",
                "caption": "Figure 2:Comparison between the traditional MoE layer and our Grove MoE layer with dynamic computation allocation. For clarity, we configuren/g=2n/g=2andk=4k=4for the Grove MoE layer.",
                "position": 312
            }
        ]
    },
    {
        "header": "4Mid-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07785/x3.png",
                "caption": "Figure 3:The group routing distribution across three configurations with different group numbersgg. As the number of groups decreases, the average number of adjugate experts activations decreases.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2508.07785/x4.png",
                "caption": "Figure 4:Evaluation results of SFT on various benchmarks.Δ\\Deltaindicates the performance improvement of SFT trained with GroveMoE-Base over Qwen3-30B-A3B-Base.",
                "position": 1336
            }
        ]
    },
    {
        "header": "5Post-Training",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]