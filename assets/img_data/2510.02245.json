[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x1.png",
                "caption": "",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x2.png",
                "caption": "",
                "position": 203
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x3.png",
                "caption": "(a)Test Performance.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x3.png",
                "caption": "(a)Test Performance.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x4.png",
                "caption": "(b)Reasoning Chain Validity.",
                "position": 358
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x5.png",
                "caption": "(c)Entropy Distribution.",
                "position": 363
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x6.png",
                "caption": "Figure 2:Overview ofExperientialGroupRelativePolicyOptimization (ExGRPO). ExGRPO operates in two phases: (a) Experience Management and (b) Policy Optimization (cf.Algorithm1).",
                "position": 418
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x7.png",
                "caption": "Figure 3:A comparison of benchmark performance for different backbone models and training variants, showing performance on both in-distribution and out-of-distribution tasks (cf.SectionE.3).",
                "position": 810
            }
        ]
    },
    {
        "header": "6Analysis and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x8.png",
                "caption": "Figure 4:Learning dynamics of On-Policy vs. ExGRPO during training Llama-3.1 8B. ExGRPO stabilizes training and achieves higher rewards, while on-policy suffers from training collapse.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x9.png",
                "caption": "Figure 5:Dynamics of experience replay buffer and retried set.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x10.png",
                "caption": "Figure 6:Dynamics of experience under different data conditions.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x11.png",
                "caption": "",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x12.png",
                "caption": "Figure 7:Comparison of validation performance of different ExGRPO variants.",
                "position": 873
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethics statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BThe Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix CExGRPO Algorithm",
        "images": []
    },
    {
        "header": "Appendix DTheoretical Analysis of ExGRPO",
        "images": []
    },
    {
        "header": "Appendix ESupplementary Materials of Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x13.png",
                "caption": "Figure 8:Dynamics of policy entropy during training. ExGRPO without policy shaping even drops dramatically at an early stage, performing worse than the on-policy baseline.",
                "position": 2903
            }
        ]
    },
    {
        "header": "Appendix FSupplementary Materials of Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02245/x14.png",
                "caption": "Table 6:Average number of questions per mini-batch for each difficulty-masked training group, confirming similar data throughput.",
                "position": 3030
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x14.png",
                "caption": "Figure 9:Dynamics of the number of questions per mini-batch for the three difficulty-masked training groups. The three series demonstrate that each training regime was exposed to a comparable volume of optimization data.",
                "position": 3064
            },
            {
                "img": "https://arxiv.org/html/2510.02245/x15.png",
                "caption": "Figure 10:A comparison of average PPL and Entropy for correct (“C.”) and wrong (“W.”) reasoning trajectories (determined by an external CoT judge), grouped by online rollout correctness buckets.",
                "position": 3105
            }
        ]
    },
    {
        "header": "Appendix GPrompt Template",
        "images": []
    }
]