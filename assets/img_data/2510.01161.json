[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01161/x1.png",
                "caption": "Figure 1:Comparison of on-policy GRPO and off-policy training under a staleness of 256 model updates on Qwen-2.5-32B.Left:Standard GRPO suffers from degradation with stale rollouts, while removing the trust region (GRPO no TR) reveals a clearprosperity-before-collapsephenomenon.\nIn contrast, M2PO achieves stable training and matches on-policy performance even under high staleness.Right:Token clipping ratio comparison shows that M2PO dramatically reduces clipping events compared to GRPO with the same staleness, while avoiding training collapse.",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01161/x2.png",
                "caption": "Figure 2:Average accuracy for RL with stale data on Qwen2.5-Math-7B.",
                "position": 204
            }
        ]
    },
    {
        "header": "4Prosperity before Collapse: Stale data contain enough training information in RL on LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01161/x3.png",
                "caption": "Figure 3:Prosperity before Collapse. Training without a trust region (TR) (ϵ=∞\\epsilon=\\infty) under stale data (s=256s=256) initially achieves higher performance than clipped training, sometimes even matching the on-policy baseline (s=0s=0). However, it eventually collapses due to uncontrolled variance.",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x4.png",
                "caption": "(a)",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x4.png",
                "caption": "(a)",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x5.png",
                "caption": "(b)",
                "position": 253
            }
        ]
    },
    {
        "header": "5Second-Moment Trust Policy Optimization",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01161/x6.png",
                "caption": "Figure 5:Training reward on Qwen-2.5-32B.",
                "position": 790
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x7.png",
                "caption": "Figure 6:Methods comparison under staleness (s=256s=256) on Llama-Instruct-3B.",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x8.png",
                "caption": "(a)",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x8.png",
                "caption": "(a)",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x9.png",
                "caption": "(b)",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x10.png",
                "caption": "Figure 8:Ablation study of theτM2\\tau_{M_{2}}threshold on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x11.png",
                "caption": "(a)",
                "position": 865
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x11.png",
                "caption": "(a)",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x12.png",
                "caption": "(b)",
                "position": 874
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "The Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "8Overview",
        "images": []
    },
    {
        "header": "9Detailed Experimental Setting",
        "images": []
    },
    {
        "header": "10Theoretical Proof",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01161/figs/clipping-token-examples.png",
                "caption": "Figure 10:Word clouds of frequently clipped tokens withϵ\\epsilonclipping.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2510.01161/x13.png",
                "caption": "Figure 11:The performance of M2PO under different stalenesson Qwen2.5-Math-7B.",
                "position": 1694
            }
        ]
    },
    {
        "header": "11Additional Experiments",
        "images": []
    }
]