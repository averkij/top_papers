[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20873/x1.png",
                "caption": "Figure 1:Attention weight analysis in VideoLLaMA2cheng2024videollamaon AVHBench datasetsung2024avhbench.We analyze 100 samples and examine the attention weights from the last decoder layer, focusing on the last token in the question. The attention is disproportionately assigned to video inputs over audio, indicating a modality bias. Our proposed FMD method mitigates this gap by promoting more balanced contributions from both modalities.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20873/x2.png",
                "caption": "Figure 2:Overview of the Fork-Merge Decoding (FMD) pipeline.The AV-LLM takes video frames, an audio waveform, and a question prompt as input. In theforkphase, FMD masks either the video or audio modality while preserving the question prompt, enabling independent reasoning over each modality. AfterLforksubscriptùêøforkL_{\\text{fork}}italic_L start_POSTSUBSCRIPT fork end_POSTSUBSCRIPTdecoder layers inœïitalic-œï\\phiitalic_œï, themergephase combines the resulting hidden states,ùê°forkvsuperscriptsubscriptùê°forkùë£\\mathbf{h}_{\\text{fork}}^{v}bold_h start_POSTSUBSCRIPT fork end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPTandùê°forkasuperscriptsubscriptùê°forkùëé\\mathbf{h}_{\\text{fork}}^{a}bold_h start_POSTSUBSCRIPT fork end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT, using an importance weightŒ±ùõº\\alphaitalic_Œ±derived from attention distributions over unmasked modality tokens; the complementary weight(1‚àíŒ±)1ùõº(1-\\alpha)( 1 - italic_Œ± )is applied to masked counterparts. This decoding scheme enables the pretrained AV-LLM to integrate both audio and visual cues effectively, allowing it to resolve questions that may be ambiguous or misleading when relying on a single modality.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Enhancing Audio-Visual understanding with Fork-Merge Decoding",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20873/x3.png",
                "caption": "(a)An example of audio-driven video hallucination (A‚Üí‚Üí\\rightarrow‚ÜíV).",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x3.png",
                "caption": "(a)An example of audio-driven video hallucination (A‚Üí‚Üí\\rightarrow‚ÜíV).",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x4.png",
                "caption": "(b)An example of video-driven audio hallucination (V‚Üí‚Üí\\rightarrow‚ÜíA).",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x5.png",
                "caption": "(c)An example of audio-visual matching (AV matching).",
                "position": 562
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x6.png",
                "caption": "(d)An example of audio-visual captioning (AV captioning).",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x7.png",
                "caption": "Figure 4:Layer-wise attention weight comparison on VideoLLaMA2cheng2024videollamausing 600 samples from the AVHBenchsung2024avhbenchdataset.We analyze the attention weights from the final token in the last decoder layer, focusing on the distribution across video and audio segments.\nDeeper merging within the network results in reduced attention to visual tokens and heightened attention to audio tokens.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x8.png",
                "caption": "Figure 5:Layer-wise ablation results on VideoLLaMA2cheng2024videollamausing 200 samples from the AVHBenchsung2024avhbenchdataset for each task.To identify the optimal merge layer, we evaluate performance across three tasks, each focused on a certain modality: A‚Üí‚Üí\\rightarrow‚ÜíV for video-targeted understanding, V‚Üí‚Üí\\rightarrow‚ÜíA for audio-targeted understanding, and AV Matching for joint audio-visual understanding.",
                "position": 605
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AArchitectural Analysis of Recent AV-LLMs",
        "images": []
    },
    {
        "header": "Appendix BFurther Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20873/x9.png",
                "caption": "Figure A.1:Attention weight analysis in VideoLLaMA2[6]on the AVHBench dataset[37].We analyze 100 samples and examine the attention weights in decoder layers afterLforksubscriptùêøforkL_{\\text{fork}}italic_L start_POSTSUBSCRIPT fork end_POSTSUBSCRIPT, focusing on the final token. FMD narrows the gap between audio and video attention weights, encouraging more balanced contributions from both modalities.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x10.png",
                "caption": "(a)Performance on 500 samples from the AVHBench dataset.",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x10.png",
                "caption": "(a)Performance on 500 samples from the AVHBench dataset.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x11.png",
                "caption": "(b)Performance on 1000 samples from the AVHBench dataset.",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x12.png",
                "caption": "Figure A.3:Cosine similarity comparison on VideoLLaMA2[6]using 100 samples from the AVHBench[37]dataset.We compute the cosine similarity between the final-layer hidden states of the intact input and those of audio- or video-perturbed inputs. The results indicate that video signals are not effectively isolated by additive Gaussian noise, whereas zero-masking reliably suppresses both modalities.",
                "position": 1703
            }
        ]
    },
    {
        "header": "Appendix CMore Qualitative Results",
        "images": []
    },
    {
        "header": "Appendix DComputational Resource",
        "images": []
    },
    {
        "header": "Appendix ELimitations and Future Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.20873/x13.png",
                "caption": "Figure A.4:Qualitative results for audio-driven video hallucination tasks using VideoLLaMA2[6].Compared to vanilla decoding, FMD generates more accurate responses by effectively leveraging both audio and visual modalities.",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x14.png",
                "caption": "Figure A.5:Qualitative results for video-driven audio hallucination tasks using VideoLLaMA2[6].FMD produces more accurate responses in most cases, including instances where visual signals lead to confusion, as shown in the third example.",
                "position": 1780
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x15.png",
                "caption": "Figure A.6:Qualitative results for audio-visual matching tasks using VideoLLaMA2[6].FMD generates correct responses even when the synthesized audio and video inputs are semantically unrelated.",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x16.png",
                "caption": "Figure A.7:Qualitative results for audio-visual description tasks using VideoLLaMA2[6].FMD effectively describes both audio and visual content, capturing fine-grained details‚Äîsuch as ‚ÄúA person touches the edge of a wooden clock‚Äù in the top example.",
                "position": 1786
            },
            {
                "img": "https://arxiv.org/html/2505.20873/x17.png",
                "caption": "Figure A.8:Failure case analysis for audio-visual description tasks using VideoLLaMA2[6].While FMD occasionally generates inaccurate details, it still captures both audio and visual content more effectively‚Äîand with finer granularity‚Äîcompared to vanilla decoding.",
                "position": 1789
            }
        ]
    },
    {
        "header": "Appendix FSocial Impact",
        "images": []
    }
]