[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/x1.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x2.png",
                "caption": "Figure 2:Comparison of Visual Unit Decoding Methods.Benefiting from the Triplet-Based Referring Paradigm, REF-VLM can adapt to more complex granularity scenarios and visual decoding tasks, enhancing the interpretability and accuracy of the MLLM’s responses.",
                "position": 652
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Unified Instruction Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/bbox.jpg",
                "caption": "Table 2:An Example of VD-CoT Applied to the Grounded Conversation Generation (GCG)\nTask.VD-CoT analyzes the image and outputs the structured visual decoding information required by TRP. The answer is generated synchronously with the triplet, and the special tokens have been simplified in the example.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x3.png",
                "caption": "Figure 3:The Framework of REF-VLM.REF-VLM employs dual-architecture visual encoders to jointly encode images into a feature pyramid, enhancing visual unit decoder performance. Additionally, visual prompts are fused with global features and share a projector, enabling parameter-free encoding of image interactions. Training samples adhere to the Triplet-Based Referring Paradigm, ensuring one-to-one mapping between REF-VLM’s latent embeddings and decoding targets.",
                "position": 802
            }
        ]
    },
    {
        "header": "4End-to-End Decoding Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/x4.png",
                "caption": "Figure 4:Architecture of Visual Unit Decoders.We propose a Latent Embeddings Router to facilitate unified multi-task training in REF-VLM, and enhance the Hungarian matching algorithm for the TRP-based one-to-one referring decoding scheme.",
                "position": 858
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Limitations and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVD-CoT Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/x5.png",
                "caption": "Figure 5:Attention Significance of <REF>Tokens for MLLM Output Tokens.We selected a general segmentation task with the prompt “please segment objects in the image <image>.” The model outputs two classification results: “a bride” and “a giant illuminated heart”, using our designed VD-CoT format. We used attention visualization techniques to generate attention significance maps for the REF token with respect to each output token of the MLLM. The attention values from each layer were averaged and visualized, as shown in the figure above. The figure demonstrates that each <REF>token exhibits high attention response values to the preceding <Phrase>, <Unit>, and output numbers.",
                "position": 2910
            }
        ]
    },
    {
        "header": "Appendix BVT-Instruct Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/x6.png",
                "caption": "Figure 6:Data Distribution Map.VT-Instruct comprises four output units—box, keypoint, depth, and mask—paired with either low (phrases) or high (sentences) text complexity, with different visual prompts unified under the same task for clarity.",
                "position": 2995
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/data_example_5.png",
                "caption": "Figure 7:Example of VT-Instruct Dataset by Using the Automated Data Construction Pipeline.Our VT-Instruct dataset contains seven distinct downstream tasks, including Visual Understanding, Referring Expression, Interactive Grounding, Grounded Conversation Generation, Open-Vocabulary Identification and Depth Estimation.",
                "position": 2998
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/x7.png",
                "caption": "Figure 8:The Comparison of parameter numbers.Our Meta-plugins uses 248M parameters, while the External plugins method requires 1028M.",
                "position": 3245
            }
        ]
    },
    {
        "header": "Appendix DTraining Details",
        "images": []
    },
    {
        "header": "Appendix EFreeform Open-Vocabulary Identification",
        "images": []
    },
    {
        "header": "Appendix FMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Caption.jpg",
                "caption": "Table 23:An Example of VD-CoT Applied to the Visual Caption Task.In this task, only textual responses are required, and no operations on image information are needed. Therefore, in the generation of VD-CoT, Unit decode is set to False, and no visual-related special tokens are generated. Similarly, in the result of Answer with Triplets, there are no decoded triplets related to visual content.",
                "position": 4863
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Detection.jpg",
                "caption": "Table 24:An Example of VD-CoT Applied to the Object Detection Task.VD-CoT analyzes the visual content and generates decoding triplets to identify objects in the image. The special tokens and references have been included for clarity.",
                "position": 4918
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/open-vocabulary_segmentation.jpg",
                "caption": "Table 25:An Example of VD-CoT Applied to the Open-Vocabulary Segmentation Task.VD-CoT analyzes the visual content and generates decoding triplets for semantic segmentation of the image. Special tokens and references are provided for clarity.",
                "position": 5027
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Referring_Expresssion.jpg",
                "caption": "Table 26:An Example of VD-CoT Applied to the Referring Expression Comprehension Task.VD-CoT analyzes the visual content and generates decoding triplets to identify the target (egg yolk) and its bounding box dimensions in the image.",
                "position": 5150
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Referring_Expression_Segmentation.jpg",
                "caption": "Table 27:An Example of VD-CoT Applied to the Referring Expression Segmentation Task.VD-CoT analyzes the visual content and generates decoding triplets to identify the target (white boat) and creates its segmentation mask in the image.",
                "position": 5211
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Referring_Expression_Generation.jpg",
                "caption": "Table 28:An Example of VD-CoT Applied to the Referring Expression Generation Task.VD-CoT analyzes the visual content and generates descriptive features for the given area [VPT] in the image.",
                "position": 5272
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Interactive_Grounding1.jpg",
                "caption": "Table 29:An Example of VD-CoT Applied to the Interactive Grounding Task.VD-CoT analyzes the visual content, segments the region [VPT], and generates a mask for the identified object (boat) in the image.",
                "position": 5328
            },
            {
                "img": "https://arxiv.org/html/2503.07413/extracted/6267636/image/VD-CoT/Interactive_Grounding2.jpg",
                "caption": "",
                "position": 5347
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x8.png",
                "caption": "Figure 9:The Visual Understanding Results of REF-VLM.",
                "position": 5390
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x9.png",
                "caption": "Figure 10:The Detection Results of REF-VLM.The text color in the model’s responses corresponds to the bounding box colors of the detected objects in the images. For example, in the top-right image, the model detects two categories: “person” and “dress.”\nThe “person” category contains two instances, represented by[0]<REF>and[1]<REF>, while the “dress” category contains one instance, represented by[0]<REF>.",
                "position": 5394
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x10.png",
                "caption": "Figure 11:The Segmentation Results of REF-VLM.The figure illustrates the segmentation and GCG segmentation outputs generated by REF-VLM. The text corresponds to the mask colors of the segmented objects in the images. For example, in the top-left image, the model segments three categories: “stool,” “person,” and “grassy field.” The “person” category contains two instances, represented by[0]<REF>and[1]<REF>, while the “stool” category contains one instance, represented by[0]<REF>. The background, labeled as “grassy field,” is also represented by[0]<REF>.",
                "position": 5399
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x11.png",
                "caption": "Figure 12:The Grounding Detection Results of REF-VLM.We use the textual prompts and visual box prompts for grounding detection tasks. In the first row (left), a textual prompt instructs the model to locate the object “bear” mentioned in the sentence. The second row displays the result, where the model successfully detects the location of the bear using abounding box. In the first row (right), a bounding box around a bird is provided as a visual prompt. The textual query uses the special token<area>to refer the boxed region in the image. The second row (right) shows the model’s output, correctly identifying the object in the boxed region as a “bird.”",
                "position": 5402
            },
            {
                "img": "https://arxiv.org/html/2503.07413/x12.png",
                "caption": "Figure 13:The Grounding Segmentation Results of REF-VLM.We use the textual prompts and visual box prompts for grounding segmentation tasks. In the first row (left), a textual prompt instructs the model to segment themiddle birdin the center of the image. The second row (left) shows the segmentation result for themiddle birdproduced by the model based on the textual prompt. In the first row (right), bounding boxes around two birds are provided as visual prompts, and the textual prompt includes a special symbol<area>to indicate the region for segmentation. The second row (right) displays the segmentation results for theleft birdandright birdbased on two types of prompts.",
                "position": 5407
            }
        ]
    },
    {
        "header": "Appendix GMore Visualization Results",
        "images": []
    }
]