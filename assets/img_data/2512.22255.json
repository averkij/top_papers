[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22255/x1.png",
                "caption": "Figure 1:Shape of Thought -Fine-tuning on synthetic CoTs, even those with incorrect final answers, can outperform training on human-written data. We generate two synthetic datasets using a stronger model:G, containing CoT traces with correct final answers, andW, containing traces with incorrect final answers. Our results shows that fine-tuning a weaker model on both G and W datasets leads to higher downstream accuracy compared to the baseline of training on human-written CoTs (H) due to the distributional differences in the data.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Background and Preliminaries",
        "images": []
    },
    {
        "header": "3Related Work",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/G-2B_MATH500_Test_acc_H_G27B_G_W.png",
                "caption": "(a)SFT on synthetic G and even W data outperforms H after evaluation on MATH500 test set.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/G-2B_MATH500_Test_acc_H_G27B_G_W.png",
                "caption": "(a)SFT on synthetic G and even W data outperforms H after evaluation on MATH500 test set.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/G2B_loss_Math_H_G_W.png",
                "caption": "(b)Corresponding training losses on the datasets.",
                "position": 268
            }
        ]
    },
    {
        "header": "5Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22255/images/gsm8k/G-2B_GSM8K_Test_acc.png",
                "caption": "(a)Similar performance trends in GSM8K after SFT on G and W data, significantly outperforming H.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/gsm8k/G-2B_GSM8K_Test_acc.png",
                "caption": "(a)Similar performance trends in GSM8K after SFT on G and W data, significantly outperforming H.",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/G-2B_training_losses_GSM8K.png",
                "caption": "(b)Corresponding training losses.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/countdown/Q-1.5B_Countdown_Test_acc.png",
                "caption": "Figure 4:Performance on Countdown. In harder tasks like Countdown where base model has near zero performance, even Qwen-2.5-1.5B learns even fromWCoTs. Learning fromGis better as compared toW. We see similar trends in other models as shown in Table1.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/countdown/Q-1.5B_Countdown_Test_acc.png",
                "caption": "Figure 4:Performance on Countdown. In harder tasks like Countdown where base model has near zero performance, even Qwen-2.5-1.5B learns even fromWCoTs. Learning fromGis better as compared toW. We see similar trends in other models as shown in Table1.",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/G-2B_MBPP200_Test_acc_BS64_ALL_with_H_Para.png",
                "caption": "(a)Results of SFT on Gemma-2-2B model for Code Generation task.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/G-2B_MBPP200_Test_acc_BS64_ALL_with_H_Para.png",
                "caption": "(a)Results of SFT on Gemma-2-2B model for Code Generation task.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/L8B_H_H_Para_G_W_all_MBPP_Code.png",
                "caption": "(b)Results of SFT on Llama-3-8B model for Code Generation task.",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/Q1.5B_H_H_Para_G_W_All_MBPP_Code.png",
                "caption": "(c)Results of SFT on Qwen-2.5-1.5B model for Code Generation task.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/G2B_Loss_All_MBPP_Code.png",
                "caption": "(a)Loss curves for Gemma-2-2B.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/G2B_Loss_All_MBPP_Code.png",
                "caption": "(a)Loss curves for Gemma-2-2B.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/L8B_Loss_All_MBPP_Code.png",
                "caption": "(b)Loss curves for Llama-3.1-8B.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/code_MBPP/Q1.5B_Loss_All_MBPP_Code.png",
                "caption": "(c)Loss curves for Qwen-2.5-1.5B.",
                "position": 453
            }
        ]
    },
    {
        "header": "6Conclusions, Limitations & Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/G-2B_MATH500_Test_acc_BS256_H_G27B_SD_New.png",
                "caption": "(a)Performance of G-2B on MATH500 using different datasets for SFT (256 batch-size, lr=1e-6).",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/G-2B_MATH500_Test_acc_BS256_H_G27B_SD_New.png",
                "caption": "(a)Performance of G-2B on MATH500 using different datasets for SFT (256 batch-size, lr=1e-6).",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/L8B_MATH500_Test_acc.png",
                "caption": "(b)Performance of Llama-3.1-8B on different datasets for SFT (256 batch-size, lr=1e-6).",
                "position": 991
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/G2B_MATH500_analysis_64BS.png",
                "caption": "(a)Performance of G-2B on different datasets across training iterations. (64 batch-size, lr=2e-5)",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/G2B_MATH500_analysis_64BS.png",
                "caption": "(a)Performance of G-2B on different datasets across training iterations. (64 batch-size, lr=2e-5)",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/G-2B_training_losses_64BS_combined.png",
                "caption": "(b)Training losses on the corresponding datasets.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/Q-1.5B_MATH500_Test_acc.png",
                "caption": "Figure 9:Performance of Qwen-2.5-1.5B on MATH500 using different datasets. We see limited gains likely due to the already high performance of the model on the task.",
                "position": 1013
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/L8B_MATH500_Test_acc.png",
                "caption": "(a)Performance of L-8B on MATH500 using different datasets for finetuning.",
                "position": 1016
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/math/L8B_MATH500_Test_acc.png",
                "caption": "(a)Performance of L-8B on MATH500 using different datasets for finetuning.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/loss_plots/L_8B_MATH_loss_plot.png",
                "caption": "(b)Training losses on the corresponding datasets.",
                "position": 1024
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/G2B_MATH500_paraphrased_Accuracy.png",
                "caption": "Figure 11:Paraphrasing experiment for G-2B. H paraphrased CoTs by G-27B-IT model performs better than the original H CoTs.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/L8B_MATH500_paraphrased_Accuracy.png",
                "caption": "Figure 12:Paraphrasing experiment for L-8B. H paraphrased CoTs by L-70B-IT model performs better than the original H CoTs.",
                "position": 1071
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/error_intro_results_acc_gemma_2_2b_64BS.png",
                "caption": "(a)Accuracy plots as we include the fully incorrect CoTs progressively.",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/error_intro_results_acc_gemma_2_2b_64BS.png",
                "caption": "(a)Accuracy plots as we include the fully incorrect CoTs progressively.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/G-2B_training_losses_error_intro_all.png",
                "caption": "(b)Loss curves for human written CoTs, CoTs leading to incorrect answers, and CoTs with progressively fully incorrect traces.",
                "position": 1082
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/G-2B_training_losses_GSM8K.png",
                "caption": "(a)Loss curves for Gemma-2-2B finetuning on GSM8K datasets.",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/G-2B_training_losses_GSM8K.png",
                "caption": "(a)Loss curves for Gemma-2-2B finetuning on GSM8K datasets.",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/Q1.5B_training_losses_GSM8K.png",
                "caption": "(b)Loss curves for Qwen-2.5-1.5B finetuning on GSM8K datasets.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/rebuttal/L-8B_training_losses_GSM8K.png",
                "caption": "(c)Loss curves for Llama-3.1-8B-8B finetuning on GSM8K datasets.",
                "position": 1106
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/countdown/L8B_Countdown_Test_acc.png",
                "caption": "(a)Results of Llama-3.1-8B model on Countdown task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/countdown/L8B_Countdown_Test_acc.png",
                "caption": "(a)Results of Llama-3.1-8B model on Countdown task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/countdown/Q-1.5B_Countdown_Test_acc.png",
                "caption": "(b)Results of Qwen-2.5-1.5B model on Countdown task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/countdown/G-2B_Countdown_Test_acc.png",
                "caption": "(c)Results of Gemma-2-2B model on Countdown task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/gsm8k/L8B_GSM8K_Test_sebset_acc.png",
                "caption": "(a)Results of Llama-3.1-8B model on GSM8K task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/gsm8k/L8B_GSM8K_Test_sebset_acc.png",
                "caption": "(a)Results of Llama-3.1-8B model on GSM8K task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/gsm8k/Q-1.5B_GSM8K_Test_acc.png",
                "caption": "(b)Results of Qwen-2.5-1.5B model on GSM8K task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2512.22255/images/gsm8k/G-2B_GSM8K_Test_acc.png",
                "caption": "(c)Results of Gemma-2-2B model on GSM8K task after SFT on Gemma-2-27B-It generated CoTs datasets leading to all correct and all incorrect answers",
                "position": 1149
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]