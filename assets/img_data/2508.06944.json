[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06944/x1.png",
                "caption": "Figure 1:Overview of AMFT’s motivation and framework.",
                "position": 249
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06944/x2.png",
                "caption": "Figure 2:AMFT learning dynamics vs. sequential baselines on math benchmarks. The left y-axis shows validation accuracy (%), while the right y-axis shows the adaptive weightμ\\mu. AMFT (solid blue) achieves a superior learning curve by dynamically adjustingμ\\mu(red dash-dotted), avoiding the difficult and manually-tuned trade-offs of sequential SFT→\\rightarrowRL methods.",
                "position": 884
            },
            {
                "img": "https://arxiv.org/html/2508.06944/x3.png",
                "caption": "Figure 3:Comparative analysis of training dynamics between AMFT and a pure RL-only (GRPO) baseline on mathematical reasoning tasks. The main 3D visualization (left) plots the learning trajectories across training steps, outcome rewards, and policy entropy. For clarity, 2D projections for policy entropy (top right) and outcome rewards (bottom right) are provided. The plots reveal two distinct behaviors: the RL-only policy rapidly converges to a low-entropy state (policy collapse), limiting its reward potential.",
                "position": 887
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Foundations of AMFT",
        "images": []
    },
    {
        "header": "Appendix BAMFT Algorithm Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CExperimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06944/x4.png",
                "caption": "Figure 4:General Points:An example of the sequential revision formulation with a verifier. The illustration is from[4].",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2508.06944/x5.png",
                "caption": "Figure 5:V-IRL:Demonstration of one navigation task. The navigation procedure is shown at the top, with the navigation instructions displayed below. Visual observation-related information is highlighted in green, while action-related information is marked in orange. The illustration is from[4].",
                "position": 2524
            }
        ]
    },
    {
        "header": "Appendix DAdditional Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.06944/x6.png",
                "caption": "Figure 6:Learning dynamics on theGeneral Pointsbenchmark. The left y-axis shows the ID Win Rate (%), while the right y-axis shows the adaptive weightμ\\mu. AMFT (solid blue) demonstrates a superior and more stable learning trajectory by dynamically adjustingμ\\mu(red dash-dotted) to learn an optimal curriculum, starting with SFT-dominance (highμ\\mu) and smoothly transitioning to RL-dominance (lowμ\\mu).",
                "position": 3130
            },
            {
                "img": "https://arxiv.org/html/2508.06944/x7.png",
                "caption": "Figure 7:Learning dynamics on theV-IRL Navigationbenchmark. The left y-axis shows the ID Success Rate (%), and the right y-axis shows the adaptive weightμ\\mu. AMFT’s learned curriculum, visualized by the red dash-dottedμ\\mucurve, enables it to build upon a strong SFT foundation and leverage RL to achieve the highest final performance, surpassing all baselines.",
                "position": 3154
            },
            {
                "img": "https://arxiv.org/html/2508.06944/Figure/2x2grid_example.jpeg",
                "caption": "Figure 10:Problem statement for the revised vision-language navigation case study, based on the provided high-resolution visual grid.",
                "position": 3326
            }
        ]
    },
    {
        "header": "Appendix EFurther Studies on AMFT Controller",
        "images": []
    }
]