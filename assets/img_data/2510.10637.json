[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1INTRODUCTION",
        "images": []
    },
    {
        "header": "2RELATED WORK",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10637/x1.png",
                "caption": "Figure 1:Pipeline of RoboSimGS. Starting from multi-view images, we first perform Scene Reconstruction to create a hybrid representation with a photorealistic 3DGS background and interactive mesh objects. A key step involves using a Multi-modal Large Language Model (MLLM) for automatic Physics Estimation and Articulation Inference. The scene is then aligned with the simulator with Sim2Real Environment Alignment. Finally, we apply Holistic Scene Augmentation to generate diverse simulated data. Policies trained on this data can be deployed directly to the real world.",
                "position": 217
            }
        ]
    },
    {
        "header": "3Methodlogy",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10637/x2.png",
                "caption": "Figure 2:Qualitative comparisonbetween simulated data from RoboSimGS (left) and real-world data (right).",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2510.10637/x3.png",
                "caption": "Figure 3:Task illustration. We design eight manipulation tasks for real-world evaluation:Stack Cubes,PickPlace,Deformable PickPlace,Upright Bottle,Move Bottle,Drawer Close,Box Close,Wiping, whose details are shown in Section.4.1.4.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2510.10637/x4.png",
                "caption": "Figure 4:Visualizationof policy performance under four challenging generalization settings designed to test robustness.",
                "position": 707
            },
            {
                "img": "https://arxiv.org/html/2510.10637/x5.png",
                "caption": "Figure 5:Data scaling analysisfor Diffusion Policy(Chi et al.,2023)on theStack Cubestask. The plot compares the success rate of policies trained on varying amounts ofreal-world dataversus purelysimulated data generated by RoboSimGS. Notably, the policy trained on 200 simulated demonstrations achieves a success rate comparable to one trained on 100 real-world demonstrations, highlighting the high quality and data efficiency of our generated data.",
                "position": 710
            }
        ]
    },
    {
        "header": "4EXPERIMENTS",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.10637/x6.png",
                "caption": "Figure 6:Policy Performance Analysis Across Training and Evaluation Domains. This chart compares the success rates of policies on eight tasks. We evaluate four distinct scenarios to analyze the domain gaps: policies are trained on data from our RoboSimGS (‘Sim’) or from real-world demonstrations (‘Real’), and then evaluated in both domains.",
                "position": 897
            }
        ]
    },
    {
        "header": "5CONCLUSIONS",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]