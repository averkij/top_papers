[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Dynamic SAE Guardrails (DSG)",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/SAE_plotv2.png",
                "caption": "Figure 1:An illustration of DSG",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/fig_tk_percent.png",
                "caption": "Figure 2:Distribution ofÏâ¢(x)ðœŒð‘¥\\rho(x)italic_Ï ( italic_x )for unlearning on WMDP-Bio. Threshold at 95th percentile (dashed red line) separates MMLU from WMDP.",
                "position": 475
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08192/x1.png",
                "caption": "Figure 3:Unlearning performance on WMDP-Bio (left) and WMDP-Cyber (right). Higher MMLU accuracy and lower WMDP accuracy is better. Clamp strengths (cð‘citalic_c) used for DSG points are shown as annotations. DSG Pareto-dominates the top four baseline methods (RMU, SCRUB, Farrell et al., SSD).",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x1.png",
                "caption": "",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x2.png",
                "caption": "",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x3.png",
                "caption": "Figure 4:(a) Scalability: Performance across increasing forget set sizes. (b) Sequential Unlearning: Performance across sequential unlearning requests",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x4.png",
                "caption": "Figure 5:Relearning attack resistance across finetuning epochs. (a) DSG demonstrates superior resistance to relearning compared to RMU. (b) Test-time DSG preserves MMLU utility better than Train-time DSG while still providing significant protection.",
                "position": 892
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x4.png",
                "caption": "",
                "position": 895
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x5.png",
                "caption": "",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/zero_shot.png",
                "caption": "Figure 6:Data efficiency analysis of DSG. (A) Performance across varying training data sizes compared to RMU. (B) Zero-shot performance on WMDP-Bio (left) and WMDP-Cyber (right) using 20 features selected via Neuropedia API with differentÏ„ðœ\\tauitalic_Ï„thresholds (shown next to each data point).",
                "position": 937
            },
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/ablations.png",
                "caption": "Figure 7:DSG Ablation studies (A) Static vs. dynamic clamping comparison with varying clamp strengths [10-500] for 20 and 30 features. (B) Effect of dynamic threshold percentile (pdynsubscriptð‘dyn\\smash{p_{\\text{dyn}}}italic_p start_POSTSUBSCRIPT dyn end_POSTSUBSCRIPT) on performance (C) Impact of importance ratio threshold (pratiosubscriptð‘ratiop_{\\text{ratio}}italic_p start_POSTSUBSCRIPT ratio end_POSTSUBSCRIPT, range 75-95) for 20 and 30 features.",
                "position": 951
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Background and Related Work Details",
        "images": []
    },
    {
        "header": "Appendix BFisher Information Approximation Proof",
        "images": []
    },
    {
        "header": "Appendix CConnecting Fisher Information to Causal Influence",
        "images": []
    },
    {
        "header": "Appendix DProof of Neyman-Pearson Optimality",
        "images": []
    },
    {
        "header": "Appendix EProof of Dynamic Clamping Dominance",
        "images": []
    },
    {
        "header": "Appendix FDistribution of token activations on WMDP-Cyber",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/fig_tk_percent_cyber.png",
                "caption": "Figure 8:Distribution of forget-set activated tokens for WMDP-Cyber. Threshold at the 95th percentile (dashed red line) effectively separates MMLU from WMDP.",
                "position": 2633
            }
        ]
    },
    {
        "header": "Appendix GUnlearning on WMDP",
        "images": []
    },
    {
        "header": "Appendix HUnlearning on MUSE",
        "images": []
    },
    {
        "header": "Appendix IRelearning attack",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08192/x6.png",
                "caption": "((a))",
                "position": 3299
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x6.png",
                "caption": "((a))",
                "position": 3302
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x7.png",
                "caption": "((b))",
                "position": 3307
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x8.png",
                "caption": "Figure 10:Relearning attack performance with reduced learning rate (1e-6). All configurations show minimal performance changes across finetuning epochs, demonstrating that relearning attack efficacy is strongly dependent on learning rate.",
                "position": 3392
            }
        ]
    },
    {
        "header": "Appendix JData Efficiency and Zero-shot Capabilities",
        "images": []
    },
    {
        "header": "Appendix KAblations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/multiplier.png",
                "caption": "Figure 11:Effect of clamp strengthcð‘citalic_con DSG performance across different feature counts. MMLU accuracy (solid lines) remains consistently high (>99%absentpercent99>99\\%> 99 %) for 10-20 features across all clamp values, while WMDP-Bio accuracy (dashed lines) drops sharply even at modest clamp strengths (c=25ð‘25c=25italic_c = 25). This demonstrates DSGâ€™s ability to effectively remove targeted knowledge while preserving general model capabilities with minimal parameter sensitivity.",
                "position": 3569
            },
            {
                "img": "https://arxiv.org/html/2504.08192/x9.png",
                "caption": "Figure 12:Total Variation Distance (TVD) between WikiText and benchmark datasets using percentage-based (ÏðœŒ\\rhoitalic_Ï) vs. raw count-based (ÏrawsubscriptðœŒraw\\rho_{\\text{raw}}italic_Ï start_POSTSUBSCRIPT raw end_POSTSUBSCRIPT) metrics. Lower TVD between WikiText and MMLU indicates better alignment of retain sets, while higher TVD between WikiText and WMDP indicates better separation between retain and forget distributions. Percentage-based metrics consistently outperform raw counts on both measures across all benchmarks.",
                "position": 3616
            }
        ]
    },
    {
        "header": "Appendix LComputational Cost (Inference Latency)",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08192/extracted/6352696/figures/sequences_activations.png",
                "caption": "Figure 13:Feature Activations on Example Sequences from Forget Sets.(A) WMDP-Bio sequence with words highlighted in green indicating activation values>0absent0>0> 0for feature ID (top) 373 and (bottom) 10933. (B) WMDP-Cyber sequence with words highlighted in green indicating activation values>0absent0>0> 0for feature ID (top) 15286 and (bottom) 2905. Activation magnitudes are reported above the words in grey.",
                "position": 3739
            }
        ]
    },
    {
        "header": "Appendix MFeature Interpretability",
        "images": []
    }
]