[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22607/x1.png",
                "caption": "Figure 1:An overview of the proposed Progressive Curriculum Reinforcement Learning (PCuRL) framework. It consists of two key components: (1) a multi-stage curriculum RL structure that utilizes online difficulty soft weighting, which partitions the training progress into different stages based on task difficulty; (2) a dynamic length reward mechanism that encourages the model to adapt its reasoning chain length according to task complexity, rather than indiscriminately increasing it. In the Easy stage, the model tends to assign higher weights to relatively easier questions for policy optimization, a pattern that similarly applies to the Medium and Hard stages.",
                "position": 190
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22607/x2.png",
                "caption": "(a)ODSW Easy",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x2.png",
                "caption": "(a)ODSW Easy",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x3.png",
                "caption": "(b)ODSW Medium",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x4.png",
                "caption": "(c)ODSW Hard",
                "position": 274
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22607/x5.png",
                "caption": "Figure 3:Performance comparison of models trained with different length reward strategies. “Dynamic-NNitalic_N” denotes models employing our dynamic length reward with a target length ofNNitalic_Nduring the final stage of curriculum RL. “Fix-NNitalic_N” refers to models trained with a fixed-length reward that enforces the fixed target length ofNNitalic_Nacross all responses. We visualize both the average response length and the overall accuracy across selected benchmarks.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x6.png",
                "caption": "(a)Average Reward Curve",
                "position": 968
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x6.png",
                "caption": "(a)Average Reward Curve",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x7.png",
                "caption": "(b)Validation Accuracy Curve",
                "position": 976
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x8.png",
                "caption": "(c)Average Response Length Curve",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2507.22607/x9.png",
                "caption": "Figure 5:Case studies ofVL-Cogito, where samples are drawn from multiple benchmarks, including MMStar, ScienceQA, Geometry@3K, and MathVision.",
                "position": 995
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Data Statistics",
        "images": []
    },
    {
        "header": "8System Prompt",
        "images": []
    }
]