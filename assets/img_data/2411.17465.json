[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17465/x1.png",
                "caption": "Figure 1:ShowUI is a Vision-Language-Action model for GUI Automation.\nGiven an environment screenshot, ShowUI efficiently processes it using UI-guided token selection for visual modeling and outputs an interaction action within the loop.",
                "position": 103
            },
            {
                "img": "https://arxiv.org/html/2411.17465/x2.png",
                "caption": "",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2411.17465/x3.png",
                "caption": "",
                "position": 118
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17465/x4.png",
                "caption": "Figure 3:Illustration of ShowUI.Given a user task query, a pre-defined action space, and an initial screenshot as observation, ShowUI proceeds by executing the next action, updating the screenshot, and continuing in this cycle.\nNotably, ShowUI features the following key innovation designs:(i) UI-Guided Visual Token Selection, which processes the input screenshot to build UI patch-wise connected graph. During training, a random subset of tokens is selected within each component for efficient visual modeling (Sec.2.1).(ii) Interleaved Vision-Language-Action Streamingto effectively handle past screenshots and actions, improving navigation performance. (Sec.2.2).",
                "position": 177
            }
        ]
    },
    {
        "header": "2ShowUI",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17465/x5.png",
                "caption": "(a)UI Connected Graph adaptively assigns connected components based on the informativeness of screenshots.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2411.17465/x5.png",
                "caption": "(a)UI Connected Graph adaptively assigns connected components based on the informativeness of screenshots.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2411.17465/x6.png",
                "caption": "(b)Two representative token compression methods, where patches of the same color indicate the same component and are redundant to each other.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/mobile_0affae67-191e-43fa-890d-778555ffbab0_1484x672_1272_781.png",
                "caption": "(a)1272 tokens→→\\rightarrow→781 components",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/mobile_0affae67-191e-43fa-890d-778555ffbab0_1484x672_1272_781.png",
                "caption": "(a)1272 tokens→→\\rightarrow→781 components",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/mobile_3b5ab7d3-8e84-4be1-83ba-801882db294b_1512x672_1296_359.png",
                "caption": "(b)1272 tokens→→\\rightarrow→359 components",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/mobile_0b63fa69-df33-442d-b7ea-9393432f71ea_1484x672_1272_265.png",
                "caption": "(c)1272 tokens→→\\rightarrow→265 components",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/mobile_1c87bbf1-afc8-4fe3-aaf0-d3bacc52a2bf_1512x672_1296_175.png",
                "caption": "(d)1272 tokens→→\\rightarrow→175 components",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/pc_01c641cb-aa78-4bae-80ea-dda820dd00eb_532x952_646_281.png",
                "caption": "(e)646 tokens→→\\rightarrow→281 components",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/pc_5da8f8cd-8ba2-490a-86e0-5c82421a9bee_532x952_646_230.png",
                "caption": "(f)646 tokens→→\\rightarrow→230 components",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/web_2bda50e8-15d0-473a-a8cb-bd0777963756_756x1344_1296_740.png",
                "caption": "(g)1296 tokens→→\\rightarrow→740 components",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/uigraph/web_0c5f692d-4c32-4533-ac2a-d6f8d7c6d7c1_756x1344_1296_369.png",
                "caption": "(h)1296 tokens→→\\rightarrow→369 components",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2411.17465/x7.png",
                "caption": "Figure 6:Illustration of Interleaved Vision-Text-Action Streaming.The visual tokens in screenshots are significantly longer (e.g., 1.3K) compared to queries or actions (e.g., fewer than 10).\nThus, we introduce two modes:(a)Action-Visual Streaming for UI navigation and(b)Action-Query Streaming for UI grounding. These modes extend the concept of multi-turn dialogue and enable more efficient utilization of training data.",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2411.17465/x8.png",
                "caption": "Figure 7:We derive three types of query (appearance, spatial relationship, and intention) from raw annotation, assisted by GPT-4o.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/weather.png",
                "caption": "(a)Example of Weather.Original:‘visibility’;Appearance:“A rectangular box with 28 km in white text.”;Spatial:“Positioned below ‘WIND’ and next to ‘PRESSURE’.”;Intention:“Check current fog or mist conditions.”",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/weather.png",
                "caption": "(a)Example of Weather.Original:‘visibility’;Appearance:“A rectangular box with 28 km in white text.”;Spatial:“Positioned below ‘WIND’ and next to ‘PRESSURE’.”;Intention:“Check current fog or mist conditions.”",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/stock.png",
                "caption": "(b)Example of Stocks.Original:‘Share option-health insurance’;Appearance:“Three vertical dots icon on a dark background.”;Spatial:“Located to the right of the health insurance headline.”;Intention:“Click to share the health insurance article.”",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/wechat.png",
                "caption": "(c)Example of WeChat.Original:‘expand_profile’;Appearance:“A rounded gray button with a person icon.”;Spatial:“Located at the top-left corner of the chat pane.”;Intention:“Expand the contact’s profile view.”",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/vlc.png",
                "caption": "(d)Example of VLC.Original:‘Play’;Appearance:“White triangle icon within a black circular button.”;Spatial:“Located at the bottom left corner of the screen.”;Intention:“Click to play the video.”",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/terminal.png",
                "caption": "(e)Example of Terminal.Original:‘create_new_tab’;Appearance:“A small ’+’ icon in a gray tab bar.”;Spatial:“Located at the far right of the tab bar.”;Intention:“Open a new terminal tab.”",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/gpt4o/todo.png",
                "caption": "(f)Example of Todo.Original:‘view as list’;Appearance:“A gray, vertical button with a box and lines icon.”;Spatial:“Positioned at the top right beside the search bar.”;Intention:“Switch to list view.”",
                "position": 478
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/streaming-history-grounding.png",
                "caption": "Figure 10:Impact by Interleaved action-query streaming on Grounding task: trained with 119K grounding data, Eval with Screenspot.",
                "position": 1327
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/streaming-history-grounding.png",
                "caption": "Figure 10:Impact by Interleaved action-query streaming on Grounding task: trained with 119K grounding data, Eval with Screenspot.",
                "position": 1330
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/streaming-history-navigation.png",
                "caption": "Figure 11:Impact by Interleaved action-visual streaming on Navigation task: trained with GUIAct, Eval with AITW.",
                "position": 1336
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_desktop_icon_screenspot_216_open_wechat.png",
                "caption": "(a)✓Instruction: “Open wechat”.With instruction-tuning, ShowUI can recognize the appearance of the WeChat icon.",
                "position": 1486
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_desktop_icon_screenspot_216_open_wechat.png",
                "caption": "(a)✓Instruction: “Open wechat”.With instruction-tuning, ShowUI can recognize the appearance of the WeChat icon.",
                "position": 1489
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_desktop_icon_screenspot_148_rotate_left.png",
                "caption": "(b)✓Instruction: “Rotate left”.ShowUI distinguishes the correct operator among multiple abstract symbols.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/False_desktop_icon_screenspot_190_zoom_in.png",
                "caption": "(c)✗Instruction: “Zoom in”.The model visually confuses the difference between zoom in and zoom out.",
                "position": 1506
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/False_desktop_text_screenspot_158_sign_in.png",
                "caption": "(d)✗Instruction: “Sign in”.There are two possible sign-in elements, but the query lacks sufficient information to determine the correct one.",
                "position": 1513
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_web_icon_screenspot_402_view_help_for_email_account.png",
                "caption": "(e)✓Instruction: “view help for email account”.ShowUI is able to associate “view help” with question mark clickable element.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/False_web_icon_screenspot_1_view_my_account.png",
                "caption": "(f)✗Instruction: “view my account”.‘View my account’ could be interpreted as ‘Click Your Profile’ or ‘User Profile’ (top right), leading to confusion.",
                "position": 1529
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_mobile_icon_screenspot_125_forwarding.png",
                "caption": "(a)✓Instruction: “Forwarding”.ShowUI can identify what a forwarding button should look like.",
                "position": 1540
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_mobile_icon_screenspot_125_forwarding.png",
                "caption": "(a)✓Instruction: “Forwarding”.ShowUI can identify what a forwarding button should look like.",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/True_mobile_icon_screenspot_58_open_allow_siri_when_locked.png",
                "caption": "(b)✓Instruction: “Open allow siri when locked”.ShowUI identifies the clickable element instead of the text itself.",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/False_mobile_text_screenspot_306_insert_from_link.png",
                "caption": "(c)✗Instruction: “Insert from link”.The query being confusing as it contain both “Insert from” and “link”",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2411.17465/extracted/6025965/figures/screenspot/False_mobile_text_screenspot_452_show_softwares.png",
                "caption": "(d)✗Instruction: “Show softwares”.The screenshot includes two software interfaces, causing confusion for the model.",
                "position": 1564
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Appendix ADatasets details",
        "images": []
    },
    {
        "header": "Appendix BSettings",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]