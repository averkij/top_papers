[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22173/logos/project_page.png",
                "caption": "",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2511.22173/logos/github.png",
                "caption": "",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2511.22173/logos/huggingface.png",
                "caption": "",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x1.png",
                "caption": "Figure 1:(Left)Strong LMs such as Claude-Sonnet-4 can self-refine effectively on AIME-24, where they already solve problems reasonably well in the first iteration. However, on saturated benchmarks such as MATH-500, there is little headroom for improvement, and on our proposed benchmark,RefineBench, performance gains remain limited. Hence,RefineBenchserves as a testbed for measuring self-refinement capability of frontier LMs.(Right)The biggest bottleneck when an LM (Gemini-2.5-Pro) refines its output is that it often struggles to identify which aspects need to be corrected. InRefineBench, beyond the self-refinement setting where the LM must independently identify and fix errors, we also introduce settings where partial hints are provided about what needs to be revised, or where the amount of feedback varies. This enables a systematic analysis of refinement capability.",
                "position": 190
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3RefineBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22173/x2.png",
                "caption": "Figure 2:An example fromRefineBench(left) and an overview of the two evaluation protocols (i.e., self-refinement, guided refinement) inRefineBench(right).",
                "position": 247
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x3.png",
                "caption": "Table 1:Basic Statistics.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x3.png",
                "caption": "Figure 3:Distribution of domain categories.",
                "position": 363
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22173/x4.png",
                "caption": "Table 4:Self-refinement performance inPasst{}_{\\texttt{t}}when we provide the evaluation criteria.",
                "position": 1099
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x4.png",
                "caption": "Table 4:Self-refinement performance inPasst{}_{\\texttt{t}}when we provide the evaluation criteria.",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x5.png",
                "caption": "",
                "position": 1164
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x6.png",
                "caption": "Figure 6:Average termination turn ratio in self-refinement across 32 instruction-tuned and reasoning models.",
                "position": 1173
            }
        ]
    },
    {
        "header": "5Discussions and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22173/x7.png",
                "caption": "Figure 7:Correlation between the average termination turn ratio andPass5.",
                "position": 1212
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x8.png",
                "caption": "Figure 8:Reasoning behavior analysis of DeepSeek-R1.",
                "position": 1241
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x9.png",
                "caption": "Figure 9:Transition analysis of the self-refinement capability of DeepSeek-R1 onRefineBenchunder the self-refinement setting. Additional results are presented in AppendixE.6.",
                "position": 1256
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Appendix ABroader Impact",
        "images": []
    },
    {
        "header": "Appendix BLimitations",
        "images": []
    },
    {
        "header": "Appendix CUse of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix DDetails ofRefineBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22173/x10.png",
                "caption": "Figure 10:Diversity ofNNchecklist items.",
                "position": 1542
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22173/x11.png",
                "caption": "Figure 11:Overall human evaluation result of checklist quality",
                "position": 2817
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x12.png",
                "caption": "Figure 12:Correlation between process quality (Acc1) and answer correctness for exact match problems in Gemini-2.5-Pro results.",
                "position": 2861
            },
            {
                "img": "https://arxiv.org/html/2511.22173/x13.png",
                "caption": "Figure 13:Transition analysis of the self-refinement capability of four different reasoning models: DeepSeek-R1-Distill-Qwen-{1.5, 7, 14, 32}B.",
                "position": 2875
            }
        ]
    },
    {
        "header": "Appendix FExtended Related Works",
        "images": []
    },
    {
        "header": "Appendix GPrompt Templates",
        "images": []
    },
    {
        "header": "Appendix HRefineBenchExamples",
        "images": []
    }
]