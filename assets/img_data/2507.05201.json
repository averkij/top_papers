[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05201/x1.png",
                "caption": "Figure 1:Overview of the MedGemma model collectionfeaturing the MedSigLIP image encoder, MedGemma 4B Multimodal and MedGemma 27B Text",
                "position": 145
            }
        ]
    },
    {
        "header": "2Methods",
        "images": []
    },
    {
        "header": "3MedGemma Evaluations",
        "images": []
    },
    {
        "header": "4MedGemma Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05201/x2.png",
                "caption": "Figure 2:Example of CXR and dermatology image dialogue via open-ended question answering with MedGemma 4B.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2507.05201/x3.png",
                "caption": "",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2507.05201/x4.png",
                "caption": "Figure 3:Example of histopathology open-ended question answering with MedGemma 4B.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2507.05201/extracted/6606653/figures/medgemmacxr.png",
                "caption": "Figure 4:The MedGemma 4B PT model was used to generate radiology reports on the MIMIC-CXR test set. A single board-certified thoracic radiologist reviewed the MIMIC-CXR report, generated report, and the corresponding CXR image to judge the quality of the reports. Images were reviewed using the original DICOMs on a clinical diagnostic viewer. Across all cases, 81% of MedGemma’s CXR reports resulted in the same or superior clinical decision in comparison to the original reports.",
                "position": 2018
            }
        ]
    },
    {
        "header": "5MedGemma Fine-tuning Demonstration",
        "images": []
    },
    {
        "header": "6MedSigLIP Evaluations",
        "images": []
    },
    {
        "header": "7MedSigLIP Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05201/x5.png",
                "caption": "Figure 5:Average results for data efficient learning on 7 Chest X-ray findings on CheXpert and CXR14 datasets compared to HAI-DEF’s CXR Foundation model based on ELIXR(Xu et al.,2023).\nIndividual results per condition per dataset can be found in Appendix FiguresA1andA2.",
                "position": 2436
            }
        ]
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "10Model availability",
        "images": []
    },
    {
        "header": "11Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AManual Evaluation of Radiology reports",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Prompts",
        "images": []
    },
    {
        "header": "Appendix CComparison of CXR data-efficient learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05201/x6.png",
                "caption": "Figure A1:Individual results for CXR data efficient learning on CheXpert datasets comparing to ELIXR(Xu et al.,2023)",
                "position": 6792
            },
            {
                "img": "https://arxiv.org/html/2507.05201/x7.png",
                "caption": "Figure A2:Individual results for CXR data efficient learning on CXR14 datasets comparing to ELIXR(Xu et al.,2023)",
                "position": 6796
            }
        ]
    },
    {
        "header": "Appendix DAdditional details about EHRQA",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05201/extracted/6606653/figures/ehrqa-medgemma-combined.png",
                "caption": "Figure A3:EHRQA detailed results, comparing MedGemma 27B (text-only) before and after RL-tuning, along with Gemini 2.5 Pro",
                "position": 6808
            }
        ]
    },
    {
        "header": "Appendix EAdditional medical reasoning examples",
        "images": []
    },
    {
        "header": "Appendix FMedGemma 27B multimodal model",
        "images": []
    }
]