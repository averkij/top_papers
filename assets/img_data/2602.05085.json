[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05085/x1.png",
                "caption": "Figure 1:Illustration of a typical dense transformer layer with FFN interpreted as a soft look-up table memory, in comparison with the attention mechanism, which is a contextual soft look-up table mechanism. The GLU variant follows a similar structure but with an additional gating mechanism.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2602.05085/x2.png",
                "caption": "Figure 2:Architecture of the proposed Locas parametric memory integrated as a sideway FFN module in transformer layers. The memory module operates in parallel with the backbone FFN, with its output scaled and added to the main pathway. This design enables genuine model capacity expansion at test time while preserving the backbone modelâ€™s pretrained representations.",
                "position": 305
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": []
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]