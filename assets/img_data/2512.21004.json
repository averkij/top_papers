[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21004/x1.png",
                "caption": "(a)Pretraining paradigm using masked next frame generation",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2512.21004/x1.png",
                "caption": "(a)Pretraining paradigm using masked next frame generation",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2512.21004/x2.png",
                "caption": "(b)Overview of our proposed framework",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2512.21004/x3.png",
                "caption": "Figure 2:Overview of the proposed pretraining pipeline. The input video is split into two branches. One branch is masked and fed into the encoder to obtain local representations, while the other branch with the entire sequence is processed by the reference encoder for representation alignment. The autoregressive predictor employs cross-attention to aggregate local features and predict the representations for the next frame. The predicted representations is then aligned with the reference encoder and passed to the flow-matching decoder to generate the VAE latent features of the next frame. The custom attention masks for autoregressive modeling are shown inFig.3.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21004/x4.png",
                "caption": "Figure 3:Custom attention masks for autoregressive modeling. The frame-wise causal mask temporally aggregates video frames, while the autoregressive mask ensures that a frame can only see previous frames. Frame-isolated mask enables individual frame generation, effectively preventing information leakage.",
                "position": 198
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21004/x5.png",
                "caption": "(a)Data scaling",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2512.21004/x5.png",
                "caption": "(a)Data scaling",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2512.21004/x6.png",
                "caption": "(b)Model scaling",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2512.21004/x7.png",
                "caption": "Figure 5:Visualization of generation. Top and middle: Masked generation on ImageNet and SSv2, where “ori” denotes the original samples. Bottom: Autoregressive generation based on a seed image from ImageNet.",
                "position": 1128
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21004/x8.png",
                "caption": "Figure 6:Loss curve of the pretraining process. The training stages highlighted with different background colors correspond toTab.8.",
                "position": 1598
            }
        ]
    },
    {
        "header": "Appendix APretraining Details",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21004/figs/tsne.png",
                "caption": "Figure 7:t-SNE visualization of ImageNet validation set representations. Different colors indicate different image categories.",
                "position": 2439
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    }
]