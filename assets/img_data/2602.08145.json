[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x1.png",
                "caption": "Figure 1:Overview of reliable and responsible foundation models.This survey comprehensively summarizes existing research from nine critical dimensions: bias and fairness, alignment, security, privacy, hallucination, uncertainty, distribution shift, explainability, and Artificial Intelligence-Generated Content\n(AIGC) detection. We organize foundation models into four categories, including\nLarge Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models, and Video Generative Models, to illustrate how each category uniquely interacts with these dimensions. Additionally, we explore how these dimensions interact and reinforce one another to highlight their synergies and shared challenges.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x2.png",
                "caption": "Figure 2:Foundation models are typically trained on diverse modalities and then adapted for downstream applications. Throughout this pipeline, various reliable and responsible issues emerge at different stages.",
                "position": 337
            }
        ]
    },
    {
        "header": "2Types of Foundation Models",
        "images": []
    },
    {
        "header": "3Bias and Fairness",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x3.png",
                "caption": "Figure 4:An example of gender bias in LLM responses.",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x4.png",
                "caption": "Figure 5:An overview of strategies for evaluating and mitigating bias in LLMs, covering evaluation via feature embedding, generated text, and token selection probability and mitigation during training or inference.",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x5.png",
                "caption": "Figure 6:Examples of gender biases in image generation models: DALL·E shows a spurious correlation between gender and profession.",
                "position": 1114
            }
        ]
    },
    {
        "header": "4Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x6.png",
                "caption": "Figure 8:Alignment is required at different stages in the foundation models. Typically, LLMs are aligned using SFT and RLHF during post-training, while using prompt engineerfing at inference time. Compared to LLMs, MLLMs require an additional step of multimodal alignment at post-training, such as Visual Instruction Tuning.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x7.png",
                "caption": "Figure 9:The evolution of Reinforcement Learning from Human Feedback (RLHF), illustrating three key areas of advancement. Panel (a) depicts the conventional RLHF pipeline and the move Beyond Conventional Reward Models. Panel (b) shows the shift Beyond Human-Annotated Data to include AI-generated feedback. Panel (c) illustrates the trend of moving Beyond Proximal Policy Optimization (PPO) towards simpler, direct preference optimization objectives.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x8.png",
                "caption": "Figure 10:An overview of four major prompt engineering methods for guiding LLMs. (a) Continuous Prompts are soft prompts represented as tunable vectors in the embedding space. (b) Discrete Prompts are interpretable natural language prompts that can be automatically generated. (c) Chain-of-Thought prompting elicits intermediate reasoning steps to solve complex problems. (d) Prompt Optimization uses feedback from the LLM itself to iteratively refine prompts and improve performance.",
                "position": 1483
            }
        ]
    },
    {
        "header": "5Security",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x9.png",
                "caption": "Figure 11:Attacks on various foundation models in training and inference stages. All models suffer from Backdoor Attack and Jailbreak Attack. For the category of “Other Attacks\", we include Prompt Injection Attacks in LLMs, Image Adversarial Attacks in MLLMs, and Adversarial Attacks in Image Generative Models.",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x10.png",
                "caption": "Figure 13:An example of image adversarial attacks for MLLMs via gradient descent, where harmful textual output is generated.",
                "position": 1631
            }
        ]
    },
    {
        "header": "6Privacy",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x11.png",
                "caption": "Figure 14:Privacy threat and protection techniques in different types of foundation models, including LLM, MLLM, and T2I models.",
                "position": 1754
            }
        ]
    },
    {
        "header": "7Hallucination",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x12.png",
                "caption": "Figure 17:Examples of factuality and faithfulness hallucinations in foundation models.",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x13.png",
                "caption": "Figure 18:An overview of hallucinations in MLLMs. The top row illustrates a typical MLLM pipeline from user prompt to model response. Hallucinations can emerge at different stages due to multiple sources. To mitigate these hallucinations, various strategies are employed, including improvements to data curation, training methods, and inference approaches.",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x14.png",
                "caption": "Figure 19:Illustration of hallucination detection in foundation models. The process generally involves three steps: (1) decomposing the model output into individual factual units, (2) verifying each fact against external knowledge or ground truth, and (3) aggregating the correctness scores to produce an overall hallucination score. The examples show applications in both LLMs and image generation models.",
                "position": 2125
            }
        ]
    },
    {
        "header": "8Uncertainty",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x15.png",
                "caption": "Figure 21:The comparison of aleatoric and epistemic uncertainty in machine learning. Aleatoric uncertainty originates from data variability, while epistemic uncertainty results from model limitations. The former is tied to inherent noise in observations, while the latter is tied to insufficient knowledge in representation.",
                "position": 2269
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x16.png",
                "caption": "Figure 22:Overview of representative methods for estimating and mitigating uncertainty in foundation models. The illustration highlights how different answers may be generated with varying confidence levels, and categorizes existing approaches into calibration, word-based, selection, and distribution-free methods.",
                "position": 2329
            }
        ]
    },
    {
        "header": "9Distribution Shift",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x17.png",
                "caption": "Figure 23:Different types of distribution shifts in the perspectives of (1) statistics, (2) image, and (3) text. The concept shift scenarios show how two distinct classes can merge into a single class when labels change.",
                "position": 2499
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x18.png",
                "caption": "Figure 25:An overview of model editing methods in LLMs. Given an incorrect response from the original model, different editing strategies correct factual errors by modifying or augmenting the model’s knowledge.",
                "position": 2692
            }
        ]
    },
    {
        "header": "10Explainability",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x19.png",
                "caption": "Figure 26:An overview of explainability in foundation models. This figure illustrates various techniques for uncovering how different model inputs and internal components influence model outputs. The right legend highlights the role of samples in different learning stages and the typology of explanation approaches.",
                "position": 2788
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x20.png",
                "caption": "Figure 28:The influence of samples in pre-training, instruction-tuning, and in-context learning stages. We highlight the beneficial and detrimental textual fragments in green and red, respectively.",
                "position": 2902
            }
        ]
    },
    {
        "header": "11AIGC Detection",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.08145/x21.png",
                "caption": "Figure 29:An overview of AIGC detection techniques. We group them into three categories: zero-shot detectors, watermark-based detection, and neural network detectors, each with further subdivisions.",
                "position": 3066
            },
            {
                "img": "https://arxiv.org/html/2602.08145/x22.png",
                "caption": "Figure 31:Examples of zero-shot, watermark, and neural network detectors for textual and visual inputs.",
                "position": 3091
            }
        ]
    },
    {
        "header": "12Intersection and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]