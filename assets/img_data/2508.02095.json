[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/x1.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/x2.png",
                "caption": "Figure 2:Distribution of Dataset Sources and Annotations.Overview of the dataset composition, illustrating the proportions of real third-person (exo-centric) videos (DAVIS[66], YouTube-VOS[88]), real first-person (ego-centric) videos (Ego4D[32]), and synthetic videos (Cosmos[4]). The real video data is further categorized by annotation types, including translational, rotational, action, counting, and false positive queries (targeting nonexistent events to assess critical reasoning).",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x3.png",
                "caption": "Figure 3:Dataset Generation and Annotation Pipeline.Our dataset was constructed by collecting real videos and generating synthetic data, followed by human-in-the-loop quality reviews to address ambiguous videos and annotations. After temporal alignment and quality assurance, human-annotated questions and ground-truth answers were created, complemented by multiple-choice (MC) answers generated by large language models (LLMs). The final dataset includes real and synthetic video data with comprehensive VLM scoring metrics.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/x4.png",
                "caption": "Figure 4:Qualitative Examples of Dataset Annotations.(Top) A third-person (exo-centric) video with translational annotations (“camel turning left from its perspective”). (Middle) A first-person (ego-centric) video with a rotational question (“clockwise rotation of ladle”). (Bottom) A synthetic scene with motion recognition “robotic dog moving left”).",
                "position": 202
            }
        ]
    },
    {
        "header": "3TheVLM4DBenchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/figures/radar_plot.png",
                "caption": "Figure 5:Comparison of accuracy across types of spatiotemporal questions.Model accuracy is shown only for the six top-performing VLMs.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2508.02095/figures/cot_v_do.png",
                "caption": "Table 1:Evaluation onVLM4DBenchmarkacross various proprietary and open-source VLMs. Top three performers in each category are highlighted fromdark(highest) tolight(third highest). Human and random selection baselines are included for reference.",
                "position": 253
            }
        ]
    },
    {
        "header": "4Evaluation ofVLM4DBenchmark",
        "images": []
    },
    {
        "header": "5Analysis: Why VLMs Don’t Work Well?",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/figures/cot_v_do.png",
                "caption": "Figure 6:Comparison of CoT and DO Accuracy Across Models.Accuracy comparison between Chain-of-Thought (CoT) and Direct Output (DO) prompting across VLMs.",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x5.png",
                "caption": "Figure 7:Heatmap of Occurances of Spatial-Temporal Terms in popular video SFT datasets.",
                "position": 625
            }
        ]
    },
    {
        "header": "6Probing Future Solutions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AVLM4D Benchmark Statistics",
        "images": []
    },
    {
        "header": "BEvaluation setup",
        "images": []
    },
    {
        "header": "CVideo Instruction Tuning Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/figures/supp/all_cats.png",
                "caption": "Figure A:Performance comparison of various VLMs across annotated\nquestion categories including counting, rotational motion, translational motion, false positives, and action recognition",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x6.png",
                "caption": "Figure B:Heatmap of Occurrences of Spatial-Temporal Terms in LLava-178k",
                "position": 2339
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x7.png",
                "caption": "Figure C:Heatmap of Occurrences of Spatial-Temporal Terms in ShareGPT4Video",
                "position": 2349
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x8.png",
                "caption": "Figure D:Heatmap of Occurrences of Spatial-Temporal Terms in VideoChat-IT",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x9.png",
                "caption": "Figure E:Heatmap of Occurrences of Spatial-Temporal Terms in ShareGPT-4o",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x10.png",
                "caption": "Figure F:Example of a clip with multiple target categories. Spatiotemporal grounding remains a challenge, as the direction descriptor isincorrect, and the translational actions provide limited insight, primarily indicating that objects/subjects are not static.",
                "position": 2358
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x11.png",
                "caption": "Figure G:Heatmap to visualize the prevalence of co-occurring target strings across the ShareGPT-4o dataset, informing our evaluation of spatiotemporal grounding in video instruction data.",
                "position": 2361
            }
        ]
    },
    {
        "header": "DExamples of VLMs’ responses",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02095/x12.png",
                "caption": "Figure H:General Input-Output Architecture of Feature4X",
                "position": 2382
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x13.png",
                "caption": "Figure I:Complete CoT and DO Responses of VLMsModels 1-4",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x14.png",
                "caption": "Figure J:Models 5-7",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x15.png",
                "caption": "Figure K:Models 8-12",
                "position": 2391
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x16.png",
                "caption": "Figure L:Models 13-19",
                "position": 2394
            },
            {
                "img": "https://arxiv.org/html/2508.02095/x17.png",
                "caption": "Figure M:Models 20-23",
                "position": 2397
            }
        ]
    },
    {
        "header": "EDetails of 4D Reconstruction",
        "images": []
    }
]