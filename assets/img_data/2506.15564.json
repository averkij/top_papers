[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15564/x1.png",
                "caption": "Figure 1:Our approach begins by encoding input texts, images, and videos into continuous embeddings and visual latents. The visual latents are processed through a dual-path extraction and spatial (-temporal) fusion mechanism to construct unified visual representations that are scalable for both multimodal understanding and generation, image and video modalities. These text embeddings and unified visual representations are then structured into a sequence for the base language model, equipped with dedicated heads. Specifically, text tokens are modeled autoregressively by a language head, while image and video latents are handled by a flow head using flow matching. We employ the omni-attention mechanism[108,121]to enable causal attention along the sequence while maintaining full attention within the unified visual representations. This design empowers our model to effectively tackle tasks such as image/video understanding, generation, and mixed-modality generation.",
                "position": 353
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15564/x2.png",
                "caption": "Figure 2:Multimodal understanding and generation examples.",
                "position": 1140
            }
        ]
    },
    {
        "header": "5Limitations and Broader Impacts",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15564/x3.png",
                "caption": "Figure 3:Text-to-video and image-to-video generation examples.",
                "position": 2343
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]