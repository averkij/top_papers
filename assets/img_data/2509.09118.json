[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09118/x1.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x2.png",
                "caption": "",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x3.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x4.png",
                "caption": "",
                "position": 116
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09118/Figure/main-fig1-2.png",
                "caption": "(a)Current work exhibits several deficiencies",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2509.09118/Figure/main-fig1-2.png",
                "caption": "(a)Current work exhibits several deficiencies",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2509.09118/Figure/main-fig2-3.png",
                "caption": "(b)Our method for robust person representation learning",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3WebPerson Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09118/Figure/fig2_end.png",
                "caption": "Figure 2:The details of person-centric image filtering and synthetic caption generation pipeline for constructing our WebPerson dataset.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2509.09118/Figure/figure.png",
                "caption": "Figure 3:Overview of our proposed method. (a) The architecture of our proposed Gradient-Attention Guided Dual-Masking Synergic¬†(GA-DMS) framework. (b) The details of the Gradient-Attention Similarity Score¬†(GASS).",
                "position": 209
            }
        ]
    },
    {
        "header": "4GA-DMS Framework",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09118/x5.png",
                "caption": "Figure 4:Visualization of token-wise weight scores and attention maps generated by NAMTan et¬†al. (2024)and our GA-DMS.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x6.png",
                "caption": "(a)Ablation onŒ±n\\alpha_{n}for masking noise tokens",
                "position": 1406
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x6.png",
                "caption": "(a)Ablation onŒ±n\\alpha_{n}for masking noise tokens",
                "position": 1409
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x7.png",
                "caption": "(b)Ablation onŒ±i\\alpha_{i}for masking informative tokens",
                "position": 1415
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x8.png",
                "caption": "Figure 6:Data scaling analysis of WebPerson dataset.The performance of our GA-DMS method in direct transfer settings.",
                "position": 1426
            }
        ]
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09118/x9.png",
                "caption": "Figure 7:Results of different layers to computeùïä\\mathbb{S}. The encoders contain 12 layers in total.",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2509.09118/x10.png",
                "caption": "Figure 8:Visualization of some examples in our WebPerson dataset.",
                "position": 2626
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]