[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1   Introduction",
        "images": []
    },
    {
        "header": "2   Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06091/x1.png",
                "caption": "Figure 1:Overview of the data generation pipeline of MIRIAD. From top to bottom the MIRIAD processing pipeline is shown. For dataset generation, 894,352 full-text articles from the medical literature were prepared for LLM processing by partitioning the articles into passage chunks of up to 1,000 text tokens. From each passage, an LLM generated medically-related question-answer (QA) pairs, grounded in the passage. In a second step, a sequence of quality control measures was implemented to ascertain relevance, factuality, and groundedness of the semi-synthetically generated QA pairs. This processing pipeline brings forth MIRIAD, a literature-scale and hierarchical resource for operationalized medical knowledge that contains nearly 6 million medical topically organized QA pairs, each grounded in and linked to peer-reviewed literature. MIRIAD enables a range of down-stream use cases, as shown in the bottom row, including serving as a corpus of external knowledge for retrieval-augmented generation (RAG), a supervised dataset for training medical information retrievers, as well as advanced interfaces for users to visually explore, search, and navigate a structured landscape of medical queries and responses, with clickable follow-up literature.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x2.png",
                "caption": "Figure 2:MIRIAD data characteristics.a, Temporal distribution of discipline-specific question–answer (QA) pairs in the MIRIAD dataset. The dataset spans 56 distinct medical domains, which have been grouped into 10 broader categories for clarity in this visualization. This figure illustrates the relationship between the medical domain of each QA pair and the publication year of the corresponding source paper. The top panel shows absolute counts of QA pairs within a given year, the bottom row shows the relative proportion of fine-grained topics within a given year bucket. The 56 fine-grained discipline categories of MIRIAD were collapsed into 10 coarse categories in a two-step process. First, an LLM processed the categories to produce a candidate mapping. A clinical expert then edited and revised the mapping. For a fine-grained version of the same plot, refer to Fig.S3showing all 56 categories.b, The first five words of a small subset of 1000 MIRIAD questions are visualized hierarchically in a sunburst plot. Color indicates that the different questions share the same first word, e.g., “What” or “How”.c, A kernel density estimate (KDE) plot over the question-and-answer length in MIRIAD is shown. The KDE plot shows that most questions contain less than 30 words and answers less than 140 words, respectively.d, Human annotation results are shown in confusion matrices for the metrics relevance, factuality, and groundedness. Percentages indicate where the LLM judge agreed with the human experts. For example, in 91.7% of the reviewed samples both human and LLM rate the QA pair as grounded in the source passage, whereas in 0.6% of the samples both agreed that the sample was not grounded, totaling in 92.3% agreement.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x3.png",
                "caption": "",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x4.png",
                "caption": "Figure 4:Topic-level analysis of RAG performance on MedMCQA.a, RAG-MIRIAD.b, RAG-Passage. To assess the effect of information structure in retrieval-augmented generation (RAG), we compare RAG-MIRIAD (top panel), which uses concise QA pairs from MIRIAD, with RAG-Passage (bottom panel), which retrieves longer unstructured text chunks from the source literature. In both settings, the total retrieved context is capped at 1000 tokens to ensure a consistent comparison and match the main RAG experiment setup. Each MedMCQA problem was categorized into one of 35 clinical topics using the LLaMA-3-8B-Instruct model. For each topic, bars represent the number of problems that were improved by RAG (green dashed), harmed by RAG (red dashed), and the resulting net effect (solid bars). RAG-MIRIAD exhibits broader and more consistent net benefits, particularly in topics such as basic medical sciences, public health, and dental medicine. In contrast, RAG-Passage shows more cancellation between helpful and harmful retrievals, demonstrating the limitations of conventional passage-based augmentation on this benchmark.",
                "position": 309
            },
            {
                "img": "https://arxiv.org/html/2506.06091/extracted/6519313/figures/retriever-fig-v2.png",
                "caption": "Figure 5:Demonstration of retriever training use case. MIRIAD can be leveraged as a large-scale supervised retrieval dataset to train improved medical information retrieval models. To our knowledge, this has previously not been possible, as large-scale medical retrieval data is lacking or not accessible. Panel a and b show how a retriever can be trained on MIRIAD to produce higher quality embeddings for medical queries and their answers. The curves display the mean; the error bars the standard deviation over 4 repetition runs.a, The training loss of the retriever model is shown on the Y-axis on a log-scale over the course of training (training steps on X-axis).b, This panel shows the retrieval model’s validation metrics for retrieval quality computed on the held-out set of QA pairs. At the baseline of training (x = 0), the metrics for the general-domain retriever is shown (e.g. 0.94 Recall @1).",
                "position": 322
            }
        ]
    },
    {
        "header": "3   Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "4   Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06091/x5.png",
                "caption": "Extended Data Figure 1:Beneficial and detrimental RAG samples do not generalize across different RAG system configurations. Visualization of Jaccard index that is used to measure the overlap between beneficial sample sets and between detrimental sample sets of different RAG pipeline settings, measured at Top-3, Top-10, and Top-20 retrieved contexts. (Beneficial samples- samples that improve QA when integrated;Detrimental samples- samples that harm QA when integrated). Error bars indicate 95% confidence intervals. Overall, the overlaps between both beneficial sample sets and the overlaps between detrimental sample sets remain low (Jaccard Index<<<0.14), suggesting that the identity of helpful or harmful samples is highly specific to the system setup, posing challenges for designing generalizable retrieval strategies.a, Fixed backbone LLM (“Mistral” refers to mixtral-8x7b-instruct) with varying embedder (all-MiniLM vs. BGE-Large).b, Fixed backbone LLM (“Llama3” refers to llama-3.1-8b-instruct) with varying embedder.c, Fixed embedder (all-MiniLM) with varying backbone LLM (Mistral vs. Llama3). d, Fixed embedder (BGE-Large) with varying backbone. In all cases, the low Jaccard index indicates limited overlap between the beneficial or detrimental samples identified under different configurations, underscoring that retrieval effects are highly setting-dependent.",
                "position": 1030
            }
        ]
    },
    {
        "header": "Supplementary materials",
        "images": []
    },
    {
        "header": "Appendix S1Additional details for dataset generation",
        "images": []
    },
    {
        "header": "Appendix S2Additional details for quality control",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06091/extracted/6519313/figures/figure_s1_confusion_matrix_raw.png",
                "caption": "Figure S1:Human annotation results. For three evaluation metrics, we report the absolute counts where the LLM judge agreed or disagreed with human annotation of QA samples.",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x6.png",
                "caption": "Figure S2:Human evaluation app.Five clinical reviewers were using the evaluation app, above shown in schematic, and below exemplified as a screenshot of the app. Reviewers were instructed, and also had a tab on the left to remind them on the rating criteria even while scrolling down through the passage text. For better user experience, we first present the QA pairs and only then let the reviewer browse through the passage below. As it was challenging for reviewers to assign fine-grained scores, we designed the metrics as tick boxes, where reviewers check whether a given QA pair is factual, grounded in the source passage, and relevant.",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2506.06091/extracted/6519313/figures/specialty-plot-56-v2.png",
                "caption": "Figure S3:A fine-grained version of the discipline and topics distribution of MIRIAD over time.Here, all 56 categories of topics and disciplines for each question-answer (QA) pair are shown. The top panel shows absolute counts of QA pairs within a given year, the bottom row shows the relative proportion of fine-grained topics within a given year bucket.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2506.06091/extracted/6519313/figures/answer-sunburst-large-fs-15.png",
                "caption": "Figure S4:Sunburst plot of MIRIAD answers.The first five words of a small subset of 1000 MIRIAD answers are visualized hierarchically in a sunburst plot. Color indicates that the different answers share the same first word, e.g. “The” or “In”. This plot exemplifies the breath of answers contained in MIRIAD. When increasing the number of samples, the diversity further increases, making it more challenging to visualize and interpret.",
                "position": 1203
            }
        ]
    },
    {
        "header": "Appendix S3Retrieval-augmented generation (RAG) for medical question answering: background, experimental details, additional results",
        "images": []
    },
    {
        "header": "Appendix S4Distribution of categories of MedMCQA (dev) problems",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06091/x7.png",
                "caption": "Figure S5:Distribution of disciplines in MedMCQA dev set",
                "position": 1558
            }
        ]
    },
    {
        "header": "Appendix S5Additional details in medical discipline categorization",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06091/x8.png",
                "caption": "Figure S6:Distribution of disciplines in MIRIAD",
                "position": 1594
            }
        ]
    },
    {
        "header": "Appendix S6Hallucination detection",
        "images": []
    },
    {
        "header": "Appendix S7Additional details in RAG individual sample contribution analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06091/x9.png",
                "caption": "Figure S7:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed embedding model all-MiniLM, top-k as 3, and different backbone LLMs.",
                "position": 1675
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x10.png",
                "caption": "Figure S8:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed embedding model all-MiniLM, top-k as 10, and different backbone LLMs.",
                "position": 1678
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x11.png",
                "caption": "Figure S9:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed embedding model all-MiniLM, top-k as 20, and different backbone LLMs.",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x12.png",
                "caption": "Figure S10:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed embedding model BGE-Large, top-k as 3, and different backbone LLMs.",
                "position": 1684
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x13.png",
                "caption": "Figure S11:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed embedding model BGE-Large, top-k as 10, and different backbone LLMs.",
                "position": 1687
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x14.png",
                "caption": "Figure S12:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed embedding model BGE-Large, top-k as 20, and different backbone LLMs.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x15.png",
                "caption": "Figure S13:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed backbone LLM Mixtral-8x7B-Instruct, top-k as 3, and different embedding models.",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x16.png",
                "caption": "Figure S14:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed backbone LLM Mixtral-8x7B-Instruct, top-k as 10, and different embedding models.",
                "position": 1696
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x17.png",
                "caption": "Figure S15:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed backbone LLM Mixtral-8x7B-Instruct, top-k as 20, and different embedding models.",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x18.png",
                "caption": "Figure S16:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed backbone LLM Llama-3.1-8B-instruct, top-k as 3, and different backbone embedding models.",
                "position": 1702
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x19.png",
                "caption": "Figure S17:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed backbone LLM Llama-3.1-8B-instruct, top-k as 10, and different backbone embedding models.",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2506.06091/x20.png",
                "caption": "Figure S18:Venn diagrams of beneficial and detrimental MIRAD samples for MedMCQA benchmark.With fixed backbone LLM Llama-3.1-8B-instruct, top-k as 20, and different backbone embedding models.",
                "position": 1708
            }
        ]
    },
    {
        "header": "Appendix S8Licensing",
        "images": []
    },
    {
        "header": "Appendix S9Intended use",
        "images": []
    }
]