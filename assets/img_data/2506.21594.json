[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Stage 1: Supervised Fine-Tuning (SFT)",
        "images": []
    },
    {
        "header": "4Stage 2: Group Relative Policy Optimization (GRPO)",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/soft_overlong_punishment_mean.png",
                "caption": "(a)Mean reward over training steps computed using the soft overlong punishment function.\nThe reward reflects penalties for excessively long completions, with values closer to zero indicating better adherence.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/soft_overlong_punishment_mean.png",
                "caption": "(a)Mean reward over training steps computed using the soft overlong punishment function.\nThe reward reflects penalties for excessively long completions, with values closer to zero indicating better adherence.",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/mean_length.png",
                "caption": "(b)Average length of generated completions (in tokens) over training steps.\nLength fluctuations indicate varying verbosity during training, with a notable instability after step 500 and a drop after step 600.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/cosine_scaled_reward.png",
                "caption": "(a)Cosine Scaled Reward",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/cosine_scaled_reward.png",
                "caption": "(a)Cosine Scaled Reward",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/repetition_penalty_reward.png",
                "caption": "(b)Repetition Penalty Reward",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/reward_reference.png",
                "caption": "(c)Reward Reference",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2506.21594/extracted/6551685/images/train_loss.png",
                "caption": "(d)Training Loss",
                "position": 721
            }
        ]
    },
    {
        "header": "5Experimental Results and Evaluation",
        "images": []
    },
    {
        "header": "6Limitations and Ethical Considerations",
        "images": []
    },
    {
        "header": "7Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix BSystem Prompts",
        "images": []
    }
]