[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01572/x1.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01572/x2.png",
                "caption": "Figure 2:The MakeAnything framework comprises two core components: (1) an Asymmetric LoRA module that generates diverse creation processes from text prompts through asymmetric LoRA, and (2) the ReCraft Model, which constructs an image-conditioned base model by merging pretrained LoRA weights with the Flux foundation model, enabling process prediction via injected visual tokens.",
                "position": 143
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01572/x3.png",
                "caption": "Figure 3:Examples from the MakeAnything Dataset, which consists of 21 tasks with over 24,000 procedural sequences.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2502.01572/x4.png",
                "caption": "Figure 4:Generation results of MakeAnything. From top:Text-to-Sequenceoutputs conditioned on textual prompts;Image-to-Sequencereconstructions via ReCraft Model;Unseen Domaingeneralization combining procedural LoRA (blue) with stylistic LoRA (red).",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2502.01572/x5.png",
                "caption": "Figure 5:Compare with baselines on different tasks.",
                "position": 291
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01572/x6.png",
                "caption": "Figure 6:Ablation study results.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2502.01572/x7.png",
                "caption": "Figure 7:Comparison results on three tasks, evaluated by GPT and humans respectively.",
                "position": 475
            }
        ]
    },
    {
        "header": "5Limitations and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation details of the GPT4-o evaluation.",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01572/extracted/6173423/images/all-9.jpg",
                "caption": "Figure 8:More generation results. From top to bottom, they are portrait, Sand Art, landscape illustration, painting, LEGO, transformer, and cook respectively.",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2502.01572/extracted/6173423/images/1-all.jpg",
                "caption": "Figure 9:More generation results. From top to bottom, they are oil painting and line draw.",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2502.01572/extracted/6173423/images/2-all.jpg",
                "caption": "Figure 10:More generation results. From top to bottom, they are ink painting and clay sculpture.",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2502.01572/extracted/6173423/images/3-all.jpg",
                "caption": "Figure 11:More generation results. From top to bottom, they are wood sculpure, Zbrush, and fabric toys.",
                "position": 1474
            }
        ]
    },
    {
        "header": "Appendix BMore results",
        "images": []
    }
]