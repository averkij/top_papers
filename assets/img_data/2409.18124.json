[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18124/x1.png",
                "caption": "Figure 1:We presentLotus, a diffusion-based visual foundation model for dense geometry prediction. With minimal training data, Lotus achieves SoTA performance in two key geometry perception tasks,i.e., zero-shot depth and normal estimation.\nâ€œAvg. Rankâ€ indicates the average ranking across all metrics, where lower values are better. Bar length represents the amount of training data used.",
                "position": 101
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x2.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18124/x3.png",
                "caption": "Figure 2:Adaptation protocol of Lotus.After the pre-trained VAE encoderâ„°â„°\\mathcal{E}caligraphic_Eencodes the imagexand annotationyto the latent space: â‘  the denoiser U-Net modelfÎ¸subscriptğ‘“ğœƒf_{\\theta}italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPTis fine-tuned usingx0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-prediction; â‘¡ we employ single-step diffusion formulation at time-stept=Tğ‘¡ğ‘‡t=Titalic_t = italic_Tfor better convergence; â‘¢ we propose a novel detail preserver, to switch the model either to reconstruct the image or generate the dense prediction via a switchersğ‘ sitalic_s, ensuring a more fine-grained prediction.\nThe noiseğ³ğ“ğ²superscriptsubscriptğ³ğ“ğ²\\mathbf{z_{T}^{y}}bold_z start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPTin bracket is used for our generativeLotus-Gand is omitted for the discriminativeLotus-D.",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x4.png",
                "caption": "Figure 3:Inference time comparison in depth estimation between Lotus and SoTA methods.Lotus is hundreds of times faster than Marigold and slightly faster than DepthAnything V2 at high resolutions.\nDepthAnything V2â€™s inference time at2048Ã—2048204820482048\\times 20482048 Ã— 2048is not plotted because it requires>80absent80>80> 80GB graphic memory.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18124/x5.png",
                "caption": "Figure 4:Adaptation protocol of Direct Adaptation.Starting with a pre-trained Stable Diffusion model, imagexand annotationyare encoded using the pre-trained VAE. The noisy annotationğ³ğ­ğ²subscriptsuperscriptğ³ğ²ğ­\\mathbf{z^{y}_{t}}bold_z start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPTis obtained by adding noise at leveltâˆˆ[1,T]ğ‘¡1ğ‘‡t\\in\\left[1,T\\right]italic_t âˆˆ [ 1 , italic_T ]. The U-Net input layer is coupled to accommodate the concatenated inputs and then fine-tuned using the standard diffusion objective,Ïµitalic-Ïµ\\epsilonitalic_Ïµ-prediction, under the original multi-step formulation.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18124/x6.png",
                "caption": "Figure 5:Comparisons among different parameterizations using various seeds.All models are trained onHypersim(Roberts etÂ al.,2021)and tested on the input image for depth estimation.\nThe standard DDIM sampler is used with 50 denoising steps. Four steps are selected for clear illustration. From left (largerÏ„ğœ\\tauitalic_Ï„) to right (smallerÏ„ğœ\\tauitalic_Ï„) is the iterative denoising process.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x7.png",
                "caption": "Figure 6:Quantitative evaluation of the predicted depth mapsğ³^Ï„ğ²subscriptsuperscript^ğ³ğ²ğœ\\mathbf{\\hat{z}^{y}_{\\tau}}over^ start_ARG bold_z end_ARG start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPTalong the denoising process.The experimental settings are same as Fig.5. Six steps are selected for illustration. The banded regions around each line indicate the variance, wider areas representing larger variance.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x8.png",
                "caption": "Figure 7:Comparisons among various training time-steps and data scalesevaluated on NYUv2 in depth estimation. All models are fine-tuned onHypersimusingx0subscriptğ‘¥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-prediction.\nDuring inference, ifTâ€²>50superscriptğ‘‡â€²50T^{\\prime}>50italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT > 50, the DDIM sampler is used with 50 denoising steps; otherwise, the number of denoising steps is equal toTâ€²superscriptğ‘‡â€²T^{\\prime}italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT.\nThe results demonstrate improved performance with decreased training time-steps. The single-step diffusion formulation (Tâ€²=1superscriptğ‘‡â€²1T^{\\prime}=1italic_T start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT = 1) exhibits best performance across different data volumes.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x9.png",
                "caption": "Figure 8:Depth mapsw/w/italic_w /andw/oğ‘¤ğ‘œw/oitalic_w / italic_othe detail preserver and reconstruction outputs.Fine-tuning the diffusion model for dense prediction tasks can potentially degrade its ability to generate highly detailed images, resulting in blurred predictions in regions with rich detail. To preserve these fine-grained details, we introduce a detail preserver that incorporates an additional reconstruction task, enhancing the modelâ€™s capacity to produce more accurate dense annotations.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x10.png",
                "caption": "Figure 9:Depth maps of multiple inferences and uncertainty maps.Areas like the sky, object edges, and intricate details (e.g., cat whiskers) typically exhibit high uncertainty.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x11.png",
                "caption": "Figure 10:Inference Pipeline of Lotus.The noiseğ³ğ“ğ²superscriptsubscriptğ³ğ“ğ²\\mathbf{z_{T}^{y}}bold_z start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPTin bracket is used forLotus-Gand omitted forLotus-D.",
                "position": 459
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Appendix AExperimental Settings",
        "images": []
    },
    {
        "header": "Appendix BDetails of Direct Adaption",
        "images": []
    },
    {
        "header": "Appendix CAnalysis of â€œdirectionâ¢(ğ³Ï„ğ²)directionsubscriptsuperscriptğ³ğ²ğœ\\text{direction}(\\mathbf{z^{y}_{\\tau}})direction ( bold_z start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPT )â€ in DDIM Process (Eq.4)",
        "images": []
    },
    {
        "header": "Appendix DPerformance ofvğ‘£vitalic_v-prediction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18124/x12.png",
                "caption": "Figure 11:Quantitative evaluation of the predicted depth mapsğ³^Ï„ğ²subscriptsuperscript^ğ³ğ²ğœ\\mathbf{\\hat{z}^{y}_{\\tau}}over^ start_ARG bold_z end_ARG start_POSTSUPERSCRIPT bold_y end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_Ï„ end_POSTSUBSCRIPTalong the denoising process.The experimental settings are same as Fig.5and6. Six steps are selected for illustration. The banded regions around each line indicate the variance, wider areas representing larger variance.",
                "position": 1290
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x13.png",
                "caption": "Figure 12:Qualitative comparison on zero-shot affine-invariant depth estimation.Lotus demonstrates higher accuracy especially in detailed areas.",
                "position": 1305
            },
            {
                "img": "https://arxiv.org/html/2409.18124/x14.png",
                "caption": "Figure 13:Qualitative comparison on zero-shot surface normal estimation.Lotus offers improved accuracy particularly in complex regions.",
                "position": 1310
            }
        ]
    },
    {
        "header": "Appendix EQualitative Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.18124/x15.png",
                "caption": "Figure 14:Applications of Lotus.â‘ Depth to 3D Point Clouds. â‘¡Joint Estimation:Simultaneous depth and normal estimation with100%percent100100\\%100 %shared parameters. â‘¢Single-View Reconstruction:Reconstructing 3D meshes from normal predictions. â‘£Multi-View Reconstruction:Reconstructing high-quality meshes using depth/normal predictionswithout RGB supervision.",
                "position": 1322
            }
        ]
    },
    {
        "header": "Appendix FApplications of Lotus",
        "images": []
    },
    {
        "header": "Appendix GFuture Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]