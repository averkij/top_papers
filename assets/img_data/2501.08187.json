[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Main",
        "images": []
    },
    {
        "header": "Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08187/x1.png",
                "caption": "Fig. 1:Overview ofInstructCell.a,Summary of incorporated single-cell data.InstructCellincorporates 299,155 scRNA-seq samples from human and mouse origins, spanning multiple organs.CPCGdenotesConditional Pseudo-cell Generation,CTAdenotesCell Type Annotation, andDSPdenotesDrug Sensitivity Prediction.b,Architecture of the multi-modal cell language model. The model processes both text and single-cell data via three primary components: a Q-Former to capture single-cell gene expression knowledge, a pre-trained LM as the backbone, and a cell reconstruction module for generating single-cell gene expression profiles.c,Construction of multi-modal single-cell instruction data. Complete instruction-response pairs are formed by combining required and optional attributes from text and single-cell modalities.d,Simulation of diverse communication styles. LLMs generate chat templates with varying traits (personality, motivation, and proficiency) to produce instructions that convey task-related information in different communication styles.",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x2.png",
                "caption": "Fig. 2:Conditional pseudo-cell generation results byInstructCell.a,UMAP visualizations of real and generated cells. The left plot shows the overlap between real and generated cells. The middle and right plots display real and generated cells, respectively, with distinct colors indicating different cell types.b,Dot plots of gene expression patterns derived from real (top) and generated (bottom) cells. Based on the test set from Tabular-Sapiens, we use Welch‚Äôstùë°titalic_t-test to identify top three significant genes for each cell type and display them along x-axis. Cell types are arranged along y-axis. The size of each dot indicates the proportion of single cells within the corresponding cell type that express the gene, while the color of the dot represents the mean expression level of the gene within that cell type. The results of the remaining two datasets are available in Fig.12.c,Quantitative evaluation of cell generation performance across four datasets. A lower‚ñ≥‚ñ≥\\triangle‚ñ≥sKNN value indicates better structural alignment, a higher pKNN value reflects improved positional correspondence, and a lower MMD value denotes a more accurate approximation of the global data distribution.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x3.png",
                "caption": "Fig. 3:Cell type annotation results byInstructCell.a,Evaluation ofInstructCell‚ÄôsCTAperformance across human heart, liver, pancreas, and mouse skin and pancreas datasets. Performance is quantified using weighted F1, macro F1, and accuracy metrics, with different colors representing different models.b,UMAP visualization of three different datasets. The left panel is colored by expert-annotated cell types from the original research, and the right panel is colored byInstructCellprediction results.c,Confusion matrices between predicted cell types and actual annotations for the three datasets. Darker shades denote a higher frequency of agreement between the model‚Äôs predictions and the actual cell type annotations.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x4.png",
                "caption": "Fig. 4:Drug sensitivity prediction results byInstructCell.a,Evaluation ofInstructCell‚ÄôsCTAperformance across human oral, lung, and mouse bone datasets. Performance is quantified using weighted F1, macro F1, and accuracy metrics, with different colors representing different models.b,UMAP visualization of the three datasets, with cells colored by drug sensitivity labels (sensitive, resistant, and holiday) for both expert-annotated results andInstructCellpredictions.c,Confusion matrices between predicted cell types and actual annotations for the three datasets. Darker shades denote a higher frequency of agreement between the model‚Äôs predictions and the actual drug sensitivity annotations.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x5.png",
                "caption": "Fig. 5:Robustness ofInstructCell.a,Quantitative comparison of theCPCGtask under seen and unseen instruction templates.\nResults are shown for‚ñ≥‚ñ≥\\triangle‚ñ≥sKNN and pKNN metrics at varying numbers of neighborsKùêæKitalic_K, as well as for MMD. Different colors denote whether the instruction templates are seen or unseen.b,Average performance ofInstructCellunder instruct and chat modes across each task. On the left side (classification tasks), the shape of each scatter point indicates whether options are provided or not, while the color distinguishes model versions. Each configuration includes 40 scatter points (20 with options and 20 without). On the right side (generative task), different colors represent different model versions.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x6.png",
                "caption": "Fig. 6:Top 10 significant genes identified byInstructCellfor each cell type in two datasets.a, bHeatmaps of the significant genes extracted fromInstructCellby using gradient saliency-based method for (a) He-2020-Liver and (b) Xin-2016 datasets.\nThe color gradient from red to blue represents gene importance, with red indicating higher importance scores and blue indicating lower scores. Red markers in each row indicate that genes among the top 10 key genes identified by the model, are either reported as marker genes for the corresponding cell type in the CellMarker2.0 database or in recent literature.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x7.png",
                "caption": "Fig. 7:A closer look ofInstructCell.a,Evaluation of response quality inInstructCellusing the LLM-as-a-judge approach. Response quality is assessed based on fluency, grammar, and inclusion of predictive results, with Claude 3.5 Sonnet[4]serving as an unbiased evaluator. Text highlighted in purple indicates additional content for theCPCGtask compared to the two classification tasks.b,Impact of Q-Former on model performance.\nPerformance comparison between the Q-Former and a standard MLP for encoding single-cell data.c,Impact of query embedding quantity on model performance.\nPerformance comparison across different numbers of query embeddings.d,Comparative performance of multi-task vs. single-task instruction tuning. For single-task instruction tuning, we divide our multi-modal instruction dataset by task type and train separate models for each specific task. We report the average metrics across all datasets for each task using theInstructCell-instructversion.e,Comparative performance of without vs. with pre-trained LM weights. We conduct experiments using theInstructCell-chatversion to explore the impact of employing or not employing pre-trained weights on model performance.f,Impact of varying template ratios on model outputs.\nFourchatversion models are trained on all classification datasets using multi-task instruction tuning with varying ratios of templates (0.5%, 5%, 50%, and 100% of the total templates). For theCTAandDSPtasks, 40 unseen instruction templates are selected for evaluation: 20 with multiple-choice options and 20 without. The mean performance and standard deviation for each template are calculated across all datasets for these two tasks. Additionally, we sample 500 unseen instruction templates and use Claude 3.5 Sonnet to score the model‚Äôs outputs for expressiveness, while unigram analysis is conducted to assess lexical diversity.",
                "position": 316
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "Methods",
        "images": []
    },
    {
        "header": "Multi-modal instruction data construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08187/x8.png",
                "caption": "Fig. 8:The examples of the prompts used to construct instruction-response templates forCPCG.a,An example of the prompts for generating personality traits.b,An example of the prompts used to generate motivations forCPCG.c,An example of the prompts used to generate instruction templates forCPCG.d,An example of the prompts used to generate response templates forCPCG.e,An example of the prompts for rewriting instruction templates to enhance diversity.",
                "position": 405
            }
        ]
    },
    {
        "header": "InstructCellmodel architecture",
        "images": []
    },
    {
        "header": "Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08187/x9.png",
                "caption": "Fig. 9:Detailed overview and UMAP visualizations of scRNA-seq datasets used in this work.A total of 11 datasets are utilized, spanning 2 species and 11 tissue types. Among them, 5 datasets are employed forCTA, 3 forDSP, and 3 forCPCG. The number of samples for each dataset is listed under the column # Samples. The middle three UMAP plots present the single-cell data landscape, showcasing the distributions across different datasets, species, and tissues. The lower UMAP plot shows the label distribution for Ma-2020, GSE110894, and PBMC68K. Covering three distinct tasks, our data include various labels, such as response labels forDSPand cell type labels for other two tasks.",
                "position": 1032
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Related work",
        "images": []
    },
    {
        "header": "Details of multi-modal instruction data",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08187/x10.png",
                "caption": "Fig. 10:Statistics of synthetic instruction-response templates.a,Length of instruction templates for different communication styles. Traits such as personality (orange), motivation (green), proficiency (red), and their combination (purple) are systematically removed to evaluate their impact, compared to the full trait version (blue). Variance for each distribution is shown in parentheses.b,Similarity of instruction templates across different communication styles. Template similarity is measured pairwise, with samples exceeding a similarity threshold of 0.75 excluded. Average similarity values for each style are reported in parentheses.c,Lexical diversity of instruction templates across communication styles. Lexical diversity is quantified using the unigram ratio, defined as the proportion of unique unigrams to the total number of unigrams in each instruction template.d,Length distribution of instruction and response templates across tasks. A scatter plot illustrates the lengths of instructions and responses for distinct tasks, with different colors representing task categories.",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x11.png",
                "caption": "Fig. 11:Qualitative examples ofInstructCell-chat.Illustrative examples for each task are presented, showcasing instructions, model-generated responses fromInstructCell-chat, the corresponding ground truth answers, and their sample sources.",
                "position": 1267
            }
        ]
    },
    {
        "header": "Baselines",
        "images": []
    },
    {
        "header": "Addtional experimental results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.08187/x12.png",
                "caption": "Fig. 12:Bubble plot for conditional pseudo-cell generation.As a supplement to Fig. 2(b), this plot highlights the top three significant genes for each cell type from the remaining two datasets in our study.\nFor model-generated cells, we display the expression ratios and average expression levels of these significant genes. Redder hues indicate higher average gene expression, while larger circles represent a higher proportion of gene expression within the corresponding cell type.",
                "position": 1339
            },
            {
                "img": "https://arxiv.org/html/2501.08187/x13.png",
                "caption": "Fig. 13:Confusion matrices and UMAP visualizations for cell type annotation datasets.a,Supplementary confusion matrices for the remaining two datasets, complementing Fig.¬†3(c). Darker shades represent a higher frequency of agreement between model predictions and the ground-truth cell type annotations.b,UMAP visualizations comparing expert-annotated cell types (left panel) with the predictions generated byInstructCell(right panel). Different colors represent distinct cell types, illustrating the model‚Äôs ability to accurately reproduce the structure of the original data.",
                "position": 1346
            }
        ]
    },
    {
        "header": "Data availability",
        "images": []
    },
    {
        "header": "Code availability",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]