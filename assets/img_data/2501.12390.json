[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12390/x1.png",
                "caption": "",
                "position": 200
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12390/x2.png",
                "caption": "Figure 2:Method.(a) After downloading geotagged photos, we train a GPS-to-image generation model conditioned on GPS tags and text prompts. The trained generative model can produce images using both conditioning signals in a compositional manner. (b) We can also extract 3D models from a landmark-specific GPS-to-image model using score distillation sampling. This diffusion model parameterizes the GPS location by the azimuth with respect to a given landmark’s center.+means we concatenate GPS embeddings and text embeddings.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x3.png",
                "caption": "Figure 3:3D Setup Comparison.We extract 3D models from 2D GPS-to-image models. (a) Traditional approaches require running SfM to estimate camera pose, followed by dense geometry estimation. Since they are based on triangulation, they are susceptible to catastrophic errors due to incorrect pose; (b) DreamFusion[64]samples images from different poses within a scene using view-dependent prompting. However, text has a limited ability to precisely control the position of the camera. (c) Our method extends DreamFusion with GPS conditioning, reducing pose uncertainty.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x4.png",
                "caption": "Figure 4:Qualitative results for Paris.We show images that have been sampled from our GPS-to-image diffusion model for various locations and prompts within Paris.",
                "position": 498
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12390/x5.png",
                "caption": "Figure 5:Qualitative comparison for GPS-to-image diffusion.We compare the qualitative results of our method against baselines using specific pairs of text prompts and GPS tags. Each column shows a text prompt and a GPS tag at the top. Text-address-to-image diffusion model is conditioned on a combination of the text prompt and the address name derived from the GPS tag. We also perform nearest neighbor in the training set based on GPS tags. Our GPS-to-image diffusion model uses a text prompt and raw GPS tag as conditioning. Google Street View images are sampled forreferenceof geolocation. Our method achieves better compositionality and visual quality.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x6.png",
                "caption": "Table 1:Evaluation of GPS-to-image diffusion.We compare our method with several baselines in terms of CLIP Score and GPS Score. NN represents the nearest neighbor and SD is for stable diffusion. The best results are inbold, and the second bests are colored in blue.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x6.png",
                "caption": "Figure 6:Average images.We select five areas for Paris and New York City respectively. Using our GPS-to-image models, we obtain representative images of the concept of “building” within these geographic regions to observe architectural styles. More examples can be found onproject webpagefor different locations and concepts.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x7.png",
                "caption": "Figure 7:Qualitative comparison for 3D monument reconstruction.We show qualitative results of DreamFusion[64]and our method on two monuments: 1) Leaning Tower of Pisa; 2) Arc de Triomphe. Our reconstructed 3D monuments have better visual quality and more accurate 3D structure. We use rendered depth to make the background of RGB rendering white.Please see AppendixA.2.2andproject webpagefor more examples.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x8.png",
                "caption": "Figure 8:Ablation.We conducted ablation studies to analyze the effectiveness of different modules in our method for GPS-to-image generation and 3D landmark reconstruction.",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x9.png",
                "caption": "Figure 9:Attention visualization.We visualize attention maps for text and GPS tokens.",
                "position": 814
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12390/x10.png",
                "caption": "",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x11.png",
                "caption": "Figure 11:Random sampling.We show some randomly sampled images from generation results of our GPS-to-image diffusion models conditioned on text prompts and GPS tags. These sampled results were used in the quantitative evaluation.",
                "position": 2173
            }
        ]
    },
    {
        "header": "A.1GPS-to-image generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12390/x12.png",
                "caption": "Figure 12:More qualitative comparison for 3D monument reconstruction.We show qualitative results of DreamFusion[64]and our method on Stonehenge. Our reconstructed 3D monuments have better visual quality and more accurate 3D structure. We use rendered depth to make the background of RGB rendering white.Please seeproject webpagefor more examples.",
                "position": 2326
            }
        ]
    },
    {
        "header": "A.2GPS-guided 3D reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12390/x13.png",
                "caption": "Figure 13:SfM/NeRF baselines.We present SfM reconstructions from COLMAP[72], Nerfacto[84]rendering results, and NeRF-W[56]rendering results for 6 evaluated landmarks. SfM reconstruction fails on (a), (b), and (c). Nerfacto[84]only succeeds on (f). NeRF-W[56]completely fails on 6 scenes.",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x14.png",
                "caption": "Figure 14:Qualitative results for angle-to-image generation.We show generated images of our angle-to-image diffusion model for the Arc de Triomphe, Statue of Liberty, and Leaning Tower of Pisa. Images are sampled conditioned on different angles estimated by GPS tags.",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2501.12390/x15.png",
                "caption": "Figure 15:Data samples.We show some random photos with their GPS tags from our collected datasets.",
                "position": 2442
            }
        ]
    },
    {
        "header": "A.3Datasets",
        "images": []
    }
]