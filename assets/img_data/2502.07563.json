[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07563/x1.png",
                "caption": "Figure 1:Computation Decomposition in LASP-2 with masking.Colored chunks represent inter-chunks.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2502.07563/x2.png",
                "caption": "Figure 2:Visualization of LASP-2H on Linear Attention and Standard Attention hybrid model.We exemplify LASP-2H on the hybrid layers of linear attention and standard attention modules with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reduce-scatter in backward, and vice versa. AG/No: all-gather in forward and no-op in backward, and vice versa. Note that the SP communication operations for linear attention operate on the memory state𝐌t∈ℝd×dsubscript𝐌𝑡superscriptℝ𝑑𝑑\\mathbf{M}_{t}\\in\\mathbb{R}^{d\\times d}bold_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT, while for standard attention, they operate on states𝐊t,𝐕t∈ℝC×dsubscript𝐊𝑡subscript𝐕𝑡superscriptℝ𝐶𝑑\\mathbf{K}_{t},\\mathbf{V}_{t}\\in\\mathbb{R}^{C\\times d}bold_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_d end_POSTSUPERSCRIPT.",
                "position": 614
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07563/x3.png",
                "caption": "Figure 3:Speed Comparison (tokens/s).Experiments were carried out on a pure Linear-Llama3-1B model, utilizing the basic linear attention module. A total of 64 A100 GPUs were employed, and the SP sizeT𝑇Titalic_Twas also set to 64. To accommodate very-long sequence lengths, such as 2048K, the batch size was kept fixed at 1 throughout this experiment.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2502.07563/x4.png",
                "caption": "Figure 4:Scalability Results.Experiments were conducted on a pure Linear-Llama3-1B model using the Basic Linear Attention module. SP sizeT𝑇Titalic_Twas always equal to number of GPUs. Batch size was fixed as 1 to accommodate very-long sequence lengths, e.g., 2048K. The sign \"×\\times×\" with a dotted line represented occurring an Out of Memory (OOM).",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2502.07563/x5.png",
                "caption": "",
                "position": 761
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]