[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13805/extracted/6372901/images/teaser-final.drawio.png",
                "caption": "Figure 1.The LearnAct Framework and LearnGUI Benchmark focus on addressing the long-tail challenges in mobile GUI agent performance through demonstration-based learning.From rule-based automation to LLM-powered agents, mobile GUI automation has evolved significantly, yet still struggles with long-tail scenarios due to interface diversity. Our LearnAct framework introduces demonstration-based learning to effectively handle these challenges, outperforming existing methods in both offline and online evaluations.",
                "position": 145
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13805/x1.png",
                "caption": "Figure 2.A toy example for demonstration learning on mobile GUI Agent.We build a benchmark named LearnGUI for demonstration learning on Mobile GUI Agent, which provides different few-shot task combinations and offers multi-dimensional metrics including task similarity, UI similarity, and action similarity between support tasks and query tasks.",
                "position": 154
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.LearnGUI Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13805/extracted/6372901/images/ui_action_similarity_scatter.drawio.png",
                "caption": "Figure 3.Joint distribution of UI similarity and action similarity in LearnGUI-Offline.The scatter plot shows the relationship between UI and action similarity measures across task pairs. The quadrant divisions represent our categorization of tasks into four profiles: UISHActSH, UISHActSL, UISLActSH, and UISLActSL, enabling analysis of how different similarity combinations affect learning transfer.",
                "position": 1063
            }
        ]
    },
    {
        "header": "4.Method: LearnAct",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13805/extracted/6372901/images/learnact-pipline.drawio.png",
                "caption": "Figure 4.Illustration of the overall framework of LearnAct.Architecture diagram showing the three main components (DemoParser, KnowSeeker, ActExecutor) and their interconnections within the LearnAct system, including data flow from human demonstrations to execution.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x2.png",
                "caption": "Figure 5.Pipeline of DemoParser Agent.Input instructions and corresponding actions and screenshots; output low-level action descriptions and create knowledge database.\nThis process transforms high-level user instructions into precise operation sequences while building a reusable domain knowledge base to improve mobile interface interaction automation efficiency.",
                "position": 1134
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x3.png",
                "caption": "Figure 6.Pipeline of KnowSeeker Agent.The KnowSeeker Agent converts demo trajectories from the knowledge base into a vector database. When executing user tasks, KnowSeeker retrieves the top-k relevant demos from the vector database for subsequent use. This approach enables efficient retrieval of similar demonstrations to assist with new task execution.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x4.png",
                "caption": "Figure 7.Pipeline of ActExecutor Agent.The ActExecutor Agent executes the low-level action descriptions generated by the Action Planner Agent. It uses the KnowSeeker Agent to retrieve relevant demonstrations from the knowledge base and execute the actions in the demonstrations. This approach enables efficient execution of low-level actions to assist with new task execution.",
                "position": 1255
            }
        ]
    },
    {
        "header": "5.Experiments",
        "images": []
    },
    {
        "header": "6.Discussion and Future Work",
        "images": []
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional LearnGUI Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13805/extracted/6372901/images/similarity_distributions_continuous.drawio.png",
                "caption": "Figure 8.Distribution of instruction, UI, and action similarity scores in LearnGUI-Offline.The histograms show the distribution of similarity scores across three dimensions: instruction similarity (top), UI similarity (middle), and action similarity (bottom). These distributions enable systematic analysis of how different types of similarity between demonstration and query tasks affect learning efficacy.",
                "position": 2727
            }
        ]
    },
    {
        "header": "Appendix BLearnAct Framework Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13805/x5.png",
                "caption": "Figure 12.Detailed performance comparison of Qwen2-VL-7B with and without LearnAct on LearnGUI-Online.The figure shows the task success rates of Qwen2-VL-7B baseline versus Qwen2-VL-7B enhanced with LearnAct across different task dimensions in the LearnGUI-Online benchmark.",
                "position": 3045
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x6.png",
                "caption": "Figure 13.Detailed performance comparison of UI-TARS-7B-SFT with and without LearnAct on LearnGUI-Online.The figure presents a comprehensive breakdown of task success rates for UI-TARS-7B-SFT baseline versus UI-TARS-7B-SFT enhanced with LearnAct across multiple task dimensions in the LearnGUI-Online benchmark.",
                "position": 3051
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x7.png",
                "caption": "Figure 14.UI-TARS-7B-SFT with LearnAct vs. Baseline in NotesRecipeIngredientCount Task.Task template: \"What quantity of {ingredient} do I need for the recipe ’{title}’ in the Joplin app? Express your answer in the format <amount> <unit> without using abbreviations.\"",
                "position": 3064
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x8.png",
                "caption": "Figure 15.UI-TARS-7B-SFT with LearnAct vs. Baseline in SimpleCalendarDeleteOneEvent Task.Task template: \"In Simple Calendar Pro, delete the calendar event on {year}-{month}-{day} at {hour}h with the title ’{event_title}’\"",
                "position": 3070
            },
            {
                "img": "https://arxiv.org/html/2504.13805/x9.png",
                "caption": "Figure 16.Qwen2-VL-7B with LearnAct vs. Baseline in ExpenseDeleteMultiple Task.Task template: \"Delete the following expenses from arduia pro expense: {expenses}.\"",
                "position": 3076
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results and Analyses",
        "images": []
    }
]