[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04432/x1.png",
                "caption": "Figure 1:We utilize thediffusion procedureto learn a video tokenizer in a self-supervised manner for unified comprehension and generation, where the spatiotemporal representations serve as the condition of a diffusion model to de-noise video clips. Additionally, the proxy diffusion model functions as a de-tokenizer to decode realistic video clips from the video representations.",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2412.04432/x2.png",
                "caption": "Figure 2:Overview ofDivot tokenization and de-tokenization. During training, sparsely sampled video frames are fed into the tokenizer to obtain spatiotemporal representations. These representations serve as the conditions for a U-Net, which is trained to de-noise the noisy VAE latents of densely sampled video frames. During inference, the video representations from the Divot tokenizer can be decoded into realistic video clips with the U-Net.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04432/x3.png",
                "caption": "Figure 3:Overview ofDivot-LLM. Video features from the Divot tokenizer are fed into the LLM to perform next-word prediction for video comprehension, while learnable queries are input into the LLM to model the distributions of Divot features using aGaussian Mixture Model(GMM) for video generation. During inference, video features are sampled from the predicted GMM distribution to decode videos using the de-tokenizer.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2412.04432/x4.png",
                "caption": "Figure 4:Paradigms formodeling video representationsfrom the Divot tokenizer with a LLM for video generation. (a)MSE Regression, where the LLM output is trained to minimize its distance with video features using Mean Squared Error (MSE) loss; (b)Diffusion Modeling, where the LLM output is fed into a denoising network as the condition to predict the noise added to video features; (c)GMM Modeling, where the LLM output is trained to predict the parameters of a Gaussian Mixture Model (GMM) for modeling video feature distributions.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04432/x5.png",
                "caption": "Figure 5:Reconstructed videos, where the Divot tokenizer obtains spatiotemporal representations of sparsely sampled video frames and the de-tokenizer decodes these representations into semantically aligned and temporally coherent video clips.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04432/x6.png",
                "caption": "Figure 6:Qualitative comparison of text-to-video generation with MLLMs that are capable of unified video comprehension and generation. Divot-LLM effectively generates videos that are semantically aligned with text prompts, accurately reflecting temporal changes.",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2412.04432/x7.png",
                "caption": "Figure 7:Qualitative examples of video storytelling by Divot-LLM. Given a story instruction, Divot-LLM can generate rich textual narratives along with corresponding video clips that are temporally coherent in an interleaved manner.",
                "position": 689
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04432/x8.png",
                "caption": "Figure 8:More qualitative examples of reconstructed videos, where the Divot tokenizer obtains spatiotemporal representations of sparsely sampled video frames and the de-tokenizer decodes these representations into semantically aligned and temporally coherent video clips.",
                "position": 2045
            },
            {
                "img": "https://arxiv.org/html/2412.04432/x9.png",
                "caption": "Figure 9:More qualitative examples of text-to-video generation by Divot-LLM, which effectively generates videos that are both semantically aligned with text prompts and temporally coherent across frames.",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2412.04432/x10.png",
                "caption": "Figure 10:Qualitative examples of video comprehension by Divot-LLM.",
                "position": 2068
            }
        ]
    },
    {
        "header": "Appendix BQualitative Examples",
        "images": []
    }
]