[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10638/x1.png",
                "caption": "Figure 1:Classifier guidanceÂ decomposition (Eq.(6)) does not always hold.We apply classifier guidanceÂ on 1D data fromğ’©â¢(Â±1.0,0.05)ğ’©plus-or-minus1.00.05\\mathcal{N}(\\pm 1.0,0.05)caligraphic_N ( Â± 1.0 , 0.05 ),ğ’©â¢(Â±0.5,0.05)ğ’©plus-or-minus0.50.05\\mathcal{N}(\\pm 0.5,0.05)caligraphic_N ( Â± 0.5 , 0.05 ), andğ’©â¢(Â±0.1,0.05)ğ’©plus-or-minus0.10.05\\mathcal{N}(\\pm 0.1,0.05)caligraphic_N ( Â± 0.1 , 0.05 )respectively.\nThe denoising diffusion process starts from left to right.\nFor each dataset, we train a vanilla conditional diffusion model and a decomposed version,i.e.,Â an unconditional diffusion model and a classifier. We generate 20k samples (10k for each class) from both sides ofÂ Eq.(6) with the same initial noises and compute the absolute differences for each step in the denoising diffusion process.\nThis plot shows the average as well as the standard deviation for the difference.\nApparently, the classifier guidanceÂ decomposition doesnâ€™t hold with equality.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2503.10638/extracted/6273155/figures/gaussian_1d_traj_cfg/cfg_mean_-1.0_1.0_std_0.05_0.05_classifier_none.png",
                "caption": "Figure 3:Classifier-free guidanceÂ distorts denoising diffusion trajectories.We apply denoising diffusion models with classifier-free guidanceÂ on a 1D dataset composed of data fromğ’©â¢(Â±1.0,0.05)ğ’©plus-or-minus1.00.05\\mathcal{N}(\\pm 1.0,0.05)caligraphic_N ( Â± 1.0 , 0.05 ).\nThe denoising diffusion process for all plots starts from the bottom to the top.\nWe use the same trained model as well as the same initial noise for all plots.\nThe trajectory differences are solely caused by different guidance scales.\nSimilar toÂ Fig.2, classifier-free guidanceÂ with large guidance scale distorts the trajectories for vanilla conditional models (the first plot) to push them away from the dataset decision boundary,i.e.,Â the origin.\nDifferent scales inÂ Fig.2and this figure arise from classifier guidanceÂ and classifier-free guidanceâ€™s differing sensitivities. Here, scale=2.5 distorts trajectories significantly, whileÂ Fig.2â€™s scale=4 causes minor changes. We hypothesize that classifier-free guidanceâ€™s greater sensitivity stems from its training with conditioning dropout.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2503.10638/extracted/6273155/figures/fractal_2d_cfg/cfg_overlap_1.0_classifier_none_flat.png",
                "caption": "Figure 5:Classifier-free guidanceÂ with flow-matching based postprocessing (Sec.3.4) on 2D fractal data.The level of entanglement is the same as that inÂ Fig.4(c).\nEach plot with â€˜Post.â€˜ in the title displays generations after applying postprocessing on generations from the corresponding previous plot.\nWe observe that the postprocessing step continuously improves the fidelity of the generations via moving outlier samples back to the real data distribution as the leaf branches become much sharper, regardless of the guidance scale we use.",
                "position": 538
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOne-for-All-Scales Postprocessing Model",
        "images": []
    },
    {
        "header": "Appendix BExperiments on CIFAR-10",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10638/x2.png",
                "caption": "Figure S6:MLP-based denoising diffusion model.Here we provide details about the network structure for our MLP-based denoising diffusion model, which is used for experiments on 1D Gaussian data inÂ Sec.3and on 2D fractal data inÂ Sec.4.1.\nIn each block, the top row provides the layerâ€™s name. The bottom row illustrates the tensor shape after the corresponding layerâ€™s processing. Here,BğµBitalic_BandDğ·Ditalic_Dstand for batch size and data channel respectively.\nFor example,Dğ·Ditalic_Dequals 1 and 2 respectively for the data of 1D Gaussian and 2D fractal.\nWe have the following position encoding function applied oneach channelof the input data:{sinâ¡(2â¢Ï€â‹…u0),â€¦,sinâ¡(2â¢Ï€â‹…uCâˆ’1),cosâ¡(2â¢Ï€â‹…u0),â€¦,cosâ¡(2â¢Ï€â‹…uCâˆ’1)}â‹…2ğœ‹subscriptğ‘¢0â€¦â‹…2ğœ‹subscriptğ‘¢ğ¶1â‹…2ğœ‹subscriptğ‘¢0â€¦â‹…2ğœ‹subscriptğ‘¢ğ¶1\\{\\sin(2\\pi\\cdot u_{0}),\\dots,\\sin(2\\pi\\cdot u_{C-1}),\\cos(2\\pi\\cdot u_{0}),%\n\\dots,\\cos(2\\pi\\cdot u_{C-1})\\}{ roman_sin ( 2 italic_Ï€ â‹… italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , â€¦ , roman_sin ( 2 italic_Ï€ â‹… italic_u start_POSTSUBSCRIPT italic_C - 1 end_POSTSUBSCRIPT ) , roman_cos ( 2 italic_Ï€ â‹… italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , â€¦ , roman_cos ( 2 italic_Ï€ â‹… italic_u start_POSTSUBSCRIPT italic_C - 1 end_POSTSUBSCRIPT ) }, whereC=128/2ğ¶1282C=128/2italic_C = 128 / 2in our setup.\nWe useui=xâ‹…expâ¡(iâ‹…log2â¡(âˆ’10000Câˆ’1))subscriptğ‘¢ğ‘–â‹…ğ‘¥â‹…ğ‘–subscript210000ğ¶1u_{i}=x\\cdot\\exp\\left(i\\cdot\\log_{2}\\left(-\\frac{10000}{C-1}\\right)\\right)italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_x â‹… roman_exp ( italic_i â‹… roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( - divide start_ARG 10000 end_ARG start_ARG italic_C - 1 end_ARG ) ), wherexğ‘¥xitalic_xis the value at the corresponding channel the position encoding is applied.\nBlocks with a grey background color are optional.\nConcretely, for classifier guidance, we train anunconditionalmodel, which does not use the conditioning informationcğ‘citalic_c. In contrast, for classifier-free guidance, we enable the embedding layer to train aconditionalmodel.\nThroughout our experiments, we repeat the second to last block four times, namely we haveN=4ğ‘4N=4italic_N = 4.",
                "position": 2297
            },
            {
                "img": "https://arxiv.org/html/2503.10638/extracted/6273155/figures/gaussian_1d_traj_cg/cg_mean_-1.0_1.0_std_0.05_0.05_classifier_bad_50k_per_class.png",
                "caption": "Figure S8:Classifier guidanceÂ with a linear classifier (Fig.2(b)) per-class visualization.",
                "position": 2553
            }
        ]
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    },
    {
        "header": "Appendix EMore Visualizations",
        "images": []
    }
]