[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10737/x1.png",
                "caption": "Figure 1:Each paper within a corpus contributes to different dimensions of scientific literature. We show how corpora from different eras of NLP (e.g., BERT-era; RLHF-era) can influence their respective dimension-specific taxonomies (we highlight certain subtrees).",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2506.10737/x2.png",
                "caption": "Figure 2:We proposeTaxoAdapt, a framework which dynamically constructs a LLM-enhanced, corpus-specific taxonomy using classification-based expansion signals. The diagram demonstrates awidthexpansion example, but the same logic is applied to depth expansion (simply without the additional sibling context).",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experimental Design",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10737/x3.png",
                "caption": "Figure 3:We show the evolution of NLP Tasks from EMNLP’22 to EMNLP’24. Due to space constraints, wehighlight specific subtrees of interest, emphasizing nodes which feature commonly-known topical trends within NLP. We also show the number of papers that TaxoAdapt maps to each of the nodes (Section3.3) in parentheses.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2506.10737/extracted/6536631/figs/gpt-icon.png",
                "caption": "Table 4:Comparison of performance across models on EMNLP’22 and ICRA’20 datasets.",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2506.10737/extracted/6536631/figs/llama-icon.png",
                "caption": "",
                "position": 930
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Settings",
        "images": []
    },
    {
        "header": "Appendix BBaselines",
        "images": []
    },
    {
        "header": "Appendix CLLM-Human Agreement Analysis",
        "images": []
    },
    {
        "header": "Appendix DHuman Evaluation on Subtopic Pseudo-Labeling & Clustering",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10737/extracted/6536631/figs/gpt-icon.png",
                "caption": "Table 7:Performance comparison on Biology Papers dataset.",
                "position": 1462
            }
        ]
    },
    {
        "header": "Appendix ECase Study on the Role of LLM General Knowledge in Taxonomy Construction",
        "images": []
    },
    {
        "header": "Appendix FNon-Computer Science Domains",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10737/x4.png",
                "caption": "Figure 4:NLP Real-World Domains output taxonomy for EMNLP’22.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2506.10737/x5.png",
                "caption": "Figure 5:NLP Real-World Domains output taxonomy for EMNLP’24.",
                "position": 1706
            }
        ]
    },
    {
        "header": "Appendix GCase Study on Evolution of NLP Real-World Domains",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10737/extracted/6536631/figs/LLM_eval_prompt.png",
                "caption": "Figure 6:LLM evaluation prompts used to compute path granularity, sibling coherence, dimension alignment, paper relevance, and coverage.",
                "position": 1723
            }
        ]
    },
    {
        "header": "Appendix HLLM Evaluation Prompts",
        "images": []
    }
]