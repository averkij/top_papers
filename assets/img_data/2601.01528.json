[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01528/x1.png",
                "caption": "Figure 1:Overview of our DrivingGen benchmark. Video models take vision, and optional language/action as inputs to generate videos. The generated videos are then passed into our evaluation suite.\nFour comprehensive and novel sets of metrics for both videos and trajectories (distribution, quality, temporal consistency, and trajectory alignment) are introduced to evaluate world models.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x2.png",
                "caption": "(a)Weather, time of day, and region distribution between existing datasets and ours.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x2.png",
                "caption": "(a)Weather, time of day, and region distribution between existing datasets and ours.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x3.png",
                "caption": "(b)Specific driving locations inside each region of our dataset.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x4.png",
                "caption": "(c)Representative examples in our benchmark, which covers diverse scenarios such as dense city traffic at night, unusual weather (e.g., fog, flood, sandstorm), and complex interactions (e.g., waiting for pedestrians, agents cutting in).",
                "position": 307
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3DrivingGen Benchmark",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Future Work and Limitations",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01528/x5.png",
                "caption": "(a)Weather and time of day distribution in our ego-condition track.",
                "position": 1269
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x5.png",
                "caption": "(a)Weather and time of day distribution in our ego-condition track.",
                "position": 1272
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x6.png",
                "caption": "Figure 4:The gallery of our ego-condition track.",
                "position": 1318
            },
            {
                "img": "https://arxiv.org/html/2601.01528/x7.png",
                "caption": "Figure 5:Human Validation of Our benchmark. Our metrics closely match human preferences. Trajectory-related metrics are less accurate in comparison to humans, likely due to noisy monocular SLAM and metric-depth recovery from generated videos with artifacts.",
                "position": 1653
            }
        ]
    },
    {
        "header": "Appendix BAppendix",
        "images": []
    }
]