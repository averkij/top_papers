[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18912/x1.png",
                "caption": "Figure 1:We propose a novel approach for learning a neural dynamics model from real-world data. Using videos captured from robot-object interactions, we obtain dense 3D tracking with a dynamic 3D Gaussian Splatting framework. We train a graph-based neural dynamics model on top of the 3D Gaussian particles for action-conditioned video prediction and model-based planning.",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18912/x2.png",
                "caption": "Figure 2:Overview of Our Framework:We first achieve dense 3D tracking of long-horizon robot-object interactions using multi-view videos and Dyn3DGS optimization. We then learn the object dynamics through a graph-based neural network. This approach enables applications such as (i) action-conditioned video prediction using linear blend skinning for motion prediction, and (ii) model-based planning for robotics.",
                "position": 129
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18912/x3.png",
                "caption": "Figure 3:Qualitative Results of 3D Gaussian Tracking.We demonstrate point-level correspondence on the objects across various timesteps. Please check ourwebsitefor more videos showcasing precise dense tracking even under different object deformations and occlusions.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2410.18912/x4.png",
                "caption": "Figure 4:Qualitative Results of Action-Conditioned 3D Video Prediction.Our videos are generated by rendering predicted Gaussians on virtual backgrounds. Robot trajectories are visualized as curved lines (yellow: current end-effector positions, purple: history end-effector positions). Compared to the MPM baseline, our video prediction results align with the ground truth frames (GT) more accurately.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2410.18912/x5.png",
                "caption": "Figure 5:Quantitative Results of model-based planning.We perform each experiment 5 times and present the results as follows: (i) the median error curve relative to planning steps, with the area between 25 and 75 percentiles shaded, and (ii) the success rate curve relative to error thresholds.",
                "position": 678
            }
        ]
    },
    {
        "header": "5Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]