[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Training Data Scaling Considerations",
        "images": []
    },
    {
        "header": "3Tokenization",
        "images": []
    },
    {
        "header": "4Model Architecture",
        "images": []
    },
    {
        "header": "5Training Setup",
        "images": []
    },
    {
        "header": "6Training Dynamics and Optimization Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17288/figures/run24_train_loss.png",
                "caption": "Figure 1:Training loss for Run 24 (20GB dataset). Loss oscillates\nand converges slowly, indicating limited data regime.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2602.17288/figures/run23_train_loss.png",
                "caption": "Figure 2:Training loss for Run 23 (200GB dataset). Loss decreases\nsmoothly with improved stability compared to small-data regime.",
                "position": 1139
            },
            {
                "img": "https://arxiv.org/html/2602.17288/figures/run20_train_loss.png",
                "caption": "Figure 3:Final optimized run (Run 20, 200GB). Training loss\nexhibits smooth monotonic convergence with long-tail stabilization.",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2602.17288/figures/train_eval_loss.png",
                "caption": "Figure 4:Training and validation loss curves for the final run.\nNo widening divergence is observed.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2602.17288/figures/grad_norm.png",
                "caption": "Figure 5:Gradient norm across training steps. Stability is achieved\nafter early warm-up.",
                "position": 1234
            },
            {
                "img": "https://arxiv.org/html/2602.17288/figures/gpu_utilization.png",
                "caption": "Figure 6:GPU utilization and power metrics during final training run.",
                "position": 1277
            }
        ]
    },
    {
        "header": "7Evaluation",
        "images": []
    },
    {
        "header": "8Empirical Observations",
        "images": []
    },
    {
        "header": "9Limitations and Lessons Learned",
        "images": []
    },
    {
        "header": "10Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ACode Repository",
        "images": []
    },
    {
        "header": "Appendix BDifference between Tokenization, Chunking, Vector and Embedding",
        "images": []
    },
    {
        "header": "Appendix CImportance ofmodel_typein config.json",
        "images": []
    },
    {
        "header": "Appendix DRelationship Betweenmodel_typeand the Tokenizer",
        "images": []
    },
    {
        "header": "Appendix EWhat Happens When Loading a Tokenizer",
        "images": []
    },
    {
        "header": "Appendix FExample:model_type = \"llama\"with DeepSeek Architecture",
        "images": []
    },
    {
        "header": "Appendix GWhy Some Models Appear to Work Despite Mismatch",
        "images": []
    },
    {
        "header": "Appendix HCorrect Handling of DeepSeek vs. LLaMA",
        "images": []
    }
]