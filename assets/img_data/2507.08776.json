[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Compressive Light-Field Tokens (CLiFT)",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08776/x1.png",
                "caption": "Figure 1:The training and the inference system overview.Top:The training consists of three steps: 1) Multi-view encoder, tokenizing the input images; 2) Latent K-means, selecting a representative set of tokens; and 3) Neural condensation, compressing the information of all the tokens into the representative set to produce Compressed Light-Field Tokens (CLiFTs).Bottom:At inference time, given a target view, we collect a relevant set of rays with CLiFTs and render a novel view.",
                "position": 147
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/re10k.png",
                "caption": "Figure 2:Main evaluation results on the RealEstate10K dataset (left) and the DL3DV dataset (right), comparing our approach with three baseline methods (LVSM-ED[12], DepthSplat[34], and MVSplat[3]). The x-axis is the data size of the scene representation. The y-axis is the rendering quality as PSNR. Our approach CLiFT is capable of changing the data size (i.e., the number of tokens) with one trained model. CLiFT achieves significant data size reduction with comparable rendering quality and the highest overall score, while providing trade-offs of data size, rendering quality, and rendering speed.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/re10k.png",
                "caption": "",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/dl3dv.png",
                "caption": "",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x2.png",
                "caption": "Figure 3:Ablation studies on our individual components, in particular, latent K-means and neural condensation. The plots compare three variants of our system by dropping latent K-means and neural condensation one by one from the system, while varying the data size. Specifically, the x-axis is the size of the scene representation.\nThe y-axis is rendering quality (PSNR, LPIPS, and SSIM), rendering speed (FPS), or rendering cost (FLOPs), measured on an NVIDIA RTX A6000 GPU.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x3.png",
                "caption": "Figure 4:Qualitative rendering results of the baselines and ours with different data size (i.e., the number of CLiFT tokens for ours).Top: Ours vs. LVSM[12]on RealEstate10K.Bottom: Ours vs. DepthSplat[34]on DL3DV. The PSNR value is recorded under each rendering.",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x4.png",
                "caption": "Figure 5:Visualization of the latent K-means clustering, whereKùêæKitalic_K=NssubscriptùëÅùë†N_{s}italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT=128.\nEach color represents a cluster, and the yellow ring indicates the centroid token. Note that clustering is performed across multiple views, so a single cluster can span multiple images. As a result, some clusters may not have a visible centroid in a given image.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x5.png",
                "caption": "Table 1:We fix the number of storage CLiFTs to represent a scene (NssubscriptùëÅùë†N_{s}italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT=4096), then vary the number of render CLiFTS (how many tokens to use for rendering) on-the-fly and measure rendering quality (PSNR), rendering speed (PSNR), and rendering cost (theoretical number as GFLOPs).",
                "position": 344
            }
        ]
    },
    {
        "header": "5Conclusions and Future Challenges",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08776/x5.png",
                "caption": "Figure 6:Typical failure cases are (Left) Camera motions deviating too much from the training distributions in RealEstate10K; and (Right) Target views not covered by the input images in DL3DV.",
                "position": 373
            }
        ]
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/supp/re10k_ssim.png",
                "caption": "Figure 7:Evaluation results (SSIM and LPIPS) on the RealEstate10K dataset, comparing our approach with three baseline methods (LVSM-ED[12], DepthSplat[34], and MVSplat[3]). The x-axis is the data size of the scene representation.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/supp/re10k_ssim.png",
                "caption": "",
                "position": 967
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/supp/re10k_lpips.png",
                "caption": "",
                "position": 971
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/supp/dl3dv_ssim.png",
                "caption": "Figure 8:Evaluation results (SSIM and LPIPS) on the DL3DV dataset, comparing our approach with two baseline methods (DepthSplat[34]and MVSplat[3]). The x-axis is the data size of the scene representation.",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/supp/dl3dv_ssim.png",
                "caption": "",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2507.08776/extracted/6616161/pictures/supp/dl3dv_lpips.png",
                "caption": "",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x6.png",
                "caption": "Figure 9:Additional qualitative results on the RealEstate10K dataset.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x7.png",
                "caption": "Figure 10:Additional qualitative results on the RealEstate10K dataset.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x8.png",
                "caption": "Figure 11:Additional qualitative results on the DL3DV dataset.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x9.png",
                "caption": "Figure 12:Additional qualitative results on the DL3DV dataset.",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x10.png",
                "caption": "Figure 13:Additional visualization of the latent K-means algorithm for the RealEstate10k dataset.",
                "position": 1027
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x11.png",
                "caption": "Figure 14:Additional visualization of the latent K-means algorithm for the DL3DV dataset.",
                "position": 1030
            },
            {
                "img": "https://arxiv.org/html/2507.08776/x12.png",
                "caption": "Figure 15:Additional visualization of the latent K-means algorithm with different values of K, which is 512, 1024, 2048, and 3072 from the top.",
                "position": 1033
            }
        ]
    },
    {
        "header": "Appendix BAdditional implementation details",
        "images": []
    }
]