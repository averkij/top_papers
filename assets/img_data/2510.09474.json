[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x1.png",
                "caption": "Figure 1:Motivation of the proposed Multimodal Policy Internalization task.The goal is to enhance the policy-following abilities of a large multimodal model without requiring the policy to be provided in-context during inference, thereby improving both performance and efficiency.",
                "position": 178
            }
        ]
    },
    {
        "header": "2Problem Formulation",
        "images": []
    },
    {
        "header": "3Dataset Creation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x2.png",
                "caption": "Figure 2:ClevrPolicydataset.Left: Illustration of policy generation, where a decision tree is first generated and converted into natural language instructions (see AppendixC.1for details on the decision node ontology, and Figures14,15for full policy examples). Right: Example input-output pair corresponding to the policy. The policy is available only during training and not during inference.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x3.png",
                "caption": "Figure 3:GTAPolicydataset.Left: illustration of the policy, consisting of two major parts, tool description and tool calling rules (see Figure16for the full policy). Right: input and output example corresponding to the policy. The visual input can contain multiple images.",
                "position": 224
            }
        ]
    },
    {
        "header": "4Multimodal Policy Internalization (MPI) Algorithms",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x4.png",
                "caption": "Figure 4:Overview of different training algorithmsfor multimodal policy internalization. Thesolid purple outlinesindicate the parts where the next-token prediction loss is computed. On the right, we illustrate the proposed three-stage training strategy, TriMPI,\nwhich enables direct policy knowledge injection through the VM-CPT stage and policy-grounded reinforcement learning through PolicyRollout. The PolicyRollout algorithm is detailed in §4.3and illustrated in Figure5.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x5.png",
                "caption": "Figure 5:Illustration of the PolicyRollout algorithm(applied to GRPO as an example). During the rollout phase, we additionally construct a set of input instances with the policy included in-context. These policy-aware responses are added to the rollout space as if they were generated from the original inputs without the policy in-context. The advantage and policy gradient are then computed on the combined rollouts, indicated by thethick red outlines. PolicyRollout enables more policy-aware exploration without introducing a gap between training and inference, leading to significant improvements in MPI, especially on complex policies.",
                "position": 397
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x6.png",
                "caption": "Figure 6:Efficiencymetrics before and after MPI.",
                "position": 624
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Limitations",
        "images": []
    },
    {
        "header": "8Reproducibility Statement",
        "images": []
    },
    {
        "header": "9Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUse of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CDataset Creation Details",
        "images": []
    },
    {
        "header": "Appendix DDetails on CoT SFT",
        "images": []
    },
    {
        "header": "Appendix EAdditional Results on Varying Policy Complexity and Model Size",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x7.png",
                "caption": "Figure 7:Qualitative example comparing different MPI algorithms.On the left, we show the inputs and the ground-truth reasoning trajectory annotated with the original policy sections. On the right, the CoT SFT model makes an error in correctly recalling the policy condition, and the CoT SFT + GRPO model makes an incorrect decision at the fifth condition, both leading to an incorrect final outcome. In contrast, the proposed TriMPI correctly recalls all policy conditions and performs reasoning consistent with the input image.",
                "position": 2081
            }
        ]
    },
    {
        "header": "Appendix FQualitative Analysis",
        "images": []
    },
    {
        "header": "Appendix GResults on Robustness to Catastrophic Forgetting",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x8.png",
                "caption": "Figure 8:Illustration of the Policy Referral evaluation setup.We take the responses from the internalized model and ask a strong LLM (Claude-4(Anthropic,2025)) to score the consistency between any policy referral in the response and the original policy. Policy referral is designed to evaluate the quality of the embedded policy knowledge beyond end-task performance.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x9.png",
                "caption": "Figure 9:Prompt to Claude-4 for the Policy Referral evaluation.",
                "position": 2199
            }
        ]
    },
    {
        "header": "Appendix HIllustration of the Policy Referral Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x10.png",
                "caption": "Figure 10:Illustration of the Policy Override evaluation setup.For ClevrPolicy, we provide a randomly sampled new policy in-context during inference as updated policy content while keeping its original unique name. For GTAPolicy, we modify the tool-calling rules, resulting in a different model version. Policy Override evaluates the model’s capability to generalize to updated or modified policies beyond overfitting. This property is particularly important in real-world scenarios, where policies are constantly changing and model behavior must be governed by both the internalized policy and the in-context instructions.",
                "position": 2209
            }
        ]
    },
    {
        "header": "Appendix IIllustration of the Policy Override Evaluation",
        "images": []
    },
    {
        "header": "Appendix JRL Effectively Leverages Non-CoT Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.09474/x11.png",
                "caption": "Figure 11:CoT data example (ClevrPolicy-T N=6).",
                "position": 2309
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x12.png",
                "caption": "Figure 12:CoT data example (ClevrPolicy-M N=6).",
                "position": 2312
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x13.png",
                "caption": "Figure 13:CoT data example (GTAPolicy).",
                "position": 2315
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x14.png",
                "caption": "Figure 14:Full policy example (ClevrPolicy-T N=6).",
                "position": 2318
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x15.png",
                "caption": "Figure 15:Full policy example (ClevrPolicy-M N=6).",
                "position": 2323
            },
            {
                "img": "https://arxiv.org/html/2510.09474/x16.png",
                "caption": "Figure 16:Full policy example (GTAPolicy).",
                "position": 2328
            }
        ]
    },
    {
        "header": "Appendix KAdditional Related Work",
        "images": []
    }
]