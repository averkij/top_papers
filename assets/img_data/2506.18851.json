[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18851/x1.png",
                "caption": "Figure 1:Overview of training samples. (a) Single-reference setting: existing methods typically extract the reference image from the target video itself. In contrast, our approach uses reference images captured in distinct contexts. (b) Our dataset also includes multi-reference samples, presenting each subject in varied contextual settings.",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18851/x2.png",
                "caption": "Figure 2:Illustration of the copy-paste problem. The shown result is generated by a SOTA video generation model (Kling[1]).",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Phantom Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18851/x3.png",
                "caption": "Figure 3:The statistical analysis of Phantom-Data.",
                "position": 226
            }
        ]
    },
    {
        "header": "4Data Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18851/x4.png",
                "caption": "Figure 4:The overview of the data pipeline for constructing cross-pair training samples.",
                "position": 267
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18851/x5.png",
                "caption": "Figure 5:Qualitative comparisons across different training strategies.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2506.18851/x6.png",
                "caption": "Figure 6:Ablation study on Contextually Diverse Retrieval and Prior-Based Identity Verification.\n(a) Reference frames from different timestamps show that longer videos offer more diverse contexts.\n(b) Retrieval from large-scale image datasets improves recall and candidate diversity.\n(c) Without prior filtering, false positives may be included.\n(d) Verification removes mismatched or overly similar identities, ensuring high-quality pairs.",
                "position": 505
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18851/x7.png",
                "caption": "Figure 9:Comparison of reference-data construction strategies.\nNa√Øve data augmentation (Data Aug) offers only limited variation, while directly adopting state-of-the-art IP-consistent generators (e.g., DreamO, GPT-4o) can introduce appearance inconsistencies, highlighted in the figure. Best viewed in color with zoom.",
                "position": 1326
            }
        ]
    },
    {
        "header": "7The Limitations of Synthetic Data",
        "images": []
    },
    {
        "header": "8User study",
        "images": []
    },
    {
        "header": "9Broader Impact",
        "images": []
    }
]