[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04883/fig/par_with_example.png",
                "caption": "Figure 1:Overview of PAR.PAR comprises the autoregressive (AR) transformerùíØŒ∏\\mathcal{T_{\\theta}}and the flow-based backbone decoderùêØŒ∏{\\mathbf{v}}_{\\theta}. During training, we downsample a backboneùê±‚àà‚ÑùL√ó3{\\mathbf{x}}\\in\\mathbb{R}^{L\\times 3}into multi-scale representations{ùê±1,‚Ä¶,ùê±}\\{{\\mathbf{x}}^{1},\\ldots,{\\mathbf{x}}\\}.AR transformerperforms next-scale prediction, producing conditional embeddings(ùê≥1,‚Ä¶,ùê≥n)({\\mathbf{z}}^{1},\\ldots,{\\mathbf{z}}^{n})from(bos,‚Ä¶,ùê±n‚àí1)(\\textit{bos},\\ldots,{\\mathbf{x}}^{n-1}). The sharedflow-based decoderlearns to denoise backbonesùê±i{\\mathbf{x}}^{i}at each scale conditioned onùê≥i{\\mathbf{z}}^{i}. At inference, PAR autoregressively generatesùê±i{\\mathbf{x}}^{i}until the final structureùê±{\\mathbf{x}}is constructed.",
                "position": 203
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04883/fig/generation_over_scale_with_rmsd_4_samples.png",
                "caption": "Figure 2:Samples generated by PAR over scales.We illustrate PAR‚Äôs generation process across five scales. Much like sculpting a statue, the model first formulates the global structural layout at coarse scales and progressively refines the details at later scales.",
                "position": 244
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Protein Autoregressive Modeling",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04883/x1.png",
                "caption": "Figure 3:Backbone generation with human prompt.Given a small number of points (e.g., 16) as prompt, PAR can generate protein backbones that adhere to the global arrangements specified by these points,withoutany finetuning. For visualization, input points are interpolated to match the length of the generated structure.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2602.04883/fig/zero_shot_motif_scaffolding.png",
                "caption": "Figure 4:Zero-shot motif scaffolding.Given a motif structure, PAR can generate diverse, plausible scaffold structures that accurately preserve the motif via teacher-forcing the motif coordinates at each scale, without additional conditioning or fine-tuning.",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2602.04883/x2.png",
                "caption": "Figure 5:Scaling effects of PAR.Performance of four metrics over varying training steps and model sizes, (a) FPSD vs. PDB, (b) FPSD vs. AFDB, (c) fS(T), (d) sc-RMSD.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2602.04883/x3.png",
                "caption": "Figure 6:Visualization of the average attention scores in PAR autoregressive transformer over 5 scales. Obtained from samples with lengths in (128, 256]. We provide attention map visualization for shorter proteins in ¬ß8.9",
                "position": 871
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Implementation and Evaluation Details",
        "images": []
    },
    {
        "header": "7Datasets",
        "images": []
    },
    {
        "header": "8More Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04883/fig/sampling_efficiency.png",
                "caption": "Figure 7:Designability analysis of multi-scale SDE/ODE sampling methods.Naively reducing the SDE sampling steps substantially degrades the designability (red). Using ODE alone exhibits limited designability (purple). Orchestrating SDE and ODE sampling enables reduced sampling steps while retaining designability (blueandgreen).",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2602.04883/x4.png",
                "caption": "Figure 8:Ablation with self-conditioning.Self-conditioning consistently improves backbone generation performance of PAR across varying protein lengths, showing that both methods are compatible. Results are obtained with 60M PAR.",
                "position": 1949
            },
            {
                "img": "https://arxiv.org/html/2602.04883/x5.png",
                "caption": "Figure 9:Protein length distribution for long protein finetuning.",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2602.04883/x6.png",
                "caption": "Figure 10:Visualization of the average attention scores in PAR autoregressive transformer over 3/4 scales.Left Length‚àà\\in(32, 64]. Right Length‚àà\\in(64, 128].",
                "position": 2529
            },
            {
                "img": "https://arxiv.org/html/2602.04883/x7.png",
                "caption": "",
                "position": 2533
            }
        ]
    },
    {
        "header": "9Other Related Work",
        "images": []
    }
]