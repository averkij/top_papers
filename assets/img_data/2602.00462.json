[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/figures/searchicon.png",
                "caption": "",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/github-mark.png",
                "caption": "",
                "position": 192
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x1.png",
                "caption": "Figure 1:Illustration of our method.LatentLenscompares latent representations of visual tokens to contextualized text representations obtained from full sentence descriptions.",
                "position": 211
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3LatentLens",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x2.png",
                "caption": "Figure 2:Illustration ofLatentLens.(1) Contextualized token representations are precomputed in multiple layers of an LLM using a large corpus of descriptions.\n(2) Latent representations of visual tokens are extracted from all layers of the LLM, and (3) compared against the precomputed contextualized token representations.\nThe interpretability of the visual token based on its top-kkdescriptions can be automatically evaluated by a VLM-judge.",
                "position": 374
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x3.png",
                "caption": "Figure 3:Interpretability of visual tokens across layers using three different “lenses”.Each curve shows the percentage of interpretable visual tokens per layer across model.(a)EmbeddingLens: a large number of visual tokens is interpretable for OLMo variants but less for Llama3 and Qwen2.(b)LogitLens: low interpretability at early layers with a stark increase at later layers for most models.(c)LatentLens: the majority of visual tokens are interpretable across all models and layers.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x4.png",
                "caption": "Figure 4:The Mid-Layer Leap: early visual tokens align to later LLM layers.For visual tokens at different stages of LLM processing, we compute their top-5 Nearest Neighbors from all other LLM layers. We find that early visual tokens, even at the input itself, align most to middle layers, e.g., layer 8 or 16. Some model combinations align most to a constant layer throughout processing, such as LLaMA3 variants. We analyze the L2 norm distributions and potential outlier effects inAppendix˜E.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x5.png",
                "caption": "Figure 5:Interpretability of visual tokens in off-the-shelf Qwen2-VL-7B-Instruct.We applyLatentLensand baselines to an off-the-shelf model that deviates from our controlled setup in many ways (e.g. everything finetuned).\nWe observe the same pattern ofLatentLenssubstantially outperforming the baselines.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x6.png",
                "caption": "Figure 6:Qualitative examples.Highest-scoring descriptions are extracted from different combinations of LLMs and Vision Encoders using bothLatentLensand LogitLens.\nEach image is shown in its patchified format, where the green patches are expected to be interpretable according to the automatic judge.\nThe described patch is shown with a red outline, and the magnitude of scores are shown in parentheses.\nForLatentLens, the contextualized token used for the similarity calculation is shown inbold.\nBest viewed in colour.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x6.png",
                "caption": "",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x7.png",
                "caption": "",
                "position": 522
            }
        ]
    },
    {
        "header": "5Qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x8.png",
                "caption": "Figure 7:LatentLensvs. LogitLens results on rendered text for OLMo + CLIP ViT-L/14-336.LogitLens predicts plausible next tokens. We omit the surrounding sentence-level context for simplicity of visualization.",
                "position": 568
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix Overview",
        "images": []
    },
    {
        "header": "Appendix BLatentLensDesign",
        "images": []
    },
    {
        "header": "Appendix CHuman Annotations and LLM Judge Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/figures/annotation_interface.png",
                "caption": "Figure 8:Screenshot of the human annotation interface. Annotators see the image with a highlighted region (red box) and the top-5 candidate descriptions. For each candidate, they select whether it is related to the region concretely, abstractly, globally, or not at all.",
                "position": 2186
            }
        ]
    },
    {
        "header": "Appendix DAblations",
        "images": []
    },
    {
        "header": "Appendix EVision vs. Text Token L2 Norms",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x9.png",
                "caption": "Figure 9:L2 norm distributions of vision vs. text tokens across layers.Each row corresponds to an LLM (OLMo-7B, LLaMA3-8B, Qwen2-7B), and each column shows a vision encoder (CLIP, DINOv2, SigLIP). Within each cell, Vision tokens (left) and Text tokens (right) are shown. Colors indicate layer depth (yellow=early, red=late). The x-axis uses log scale. Black dotted lines mark the 99th percentile; red dashed lines mark the maximum value.",
                "position": 2374
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x10.png",
                "caption": "",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x11.png",
                "caption": "",
                "position": 2380
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x12.png",
                "caption": "",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x13.png",
                "caption": "",
                "position": 2384
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x14.png",
                "caption": "",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x15.png",
                "caption": "",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x16.png",
                "caption": "",
                "position": 2389
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x17.png",
                "caption": "",
                "position": 2390
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x18.png",
                "caption": "Figure 10:Distribution of embedding dimension values for max L2 norm visual tokens.Each row corresponds to an LLM (OLMo-7B, LLaMA3-8B, Qwen2-7B), and each column shows a vision encoder. We extract the visual token with the largest L2 norm and plot a histogram of its individual embedding dimension values. All distributions are Gaussian-like, indicating that high L2 norms result from uniform scaling across all dimensions rather than sparse outliers.",
                "position": 2429
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x19.png",
                "caption": "",
                "position": 2434
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x20.png",
                "caption": "",
                "position": 2435
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x21.png",
                "caption": "",
                "position": 2438
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x22.png",
                "caption": "",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x23.png",
                "caption": "",
                "position": 2440
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x24.png",
                "caption": "",
                "position": 2443
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x25.png",
                "caption": "",
                "position": 2444
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x26.png",
                "caption": "",
                "position": 2445
            }
        ]
    },
    {
        "header": "Appendix FLayer Alignment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x27.png",
                "caption": "Figure 11:Vision tokens follow only minor drift through LLM processing:We compare the same token (e.g. same position in the image or text passage) to its input-layer embedding across layers. We find that text tokens early on have little similarity with their initial embeddings, perhaps following some process of contextualization, abstraction, or simply preparing for next-token prediction. On the other hand, visual tokens display a much higher cosine similarity to their input embeddings, especially until middle layers.",
                "position": 2468
            }
        ]
    },
    {
        "header": "Appendix GResults for an off-the-shelf model",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x28.png",
                "caption": "(a)Layer alignment",
                "position": 2496
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x28.png",
                "caption": "(a)Layer alignment",
                "position": 2499
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x29.png",
                "caption": "(b)Token drift",
                "position": 2504
            }
        ]
    },
    {
        "header": "Appendix HFine-grained Interpretation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x30.png",
                "caption": "Figure 13:Breakdown of interpretation types.Top row: categories (Concrete 65%, Abstract 19%, Global 16%).\nSecond row: top most frequent nearest-neighbor words per category.\nLower rows: example Visual Genome phrases showing context, with target word inbold.\nData aggregated across all 10 models (9 controlled setups plus Qwen2-VL).",
                "position": 2529
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x31.png",
                "caption": "Figure 14:Interpretation types for all 9 model combinations.Each bar shows the breakdown of interpretable tokens into concrete (directly visible), abstract (conceptually related), and global (present elsewhere in image).\nConcrete interpretations dominate across all models and layers (70–90%).\nSigLIP models show notably higher global interpretations (20–30%), consistent with less localized nearest neighbors.",
                "position": 2571
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x32.png",
                "caption": "Figure 15:Interpretation types for Qwen2-VL-7B-Instruct.Similar to the frozen LLM models, concrete interpretations dominate.\nA slight decrease in concrete (and increase in abstract) is observed in later layers (26–27), possibly reflecting the unfrozen LLM’s learned abstraction.",
                "position": 2577
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x33.png",
                "caption": "Figure 16:Parts-of-speech distribution across layers for 9 trained models.Nouns dominate (45–50%), followed by proper nouns and verbs.\nThe distribution is relatively stable across layers with some per-model variation.",
                "position": 2600
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x34.png",
                "caption": "Figure 17:Parts-of-speech distribution for Qwen2-VL-7B-Instruct.Similar pattern to trained models: nouns dominate, followed by proper nouns and verbs.",
                "position": 2605
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x35.png",
                "caption": "Figure 18:Visual attribute word frequency for 9 trained models.Color words are common (5–6% early, declining to 3% late), while shape and texture words are rare (<<1%).",
                "position": 2609
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x36.png",
                "caption": "Figure 19:Visual attribute word frequency for Qwen2-VL-7B-Instruct.Similar pattern: color words dominate visual attributes, shape and texture are rare.",
                "position": 2613
            }
        ]
    },
    {
        "header": "Appendix IPhrase-Level Interpretation Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x37.png",
                "caption": "Figure 20:Phrase annotation examples (1–4).Each panel shows a vision token’s patch (red box) preprocessed as the vision encoder sees it.LatentLens: Top contextual nearest neighbor phrase fromLatentLens.Random: A random VG phrase containing the same token.",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x38.png",
                "caption": "",
                "position": 2649
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x39.png",
                "caption": "",
                "position": 2659
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x40.png",
                "caption": "",
                "position": 2659
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x41.png",
                "caption": "",
                "position": 2670
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x42.png",
                "caption": "",
                "position": 2670
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x43.png",
                "caption": "",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x44.png",
                "caption": "",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x45.png",
                "caption": "Figure 21:Phrase annotation examples (5–8).",
                "position": 2697
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x46.png",
                "caption": "",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x47.png",
                "caption": "",
                "position": 2711
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x48.png",
                "caption": "",
                "position": 2711
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x49.png",
                "caption": "",
                "position": 2722
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x50.png",
                "caption": "",
                "position": 2722
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x51.png",
                "caption": "",
                "position": 2732
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x52.png",
                "caption": "",
                "position": 2732
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x53.png",
                "caption": "Figure 22:Phrase annotation examples (9–12).",
                "position": 2744
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x54.png",
                "caption": "",
                "position": 2748
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x55.png",
                "caption": "",
                "position": 2758
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x56.png",
                "caption": "",
                "position": 2758
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x57.png",
                "caption": "",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x58.png",
                "caption": "",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x59.png",
                "caption": "",
                "position": 2779
            },
            {
                "img": "https://arxiv.org/html/2602.00462/x60.png",
                "caption": "",
                "position": 2779
            }
        ]
    },
    {
        "header": "Appendix JDynamic Corpus Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/x61.png",
                "caption": "",
                "position": 2919
            }
        ]
    },
    {
        "header": "Appendix KCaptioning Quality Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/figures/sample_caption_image.jpg",
                "caption": "Figure 23:Sample captions for the same validation image.Per-image DCScores shown in parentheses.",
                "position": 3061
            }
        ]
    },
    {
        "header": "Appendix LQualitative examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer0_ex0.png",
                "caption": "Figure 24:Layer 0 (input):Four randomly sampled visual tokens at layer 0.",
                "position": 3099
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer0_ex1.png",
                "caption": "",
                "position": 3116
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer0_ex2.png",
                "caption": "",
                "position": 3130
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer0_ex3.png",
                "caption": "",
                "position": 3143
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer8_ex0.png",
                "caption": "Figure 25:Layer 8 (early-mid):Four randomly sampled visual tokens at layer 8.",
                "position": 3157
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer8_ex1.png",
                "caption": "",
                "position": 3174
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer8_ex2.png",
                "caption": "",
                "position": 3188
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer8_ex3.png",
                "caption": "",
                "position": 3201
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer16_ex0.png",
                "caption": "Figure 26:Layer 16 (middle):Four randomly sampled visual tokens at layer 16.",
                "position": 3215
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer16_ex1.png",
                "caption": "",
                "position": 3232
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer16_ex2.png",
                "caption": "",
                "position": 3246
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer16_ex3.png",
                "caption": "",
                "position": 3259
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer24_ex0.png",
                "caption": "Figure 27:Layer 24 (late):Four randomly sampled visual tokens at layer 24.",
                "position": 3273
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer24_ex1.png",
                "caption": "",
                "position": 3290
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer24_ex2.png",
                "caption": "",
                "position": 3304
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layer24_ex3.png",
                "caption": "",
                "position": 3317
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layerfinal_ex0.png",
                "caption": "Figure 28:Final layer:Four randomly sampled visual tokens at the final layer (31 for OLMo/LLaMA, 27 for Qwen2).",
                "position": 3331
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layerfinal_ex1.png",
                "caption": "",
                "position": 3348
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layerfinal_ex2.png",
                "caption": "",
                "position": 3362
            },
            {
                "img": "https://arxiv.org/html/2602.00462/figures/random_method_comparison/layerfinal_ex3.png",
                "caption": "",
                "position": 3375
            },
            {
                "img": "https://arxiv.org/html/2602.00462/mosaic.png",
                "caption": "",
                "position": 3447
            }
        ]
    },
    {
        "header": "Appendix MBehind The Scenes",
        "images": []
    }
]