[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13863/x1.png",
                "caption": "Figure 1:Samples from our Fluid 10.5B autoregressive model with continuous tokens.",
                "position": 98
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: Autoregressive Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13863/x2.png",
                "caption": "Figure 2:Autoregressive models with different orders.(a) A raster-order autoregressive model predicts one next token based on the known ones, implemented using a GPT-like transformer with causal attention. (b) A random-order autoregressive model predicts one or multiple tokens simultaneously given a random order, implemented using a BERT-like transformer with bidirectional attention.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13863/x3.png",
                "caption": "Figure 3:Our text-to-image generation framework.A pre-trained image tokenizer converts the image into either discrete or continuous tokens. The text is embedded using a pre-trained T5 encoder, followed by a trainable text aligner. The transformer then takes cross-attention from the text embeddings to predict the missing tokens (only random order model is shown here).",
                "position": 216
            },
            {
                "img": "https://arxiv.org/html/2410.13863/extracted/5933431/figures/recon/monalisa/gt.png",
                "caption": "Figure 4:Reconstruction quality of the tokenizers.Image resolution is 256x256. The discrete tokenizer is significantly worse than the continuous tokenizer.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2410.13863/extracted/5933431/figures/recon/monalisa/vq.png",
                "caption": "",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2410.13863/extracted/5933431/figures/recon/monalisa/kl.png",
                "caption": "",
                "position": 232
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13863/x4.png",
                "caption": "Figure 5:Validation loss scales as a power-law with model size.The validation loss is evaluated on 30K images randomly sampled from the MS-COCO 2014 training set. The x and y axes are in log-scale. The change in y is relatively small for each plot, making the log-scale alike linear-scale.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x5.png",
                "caption": "",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x6.png",
                "caption": "Figure 7:Validation losses and evaluation performance scale with increasing training steps and computes.We use random-order models with continuous tokens. Results for other autoregressive variants are included in the appendix. The training compute is computed as model GFLOPs×\\times×batch size×\\times×training steps×\\times×3, where the factor of 3 accounts for the backward pass being approximately twice as compute-intensive as the forward pass.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x7.png",
                "caption": "Figure 8:Validation loss and evaluation metrics are highly correlated.We use random-order models with continuous tokens. The Pearson correlation coefficients for FID and GenEval scores are 0.917 and -0.931, respectively. We also observe that the linear correlation slightly weakens and becomes less pronounced for the 3.1B model.",
                "position": 321
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x8.png",
                "caption": "Figure 9:Visual quality and image-text alignment improves with increasing model size.Best viewed zoomed-in.Fluid•achieves the highest visual quality and best image-text alignment.",
                "position": 338
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13863/x9.png",
                "caption": "Figure 10:Validation loss and FID w.r.t. training FLOPs for raster-order models with discrete tokens.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x10.png",
                "caption": "",
                "position": 1466
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x11.png",
                "caption": "",
                "position": 1471
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x12.png",
                "caption": "Figure 13:Validation loss and FID w.r.t. training FLOPs for raster-order models with discrete tokens.",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x13.png",
                "caption": "Figure 14:Additional images generated from our 10.5B Fluid model.",
                "position": 1552
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x14.png",
                "caption": "Figure 15:Additional images generated from our 10.5B Fluid model.",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x15.png",
                "caption": "Figure 16:Additional qualitative comparisons between images generated from 3.1B and 10.5B Fluid model.",
                "position": 1562
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x16.png",
                "caption": "Figure 17:Additional qualitative comparisons between images generated from 3.1B and 10.5B Fluid model.",
                "position": 1567
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x17.png",
                "caption": "Figure 18:Failure cases for raster order generation with continuous tokens.",
                "position": 1627
            },
            {
                "img": "https://arxiv.org/html/2410.13863/x18.png",
                "caption": "Figure 19:Failure cases for random order generation with continuous tokens. In very rare cases, an abnormal bright spots can overshadow other tokens. Increasing the diffusion steps solves this issue.",
                "position": 1635
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]