[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08120/x1.png",
                "caption": "Figure 1:UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace is the first unified multimodal model specifically designed for face understanding and generation, encompassing tasks such as visual question answering, face image captioning and text-to-face image generation. The generated responses and images demonstrate UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace‚Äôs significant potential in capturing fine-grained face attributes.",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2503.08120/x2.png",
                "caption": "Figure 2:Pipeline and examples of UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace-130K construction. Left: A three-stage pipeline for building UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace-130K. Step-1: High-quality face images are collected. Step-2: Detailed captions are generated by GPT-4o with a face attribute model trained to classify fine-grained appearance, action, and emotion. Step-3: Question-answering pairs are created. These stages collectively refine GPT-4o-generated captions and produce fine-grained descriptions for VQAs generation. Right: A representative example showcasing UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace-130K‚Äôs ability to correct (e.g.,gender), enhance (e.g.,bags under eyes), and reason (e.g.,talking, slight tiredness) in GPT-4o-generated captions.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Fine-grained Facial Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08120/x3.png",
                "caption": "Figure 3:Our UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace architecture integrates Text-to-Image (T2I) and Multimodal Understanding (MMU) tasks. Text inputs are encoded via a tokenizer, while input images are processed through a VQGAN encoder, merging into a unified token sequence. A noise scheduler masks a subset of image tokens, which are then processed by a Transformer with Mixture-of-Experts (MoE) layers. These MoE layers are grouped for generation and understanding tasks, with the first operating at the token level using shared and routed experts, and the second incorporating domain-specific features at the sequence level. This hierarchical design enables fine-grained facial feature processing. The noise scheduler outputspt‚Å¢(ùê±t|ùê±0)subscriptùëùùë°conditionalsubscriptùê±ùë°subscriptùê±0p_{t}(\\mathbf{x}_{t}|\\mathbf{x}_{0})italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )for D3Diff loss computation, combined with text autoregressive loss to form the training objective.",
                "position": 157
            }
        ]
    },
    {
        "header": "3UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08120/ICCV2025-Author-Kit/figures/t2i_demo_results_v1.pdf",
                "caption": "Figure 4:Comparative analysis of face images generation quality across SDXL[43], TokenFlow[44], OmniFlow[23], Show-o[67], and UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace. Our proposed UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace effectively captures more detailed information from prompts. We highlight fine-grained attributes.",
                "position": 290
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08120/x4.png",
                "caption": "Figure 5:Activation frequency of Token-Level and Sequence-Level MoE in different layers. The left column corresponds to understanding tasks, while the right column corresponds to generation tasks. Larger circles indicate experts that are activated more frequently.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2503.08120/x5.png",
                "caption": "Figure 6:Comparison of visual question-answering results and GPT-4o-based scores.",
                "position": 664
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Appendix A: Related Works",
        "images": []
    },
    {
        "header": "2Appendix B: Implementations Details",
        "images": []
    },
    {
        "header": "3Appendix C: Absorbing-state Case with Independence between Tokens.",
        "images": []
    },
    {
        "header": "4Appendix D: Relationship Between Score Loss and Masked Generative Loss.",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.08120/x6.png",
                "caption": "Figure 1:More comparison of generated face images with other models.",
                "position": 2635
            },
            {
                "img": "https://arxiv.org/html/2503.08120/x7.png",
                "caption": "Figure 2:More face images generated by UniF2superscriptF2\\textbf{F}^{2}F start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTace",
                "position": 2638
            },
            {
                "img": "https://arxiv.org/html/2503.08120/x8.png",
                "caption": "Figure 3:Comparison of captioning results and DeepSeeek-v3-based scores. We highlight fine-grained attributes with blue and errors in answers with red.",
                "position": 2641
            },
            {
                "img": "https://arxiv.org/html/2503.08120/x9.png",
                "caption": "Figure 4:Prompts for building dataset. The first and second prompts are to GPT-4o, while the last prompt is to GPT-4. In the first prompt, the content in ‚Äú[]‚Äù is used only when the image data includes built-in captions, such as in MM-CelebA-HQ dataset.",
                "position": 2644
            }
        ]
    },
    {
        "header": "5Appendix E: Implementation of the Resampler",
        "images": []
    }
]