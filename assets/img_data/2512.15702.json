[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15702/x1.png",
                "caption": "Figure 1:We introduceResampling Forcing, an end-to-end, teacher-free training framework for autoregressive video diffusion models.Top: The teacher forcing accumulates errors and leads to video collapse.Middle: Distilled from a short bidirectional teacher, Self Forcing suffers from the degraded quality on longer videos.Bottom: Our method offers stable quality by native training on long videos.",
                "position": 101
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15702/x2.png",
                "caption": "Figure 2:Error Accumulation.Top: Models trained with ground truth input add and compound errors autoregressively.Bottom: We train the model on imperfect input with simulated model errors, stabilizing the long-horizon autoregressive generation. The gray circle represents the closest match in the ground truth distribution.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2512.15702/x3.png",
                "caption": "Figure 3:Resampling Forcing.(a) To simulate inference-time model error, we add noise on clean videos to a sampled timesteptst_{s}, then use the online model weights to autoregressively complete the remaining denoising steps. (b) The model is parallel trained with frame-level diffusion loss. (c) A sparse causal mask restricts each frame to attend only to its clean history frames.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2512.15702/x4.png",
                "caption": "Figure 4:History Routing Mechanism.Our routing mechanism dynamically selects the top-kkimportant frames to attend. In this illustration, we show ak=2k=2example, where only the 1st and 3rd frames are selected for the 4th frame‚Äôs query tokenùíí4\\boldsymbol{q}_{4}.",
                "position": 324
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.15702/x5.png",
                "caption": "Figure 5:Qualitative Comparisons.Top: We compare with representative autoregressive video generation models, showing our method‚Äôs stable quality on long video generation.Bottom: Compared with LongLive[73]that distilled from a short bidirectional teacher, our method exhibits better causality. We use dashed lines to denote the highest liquid level, and red arrows to highlight the liquid level in each frame.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2512.15702/x6.png",
                "caption": "Figure 6:Comparing Timestep Shifting.A moderate shifting scale for resampling timesteptst_{s}is necessary to balance between error accumulation and content drifting.",
                "position": 681
            },
            {
                "img": "https://arxiv.org/html/2512.15702/x7.png",
                "caption": "Figure 7:Sparse History Strategies.We compare dense causal attention, dynamic history routing, and sliding window attention in terms of appearance consistency.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2512.15702/x8.png",
                "caption": "Figure 8:History Routing Frequency.We visualize the beginning 20 frames‚Äô frequency of being selected when generating the 21st frame. For readability, the maximum bar is truncated and labeled with its exact value.",
                "position": 694
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]