[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/icons8-github-60.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/hf-logo.png",
                "caption": "",
                "position": 92
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/teaser_sub1.png",
                "caption": "",
                "position": 100
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/teaser_sub_2.png",
                "caption": "",
                "position": 102
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TAD Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/annotations.png",
                "caption": "Figure 2:TAD annotation and question generation pipeline. Boxes with green dash: Steps with human verification and quality assessment.",
                "position": 276
            }
        ]
    },
    {
        "header": "4Proposed Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/Tcogmap.png",
                "caption": "Figure 3:Overview for the proposed TCogMap method.",
                "position": 433
            }
        ]
    },
    {
        "header": "5TAD Benchmarking",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/blind.png",
                "caption": "Figure 4:Blind test results for InternvL3-8B.",
                "position": 1231
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "7Benchmark and Codes",
        "images": []
    },
    {
        "header": "8TAD Benchmark: Details and Additional Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/action_hist_all.png",
                "caption": "Figure 5:Distribution of the actions in theExact Answer Action Recognitionquestion type. Shorthand notations: Lane (L): change lane to left, Lane (R): change lane to right, Turn (L): Turn left, Turn (R): Turn right, and Straight: Straight, constant speed",
                "position": 1338
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/word_cloud_5.jpg",
                "caption": "Figure 6:Word cloud for the questions in the TAD benchmark (excluding the instructions)",
                "position": 1355
            }
        ]
    },
    {
        "header": "9Parameters for the Baselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/QuestionOverview_V3_Cropped.png",
                "caption": "Figure 7:Pictorial visualization example of the question types and format in the TAD benchmark.",
                "position": 1566
            }
        ]
    },
    {
        "header": "10Scene-CoT: Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/Cap4Time_V4_Cropped.png",
                "caption": "Figure 8:Overview of the proposed Scene-CoT method.",
                "position": 1574
            },
            {
                "img": "https://arxiv.org/html/2512.05277/x1.png",
                "caption": "Figure 9:Exact prompts and sample output with Scene-CoT for a question in TAD.",
                "position": 1583
            }
        ]
    },
    {
        "header": "11Additional Ablation Studies",
        "images": []
    },
    {
        "header": "12Data Annotation Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05277/figs/annotation_demo.png",
                "caption": "Figure 10:Data annotation pipeline.",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/qualitative_index_5_mc_action.png",
                "caption": "Figure 11:Qualitative example forMultiple Choice Action Recognitiontask using the baseline (InternVL3-8B) and the proposed methods.Greentext indicates a correct answer,reddenotes an incorrect answer.",
                "position": 2467
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/qualitative_index_2582_action_exact.png",
                "caption": "Figure 12:Qualitative example forExact Answer Action Recognitiontask using the baseline (InternVL3-8B) and proposed methods.Greentext indicates a correct answer,reddenotes an incorrect answer.",
                "position": 2470
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/qualitative_example_ego_action_duration_cropped.png",
                "caption": "Figure 13:Qualitative example forAction Durationtask using the baseline (InternVL3-8B) and proposed methods.Greentext indicates a correct answer,reddenotes an incorrect answer.",
                "position": 2473
            },
            {
                "img": "https://arxiv.org/html/2512.05277/figs/qualitative_example_relative_temporal_v2_Cropped.png",
                "caption": "Figure 14:Qualitative example forRelative Temporal Action Localizationtask using the baseline (InternVL3-8B) and proposed methods.Greentext indicates a correct answer,reddenotes an incorrect answer.",
                "position": 2476
            },
            {
                "img": "https://arxiv.org/html/2512.05277/x2.png",
                "caption": "Figure 15:Prompts used for question answering. Note that for the baseline and TCogMap, these prompts are provided to the VLM as input, whereas for Scene-CoT, this prompt is given to the LLM.",
                "position": 2488
            }
        ]
    },
    {
        "header": "13Qualitative Examples",
        "images": []
    }
]