[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07086/x1.png",
                "caption": "Figure 1:The Sombre State of LM Reasoning for Math.(left)when re-evaluating recent 1.5B reasoning-enhanced models on AIME-24 using a standardized framework (see Section4), we find substantial drops to reported results in the original papers,(right)the observed improvements from recent methods (gray highlighted area) fall entirely within the variance range (orange box plots) of DeepSeek-R1 1.5B model performance.\nThis suggests that these methods do not significantly outperform the base model‚Äîunderscoring the importance of rigorous, multi-seed evaluation protocols for obtaining reliable performance estimates.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Exploring the Design Space of Reasoning: What Matters Most?",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07086/x2.png",
                "caption": "Figure 2:Accuracy varies significantly across random seeds.We find significantly high Pass@1 variation across 20 different random seeds for nine models on AIME‚Äô24, AMC‚Äô23, and MATH500. Variance is particularly high on AIME‚Äô24 (upto 15%) and AMC‚Äô23 (upto 13%) due to the small number of test samples, highlighting instability of single-seed evaluations.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x3.png",
                "caption": "Figure 3:Bootstrapped seed averaging is reliable only beyond a threshold.We plot the variance of Mean Pass@1 scores on AIME‚Äô24 when averaging overK=1ùêæ1K=1italic_K = 1toK=10ùêæ10K=10italic_K = 10seed runs, finding that the variance is extremely high for smallKùêæKitalic_Kand significantly reduced byK=10ùêæ10K=10italic_K = 10. This suggests that using multi-seed evaluations (K‚â•10ùêæ10K\\geq 10italic_K ‚â• 10) would yield more stable estimates. For results on AMC23 and MATH500 see Figures12and13respectively.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x4.png",
                "caption": "Figure 4:Higher temperatures yield better accuracies.We find across all three datasets, higher temperatures produce better peak accuracy but introduce instability, revealing a tradeoff between performance and reproducibility. Results obtained by varying temperature from 0 to 1 in increments of 0.1, while keepingtop_pfixed at 0.9.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x5.png",
                "caption": "Figure 5:Highertop_pvalues improve performance at no cost to stability.Across all datasets, we find that highertop_pvalues generally improve performance while preserving similar amounts of variance as lowertop_pvalues. Results were obtained by varyingtop_pfrom 0 to 1 in increments of 0.1, while holding the temperature constant at 0.8.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x6.png",
                "caption": "Figure 6:Accuracies vary significantly across temperature values.Across nine different models and three datasets, we observe consistently large variations in performance (upto 15%) induced by changing the temperature. Results were obtained by varying the temperature from 0 to 1 in increments of 0.1, while holdingtop_pconstant at 0.9.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x7.png",
                "caption": "Figure 7:Accuracies vary significantly acrosstop_pvalues.Across nine different models and three datasets, we observe consistently large variations in performance (upto 8%) induced by changing thetop_pvalue. Results were obtained by varyingtop_pfrom 0 to 1 in increments of 0.1, while holding the temperature constant at 0.8.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x8.png",
                "caption": "(a)AIME24.Significant differences are observed in model performance across compute clusters.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x8.png",
                "caption": "(a)AIME24.Significant differences are observed in model performance across compute clusters.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x9.png",
                "caption": "(b)AMC23.Similar variability is seen across hardware in AMC23 results.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x10.png",
                "caption": "Figure 9:Models are extremely sensitive to output token lengths.We sweep across differentmax_new_tokens(number of tokens that models are allowed to generate) for DeepScaleR-1.5B and DeepSeek-R1-Distill-1.5B/7B on three datasets and find that they are heavily sensitive to output length limits, with premature truncation degrading the performance.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x11.png",
                "caption": "Figure 10:Using no prompt templates yields worse performance.We compare Pass@1 scores across three prompt formats: (1) math-specific prompt with chat template, (2) default chat template only, and (3) no template. Instruction-tuned models perform best with structured prompts and templates; omitting templates leads to consistent performance drops.",
                "position": 541
            }
        ]
    },
    {
        "header": "4Way Forward: Standardization in Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07086/x12.png",
                "caption": "Figure 11:Response Length vs. Accuracy.Histogram of correct vs. incorrect responses by response length, averaged over random seeds across AIME24, AIME25, AMC23, MATH500, Minerva and OlympiadBench benchmarks. Longer outputs tend to be more error-prone, even in complete responses not close to the maximum sequence length.",
                "position": 1047
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07086/x13.png",
                "caption": "Figure 12:Variance of mean Pass@1 on AMC‚Äô23.Bootstrapped estimates show substantial variance even withK=5ùêæ5K=5italic_K = 5evaluation runs, highlighting the instability of single-seed evaluations.",
                "position": 2832
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x14.png",
                "caption": "Figure 13:Variance of mean Pass@1 on MATH500.Similar to AIME‚Äô24 and AMC‚Äô23, the estimates remain volatile across seeds. EvenK=5ùêæ5K=5italic_K = 5runs do not eliminate variance, underscoring the need for largerKùêæKitalic_K.",
                "position": 2835
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x15.png",
                "caption": "Figure 14:Performance variation across compute clusters on MATH500.Differences in GPU type and environment lead to non-trivial shifts in performance, reinforcing the importance of hardware standardization.",
                "position": 2845
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x16.png",
                "caption": "Figure 15:Impact ofmax_new_tokenson OpenRS models.Models with long context support (131,072 tokens) experience degraded performance whenmax_new_tokensis set too low.",
                "position": 2902
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x17.png",
                "caption": "Figure 16:Impact ofmax_new_tokenson OpenThinker and S1.1 models.Despite shorter context limits (32,768 tokens), performance still degrades noticeably when output length is constrained.",
                "position": 2905
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x18.png",
                "caption": "Figure 17:Response Length vs. Correctness ‚Äî Models (1/2).Average number of correct and incorrect responses across response length bins for a subset of models. Longer responses consistently correlate with incorrect predictions.",
                "position": 2921
            },
            {
                "img": "https://arxiv.org/html/2504.07086/x19.png",
                "caption": "Figure 18:Response Length vs. Correctness ‚Äî Models (2/2).Continuation of model-wise response length analysis. The same trend holds across the remaining models, with incorrect answers being disproportionately long.",
                "position": 2924
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]