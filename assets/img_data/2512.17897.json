[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17897/figures/data/fig_teaser.png",
                "caption": "",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17897/x1.png",
                "caption": "Figure 2:Overview of RadarGen.(Left) Multi-view posed images at timettandt+Δ​tt+\\Delta tare fed through foundation model of metric depth estimation[55], semantic segmentation[13], and optical flow[88], enabling projection of the scene to BEV, encoding different information through color. (Middle) Encoded BEV representation is concatenated with a modality indicator specifying which map type to generate. During inference, the map is initialized as noise; during training, noise is added to the GT maps, and the Latent Encoder/Decoder are frozen while SANA’s DiT[83,54]is fine-tuned. (Right) During inference, the generated Point Density Map is deconvolved using an IRL1 Solver. The resulting sparse map is used to retrieve the RCS and Doppler values at corresponding locations to yield the final generated radar point cloud. Point color represents Doppler and point size represents RCS.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2512.17897/figures/data/fig_method_radar_maps.png",
                "caption": "Figure 3:Overview of representing radar as images (Sec.˜4.1). Constructing radar maps from a radar point cloud requires first rasterizing each point to BEV. The point locations are then convolved with a Gaussian kernelKσK_{\\sigma}to produce the Point Density MapMpM_{p}. A Voronoi tessellation is also constructed, where each cell inherits the RCS and Doppler attributes from its corresponding point, producing the mapsMrM_{r}andMdM_{d}respectively. Point color represents Doppler and point size represents RCS.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17897/x2.png",
                "caption": "Figure 4:Qualitative results.Our model generates point clouds with higher geometric and attribute fidelity to the ground truth compared to the baseline.RadarGenuses inputsttandt+Δ​tt+\\Delta t, while the baseline uses onlytt. Ground truth bounding boxes are highlighted in color.",
                "position": 510
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17897/x3.png",
                "caption": "Figure 5:Scene editing.Modifying the input images using an off-the-shelf image editing tool updates the radar response, demonstrating object removal (left) and insertion (right).",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2512.17897/x4.png",
                "caption": "Figure 6:Impact ofσ\\sigmaand recovery method on PCL reconstruction.peakselects local maxima;randomsamples pixels proportional to the probability map (without replacement);peak+randomtakes peaks first, then fills remaining points by probability sampling excluding the peaks. MSE is calculated between the input and output to AE, whose range is [-1,1].",
                "position": 785
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix BAdditional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17897/x5.png",
                "caption": "Figure 7:Additional qualitative results.Additional demonstration ofRadarGencompared to the baseline and ground truth in highway and rural scenarios. Our model generates point clouds with higher geometric and attribute fidelity to the ground truth compared to the baseline.RadarGenuses inputsttandt+Δ​tt+\\Delta t, while the baseline uses onlytt. Ground truth bounding boxes are highlighted in color.",
                "position": 2288
            },
            {
                "img": "https://arxiv.org/html/2512.17897/x6.png",
                "caption": "Figure 8:Additional qualitative results.Additional demonstration ofRadarGencompared to the baseline and ground truth in urban environments. Our model generates point clouds with higher geometric and attribute fidelity to the ground truth compared to the baseline.RadarGenuses inputsttandt+Δ​tt+\\Delta t, while the baseline uses onlytt. Ground truth bounding boxes are highlighted in color.",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2512.17897/x7.png",
                "caption": "Figure 9:Additional seeds.Our model can generate multiple sets of point clouds for a single scene by replacing the diffusion process seed.RadarGenuses inputsttandt+Δ​tt+\\Delta t. Ground truth bounding boxes are highlighted in color.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2512.17897/x8.png",
                "caption": "Figure 10:Recovery methods.Comparison of Random, Peak, and Deconvolution sparse point cloud recovery methods. Random sampling exhibits inconsistent density characterized by clustering and empty regions. Peak recovery fills the space uniformly but suffers from low density. Our Deconvolution method achieves coverage while maintaining density where necessary.RadarGenuses inputsttandt+Δ​tt+\\Delta t. Ground truth bounding boxes are highlighted in color.",
                "position": 2304
            },
            {
                "img": "https://arxiv.org/html/2512.17897/x9.png",
                "caption": "Figure 11:BEV conditioning maps.Visualization of the BEV appearance, semantic, and relative radial velocity maps produced from inputs at timesttandt+Δ​tt+\\Delta t. The appearance map retains the camera image colors. Semantic classes are color-coded as:Road,Sidewalk,Building,Vegetation,Car, andPerson. For the velocity map, lighter colors indicate positive velocity while darker colors indicate negative velocity.",
                "position": 2307
            },
            {
                "img": "https://arxiv.org/html/2512.17897/x10.png",
                "caption": "Figure 12:Qualitative analysis of limitations.Visual comparison ofRadarGenagainst the ground truth radar in a low-light night scene. In this setting, the underlying foundation models struggle to accurately recognize vehicles and estimate velocities.RadarGenwas not trained on such scenarios. Ground truth bounding boxes are highlighted in color.",
                "position": 2317
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]