[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/final_images/42725_OverviewVisual_v6.png",
                "caption": "Figure 1:Overview of key insights. We investigate the prevalence ofundisclosed private testing and selective score reportingon the Arena (Section3), and highlight significantdata access disparitiesbetween proprietary and open-source providers (Section4.1). These disparities enableoverfitting to the Arena(Section4.2). Furthermore,model deprecation practiceslack transparency, with many models silently deprecated without any notification to providers. We demonstrate how these deprecations contribute to unreliable rankings on the leaderboard (Section5).",
                "position": 275
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/x1.png",
                "caption": "Figure 2:Number of public models vs. maximum arena score per provider.Marker size indicates total number of battles played. Proprietary model providers tend to achieve higher leaderboard scores, which appear to correlate with both the number of models they release and the number of Arena battles played. While model capability is an important factor, we explore inSection3andSection4how increased exposure to the Arena (through more models and battles) may confer additional advantages, such as better model selection or adaptation to the evaluation distribution.\nThis figure summarizes publicly disclosed results as of April 23rd, 2025.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x2.png",
                "caption": "Figure 3:Volume of Arena battles involving proprietary, open-weight, and fully open-source model providers from January 2024 to March 2025, based onleaderboard-stats.Proprietary models consistently received the largest share of data—ranging from 54.3% to 70.1%. Open-weight and fully open-source models receive significantly less data, in some cases receiving less than half the amount of data as proprietary developers.\nThis imbalance in data access exacerbates the performance gap, reinforcing unequal access to high-quality in-distribution data.",
                "position": 310
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x3.png",
                "caption": "Figure 4:Data availability to model providers.We observe large differences in data access between providers, with 61.4% of all data going to proprietary providers.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x4.png",
                "caption": "Figure 5:Maximum observed sampling rate for models from different providers. The sampling rate determines the amount of times a model is shown to everyday users, and the amount of data a provider receives. We observe large discrepancies across providers, with substantially higher sampling rates for OpenAI, Google, xAI, and Meta compared to others.",
                "position": 348
            }
        ]
    },
    {
        "header": "2Overview of Methodology",
        "images": []
    },
    {
        "header": "3Results: Impact of Private Testing and Selective Retraction on Arena Scores",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/x5.png",
                "caption": "Figure 6:Number of privately-tested models per provider based onrandom-sample-battles(January–March 2025). Meta, Google, and Amazon account for the highest number of private submissions, with Meta alone testing 27 anonymous models in March alone.",
                "position": 738
            },
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/final_images/bestofk_sim_2.png",
                "caption": "Figure 7:Impact of the number of private variants tested on the best Expected Arena Score.We simulate a family of model variants with a latent average Arena Score of 1200. As we progressively increase the number of private variants tested—and subsequently discover their corresponding Arena Scores—the probability of selecting models from the higher end of the performance distribution also rises. This enables the provider to effectively identify the model with the highest score.",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/final_images/bestofk_with_annotations.png",
                "caption": "Figure 8:Simulated impact of best-of-N submission strategies on Arena leaderboard rankings.Model family Ahas a lower average Arena Score thanModel family B, yet by submitting multiple private variants and selecting the best-performing one, it can surpass the sole public submission fromModel family B. This simulation shows how providers can gain leaderboard advantage by evaluating several variants privately and publishing only the top-scoring one.",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x6.png",
                "caption": "Figure 9:Allowing retraction of scores allows providers to skew Arena scores upwards.We run a real-world experiment to measure the benefits of private testing. We show that it is possible to increase Arena scores even in the most conservative case of identical checkpoints, and further amplify the difference by strategically testing different checkpoints.Left: Identical Checkpoints.Arena Scores forAya-Vision-8Byield different Arena scores (1069 vs. 1052).Right: Strategically Selected Checkpoints.Arena Scores for two different variants ofAya-Vision-32B, which were both considered high-performing final round candidates according to internal metrics. We observe large differences in final scores (1097 vs. 1059) for the two different model variants.",
                "position": 800
            }
        ]
    },
    {
        "header": "4Results: Impact of Data Access Asymmetries on Arena Scores",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/x7.png",
                "caption": "Figure 10:Use of Chatbot Arena dataset significantly improves win-rates on ArenaHard. Increasing the amount of arena data in a supervised fine-tuning mixture (0%→→\\rightarrow\\leavevmode\\nobreak\\→30%→→\\rightarrow→70%) significantly improves win-rates of the resulting model against both the model variant where no Chatbot Arena data is used and alsoLlama-3.1-8B. The win-rates are measured on ArenaHard(Li et al.,2024c), which has a high correlation of 98.6% to Chatbot Arena.",
                "position": 897
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x7.png",
                "caption": "",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x8.png",
                "caption": "",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x9.png",
                "caption": "Figure 11:Language distribution of prompts submitted to Chatbot Arena from April 2023 to January 2025.Based on thehistorical-battlesdataset, this figure tracks the monthly share of prompt languages. Only languages with dedicated Chatbot Arena leaderboards are shown individually; the rest are grouped under “Other”. A clear shift is observed: English prompt share dropped from over 80% to nearly 50%, while usage of Chinese, Russian, and Korean prompts increased significantly.",
                "position": 925
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x10.png",
                "caption": "Figure 12:Monthly prompt duplication rates.Prompts are from November 2024 to April 2025, excluding February 2025 due to insufficient data. Duplication is measured using two similarity metrics:Exact MatchandHigh Similarity(cosine similarity of text embedding>0.95absent0.95>0.95> 0.95). For simplicity, this analysis is limited to single-turn conversations. The chart presents the percentage of battles in which duplicate or near-duplicate prompts were detected each month.",
                "position": 934
            }
        ]
    },
    {
        "header": "5Results: Impact of Model Deprecation on Arena Scores",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/x11.png",
                "caption": "Figure 13:Share of proprietary and open models that either officially deprecated or inactive on the arena based onleaderboard-statsduring the period March 3rd-April 23rd, 2025. Overall, open-weight and fully open-source models are more likely to become deprecated or inactive compared to proprietary models.",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x12.png",
                "caption": "Figure 14:Impact of evolving task distributions and model deprecation on model rankings.Left:Two-phase task distribution used in the simulation. Phase 1 isTask-1 heavy, with most battles based on Task-1; Phase 2 isTask-2 heavy, with battles predominantly based on Task-2.Right:Model rankings under changing task distributions and deprecation settings.\nScenario I only differs from Scenario II in that Model D is deprecated halfway through the battle history (after phase 1).\nThis deprecation causes Scenario II to produce a completely different ranking over models as compared to Scenario I.",
                "position": 1070
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x13.png",
                "caption": "Figure 15:Impact of comparison graph sparsity on model rankings.Left:Rankings for models D, E, F, and G diverge from the gold rankings when the comparison graph is sparse, whereas model rankings fully align with the gold rankings when the comparison graph is dense.Right:Visualization of the comparison graphs in sparse and dense settings. An edge between two models indicates a head-to-head matchup, annotated with the number of wins for each model. For example, in the sparse graph, Model A and Model B played 437 matches, with Model A winning 266 and Model B winning 171.",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x13.png",
                "caption": "",
                "position": 1338
            }
        ]
    },
    {
        "header": "6Recommendations and Guidelines for Improving Leaderboards",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "10Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AChatbot Arena Background",
        "images": []
    },
    {
        "header": "Appendix BBradley-Terry Rating Model",
        "images": []
    },
    {
        "header": "Appendix CUnbiased Sampling: Why Selecting the Maximum Introduces Bias?",
        "images": []
    },
    {
        "header": "Appendix DData sources",
        "images": []
    },
    {
        "header": "Appendix EOur Scraping Methodology of LMArena Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/emojis/smile.png",
                "caption": "Table 4:Private variants identified for different providers. The table lists the private models captured in ourscraped-random-sampleorscraped-vision-sample, along with the number of responses revealing their identities and corresponding examples. Notably, the modelraspberrywithheld its identity in most responses (37 total) but disclosed “Amazon” as its provider in three instances. It’s possible that some private models appeared in more battles, but we couldn’t capture their responses to our de-anonymizing prompt due to scraping errors. Additionally, a few battles occurred early in the project before we introduced asking the de-anonymizing prompt in our scraping methodology. Note that models with prefixes \"p2l\" and \"experimental-router\" identify as OpenAI/Google models but we assign them to LMArena as they are part of prompt-to-leaderboard work being lead by LMArena.(Frick et al.,2025).",
                "position": 5057
            },
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/emojis/wink.png",
                "caption": "",
                "position": 5621
            },
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/emojis/blush.png",
                "caption": "",
                "position": 5630
            },
            {
                "img": "https://arxiv.org/html/2504.20879/extracted/6395587/images/emojis/wave.png",
                "caption": "",
                "position": 6009
            }
        ]
    },
    {
        "header": "Appendix FLicense Categories",
        "images": []
    },
    {
        "header": "Appendix GData Access Estimation for Different Providers",
        "images": []
    },
    {
        "header": "Appendix HAnalysis of Prompt Repetitions in Arena Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/x14.png",
                "caption": "(a)Embedding cosine similarity.",
                "position": 7888
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x14.png",
                "caption": "(a)Embedding cosine similarity.",
                "position": 7891
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x15.png",
                "caption": "(b)Exact string match.",
                "position": 7896
            }
        ]
    },
    {
        "header": "Appendix ISilent Model Deprecation: Additional Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20879/x16.png",
                "caption": "Figure 17:Share of active and deprecated models by provider including official and silent deprecations based on model activity between March 3-April 23, 2025.",
                "position": 7915
            },
            {
                "img": "https://arxiv.org/html/2504.20879/x17.png",
                "caption": "Figure 18:Share of official and silent deprecations for proprietary, open-weight and open-source models based on model activity between March 3-April 23, 2025.",
                "position": 7919
            }
        ]
    },
    {
        "header": "Appendix JTransitivity Under Changing Evaluation Conditions: Additional Details",
        "images": []
    },
    {
        "header": "Appendix KOverfitting Experiments: Additional Evaluations",
        "images": []
    }
]