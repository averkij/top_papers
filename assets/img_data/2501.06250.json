[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/teaser.png",
                "caption": "Figure 1:The three major phases in Cel-Animation history: the Handcrafted Cel Age (1920s–2010s), the Computer-Assisted Cel Age (1980s–present), and the emerging AIGC Cel Era (2020s onward). A layered structure of Cel-Animation is also shown. The names of the works featured in the teaser can be found inAppendixA.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/pipeline.png",
                "caption": "Figure 2:Comprehensive workflow diagram of a traditional animation production pipeline, illustrating the stages of planning, pre-production, production, and post-production, with detailed processes such as scripting, setting, storyboarding, layout, keyframe animation, inbetweening, colorization, photography, dubbing, etc.",
                "position": 239
            }
        ]
    },
    {
        "header": "IIPreliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/settei.png",
                "caption": "Figure 3:Examples of settings (setteis) of items, characters, scenes, and mecha. The examples are fromDelicious in Dungeon(2024),Girls Band Cry(2024),Violet Evergarden(2018), andGundam Build Divers(2018), respectively.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/production.png",
                "caption": "Figure 4:A production example showing the transformation of a scene from storyboard to final compositing, demonstrating key stages including layout (L/O), keyframe animation, coloring, and background integration.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/inbetweening.png",
                "caption": "Figure 5:The conventional process of inbetweening involves adding inbetween frames between every two keyframes to create smoother motion and transitions.\nIn this example, Inbetween(3) is created first based on Keyframe(1) and Keyframe(5).\nThen, Inbetween(2) can be created based on Keyframe(1) and Inbetween(3). Similarly, Inbetween(4) can be obtained in the same way.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/colorization.png",
                "caption": "Figure 6:Color chart for directing the colorization. Example fromViolet Evergarden(2018).",
                "position": 330
            }
        ]
    },
    {
        "header": "IIIGenAI for Cel-Animation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/script-ai.png",
                "caption": "Figure 8:LLMs for Script Generation: Users collaborate with generative AI to develop story scripts. The AI agent facilitates plot planning, story expansion, and screenplay creation through iterative feedback, revision-based refinement, and role-playing interactions. Example adapted from[45].",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/settei-ai.png",
                "caption": "Figure 9:Examples of AI-generated Settings with Midjourney[50]and Stable Diffusion[44], showing various designs including items, characters, scenes, and mecha.",
                "position": 854
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/storyboard-ai.png",
                "caption": "Figure 10:AI-generated Storyboard. Identity preservation during sequential generation and alignment between story scripts and image semantics are the two core components of storyboard generation. This figure illustrates an example adapted from StoryWeaver[51].",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/layout-ai.png",
                "caption": "Figure 11:AI-generated Layouts.(Top)Layout generation guided by sketches and textual descriptions using various methods, showcasing their ability to refine object placement and appearance.(Bottom)Camera motion control enables synchronized foreground and background motion, dynamic perspective shifts, and continuous scene adjustments across frames. Adapted from[65]and[70].",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/keyframe-ai.png",
                "caption": "Figure 12:Keyframe animation with AI. Conditioned on the source image and the target 2D landmarks, the model is capable of generating the deformed avatar with the target pose. Example adapted from[71].",
                "position": 907
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/inbetweening-ai.png",
                "caption": "Figure 13:GenAI for inbetweening and interpolation. The model can generate inbetweens with sketches and colored frames as input. Example adapted from[24].",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/colorization-ai.png",
                "caption": "Figure 14:Colorization. Stylization models achieve domain transfer from sketch images to target picture styles. This example is adapted from[86].",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/photography-ai.png",
                "caption": "Figure 15:Compositing & Photography: This example adapted from[99]demonstrate the capability of achieving illumination harmonization and editing with Diffusion Models.",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/dubbing-ai.png",
                "caption": "Figure 16:GenAI for Dubbing. During this procedure, models detect the facial regions of the target avatar and replace with synchronized facial expressions and lip movements conditioned on given audio. The figure depicts an example sourced from[124].",
                "position": 998
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/cutting-ai.png",
                "caption": "Figure 17:GenAI-enabled video cutting workflow using RAVA[112], which can cut video by leveraging scene detection, planning, and automated FFmpeg execution.",
                "position": 1015
            }
        ]
    },
    {
        "header": "IVDiscussion",
        "images": []
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AA. Cel-Animations Featured in the Teaser",
        "images": []
    },
    {
        "header": "Appendix BB. Details about Quality Control & Inspection",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_3D_Assistance.png",
                "caption": "",
                "position": 2659
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_bg.png",
                "caption": "",
                "position": 2719
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_book.png",
                "caption": "",
                "position": 2735
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_genga.png",
                "caption": "",
                "position": 2816
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_ed.png",
                "caption": "",
                "position": 2833
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_inbetween.png",
                "caption": "",
                "position": 2850
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_layout.png",
                "caption": "",
                "position": 2866
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_moodboard.png",
                "caption": "",
                "position": 2883
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_nigen.png",
                "caption": "",
                "position": 2900
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_op.png",
                "caption": "",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_photograph.png",
                "caption": "",
                "position": 2934
            },
            {
                "img": "https://arxiv.org/html/2501.06250/extracted/6119760/figs/app_storyboard.png",
                "caption": "",
                "position": 2976
            }
        ]
    },
    {
        "header": "Appendix CC. Terminology",
        "images": []
    }
]