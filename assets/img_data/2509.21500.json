[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21500/figure/hypothesis-new.png",
                "caption": "Figure 1:Chasing the Tail with Rubric-Based Rewards",
                "position": 188
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3High-Reward Region Accuracy is Key to Overcoming Reward Over-optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21500/x1.png",
                "caption": "(a)Win rate with reward misspecification",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2509.21500/x1.png",
                "caption": "(a)Win rate with reward misspecification",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2509.21500/x2.png",
                "caption": "(b)Win rate when different proportions of top responses are correctly ranked",
                "position": 267
            },
            {
                "img": "https://arxiv.org/html/2509.21500/figure/single.png",
                "caption": "(a)Single-round Improvement",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2509.21500/figure/single.png",
                "caption": "(a)Single-round Improvement",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2509.21500/figure/iterative.png",
                "caption": "(b)Iterative Improvement",
                "position": 412
            }
        ]
    },
    {
        "header": "4Principles for Constructing Rubrics",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21500/x3.png",
                "caption": "Figure 4:Refinement withgreatanddiverseresponses mitigates reward over-optimization. Training rewardsrrrise similarly across settings, but only models trained with iteratively refined, diverse rubrics sustain higher win-rates (a proxy for ground-truth rewardrâˆ—r^{*}) and delay the collapse that signals reward over-optimization.",
                "position": 617
            }
        ]
    },
    {
        "header": "6Related work",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Usage of Large Language Models",
        "images": []
    },
    {
        "header": "Appendix ATheoretical Results",
        "images": []
    },
    {
        "header": "Appendix BPrompts Used for Experiments",
        "images": []
    },
    {
        "header": "Appendix CHyperparameter",
        "images": []
    },
    {
        "header": "Appendix DEmpirical Results on RLHF",
        "images": []
    },
    {
        "header": "Appendix ELLM Judge for evaluation",
        "images": []
    },
    {
        "header": "Appendix FFrontier Models Used to Create Candidate Responses",
        "images": []
    },
    {
        "header": "Appendix GPrinciples of Selecting Prompts",
        "images": []
    },
    {
        "header": "Appendix HPattern detection on rubric refinements",
        "images": []
    },
    {
        "header": "Appendix IExamples of Rubrics and Rubric Refinements",
        "images": []
    }
]