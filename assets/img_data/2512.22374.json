[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22374/x1.png",
                "caption": "Figure 1:Qualitative Any-Step Generation.We showcase diverse text-to-image results from our model at different inference step counts, demonstrating coherent semantics, strong text alignment. Text prompts are provided in the supplementary material.",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2512.22374/x2.png",
                "caption": "Figure 2:Self-Evaluating Model.(a)Overview.The model is trained with two complementary objectives: learning from data (b) and self-evaluation (c).\n(b)Learning from data.Given a real sampleùê±0\\mathbf{x}_{0}, we add noise to obtainùê±t\\mathbf{x}_{t}and trainGŒ∏t‚ÜísG_{\\theta}^{t\\rightarrow s}with anx0x_{0}-prediction loss, providing local trajectory supervision.\n(c)Self-evaluation with classifier score.Whens<ts<t, we re-noise the generatedùê±^0\\hat{\\mathbf{x}}_{0}toùê±^s\\hat{\\mathbf{x}}_{s}and run the same network in evaluation mode (stop-gradient) twice: once with conditionùêú\\mathbf{c}and once with the null promptœï\\phi. The difference between these outputs yields a self-evaluation score, which is treated as a feedback gradient onùê±^0\\hat{\\mathbf{x}}_{0}and back-propagated through the denoising path, enforcing global distribution matching in a teacher-free manner.",
                "position": 159
            }
        ]
    },
    {
        "header": "2Background: Flow Matching",
        "images": []
    },
    {
        "header": "3Self-Evaluating Model",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22374/x3.png",
                "caption": "Figure 3:Qualitative Any-Step Comparison.Generated images from all methods at various inference steps. Our approach consistently produces detailed, semantically accurate, and visually appealing images aligned with textual prompts at all step counts. In extremely few-step scenarios (e.g., 2-step), FLUX, SANA, and SDXL fail to generate recognizable results, while LCM and TiM exhibit semantic and structural degradation.\nWhen using more inference steps, all methods improve, but our method retains superior quality, realism, and text alignment.\nAt 50 steps, normal Flow Matching realm, our method is competitive with FLUX, despite FLUX being a much larger model.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2512.22374/x4.png",
                "caption": "Figure 4:Controlled Ablation Study.We compare our method to alternative pretraining methods - Flow Matching and IMM. Full prompts appear in supplementary.\nOur method produces favorable results across all step budgets.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2512.22374/x5.png",
                "caption": "Figure 5:Training Progress Comparison.GenEval scores across different inference steps (2, 4, 8, and 50) for our method and Flow Matching over training iterations (from 50k to 300k). Our approach consistently outperforms Flow Matching at all inference steps, indicating its superior effectiveness and robustness.",
                "position": 1067
            },
            {
                "img": "https://arxiv.org/html/2512.22374/x6.png",
                "caption": "Figure 6:(Left)Models trained only with the classifier score component fromEq.13have clear checkerboard artifacts in extreme few-step regime, 2 steps in this example.(Right)Incorporating the auxiliary term fromEq.13in later stages of training helps mitigating these artifacts.\nResults are from our 2B model.",
                "position": 1070
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "S.1Derivation of the Self-Evaluation Loss",
        "images": []
    },
    {
        "header": "S.2Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22374/x7.png",
                "caption": "Figure S.1:More results with 2 and 4 steps.We showcase diverse text-to-image results from our model at 2 and 4 inference step counts,\ndemonstrating coherent semantics, strong text alignment.",
                "position": 1356
            },
            {
                "img": "https://arxiv.org/html/2512.22374/x8.png",
                "caption": "Figure S.2:More results with 8 and 50 steps.We showcase diverse text-to-image results from our model at 8 and 50 inference step counts,\ndemonstrating coherent semantics, strong text alignment.",
                "position": 1360
            }
        ]
    },
    {
        "header": "S.3Additional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22374/x9.png",
                "caption": "Figure S.3:Visualization of two special cases for choosing the secondary timestepsks_{k}during inference.Top rows:sk=tks_{k}=t_{k}, bottom rows:sk=tk+1s_{k}=t_{k+1}.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2512.22374/x10.png",
                "caption": "Figure S.4:One-step generation without classifier-free guidance. We show results of when selecting differentss.",
                "position": 1497
            }
        ]
    },
    {
        "header": "S.4Prompts of Results",
        "images": []
    },
    {
        "header": "S.5Limitations and Future Work",
        "images": []
    }
]