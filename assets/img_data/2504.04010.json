[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04010/x1.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04010/x2.png",
                "caption": "Figure 2:Overview of DiTaiListener.a) Given the listener’s appearance (reference frame), speaker’s motion, encoded via EMOCA 3DMM coefficients, speech (Wav2Vec2) and an input text control, DiTaiListener learns to generate listener face and head motions in pixel space through a video diffusion model powered by a modified DiT. b) We introduce a Causal Temporal Multimodal Adapter for seamless integration of multimodal speaker input in a temporally causal manner. c) Our long video generation pipeline consists of two video generation models. DiTaiListener-Gen generates video blocks that are fused by the DiTaiListener-Edit model that facilitates the smooth transition between two blocks, improving smoothness and reducing computational cost compared to existing long video generation strategies, e.g., prompt traveling and teacher forcing.",
                "position": 130
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04010/x3.png",
                "caption": "Figure 3:Qualitative Comparison on ViCo test set.Our method generates high-quality, photorealistic facial images with diverse and natural social behaviors, including head movements and blinks, whereas baseline methods often produce less varied and expressive responses.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2504.04010/x4.png",
                "caption": "Figure 4:Listener generation from DiTaiListener on out-of-domain identities.Our method can integrate expressions from text conditions and synthesize diverse responses to the speakers.",
                "position": 264
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04010/x5.png",
                "caption": "Figure 5:Qualitative comparison of long video generation.Our method generates smoother videos with fewer transition artifacts compared to prompt traveling and teacher forcing methods.",
                "position": 834
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Detailed User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.04010/extracted/6337681/figs/User_Study_Example.png",
                "caption": "Figure 6:A screenshot of user study survey example. The methods are anonymized as A, B, C, D, E, and the order is randomized.",
                "position": 1900
            }
        ]
    },
    {
        "header": "7Details of Dataset Processing",
        "images": []
    },
    {
        "header": "8Details of 3DMM Space Evaluation Metrics",
        "images": []
    },
    {
        "header": "9Text control",
        "images": []
    }
]