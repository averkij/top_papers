[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12207/x1.png",
                "caption": "MoS-Image",
                "position": 183
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x1.png",
                "caption": "MoS-Image",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x2.png",
                "caption": "MoS-Edit",
                "position": 191
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12207/x3.png",
                "caption": "(a)Cross-Attention",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x3.png",
                "caption": "(a)Cross-Attention",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x4.png",
                "caption": "(b)Self-Attention",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x5.png",
                "caption": "(c)MoT",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x6.png",
                "caption": "(d)MoS (Ours)",
                "position": 251
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Mixture of States: Unifying Asymmetric Transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12207/x7.png",
                "caption": "Figure 3:MoS Design Details.MoS introduces a new paradigm for multimodal interaction within transformer architectures. Rather than depending on manually designed fusion strategies, MoS employs a learned router to establish token-level sparse and dynamic connections between transformer blocks. For illustration, we use image generation as the running example and thus refer to the understanding-tower features as textual embeddings.",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x8.png",
                "caption": "Figure 4:Image Editing Inference Pipeline.Both the understanding and generation towers take the reference image as input, with their interaction facilitated through the MoS module.",
                "position": 420
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12207/x9.png",
                "caption": "(a)Sample-wise prediction.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x9.png",
                "caption": "(a)Sample-wise prediction.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x9.png",
                "caption": "(a)Sample-wise prediction.",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x10.png",
                "caption": "(b)Token-specific prediction.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x11.png",
                "caption": "Table 3:FID and CLIP results on MJHQ comparing hand-crafted routing and MoS.MoS significantly outperforms the hand-crafted routing baseline.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x12.png",
                "caption": "",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x13.png",
                "caption": "(a) Latency contribution of individual components.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x13.png",
                "caption": "(a) Latency contribution of individual components.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x14.png",
                "caption": "(b) Latency comparison with other methods.",
                "position": 744
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation and Future Studies",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12207/x15.png",
                "caption": "(a)Global Attention",
                "position": 2641
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x15.png",
                "caption": "(a)Global Attention",
                "position": 2644
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x16.png",
                "caption": "(b)Global Hidden States",
                "position": 2649
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x17.png",
                "caption": "(a) FID comparison across feature types used for interaction.",
                "position": 2661
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x17.png",
                "caption": "(a) FID comparison across feature types used for interaction.",
                "position": 2664
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x18.png",
                "caption": "(b) CLIP comparison across feature types used for interaction.",
                "position": 2669
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x19.png",
                "caption": "(c) FID performance with and without separate normalization in the router.",
                "position": 2674
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x20.png",
                "caption": "(d) CLIP performance with and without separate normalization in the router.",
                "position": 2679
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x21.png",
                "caption": "(a) FID performance by usingϵ\\epsilon-greedy.",
                "position": 2690
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x21.png",
                "caption": "(a) FID performance by usingϵ\\epsilon-greedy.",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x22.png",
                "caption": "(b) CLIP performance by usingϵ\\epsilon-greedy.",
                "position": 2698
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x23.png",
                "caption": "(c) FID performance across different top-k settings.",
                "position": 2703
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x24.png",
                "caption": "(d) CLIP performance across different top-k settings.",
                "position": 2708
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x25.png",
                "caption": "(a) FID performance by using different Und. Tower.",
                "position": 2716
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x25.png",
                "caption": "(a) FID performance by using different Und. Tower.",
                "position": 2719
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x26.png",
                "caption": "(b) CLIP performance by using different Und. Tower.",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x27.png",
                "caption": "(c) FID performance across different interaction setting.",
                "position": 2729
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x28.png",
                "caption": "(d) CLIP performance across different interaction setting.",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x29.png",
                "caption": "(a) G-SC (Semantic Consistency) Performance with varying context.",
                "position": 2749
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x29.png",
                "caption": "(a) G-SC (Semantic Consistency) Performance with varying context.",
                "position": 2752
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x30.png",
                "caption": "(b) G-PQ (Perceptual Quality) Performance with varying context.",
                "position": 2757
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x31.png",
                "caption": "(c) G-O (Overall Score) Performance with varying context.",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x32.png",
                "caption": "(a) GenEval Score using different inference steps",
                "position": 2769
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x32.png",
                "caption": "(a) GenEval Score using different inference steps",
                "position": 2772
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x33.png",
                "caption": "(b) GenEval Score using different CFG scales.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x34.png",
                "caption": "(c) GenEval Score using or not using rescale strategy.",
                "position": 2782
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x35.png",
                "caption": "(d) GenEval Score using different schedulers.",
                "position": 2787
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x36.png",
                "caption": "Figure 14:Visualization of the Router across Time Steps.The results show that different tokens induce distinct connection patterns, indicating that the router dynamically adjusts its layer-to-layer routing based on token-specific semantics.",
                "position": 2810
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x37.png",
                "caption": "Figure 15:Visualization of MoS-L on text-to-image generation.The samples are produced under a dynamic resolution setting, with the maximum side length capped at 2048 pixels.",
                "position": 4557
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x38.png",
                "caption": "Figure 16:Visualization of MoS-L/S and baseline methods on text-to-image generation.All models are evaluated with their default parameters inDiffusers. We present results on challenging cases. These include scenarios such as arranging foods with distinct categories, colors, and patterns; posters combining natural objects with visual text; and purely textual prompts, such as generating a menu. MoS-L demonstrates competitive performance in these demanding settings. Zoomed-in view for better clarity.",
                "position": 4560
            },
            {
                "img": "https://arxiv.org/html/2511.12207/x39.png",
                "caption": "Figure 17:Visualization of MoS-L/S and baseline methods on instruction-based image editing.All models are evaluated using their default parameters inDiffusers. We showcase results on hybrid instructions and the cases involving visual text editing. Zoomed-in for better clarity.",
                "position": 4563
            }
        ]
    },
    {
        "header": "8Additional Discussions on Router Designs",
        "images": []
    }
]