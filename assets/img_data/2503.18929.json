[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18929/x1.png",
                "caption": "Figure 1:TBA performs rapid, scalable exploration of model responses, improving RL efficiency on the GSM8K mathematical reasoning task. All plotted points use 4xA100 GPUs (or comparable L40S GPUs).\nDPO and RLOO baselines taken fromNoukhovitch et¬†al. (2025), PPO and VinePPO baselines taken fromKazemnejad et¬†al. (2024). The baseline model is the SFTed RhoMath-1B(Lin et¬†al.,2024)model, which obtains 40.3% accuracy after SFT and before RL. AppendixAhas details.",
                "position": 121
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18929/x2.png",
                "caption": "Figure 2:Fast, scalable LLM post-training with TBA.Continuously (solid lines), multipleSearchernodes (left) collect trajectories, while aTrainernode (right) samples from a replay buffer to train the policy off-policy. Periodically (dashed lines), updated policy weights are sent toSearchernodes, and new trajectories are added to theTrainernode‚Äôs buffer. This avoids bottlenecks at any given node, which can be 1 or more GPUs, keeping resource utilization high.",
                "position": 315
            }
        ]
    },
    {
        "header": "4TBA: Fast, Scalable LLM Post-Training",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18929/x3.png",
                "caption": "Figure 3:TBA scales search and improves RL efficiency on the TL;DR summarization task. All plotted points use 4xA100 GPUs, but TBA allocates 3 GPUs to search, and Online DPO allocates 1 GPU to search. TBA produces large-scale off-policy data that its trajectory balance objective can leverage, creating massive efficiency benefits. Online DPO baselines taken fromNoukhovitch et¬†al. (2025). Dashed and solid lines use 256 and 425 updates, respectively. AppendixAhas details.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2503.18929/x4.png",
                "caption": "Figure 4:TBA defines a new KL vs. win-rate Pareto frontier for the TL;DR summarization task.The baseline ‚ÄúOnline DPO‚Äù frontier is created by increasing the degree of off-policyness, starting from on-policy Online DPO, results from(Noukhovitch et¬†al.,2025). The TBA frontier is created by altering the training steps, searcher count, and KL annealing schedule as described in AppendixA.",
                "position": 437
            },
            {
                "img": "https://arxiv.org/html/2503.18929/x5.png",
                "caption": "Figure 5:TBA reaches the RT diversity-toxicity Pareto frontier and improves as search is scaled.(Left)On the GPT-2 automated red-teaming task ofLee et¬†al. (2025), TBA produces results on the diversity vs. toxicity Pareto frontier in less training time. Baselines taken fromLee et¬†al. (2025).(Right)Each searcher uses one V100 GPU for generating attacks. We report means and standard errors from multiple runs of the automated red-teaming task with GPT-2 at each searcher/GPU count.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2503.18929/extracted/6306290/figures/gsm8k_ablation.png",
                "caption": "Table 2:TBA speeds up the wall-clock time required to reach the Pareto frontier for the red-teaming task. The GFlowNet performances are taken fromLee et¬†al. (2025), while the training speeds are computed by us with their code. With the GPT-2 models, TBA performance improves with searcher count. With the Llama models, we trade attack toxicity for attack diversity by scaling the TBA buffer‚Äôs maximum size from 130,000 to 150,000 samples*, retaining more off-policy data.",
                "position": 509
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Details",
        "images": []
    },
    {
        "header": "Appendix BGSM8K Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.18929/extracted/6306290/figures/gsm8k_ablation.png",
                "caption": "Figure 6:GSM8K ablation studies.All experiments begin with the base hyperparameters listed in Table3and make the depicted modifications, except when studying the synchronization periodkùëòkitalic_kin the top right plot (where we use Beta Final Value0.0050.0050.0050.005because0.0040.0040.0040.004led to instability withk=4ùëò4k=4italic_k = 4). We report the mean and standard error from 2 runs of each configuration.",
                "position": 1890
            },
            {
                "img": "https://arxiv.org/html/2503.18929/extracted/6306290/figures/tldr_ablate.png",
                "caption": "Figure 7:TL;DR ablation studies.All experiments begin with the base hyperparameters listed in Table4then make the depicted modifications. More searcher nodes and more training steps tend to improve win rate at the cost of higher perplexity.",
                "position": 1930
            }
        ]
    },
    {
        "header": "Appendix CTL;DR Ablation Studies",
        "images": []
    }
]