[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x1.png",
                "caption": "",
                "position": 222
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Pretraining datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x2.png",
                "caption": "Figure 1:Performance of models trained on different subsets of FineMath and other math datasets.",
                "position": 417
            }
        ]
    },
    {
        "header": "4Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x3.png",
                "caption": "Figure 2:Dataset mixtures across training stages. Detailed descriptions are provided inSection4. The x-axis represents the number of training tokens.",
                "position": 520
            }
        ]
    },
    {
        "header": "5Post-training",
        "images": []
    },
    {
        "header": "6SmolLM2 135M and 360M",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x4.png",
                "caption": "Figure 3:Learning rate during SmolLM2 training. We used WSD scheduler with 2000 steps warmup, learning rate5.0×10−45.0superscript1045.0\\times 10^{-4}5.0 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPTand 10% decay.",
                "position": 2785
            }
        ]
    },
    {
        "header": "Appendix BEnglish web ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x5.png",
                "caption": "Figure 4:Evaluation of models trained on FineWeb-Edu and DCLM for 350B tokens. FineWeb-Edu excels at knowledge and reasoning tasks, while DCLM demonstrates stronger performance on commonsense reasoning benchmarks. A 60/40 mixture of FineWeb-Edu and DCLM achieves balanced performance across all tasks.",
                "position": 2796
            }
        ]
    },
    {
        "header": "Appendix CFineMath",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x6.png",
                "caption": "Figure 5:Results of annealing ablations comparing OWM and the text component of InfiMM-WebMath. InfiMM-WebMath consistently outperforms OWM on GSM8K, while OWM has a slight advantage on MATH. Despite training on 60B math tokens (equivalent to 5 epochs for OWM and 1.5 epochs for InfiMM-WebMath), performance remains far below state-of-the-art LLMs, highlighting the need for a new math dataset.",
                "position": 2810
            }
        ]
    },
    {
        "header": "Appendix DStack-Edu",
        "images": []
    },
    {
        "header": "Appendix EDetailed pretraining results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x7.png",
                "caption": "Figure 6:Progression of MMLU MCF and MLU CF during the training. We observe above-random (>25%) accuracy on MMLU MCF after 6T tokens of training, while MMLU CF appears to plateau.",
                "position": 3131
            }
        ]
    },
    {
        "header": "Appendix FPost-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02737/x8.png",
                "caption": "Figure 7:Needle in the Haystack evaluation of SmolLM2 with 8192 context length.",
                "position": 3363
            }
        ]
    },
    {
        "header": "Appendix GLong context evaluations",
        "images": []
    }
]