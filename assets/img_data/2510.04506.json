[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04506/x1.png",
                "caption": "Figure 1:Joint comparison of generative and embedding performance across existing baselines and ourGracemodels.Gracemodels shift instruction-tuned bases upward, simultaneously improving embedding performance while retaining generative competence.",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04506/x2.png",
                "caption": "Figure 2:Comparison of standard contrastive learning (top) and our RL-based method (bottom). Given a query with positive (D+D^{+}) and negative (D−D^{-}) documents, our policy model generates rationales forqq,D+D^{+}, andD−D^{-}, concatenates them to obtain the final representation, and is optimized with rewards that increase similarity between theqqandD+D^{+}while decreasing similarity between theqqandD−D^{-}.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Generative Representation Learning via Contrastive Policy Optimization",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04506/x3.png",
                "caption": "Figure 3:Reward function ablation study forGrace-3B showing performance across different combinations of the consistency weight (λ1\\lambda_{1}) and the hard negative mining weight (λ2\\lambda_{2}). Left: supervised training; Right: unsupervised training. The heat intensity indicates performance levels, with darker red representing higher scores.",
                "position": 1077
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04506/x4.png",
                "caption": "(a)Latency breakdown across different approaches.",
                "position": 1444
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x4.png",
                "caption": "(a)Latency breakdown across different approaches.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x5.png",
                "caption": "(b)Performance–latency trade-off.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x6.png",
                "caption": "(a)Accuracy progression across training steps.",
                "position": 1477
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x6.png",
                "caption": "(a)Accuracy progression across training steps.",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x7.png",
                "caption": "(b)Response length progression across training steps.",
                "position": 1486
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x8.png",
                "caption": "(a)Performance of various representation approaches in supervised fine-tuning.",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x8.png",
                "caption": "(a)Performance of various representation approaches in supervised fine-tuning.",
                "position": 1503
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x9.png",
                "caption": "(b)Performance of various representation approaches in unsupervised training.",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x10.png",
                "caption": "(a)Sensitivity w.r.t. batch size (b​sbs).",
                "position": 1516
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x10.png",
                "caption": "(a)Sensitivity w.r.t. batch size (b​sbs).",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2510.04506/x11.png",
                "caption": "(b)Sensitivity w.r.t. rollout number (nn).",
                "position": 1525
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]