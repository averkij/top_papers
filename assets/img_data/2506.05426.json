[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05426/x1.png",
                "caption": "Figure 1:t-SNE visualization of expert assignments on Cheetah-Vel where tasks differ in target velocities.\nLeft: token-wise MoE enables different experts to process tokens with distinct semantics.\nRight: task-wise MoE effectively manages a broad task distribution, where the difference between expert assignments is positively related to the difference between tasks.",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x2.png",
                "caption": "Figure 2:The overview of T2MIR.\n(a) We substitute the feedforward layer in causal transformer blocks with two parallel MoE layers and concatenate their outputs to feed into the next layer.\n(b)Token-wise MoE: it automatically captures distinct semantic features within the multi-modal(s,a,r)ùë†ùëéùëü(s,a,r)( italic_s , italic_a , italic_r )inputs, and uses‚Ñíbalancesubscript‚Ñíbalance\\mathcal{L}_{\\text{balance}}caligraphic_L start_POSTSUBSCRIPT balance end_POSTSUBSCRIPTas regularization loss to avoid tokens from all modalities collapsing onto minority experts.\n(c)Task-wise MoE: it assigns diverse tasks to specialized experts, and includes a contrastive learning loss‚Ñícontrastivesubscript‚Ñícontrastive\\mathcal{L}_{\\text{contrastive}}caligraphic_L start_POSTSUBSCRIPT contrastive end_POSTSUBSCRIPTto enhance task-wise routing via more precise capture of task-relevant information, whereœÑisubscriptùúèùëñ\\tau_{i}italic_œÑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTis the query andœÑi‚àó/œÑi‚Ä≤subscriptùúèsuperscriptùëñsubscriptùúèsuperscriptùëñ‚Ä≤\\tau_{i^{*}}/\\tau_{i^{\\prime}}italic_œÑ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT end_POSTSUBSCRIPT / italic_œÑ start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPTare positive/negative keys.",
                "position": 245
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x3.png",
                "caption": "Figure 3:Test return curves of two T2MIR implementations against baselines using Mixed datasets.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x4.png",
                "caption": "Figure 4:Ablation results of both T2MIR-AD and T2MIR-DPT architectures using Mixed datasets.",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x5.png",
                "caption": "Figure 5:Test return curves of T2MIR against baselines using Medium-Expert and Medium datasets.",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x6.png",
                "caption": "Figure 6:Analysis results of the number of selected experts against the total in token- and task-wise MoE on T2MIR-AD.\nFor example,2/6denotes selecting the top 2 from 6 experts.",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x7.png",
                "caption": "Figure 7:Cosine similarity of gradients between Point-Robot tasks in four quadrants (I-IV), comparing T2MIR-AD (MoE) with AD (MLP).",
                "position": 1151
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x8.png",
                "caption": "Figure 8:Analysis results of the number of selected experts against total experts in token- and task-wise MoE on T2MIR-DPT.\nFor example,2/6denotes selecting the top 2 from 6 experts.",
                "position": 3262
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x9.png",
                "caption": "Figure 9:Test return curves of T2MIR with different contrastive loss weights.",
                "position": 3608
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x10.png",
                "caption": "Figure 10:Test return curves of T2MIR with different positions of MoE layer.",
                "position": 3800
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x11.png",
                "caption": "Figure 11:Test return curves of T2MIR against baselines on Cheetah-Vel-3_Clustering using Mixed datasets.",
                "position": 4711
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x12.png",
                "caption": "Figure 12:t-SNE visualization of expert assignments on Push.\nLeft: token-wise MoE enables different experts to process three-modality tokens.\nRight: task-wise MoE effectively manages a task distribution, where trajectories from same task are prone to be distributed to same experts.",
                "position": 4731
            },
            {
                "img": "https://arxiv.org/html/2506.05426/x13.png",
                "caption": "Figure 13:Probability of task assignments to some experts.\nThe goal velocities are sampled from three distributions and divided by dashed lines.",
                "position": 4738
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]