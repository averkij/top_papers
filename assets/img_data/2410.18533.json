[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18533/x1.png",
                "caption": "Figure 1:(a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score¬†(long-context understanding ability) and recall score¬†(generation ability) of LCMs on the synthetic retrieval long-context task¬†(multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18533/x2.png",
                "caption": "Figure 2:Dataset construction pipeline of LOGO.",
                "position": 300
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18533/x3.png",
                "caption": "Figure 3:Results of the Needle-in-a-Haystack testing.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2410.18533/x4.png",
                "caption": "Figure 4:Evaluation results of language modeling task. The solid and dashed curves represent the PPL of the baselines and LOGO, respectively.",
                "position": 626
            },
            {
                "img": "https://arxiv.org/html/2410.18533/x5.png",
                "caption": "Figure 5:Model performance on short-context tasks, including MMLU, TruthfulQA, and ARC.",
                "position": 654
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18533/x6.png",
                "caption": "Figure 6:Ablation study results. (a) Comparison among different settings on the language modeling task¬†(PPL) and real-world tasks¬†(Avg.¬†score on LongBench testing set); (b) Reward difference distribution under differentMùëÄMitalic_Msettings; (c) Training GPU memory consumption of different settings.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2410.18533/x7.png",
                "caption": "Figure 7:Comparison between SFT and LOGO training strategies on the synthetic retrieval task.",
                "position": 684
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix BDetails of Experiments in Introduction",
        "images": []
    },
    {
        "header": "Appendix CDesign of LOGO Training Objective and Error Pattern Definition in LCMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18533/x8.png",
                "caption": "Figure 9:Demonstration and statistical analysis of different error patterns in long context tasks, where we have the following definitions of misalignment: (1) Instruction Unfollow: The entities in the model‚Äôs prediction are different from the entities in the question; (2) Hallucination: The entities in the prediction overlaps with the entities in the question, but the answer is incorrect.",
                "position": 1584
            }
        ]
    },
    {
        "header": "Appendix DPositional Indices Synthesis Details",
        "images": []
    },
    {
        "header": "Appendix ECase Study of LOGO Data",
        "images": []
    }
]