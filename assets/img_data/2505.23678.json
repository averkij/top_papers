[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23678/extracted/6493857/Figures/Figure1_V4.jpg",
                "caption": "Figure 1:Grounded visual reasoning enables interpretable and accurate answers.ViGoRL decomposes the task into a sequence of natural language thoughts anchored in image regions. In contrast, Vanilla GRPO and SFT baselines produce ungrounded and incorrect responses.",
                "position": 171
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23678/extracted/6493857/Figures/Figure3_2_V4.jpg",
                "caption": "Figure 2:Without actively reinforcing visually grounded behaviors, RL collapses onto shortcuts that maximize immediate rewards at the expense of richer visual reasoning.Standard CoTandVanilla GRPO(left and center) exhibit visually ungrounded reasoning, relying on vague references to scene elements (shown in yellow), which often results in incorrect answers (marked in red). In contrast,Visually Grounded RL(right) explicitly references object positions, demonstrating precise spatial grounding (shown in blue) and more often producing correct reasoning outcomes (marked in green). See Section5.3for further analysis.",
                "position": 276
            }
        ]
    },
    {
        "header": "4Visually Grounded Reinforcement Learning (ViGoRL)",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23678/extracted/6493857/Figures/Figure2_V3.jpg",
                "caption": "Figure 3:Overview of the ViGoRL approach.(Left) We use MCTS with a teacher model to generate reasoning chains grounded in specific image regions. (Middle) These reasoning trees are linearized and used for supervised fine-tuning (SFT) to train a base model. (Right) We apply GRPO with an outcome-based reward to further refine the grounded reasoning.",
                "position": 309
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23678/x1.png",
                "caption": "Figure 4:Human evaluation of grounded reasoning. Participants judged the grounded predictions as both accurate and helpful when correct.",
                "position": 1110
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "A1Appendix Overview",
        "images": []
    },
    {
        "header": "A2Limitations and Future Work",
        "images": []
    },
    {
        "header": "A3Broader Impacts",
        "images": []
    },
    {
        "header": "A4Additional Discussion",
        "images": []
    },
    {
        "header": "A5Additional Methodological Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23678/extracted/6493857/Figures/turn_bonus_plot.jpg",
                "caption": "Figure A1:Response length with and without turn bonus. Without the bonus, the model converges to always taking a single turn (as also verified by examining model outputs), whereas the bonus enables the model to stabilize multi-turn.",
                "position": 2606
            }
        ]
    },
    {
        "header": "A6Behavioral Analysis Protocol",
        "images": []
    },
    {
        "header": "A7Human Evaluation Setup",
        "images": []
    },
    {
        "header": "A8Additional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23678/x2.png",
                "caption": "Figure A2:Example reasoning traces from vanilla GRPO on the RoboSpatial dataset, which does GRPO directly on the base model without warm start.",
                "position": 3463
            },
            {
                "img": "https://arxiv.org/html/2505.23678/x3.png",
                "caption": "Figure A3:Example reasoning traces from ViGoRL on the RoboSpatial dataset.",
                "position": 3466
            }
        ]
    },
    {
        "header": "A9Example Model Outputs",
        "images": []
    }
]