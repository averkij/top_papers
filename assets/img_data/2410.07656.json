[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07656/x1.png",
                "caption": "Figure 1:Left:Hidden state norms and the meanùúΩùúΩ{\\bm{\\theta}}bold_italic_Œ∏value in trained JumpReLU activations within SAE modules.Right:Dynamics of hidden state norm changes and the differences in norms of matched decoder columnsùëædecsubscriptùëædec{\\bm{W}}_{\\mathrm{dec}}bold_italic_W start_POSTSUBSCRIPT roman_dec end_POSTSUBSCRIPTafter the folding operation. These results suggest thatùúΩùúΩ{\\bm{\\theta}}bold_italic_Œ∏captures the growth of hidden state norms. After foldingùúΩùúΩ{\\bm{\\theta}}bold_italic_Œ∏into the weights, the decoder weightsùëædec‚Ä≤superscriptsubscriptùëædec‚Ä≤{\\bm{W}}_{\\mathrm{dec}}^{\\prime}bold_italic_W start_POSTSUBSCRIPT roman_dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPTbecome dependent on the dynamics of hidden state norms, leading to a lower overall MSE during matching. For more details on the folding operation and method description, see Section3.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x1.png",
                "caption": "",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x2.png",
                "caption": "",
                "position": 210
            }
        ]
    },
    {
        "header": "3Matching Sparse Autoencoder Features",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07656/x3.png",
                "caption": "Figure 2:A schematic illustration of the differences in SAE matching with and without folded parameters. When no folding is performed (top),ùúΩùúΩ{\\bm{\\theta}}bold_italic_Œ∏encapsulates differences in hidden state norms, causing featuresùíá(A)superscriptùíáùê¥{\\bm{f}}^{(A)}bold_italic_f start_POSTSUPERSCRIPT ( italic_A ) end_POSTSUPERSCRIPTandùíá(B)superscriptùíáùêµ{\\bm{f}}^{(B)}bold_italic_f start_POSTSUPERSCRIPT ( italic_B ) end_POSTSUPERSCRIPTto have different scales, while the columns of decoder weightsùëædeci,:(A)superscriptsubscriptùëæsubscriptdecùëñ:ùê¥{\\bm{W}}_{\\mathrm{dec}_{i,:}}^{(A)}bold_italic_W start_POSTSUBSCRIPT roman_dec start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_A ) end_POSTSUPERSCRIPTandùëædeci,:(B)superscriptsubscriptùëæsubscriptdecùëñ:ùêµ{\\bm{W}}_{\\mathrm{dec}_{i,:}}^{(B)}bold_italic_W start_POSTSUBSCRIPT roman_dec start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_B ) end_POSTSUPERSCRIPThave similar norms. Matching similar columns leads to differences in the actual reconstructions of the inputùíô^^ùíô\\hat{{\\bm{x}}}over^ start_ARG bold_italic_x end_ARG, which we hypothesize is detrimental for matching SAE features. WithùúΩùúΩ{\\bm{\\theta}}bold_italic_Œ∏folding (bottom), we transfer the differences in input (and thus feature) norms to the decoder weightsùëædeci,:(A)‚Ä≤superscriptsubscriptùëæsubscriptdecùëñ:superscriptùê¥‚Ä≤{\\bm{W}}_{\\mathrm{dec}_{i,:}}^{{(A)}^{\\prime}}bold_italic_W start_POSTSUBSCRIPT roman_dec start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_A ) start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPTandùëædeci,:(B)‚Ä≤superscriptsubscriptùëæsubscriptdecùëñ:superscriptùêµ‚Ä≤{\\bm{W}}_{\\mathrm{dec}_{i,:}}^{{(B)}^{\\prime}}bold_italic_W start_POSTSUBSCRIPT roman_dec start_POSTSUBSCRIPT italic_i , : end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_B ) start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT, thereby matching features while accounting for differences in input norms. As a result, reconstructions of matched features are closer to each other than in the unfolded variant of the algorithm. See Section3.1for more details.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07656/x4.png",
                "caption": "Figure 3:Results of the MSE objective for different layer matching methods. \"Vanilla matching\" refers to matching without any permutations. The \"Matched\" and \"Folded+Matched\" variants correspond to unfolded and folded matching, respectively. In all cases, MSE is evaluated with folded parameters (i.e., for unfolded matching, parameters are first matched, then folded, and finally MSE is evaluated). When considering input scales differences (see Section3.1), this can be interpreted as the MSE in the scale of actual input reconstructions in the relevant layers. The unfolded matching consistently showed higher MSE in this scale, supporting Hypothesis2. Note thatbdecsubscriptùëèdecb_{\\mathrm{dec}}italic_b start_POSTSUBSCRIPT roman_dec end_POSTSUBSCRIPTis omitted as it does not affect the order of features in the SAE layer. For further details, refer to Section5.1.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x4.png",
                "caption": "",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x5.png",
                "caption": "",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x6.png",
                "caption": "",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x7.png",
                "caption": "Figure 4:External LLM evaluation split by layers. As before - folding thresholds results in an optimistic labeling, it also affects deeper layers making them also more optimistic.",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x7.png",
                "caption": "",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x8.png",
                "caption": "",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x9.png",
                "caption": "Figure 5:Features matched with folded parameters from the 19th to the 20th layer using the proposed method are sorted by their MSE values across the relevant SAE decoder weights. Features with small MSE values (on the left) indicate semantic similarity, while those with large MSE values (on the right) indicate that no similar features were found. For further details, refer to Section5.2.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x10.png",
                "caption": "Figure 6:The distribution of MSE values with folded matching, grouped by three labels provided by an external LLM, is presented. Lower MSE values between pairs of layers indicate better matching. Note that the MSE value distribution varies across different layer pairs due to differences in weight scales For further analysis, refer to Section5.2.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x10.png",
                "caption": "",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x11.png",
                "caption": "",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x12.png",
                "caption": "",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x13.png",
                "caption": "Figure 7:Left:External LLM evaluation of feature matching across layers with folded matching.Right:Matching Score across layers. The LLM evaluation shows near-zero \"SAME\" features in the initial layers, increasing after the 10th layer. The Matching Score shows a similar pattern but does not drop to zero in the initial layers, indicating some predictive ability. These results suggest that initial layers may have more polysemantic features, making them harder to evaluate using Neuronpedia descriptions. See Section5.3for more details.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x13.png",
                "caption": "",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x14.png",
                "caption": "",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x15.png",
                "caption": "Figure 8:Results of the external LLM evaluation. We compare features from 10th, 12th and 20th layers with matched features in subsequent layers. It can be observed that feature similarity gradually declines, but remains significant for approximately five layers. See Section5.4for more details.",
                "position": 507
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x15.png",
                "caption": "",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x16.png",
                "caption": "",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x17.png",
                "caption": "",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x18.png",
                "caption": "Figure 9:Schematic illustration of layer pruning with matched features. Instead of the standard evaluation (left), we compute features at layertùë°titalic_t, apply the permutation to match them to layert+1ùë°1t+1italic_t + 1, and then reconstruct the hidden state for layert+1ùë°1t+1italic_t + 1, effectively skipping layertùë°titalic_t. See Section5.5for more details.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x19.png",
                "caption": "Figure 10:Left:Change in cross-entropy loss (Œî‚Å¢LŒîùêø\\Delta Lroman_Œî italic_L) after pruning each layer using matched features.Right:Explained variance of the approximated hidden states compared to the true hidden states. Starting from the 10th layer, pruning results in minimal performance loss, indicating that matched features effectively approximate the skipped layers. See Section5.5for more details.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x19.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x20.png",
                "caption": "",
                "position": 592
            }
        ]
    },
    {
        "header": "6Limitations and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07656/x21.png",
                "caption": "Figure 11:Matching Score evaluation of feature matching with different modes: encoder-only, decoder-only, and encoder-decoder matching; evaluated between19191919-th and20202020-th layers. Encoder-only suffers from poor performance, while decoder-only performs on par with the encoder-decoder scheme.",
                "position": 810
            }
        ]
    },
    {
        "header": "Appendix AOn the Matching with Different Sparsity",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07656/x22.png",
                "caption": "Figure 12:Left:Change in cross-entropy loss (Œî‚Å¢LŒîùêø\\Delta Lroman_Œî italic_L) with respect to the sparsity level (meanl0subscriptùëô0l_{0}italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-norm) of the SAE.Right:Explained variance of the approximated hidden states at different sparsity levels. Interestingly, explained variance has a peak on value atl0‚âà70subscriptùëô070l_{0}\\approx 70italic_l start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚âà 70. See SectionAfor discussion.",
                "position": 816
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x22.png",
                "caption": "",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2410.07656/x23.png",
                "caption": "",
                "position": 823
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07656/extracted/5927996/figures/example_1.png",
                "caption": "Figure 13:Examples of paths first, second, third and fourth features from 20th layer to 25th layer.",
                "position": 1047
            },
            {
                "img": "https://arxiv.org/html/2410.07656/extracted/5927996/figures/example_1.png",
                "caption": "",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2410.07656/extracted/5927996/figures/example_2.png",
                "caption": "",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2410.07656/extracted/5927996/figures/example_3.png",
                "caption": "",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2410.07656/extracted/5927996/figures/example_4.png",
                "caption": "",
                "position": 1063
            }
        ]
    },
    {
        "header": "Appendix CPath Examples",
        "images": []
    }
]