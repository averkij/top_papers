[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02423/images/logo.png",
                "caption": "",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02423/x1.png",
                "caption": "Figure 1:(A) Page transition sequences: green arrows show optimal paths, red arrows show detours.\n(B) Tree-structured navigation map with optimal (green) and redundant (red) paths.\n(C) SFT enables memorizing page transitions for basic navigation.\n(D) ST-RL generalizes to unseen paths, enhancing navigation.\n(E) MT-RL improves exploration and task success through environment interaction.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02423/x2.png",
                "caption": "Figure 2:(A) Overview of the interactive training framework: the GUI agent receives observations (oo), actions (aa), rewards (rr), and interacts with the GE-Lab environment to receive feedback and execute actions.\n(B) Reinforcement learning: ST-RL operates on pre-constructed trajectories, while MT-RL extends to multi-step online rollouts, enabling the agent to generate observation-action sequences interactively within the environment. (C) The navigation task is formalized as a POMDP, where historical states and current observations jointly inform the agentâ€™s decisions.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2512.02423/images/case_study.png",
                "caption": "Figure 3:Case Study. Left: Environmental navigation map. Right: Detailed visualization of the page transition flow from the initial to the final state.",
                "position": 398
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02423/x3.png",
                "caption": "Figure 4:(A-B) Accuracy across different settings and difficulty levels within the static benchmark. (C-D) Success rates across Pass@N and difficulty levels within interactive Benchmark.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x4.png",
                "caption": "",
                "position": 523
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02423/x5.png",
                "caption": "Figure 5:(A) Heatmap visualization of performance in different OOD environments. (B-C) The impact of different SFT stages on ST-RL, showing performance (e.g., mean and standard deviation) respectively. (D) Results of action space modification to counter reward hacking in MT-RL.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x6.png",
                "caption": "",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x7.png",
                "caption": "",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x8.png",
                "caption": "",
                "position": 658
            }
        ]
    },
    {
        "header": "6Generalization to Real-World",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02423/x9.png",
                "caption": "Figure 6:The navigation map of Env-Base.",
                "position": 1157
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x10.png",
                "caption": "Figure 7:Out-of-Distribution Environment Configurations: Env-Base serves as the in-domain baseline, while Env-Image, Env-Name, Env-Position, and Env-Noise introduce controlled perturbations targeting visual appearance, semantic labels, spatial layout, and visual complexity, respectively. Each environment maintains the same underlying task structure while introducing specific domain shifts to probe different aspects of model robustness.",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x11.png",
                "caption": "Figure 8:Real-world Data with Expanded Action Space.",
                "position": 1556
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x12.png",
                "caption": "Figure 9:Case Study 1: Demonstrating Basic Navigation and Error Recovery. Task: From page_54 to page_171.",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x13.png",
                "caption": "Figure 10:Case Study 2: Navigational Precision and Efficiency. Task: From page_121 to page_180.",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2512.02423/x14.png",
                "caption": "Figure 11:Case Study 3: Complex Navigation and Novel Path Discovery. Task: From page_51 to page_180.",
                "position": 1690
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]