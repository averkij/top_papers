[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13157/extracted/6370271/images/teaser_figure.jpg",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13157/x1.png",
                "caption": "Figure 2:Overview of the data generation framework.To address the challenges of ground-aerial camera registration and novel-view synthesis, we propose a flexible framework combiningpseudo-syntheticrenderings from 3D city-wide meshes (e.g.Google Earth) withreal, ground-level images (e.g.MegaDepth[29]). The pseudo-synthetic data is captured at varying altitudes, while the real, crowd-sourced images help improve visual fidelity especially for ground-level images where mesh-based renderings lack detail. The pipeline generates pseudo-synthetic images from different altitudes, co-registers them with real images, and aligns ground-level images with aerial data for 3D reconstruction. This hybrid dataset of real and pseudo-synthetic images provides geometric supervision that helps improve performance on downstream tasks such as ground-aerial camera registration and novel view synthesis, particularly in ground-aerial settings.",
                "position": 105
            }
        ]
    },
    {
        "header": "3Generating Aerial-Ground 3D Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13157/x2.png",
                "caption": "Figure 3:Feature matching between real and pseudo-synthetic images.The pseudo-synthetic rendering has a noticeable domain gap compared to the real MegaDepth image (e.g., no transients, simplistic lighting) but still enables reliable feature matching[47]to register real images into the pseudo-synthetic reconstruction.",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2504.13157/extracted/6370271/images/training_lv.jpg",
                "caption": "Figure 4:AerialMegaDepth data (top: MegaDepth, bottom: Google Earth) featuresdiverse viewpoints & lighting conditions.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2504.13157/extracted/6370271/images/result_pairwise.jpg",
                "caption": "Figure 5:Zero-shot ground-aerial camera and geometry prediction results.Given two input images, one aerial and one ground, we compare the performance of the baseline DUSt3R[65]with the model fine-tuned on our varying-altitude data. The results demonstrate significant improvements over the baseline inunseen, challengingground-aerial scenarios, showing the effectiveness of fine-tuning DUSt3R[65]with our data. Additionally, the last column presents qualitative results on a challenging ground-aerial pair from the WxBS[39]dataset, which involves significant viewpoint change.",
                "position": 172
            }
        ]
    },
    {
        "header": "4Learning Aerial-Ground 3D Reconstruction",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13157/extracted/6370271/images/matches_mast3r.jpg",
                "caption": "Figure 6:Challenging ground-aerial feature matching.Fine-tuned MASt3R[26]achieves accurate and robust feature matching across ground-aerial pairs with extreme viewpoint changes (correspondences extracted via reciprocal nearest neighbor from MASt3R’s local feature maps). This highlights the effectiveness of our AerialMegaDepth data in improving matching performance.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2504.13157/extracted/6370271/images/result_1aNg.jpg",
                "caption": "Figure 7:3D reconstruction from one aerial and four ground images withvirtually no overlap.We use the global alignment process of DUSt3R[65]to merge pointmaps predictions. Despite the lack of overlap among the ground images, we find that incorporating a reference aerial image can effectively serve as a “map”, significantly improving pose estimation accuracy when fine-tuned on our cross-view data.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2504.13157/extracted/6370271/images/nvs_result_real.png",
                "caption": "Figure 8:Results of extreme viewpoint change in novel-view synthesiswith ZeroNVS[46]finetuned on MegaScenes[61](ZeroNVS MS) & additionally finetuned on our data. Though by no means perfect, note the big improvement in visual quality and viewpoint accuracy.",
                "position": 586
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13157/x3.png",
                "caption": "Figure 9:Our model generalizes well to lighting variations.",
                "position": 1822
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]