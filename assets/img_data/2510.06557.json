[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.06557/x1.png",
                "caption": "Figure 2:(a)Delethink 24K (a Markovian Thinker) matches and surpasses LongCoT-RL 24K in accuracy during RL training while using less compute; both methods improve as the thinking budget scales from 8K to 24K.(b)Beyond the trained thinking budget, Delethink significantly outperforms and keeps improving while others plateau; within the budget, Delethink and LongCoT-RL 24K scale similarly with test-time compute (reported using sequential sampling).(c)Training cost of R1-Distill 1.5B vs. average thinking length with an optimized stack of verl(Sheng et¬†al.,2024)+ SGLang(Zheng et¬†al.,2024)on H100s: quadratic for LongCoT and linear for Delethink, as predicted.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x2.png",
                "caption": "Figure 3:Computational profiles of LongCoT-RL and Delethink scaling fromnnton‚ÄãSnStokens.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x3.png",
                "caption": "Figure 4:(Left)On IID math tasks (AIME‚Äô24/‚Äô25, HMMT‚Äô25), Delethink outperforms LongCoT-RL 24K. Shaded regions show gains from test-time scaling (through sequential sampling), where Delethink improves the performance even more; on OOD tasks (GPQA-Diamond, LiveCodeBench) gains are modest, yet Delethink still matches or slightly beats LongCoT-RL 24K.(Right)Per-GPU rollout throughput during RL training (R1-Distill 1.5B, H100 cluster). Delethink‚Äôs RL environment design keeps peak memory constant, sustaining throughput as thinking scales; LongCoT‚Äôs memory grows linearly, driving throughput down at longer budgets.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x4.png",
                "caption": "Figure 5:(Left)Smoothed entropy over RL steps for Delethink and LongCoT-RL. Both remain roughly flat and non-collapsing(Cui et¬†al.,2025), indicating stable learning. Note that rising entropy typically precedes divergence.\n(Right)\nDelethink and LongCoT use their thinking budgets well. At longer lengths, Delethink produces more correct answers, showing it spends its budget effectively.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x5.png",
                "caption": "Figure 6:We varyùíû\\mathcal{C}while holding the total thinking budget at‚âà24K\\approx\\text{24K}tokens, so smallerùíû\\mathcal{C}implies a smaller Markovian state.(Left)smoothed training reward vs. RL step.(Right)validation accuracy (AIME‚Äô24) vs. RL step. Delethink 24K withùíû=8‚ÄãK\\mathcal{C}=8\\text{K}and4‚ÄãK4\\text{K}performs similarly on both training reward and validation score, whereas 2K trails but improves steadily over the base model during Delethink RL.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x6.png",
                "caption": "Figure 7:Scaling the Thinking Budget at Inference.Within the train-time budget, Delethink 24K and LongCoT-RL both gain accuracy as thinking tokens increase. Beyond that budget (shaded region), only Delethink continues to improve scaling from 24K up to 128K, while LongCoT-RL 24K and 8K plateau at their respective training limits.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x7.png",
                "caption": "Figure 8:Scaling Delethink to 96K.AIME‚Äô24/‚Äô25 accuracy and average trace length vs. RL step for Delethink 96K; dashed curves extend the thinking budget to 256K and 128K tokens for Delethink 96K and Delethink 24K, respectively. Despite only 150 RL steps, 96K surpasses both the baseline and its extended thinking variant, with mean trace lengths reaching up to 42K.",
                "position": 683
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x8.png",
                "caption": "Figure 9:Delethink Tracing at Initialization.Even at initialization of RL, applying Delethink Tracing, without extra prompts or training,recovers most of LongCoT performance across thinking budgets. This indicates early signs ofMarkovian Thinkingin the base model.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x9.png",
                "caption": "Figure 10:State-of-the-art reasoning LLMs, GPT-OSS-120B and Qwen3-30B-A3B, are capable of Markovian Thinking zero-shot, providing a strong initialization for training, signaling scalability. Delethink closely tracks LongCoT and recovers most of its final accuracy (the curves nearly coincide on Qwen3)",
                "position": 762
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x10.png",
                "caption": "Figure 11:(Left) Problem-solving overlap of Delethink vs. LongCoT: on AIME ‚Äô24 and ‚Äô25 they solve nearly the same set of questions. On GPQA, each solves an equal number that the other misses.\n(Right) CrossWordBench stress-tests Delethink: deleting previous tokens removes access to already found words. Delethink remains competitive, but its zero-shot limits are evident.",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x11.png",
                "caption": "Figure C.1:When training R1DistillQwen1.5B with Delethink with8‚Äãk8\\text{k}thinking context, for thinking lengths‚àº30‚Äãk\\sim 30\\text{k}the number of FLOPs for both is equal. This is because the non-attention components like dense layers dominate here. However, after30‚Äãk30\\text{k}quadratic cost of attention dominates.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x12.png",
                "caption": "Figure E.3:Ablation of the Markovian state sizemmat fixed per-chunk contextùíû=8‚Äãk\\mathcal{C}=8\\text{k}. R1-Distill models (1.5B to 14B) achieve stable accuracy even with small carried states (m‚â™ùíûm\\ll\\mathcal{C}), indicating that only modest memory is required for Markovian behavior. In contrast, Qwen3 (native 256k context) shows clear gains from larger state sizes, reflecting its long-context prior and the longer Delethink traces on AIME and LiveCodeBench tasks.",
                "position": 2369
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x13.png",
                "caption": "Figure E.4:Ablation of the Markovian state sizemmat fixed per-chunk contextùíû=4‚Äãk\\mathcal{C}=4\\text{k}.\nIn contrast toùíû=8‚Äãk\\mathcal{C}=8\\text{k}, R1-Distill 7B shows gains from larger state sizes across most tasks,\nwhereas R1-Dstill 1.5B only benefits from larger state sizes in AIME‚Äô25 and GPQA.",
                "position": 2372
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x14.png",
                "caption": "Figure E.5:Thinking Context Size Ablation. Scaling behavior of R1-Distill models across AIME‚Äô24, AIME‚Äô25, GPQA-Diamond, and LiveCodeBench under varying per-hunk contextsùíû‚àà{2k,4k,8k\\mathcal{C}\\in\\{2\\text{k},4\\text{k},8\\text{k}.\nAccuracy is plotted against total thinking tokens, with smaller contexts requiring proportionally more iterations.",
                "position": 2395
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x15.png",
                "caption": "Figure E.6:Limitation of extremely low thinking context size.\n(a) EOS rate across tasks shows that under a 2k context, R1 models almost never terminate within budget, unlike 4k and 8k.\n(b) On AIME‚Äô24,ùíû=2‚Äãk\\mathcal{C}=2\\text{k}produces much longer responses for all model sizes, indicating difficulty finalizing reasoning due to short local horizon and positional bias.",
                "position": 2408
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x16.png",
                "caption": "Figure F.7:Delethink Tracing on a LongCoT‚ÄìRL 24K checkpoint (AIME‚Äô25, avg@128). LongCoT plateaus near its trained budget, while Delethink (ùíû=8‚Äãk\\mathcal{C}=8\\text{k},ùíû=16‚Äãk\\mathcal{C}=16\\text{k}) keeps scaling;ùíû=8‚Äãk\\mathcal{C}=8\\text{k}yields the larger gain. The shaded region marks the trained budget.",
                "position": 2429
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x17.png",
                "caption": "Figure G.8:Budget forcing increases length but lowers accuracy. AIME‚Äô24 accuracy for R1‚ÄìDistill‚ÄìQwen‚Äì1.5B as the number of forced cut&continue rounds increases.\nAverage thinking tokens rise from about 17k to 25k, yet accuracy drops monotonically.\nVertical guides mark approximate2√ó2\\times,6√ó6\\times,12√ó12\\times, and20√ó20\\timesforcing.",
                "position": 2524
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x18.png",
                "caption": "Figure H.10:The average thinking length per question and its corresponding accuracy. Delethink Tracing truly test-time scales R1DistillQwen1.5B performance on AIME‚Äô25.",
                "position": 2616
            },
            {
                "img": "https://arxiv.org/html/2510.06557/x19.png",
                "caption": "Figure I.11:Example variance ofavg‚Äã@‚Äãk\\mathrm{avg@}kon AIME‚Äô25 for R1‚ÄìDistill Qwen‚Äì1.5B (LongCoT, 32k) usingN=512N=512continuations andB=5000B=5000bootstrap replicates. Variance increases sharply askkdecreases; small-QQtasks makeavg‚Äã@‚Äã16\\mathrm{avg@}16particularly unstable.",
                "position": 2636
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]