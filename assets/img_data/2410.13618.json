[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13618/x1.png",
                "caption": "Figure 1:Performance vs log-scaled trainable parameters for FGVC (left) and StanfordCars (right) on ViT Base. Our LoLDU methods withr={1,8,16,32,64,128,256,512,768}ğ‘Ÿ18163264128256512768r=\\{1,8,16,32,64,128,256,512,768\\}italic_r = { 1 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 768 }exhibit superior parameter efficiency and performance when contrasted with Linear Probing[13](LP, fine tuning the classifier head only111Kindly note that the parameter count reported does not include the classification head, as it must be trained using all methods.), FourierFT[14](n={3000,10000}ğ‘›300010000n=\\{3000,10000\\}italic_n = { 3000 , 10000 }), LoRA[9](r=16ğ‘Ÿ16r=16italic_r = 16), and Full Fine-Tuning. LoLDUr=768outperforms LoRAr=16with 96.837% fewer trainable parameters. Particularly noteworthy is that LoLDU withr=1ğ‘Ÿ1r=1italic_r = 1achieves competitive scores with just 24 trainable parameters, while LoLDU withr=768ğ‘Ÿ768r=768italic_r = 768attains the highest accuracy: 42.15% for FGVC and 66.66% for StanfordCars, showcasing the scalability and effectiveness of our approach. Full Fine-Tuning (85.8M parameters) and Linear Probing represent the upper and lower performance bounds, respectively.",
                "position": 156
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13618/x2.png",
                "caption": "Figure 2:Comparison of LoRA (left) and our LoLDU (right) method.In LoRA, tunable parameters are low-rank (rğ‘Ÿritalic_r) matricesAğ´Aitalic_AandBğµBitalic_B, withÎ”â¢W=Bâ¢AÎ”ğ‘Šğµğ´\\Delta W=BAroman_Î” italic_W = italic_B italic_A. For each weightWğ‘ŠWitalic_W, there arerÃ—(diâ¢n+doâ¢uâ¢t)ğ‘Ÿsubscriptğ‘‘ğ‘–ğ‘›subscriptğ‘‘ğ‘œğ‘¢ğ‘¡r\\times(d_{in}+d_{out})italic_r Ã— ( italic_d start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT + italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT )trainable parameters. LoLDU, however, optimizes a diagonal matrix for scale transformation, preserving original model knowledge during tuning. The weight update in LoLDU isÎ”â¢W=Ïƒâ‹…Pâ‹…(Lr,diagâ¢(zr),Ur)Î”ğ‘Šâ‹…ğœğ‘ƒsubscriptğ¿ğ‘Ÿdiagsubscriptğ‘§ğ‘Ÿsubscriptğ‘ˆğ‘Ÿ\\Delta W=\\sigma\\cdot P\\cdot(L_{r},\\text{diag}(z_{r}),U_{r})roman_Î” italic_W = italic_Ïƒ â‹… italic_P â‹… ( italic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , diag ( italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) , italic_U start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ), involvingr+1ğ‘Ÿ1r+1italic_r + 1trainable parameters. The permutation matrixPğ‘ƒPitalic_P, while omitted in this figure for simplicity, is included in Figure3",
                "position": 202
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13618/x3.png",
                "caption": "Figure 3:Schematic representation of our LoLDU method.The left diagram illustrates the forward pass, demonstrating the transformation of the inputxâˆˆâ„diâ¢nğ‘¥superscriptâ„subscriptğ‘‘ğ‘–ğ‘›x\\in\\mathbb{R}^{d_{in}}italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPTinto the outputhâˆˆâ„doâ¢uâ¢tâ„superscriptâ„subscriptğ‘‘ğ‘œğ‘¢ğ‘¡h\\in\\mathbb{R}^{d_{out}}italic_h âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPTvia a residual subspace matrixL[r:]â¢D[r:]â¢U[r:]L_{[r:]}D_{[r:]}U_{[r:]}italic_L start_POSTSUBSCRIPT [ italic_r : ] end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT [ italic_r : ] end_POSTSUBSCRIPT italic_U start_POSTSUBSCRIPT [ italic_r : ] end_POSTSUBSCRIPTand a decomposed subspace matrixÏƒâ¢Lrâ¢Drâ¢Urğœsubscriptğ¿ğ‘Ÿsubscriptğ·ğ‘Ÿsubscriptğ‘ˆğ‘Ÿ\\sigma L_{r}D_{r}U_{r}italic_Ïƒ italic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT italic_U start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. The right diagram shows the initialization process, where the residual matrix is obtained by performing LDU decomposition on the pre-trained weights, then subtracting the top-rğ‘Ÿritalic_rsubmatrices (top-rğ‘Ÿritalic_rrows and columns) from the permutation matrix (P), lower triangular (L), scaled diagonal (D), and upper triangular (U) matrices. Diagonal matrix is trainable (orange), while the other matrices remain fixed (blue). LoLDU enables efficient adaptation of pre-trained models via low-rank updates, reducing both computational cost and parameter count.",
                "position": 224
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13618/x4.png",
                "caption": "Figure 4:Comprehensive Analysis of Rank Ablation Study Results.This figure presents the performance of the ViT-base model on various image classification tasks using the LoLDU method with different ranks. The x-axis shows ranks (1 to 768), and the y-axis indicates accuracy for datasets: FGVC, StanfordCars, CIFAR10, CIFAR100, EuroSAT, and Flowers.",
                "position": 894
            },
            {
                "img": "https://arxiv.org/html/2410.13618/x5.png",
                "caption": "Figure 5:Concept Learning Progression In Text-to-Image Generation.Top row: target concept. Subsequent rows: generated images using LoLDU (our method), DreamBooth[6], and Textual Inversion[5], respectively, at training steps 0-600. LoLDU exhibits accelerated convergence, achieving concept acquisition withinâˆ¼similar-to\\simâˆ¼100 steps, surpassing baseline methods in efficiency.",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2410.13618/x6.png",
                "caption": "Figure 6:Visualized Results of the Image Generation Task.From left to right: target reference images, outputs from LoLDU (ours), DreamBooth, and Textual Inversion. Each row represents a distinct category with a specified prompt (annotated under each row). LoLDU demonstrates efficacy in generating diverse, prompt-adherent images while preserving key attributes from the reference set.",
                "position": 1248
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.13618/x7.png",
                "caption": "Figure 7:Learning Rate Ablation Study.The figure demonstrates the effect of different learning rates on ViT-base model accuracy across FGVC, StanfordCars, CIFAR10, CIFAR100, EuroSAT, and Flowers datasets.",
                "position": 2139
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]