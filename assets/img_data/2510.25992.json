[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25992/x1.png",
                "caption": "Figure 1:Performance of our method (SRL) against baselines on math reasoning benchmarks, with all models trained on the challenging s1k dataset(Muennighoff et al.,2025). Our key observations are: (1) Directly applying SFT on this dataset leads to performance degradation compared to the base model. (2) While RLVR can improve generalization over SFT, the gains are marginal. (3) Our proposed SRL method substantially outperforms these baselines, and the SRL‚Üí\\rightarrowRLVR pipeline achieves the highest performance, overcoming the challenges of training on difficult data.",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25992/x2.png",
                "caption": "Figure 2:Illustration of SRL as compared to RL(VR) and SFT.(a) RL(VR)takes a query as input and performs k rollouts. The final answer correctness is used as the reward.(b) SFTuses both a queryùê±\\mathbf{x}and a complete teacher responseùê≤\\mathbf{y}as input, training with a per-token loss to maximize the probabilityp‚Äã(ùê≤|ùê±)p(\\mathbf{y}|\\mathbf{x}).(c) SRLalso uses a query and a teacher response. It breaks the response into step actions and, at each step, uses the previous steps as context. The model generates a next step action along with its step-wise inner thoughts, and the rewardrkr_{k}is based on the similarity between the model‚Äôs and the teacher‚Äôs action.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25992/x3.png",
                "caption": "Figure 3:Given a solution trajectory, we take each summarized step as an action to be learned and take the partial solution before the step as the context of our newly created data. The model is then prompted to generate its thinking process followed by the action for the current step. A reward (r2r_{2}in the figure) is then calculated based on the similarity between the model‚Äôs and the expert‚Äôs action.",
                "position": 250
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25992/x4.png",
                "caption": "Figure 4:Reasoning length distribution for base model and model trained with SRL.",
                "position": 638
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AIllustration of SRL on SWE Tasks.",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25992/x5.png",
                "caption": "Figure 5:Illustration of applying SRL to SWE tasks. We take two rounds of the past expert actions and corresponding observations in context and prompt the LLM to think before reaches the next action. The model action is compared with the expert action to compute the sequence similarity reward.",
                "position": 1345
            }
        ]
    },
    {
        "header": "Appendix BExperiment Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Output Examples",
        "images": []
    },
    {
        "header": "Appendix DLLM Usage",
        "images": []
    }
]