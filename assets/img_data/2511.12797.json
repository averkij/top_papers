[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12797/x1.png",
                "caption": "Figure 1:We design parallel symbolic reasoning tasks that allow direct comparison of ICL behavior across modalities (linguistic and genomic).1Few-shot bitstring program-synthesis tasks (e.g., identity, NOT, majority, reverse) require models to infer mappings from examples.2Each task is rendered in two modality-specific encodings: genomic (bitstrings mapped to random nucleotides A/T/C/G) and linguistic (bitstrings mapped to random digits), preserving abstract structure but differing in surface form.3Both genomic (Evo2) and linguistic (Qwen3) models receivekk-shot demonstrations and are greedily decoded to compute exact-match accuracy.4Both models show log-linear accuracy gains with more demonstrations.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work and Broader Context",
        "images": []
    },
    {
        "header": "3Experimental Framework for Cross-domain In-context Learning",
        "images": []
    },
    {
        "header": "4Empirical Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12797/figures/accuracy_line_2x2_grid.png",
                "caption": "Figure 2:Few-shot performance of Qwen3 and Evo2 models. (a) Evo2 model performance with respect to log(shots). All models monotonically improve – the 7B and 40B have roughly equivalent performance, and the 1B trails behind them. (b) Qwen3 model performance with respect to log(shots).\nAll models improve, but not always monotonically. Smaller models struggle in 4-16 shot range.\n(c) At comparable sizes, Evo2 outperforms Qwen3. (d) Averaged performance across both model families shows consistent improvement with respect to log(shots). All models exceed the mode baseline shown in gray color.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x2.png",
                "caption": "Figure 3:Performance atn=128n=128shots. All model accuracies increase monotonically with respect to parameter count.",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x3.png",
                "caption": "Figure 4:Accuracy vs. BitLoad averaged across all tasks (BitLoad; Eq.3).\nQwen declines sharply with increasing BitLoad, while Evo degrades more gradually, indicating greater robustness.\nDetails in §4.3.",
                "position": 413
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AFurther Details on Synthetic Task Definition",
        "images": []
    },
    {
        "header": "Appendix BA Global Metric Over All Programs",
        "images": []
    },
    {
        "header": "Appendix CFull Accuracy Results",
        "images": []
    },
    {
        "header": "Appendix DMeta-Regression for ICL Efficacy with Number of Demonstrations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12797/figures/accuracy_scatter_linreg.png",
                "caption": "(a)Few-shot performance of Qwen3 and Evo2 models. All models show\nconsistent linear improvement with respect tolog⁡(shots)\\log(\\text{shots}). In contrast,\nno such improvement occurs for the naive baseline.",
                "position": 1597
            },
            {
                "img": "https://arxiv.org/html/2511.12797/figures/accuracy_scatter_linreg.png",
                "caption": "(a)Few-shot performance of Qwen3 and Evo2 models. All models show\nconsistent linear improvement with respect tolog⁡(shots)\\log(\\text{shots}). In contrast,\nno such improvement occurs for the naive baseline.",
                "position": 1600
            },
            {
                "img": "https://arxiv.org/html/2511.12797/figures/accuracy_regression_intercept_se.png",
                "caption": "(b)Baseline accuracy decreases slightly with scale as the model gains\nmore parameters for both Evo2 and Qwen3.",
                "position": 1608
            },
            {
                "img": "https://arxiv.org/html/2511.12797/figures/accuracy_regression_slope_se.png",
                "caption": "(c)ICL rate vs. model size: sharp gains up to  4B for Qwen3; mild boost\nfrom Evo2 1B to 7B; both plateau after 4–7B.",
                "position": 1615
            },
            {
                "img": "https://arxiv.org/html/2511.12797/figures/understandable_mistakes.png",
                "caption": "Figure 6:Qwen3-4B’s rate of understandable mistakes with respect to the number of shots. Despite starting at 16% in the one-shot regime, they fall to less than 1% by 8 shots and vanish entirely at 32 shots. This underscores how understandable mistakes are only a confound at very low shot counts and can essentially be ignored past 4 shots.",
                "position": 1699
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x4.png",
                "caption": "Figure 7:BitLoad vs Accuracy for all Evo models at all ICL shot-numbers. The per-BitLoad distribution of model performance at each shot level shows an uneven affinity of models for solving tasks at different BitLoads. However, all trends support our overall hypotheses.",
                "position": 1705
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x5.png",
                "caption": "Figure 8:BitLoad vs Accuracy for all Qwen models at all ICL shot-numbers.",
                "position": 1708
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x6.png",
                "caption": "Figure 9:BitDiversity vs Accuracy for all Evo models at all ICL shot-numbers. Model performance follows a more natural pattern of increasing performance with decreasing output entropy. This highlights that it is difficult for models to decipher ICL patterns for high entropy outputs.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x7.png",
                "caption": "Figure 10:BitDiversity vs Accuracy for all Qwen models at all ICL shot-numbers.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x8.png",
                "caption": "Figure 11:Enumerating cases of 0 BitDiversity. For our defined tasks, around 25% of the true targets have 0 BitDiversity (BD). With low-shots, models tend to produce a large number of 0-BD predictions. But this number decreases significantly with increasing shots and tends to match the actual prior value. Similarly, a majority of the baseline (1-shot) performance of models can be explained through 0-BD outputs. With a higher number of shots, the model starts learning higher entropy patterns and presents stronger evidence of ICL.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x9.png",
                "caption": "",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2511.12797/x10.png",
                "caption": "",
                "position": 1721
            }
        ]
    },
    {
        "header": "Appendix EAnalysis of ICL with Program Synthesis Tasks: Bit-Diversity",
        "images": []
    }
]