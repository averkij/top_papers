[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22054/x1.png",
                "caption": "",
                "position": 156
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22054/x2.png",
                "caption": "(a)Scaling trend.Larger training dataset yields consistently higher zero-shotδ1\\delta_{1}accuracy.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x2.png",
                "caption": "(a)Scaling trend.Larger training dataset yields consistently higher zero-shotδ1\\delta_{1}accuracy.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x3.png",
                "caption": "(b)Task performance.Radar plot over downstream tasks; Larger area indicates better performance.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22054/x4.png",
                "caption": "Figure 3:Percentile Depth Range Comparison from Seven Datasets (Real-world vs. Our Pseudo Labels).",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x5.png",
                "caption": "Figure 4:Skip-Connection in ViT-DPT Architecture.",
                "position": 374
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22054/x6.png",
                "caption": "Figure 5:Visualization of Depth SR and Completion.Our method better recovers missing regions with improved structure.",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x7.png",
                "caption": "Figure 6:Zero-shot Visual Comparisons on Challenging Test Samples.Our model robustly captures details of thin structures and in scenes with difficult lighting where competitors often fail.",
                "position": 1619
            },
            {
                "img": "https://arxiv.org/html/2601.22054/figs/supp_student_depthmap_vs.jpg",
                "caption": "Figure 7:Qualitative Comparison of Monocular Depth Estimation. Compared with MoGe2 and UniDepthv2, our distilled model produces more detailed and geometrically plausible predictions for both depth maps and point maps.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x8.png",
                "caption": "Figure 8:Qualitative Comparison of Point Maps.Thered arrowsindicate the GT distance, theyellow arrowsindicate the distance from predicted point map.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x9.png",
                "caption": "Figure 9:Qualitative Comparison of Depth Map. Compared with MoGe2 and UniDepthv2, our Student-PointMap model, which is finetuned from MoGe2 using pseudo-labels predicted by the our pretrained model, achieves more stable and accurate depth estimation.",
                "position": 1955
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x10.png",
                "caption": "Figure 10:Auxiliary monocular depth inputs improve performance of MapAnything. Thered arrowsindicate the GT distance, theyellow arrowsindicate the distance from 3D reconstruction.",
                "position": 2225
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x11.png",
                "caption": "Figure 11:Enhancing VLA Planning with Metric Anything.We distill the depth-perception capability of Metric Anything into the VLA model by supervising it to predict metric-aware depth tokens.",
                "position": 2244
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x12.png",
                "caption": "Figure 12:Enhancing 3D Spatial Reasoning with a Frozen ViT from Metric Anything .We evaluate our approach on the VIS Benchmark, covering video question-answering tasks like estimating object size, object’s distances, appearance order, route planning, and room size. Compared to mainstream large models, our method demonstrates robust and superior performance in 3D spatial understanding.",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x13.png",
                "caption": "Figure 13:Enhancing 3D Spatial Reasoning in MLLMs.We enhance VLM capabilities by employing the frozen, pretrained ViT from Metric Anything as the visual encoder, thereby preserving its strong spatial understanding during fine-tuning.",
                "position": 2655
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22054/x14.png",
                "caption": "Figure 14:Test-time Resolution Scaling. Qualitative results of depth estimation on an example image at1×1\\times,3×3\\times, and9×9\\timesthe base input resolution. Higher resolutions recover finer details.",
                "position": 2805
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x15.png",
                "caption": "Figure 15:Analysis of Our Proposed Distance-Balanced Loss.Left: The training loss for differentCCvalues. Right: The loss function curves. (Eq.5in the main manuscript ).",
                "position": 3128
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x16.png",
                "caption": "",
                "position": 3138
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x17.png",
                "caption": "Figure 16:Robustness in Night-Time Driving. We deployed a test vehicle to evaluate performance under low-light conditions. As scene brightness drops, visual signals deteriorate and object details fade. Despite this severe degradation, our model maintains remarkably robust.",
                "position": 3144
            }
        ]
    },
    {
        "header": "6Generalizability to Unseen Sensors, Scenarios, and Extreme Environmental Conditions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22054/x18.png",
                "caption": "Figure 17:Sensor Configuration for Real-World Generalization Evaluation.Our real-world test vehicle is equipped with three cameras (front, left-front, right-front) and a 128-beam solid-state LiDAR. Due to the LiDAR’s limited vertical field of view(pitch angle limitation), its captured point cloud does not fully cover the cameras’ combined frustums, leaving large image regions without metric depth cues.",
                "position": 3155
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x19.png",
                "caption": "Figure 18:Generalization to Real-World Sensor Configurations. We deployed a test vehicle to evaluate in-the-wild depth super-resolution and completion performance of our pre-trained model without any fine-tuning.",
                "position": 3159
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x20.png",
                "caption": "Figure 19:Robustness in Adverse Weather. In the real-world deployment, we used a test vehicle to evaluate our pre-trained model for depth super-resolution and completion in rainy and foggy weather conditions without fine-tuning. These adverse conditions significantly affect scene reflectance, causing the LiDAR to produce numerous artifacts or completely occlude critical objects. For example, the degraded data can lead to flat ground surfaces being misinterpreted as uneven or crucial obstacles like pillars being missed. However, our model robustly ignores these erroneous inputs and generates accurate depth predictions based on visual cues, thereby demonstrating the complementary strengths of the two sensing modalities.",
                "position": 3201
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x21.png",
                "caption": "Figure 20:Generalization to Unseen Visual Domains.Depth prediction results onfisheye images, an unseen domain characterized by severe radial distortion. The model was applied in a zero-shot setting without fine-tuning.",
                "position": 3204
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x22.png",
                "caption": "Figure 21:Generalization to Unseen Visual Domains.Depth prediction visualization for diverse in-the-wild images.",
                "position": 3207
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x23.png",
                "caption": "Figure 22:Generalization to Unseen Visual Domains.Visualizing depth predictions onpanoramic images, an unseen domain during training. Our model successfully handles such extreme distortion and novel viewpoints.",
                "position": 3210
            },
            {
                "img": "https://arxiv.org/html/2601.22054/x24.png",
                "caption": "Figure 23:Generalization to Unseen Visual Domains.Additional visualizations of depth predictions on diverse in-the-wild images.",
                "position": 3213
            }
        ]
    },
    {
        "header": "7Training Details",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]