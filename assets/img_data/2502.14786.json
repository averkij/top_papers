[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14786/x1.png",
                "caption": "Figure 1:SigLIP 2 adds the captioning-based pretraining from LocCa[62]as well as self-distillation and masked prediction from SILC[45]and TIPS[38](during the last 20% of training) to the sigmoid loss from SigLIP[71]. For some variants, the recipe additionally involves fine-tuning with data curation[61]or adaptation to native aspect ratio and variable sequence length[6,12].",
                "position": 274
            }
        ]
    },
    {
        "header": "2Training recipe",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14786/x2.png",
                "caption": "Figure 2:Per-language image-text retrieval performance for SigLIP, SigLIP 2 and mSigLIP on Crossmodal-3600[58]. SigLIP 2 almost matches the performance of mSigLIP (SigLIP trained on multilingual data) despite performing substantially better on English vision-language tasks (Table1).",
                "position": 1031
            },
            {
                "img": "https://arxiv.org/html/2502.14786/x3.png",
                "caption": "Figure 3:Comparing the NaFlex (a single checkpoint per model size supporting native aspect ratio and variable sequence length/resolution) and the standard square-input SigLIP 2 variants which use a separate checkpoint for each sequence length/resolution. The sequence lengths annotated on the x-axis correspond to training sequence lengths for NaFlex. NaFlex interpolates fairly well between training resolutions, but does not extrapolate well (not shown).",
                "position": 1052
            },
            {
                "img": "https://arxiv.org/html/2502.14786/x4.png",
                "caption": "Figure 4:Comparison of different vision encoders after training a Gemma 2 LLM for 50M steps with a frozen vision encoder (PaliGemma[7]stage 1), followed by fine-tuning the VLM on individual datasets (PaliGemma stage 3). SigLIP 2 performs better than SigLIP and AIMv2[20]for different model sizes and resolutions. Same data as in Table6.",
                "position": 1066
            }
        ]
    },
    {
        "header": "3Experiments and results",
        "images": []
    },
    {
        "header": "4Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14786/x5.png",
                "caption": "Figure 5:10-shot and 0-shot accuracy for geographically diverse object classification tasks (Dollar Street, GeoDE), as well as geolocalization (GeoDE country/region) and landmark localization (GLDv2) tasks. SigLIP 2 consistently performs better than SigLIP (see Table8for additional results).",
                "position": 1650
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14786/x6.png",
                "caption": "Figure 6:Representation bias (association of random objects with gender; lower is better) for different models.",
                "position": 1660
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFull PaliGemma results",
        "images": []
    },
    {
        "header": "Appendix BFull NaFlex results",
        "images": []
    },
    {
        "header": "Appendix CFull cultural diversity and fairness results",
        "images": []
    }
]