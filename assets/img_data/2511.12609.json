[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12609/figures/huggingface-logo.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.12609/figures/github-mark.png",
                "caption": "",
                "position": 175
            },
            {
                "img": "https://arxiv.org/html/2511.12609/figures/website-logo.png",
                "caption": "",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2511.12609/figures/abstract.png",
                "caption": "Figure 1:The performance of Uni-MoE-2.0-Omni and previous SOTA omnimodal large models.",
                "position": 187
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Uni-MoE-2.0-Omni",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12609/figures/model.png",
                "caption": "Figure 2:The Uni-MoE-2.0-Omni architecture processes multimodal data through a unified tokenization strategy. Audio is tokenized in 30-second clips, augmented with generation tokens for voice control in the Context-Aware MoE-TTS module, while images are encoded using a sliding window technique. Image Generation Tokens bridge the model to a Task-Aware Diffusion Transformer for end-to-end generation tasks. The model’s comprehension is powered by Omni-Modality 3D RoPE, which aligns inputs across time, and a dynamic-capacity MoE layer. This MoE layer dynamically routes information using diverse experts, with stability ensured by null experts (for token skipping) and modality-specific routed experts (A, V, T indicate audio, visual, and textual expert pretrained on corresponding data). In contrast, compact shared experts (only 1/8 size of routed experts) enable efficient cross-modal knowledge transfer.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2511.12609/figures/MoE_TTS_model.png",
                "caption": "Figure 3:The illustration of Context-Aware MoE-TTS. This figure uses different colored blocks to represent distinct token types, illustrating our long-context streaming decoding method. Furthermore, the Uni-MoE-TTS module will be released separately, featuring three unique and controllable voice styles.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2511.12609/figures/taskDiT.png",
                "caption": "Figure 4:The overview of the Task-aware Diffusion Transformer (Task-DiT). The role of the projection modules is to map external, task-conditioning features into the latent space of the Diffusion Transformer, where they are utilized as context in cross-attention blocks to guide the image generation.",
                "position": 764
            }
        ]
    },
    {
        "header": "3Training and Data Recipes",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12609/figures/training_stage.png",
                "caption": "Figure 5:The training recipe for adapting an LLM into an omnimodal large model.",
                "position": 829
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.12609/figures/cases_image_gen.png",
                "caption": "Figure 6:The cases of Image Generation, Image Edition, Controllable Generation, and Low-Level Image Restoration.",
                "position": 2942
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x1.png",
                "caption": "(a)Overall",
                "position": 2967
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x1.png",
                "caption": "(a)Overall",
                "position": 2970
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x2.png",
                "caption": "(b)Image Understanding",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x3.png",
                "caption": "(c)Video Understanding",
                "position": 2980
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x4.png",
                "caption": "(d)Audio Understanding",
                "position": 2985
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x5.png",
                "caption": "(e)Omni Understanding",
                "position": 2991
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x6.png",
                "caption": "(f)Image Generation.",
                "position": 2996
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x7.png",
                "caption": "(g)Image Edit",
                "position": 3001
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x8.png",
                "caption": "(h)Audio Generation.",
                "position": 3006
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x9.png",
                "caption": "(a)Overall",
                "position": 3013
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x9.png",
                "caption": "(a)Overall",
                "position": 3016
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x10.png",
                "caption": "(b)Image Understanding",
                "position": 3021
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x11.png",
                "caption": "(c)Video Understanding",
                "position": 3026
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x12.png",
                "caption": "(d)Audio Understanding",
                "position": 3031
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x13.png",
                "caption": "(e)Omni Understanding",
                "position": 3037
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x14.png",
                "caption": "(f)Image Generation",
                "position": 3042
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x15.png",
                "caption": "(g)Image Edit",
                "position": 3047
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x16.png",
                "caption": "(h)Audio Generation",
                "position": 3052
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x17.png",
                "caption": "(a)Layer0",
                "position": 3070
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x17.png",
                "caption": "(a)Layer0",
                "position": 3073
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x18.png",
                "caption": "(b)Layer9",
                "position": 3078
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x19.png",
                "caption": "(c)Layer18",
                "position": 3084
            },
            {
                "img": "https://arxiv.org/html/2511.12609/x20.png",
                "caption": "(d)Layer27",
                "position": 3089
            },
            {
                "img": "https://arxiv.org/html/2511.12609/figures/image_gen_thinking.png",
                "caption": "Figure 10:Comparison of image generation results with and without thinking guidance. The “w.o. Thinking” column shows images generated directly from prompts, while the “w. Thinking” column illustrates results produced after incorporating step-by-step reasoning. The middle column presents the structured thinking process guiding the model toward more accurate and contextually faithful image synthesis.",
                "position": 3183
            }
        ]
    },
    {
        "header": "5Discussion and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "8Contributors",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]