[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16615/x1.png",
                "caption": "",
                "position": 92
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Log-linear Sparse Attention Mechanism",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16615/x2.png",
                "caption": "Figure 2:Illustration of index reordering. The default raster indices do not effectively cluster similar pixels during 1D pooling, while using index ordering guarantees that similar pixels receive neighboring 1D indices.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2512.16615/x3.png",
                "caption": "Figure 3:Acceleration ratio of different attention methods compared to PyTorch Attention (FlashAttention2). We evaluate training and inference with block sizeBâˆˆ{16,64}B\\in\\{16,64\\}across varying sequence lengths on an H200 GPU.",
                "position": 806
            },
            {
                "img": "https://arxiv.org/html/2512.16615/x4.png",
                "caption": "Figure 4:The throughput of sparse key-value backward. Experiments are conducted on an H200 GPU using tokens with6464heads and head dimension6464. We setK=8K=8andB=16B=16for sparse Top-KKattention.",
                "position": 823
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "AImplementation Details",
        "images": []
    },
    {
        "header": "BAdditional Experiment Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16615/x5.png",
                "caption": "Figure 5:The FID curves of different training strategies. Compared to training from scratch, starting from a model pretrained on low-resolution data significantly reduces training cost.",
                "position": 1134
            },
            {
                "img": "https://arxiv.org/html/2512.16615/x6.png",
                "caption": "Figure 6:The FID and Inception Score curves of the first44epochs using VSA, SLA, and LLSA on PixelFlow ImageNet-256 benchmark.",
                "position": 1137
            },
            {
                "img": "https://arxiv.org/html/2512.16615/x7.png",
                "caption": "Figure 7:The qualitative results of pixel space DiT-S using LLSA trained on FFHQ-128, FFHQ-256, and FFHQ-512. For FFHQ-512, the model is only trained for two epochs. We believe that better quality can be obtained by longer training.",
                "position": 1153
            },
            {
                "img": "https://arxiv.org/html/2512.16615/x8.png",
                "caption": "Figure 8:The qualitative comparison of SLA, VSA, and LLSA trained on PixelFlow-L ImageNet-256. The reference images are generated by a well-trained full-attention PixelFlow model from the official repository.",
                "position": 1156
            }
        ]
    },
    {
        "header": "CQualitative Results",
        "images": []
    }
]