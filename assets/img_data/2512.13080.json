[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13080/x1.png",
                "caption": "Figure 1:Overview of the proposedVisual-Physical Alignmentframework.We start from diverse human demonstration videos to extract 3D visual annotations and 3D action annotations, capturing how humans interact with the physical world with corresponding visual observations. These annotations enable the two-stageSpatial-Aware VLA Pretraining, which teaches VLA models to ground 2D visual inputs in 3D spatial understanding: (1) 3D-Visual Pretraining: starting from a VLM backbone, human demonstration videos with 3D visual annotations are used to align 2D visual features with 3D spatial representations via a dual-encoder fusion module. (2) 3D-Action Pretraining: human hand trajectories provide 3D motion supervision, enabling the model to learn physically grounded action priors. Then in the third stage, the pretrained model,\\ModelName, is adapted to robot manipulation tasks, resulting in robust and generalizable policies in simulation and real-world settings.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13080/x2.png",
                "caption": "Figure 2:Overview of theHand3D-visual. By integrating point cloud estimation, object localization, and hand pose annotations from human manipulation videos, we bridge 2D visual observations with 3D physical action space to provide visual-physical aligment supervision for VLA models.",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2512.13080/x3.png",
                "caption": "Figure 3:Examples ofHand3D-visual.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2512.13080/x4.png",
                "caption": "Figure 4:Model architecture of\\ModelName. A dual-encoder including a semantic vision encoder and a 3D encoder produces fused spatial–semantic features through a cross-attention fusion layer. During pre-training, the vision tokens are aligned with text and motion tokens using 3D visual and 3D action annotations. During post-training, action queries interact with fused visual–language features to produce conditions, which is combined with the robot state and processed by a flow-matching action head to predict actions for robotic manipulation.",
                "position": 503
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13080/x5.png",
                "caption": "Figure 5:Real Robot Task Settings.",
                "position": 882
            },
            {
                "img": "https://arxiv.org/html/2512.13080/x6.png",
                "caption": "Figure 6:Qualitative examples of\\ModelNameperforming real robot tasks.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2512.13080/x7.png",
                "caption": "Figure 7:Failure examples of\\ModelNameand InternVL-3.5 performing real robot tasks.",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2512.13080/x8.png",
                "caption": "Figure 8:Comparison between\\ModelName-PT and InternVL3.5 onHand3D-test.\nLeft: histogram of direction scores;\nRight: histogram of distance errors",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2512.13080/x9.png",
                "caption": "Figure 9:Visualization of the predicted motion trajectories from\\ModelNameafter second stage pretraining (blue lines) and the ground-truth trajectories (red lines).",
                "position": 1112
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]