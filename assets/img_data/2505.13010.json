[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13010/extracted/6453653/biasedSent.png",
                "caption": "Figure 1:Bias detection and Classification scheme.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13010/extracted/6453653/bias_detector_architecture.png",
                "caption": "Figure 2:Architecture of the Bias Detector Model. The output vector of the CLS token serves as input to the classifier head",
                "position": 170
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13010/extracted/6453653/kfoldval.png",
                "caption": "Figure 3:5-Fold cross validation results on both models",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2505.13010/extracted/6453653/Attention2FN.png",
                "caption": "Figure 4:Attention weights heatmap for False Negatives",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2505.13010/extracted/6453653/AttentionFP.png",
                "caption": "Figure 5:Attention weights heatmap for False Positives",
                "position": 424
            }
        ]
    },
    {
        "header": "6Limitations and Future Works",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]