[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22434/x1.png",
                "caption": "Figure 1:Challenges for multi-image understanding.While recent works support multiple images as input, most of them focus on scenarios where each image can be interpreted independently (e.g., Example 1), which remains relatively easy for current state-of-the-art VLMs.\nHowever, many real-world tasks (e.g., Example 2-4) require models to compare subtle visual differences, align visual cues across images, and reason about object correspondences—capabilities that current VLMs still struggle with.\nWe gather Example 1 from MuirBench[32], Example 3 from VLM2-bench[40], Example 2,4 from real world samples.",
                "position": 141
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22434/x2.png",
                "caption": "Figure 2:Demonstrations for contrastive samples.The first row shows two triplets from the video, and the second row demonstrates samples from image editing datasets.\nThese samples are visually similar but contain subtle differences (marked with red circles), on which we apply random cropping and resizing.\nIn each triplet, the first two images are the same, and the third image is different.",
                "position": 206
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22434/x3.png",
                "caption": "Figure 3:Demonstrations for visual reasoning.Given a question,MiCofirst examines the details of each image to identify answer-related visual cues, and then performs cross-image comparisons to derive the final answer.\nThe reasoning processes are marked in gray, with key contents underlined.",
                "position": 1197
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Results",
        "images": []
    },
    {
        "header": "Appendix CQualitative Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22434/x4.png",
                "caption": "Figure 4:Demonstrationsfor detailed comparison and jigsaw solving.",
                "position": 2167
            },
            {
                "img": "https://arxiv.org/html/2506.22434/x5.png",
                "caption": "Figure 5:Demonstrationsfor IQ test, functional correspondence, and visual similarity.",
                "position": 2170
            }
        ]
    },
    {
        "header": "Appendix DPotential Social Impact",
        "images": []
    }
]