[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16921/x1.png",
                "caption": "",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16921/x2.png",
                "caption": "(a)Iberian Peninsula languages",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x2.png",
                "caption": "(a)Iberian Peninsula languages",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x3.png",
                "caption": "(b)Spanish varieties in Ibero-America",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Iberbench",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16921/extracted/6363510/images/iberbench-diagram.png",
                "caption": "Figure 2:IberBench overview. Users can view rankings, plots, and reports; request LLMs for evaluation through the UI; and propose new datasets to the organization. The organization reviews these proposals for possible inclusion in the leaderboard. Once approved, datasets and models are prepared, evaluated, and hosted to be displayed in the UI.",
                "position": 273
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16921/extracted/6363510/emojis/chat.png",
                "caption": "Table 5:LLMs evaluated in IberBench. Includes the model name, type, number of parameters in billions, and Iberian languages included in the pre-training and fine-tuning mixtures.indicates base models solely pre-trained via causal language modeling;marks models further fine-tuned for instruction-following or chat tasks;denotes cases where training details are unspecified in the source. Fine-tuning languages for base models are marked as “-”, since no fine-tuning has been performed on these models. All LLMs can be accessed in theHuggingFace Hubsearching by model name.",
                "position": 1741
            },
            {
                "img": "https://arxiv.org/html/2504.16921/extracted/6363510/emojis/question.png",
                "caption": "",
                "position": 1761
            },
            {
                "img": "https://arxiv.org/html/2504.16921/extracted/6363510/emojis/base.png",
                "caption": "",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x4.png",
                "caption": "Figure 3:Performance per model size (number of parameters), averaged across all the languages and tasks. Legend shows model families, e.g., the Llama-3.2 family includesLlama-3.2-1B-InstructandLlama-3.2-3b-Instruct, which are plotted with the same marker and color. The random baseline is shown as horizontal line.",
                "position": 1985
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x5.png",
                "caption": "Figure 4:Averaged performance per model type.",
                "position": 1988
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x6.png",
                "caption": "Figure 5:Performance per task category across all models and languages. The random baseline for each category is marked with a black cross.",
                "position": 2040
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x7.png",
                "caption": "Figure 6:Performance in fundamental and industry-relevant tasks, averaged across all the languages and models.",
                "position": 2043
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x8.png",
                "caption": "Figure 7:Performance per language and Spanish variety averaged across LLMs and tasks. For clarity, we remove the “ambiguous” Spanish variety.",
                "position": 2097
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x9.png",
                "caption": "Figure 8:LLM performances in Iberian languages, averaged across tasks. Vertical lines denote the random baseline.",
                "position": 2100
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x10.png",
                "caption": "Figure 9:Model performance in Spanish varieties. Vertical lines denote the random baseline.",
                "position": 2116
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations and Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets and Sources",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16921/x11.png",
                "caption": "Figure 10:Ranking of models per task category.",
                "position": 3571
            },
            {
                "img": "https://arxiv.org/html/2504.16921/x12.png",
                "caption": "Figure 11:Ranking of models per Iberian language.",
                "position": 3574
            }
        ]
    },
    {
        "header": "Appendix BModel Rankings",
        "images": []
    }
]