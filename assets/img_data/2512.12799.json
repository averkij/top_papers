[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12799/x1.png",
                "caption": "Figure 1:(a) presents the pipeline of mainstream vision-action (VA) models for end-to-end autonomous driving. (b) illustrates mainstream Vision-Language-Action (VLA) models. (c) shows our DrivePI, which combines coarse-grained linguistic understanding with fine-grained 3D perception and prediction, inheriting advantages both existing VA models and VLA models.",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2512.12799/sec/figures/framework.png",
                "caption": "Figure 2:The pipeline of DrivePI consists of the following steps. First, we employ a vision encoder to extract features from images and LiDAR data, obtaining latent BEV features that are then converted into vision tokens by a spatial projector. Next, we feed both vision tokens and text tokens into the MLLM to generate output tokens. The MLLM produces responses through four specialized heads: a text head for scene understanding in an auto-regressive manner, a 3D occupancy head for accurate spatial perception, an occupancy flow head for pixel-level motion prediction, and an action diffusion head for trajectory planning.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12799/x2.png",
                "caption": "Figure 3:The illustration of our multi-stage data pipeline. We first generate captions of front and back views, respectively. Then, we use InternVL3-78B (adopts Qwen2.5-72B[bai2025qwen2]as the language model) to combine these captions to merge and polish generated scene descriptions. Moreover, we generate text-occupancy and text-flow QA pairs based on occupancy and flow ground truth by multi-turn conversations to improve the 4D spatial understanding ability. Finally, we generate text-planning QA pairs to allow MLLM to predict the future actions of ego-vehicle.",
                "position": 163
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12799/x3.png",
                "caption": "Figure 4:The visualization of coarse-grained and fine-grained understanding of DrivePI. We present the results of scene description, 3D occupancy, trajectory prediction results to illustrate the coarse-grained understanding of DrivePI.",
                "position": 1053
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6More Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12799/sup_figures/cap1.png",
                "caption": "Table 10:An example generated by our multi-stage data engine.",
                "position": 1419
            }
        ]
    },
    {
        "header": "7Details of Coarse-grained Spatial Understanding",
        "images": []
    },
    {
        "header": "8Details of Fine-grained Spatial Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12799/sup_figures/cam_fine_plan_1.png",
                "caption": "Table 11:The visualization of remaining stationary with coarse-grained and fine-grained results.",
                "position": 1606
            },
            {
                "img": "https://arxiv.org/html/2512.12799/x4.png",
                "caption": "",
                "position": 1744
            },
            {
                "img": "https://arxiv.org/html/2512.12799/sup_figures/cam_fine_plan_2.png",
                "caption": "Table 12:The visualization of going straight with coarse-grained and fine-grained results.",
                "position": 1756
            },
            {
                "img": "https://arxiv.org/html/2512.12799/x5.png",
                "caption": "",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2512.12799/sup_figures/cam_fine_plan_3.png",
                "caption": "Table 13:The visualization of turning with coarse-grained and fine-grained results.",
                "position": 1906
            },
            {
                "img": "https://arxiv.org/html/2512.12799/x6.png",
                "caption": "",
                "position": 2044
            }
        ]
    },
    {
        "header": "9More Visualization",
        "images": []
    }
]