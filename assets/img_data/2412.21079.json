[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21079/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21079/x2.png",
                "caption": "Figure 2:Comparisonsof the implicit and our explicit correspondence prediction for the images in the wild.\nThe implicit correspondence from cross-image attention calculation is less accurate and unstable with the change of denoising steps and network layers.",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x3.png",
                "caption": "Figure 3:FrameworkofEdicho.\nTo achieve consistent editing, we first predict the explicit correspondence with extractors for the input images.\nThe pre-computed correspondence is injected into the pre-trained diffusion models and guide the denoising in the two levels of (a) attention features and (b) noisy latents in CFG.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x4.png",
                "caption": "Figure 4:Qualitative comparisons on local editing with Adobe Firefly (AF)[45], Anydoor (AD)[12], and Paint-by-Example (PBE)[58].\nThe inpainted areas of the inputs are highlighted in red.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x5.png",
                "caption": "Figure 5:Qualitative comparisons on global editing with MasaCtrl (MC)[7], StyleAligned (SA)[18], and Cross-Image-Attention (CIA)[1].",
                "position": 366
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21079/x6.png",
                "caption": "Figure 6:Ablation studies on the (a) correspondence-guided attention manipulation (Corr-Attention) and (b) correspondence-guided CFG (Corr-CFG).",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x7.png",
                "caption": "Figure 7:With outputs from our consistent editing method (upper) and the customization[48]techniques,\ncustomized generation (lower) could be achieved by injecting the edited concepts into the generative model.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x8.png",
                "caption": "Figure 8:We adopt the neural regressor Dust3R[55]for 3D reconstruction based on the edits by matching the 2D points in a 3D space.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x9.png",
                "caption": "Figure 9:Diverse results of consistent image inpainting (a) and translation (b) by the proposed method.\nEditing results for an image set of three images are demonstrated in (c).",
                "position": 542
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21079/x10.png",
                "caption": "Figure S1:Additional ablations on the correspondence-guided attention (upper) and CFG (lower).",
                "position": 1457
            }
        ]
    },
    {
        "header": "Appendix DAdditional Correspondence Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21079/x11.png",
                "caption": "Figure S2:Additional correspondence prediction comparisons.\nThe numbers behind “Implicit” respectively indicate the network layer and denoising step for correspondence prediction.",
                "position": 1472
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x12.png",
                "caption": "Figure S3:Additional correspondence prediction results with attention visualization. Regions with the highest attention weights are outlined with dashed circles.",
                "position": 1486
            }
        ]
    },
    {
        "header": "Appendix EUser Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.21079/x13.png",
                "caption": "Figure S4:User study results of consistent local editing (left) and global editing (right).",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2412.21079/x14.png",
                "caption": "Figure S5:Additional qualitative results of the proposed method for local (upper three) and global editing (lower three ones).\nThe inpainted regions for local editing are indicated with the\nlight red color.\n“Fixed Seed” indicates editing results from the same random seed (the same initial noise).",
                "position": 1520
            }
        ]
    },
    {
        "header": "Appendix FAdditional Results",
        "images": []
    }
]