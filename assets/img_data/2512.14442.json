[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14442/x1.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14442/x2.png",
                "caption": "Figure 2:Vision Foundation Models are good at fine-grained grounding, but are poor at reasoning. Vision Language Models are good at reasoning, but are poor at visual grounding. Some works finetuned VLMs for better grounding ability, but both abilities are underwhelming.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x3.png",
                "caption": "Figure 3:The pipeline of our A4-Agentframework, which decouples affordance prediction into three stages.(1) Dreamer: Imagines the interaction by generating a simulated image.(2) Thinker: Reasons over the original and simulated images to produce a textual description of the actionable object part.(3) Spotter: Takes this description to locate the part with bounding boxes and keypoints, then refines them into a precise segmentation mask.",
                "position": 176
            }
        ]
    },
    {
        "header": "3Motivation",
        "images": []
    },
    {
        "header": "4A4-Agent:Agentic Affordance Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14442/x4.png",
                "caption": "Figure 4:Qualitative comparison on ReasonAff dataset. Our method continuously predicts appropriate components according to task requirements, achieving results most consistent with ground truth and even surpassing Affordance-R1 specifically trained on this dataset.",
                "position": 235
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14442/x5.png",
                "caption": "Figure 5:Qualitative comparison on RAGNet dataset. Our zero-shot method effectively reasons over task instructions to identify correct regions and precisely localize them with masks, closely matching ground truth. This outperforms baseline methods including AffordanceVLM trained on this dataset.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x6.png",
                "caption": "Figure 6:Qualitative results on open-world images. A4-AgentÂ demonstrates robust affordance reasoning across diverse scenarios, consistently produces reasonable regions based on complex instructions.",
                "position": 664
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7More Implementation Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14442/x7.png",
                "caption": "Figure 7:Illustration of different categories of baseline methods.",
                "position": 1157
            }
        ]
    },
    {
        "header": "8More Exploratory Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.14442/x8.png",
                "caption": "Figure 8:Full Demonstration of Intermediate Results.Sampled from results on the ReasonAff dataset.",
                "position": 1360
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x9.png",
                "caption": "Figure 9:Full Demonstration of Intermediate Results.Sampled from results on the ReasonAff dataset.",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x10.png",
                "caption": "Figure 10:Full Demonstration of Intermediate Results.Sampled from results on the UMD dataset.",
                "position": 1366
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x11.png",
                "caption": "Figure 11:Full Demonstration of Intermediate Results.Sampled from results on the UMD dataset.",
                "position": 1369
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x12.png",
                "caption": "Figure 12:Full Demonstration of Intermediate Results.Sampled from results on the RAGNet dataset.",
                "position": 1372
            },
            {
                "img": "https://arxiv.org/html/2512.14442/x13.png",
                "caption": "Figure 13:Full Demonstration of Intermediate Results.Sampled from results on the RAGNet dataset.",
                "position": 1375
            }
        ]
    },
    {
        "header": "9More Intermediate Results",
        "images": []
    }
]