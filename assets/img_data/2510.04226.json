[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04226/x1.png",
                "caption": "Figure 1:In this work, we measure epistemic diversity – via variability in claims about the world – for characterizing knowledge collapse in LLMs.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Problem Setup and Notation",
        "images": []
    },
    {
        "header": "4Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04226/x2.png",
                "caption": "Figure 2:Histograms of the top ten clusters for four topics after generating text, decomposing, and clustering decomposed claims across all models in our study. The frequency of claims in each cluster,xix_{i}, is represented by the colored bars. By the 10th cluster,xix_{i}is halved for all four topics, indicating a large decay rate forxix_{i}. The top clusters for each topic convey broad and general information for each topic.",
                "position": 385
            }
        ]
    },
    {
        "header": "5Measuring Epistemic Diversity",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04226/x3.png",
                "caption": "Figure 3:Information diversity\nvs. model release date. Each point is a single model, with lines connecting models of approximately the same size across released versions. Error bars are 95% boostrapped confidence intervals based on the HSD of each topic (N=155). Absolute diversity is low for all models compared to a very modest search baseline (top 20 Google search results for each topic). However, for all families except Qwen and most sizes, we see a trend of improved diversity.",
                "position": 439
            }
        ]
    },
    {
        "header": "6Empirical Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04226/x4.png",
                "caption": "Figure 4:Heatmap of the Jensen-Shannon divergence (JSD) across models, based on the empirical probability distributions over clusters (pip_{i}) for each topic. A higher JSD means that the distributions generated by the two models are more different. Open-weight models tend to be more similar to each other than to GPT. All LLMs are more different from the search baseline than to each other, indicating a marked difference in the distribution of information in the search baseline from the LLMs.",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2510.04226/x5.png",
                "caption": "Figure 5:Average diversity per country across all models with bootstrapped 95% confidence intervals. Bars are sorted according to the difference between RAG and IFT diversity. Countries tend to have similar diversity to each other with instruction fine-tuning only. However, RAG appears to have an uneven impact on different countries, where the US and general topics see the most benefit",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2510.04226/x6.png",
                "caption": "Figure 6:Comparison of the diversity of claims generated by the models matched to English Wikipedia and local language Wikipedia. To capture minimal representativenessPeterson (2025), we match claims from Wikipedia in English and the local language, respectively, to claims generated by our models in the IFT setting, and restrict the measurement of HSD to only matched claims. Bars are sorted according to the difference between English and local language representativeness.",
                "position": 619
            }
        ]
    },
    {
        "header": "7Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AReplication Details",
        "images": []
    }
]