[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07962/x1.png",
                "caption": "Figure 1:Efficiency and performance comparison betweenSFTandLightReasoner. LightReasoner achieves competitive or superior accuracy while substantially reducing resource consumption.",
                "position": 152
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x2.png",
                "caption": "",
                "position": 162
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x3.png",
                "caption": "",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x4.png",
                "caption": "",
                "position": 174
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07962/x5.png",
                "caption": "Figure 2:Most tokens show minimal KL divergence, with only few exhibiting elevated values.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x5.png",
                "caption": "Figure 2:Most tokens show minimal KL divergence, with only few exhibiting elevated values.",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x6.png",
                "caption": "Figure 3:Predictable tokens yield near-zero KL divergence, while critical steps trigger notable spikes.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x7.png",
                "caption": "Figure 4:Overview of the LightReasoner framework.Sampling Stage: Expert and Amateur models generate distributionsπE\\pi_{E}andπA\\pi_{A}. Informative step selection retains steps withDKL​(πE∥πA)>βD_{\\text{KL}}(\\pi_{E}\\parallel\\pi_{A})>\\beta, and contrastive supervision constructs soft labelsvCv_{C}capturing the Expert’s advantage through Expert-Amateur contrast.Fine-tuning Stage: The Expert model is enhanced by minimizing the KLD between its output andvCv_{C}.",
                "position": 286
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07962/x8.png",
                "caption": "Table 2:Efficiency comparison between SFT and LightReasoneracross total time, sampled problems, tuned tokens, and average accuracy improvement over 7 benchmarks.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x8.png",
                "caption": "Figure 5:LightReasonerconsistently improves zero-shot pass@1 accuracy across 7 mathematical evaluation benchmarks for baseline models.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x9.png",
                "caption": "",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x10.png",
                "caption": "Figure 6:Expert-Amateur Pairing Effects.Each point represents a fixed expert model paired with an amateur model. The performance gains achieved by LightReasoner decrease as the expertise gap closes.",
                "position": 1271
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x10.png",
                "caption": "Figure 6:Expert-Amateur Pairing Effects.Each point represents a fixed expert model paired with an amateur model. The performance gains achieved by LightReasoner decrease as the expertise gap closes.",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x11.png",
                "caption": "Figure 7:Impact of Ablation.Removing key components from LightReasoner consistently degrades performance, emphasizing their critical roles.",
                "position": 1279
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BFrom KL Divergence to Contrast Score",
        "images": []
    },
    {
        "header": "Appendix CConnection between Selection, Contrast, and Training",
        "images": []
    },
    {
        "header": "Appendix DRelation to Reinforcement Learning",
        "images": []
    },
    {
        "header": "Appendix EEntropy Dynamics",
        "images": []
    },
    {
        "header": "Appendix FSupplementary Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.07962/x12.png",
                "caption": "Table 7:LoRA configurationused in LightReasoner, shared across all models.",
                "position": 2313
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x12.png",
                "caption": "Figure 8:Perplexity convergence.PPL curves show training stabilizes around 1000 steps, supporting our choice of tuning horizon.",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x13.png",
                "caption": "Table 8:LoRA adapter configurationused for SFT, applied across all models.",
                "position": 2392
            },
            {
                "img": "https://arxiv.org/html/2510.07962/x13.png",
                "caption": "Figure 9:SFT training loss.Curve lengths vary with the number of correct demonstrations, but all runs reach convergence.",
                "position": 2438
            }
        ]
    },
    {
        "header": "Appendix GCase Study",
        "images": []
    }
]