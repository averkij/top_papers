[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06520/x1.png",
                "caption": "",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06520/x2.png",
                "caption": "Figure 2:Illustration of our RL training process. In this case, the model generates three samples by itself, calculates the rewards, and optimizes towards samples that achieve higher rewards.",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2503.06520/x3.png",
                "caption": "Figure 3:Seg-Zero includes a reasoning model and a segmentation model. The reasoning model is a MLLM that generates a reasoning chain and provides segmentation prompts. Subsequently the segmentation model produces pixel-wise mask.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06520/x4.png",
                "caption": "Figure 4:User prompt for Seg-Zero. ‚Äò{Question}‚Äô is replaced with object descriptionùêìùêì\\mathbf{T}bold_Tin the training and inference.",
                "position": 223
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06520/x5.png",
                "caption": "Figure 5:Visual QA task comparison. SFT suffers catastrophic forgetting, while RL preserves general Visual QA ability.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2503.06520/x6.png",
                "caption": "Figure 6:Qualitative Results on ReasonSeg[17]. The reason chain helps analyze user instructions and segment the correct objects.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2503.06520/x7.png",
                "caption": "Figure 7:Changes in completion length during training. Larger scale model tends to generate longer response.",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2503.06520/x8.png",
                "caption": "Figure 8:Changes in rewards during training. We show the mean value across a batch.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2503.06520/x9.png",
                "caption": "Figure 9:More examples.",
                "position": 692
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]