[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21244/x1.png",
                "caption": "Figure 1:An example ofinterference token purification: Removing a few interference tokens corrects the reasoning rollout and turns it into a successful one.",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2601.21244/x2.png",
                "caption": "Figure 2:(a) Zero-Reward Prompt Analysis:Comparison of the zero-reward prompt ratio across different models and rollout sizes (nn).Lenssignificantly reduces the proportion of zero-reward samples compared to GRPO, enhancing training efficiency.(b) Distribution of token-level Interference Scores(log scale): Only a few tokens exhibit high interference.(c) Rollout Accuracy Improvement:Removing these interference tokens leads to an improvement in rollout success rates (Average@8).",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2601.21244/x3.png",
                "caption": "Figure 3:Method Overview.In the first stage,Lensidentifies and purifies interference tokens within low-success prompts viaInterference Score(Defined in Section2.1), thereby generating a higher proportion of successful rollouts. In the second stage,Lensuses successful rollouts from the denoised prompts as high-reward supervision to calibrate policy optimization on the original prompt, correcting gradient updates distorted by interference.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Lens: Less Noise Sampling Framework",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21244/x4.png",
                "caption": "Figure 4:Learning curves ofLensand GRPO across model scales and task difficulties.We compare Qwen3-4B/8B-Base backbones on MATH-500 (Medium) and OlympiadBench (High).Lensconverges faster and achieves comparable or higher final accuracy than GRPO under the same training step, indicating more efficient optimization.",
                "position": 1545
            }
        ]
    },
    {
        "header": "4Further Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21244/x5.png",
                "caption": "Figure 5:Sampling accuracy distribution across three training phases.We compare the sampling distributions of GRPO, GRPOextended{}_{\\text{extended}}andLensacross the early, middle and late training stages.",
                "position": 1604
            },
            {
                "img": "https://arxiv.org/html/2601.21244/x6.png",
                "caption": "Figure 6:Training efficiency comparison ofLensand GRPO on MATH-500 and OlympiadBench.The gray dashed lines indicate the number of training steps required for both methods to reach the highest average accuracy of the\nbaseline during the entire training period.Lensdemonstrates superior sample efficiency and faster convergence.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2601.21244/x7.png",
                "caption": "Figure 7:Performance convergence ofLenson MATH-500 with Qwen2.5-3B/7B.Lensmatches or exceeds GRPO throughout training. Insets highlight that the optimal threshold depends on model capacity: the weaker 3B model requires a higher threshold, while the stronger 7B model achieves optimal results with a lower threshold.",
                "position": 1632
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Training Settings",
        "images": []
    },
    {
        "header": "Appendix BPerformance on Various Models",
        "images": []
    },
    {
        "header": "Appendix CSuccess Rate Sensitivity Analysis",
        "images": []
    },
    {
        "header": "Appendix DComputational Overhead",
        "images": []
    },
    {
        "header": "Appendix EPruning Strategies Analysis",
        "images": []
    },
    {
        "header": "Appendix FTraining Dynamics Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21244/x8.png",
                "caption": "Figure 8:Training dynamics across different model scales.Each row reports average accuracy, entropy, and response length during training for Qwen3-4B-Base (top) and Qwen3-8B-Base (bottom). Compared with GRPO and GRPOextended{}_{\\text{extended}},Lensexhibits more consistent and stable trends.",
                "position": 3399
            }
        ]
    },
    {
        "header": "Appendix GChecklist",
        "images": []
    }
]