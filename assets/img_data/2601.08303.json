[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08303/x1.png",
                "caption": "",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08303/x2.png",
                "caption": "Figure 2:Efficient DiT Overview.Left: Our model consists of three stages:Down, Middle and Up. Down and Up blocks operate on high-resolution latent while using our novel Adaptive Sparse Self-Attention (ASSA) layers. Middle blocks operate at latents downsampled by2×22\\times 2window and use standard Self-Attention (SA) layers. Other layers in the blocks are Cross-Attention (CA) for modulating with input text conditioning and Feed-Forward (FFN) layer.Right: We delve deeper into our ASSA layer. It consists of two parallel attention processing branches: (i) coarse-grained key-value compression for overall structure, and (ii) fine-grained blockwise neighborhood attention features. Finally, the layers to weight these two features are adaptively per head through an input-dependent weighting function.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x3.png",
                "caption": "Figure 3:Efficient DiT Ablations.We plot the performance (validation loss) and model footprint (parameters & latency on iPhone 16 Pro Max) for various stages in our ablations. Using a baseline DiT yields extremely high latency. Our multi-stage design with ASSA layers and additional enhancements results in an Efficient DiT with comparable latency and better performance than the state-of-the-art on-device model SnapGen[28].",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x4.png",
                "caption": "Figure 4:Illustration of Blockwise Neighborhood Attention.(a)Naive Neighborhood Attention, where each query attends to its local window of 3 neighboring tokens.(b)Corresponding self-attention mask showing the limited receptive field for each query.(c)Blockwise Neighborhood Attention extends this concept by grouping tokens into 8 local blocks, enabling efficient attention computation while preserving locality.",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x5.png",
                "caption": "Figure 5:Elastic Training Framework.Given a supernetwork, we define sub-networks as different granularities of the hidden dimension.\nDuring training, we sample sub-networks uniformly and supervise them using the output from the supernetwork. In addition, we use standard diffusion loss on all granularities. This leads to more stable training and imparts knowledge to sub-networks.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x6.png",
                "caption": "Figure 6:Knowledge-guided Distribution Matching Distillation (K-DMD).Our step distillation method combines distribution matching with knowledge transfer\nfrom a few-step teacher.",
                "position": 467
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08303/x7.png",
                "caption": "Figure 7:Human Evaluation.We conduct a user study comparing our small (0.4B) and full (1.6B) variants with three baselines—SANA (1.6B), SD3-Medium (2B), and Flux.1-dev (12B)—across three key attributes: realism, visual fidelity, and text–image alignment.",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x8.png",
                "caption": "Figure 8:Few-step Generation.Comparison of images produced by the tiny (0.3B), small (0.4B), and full (1.6B) models under 28-step (w/o K-DMD) and 4-step (w/ K-DMD) settings. Numbers in the corners denote DPG / GenEval scores.",
                "position": 782
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "ADiscussion of On-Device Latency",
        "images": []
    },
    {
        "header": "BDemo on Mobile Device",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08303/x9.png",
                "caption": "Figure 1:On-device Image Generation Demo.Screenshots from our on-device application running on an iPhone 16 Pro Max.\nThe left panel shows results from the small (0.4B) model, and the right panel shows results from the full variant with 4-bit quantization.",
                "position": 1996
            }
        ]
    },
    {
        "header": "COn-device Deployment Details",
        "images": []
    },
    {
        "header": "DAdditional Illustration of Blockwise Neighborhood Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08303/x10.png",
                "caption": "Figure 2:Illustration of Blockwise Neighborhood Attention (BNA).Visualization of BNA under different hyperparameter settings of block number (bb) and neighborhood radius (rr), showing the corresponding spatial neighbor coverage and attention sparsity.",
                "position": 2023
            }
        ]
    },
    {
        "header": "EDetailed Results on T2I Benchmarks",
        "images": []
    },
    {
        "header": "FQualitative Comparison on ImageNet",
        "images": []
    },
    {
        "header": "GAdditional Qualitative Comparison on T2I",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08303/x11.png",
                "caption": "Figure 3:Qualitative comparison on ImageNet-1K.Visual comparison between on-device models SnapGen[28](0.4B, left in each pair, validation loss = 0.5131) and our small model (0.4B, right in each pair, validation loss = 0.5090).\nOur model produces sharper textures, more consistent colors, and improved structural fidelity across diverse categories.",
                "position": 2675
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x12.png",
                "caption": "Figure 4:Additional Qualitative Comparison.Our models demonstrate competitive visual quality and superior prompt-following ability. Input text prompts are shown above each image grid; all images are generated at102421024^{2}resolution. Zoom in for details.",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2601.08303/x13.png",
                "caption": "Figure 5:Additional Qualitative Comparison.Our models demonstrate competitive visual quality and superior prompt-following ability. Input text prompts are shown above each image grid; all images are generated at102421024^{2}resolution. Zoom in for details.",
                "position": 2683
            }
        ]
    },
    {
        "header": "HTraining Implementation Details",
        "images": []
    }
]