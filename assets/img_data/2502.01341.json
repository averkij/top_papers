[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01341/x1.png",
                "caption": "",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01341/x2.png",
                "caption": "Figure 1:Performance of Different VLM Connectors.The proposedAlignconnector outperforms other methods across benchmarks using the same training configuration. Radial distance is proportion of maximal score, truncated at0.70.70.70.7(black dot).",
                "position": 185
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01341/x3.png",
                "caption": "Figure 2:AlignVLMModel Architecture.The vision encoder extracts image features, which are processed to produce probabilities over the LLM embeddings. A weighted average combines these probabilities with embeddings to generate vision input vectors. Text inputs are tokenized, and the corresponding embeddings are selected from the embedding matrix, which is then used as input to the LLM. We display the vision layers inblue, and the text layers inpurple.",
                "position": 248
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/prob_dist_image.png",
                "caption": "Figure 3:Probability distribution over the LLM text tokens, showing dense probabilities and higher values for tokens associated with white space in document images.",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2502.01341/x4.png",
                "caption": "Figure 4:Comparison of Llama-3.2-3b-Alignand Llama-3.2-3B-MLP\non the Easy and Hard VCR tasks.",
                "position": 1040
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/vcr_example1.png",
                "caption": "(a)Positive Example 1",
                "position": 1046
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/vcr_example1.png",
                "caption": "(a)Positive Example 1",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/vcr_example2.png",
                "caption": "(b)Positive Example 2",
                "position": 1086
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/vcr_example3.png",
                "caption": "(c)Negative Example 1",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/vcr_example4.png",
                "caption": "(d)Negative Example 2",
                "position": 1160
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01341/x5.png",
                "caption": "Figure 6:Mapping Visual-to-Text tokens.The left column shows the visual input to the model. In contrast, the right column visualizes the decoded tokens on a 14Ã—14 grid, displaying the top k=2 tokens corresponding to the most likely LLM tokens predicted for the respective visual feature in each cell.",
                "position": 2128
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_1.jpg",
                "caption": "(a)Positive Example #1",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_1.jpg",
                "caption": "(a)Positive Example #1",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_2.jpg",
                "caption": "(b)Positive Example #2",
                "position": 2207
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_3.jpg",
                "caption": "(c)Negative Example #1",
                "position": 2260
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_4.jpg",
                "caption": "(d)Negative Example #2",
                "position": 2314
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_5.jpg",
                "caption": "(a)Positive Example #1",
                "position": 2370
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_5.jpg",
                "caption": "(a)Positive Example #1",
                "position": 2386
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_6.jpg",
                "caption": "(b)Positive Example #2",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_7.jpg",
                "caption": "(c)Negative Example #1",
                "position": 2493
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_8.jpg",
                "caption": "(d)Negative Example #2",
                "position": 2546
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_9.jpg",
                "caption": "(a)Positive Example #1",
                "position": 2602
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_9.jpg",
                "caption": "(a)Positive Example #1",
                "position": 2618
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_10.jpg",
                "caption": "(b)Positive Example #2",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_11.jpg",
                "caption": "(c)Negative Example #1",
                "position": 2725
            },
            {
                "img": "https://arxiv.org/html/2502.01341/extracted/6174686/figures/case_12.jpg",
                "caption": "(d)Negative Example #2",
                "position": 2778
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]