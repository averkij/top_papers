[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06006/x1.png",
                "caption": "Figure 1:Illustration of our two strategies to bootstrap a world model from a dynamics model in Vision-Language Models: (i) synthesising trajectories for weak supervision (left) and (ii) inference-time verification of candidate observations (right).",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x2.png",
                "caption": "Figure 2:Comparison of predicted negative log-likelihoods (lower values indicate stronger model preference) for ground-truth real-world trajectories versus four types of negative trajectories.Top: Action prediction task for the dynamics model (observation√ó\\times√óobservation‚Üí‚Üí\\rightarrow‚Üíaction).Bottom: Next observation prediction task for the world model (observation√ó\\times√óaction‚Üí‚Üí\\rightarrow‚Üíobservation).\nThe legend shows the percentage of times the model prefers the ground-truth trajectory (‚Üë‚Üë\\uparrow‚Üë) over the negatives (‚Üì‚Üì\\downarrow‚Üì).",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x3.png",
                "caption": "",
                "position": 173
            }
        ]
    },
    {
        "header": "2VLMs Lack a Consistent Preference for Real-World Trajectories",
        "images": []
    },
    {
        "header": "3Bootstrapping a World Model from a Dynamics Model in VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06006/x4.png",
                "caption": "Figure 3:Heatmap visualization of image token weights predicted by the recognition model on examples from UCF-101, Something-Something, MagicBrush, and Kubric.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06006/x5.png",
                "caption": "Figure 4:Comparison of negative log-likelihoods (lower values indicate stronger model preference) of the action predicted by CDM for ground-truth trajectories versus four types of negative trajectories.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x6.png",
                "caption": "Figure 8:GPT-4o scores for test-time verification withKùêæKitalic_Ksamples, whereK‚àà{1,2,4,8}ùêæ1248K\\in\\{1,2,4,8\\}italic_K ‚àà { 1 , 2 , 4 , 8 }. We use ablue linefor C-FT and ared linefor CWM, plotting the standard deviation as the shaded area. We indicate the scores for GoT (GT) and SmartEdit (SE) as horizontal lines.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x7.png",
                "caption": "Figure 9:A qualitative case of real-world observation prediction, demonstrating CWM‚Äôs ability to steer predictions using language and perform sequential predictions. More cases fromAurora-Benchare in AppendixA.4.",
                "position": 758
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06006/x8.png",
                "caption": "Figure 10:Qualitative examples of the predicted next observation from the state-of-the-art specialised image editing models, and our models including C-FT and CWM, onAurora-Bench.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x9.png",
                "caption": "Figure 11:Distributions of triplet log-likelihoods predicted by CDM on Movements-in-Time, UCF-101, and Kinetics-700, based on 7K synthetic triplets per dataset. Triplets are uniformly sampled from each action class while maximising overall predicted likelihoods.",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x9.png",
                "caption": "",
                "position": 1903
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x10.png",
                "caption": "",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x11.png",
                "caption": "",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x12.png",
                "caption": "(a)Detailed GPT4o scores for CWM trained with the standard loss.",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x12.png",
                "caption": "(a)Detailed GPT4o scores for CWM trained with the standard loss.",
                "position": 2102
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x13.png",
                "caption": "(b)Detailed GPT4o scores for CWM trained with the L2-weighted loss.",
                "position": 2108
            },
            {
                "img": "https://arxiv.org/html/2506.06006/x14.png",
                "caption": "Figure 13:The screenshot for the instructions given to participants and the interface developed for conducting the evaluation.",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]