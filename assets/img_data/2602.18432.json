[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18432/x1.png",
                "caption": "Figure 1.Our method generates full-body 3D motion for a virtual agent that is spatially aware of the user while engaging in a conversation.\nGiven the user‚Äôs floor-projected head trajectory and dyadic audio, we generate the agent‚Äôs complete 3D motion.\nTrajectory colors indicate time: blue‚Üí\\rightarrowgreen (user) and yellow‚Üí\\rightarrowred (agent). Seeproject pagefor results.",
                "position": 101
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18432/x2.png",
                "caption": "Figure 2.Given the user‚Äôs 3D position and dyadic conversational audio, our model generates 3D motion that is conversationally and spatially aware (left).\nWe use a fully causal transformer-based VAE with interleaved latent tokens at a fixed temporal stride; both encoder and decoder employ causal attention, where eachŒº/œÉ\\mu/\\sigmatoken attends only to preceding frames and earlier latents (center).\nThese latents are passed to a transformer-based flow matching model that also uses causal masking and optionally accepts a gaze score for controlling the agent‚Äôs eye contact (right).\nOur lightweight architecture enables real-time, autoregressive streaming without distillation.",
                "position": 158
            }
        ]
    },
    {
        "header": "2.Related work",
        "images": []
    },
    {
        "header": "3.Real-time, Auto-regressive Motion Synthesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18432/x3.png",
                "caption": "Figure 3.We represent each jointjjas a 3D icosahedron.\nThe centroid of the vertices yields the global positionùö∑j\\boldsymbol{\\Pi}_{j}, and we recover the global orientationùõÄj\\boldsymbol{\\Omega}_{j}via SVD against a reference icosahedron.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2602.18432/x4.png",
                "caption": "Figure 4.Our training data spans a wide range of gaze behaviors, from sustained eye contact to complete gaze aversion (left).\nTo enable controllable gaze at inference, we compute a gaze scoregg, whereùêùx\\mathbf{d}_{x}is the agent‚Äôs facing direction andùêùy\\mathbf{d}_{y}points toward the user (right).\nThe score approaches11when facing the user directly and‚àí1-1when facing away.",
                "position": 336
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18432/x5.png",
                "caption": "Figure 5.We visualize the agent‚Äôs facing direction via projected lines (agent: yellow‚Üí\\rightarrowred; user: blue‚Üí\\rightarrowgreen). With no alignmentg=‚àÖg=\\emptyset, the agent‚Äôs gaze is more diverse; as we increasegg, the agent increasingly turns towards the user.",
                "position": 837
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.18432/x6.png",
                "caption": "Figure 6.Sequences from our real-time demo system, rendered with a photorealistic avatar. The top row visualizes the user‚Äôs headset location as a silver sphere. The bottom row shows the generated avatar from the user‚Äôs (headset) viewpoint. Our method generates realistic conversational motion that is responsive to the user‚Äôs spatial motion. Full videos are available on ourproject page.",
                "position": 904
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Material",
        "images": []
    }
]