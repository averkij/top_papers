[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19950/x1.png",
                "caption": "Figure 1:The observed log-distribution pattern is evident not only in the magnitude of attention scores but also in the positions of attention spikes. These spikes become sparser as the model attends to tokens further from the most recent position, indicating that the model not only focuses on nearby tokens. This phenomenon, illustrated here with Llama3-8B-Instruct(Dubey etÂ al.,2024)on the GSM8K dataset(Cobbe etÂ al.,2021), is consistent across different tasks and models, as further detailed in Section2.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19950/x2.png",
                "caption": "Figure 2:The maximum attention score of each token position across four consecutive decoding steps, marking the high attention positions for illustrating the unpredictable nature of attention scores. This analysis was conducted using Llama3-8B-Instruct(Dubey etÂ al.,2024)on the GSM8K(Cobbe etÂ al.,2021)and OpenBookQA(Mihaylov etÂ al.,2018)datasets.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x3.png",
                "caption": "Figure 3:Attention distribution across different token positions, represented as boxplots based on 25% quantiles across all attention heads. The median and overall distribution of attention scores for sink tokens(Xiao etÂ al.,2023)(tokens 0 and 1) are greater than the sum of the most recent 128 tokens. The attention scores are derived from experiments using Llama3-8B-Instruct(Dubey etÂ al.,2024)and the GSM8K(Cobbe etÂ al.,2021)dataset.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x4.png",
                "caption": "Figure 4:The attention coverage without the first two sink tokens for different selection methods(Liu etÂ al.,2024c; Xiao etÂ al.,2023; Zhang etÂ al.,2024)and different models(Dubey etÂ al.,2024; Yang etÂ al.,2024; Abdin etÂ al.,2024), tested on a subset of the GSM8K(Cobbe etÂ al.,2021)dataset. Details of LogQuant will be introduced in Section2.5.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x5.png",
                "caption": "Figure 5:Eviction and Quantization Loss on Attention Distribution",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x6.png",
                "caption": "Figure 6:LogQuantâ€™s KV cache compression workflow. The number of reserved original-precision tokens increases from2â¢W2ğ‘Š2W2 italic_Wto3â¢W3ğ‘Š3W3 italic_W. We then apply a log-sparse strategy to filter the first2â¢W2ğ‘Š2W2 italic_Wtokens, quantize half of these tokens, and compress the reserved token length back to2â¢W2ğ‘Š2W2 italic_W.",
                "position": 304
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19950/x7.png",
                "caption": "Figure 7:Accuracy(EM) with different compression ratio in GSM8K tasks for different models.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x8.png",
                "caption": "Figure 8:memory usage and throughput comparison between 2bit LogQuant and 16bit baseline under huggingface generation pipeline with llama3.1-8B and H100.",
                "position": 678
            }
        ]
    },
    {
        "header": "4Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground & Related Work: KV Cache Compression",
        "images": []
    },
    {
        "header": "Appendix BOverview of Test Datasets",
        "images": []
    },
    {
        "header": "Appendix CMeta Data of Precision Comparison",
        "images": []
    },
    {
        "header": "Appendix DMeta Data of LongBench Results",
        "images": []
    }
]