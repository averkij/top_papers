[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19950/x1.png",
                "caption": "Figure 1:The observed log-distribution pattern is evident not only in the magnitude of attention scores but also in the positions of attention spikes. These spikes become sparser as the model attends to tokens further from the most recent position, indicating that the model not only focuses on nearby tokens. This phenomenon, illustrated here with Llama3-8B-Instruct(Dubey et al.,2024)on the GSM8K dataset(Cobbe et al.,2021), is consistent across different tasks and models, as further detailed in Section2.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19950/x2.png",
                "caption": "Figure 2:The maximum attention score of each token position across four consecutive decoding steps, marking the high attention positions for illustrating the unpredictable nature of attention scores. This analysis was conducted using Llama3-8B-Instruct(Dubey et al.,2024)on the GSM8K(Cobbe et al.,2021)and OpenBookQA(Mihaylov et al.,2018)datasets.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x3.png",
                "caption": "Figure 3:Attention distribution across different token positions, represented as boxplots based on 25% quantiles across all attention heads. The median and overall distribution of attention scores for sink tokens(Xiao et al.,2023)(tokens 0 and 1) are greater than the sum of the most recent 128 tokens. The attention scores are derived from experiments using Llama3-8B-Instruct(Dubey et al.,2024)and the GSM8K(Cobbe et al.,2021)dataset.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x4.png",
                "caption": "Figure 4:The attention coverage without the first two sink tokens for different selection methods(Liu et al.,2024c; Xiao et al.,2023; Zhang et al.,2024)and different models(Dubey et al.,2024; Yang et al.,2024; Abdin et al.,2024), tested on a subset of the GSM8K(Cobbe et al.,2021)dataset. Details of LogQuant will be introduced in Section2.5.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x5.png",
                "caption": "Figure 5:Eviction and Quantization Loss on Attention Distribution",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x6.png",
                "caption": "Figure 6:LogQuant’s KV cache compression workflow. The number of reserved original-precision tokens increases from2⁢W2𝑊2W2 italic_Wto3⁢W3𝑊3W3 italic_W. We then apply a log-sparse strategy to filter the first2⁢W2𝑊2W2 italic_Wtokens, quantize half of these tokens, and compress the reserved token length back to2⁢W2𝑊2W2 italic_W.",
                "position": 304
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19950/x7.png",
                "caption": "Figure 7:Accuracy(EM) with different compression ratio in GSM8K tasks for different models.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2503.19950/x8.png",
                "caption": "Figure 8:memory usage and throughput comparison between 2bit LogQuant and 16bit baseline under huggingface generation pipeline with llama3.1-8B and H100.",
                "position": 678
            }
        ]
    },
    {
        "header": "4Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABackground & Related Work: KV Cache Compression",
        "images": []
    },
    {
        "header": "Appendix BOverview of Test Datasets",
        "images": []
    },
    {
        "header": "Appendix CMeta Data of Precision Comparison",
        "images": []
    },
    {
        "header": "Appendix DMeta Data of LongBench Results",
        "images": []
    }
]