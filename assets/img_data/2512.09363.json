[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09363/figs/teaser.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09363/figs/framework.png",
                "caption": "Figure 2:Overall framework of StereoWorld.Before training, we use Video Depth Anything[chen2025video]and Stereo Any Video[jing2025stereo]to obtain the depth mapsDrD_{r}and disparity mapsD​i​s​pgt{Disp}_{\\text{gt}}, and the left-view videos are then concatenated with the right-view videos and corresponding depth maps along the frame dimension in the latent space as conditioning inputs. During training, a lightweight differentiable stereo projector estimates the disparity between the input left-view and the generated right-view, which is supervised by disparity mapsD​i​s​pgt{Disp}_{\\text{gt}}via disparity loss to enforce accurate geometric correspondence. Additionally, the last few DiT blocks are duplicated to form dual branches, allowing the model to learn RGB and depth distributions separately to further supplement geometric information. During inference, only the shared and RGB DiT blocks are used, taking the monocular video as the sole input.",
                "position": 253
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09363/figs/struct.png",
                "caption": "Figure 3:Non-overlapping regions between stereo views.Horizontal camera translation introduces non-overlapping content, which disparity supervision alone cannot constrain, motivating the use of depth-based supervision.",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2512.09363/figs/long.png",
                "caption": "Figure 4:Temporal tiling strategy.During training, the first few frames of noisy latents are replaced with ground-truth frames with a probabilitypp. During inference, long videos are split into overlapping segments, with the last frames of the previous segment used to guide the next, ensuring temporal consistency.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2512.09363/figs/big.png",
                "caption": "Figure 5:Spatial tiling strategy.During inference, high-resolution videos are encoded into latents, which are split into overlapping tiles. Each tile is denoised independently, and then the tiles are stitched back to the original size with overlapping regions fused before decoding.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2512.09363/figs/compare.png",
                "caption": "Figure 6:Qualitative comparisons with state-of-the-art methods.It shows that our method achieves the best generation quality, preserving fine details while maintaining strong visual consistency with the left view. Crucially, our method achieves far better text rendering quality than all baselines.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2512.09363/figs/compare2.png",
                "caption": "Figure 7:Qualitative comparisons with state-of-the-art methods in the temporal dimension.Our method maintains superior temporal consistency while preserving high visual quality and fine-grained detail fidelity compared to other methods.",
                "position": 470
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09363/figs/ablation.png",
                "caption": "Figure 8:Qualitative comparison results of ablation study.Our full model exhibits better disparity shifts and structural perception.",
                "position": 529
            }
        ]
    },
    {
        "header": "5Conclusion and Limitations",
        "images": []
    }
]