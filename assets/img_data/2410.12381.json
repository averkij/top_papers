[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x1.png",
                "caption": "Figure 1:An example coding task inHumanEval-Vthat all LMMs evaluated in this work cannot solve, including GPT-4o and Claude 3.5 Sonnet. Related error analysis can be found in AppendixA.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x2.png",
                "caption": "Figure 2:TheHumanEval-Vconstruction pipeline, with representative examples for each stage.",
                "position": 169
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x3.png",
                "caption": "Figure 3:The prompting template used for LMMs to generate code solutions. The {code_context} placeholder is for the corresponding function signature.",
                "position": 287
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x4.png",
                "caption": "Table 2:Performance of 19 LMMs onHumanEval-V. Scores are shown as percentages, with the highest values highlighted inbold. We also include model size (Params) and code Parsing Success Rate (Pars. Rate).",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x4.png",
                "caption": "Table 3:Correlation analysis betweenHumanEval-Vpass@10 results and three popular multimodal benchmarks spanning multidisciplinary abilities.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x5.png",
                "caption": "Figure 4:A coding task that InternVL-2-26B fails to solve with grounded image description.",
                "position": 752
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AError Analysis on the Example Task",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x6.png",
                "caption": "Figure 5:Examples of incorrect solutions generated by GPT-4o and Claude 3.5 Sonnet for the coding task shown in Figure1.",
                "position": 1776
            }
        ]
    },
    {
        "header": "Appendix BRepresentative Images inHumanEval-V",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x7.png",
                "caption": "Figure 6:A curated selection of representative images inHumanEval-V, covering visual elements like trees, graphs, matrices, maps, grids, flowcharts, and more.",
                "position": 1791
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x8.png",
                "caption": "Figure 7:The correlations between six multimodal benchmarks, includingHumanEval-V. Each subplot, except on the diagonal, visualizes the relationship between two benchmarks.",
                "position": 2345
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x9.png",
                "caption": "Figure 8:An example of image description annotation.",
                "position": 2359
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x10.png",
                "caption": "Figure 9:An example of image description annotation.",
                "position": 2362
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x11.png",
                "caption": "Figure 10:The template used for prompting LMMs to solve code generation tasks with image descriptions. The {image_description} placeholder is replaced with the annotated image description. The {code_context} placeholder is replaced with the corresponding function signature.",
                "position": 2365
            }
        ]
    },
    {
        "header": "Appendix DBenchmark Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12381/x12.png",
                "caption": "Figure 11:A negative example in our data screening process, sourced from CodeForces (https://codeforces.com/problemset/problem/294/B), where the image is non-essential for solving the problem.",
                "position": 2385
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x13.png",
                "caption": "Figure 12:A negative example in our data screening process, sourced from GeeksforGeeks (https://www.geeksforgeeks.org/problems/last-cell-in-a-matrix/1), where the visual elements require extensive textual descriptions to interpret.",
                "position": 2388
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x14.png",
                "caption": "Figure 13:A positive example in our data screening process, sourced from Stack Overflow (https://stackoverflow.com/questions/69163515).",
                "position": 2391
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x15.png",
                "caption": "Figure 14:A positive example in our data screening process, sourced from CodeForces (https://codeforces.com/problemset/problem/1381/E).",
                "position": 2394
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x16.png",
                "caption": "Figure 15:A positive example in our data screening process, sourced from CodeForces (https://codeforces.com/problemset/problem/1996/B).",
                "position": 2397
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x17.png",
                "caption": "Figure 16:The adapted coding task from Figure13as incorporated intoHumanEval-V.",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x18.png",
                "caption": "Figure 17:The adapted coding task from Figure14as incorporated intoHumanEval-V.",
                "position": 2403
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x19.png",
                "caption": "Figure 18:The adapted coding task from Figure15as incorporated intoHumanEval-V.",
                "position": 2406
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x20.png",
                "caption": "Figure 19:A mutated version of the coding task from Figure16.",
                "position": 2447
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x21.png",
                "caption": "Figure 20:A mutated version of the coding task from Figure17.",
                "position": 2450
            },
            {
                "img": "https://arxiv.org/html/2410.12381/x22.png",
                "caption": "Figure 21:A mutated version of the coding task from Figure18.",
                "position": 2453
            }
        ]
    },
    {
        "header": "Appendix EDiscussion on the MMCode Dataset",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    },
    {
        "header": "Appendix GDetails of the Evaluated Models",
        "images": []
    }
]