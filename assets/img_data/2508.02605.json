[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02605/remomask_logo.png",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2508.02605/x1.png",
                "caption": "Figure 1:Comparison between t2m models.(a) The conventional t2m models. (b) The Existing RAG-t2m models. (c) The framework of our proposed ReMoMask.",
                "position": 101
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Related Work",
        "images": []
    },
    {
        "header": "The Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02605/x2.png",
                "caption": "Figure 2:Overview of ReMoMask. (a) Bidirectional Momentum Contrastive Retrieval (BMM) uses two momentum queues, enabling a large pool of negative samples for contrastive learning.\n(b) ReMoMask quantizes a motion sequence into a 2D token map, capturing not only temporal dynamics but also spatial structure. After that, a Part-Level BMM Retriever retrieves relevant text and motion features based on the prompt embedding. All these conditions are fused via an SSTA module in a 2D RAG-Mask-Transformer together with the latent motion representaion.\n(c) Semantic Spatial-temporal Attention (SSTA) first flattens the masked 2D token map into a 1D structure, then redefines the Q, K, V matrix utilizing the conditions above, providing effective semantic alignment between the conditions and the spatial-temporal information of motion",
                "position": 178
            }
        ]
    },
    {
        "header": "Experiment",
        "images": []
    },
    {
        "header": "Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02605/question_2.png",
                "caption": "Figure 3:Motion Quality User Study",
                "position": 1934
            },
            {
                "img": "https://arxiv.org/html/2508.02605/question_1.png",
                "caption": "Figure 4:Text-Motion Correspondence User Study",
                "position": 1939
            }
        ]
    },
    {
        "header": "Appendix BLimitation and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.02605/demo_00.png",
                "caption": "Figure 5:We randomly sample and visualize 16 motions generated by the proposed ReMoMask framework. These examples are conditioned on diverse prompts randomly selected from the HumanML3D(Guo et al.2022b), providing qualitative evidence of the model’s ability to synthesize a wide range of realistic and semantically coherent motions.",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2508.02605/compare_00.png",
                "caption": "Figure 6:Comparison of the proposed ReMoMask with three state-of-the-art methods: MoGenTS(Yuan et al.2024), TMR(Petrovich, Black, and Varol2023), and ReMoDiffuse(Zhang et al.2023d)We visualize motion sequences generated in response to three distinct text prompts. Each row corresponds to a specific prompt, and each column represents the output of a different method. The results demonstrate that ReMoMask produces more realistic and semantically aligned motions compared to existing approaches.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2508.02605/x3.png",
                "caption": "Figure 7:This figure illustrates the User Interface (UI) used in the ReMoMask User Study. Participants are presented with two motion videos, labeled as Motion A and Motion B, alongside a shared textual prompt. The motion clips are sampled from outputs generated by different models or the ground truth (GT), with model identities anonymized and video order randomized. Participants are asked to answer two evaluative questions: (1) “Which of the two motions is more realistic?”, assessing the visual plausibility and motion quality; and (2) “Which of the two motions corresponds better to the text prompt?”, evaluating the semantic alignment between the motion and the given description. This dual-question design enables a comprehensive human assessment of both motion realism and text-motion correspondence.",
                "position": 1974
            }
        ]
    },
    {
        "header": "Appendix CVisualization",
        "images": []
    }
]