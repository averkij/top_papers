[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01062/x1.png",
                "caption": "Figure 1:Test-time scaling does not lead to reliable gains onSealQAquestions, with performance often plateauing or even declining early.",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2506.01062/x2.png",
                "caption": "Figure 2:Accuracy ofLLMsacross benchmarks.SealQAposes significant challenges to frontier models.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2506.01062/x3.png",
                "caption": "Figure 3:SealQArequires intensive reasoning to resolve ambiguity, filter out misinformation, or reconcile conflicting evidence.",
                "position": 162
            }
        ]
    },
    {
        "header": "2Data collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01062/x4.png",
                "caption": "Figure 4:SealQAquestions test a broad range of reasoning skills that are often overlooked in existing benchmarks.",
                "position": 176
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01062/x5.png",
                "caption": "Figure 5:Advanced reasoning models such asDeepSeek-R1-671Bando3-miniare highly vulnerable to noisy search results.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2506.01062/x6.png",
                "caption": "Figure 6:FrontierLLMsfail to reliably identify relevant documents inLongSealwhen numerous distractors are present, despite being less prone to“lost-in-the-middle”failures(Liu et al.,2024).",
                "position": 794
            }
        ]
    },
    {
        "header": "4Related work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Ethical considerations and risks",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASeal-Hardresults by question category",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01062/x7.png",
                "caption": "Figure 7:OnSeal-Hard, open-weight models likeLlama-4-ScoutandDeepSeek-R1choose to “not attempt” questions more often than proprietary models such asGPT-4.1,o4-mini, ando3.",
                "position": 2446
            }
        ]
    },
    {
        "header": "Appendix BSeal-Hardresults by answer type",
        "images": []
    }
]