[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01925/x1.png",
                "caption": "Figure 1:Illustration of theReProframework. We incorporate a rectifying process-level reward into the RLVR training to enhance LLM reasoning. Initially, we conceptualize the reasoning trajectories generated by LLMs as an optimization process of the LLMs‚Äô internal state (¬ßÀú3.1&¬ßÀú3.2). We then propose a two-fold score to evaluate the optimization process and utilize this score as a reward to rectify the LLM thought (¬ßÀú3.3&¬ßÀú3.4).",
                "position": 286
            }
        ]
    },
    {
        "header": "3RePro: Rectifying LLM Thought",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01925/x2.png",
                "caption": "Figure 2:Empirical evidence supporting‚àíùí•~-\\tilde{\\mathcal{J}}as a proxy metric. The left panel presents the question and its corresponding answer, while the right panel plots‚àíùí•~-\\tilde{\\mathcal{J}}as a function of reasoning trajectory tokens.",
                "position": 351
            }
        ]
    },
    {
        "header": "4Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01925/x3.png",
                "caption": "Figure 3:Dynamics of the reasoning token cost during the training process ofReProon DeepSeek-R1-Distill-Qwen-1.5B.",
                "position": 782
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x4.png",
                "caption": "Figure 4:Ablation experiments of weightwwandReProweightŒ±\\alpha.",
                "position": 785
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x4.png",
                "caption": "Figure 4:Ablation experiments of weightwwandReProweightŒ±\\alpha.",
                "position": 788
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x5.png",
                "caption": "Figure 5:Dynamics of thebacktrackingpattern.",
                "position": 793
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x6.png",
                "caption": "Figure 6:Comparison of the inference reasoning token cost of DeepSeek-R1-Distill-Qwen-1.5B.",
                "position": 817
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADiscussions",
        "images": []
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01925/x7.png",
                "caption": "Figure 7:Comparison of the inference reasoning token cost ofReProand baselines on Qwen3-1.7B.",
                "position": 1795
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x8.png",
                "caption": "Figure 8:Comparison of the inference reasoning token cost ofReProand baselines on Hunyuan-1.8B-Instruct.",
                "position": 1798
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x9.png",
                "caption": "Figure 9:Comparison of the inference reasoning token cost ofReProand baselines on Qwen3-8B.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2512.01925/x10.png",
                "caption": "Figure 10:Token growth ofReProand GRPO on base models.",
                "position": 2124
            }
        ]
    },
    {
        "header": "Appendix DCases",
        "images": []
    }
]