[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01440/x1.png",
                "caption": "Figure 1:Ranking of optimizers forğŸ•ğŸğŸâ€‹ğŒ\\mathbf{720M}Llama-based models.We plot thefinal validation lossobtained by the best-tuned optimizers on the FineWeb dataset.\nWe use a batch size of1â€‹ğŒ1\\mathbf{M}tokens and train multiple methods beyond and below the Chinchilla optimal duration, which is14.4â€‹ğ14.4\\mathbf{B}for model of this size.AdEMAMixandMARSare the best optimizers in this setup, with a noticable gap in performance compared to other methods.\nWe also plot theAdamWbaseline in both figures to distinguish the group of methods that consistently perform worse thanAdamWfrom the group of optimizers that outperform it for some training durations.\nSee Â§3andAppendixËœEfor a detailed description of our experimental setup, including hyperparameters.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x2.png",
                "caption": "",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x3.png",
                "caption": "Figure 2:Training dynamics of leading optimizers onğŸ“ğŸğŸâ€‹ğŒ\\mathbf{520M}MoE model pretraining.We use a batch size of131â€‹ğ¤131\\mathbf{k}tokens, and train models for both short runs, i.e., less than Chinchilla optimal duration, and for extended runs beyond this regime.\nThe dashed blue lines correspond to the final validation loss ofAdamWbaselines trained for both42â€‹ğ¤42\\mathbf{k}and336â€‹ğ¤336\\mathbf{k}steps.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Background & Related Work",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01440/x4.png",
                "caption": "Figure 4:Scaling batch size vs. scaling the number of iterations.Our results demonstrate that: (left) scaling the batch size significantly improvesMARS,Signum,LionandProdigymaking them as good asAdamWeven for a long training for16.8â€‹ğ16.8\\mathbf{B}tokens.\nWhich was not the case inLABEL:fig:benchmark-124(b), where we still observed a significant gap in performance; and (right): indeed, with scaling of the number of iterations, the gap betweenSOAPandAdEMAMixnarrow and, finally, increases.\nBut, on the other hand, with increase of theAdEMAMixÎ²3\\beta_{3}parameter, the performance gap withSOAPreappears.",
                "position": 361
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x5.png",
                "caption": "Figure 7:Warmup ablation.For124â€‹ğŒ124\\mathbf{M}model trained on the batches of256Ã—512256\\times 512tokens, we perform a sweep over the linear warmup durations of{1.56%,6.25%,25%}\\{1.56\\%,6.25\\%,25\\%\\}of the length of training, which corresponds to{2,8,32}â€‹ğ¤\\{2,8,32\\}\\mathbf{k}steps, respectively.\nClearly, sign-based optimizers,Sophia, andSF-AdamWbenefit from the increased warmup.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x6.png",
                "caption": "Figure 10:Ranking of optimizers forğŸğŸğŸâ€‹ğŒ\\mathbf{210M}models with the batch size ofğŸğŸ“ğŸ”Ã—ğŸ“ğŸğŸ\\mathbf{256\\times 512}tokens.Increasing a model size from124â€‹ğŒ124\\mathbf{M}to210â€‹ğŒ210\\mathbf{M}results in almost identical ranking of optimizers compared toLABEL:fig:benchmark-124(b).\nAt this scale, we observe a smooth transition in our benchmarking.",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x7.png",
                "caption": "Figure 13:Ablation ofzz-loss regularization.Incorporating thezz-loss regularizer does not improve the final loss or reduce the spikiness of the loss curves.\nMoreover, combiningzz-loss with small weight decay and decayingÎ³\\gammadown to10%10\\%, further degrades overall performance.\nNotably, these changes can reverse the relative ranking of optimizers compared to the results reported by Vyas et al.vyas2024soapimprovingstabilizingshampoo.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x8.png",
                "caption": "Figure 16:Wall-clock time comparison.SOAPslows down the most as model size increases.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x9.png",
                "caption": "Figure 17:Ranking optimizers forğŸ“ğŸğŸâ€‹ğŒ\\mathbf{520M}MoE models withğŸğŸ“ğŸ”Ã—ğŸ“ğŸğŸ\\mathbf{256\\times 512}batch size.We report results for models trained for both42â€‹ğ¤42\\mathbf{k}iterationsÂ (left), and336â€‹ğ¤336\\mathbf{k}(right).\nMoE configuration correspond to one of the124â€‹ğŒ124\\mathbf{M}dense model.\nOptimizer rankings closely mirror those inLABEL:fig:benchmark-124(b), indicating that our benchmarking results transfer smoothly from dense models to MoEs.\nWe also see thatSOAPoutperformsAdEMAMixin336â€‹ğ¤336\\mathbf{k}steps runÂ (see alsoFigureËœ2), however, with re-tuned beta parameters we might expect the opposite results in longer trainingÂ (see Figures4andLABEL:fig:ap_retuning_betas(b)).",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x10.png",
                "caption": "Figure 18:Comparing optimizers for training ağŸ“ğŸğŸâ€‹ğŒ\\mathbf{520M}parameter MoE.Training dynamics of leading optimizers is inFigureËœ2.\nResults closely remind those inLABEL:fig:benchmarking-124m-losses(a,b).\nTheAdamWbaseline by far outperformsSophia,SF-AdamW,MARS, and sign-based methods for44â€‹ğ44\\mathbf{B}training horizon.\nRemarkably, in the same way asProdigyfollowedAdamWinLABEL:fig:benchmarking-124m-losses(b), we observe a similar situation for the MoE model.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x11.png",
                "caption": "",
                "position": 664
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOptimizers we study",
        "images": []
    },
    {
        "header": "Appendix BImplementation",
        "images": []
    },
    {
        "header": "Appendix CModel & Data",
        "images": []
    },
    {
        "header": "Appendix DAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.01440/x12.png",
                "caption": "Figure 20:Warmup sweep forAdamW.We observe that the smaller yet reasonable warmup value is the best.\nHowever, this is not the case for other methods likeSignum,Lion,Sophia, andSF-AdamWâ€”seeFigureËœ7.",
                "position": 4227
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x13.png",
                "caption": "Figure 21:Larger weight decay achieves significantly better results when training on fewer tokens.We observe that the majority of runs with the large weight decay of0.50.5consistently outperform those with weight decay of0.10.1for all training durations except for the long training on16.8â€‹ğ16.8\\mathbf{B}tokens.\nNotably,SignumandLionwith large weight decay perform even better thanAdamWwith the same learning rateâ€”seeLABEL:fig:wdablation_main.\nWe also consider a setting without weight decay.\nWe observe that this is suboptimal for most of other optimizers, while the typical weight decay of0.10.1remains the best for long training durations.\nAn interesting pattern emerges for optimizers that treat one-dimensional and two-dimensional parameters differently, such asMuonandMARS.\nFor these, runs with large weight decay (0.50.5) consistently underperform those with0.10.1and, in some cases, even those without weight decay.\nForMuon, we attribute this effect to its algorithmic design, in which weight decay is not employed to optimize matrix parameters (seeAlgorithmËœ7), in contrast toD-Muon, where the observed patterns are reliably similar to those seen withAdamW.\nForMARS, we only vary the weight decay corresponding to matrix parameters while keeping0.10.1for all scalar, one-dimensional and final layer parameters.\nIn this case, we conclude that the gap between large and small weight decay values narrows significantly faster.",
                "position": 4254
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x14.png",
                "caption": "",
                "position": 4263
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x15.png",
                "caption": "",
                "position": 4268
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x16.png",
                "caption": "",
                "position": 4274
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x17.png",
                "caption": "",
                "position": 4279
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x18.png",
                "caption": "",
                "position": 4284
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x19.png",
                "caption": "",
                "position": 4290
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x20.png",
                "caption": "",
                "position": 4295
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x21.png",
                "caption": "",
                "position": 4300
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x22.png",
                "caption": "",
                "position": 4306
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x23.png",
                "caption": "",
                "position": 4311
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x24.png",
                "caption": "",
                "position": 4316
            },
            {
                "img": "https://arxiv.org/html/2509.01440/",
                "caption": "Figure 22:Learning rate sensitivity.In the current setting, onlySOAP,SF-AdamW, andD-Muonreach the better performance with the large learning rate of0.0020.002.\nConversely,Sophiaand all sign-based methods (SignumandLion) diverge with this learning rate value.MARSandProdigyshow a remarkably consistent performance across the learning rate sweep.\nAnd,Prodigydiverges for sufficiently large value ofÎ³max\\gamma_{\\max}â€”seeLABEL:fig:ap_prodigy_effective_lrfor more insights regarding the learning rate ofProdigy.",
                "position": 4342
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x26.png",
                "caption": "",
                "position": 4351
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x27.png",
                "caption": "",
                "position": 4356
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x28.png",
                "caption": "",
                "position": 4362
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x29.png",
                "caption": "",
                "position": 4367
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x30.png",
                "caption": "",
                "position": 4372
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x31.png",
                "caption": "",
                "position": 4378
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x32.png",
                "caption": "",
                "position": 4383
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x33.png",
                "caption": "",
                "position": 4388
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x34.png",
                "caption": "",
                "position": 4394
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x35.png",
                "caption": "",
                "position": 4399
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x36.png",
                "caption": "",
                "position": 4404
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x37.png",
                "caption": "Figure 24:Comparisons between cosine, WSD, and the linear schedulers.We complement results inLABEL:fig:wsdvscosineby extending them to all the optimizers considered in our benchmarking.\nIn most cases, the tuned cosine baseline performs similarly to runs using the linear scheduler, with both slightly outperforming WSD.\nHowever, certain optimizers still tend to â€œpreferâ€ differentÎ³\\gamma-schedulers.\nFor example,Muonshows a preference in WSD (seeLABEL:fig:wsdvscosine(a)),AdamWperforms better with the cosine scheduler,SignumandLionappear to favor the linear scheduler.\nWhile the performance differences are not particularly large, they are still meaningful in the context of benchmarking.\nTherefore, we adopt the cosine scheduler as our default, as even small gaps can substantially impact our setup.",
                "position": 4443
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x38.png",
                "caption": "",
                "position": 4452
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x39.png",
                "caption": "",
                "position": 4458
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x40.png",
                "caption": "",
                "position": 4463
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x41.png",
                "caption": "",
                "position": 4469
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x42.png",
                "caption": "",
                "position": 4474
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x43.png",
                "caption": "",
                "position": 4480
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x44.png",
                "caption": "",
                "position": 4485
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x45.png",
                "caption": "Figure 25:Gradient norm patterns for cosine, linear, and WSDÎ³\\mathbf{\\gamma}-schedulers.We run all optimizers on124â€‹ğŒ124\\mathbf{M}models and track the gradient norms (before clipping) for runs using differentÎ³\\gamma-schedulers.\nFor most optimizers, we see that gradient norms tend to increase over the course of training with cosine and linear schedules.\nIn contrast, WSD tends to produce flatter gradient norm trajectories, with consistently lower magnitudes toward the end of training compared to the other schedulers.\nSince the WSD scheduler maintains a constant learning rate until the cooldown phase (the final20%20\\%of the training length), we observe a more stable gradient norm behavior in later stages.\nIn this regard, our findings align with prior works[69,25], which explore a connection between the learning rate schedule and gradient norm dynamics.\nInterestingly,SignumandLionâ€”seeLABEL:fig:grad-norms-main-partâ€”exhibit a pronounced drop in gradient norm during the cooldown phase, setting them apart from the other optimizers.",
                "position": 4511
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x46.png",
                "caption": "",
                "position": 4520
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x47.png",
                "caption": "",
                "position": 4525
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x48.png",
                "caption": "",
                "position": 4531
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x49.png",
                "caption": "",
                "position": 4536
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x50.png",
                "caption": "",
                "position": 4541
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x51.png",
                "caption": "",
                "position": 4547
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x52.png",
                "caption": "",
                "position": 4552
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x53.png",
                "caption": "",
                "position": 4557
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x54.png",
                "caption": "Figure 26:Gradient norm patterns for weight decay sweep.We complement our weight decay ablationÂ (LABEL:fig:wdablation_mainand21) by tracking the gradient norms for all the optimizers studied in our benchmark.\nTo highlight the effect of changing the weight decay, we use the same cosineÎ³\\gamma-scheduler for all optimizers and keep the other best hyperparameters found, sweeping only the weight decay values as described inÂ Â§3â€”i.e., we fix the maximum learning rate and only change the weight decay.\nForMuon, we only sweep the weight decay for{ğšğš–ğš‹ğšğšğšœ,ğšœğšŒğšŠğš•ğšŠğš›â€‹_â€‹ğš™ğšŠğš›ğšŠğš–ğšœ,ğš•ğš–â€‹_â€‹ğš‘ğšğšŠğš}\\{\\mathtt{embeds},\\mathtt{scalar\\_params},\\mathtt{lm\\_head}\\}(as in the initial implementation, the weight decay has not been applied to matrix parameters), while forMARS, we only sweep the weight decay of22D parameters.\nOur observations reveal that, regardless of optimizer used, runs with a larger weight decay result in higher gradient norms.\nForMuon,AdEMAMix,Sophia, and sign-based methods, runs with moderateÎ»=0.1\\lambda=0.1result in the most flattened and smallest gradient norms in magnitude.\nWhile forAdamW-like methods,D-Muon,SOAP,Prodigy, andSF-AdamW, this holds forÎ»=0\\lambda=0.\nWe attribute the discrepancies betweenD-MuonandMuonto the latterâ€™s absence of weight decay for matrix parameters.\nAs shown inLABEL:fig:wdablation_mainand21,AdEMAMixcan benefit from large weight decay for longer training durations.\nRuns ofAdEMAMixwithÎ»=0.5\\lambda=0.5are still outperform those withÎ»=0.1\\lambda=0.1.\nInterestingly, this is reflected in the gradient norms, as the absolute values corresponding toÎ»=0.5\\lambda=0.5are much smaller than those of the respective runs of otherAdamW-like optimizers.",
                "position": 4584
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x55.png",
                "caption": "",
                "position": 4593
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x56.png",
                "caption": "",
                "position": 4598
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x57.png",
                "caption": "",
                "position": 4604
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x58.png",
                "caption": "",
                "position": 4609
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x59.png",
                "caption": "",
                "position": 4614
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x60.png",
                "caption": "",
                "position": 4620
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x61.png",
                "caption": "",
                "position": 4625
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x62.png",
                "caption": "",
                "position": 4630
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x63.png",
                "caption": "",
                "position": 4636
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x64.png",
                "caption": "",
                "position": 4641
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x65.png",
                "caption": "",
                "position": 4646
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x66.png",
                "caption": "Figure 27:Gradient norm patterns for learning rate sweep.In this experiment, we complement the result on the learning rate sweep for optimizersÂ (LABEL:fig:lrsensitivityand22) by tracking the gradient norms.\nWe follow the same setup as for theÎ³\\gamma-sensitivity ablation, varying the learning rates while training124â€‹ğŒ124\\mathbf{M}language models for16.8â€‹ğ16.8\\mathbf{B}tokens using a cosineÎ³\\gamma-scheduler withÎ³end=0.01Ã—Î³max\\gamma_{\\text{end}}=0.01\\times\\gamma_{\\max}.\nExcept forLionandSignum, we see that smallerÎ³max\\gamma_{\\max}leads to larger magnitude of the gradient normsâ€”unless the learning rate is high enough to nearly lead to divergence, e.g.,Î³max=10\\gamma_{\\max}=10forProdigy.\nInterestingly, we connect the â€œbumpâ€ shape of the gradient norms for sign-based methods with the fact thatÎ³max=0.001\\gamma_{\\max}=0.001, used for them, is close to the â€œcriticalâ€ value, an increase of which also leads to divergenceâ€”and our experiments with these optimizers on larger models support this, as we were able to decreaseÎ³\\gammain order to train properly.",
                "position": 4671
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x67.png",
                "caption": "",
                "position": 4680
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x68.png",
                "caption": "",
                "position": 4685
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x69.png",
                "caption": "",
                "position": 4691
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x70.png",
                "caption": "",
                "position": 4696
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x71.png",
                "caption": "",
                "position": 4701
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x72.png",
                "caption": "",
                "position": 4707
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x73.png",
                "caption": "",
                "position": 4712
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x74.png",
                "caption": "",
                "position": 4717
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x75.png",
                "caption": "",
                "position": 4723
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x76.png",
                "caption": "",
                "position": 4728
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x77.png",
                "caption": "",
                "position": 4733
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x78.png",
                "caption": "Figure 30:Sophiadiverges in the large-batch setup, when training for many iterations.In the small-batch setup, we observed thatSophiaexhibited convergence issues.\nWith batch size256Ã—512256\\times 512,Sophiainitially converges reliably across all training durations for124â€‹ğŒ124\\mathbf{M}models used in our benchmarking.\nHowever, when extending training beyond16.8â€‹ğ16.8\\mathbf{B}tokens, divergence reappears.\nTo clearly visualize so, we present the best stable run (T=128â€‹ğ¤T=128\\mathbf{k}steps,16.8â€‹ğ16.8\\mathbf{B}tokens) with the unstable one (T=256â€‹ğ¤T=256\\mathbf{k}steps,33.6â€‹ğ33.6\\mathbf{B}tokens), using identical hyperparameters.\nThe dashed line marks the iterationt=129720t=129720where divergence begins.\nThis instability raises serious concerns about the practicality ofSophiafor long training runs at scale.",
                "position": 4769
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x79.png",
                "caption": "Figure 33:ADOPTstill needsÎ²2\\beta_{2}.One of the main theoretical claims of Taniguchi et al.[134]â€”thatADOPTconverges with anyÎ²2\\beta_{2}.\nThe authors verify those on a toy problem motivated by Reddi et al.[115].\nHowever, in LLM training, the choice ofÎ²2\\beta_{2}still matters significantly.\nOur results demonstrate that, despite theoretical guarantees, performance strongly depends on tuningÎ²2\\beta_{2}in practice.",
                "position": 4841
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x80.png",
                "caption": "Figure 36:Muonâ€™s dependence on the number of Newton-Schulz iterations.We perform a short ablation targeting the final loss ofMuon(AlgorithmËœ8) by varying the number of Newton-Schulz iterations.\nTraining is done for16â€‹ğ¤16\\mathbf{k}steps with a batch size of256Ã—512256\\times 512tokens, sweepingTNSâˆˆ{1,5,10,20}T_{\\mathrm{NS}}\\in\\{1,5,10,20\\}.\nWe find that increasingTNST_{\\mathrm{NS}}beyond55does not improve performance, while unnecessarily increasing wall-clock time.",
                "position": 4883
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x81.png",
                "caption": "Figure 37:Comparison of different update rules forSignum.We evaluate three variants of theSignumupdate: Nesterov (our default), dampeningâ€”which resembles an EMA ofğ’t\\boldsymbol{m}_{t}when the dampening parameterÏ„\\tauequals the momentumÎ²\\betaâ€”and the â€œplainâ€Signumwithout Nesterov momentum or dampening.\nValidation perplexity is reported for two training horizons in (256Ã—512256\\times 512) batch size setting.\nThe Nesterov variant corresponds to the runs included in our main benchmarking resultsÂ (LABEL:fig:benchmark-124andLABEL:fig:benchmarking-124m-losses).\nWhile Nesterov style momentum consistently achieves the best performance, the relative perplexity gap compared to the other variants decreases as the training horizon increases.",
                "position": 4924
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x82.png",
                "caption": "Figure 40:ADOPTâ€™s sensitivity toÎµ\\varepsilon.Interestingly, the suggested by the authorsÎµ=10âˆ’6\\varepsilon=10^{-6}is the best hyperparameter for this method.\nThere is not a noticeable difference in convergence forÎµ={10âˆ’6,10âˆ’7,10âˆ’8,10âˆ’9,10âˆ’10}\\varepsilon=\\{10^{-6},10^{-7},10^{-8},10^{-9},10^{-10}\\}, but the values of10âˆ’510^{-5}and above give a much morse results.",
                "position": 5071
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x83.png",
                "caption": "Figure 42:Wall-clock time performance: gathered.We report the wall-clock time (in seconds) for training each model for100100iterations using a small batch size of16Ã—51216\\times 512tokens on a single GPU, without gradient accumulation ortorch.compile.\nBars show the ranking of optimizers from fastest (Signum) to slowest (SOAP) gathered across all model scales.\nWhile the differences between most optimizers are small,SOAPis consistently slower.\nThe absolute times may vary depending on the hardware, but the relative patterns remain consistent.",
                "position": 5226
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x84.png",
                "caption": "",
                "position": 5236
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x85.png",
                "caption": "",
                "position": 5242
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x86.png",
                "caption": "Figure 43:Wall-clock time performance: individual.ComplementingFiguresËœ16and42, this figure shows the evolution of wall-clock time per100100iterations for each optimizer as model size increases.\nOptimizers already shown in the main part are omitted.\nTo improve visualization, the abscissa is re-scaled to highlight the increase in wall-clock time with model size.",
                "position": 5253
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x87.png",
                "caption": "",
                "position": 5262
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x88.png",
                "caption": "",
                "position": 5268
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x89.png",
                "caption": "",
                "position": 5273
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x90.png",
                "caption": "",
                "position": 5279
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x91.png",
                "caption": "",
                "position": 5284
            },
            {
                "img": "https://arxiv.org/html/2509.01440/x92.png",
                "caption": "",
                "position": 5290
            }
        ]
    },
    {
        "header": "Appendix EHyperparameter tuning",
        "images": []
    }
]