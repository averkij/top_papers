[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/x1.png",
                "caption": "",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/x2.png",
                "caption": "Figure 2:LoRAShop Framework.LoRAShop enables multi-subject generation and editing over a two-stage training-free pipeline. First, we extract the subject priorM^c‚Ä≤subscript^ùëÄsuperscriptùëê‚Ä≤\\hat{M}_{c^{\\prime}}over^ start_ARG italic_M end_ARG start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, which gives a coarse-level prior on where the concept of interest,c‚Ä≤superscriptùëê‚Ä≤c^{\\prime}italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT, is located. Following, we introduce a blending mechanism over the transformer block residuals, which both enables seamless blending of customized features and bounds the region-of-interest for the LoRA adapter utilized.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/x3.png",
                "caption": "Figure 3:Editing Generated & Real Images with LoRAShop.We provide qualitative editing results with different human concepts. LoRAShop can achieve both edits on real and generated images. Due to non-intersecting subject prior extraction scheme of our framework, LoRAShop can perform edits with multiple concepts in one denoising pass.",
                "position": 118
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/x4.png",
                "caption": "Figure 4:Ablation Study.Ablation on transformer blocks, where Block 19 shows superior ability for separation between subjects.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x5.png",
                "caption": "Figure 5:Qualitative Comparisons.We provide qualitative comparisons on three mainstream tasks: single-subject generation, multi-subject generation and face swapping. Over all of the benchmarked tasks, LoRAShop provides superior performance against competing approaches.",
                "position": 231
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/x6.png",
                "caption": "Figure 6:Ablation Study.(a) Ablations on hyperparameters time steptùë°titalic_t, subject‚Äôs prior extraction stepŒ≥ùõæ\\gammaitalic_Œ≥, and the posterior threshold for binarization of the subject‚Äôs prior masksœÑùúè\\tauitalic_œÑ. (b) Ablation on transformer blocks, where Block 19 shows superior ability for separation between subjects.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x7.png",
                "caption": "Figure 7:Qualitative Comparisons with Multi Composition Methods.We compare our method with multi-composition methods operating on multiple LoRA adapters,LoRAShopoutperforms the competing approaches while not relying on a pose input, and thus generate compositions with diverse settings.",
                "position": 562
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix ADetails of User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/user_study_setup.png",
                "caption": "Figure 8:User Interface of our User Study.",
                "position": 1265
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Generation and Editing Examples",
        "images": []
    },
    {
        "header": "Appendix CAdditional Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/x8.png",
                "caption": "Figure 9:Multi-subject composition results on four human subjects. As our approach does not rely on any other external conditioning like pose conditioning,LoRAShopcan utilize the generative capabilities of FLUX, and thus generate outputs with high fidelity and superior prompt alignment. In the provided examples, we utilize the concepts<Margot>,<Gal>,<Kiernan>and<Beer>.",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x9.png",
                "caption": "Figure 10:Multi-subject composition results for three subjects. We provide generation results for the subjects<Pitt>,<Elon>and<DiCaprio>. We provide generation results on three different generation prompts, with different compositions of the subjects.",
                "position": 1288
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x10.png",
                "caption": "Figure 11:Multi-subject composition results for two subjects. We provide generation results for the concepts<Armas>,<Sabrina>,<Pitt>,<DiCaprio>. As we demonstrate in the examples,LoRAShopcan perform compositions between the same type (e.g. woman-woman) and different type (e.g. man-woman) of identity concepts.",
                "position": 1291
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x11.png",
                "caption": "Figure 12:Multi-subject composition results generated by our method on different types of objects. As can be seen in the examplesLoRAShopcan perform combinations between different types of concepts.",
                "position": 1294
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x12.png",
                "caption": "Figure 13:Face Swapping results withLoRAShop. As we demonstrate in the provided examples, our editing approach offers a seamless blending between the input subject and the target identity, while preserving the input characteristics.",
                "position": 1297
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x13.png",
                "caption": "Figure 14:Face Swapping results withLoRAShop.",
                "position": 1300
            },
            {
                "img": "https://arxiv.org/html/2505.23758/x14.png",
                "caption": "Figure 15:Comparison with state-of-the-art multi composition methods, on two subject generation task.",
                "position": 1303
            }
        ]
    },
    {
        "header": "Appendix DDetailed Masking and Blending Algorithm",
        "images": []
    },
    {
        "header": "Appendix EExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/ana_icon.png",
                "caption": "Table 5:Image-and-text comparison table.",
                "position": 1541
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/billie_icon.png",
                "caption": "",
                "position": 1581
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/watson_icon.png",
                "caption": "",
                "position": 1597
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/gal_icon.png",
                "caption": "",
                "position": 1613
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/kiernan_icon.png",
                "caption": "",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/margot_icon.png",
                "caption": "",
                "position": 1645
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/stone_icon.png",
                "caption": "",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/beer_icon.png",
                "caption": "",
                "position": 1677
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/sabrina_icon.png",
                "caption": "",
                "position": 1693
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/swift_icon.png",
                "caption": "",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/dicaprio_icon.png",
                "caption": "Table 6:Image-and-text comparison table.",
                "position": 1727
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/pitt_icon.png",
                "caption": "",
                "position": 1767
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/lee_icon.png",
                "caption": "",
                "position": 1783
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/elon_icon.png",
                "caption": "",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/messi_icon.png",
                "caption": "",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/glasses_icon.png",
                "caption": "Table 7:Image-and-text comparison table.",
                "position": 1833
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/jacket_icon.png",
                "caption": "",
                "position": 1873
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/dress_icon.png",
                "caption": "",
                "position": 1889
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/tower_icon.png",
                "caption": "",
                "position": 1905
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/cat_icon.png",
                "caption": "",
                "position": 1921
            },
            {
                "img": "https://arxiv.org/html/2505.23758/extracted/6491799/figs/icons/royce_icon.png",
                "caption": "",
                "position": 1937
            }
        ]
    },
    {
        "header": "Appendix FList of LoRA Adapters",
        "images": []
    }
]