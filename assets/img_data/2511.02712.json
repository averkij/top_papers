[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02712/x1.png",
                "caption": "Figure 1:Selected examples of inputs and outputs obtained from VidEmo. Apart from providing toolkits for basic attribute perception and expression analysis (top), VidEmo extends the cognitive capacity and is able to generate fine-grained emotional captions with explainable rationale (bottom).",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02712/x2.png",
                "caption": "Figure 2:Results Overview.Our best model,VidEmo-T1, shows superior performance across 15 face perception tasks, surpassing advanced milestone (Gemini 2.0: 5thFeb, 2025) on 14 of 15 tasks.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VidEmo: Video Emotion Foundation Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02712/x3.png",
                "caption": "Figure 3:Pipeline of VidEmo.(a) Training: The model is trained using curriculum emotion learning, divided into three stages: attribute, expression, and emotion tuning. A reference model provides initial parameters, and a policy model is trained with reward feedback.\n(b) Reasoning: The policy model performs hierarchical reasoning by sampling from the best attributes, expressions, and emotions to generate the final emotional output.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x4.png",
                "caption": "Figure 4:Visualizationon attribute perception, expression analysis, and emotion understanding.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Emo-CFG: Emotion-Centric Fine-Grained Video Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02712/x5.png",
                "caption": "Figure 5:Data Curation Pipeline of the Emo-CFG dataset.(a) The source of data from 17 datasets.\n(b) The illustration of data labeling steps.\n(c) The illustration of data verification loop.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x6.png",
                "caption": "Figure 6:Data Statistics of our Emo-CFG dataset.(a) The data taxonomy from three types of face perception tasks.\n(b) The temporal and spatial distribution of video data.\n(c) The data label distribution and examples.\n(d) The comparison with other emotion and video datasets.",
                "position": 318
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation and Training Details",
        "images": []
    },
    {
        "header": "Appendix BEmo-CFG Dataset Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02712/x7.png",
                "caption": "Figure 7:Length Distribution of caption data from the sources of Emo-CFG.",
                "position": 7410
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x8.png",
                "caption": "Figure 8:Length Distribution of rationale data from the sources of Emo-CFG.",
                "position": 7419
            }
        ]
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Settings",
        "images": []
    },
    {
        "header": "Appendix EFull Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02712/x9.png",
                "caption": "Figure 12:Visualization comparison results for appearance recognition and appearance caption.",
                "position": 8391
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x10.png",
                "caption": "Figure 13:Visualization comparison results for action recognition and action caption.",
                "position": 8394
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x11.png",
                "caption": "Figure 14:Visualization comparison results for head pose estimation and identity verification.",
                "position": 8397
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x12.png",
                "caption": "Figure 15:Visualization comparison results for open attribute perception.",
                "position": 8400
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x13.png",
                "caption": "Figure 16:Visualization comparison results for single-label emotion recognition, multi-label emotion recognition and fine-grained emotion recognition.",
                "position": 8403
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x14.png",
                "caption": "Figure 17:Visualization comparison results for micro-expression detection and action unit detection.",
                "position": 8406
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x15.png",
                "caption": "Figure 18:Visualization comparison results for single-label sentiment recognition and fine-grained sentiment recognition.",
                "position": 8409
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x16.png",
                "caption": "Figure 19:Visualization comparison results for conversation reasoning and emotion caption.",
                "position": 8412
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x17.png",
                "caption": "Figure 20:Visualization comparison results for fine-grained emotion caption. We achieve competitive performance with Gemini 2.0 on six different metrics.",
                "position": 8415
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x18.png",
                "caption": "Figure 21:Visualization samples for attribute perception in classification-type tasks.",
                "position": 8439
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x19.png",
                "caption": "Figure 22:Visualization samples for attribute caption in caption-type tasks.",
                "position": 8442
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x20.png",
                "caption": "Figure 23:Visualization samples for expression analysis.",
                "position": 8445
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x21.png",
                "caption": "Figure 24:Visualization samples for emotion understanding in fine-grained caption task.",
                "position": 8448
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x22.png",
                "caption": "Figure 25:Visualization samples for emotion understanding in rationale analysis task.",
                "position": 8451
            },
            {
                "img": "https://arxiv.org/html/2511.02712/x23.png",
                "caption": "Figure 26:Visualization samples of meta labels of face boxes, face landmarks and parsing masks.",
                "position": 8454
            }
        ]
    },
    {
        "header": "Appendix FVisualization",
        "images": []
    }
]