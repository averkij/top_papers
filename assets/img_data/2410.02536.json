[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02536/extracted/5911414/figures/complexity_fig1_v5.png",
                "caption": "Figure 1:Our framework for investigating the link between complexity and intelligence. We pretrain Large Language Models (LLMs) on Elementary Cellular Automata (ECAs) from different complexity classes using next-token prediction, then evaluate them on downstream reasoning and chess move prediction tasks. We use various measures to analyze the complexity of ECA rules, and quantify the relationship between complexity and downstream performance.",
                "position": 265
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02536/extracted/5911414/figures/complexity_vs_perf/complexity_vs_perf.png",
                "caption": "Figure 2:Relationship between downstream task performance and rule complexity.Left:Eight representative ECA rules, two from each of Wolframâ€™s four complexity classes. Performance of models trained on these rules is highlighted in the top row.Top:Model performance in relation to the Lempel-Ziv complexity of each rule. The left and center panels show efficiency (1 divided by number of epochs to reach 80% validation accuracy) for the easy and hard reasoning tasks, respectively. The right panel shows move prediction accuracy for the chess task. The rules depicted on the left are highlighted in the plot with triangles and annotated with the rule number. The correlation coefficient is shown in the top-left corner of each plot. An asterisk next to the value indicates a significant relationship (p<0.05ð‘0.05p<0.05italic_p < 0.05).Bottom:Downstream task performance based on Wolfram classification of each rule. Models trained on chaotic and complex rules perform better than models trained on uniform and simple rules.",
                "position": 350
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02536/x1.png",
                "caption": "Figure 3:Relationship between downstream task performance and rule complexity for other complexity measures. Rows depict easy reasoning, hard reasoning, and chess move prediction tasks, while columns show compression complexity, Lyapunov exponent, and Krylov complexity, respectively. We observe the same general patterns that we see with the Lempel-Ziv complexity in Figure2.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2410.02536/x2.png",
                "caption": "Figure 4:Attention scores for the final 10 states prior to the target state, showing that models trained on more complex rules rely more heavily on past states for prediction.Left:Visualization of the last 10 states and the target state for representative rules from each of Wolframâ€™s complexity classes.Center:Attention scores for each of the last 10 states, highlighting that models trained on complex and chaotic rules focus more on recent states, while models trained on uniform rules exhibit consistently low attention. Periodic rules demonstrate a repeating attention pattern, suggesting that the model is learning to attend to earlier cycles of the same state rather than general state history.Right:Average attention across the final 10 states for all rules, plotted against Lempel-Ziv complexity. A strong positive correlation (r=0.66) indicates that models trained on higher complexity rules attend more highly to historical states during prediction.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2410.02536/x3.png",
                "caption": "Figure 5:Performance comparison between models trained on short-term and long-term prediction tasks. Each point represents an ECA rule, with the x-axis showing the performance of models trained on short-term (1-step) prediction and the y-axis showing performance of models trained on long-term (5-step) prediction. The points are colored by the Lempel-Ziv complexity of each rule. The dashed line represents equal performance between the two models. Points below the line indicate better performance for short-term prediction models.",
                "position": 400
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVisualizing Learned Representations Using CKA Similarities",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.02536/x4.png",
                "caption": "Figure 6:UMAP projection of CKA similarities between models trained on ECA rules. Each point represents a model, colored by its corresponding rule complexity (Lempel-Ziv, Compression, Lyapunov, and Krylov complexities). Models trained on rules with similar complexities cluster together, indicating that they learn similar representations. Chaotic rules like Rule 105 and 150 are notably closer to models trained on lower complexity rules, while models trained on complex, non-chaotic rules cluster more distinctly.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2410.02536/x5.png",
                "caption": "Figure 7:UMAP projection of CKA similarities between models trained on ECA rules, annotated with the rule number.",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2410.02536/x6.png",
                "caption": "Figure 8:Efficiency comparison between models trained on short-term and long-term prediction tasks, colored by the other complexity measures. We observe the same trend as with Lempel-Ziv complexity in Figure5.",
                "position": 1225
            }
        ]
    },
    {
        "header": "Appendix BLong vs Short Term Prediction with other complexity measures",
        "images": []
    }
]