[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21181/x1.png",
                "caption": "Figure 1:Cross-modal hallucinations and their mitigation through Modality-Adaptive Decoding (MAD).The base model hallucinates non-existent visual content (red text) and audio events (blue text) when describing audio-visual inputs. MAD eliminates these hallucinations by adaptively suppressing cross-modal interference, producing accurate descriptions grounded in actual content.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2601.21181/x2.png",
                "caption": "Figure 2:Overall MAD pipeline.Given audio-visual inputs and a question, MAD extracts modality-adaptive weights by querying the model to identify relevant modalities [Step 1].\nDuring generation, MAD fuses contrastive logits computed from four modality configurations using these weights to dynamically emphasize relevant modalities [Step 2].\nIn this example, despite a hammer being visible in the video, MAD correctly predicts ``No'' by prioritizing audio content, whereas the baseline predicts ``Yes'' due to cross-modal interference. This demonstrates MAD's effectiveness in mitigating video-driven audio hallucinations through adaptive, question-aware decoding.",
                "position": 195
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21181/x3.png",
                "caption": "Figure 3:Distribution of extracted modality weights(wv,wa,wa​v)(w_{v},w_{a},w_{av})across question types on video. The weights align with intuitive modality dependencies, confirming that MAD correctly identifies the required modalities without supervision.",
                "position": 438
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Impact ofγ\\gamma",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21181/figures/fig_suppl_a1.png",
                "caption": "Figure 4:Impacts ofγ\\gammain VideoLLaMA2-AV",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2601.21181/figures/fig_suppl_a2.png",
                "caption": "",
                "position": 1818
            }
        ]
    },
    {
        "header": "7Qualitative Analysis of Modality-Adaptive Weights",
        "images": []
    },
    {
        "header": "8Modality Weight Distribution Analysis Across Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21181/x4.png",
                "caption": "Figure 6:Analysis on AVHBench",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2601.21181/x5.png",
                "caption": "Figure 7:Analysis on CMM",
                "position": 1990
            }
        ]
    },
    {
        "header": "9Robustness Analysis of Modality Query Prompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21181/figures/fig_suppl3.png",
                "caption": "Figure 8:Performance consistency across different modality query prompts on AVHBench and CMM benchmarks using VideoLLaMA2-AV. Circles denote mean±\\pmstandard deviation; vertical bars show minimum and maximum accuracy across all five prompt variants.",
                "position": 2065
            }
        ]
    },
    {
        "header": "10Computational Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21181/x6.png",
                "caption": "Figure 9:Qualitative Results in VideoLLaMA2-AV",
                "position": 2117
            },
            {
                "img": "https://arxiv.org/html/2601.21181/x7.png",
                "caption": "Figure 10:Qualitative Results in Qwen2.5-Omni",
                "position": 2121
            }
        ]
    },
    {
        "header": "11Qualitative Results",
        "images": []
    }
]