[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21749/x1.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LeX-Art",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21749/x2.png",
                "caption": "Figure 2:Illustration of LeX-Art Framework.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x3.png",
                "caption": "Figure 3:The framework of data construction pipeline.Theredwords in the R1 enhanced prompt are not rendered in the generated image, and it is fixed after the knowledge-augmented recaption by gpt-4o.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x4.png",
                "caption": "Figure 4:Images generated by FLUX.1 [dev][25]based on different prompts.The origin caption from the first raw to the bottom raw: (1) A poster with the words Good Music remixed and unreleased on it, with text on it: “UNRELEASED”, “REMIXED”, “GOOD.MUSIC”, “KANYEWEST”, “SPERIOD”. (2) A movie poster, with text on it: “AFACE”, “WITHOUT”, “EYES”, “DOL”, “JUL”. (3) A menu of a fast food restaurant that contains “Sandwich Combo”, “Grilled Chicken”, “Lettuce”, “Tomato”, “Mayo”, “Fries&Drink”, and “Pepsi”.",
                "position": 201
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x5.png",
                "caption": "Figure 5:Image quality score and image aesthetics score distribution of AnyText dataset[43]and LeX-10K.We randomly sampled 10K data entries from AnyWord-3M. Using Q-Align[50], we calculated the quality scores and aesthetic scores for these 10K data entries along with the images in LeX-10K, and visualized the distributions of these two types of scores. We observed that LeX-10K generally has higher quality scores and aesthetic scores overall.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x6.png",
                "caption": "Figure 6:Overview of LeX-Bench.Prompts in LeX-Bench are split into three levels: 630 Easy-Level (2–4 words), 480 Medium-Level (5–9 words), and 200 Hard-Level (10–14 words). Prompts of the easy level also contain text attributes: color, font, position.",
                "position": 333
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21749/extracted/6315872/figures/win_tie_lose.png",
                "caption": "Figure 7:Human preference result on text accuracy, text recall rate and aesthetics for LeX-Lumina.For ease\nof illustration, we visualize the proportion of votes where LeX-Lumina wins, loses and ties with Lumina-Image 2.0.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x7.png",
                "caption": "Figure 8:Qualitative comparison between LeX-Lumina, LeX-FLUX and glyph-conditioned models.We compare our models with AnyText[43]and TextDiffuser[5]for five different prompts. We observe that our models generally achieve high fidelity, better text attribute controllability and higher aesthetics.",
                "position": 1208
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x8.png",
                "caption": "Figure 9:Qualitative comparison between Lumina-Image 2.0[35]and LeX-Lumina.The first column shows Lumina-Image-2.0 without LeX-Enhancer using Simple Caption; the second column shows the trained LeX-Lumina without LeX-Enhancer using Simple Caption; the third column shows Lumina-Image-2.0 with LeX-Enhancer enabled; and the fourth column shows LeX-Lumina with LeX-Enhancer enabled. We observe that (1) LeX-Lumina exhibits a better text rendering capability in terms of text fidelity and aesthetics; (2) LeX-Enhancer exhibits a strong capability for enhancing simple prompts.",
                "position": 1211
            },
            {
                "img": "https://arxiv.org/html/2503.21749/x9.png",
                "caption": "Figure 10:Qualitative comparison between FLUX.1 [dev][25]and LeX-FLUX.The first column shows FLUX.1 [dev] without LeX-Enhancer using Simple Caption; the second column shows the trained LeX-FLUX without LeX-Enhancer using Simple Caption; the third column shows FLUX.1 [dev] with LeX-Enhancer enabled; and the fourth column shows LeX-FLUX with LeX-Enhancer enabled. We observe that (1) LeX-FLUX exhibits a better text rendering capability in terms of text fidelity and text attributes controllability; (2) LeX-Enhancer exhibits a strong capability for enhancing simple prompts.",
                "position": 1214
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInstruction Used in Prompt Enhancer",
        "images": []
    },
    {
        "header": "Appendix BInstruction Used in Knowledge-Augmented Recaption",
        "images": []
    },
    {
        "header": "Appendix CInstruction Used in Prompt Refinement (LeX-Bench Curation)",
        "images": []
    },
    {
        "header": "Appendix DThe Format of Text Conditions",
        "images": []
    },
    {
        "header": "Appendix ENormalized Edit Distance Algorithm",
        "images": []
    },
    {
        "header": "Appendix FData Samples from LeX-10K",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21749/x10.png",
                "caption": "Figure 11:Comparison of data samples from AnyWord-3M[43], MAION-10M[5]and LeX-10K.",
                "position": 2295
            }
        ]
    },
    {
        "header": "Appendix GThe Evaluation of Text Attributes",
        "images": []
    },
    {
        "header": "Appendix HMore Generated Samples of LeX-FLUX and LeX-Lumina",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21749/x11.png",
                "caption": "Figure 12:Showcase of text rendering results from LeX-Lumina (first two rows) and LeX-FLUX (last two rows) on text-to-image tasks. The examples demonstrate the models’ ability to generate clear, well-aligned, and aesthetically pleasing text within images.",
                "position": 2339
            }
        ]
    },
    {
        "header": "Appendix IJustification of Pairwise Normalized Edit Distance (PNED) as a Metric",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21749/extracted/6315872/figures/pned.png",
                "caption": "Figure 13:Validation of PNED metric through controlled perturbation experiments. The solid blue line represents PNED scores with maintained sequence order, while the dashed red line shows scores with shuffled sequences.",
                "position": 2394
            }
        ]
    },
    {
        "header": "Appendix JSelf-Enhancement and Knowledge Distillation",
        "images": []
    }
]