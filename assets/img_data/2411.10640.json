[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10640/x1.png",
                "caption": "Figure 1:Comparison with mainstream MLLMs.We compare the performance of several mainstream MLLMs with a parameter count similar to that of BlueLM-V-3B across multiple benchmarks. BlueLM-V-3B leads in the majority of datasets.",
                "position": 88
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10640/x2.png",
                "caption": "Figure 2:Model architecture of BlueLM-V-3B.The architecture of BlueLM-V-3B follows the classical LLaVA approach. We integrate a dynamic resolution processing module (as in LLaVA-NeXT[70]and InternVL 1.5[22]) to enhance model capabilities and apply token downsampling to reduce deployment complexity.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x3.png",
                "caption": "Figure 3:Existing methods overly enlarge images.(A) For LLaVA-NeXT, an image with resolution 394√ó\\times√ó390 selects a 2:2 aspect ratio and is resized and padded to 768√ó\\times√ó768 (4√ó\\times√óarea enlargement). (B) For InternVL 1.5, an image with resolution 380√ó\\times√ó76 chooses a 5:1 aspect ratio and is directly resized to 1920√ó\\times√ó384 (25√ó\\times√óarea enlargement). BlueLM-V-3B, in contrast, selects a 1:1 aspect ratio for both resolutions, resulting in the minimum number of image tokens after ViT encoding, which can facilitate both model training and deployment.",
                "position": 125
            }
        ]
    },
    {
        "header": "3BlueLM-V-3B",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10640/x4.png",
                "caption": "Figure 4:Batched image encoding on NPU.We design a parallel processing scheme for image patches on the NPU. The figure illustrates the case of 4 patches being processed in parallel.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x5.png",
                "caption": "Figure 5:Pipeline parallelism in image encoding.We design a pipeline parallelism scheme for image encoding. The Conv2D layer in the vision embedding module of SigLIP (on the CPU) and the vision transformer blocks (on the NPU) for different image patches run parallel to improve inference speed. This image illustrates the pipeline parallelism scheme combined with batched image patch encoding.",
                "position": 209
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x6.png",
                "caption": "Figure 6:Overall framework of deploying BlueLM-V-3B.We decouple ViT image processing from user instruction (text or audio) handling to enhance overall efficiency. The text responses by LLM can be further converted on the fly to audio responses.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Training Recipe",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10640/x7.png",
                "caption": "Figure 7:ViT inference time for 2:4 resolution aspect ratio.We experiment with 1, 2, 4, and 6 image patches per batch on the NPU, using a 2:4 resolution aspect ratio (comprising one global patch and 8 local patches). Overall, processing 4 patches per batch delivers the fastest performance.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x8.png",
                "caption": "Figure 8:Latency and output speed comparison.We compare the latency and output generation speed with processing different numbers of input tokens in parallel. t{xùë•xitalic_x}/t1 implies processingxùë•xitalic_xinput tokens in parallel. The output token is fixed to 1 per trunk as the LLM can only generate one token for each forward process.",
                "position": 672
            }
        ]
    },
    {
        "header": "6Conclusion and Future Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Relaxed Aspect Ratio Matching",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10640/x9.png",
                "caption": "Figure 9:Case study.(A) LLaVA-NeXT chooses resolution 384√ó\\times√ó768 for an image with the original size of 380√ó\\times√ó393. (B) InternVL 1.5 chooses resolution 1920√ó\\times√ó384 for an image with the original size of 500√ó\\times√ó102.",
                "position": 2934
            }
        ]
    },
    {
        "header": "8Open-source Training Dataset",
        "images": []
    },
    {
        "header": "9Hyper-parameters for Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.10640/x10.png",
                "caption": "",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x11.png",
                "caption": "",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x12.png",
                "caption": "",
                "position": 3173
            },
            {
                "img": "https://arxiv.org/html/2411.10640/x13.png",
                "caption": "",
                "position": 3176
            }
        ]
    },
    {
        "header": "10Visualization on Practical Cases",
        "images": []
    }
]