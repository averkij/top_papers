[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07812/x1.png",
                "caption": "Figure 1:Counting performance under different settings.Left (Unbalanced):We compare different LVLMs by analyzing the trade-off between the number of query images and the total number of images without controlling for number of instances.Mid (Balanced): We fix the total number of images to 7 and of object instances distributed across query images to 4,3 and 2. In both settings, performance consistently drops when instances are spread over multiple images.Right (Multi Concept): We increase the complexity by adding more classes (concepts) to the counting task, and observe a steep performance drop, indicating limited capacity for multi-concept tracking.",
                "position": 175
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Challenges and Insights in Multi-Image LVLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07812/x2.png",
                "caption": "Figure 2:MIMIC Bench: examples of each task.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x3.png",
                "caption": "Figure 3:Effect of vision token sequence length on performance.Left: Sequence length reduction via 1-D pooling. The square denotes the original sequence length.Right: Control experiment reducing the information via pixel space pooling while keeping the sequence length fixed. Results are reported for counting task with 3 query images and total 10 images.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x4.png",
                "caption": "Figure 4:Inter-image and intra-image token attention across layers.The attention patterns transitions from cross-image to intra-image interactions as we advance in depth.",
                "position": 366
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07812/x5.png",
                "caption": "Figure 5:Answer-to-Image Attention:The baseline LLaVA OV (top row) fails to attend to the potted plant in the third image, whereas our method (bottom row) correctly focuses on the relevant object. Visualization is shown at the 15th layer of the LLM.",
                "position": 1259
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.07812/x6.png",
                "caption": "Figure 6:Performance vs. Number of Images.\nWe report performance as a function of the total number of input images for the Listing, Odd-One tasks, and common tasks.",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x7.png",
                "caption": "Figure 7:Our Masked Attention.Vision tokens are restricted to attend only to tokens from the same image, following a block-diagonal attention pattern, while text tokens in both the prefix and suffix follow the standard autoregressive attention.",
                "position": 2271
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x8.png",
                "caption": "Figure 8:Comparison on Counting (balanced). Our fine-tuned model performs substantially better when object instances are distributed across multiple images. For example, when four instances are spread across four images, performance improves from 9% to 45.8%, indicating enhanced cross-image information aggregation.",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x9.png",
                "caption": "Figure 9:Inter-image and intra-image token attention across layers for 6 images.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x10.png",
                "caption": "Figure 10:Counting (unbalanced) performance with bigger and latest models. We analyze the trade-off between the number of query images and the total number of images for bigger (LLaVA-OV 72B) and latest models (Qwen2.5-7B and Qwen3VL-8B).",
                "position": 2284
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x11.png",
                "caption": "Figure 11:Samples from the MIMIC training dataset.Unlike the evaluation benchmark, the training data follows the LLaVA data format and includes multi-turn conversations with option-based answers.",
                "position": 2288
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x12.png",
                "caption": "Figure 12:Prompt Templates for Different Tasks.For each task, a prompt is constructed by randomly sampling one template from the task-specific template set (PtaskP_{\\text{task}}) and one from the connector template set (PconnectorP_{\\text{connector}}). The two templates are then combined to form the final prompt, i.e.,P=Ptask∥PconnectorP=P_{\\text{task}}\\|P_{\\text{connector}}.",
                "position": 2396
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x13.png",
                "caption": "Figure 13:Our MIMIC Evaluation Benchmark. We show some samples from the category ‘common’ from our benchmark.",
                "position": 2399
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x14.png",
                "caption": "Figure 14:Our MIMIC Evaluation Benchmark. We show some samples from the category ‘Odd One’ from our benchmark.",
                "position": 2402
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x15.png",
                "caption": "Figure 15:Our MIMIC Evaluation Benchmark. We show some samples from the category ‘Listing’ from our benchmark.",
                "position": 2405
            },
            {
                "img": "https://arxiv.org/html/2601.07812/x16.png",
                "caption": "Figure 16:Our MIMIC Evaluation Benchmark. We show some samples from the category ‘Counting’ from our benchmark.",
                "position": 2409
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]