[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/extracted/6314241/images/book.png",
                "caption": "",
                "position": 108
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/x1.png",
                "caption": "Figure 1:Overview of SemLA. During test-time, SemLA uses CLIP as a domain navigator, to retrieve and fuse relevant adapters, to get a LoRA tailored to the target domain.",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method and Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/x2.png",
                "caption": "Figure 2:Construction and Expansion of the LoRA Adapter Library.Each LoRA adapter is created by fine-tuning on a specific dataset and subsequently added to the library. The library index for each adapter is represented by the CLIP centroid of its training data.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Experiment Setup and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/x3.png",
                "caption": "Figure 3:Adapter contribution heatmap. Rows represent individual test datasets, and columns correspond to specific LoRA adapters. The color intensity of each cell indicates the frequency and weight of selection (with values below 0.1 omitted). The diagonal is empty due to the leave-one-out strategy.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x4.png",
                "caption": "Figure 4:Adapter weight distribution for MUSES-Fog-Night.The fused adapter combines knowledge from foggy and night-time conditions by weighting relevant adapters. Adapters with a weight lower than 5% are not included.",
                "position": 981
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x5.png",
                "caption": "Figure 5:CLIP-guidance effectiveness for LoRA selection on ACDC.Each point represents an image-adapter combination, with adapters separated by color. x-axis: distance from the corresponding image embedding to the adapter embedding. y-axis: improvement in mIoU when using the adapter relative to the zero-shot base network. The linear regression curve (dashed line) indicates that embedding similarity correlates with higher mIoU.\nWe show the full adapter library, excluding those trained on ACDC.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x6.png",
                "caption": "Figure 6:Qualitative results on ACDC and NYU.From left to right: Input Image (RGB), ground-truth semantic masks (GT) and predictions by the Zero-Shot model[10], Uniform Merging[35], and SemLA.",
                "position": 1170
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/x7.png",
                "caption": "",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x8.png",
                "caption": "Figure 8:Adapters weight distribution for each benchmark dataset.Each pie chart is divided into sections proportional to the average contribution provided by each adapter based on CAT-Seg leave-one-out adaptation settings.",
                "position": 2542
            }
        ]
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BInterpretability of the LoRA Library",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/x9.png",
                "caption": "Figure 9:Correlation between LoRA support score and mIoU.Higher support scores correlate with better segmentation performance.",
                "position": 2661
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x10.png",
                "caption": "Figure 10:Hyper-parameters Study.Impact ofKùêæKitalic_K(number of adapters) andœÑùúè\\tauitalic_œÑ(temperature) on overall performance (mIoU).",
                "position": 2664
            }
        ]
    },
    {
        "header": "Appendix CAblations and Analysis",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21780/x11.png",
                "caption": "(a)ACDC Fog",
                "position": 3226
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x11.png",
                "caption": "(a)ACDC Fog",
                "position": 3229
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x12.png",
                "caption": "(b)BDD",
                "position": 3234
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x13.png",
                "caption": "(c)Cityscapes (CS)",
                "position": 3240
            },
            {
                "img": "https://arxiv.org/html/2503.21780/x14.png",
                "caption": "(d)NYU",
                "position": 3245
            }
        ]
    },
    {
        "header": "Appendix EDiscussion: Real-World Deployment",
        "images": []
    }
]