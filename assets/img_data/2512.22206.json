[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3CosineGate Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22206/figure2_cosinegate_geometry.png",
                "caption": "Figure 2:Geometric intuition behind Cosine Incompatibility Ratio (CIR).\nWhen the residual vectorFi​(xi)F_{i}(x_{i})is directionally aligned withxix_{i},\nthe update is redundant and can be skipped. Orthogonal residuals introduce\nnew representational directions and should be computed.",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2512.22206/ICR_Gate.png",
                "caption": "Figure 3:CosineGate routing pipeline.\nCIR and controller outputs define gate logits, which are relaxed via\nGumbel-Softmax during training and thresholded at inference.\nThe FLOPs penaltyλflops\\lambda_{\\text{flops}}operates only at the loss level\nthroughg¯\\overline{g}and influences gates indirectly via backpropagation.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2512.22206/higherlevel.png",
                "caption": "Figure 4:Network-level CosineGate architecture.\nEach residual block is augmented with an independent gate,\nenabling adaptive FLOPs allocation across network depth.",
                "position": 841
            },
            {
                "img": "https://arxiv.org/html/2512.22206/higherlevel.png",
                "caption": "Figure 4:Network-level CosineGate architecture.\nEach residual block is augmented with an independent gate,\nenabling adaptive FLOPs allocation across network depth.",
                "position": 844
            },
            {
                "img": "https://arxiv.org/html/2512.22206/deepdive.png",
                "caption": "Figure 5:Single CosineGate block.\nCIR measures directional novelty, Gumbel-Softmax enables\ndifferentiable routing, and hard thresholds ensure deterministic inference.",
                "position": 851
            }
        ]
    },
    {
        "header": "4Results and Experimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22206/TrainAcc2.png",
                "caption": "Figure 6:Training accuracy over 160 epochs (Balanced).",
                "position": 1130
            },
            {
                "img": "https://arxiv.org/html/2512.22206/TrainAcc2.png",
                "caption": "Figure 6:Training accuracy over 160 epochs (Balanced).",
                "position": 1133
            },
            {
                "img": "https://arxiv.org/html/2512.22206/TestAcc2.png",
                "caption": "Figure 7:Test accuracy over 160 epochs (Balanced).",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2512.22206/flops_skip_comparison.png",
                "caption": "Figure 8:FLOPs utilization and skip rates across training (Balanced). Progressive FLOPs pressure induces a stable compute regime without early collapse.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2512.22206/pareto_frontier.png",
                "caption": "Figure 9:Accuracy–efficiency Pareto frontier induced by CosineGate across Aggressive/Balanced/Conservative configurations.",
                "position": 1147
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    }
]