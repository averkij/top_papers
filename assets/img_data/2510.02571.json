[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02571/x1.png",
                "caption": "Figure 1:Video models are unable to express their uncertainty, posing a critical limitation especially in tasks where they lack requisite knowledge. Here, the video model generates an inaccurate video (showing Albert Einstein), when prompted to generate a video of Jeff Einstein. To this end, we introduce ametricfor evaluating the calibration of video models, acalibrated uncertainty quantification method(S-QUBED) which uses latent modeling to disentangle aleatoric and epistemic uncertainty, and aUQ datasetfor benchmarking calibration.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Uncertainty Quantification of Generative Video Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02571/x2.png",
                "caption": "Figure 2:S-QUBEDarchitecture.Given a text promptℓ\\ell, our goal is to quantify the uncertainty of the video generation model. We first generatennlatent prompts consistent withℓ\\ellin line with the prompt refinement used by video models, modeling the aleatoric uncertainty as the entropy of the distribution over latent prompts. Then, for each latent prompt, we generatemmvideos, modeling the epistemic uncertainty as the conditional entropy of the distribution over generated videos. Finally, aggregating the two types of uncertainties yields the total predictive uncertainty.",
                "position": 167
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.02571/x3.png",
                "caption": "Figure 3:Calibration Metrics for Video Models.Top: We examine the statistical significance of the Kendall rank correlation between uncertainty and widely-used perceptual metrics. We find that the CLIP cosine similarity score provides the most significant correlation.Bottom: With the CLIP accuracy metric, we observe that low human-annotated uncertainty corresponds to smaller variance in the generated videos and greater accuracy with respect to the ground-truth video. As uncertainty increases, video prediction accuracy decreases.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2510.02571/x4.png",
                "caption": "Figure 4:Total Predictive Uncertainty for Video Models.We assess the calibration of the total predictive uncertainty computed by S-QUBED.Top: correlation between video prediction accuracy and total uncertainty for Panda-70M and VidGen-1M . We observe a statistically significant correlation between accuracy and uncertainty for both datasets, signified by the smallpp-values.Bottom: visualization of two samples from Panda-70M.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2510.02571/x5.png",
                "caption": "Figure 5:Disentangling Aleatoric and Epistemic Uncertainty for Video Models.We demonstrate the calibration of the aleatoric uncertainty estimates of S-QUBED in tasks with no epistemic uncertainty, showing statistically significant negative correlation. We do the same for epistemic uncertainty.",
                "position": 541
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Appendix",
        "images": []
    }
]