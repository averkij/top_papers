[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17008/x1.png",
                "caption": "Figure 1:Comparison of advantage computation in GRPO, token-PPO, and turn-PPO. In turn-PPO, the state is defined assn:=(⊕n′<n(Qn′,Rn′))⊕Qns_{n}:=\\left(\\oplus_{n^{\\prime}<n}(Q_{n^{\\prime}},R_{n^{\\prime}})\\right)\\oplus Q_{n}and the action asan:=Rna_{n}:=R_{n}. For the critic in token-PPO and turn-PPO, the position ofV^h\\hat{V}_{h}in the figure indicates that it is conditioned on all tokens up to that point.",
                "position": 347
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17008/x2.png",
                "caption": "Figure 2:In the first two plots, we show GRPO validation reward curves during training on Webshop and Sokoban for Qwen2.5 and Qwen3. In the third one, we show rewards for GRPO and its variants with respect to std dev, KL, and batch size diversity. In the last one, we show evolution of standard deviation throughout training.",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2512.17008/x3.png",
                "caption": "Figure 3:Comparison of turn-PPO and token-PPO in mean reward across multiple settings and and clipping ratio for Sokoban. Turn-PPO shows superior performance in most settings, highlighting the benefit of turn-level advantage estimation.",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2512.17008/x4.png",
                "caption": "Figure 4:Ablation studies on (left) number of diverse samples in a batch, (middle) discount factorγ\\gamma, and (right) bias–variance trade-off parameterλ\\lambda, showing their impact on mean reward using WebShop and Qwen3 with reasoning.",
                "position": 623
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining details",
        "images": []
    },
    {
        "header": "Appendix BTrajectory examples",
        "images": []
    }
]