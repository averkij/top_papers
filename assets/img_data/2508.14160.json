[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14160/x1.png",
                "caption": "Figure 1:RynnECis a video multi-modal large language model (MLLM) specifically designed for embodied cognition tasks. It can accept inputs interwoven from video, region masks, and text, and produce output in the form of text or masks based on the question. RynnEC is capable of addressing a diverse range of object and spatial questions within embodied contexts and plays a significant role in indoor embodied tasks.",
                "position": 160
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14160/x2.png",
                "caption": "Figure 2:Embodied Cognition Question-Answer (QA) Data Generation Pipeline:First, objects within the scene are segmented from the video. Subsequently, object and spatial QA pairs are generated via two distinct branches.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2508.14160/x3.png",
                "caption": "Figure 3:Overview of embodied cognition dimensions in RynnEC-Bench.RynnEC-Bench includes two subsets: object cognition and spatial cognition, evaluating a total of 22 embodied cognitive abilities.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2508.14160/x4.png",
                "caption": "Figure 4:Training paradigm of RynnEC.The model is trained in four progressive stages:\n1) Mask Alignment, 2) Object Understanding, 3) Spatial Understanding, and 4) Referring Segmentation.",
                "position": 376
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14160/x5.png",
                "caption": "Figure 5:More granular assessments of object cognition and spatial cognition.We compare the best-performing MLLM from each category with our RynnEC-7B.",
                "position": 1192
            },
            {
                "img": "https://arxiv.org/html/2508.14160/x6.png",
                "caption": "Figure 6:Performance on VSI-Bench[69].Left: per-subtask comparison with VideoLLaMA3, the base model of our RynnEC. Right: overall comparison with generalist MLLMs and embodied MLLMs without explicit 3D encoding.",
                "position": 1250
            },
            {
                "img": "https://arxiv.org/html/2508.14160/x7.png",
                "caption": "Figure 7:The example of RynnEC assisting robots in performing long-range tasks.The robot accomplishes the two designated tasks within the RoboTHOR simulator[14]. RynnEC facilitates the robot in achieving fine-grained environmental cognition throughout the task execution.",
                "position": 1343
            }
        ]
    },
    {
        "header": "5Conclusion and Future Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.14160/figures/append-qual/qual-all-1.png",
                "caption": "Figure 8:Visualization of question answering examples.Part\n1 out of 2.",
                "position": 2979
            },
            {
                "img": "https://arxiv.org/html/2508.14160/figures/append-qual/qual-all-3.png",
                "caption": "Figure 9:Visualization of question answering examples.Part\n2 out of 2.",
                "position": 2983
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]