[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x1.png",
                "caption": "",
                "position": 99
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x2.png",
                "caption": "Figure 2:Method Overview.We propose a novel method to feed-forwardly reconstruct personalized skinned avatars via a universal clothed human model. Specifically, givenNùëÅNitalic_Nframes of posed human images{ùêài}subscriptùêàùëñ\\{\\mathbf{I}_{i}\\}{ bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }from front and back views, we first estimate their normal and segmentation images, and then unpose them for each frame and view to produce pixel-aligned initial conditions in a 3D canonicalization process (Section3.1). Next, we propose to aggregate mult-frame references and produce a single bi-plane featureùêÅùêÅ\\mathbf{B}bold_Bas the representation of the subject identity. By sampling from this feature, we jointly decode personalized canonical avatar meshùêåùêå\\mathbf{M}bold_M, skinning weightsùêñùêñ\\mathbf{W}bold_Wand pose-dependent vertex displacementŒî‚Å¢ùêïŒîùêï{\\Delta}\\mathbf{V}roman_Œî bold_V(Section3.2) from a canonical tetrahedral grid. Finally, we adopt a multi-stage training process to train the model with posed-space ground truth and canonical-space regularization (Section3.3).",
                "position": 128
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x3.png",
                "caption": "Figure 3:Qualitative Comparison.Our method produces superior animation quality when reposed to an unseen pose for challenging poses, body shapes and cloth types, which reduces deformation artifacts,e.g.stretched triangles, and generates plausible wrinkles.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2503.19207/extracted/6298057/sec/images/gene.png",
                "caption": "Figure 4:Method Generalizability.We show the pretrained universal model can directly apply to causally taken photos and synthetic images from Renderpeople[2], which demonstrates its practical applications. When applied to phone photos, we do not require perfect alignment of front and back views and useestimatedposes from monocular images for canonicalization. More details are in appendix.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x4.png",
                "caption": "Figure 5:Effects of multi-frame aggregation.Given a set of unposed normal frames from different poses in the left, we show results of fused canonical shapes using the firstNùëÅNitalic_Nframes at each column in the right. we observe that aggregation from multiple frames produces more plausible canonical shapes by correcting unposing artifacts,e.g.on skirts and hairs, while preserving person-specific details.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x5.png",
                "caption": "Figure 6:Effects of canonicalization.By taking canonicalization results as inputs, the universal model can learn to reduce unposing artifacts and preserve fine-grained details, compared to directly sampling features from posed inputs via forward warping as[17].",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x6.png",
                "caption": "Figure 7:Effects of personalized skinning weights.We show personalized skinning weights reduce deformation artifacts,e.g.under armpit, and can be more robustly estimated when trained with multiple input and target frames. Note we show results deformed by LBS only,i.e.without pose-dependent deformation.",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x7.png",
                "caption": "Figure 8:Effects of Pose-dependent Deformation.The deformation module can correct LBS artifacts (wrist and arm bending in first row) and generate plausible garment dynamics (sleeves draping in second row), which improves animation realism.",
                "position": 504
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "A. Data Acquisition",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/extracted/6298057/sec/images/fig_data.png",
                "caption": "Figure 9:Samples of dome data.Our dataset contains diverse posed clothed humans paired with high-quality 3D scans as ground truths, which facilitates learning an effective universal prior.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x8.png",
                "caption": "Figure 10:Unposing Comparison.We compare the results between naive unposing (used in the inference pipeline) and pseudo GT via optimization (used for data preparation). The second approach produces more plausible results but requires significantly more time. Note we filter edges with length larger than1√ó10‚àí41superscript1041\\times 10^{-4}1 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPTto reduce noises.",
                "position": 548
            }
        ]
    },
    {
        "header": "B. Canonicalization Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/extracted/6298057/sec/images/rebuttal1.png",
                "caption": "Figure 11:Illustration of settings for photos.We useestimatedbody poses and do not require perfect alignment between views.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x9.png",
                "caption": "Figure 12:Illustration of Lifted Surface Meshes.Note we removed the over-stretched edges after unposing. The lifting process produces two unposed surface meshes but can not be perfectly aligned in boundary.",
                "position": 569
            }
        ]
    },
    {
        "header": "C. More Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x10.png",
                "caption": "Figure 13:Visualization in Four Views.By only taking inputs of front and back views, our method can infer plausible side-view geometry and produce a consistent boundary.",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x11.png",
                "caption": "Figure 14:Results of Inferred Body Shape.Our method can produce personalized body shapes based on input conditions and is not restricted to the template shape.",
                "position": 581
            }
        ]
    },
    {
        "header": "D. Network Architecture",
        "images": []
    },
    {
        "header": "E. More Animation Comparison",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x12.png",
                "caption": "Figure 15:Animation comparison with SCANimate.For[49], we use FRESA reconstructions as reference posed meshes. Note that hand motions are missing as it is SMPL-based.",
                "position": 615
            }
        ]
    },
    {
        "header": "Comparison with Reconstruction Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x13.png",
                "caption": "Figure 16:Qualitative comparison with single image reconstruction methods.Our method produces high-quality geometry details comparable to ECON[60], SIFU[67], and PSHuman[28]on both dome data and phone photos.",
                "position": 624
            }
        ]
    },
    {
        "header": "F. Texture Reconstruction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/extracted/6298057/sec/images/texture_b.png",
                "caption": "Figure 17:Results of Textured Meshes.Our method can be extended to produce high-resolution texture for realistic rendering.",
                "position": 633
            }
        ]
    },
    {
        "header": "G. Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19207/x14.png",
                "caption": "Figure 18:Failure Cases.With only the pose vector as condition, our method fails to produce complex hair motions and dynamics of extremely loose garments.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x15.png",
                "caption": "Figure 19:Model Architecture for multi-frame encoderfe‚Å¢(‚ãÖ)subscriptùëìùëí‚ãÖf_{e}(\\cdot)italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ( ‚ãÖ ).Note we stack two views together and omit the superscriptvùë£vitalic_v. The final bi-plane feature is obtained by summing the feature for each frameùêÅisubscriptùêÅùëñ\\mathbf{B}_{i}bold_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.‚äïdirect-sum\\oplus‚äïdenotes channel-wise concatenation.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x16.png",
                "caption": "Figure 20:Model Architecture for canonical geometry decoderfg‚Å¢(‚ãÖ)subscriptùëìùëî‚ãÖf_{g}(\\cdot)italic_f start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( ‚ãÖ ).",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x17.png",
                "caption": "Figure 21:Model Architecture for skinning weight decoderfs‚Å¢(‚ãÖ)subscriptùëìùë†‚ãÖf_{s}(\\cdot)italic_f start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( ‚ãÖ ).",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2503.19207/x18.png",
                "caption": "Figure 22:Model Architecture for pose-dependent vertex displacement decoderfc‚Å¢(‚ãÖ)subscriptùëìùëê‚ãÖf_{c}(\\cdot)italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ‚ãÖ ).",
                "position": 654
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]