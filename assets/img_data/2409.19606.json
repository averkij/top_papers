[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19606/x1.png",
                "caption": "Figure 1:The performance of the baseline modelOLMoE-1B-7Band the model with hyper-connections,OLMoE-1B-7B-DHC√ó\\times√ó4.(1)and(2)show the training loss (0.99 EMA smoothed) and the C4-en validation loss, respectively. Our method converges 1.8 times faster compared to the baseline and maintains a significant advantage at the 500B tokens.(3)and(4)show the accuracy curves onHellaSwagandARC-Challenge, demonstrating the superior performance of theOLMoE-1B-7B-DHC√ó\\times√ó4model.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x2.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x3.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x4.png",
                "caption": "",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x5.png",
                "caption": "Figure 2:Hyper-connections with expansion raten=2ùëõ2n=2italic_n = 2.(a) The highlightedgreenandblueconnections compose depth-connections between the output of the layer and the blue hidden vector. (b) The highlightedblueandyellowconnections represent a part of width-connections between the hidden vectorsh1subscript‚Ñé1h_{1}italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(blue) andh2subscript‚Ñé2h_{2}italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(yellow). (c) The matrix representation of hyper-connections.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Method",
        "images": []
    },
    {
        "header": "3Why Hyper-Connections",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19606/x6.png",
                "caption": "Figure 4:Sequential and parallel arrangements of hyper-connections withn=2ùëõ2n=2italic_n = 2.",
                "position": 461
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19606/x7.png",
                "caption": "Figure 5:Comparison of training loss curves for different expansion rate. The left subfigure includes models with dynamic hyper-connections (DHC) at various expansion rates, while the right subfigure shows the effect of omitting the tanh function. Both subfigures illustrate how increasing the expansion rate leads to improved training loss performance over500500500500B tokens. Results are smoothed using an exponential moving average with a coefficient of 0.99.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x8.png",
                "caption": "",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x9.png",
                "caption": "Figure 6:(1)and(2)Training loss (0.99 EMA smoothed) and C4-en validation loss forOLMo-7BandOLMo-7B-DHC√ó\\times√ó4models.(3)and(4)Accuracy curves onhellaswagandsciq, demonstrating the superior performance of theOLMo-7B-DHC√ó\\times√ó4model.",
                "position": 1088
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x10.png",
                "caption": "",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x11.png",
                "caption": "",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2409.19606/x12.png",
                "caption": "",
                "position": 1107
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMoE 1B/7B Model Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19606/x13.png",
                "caption": "Figure 8:Loss curves in V3 validation set and accuracy curves on downstream tasks forOLMoE-1B7BandOLMoE-1B7B-DHC√ó4absent4\\times{4}√ó 4models.",
                "position": 1842
            }
        ]
    },
    {
        "header": "Appendix B7B Model Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19606/x14.png",
                "caption": "Figure 9:Loss curves in V3 validation set and accuracy curves on downstream tasks forOLMo-7BandOLMo-7B-DHC√ó\\times√ó4models.",
                "position": 1850
            }
        ]
    },
    {
        "header": "Appendix CVision Experiments",
        "images": []
    },
    {
        "header": "Appendix DMore Visualization and Analysis",
        "images": []
    },
    {
        "header": "Appendix EDerivation of Non-Trainable Hyper-Connection Matrix for Residual Connections",
        "images": []
    },
    {
        "header": "Appendix FSequential-Parallel Duality",
        "images": []
    },
    {
        "header": "Appendix GPseudocode of Hyper-connections",
        "images": []
    },
    {
        "header": "Appendix HPyTorch Implementation of Hyper-connections",
        "images": []
    },
    {
        "header": "Appendix IValidation Sets and Downstream Tasks",
        "images": []
    },
    {
        "header": "Appendix J1B Model Experiments",
        "images": []
    }
]