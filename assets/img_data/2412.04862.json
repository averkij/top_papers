[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04862/x1.png",
                "caption": "Figure 1:A procedure of instruction-tuning data construction. First, we extract the core knowledge from large-volume web corpora and classify it within the taxonomy we defined in advance. Next, instruction-tuning data is generated based on the knowledge. To construct additional training data that is more complex, we leverage an instruction-evolving method[58]that lets the final dataset cover various fields with varying levels of difficulty.",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2412.04862/x2.png",
                "caption": "Figure 2:Overview of the preference optimization pipeline. (Top) Preference Data Creation: It shows the process of constructing preference data{x,yw,yl}ùë•subscriptùë¶ùë§subscriptùë¶ùëô\\{x,y_{w},y_{l}\\}{ italic_x , italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }by scoring the responsesyùë¶yitalic_ygenerated from a model for the promptxùë•xitalic_xusing a reward model. (Bottom) Preference Optimization: Sequential training process whereM0subscriptùëÄ0M_{0}italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTinitialized from the SFT model is trained through DAA to obtainM1subscriptùëÄ1M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandM2subscriptùëÄ2M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.",
                "position": 441
            }
        ]
    },
    {
        "header": "3Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04862/x3.png",
                "caption": "Figure 3:NIAH results of EXAONE 3.5 language models. The x-axis represents the token length of the input text, while the y-axis shows the relative position within the text, expressed as a percentage (0% corresponds to the beginning, and 100% to the end). The results are represented using a color-coded scheme: green indicates successful retrievals, and red represents unsuccessful ones. EXAONE 3.5 language models achieve near-perfect accuracy in retrieving information across various document depths and context lengths in English and Korean.",
                "position": 1831
            }
        ]
    },
    {
        "header": "4Responsible AI",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Deployment",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix AContributors",
        "images": []
    },
    {
        "header": "Appendix BModel License",
        "images": []
    },
    {
        "header": "Appendix CDecontamination Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04862/extracted/6036302/figures/decontam_fixed.png",
                "caption": "Figure 4:A summary of the decontamination method employed to train EXAONE 3.5 language models. Adopting an approach borrowed from the GPT-4 method, we increase the number of random sample toN=10ùëÅ10N=10italic_N = 10for stricter decontamination.",
                "position": 4132
            }
        ]
    },
    {
        "header": "Appendix DEvaluation Details",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]