[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20615/x1.png",
                "caption": "Figure 1:Comparison of video avatar generation approaches. (a) Speech-driven and pose-driven methods produce passive motions with limited semantic understanding. In contrast, (b) ourOnlineReasoning andCognitiveArchitecture (ORCA) enables complex, multi-step task execution through OTAR (Observe-Think-Act-Reflect) closed-loop reasoning.",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20615/x2.png",
                "caption": "Figure 2:Overview of the ORCA framework. ORCA operates through a closed-loop OTAR cycle: Observe updates internal world state from generated clips, Think (System 2) decomposes tasks and predict next state, Act (System 1) translates subgoals into action captions for I2V generation, and Reflect verifies completion to accept/reject outcomes. This hierarchical dual-system architecture enables robust long-horizon task execution through continuous state tracking and adaptive replanning.",
                "position": 217
            }
        ]
    },
    {
        "header": "4L-IVA Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20615/x3.png",
                "caption": "Figure 3:L-IVA Benchmark Overview.Top: Statistical analysis showing (left) balanced scene distribution across 5 categories, (center) data source composition with 92 synthetic and 8 real images, and (right) task complexity distribution averaging 5.0 sub-goals per task.Bottom: Representative scenes from our benchmark including Garden, Kitchen, and livestream scenarios, demonstrating diverse real-world settings requiring multi-step object interactions.",
                "position": 500
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20615/x4.png",
                "caption": "Figure 4:Qualitative Comparison on Transfer Plant Task.We compare four methods on long-horizon video generation.Top:Ground truth subgoals for reference.Red boxesindicate execution failures or error accumulation.\nOpen-Loop planner cannot detect execution errors.\nReactive agent lacks world state knowledge, leading to repetitive actions.\nVAGEN’s I2V errors corrupt the final state without reflection.ORCA (Ours)successfully completes all subgoals with consistent execution quality.",
                "position": 508
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Supplementary Material",
        "images": []
    },
    {
        "header": "8Benchmark Construction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20615/figs/pipe_new.png",
                "caption": "Figure 5:Overview of the L-IVA Benchmark Construction Pipeline.(a) Our data curation process employs a hybrid strategy:Pipeline Asources real-world images from Pexels, filtered by scene affordance and annotated via Gemini-2.5-Pro.Pipeline Butilizes a goal-first design for synthetic data, where scenes are generated by Nanobanana to strictly align with intended interactions. (b) A representative scene image (e.g., ”Checking beehives”) from the benchmark. (c) The corresponding structured metadata (YAML), including object inventory, high-level intention, subgoals, and reference prompts.",
                "position": 943
            }
        ]
    },
    {
        "header": "9Evaluation Protocols",
        "images": []
    },
    {
        "header": "10Prompt Details for ORCA",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.20615/x5.png",
                "caption": "Figure 6:Qualitative Analysis of Failure Cases attributed to Foundation Model Limitations.(a)Temporal Information Loss:Due to discrete frame sampling, the VLM misses the ”teleportation” artifact where the fertilizer bag instantaneously appears in the hand (red arrow), falsely accepting it as a valid pickup.\n(b)Lack of 3D Spatial Awareness:The VLM misinterprets the depth of the scene, instructing the avatar to fetch a watering can that is actually in the distant background, resulting in an unnatural reaching motion.\n(c)Weak Instruction Following:For fine-grained tasks like ”light the alcohol lamp,” the I2V model consistently fails to execute the interaction despite ORCA triggering multiple retries (separated by dashed lines).\n(d)Object Disappearance:A clear example of generative instability where a key object (the water bottle) vanishes mid-clip (red box) despite no interaction occurring.",
                "position": 1122
            },
            {
                "img": "https://arxiv.org/html/2512.20615/x6.png",
                "caption": "Figure 7:Prompt for Initialization module and observer",
                "position": 1171
            },
            {
                "img": "https://arxiv.org/html/2512.20615/x7.png",
                "caption": "Figure 8:Prompt for Thinking, System-1 and Reflector",
                "position": 1174
            },
            {
                "img": "https://arxiv.org/html/2512.20615/x8.png",
                "caption": "Figure 9:The user interface for human evaluation. For each test case, annotators are presented with a high-level goal (Intention) and generated videos from four anonymized methods (Video A and Video B are shown here; C and D are omitted for brevity). Evaluators are asked to assess each video based on two criteria: (1) a Physical Score (1-5 Likert scale) regarding simulation stability, and (2) a Subgoals Checklist to verify task completion. Finally, they select the best and worst videos based on overall quality.",
                "position": 1177
            }
        ]
    },
    {
        "header": "11Discussion on Failure Cases and Limitations",
        "images": []
    }
]