[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19930/x1.png",
                "caption": "Figure 1:Domain-Specific Performance of AdaMLLM and General MLLM.For each of the two domains—biomedicine and food—we conduct post-training to adapt the general MLLM to the target domain and evaluate model performance on various domain-specific tasks.Biomedicineandfoodtasks are coloredgrayandorange, respectively.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19930/x2.png",
                "caption": "Figure 2:Method Overview.(A) We fine-tune a unified visual instruction synthesizer that generates diverse tasks based on image-caption pairs across various domains. (B) Using this synthesizer, we synthesize tasks based on domain-specific image-caption pairs and then apply a consistency-based data filter. The filtered synthetic tasks, combined with the original image captioning tasks, are employed to train general MLLMs through a single-stage post-training process, MLLMtraining lossis computed only on the part colored inorange.",
                "position": 216
            }
        ]
    },
    {
        "header": "4Experiment Settings",
        "images": []
    },
    {
        "header": "5Main Results",
        "images": []
    },
    {
        "header": "6Ablations",
        "images": []
    },
    {
        "header": "7Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19930/x3.png",
                "caption": "Figure 3:Task Type Distributionof all our synthetic tasks based on three image-caption sources.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2411.19930/x4.png",
                "caption": "Figure 4:Cases of Instruction-Response Pairssynthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for cases (A), (B), (C) are Recipe1M,PMCR⁢a⁢wsuperscriptPMC𝑅𝑎𝑤\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPTandPMCR⁢e⁢f⁢i⁢n⁢e⁢dsuperscriptPMC𝑅𝑒𝑓𝑖𝑛𝑒𝑑\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively. Certain portions are omitted and are represented as (…).",
                "position": 1103
            }
        ]
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASeed Data Construction and Distribution",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19930/x5.png",
                "caption": "Figure 5:Distribution of Image Domains and Task Types in Seed Data.",
                "position": 2034
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details of Visual Instruction Synthesis",
        "images": []
    },
    {
        "header": "Appendix CMLLM Post-Training Settings and Costs",
        "images": []
    },
    {
        "header": "Appendix DScoring Criteria for Data Quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19930/x6.png",
                "caption": "Figure 6:Prompt Template for Consistency-Based Filter (Part 1), continued in Part 2.",
                "position": 2743
            },
            {
                "img": "https://arxiv.org/html/2411.19930/x7.png",
                "caption": "Figure 7:Prompt Template for Consistency-Based Filter (Part 2).",
                "position": 2746
            },
            {
                "img": "https://arxiv.org/html/2411.19930/x8.png",
                "caption": "Figure 8:Cases of Instruction-Response Pairs (Part 1)synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for the cases are Recipe1M,PMCR⁢a⁢wsuperscriptPMC𝑅𝑎𝑤\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPTandPMCR⁢e⁢f⁢i⁢n⁢e⁢dsuperscriptPMC𝑅𝑒𝑓𝑖𝑛𝑒𝑑\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively. Continued in Part 2. In the first case, the rule-based task simply transforms the recipe caption, ignoring the image content. In contrast, our task involves analyzing the food’s state in the image and applying food-related knowledge to infer its texture, demonstrating a higher level of\\uldomain knowledge utilization. In the second case, the GPT-4 generated task straightforwardly asks about the pointing of the red arrow, while ours requires a detailed analysis and inference, showing greater\\ultask complexity.",
                "position": 3217
            },
            {
                "img": "https://arxiv.org/html/2411.19930/x9.png",
                "caption": "",
                "position": 3221
            },
            {
                "img": "https://arxiv.org/html/2411.19930/x10.png",
                "caption": "Figure 9:Cases of Instruction-Response Pairs (Part 2)synthesized by our method, manual rules, GPT-4, and GPT-4V, the image-caption sources for the cases are Recipe1M,PMCR⁢a⁢wsuperscriptPMC𝑅𝑎𝑤\\text{PMC}^{Raw}PMC start_POSTSUPERSCRIPT italic_R italic_a italic_w end_POSTSUPERSCRIPTandPMCR⁢e⁢f⁢i⁢n⁢e⁢dsuperscriptPMC𝑅𝑒𝑓𝑖𝑛𝑒𝑑\\text{PMC}^{Refined}PMC start_POSTSUPERSCRIPT italic_R italic_e italic_f italic_i italic_n italic_e italic_d end_POSTSUPERSCRIPT, respectively. In this case, our task stands out as a multiple-choice question, showcasing\\ultask diversity.",
                "position": 3225
            }
        ]
    },
    {
        "header": "Appendix ETask Evaluation Details",
        "images": []
    }
]