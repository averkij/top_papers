[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05795/x1.png",
                "caption": "Figure 1:Layerwise output variance.This figure compares the output variance across various layers for different setups: (1) Pre-LN; (2) Pre-LN with Scaled Initialization; and (3) LayerNorm Scaling. The experiments are conducted on the LLaM-130M model trained for 10,000 steps. The proposed LayerNorm Scaling effectively controls the variance across layers.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2502.05795/x2.png",
                "caption": "Figure 2:Performance drop of layer pruning across different LLMs.(a) BERT-Large (Post-LN), (b) Mistral-7B (Pre-LN), (c) Qwen-7B (Pre-LN), (d) DeepSeek-7B (Pre-LN), (e) LLaMA2-7B (Pre-LN), and (f) LLaMA2-13B (Pre-LN). The results show that Pre-LN models exhibit significant inefficiency in deeper layers, while Post-LN models maintain strong deep-layer contributions.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Empirical Evidence of the Curse of Depth",
        "images": []
    },
    {
        "header": "3Analysis of the Curse of Depth",
        "images": []
    },
    {
        "header": "4LayerNorm Scaling",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05795/x3.png",
                "caption": "Figure 3:Comparison between Pre-LN (a) and LayerNorm Scaling (b). LayerNorm Scaling applies a scaling factor inversely proportional to the square root of the layer indexlùëôlitalic_l, preventing excessive variance growth and stabilizing training dynamics across layers.",
                "position": 442
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05795/x4.png",
                "caption": "Figure 4:Performance drop of layer pruning on LLaMA-130M. LayerNorm Scaling enables deep layers to make a meaningful contribution to the model.",
                "position": 867
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProofs of the Theorems",
        "images": []
    },
    {
        "header": "Appendix BTraining Loss Curve",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.05795/x5.png",
                "caption": "Figure 5:Training loss of LLaMA-1B with Pre-LN and LayerNorm Scaling.",
                "position": 2579
            },
            {
                "img": "https://arxiv.org/html/2502.05795/x6.png",
                "caption": "Figure 6:Variance growth across layers in LLaMA-130M with Pre-LN.Each subplot shows the variance at different training stages (1000, 3000, and 6000 epochs).\nIn all cases, the variance follows an exponential growth pattern as depth increases, indicating that deeper layers experience uncontrolled variance amplification regardless of training progress.",
                "position": 2589
            }
        ]
    },
    {
        "header": "Appendix CVariance Growth in Pre-LN Training",
        "images": []
    }
]