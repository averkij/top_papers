[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12806/x1.png",
                "caption": "Figure 1:User interface of the MCPEval framework. The dashboard provides streamlined access to core functionalities such as automatic task generation, verification, model evaluation, and result analysis. It integrates real-time activity tracking and system overviews to ensure transparency and ease of use.",
                "position": 244
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12806/x2.png",
                "caption": "Figure 2:Two-step MCP-based task generation workflow, including initial generation phase and verification phase.",
                "position": 286
            }
        ]
    },
    {
        "header": "3MCPEvalFramework",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12806/x3.png",
                "caption": "Figure 3:MCPEvalevaluation workflow shows MCP client/server interaction, tool call correctness checking, LLM judger assessment, automated report generation.",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Results and Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12806/x4.png",
                "caption": "Figure 4:Domain performance analysis: (a) Domain ranking by LLM judger, (b) Trajectory vs completion comparison, (c) Task distribution, (d) Performance gaps by domain.",
                "position": 875
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x5.png",
                "caption": "Figure 5:Performance gap analysis: (a) Overall gap distribution, (b) Model-wise gaps, (c) Domain-wise gaps, (d) Gap-performance correlation.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x6.png",
                "caption": "Figure 6:Model performance analysis from tool call anlysis and llm judger.",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x7.png",
                "caption": "Figure 7:Trajectory vs Completion performance correlation across domains.",
                "position": 911
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Broader Impact",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComprehensive Experiments and Results",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Criteria",
        "images": []
    },
    {
        "header": "Appendix CModel Versions",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12806/x8.png",
                "caption": "Figure 8:Model performance analysis: (a) Overall ranking by LLM judger, (b) Trajectory vs completion performance scatter plot, (c) Model category comparison, (d) Top 5 models by task type.",
                "position": 1932
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x9.png",
                "caption": "Figure 9:Domain performance analysis: (a) Domain ranking by LLM judger, (b) Trajectory vs completion comparison, (c) Task distribution, (d) Performance gaps by domain.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x10.png",
                "caption": "Figure 10:Aspect performance analysis: (a) Trajectory aspects (LLM judger), (b) Completion aspects (LLM judger), (c) Trajectory aspects (individual tasks), (d) Completion aspects (individual tasks).",
                "position": 2190
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x11.png",
                "caption": "Figure 11:Performance gap analysis: (a) Overall gap distribution, (b) Model-wise gaps, (c) Domain-wise gaps, (d) Gap-performance correlation",
                "position": 2367
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x12.png",
                "caption": "Figure 12:Correlation analysis between tool call evaluation and LLM judger assessment: (a) Tool Call vs LLM Judge Combined scores, (b) Tool Call vs LLM Judge Trajectory scores, (c) Tool Call vs LLM Judge Completion scores, (d) LLM Judge Trajectory vs Completion scores. Each point represents a model-domain combination, colored by domain.",
                "position": 2577
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x13.png",
                "caption": "Figure 13:Tool name vs parameter matching performance analysis: Scatter plot showing the correlation between average name match scores and average parameter match scores across all models, averaged across domains. Each point represents one model’s overall performance in the two fundamental aspects of tool calling.",
                "position": 2661
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x14.png",
                "caption": "Figure 14:Comprehensive performance analysis: (a) Domain performance across different evaluation metrics, (b) Tool name vs parameter matching correlation, (c) Model category comparison between OpenAI and open-source models, (d) Top 5 models by task type performance ranking.",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x15.png",
                "caption": "Figure 15:Domain performance across evaluation metrics: Line chart showing how each benchmark domain (Airbnb, Healthcare, Sports, National Park, Finance) performs across four key metrics - Name Match Score, Parameter Match Score, Order Match Score, and Overall Score. Each line represents one domain’s performance profile across the evaluation dimensions.",
                "position": 2680
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x16.png",
                "caption": "Figure 16:Summary of different tasks’ performance.",
                "position": 2819
            }
        ]
    },
    {
        "header": "Appendix DMore Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.12806/x17.png",
                "caption": "Figure 17:Summary of different models’ performance on the Airbnb task.",
                "position": 2830
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x18.png",
                "caption": "Figure 18:Summary of different models’ performance on the Yahoo Finance task.",
                "position": 2833
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x19.png",
                "caption": "Figure 19:Summary of different models’ performance on the Healthcare task.",
                "position": 2836
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x20.png",
                "caption": "Figure 20:Summary of different models’ performance on the Sports task.",
                "position": 2839
            },
            {
                "img": "https://arxiv.org/html/2507.12806/x21.png",
                "caption": "Figure 21:Summary of different models’ performance on the National Park task.",
                "position": 2842
            }
        ]
    },
    {
        "header": "Appendix EComprehensive Related Work Discussions",
        "images": []
    }
]