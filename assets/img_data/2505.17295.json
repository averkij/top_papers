[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17295/x1.png",
                "caption": "",
                "position": 64
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x2.png",
                "caption": "Figure 1:Overview and motivation behind the ScanBot dataset.\n(a) Embodied AI must generalize not only across tasks and environments, but also across tools, each with distinct control and perception demands.\n(b) Gripper tasks involve discrete object interaction, while scanner tasks require precise region localization and smooth, continuous motion.\n(c) Traditional laser scanning follows fixed, task-agnostic paths, leading to inefficient coverage and wasted time on irrelevant areas.\n(d) ScanBot includes 6 real-world components and 6 3D-printed shapes, enabling 6 task types and 4 evaluation capabilities for instruction-conditioned surface scanning.",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3ScanBot Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17295/x3.png",
                "caption": "Figure 2:Hardware setup of the ScanBot system. A UR3 robotic arm is equipped with a Keyence LJ-X8200 laser profiler and an Intel RealSense D435i RGB-D camera mounted on the end-effector. A GoPro HERO8 captures third-person views from a fixed tripod. The entire setup operates within a black-curtained environment to ensure consistent and interference-free measurements.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x4.png",
                "caption": "Figure 3:Overview of the 12 scanned objects in the ScanBot dataset. The top two rows show six real-world electronic components (four GPU boards, one RAM module, and one WiFi card) alongside their corresponding point clouds. The bottom two rows present six 3D-printed parts grouped into three comparison sets: (1) black and white triangles with no surface features, (2) two cubes with identical shape and color but different embossed patterns, and (3) two cylinders with identical features but different colors.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x5.png",
                "caption": "Figure 4:Multiview examples and annotated features in the ScanBot dataset. The first column shows first-person views captured by the Intel RealSense D435i mounted on the robot’s end-effector. The second column presents third-person overviews recorded by a fixed GoPro camera.\nThe third column highlights annotated object features.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x6.png",
                "caption": "Figure 5:Distribution of scanning tasks across six instruction types in the ScanBot dataset. The pie chart shows the number of tasks belonging to each task type, with values labeled inside each segment.",
                "position": 428
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Limitations and Future Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17295/x7.png",
                "caption": "Figure 6:Feature-level ground-truth scans for 3D-printed objects.",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x8.png",
                "caption": "Figure 7:Feature-level ground-truth scans for real-world electronic components.",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x9.png",
                "caption": "Figure 8:Measurement geometry of the Keyence LJ-X8200 scanner.\nAt a reference distance of 245 mm, the scanner captures a 72 mm-wide region with a 68 mm vertical measurement range.",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x10.png",
                "caption": "Figure 9:Scanner profile previews used for height calibration. (a) Profile centered at reference height. (b) Profile drops low when the scanner is too far. (c) Profile shifts upward when too close.",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x11.png",
                "caption": "Figure 10:Example of an instruction-conditioned scanning trajectory. The task “Scan the top surface of the white cube” is executed using first-person (top row) and third-person (bottom row) camera views, showing each stage of the scanning process.",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2505.17295/x12.png",
                "caption": "Figure 11:Examples of failed surface reconstructions caused by inappropriate configurations.\n(a) A clean, high-fidelity result under optimal settings.\n(b) Low robot speed causes vertical stretching and noise accumulation.\n(c) High robot speed results in sparse, incomplete coverage.\n(d) Short exposure time leads to missing geometry, particularly on darker surfaces.\n(e) Excessively long exposure creates oversaturation and color washout.\n(f) Low scanning frequency (10 Hz) produces distorted, flattened results due to motion undersampling.",
                "position": 1292
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    }
]