[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x1.png",
                "caption": "",
                "position": 437
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x2.png",
                "caption": "Figure 2:The overview of the explanation pipeline, where images are forwarded through the LMM with the integrated SAE, and the activations of the top 256 most activated features are cached. For each feature, the top 5 images with the highest activations are selected, followed by the execution of zero-shot image explanations using a large LMM.",
                "position": 483
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x3.png",
                "caption": "Figure 3:An overview of the evaluation pipeline for calculating IOU scores. Initially, a small LLM is used to refine the explanation into a concise description, which is then employed to generate the segmentation mask. The IOU score is subsequently computed by comparing the mask to the binarized activated region.",
                "position": 550
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x4.png",
                "caption": "Figure 4:A comparison of several visual concepts and their activated areas. We compare several visual concepts and their corresponding activated areas, showcasing one example for each concept across different features. For each feature, we calculate the IOU by averaging the IOUs from the top-5 activated images. Although some features yield relatively low IOU scores, we find that the explanations are still semantically accurate with respect to the activated regions.",
                "position": 650
            }
        ]
    },
    {
        "header": "4Probing into the Features",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x5.png",
                "caption": "Figure 5:The feature that relates to sad. We probe and find out the feature that activated on ”sad”. By clamping this feature, we can enforce the model to share the feeling of sad",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x6.png",
                "caption": "Figure 6:The feature that relates to happy. We find out that the feature is related with joy and celebrate action that relate to happiness. By clamping this feature, we can enforce the model to share the feeling happiness with others.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x7.png",
                "caption": "Figure 7:A feature that relates to the concept ”eat”. We further investigate about the concept behind this feature and find out that model can reason from a visual action ”eat” into the concept ”concept” and ”greedy”",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x8.png",
                "caption": "Figure 8:An example of the hallucination on LLaVA. Bolivia is not shown on the image but the model still answer yes.",
                "position": 860
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x9.png",
                "caption": "Figure 9:Feature that relates to the text ”Barcelona”. By clamping this feature to high value, we intervene the reasoning steps and hallucination inFig.8disappears.",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x10.png",
                "caption": "Figure 10:The high attribution area of different images and on the text. For images, we observe that features with high attribution mostly activate on positions that relate to key information about the question. For text, we observe that the ”Bolivia” token contributes the most to the answer ”yes”",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x11.png",
                "caption": "Figure 11:Feature that relates to the text ”Los”. We validate our assumption by finding another feature that relates to text and mitigate the hallucination.",
                "position": 869
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x12.png",
                "caption": "Figure 12:The feature related to money and its steering effect.",
                "position": 1456
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x13.png",
                "caption": "Figure 13:The feature related to speech and its steering effect.",
                "position": 1459
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x14.png",
                "caption": "Figure 14:The feature related to unix and its steering effect.",
                "position": 1462
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x15.png",
                "caption": "Figure 15:The feature related to chair and its steering effect.",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x16.png",
                "caption": "Figure 16:The feature related to money and its steering effect.",
                "position": 1468
            }
        ]
    },
    {
        "header": "8Detail about Prompt",
        "images": []
    },
    {
        "header": "9More Steering Examples",
        "images": []
    },
    {
        "header": "10CLIP-Score and IOU details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.14982/x17.png",
                "caption": "Figure 17:Low level features in the LMM. These features activate in most of the images and showcase the model’s basic cognition and perception abilities.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2411.14982/x18.png",
                "caption": "Figure 18:The feature related to money and its steering effect.",
                "position": 1715
            }
        ]
    },
    {
        "header": "11Feature Probing",
        "images": []
    },
    {
        "header": "12Low Level Perception Features Examples",
        "images": []
    }
]