[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09433/x1.png",
                "caption": "Figure 1:CaPa pipeline.\nWe first generate 3D geometry using a 3D latent diffusion model.\nUsing the learned 3D latent space with ShapeVAE, we train a 3D Latent Diffusion Model that generates 3D geometries, guided by multi-view images to ensure alignment between the generated shape and texture.\nAfter the 3D geometry is created, we render four orthogonal views of the mesh, which serve as inputs for texture generation.\nTo produce a high-quality texture while preventing the Janus problem, we utilize a novel, model-agnostic spatially decoupled attention.\nFinally, we obtain a hyper-quality textured mesh through back projection and a 3D-aware occlusion inpainting algorithm.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x2.png",
                "caption": "Figure 2:Spatially Decoupled Cross Attention.\nTo produce high-quality multi-view images for a given geometry, we design a model-agnostic Spatially Decoupled Cross Attention.\nDuring cross-attention in denoising U-Net, we replicate hidden feature channels so that each duplicated channels focuses solely on the designated view.\nSince the design is model-agnostic, we can utilize an external ControlNet to guide the textures aligned with the input mesh.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x3.png",
                "caption": "Figure 3:3D-Aware Occlusion Inpainting.First, we cluster the normal and spatial coordinates of the occluded face.\nUsing clustered centers as viewpoints, we create specialized UV maps through projection mapping.\nThis approach captures surface locality, allowing 2D diffusion-based inpainting to effectively fill occluded regions. Note that this UV map is utilized solely for occlusion.",
                "position": 436
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09433/extracted/6135590/figs/other_texture.png",
                "caption": "Figure 4:Comparison of Texturing Method.Unlike prior works, CaPa effectively resolved the Janus problem with consistent ID.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x4.png",
                "caption": "Figure 5:Qualitative Comparison of Image-to-3D Generation.\nWe compare CaPa with state-of-the-art Image-to-3D methods.\nHere, all the assets are converted topolygonal mesh, using its official code.\nThe proposed CaPa significantly outperforms both geometry stability and texture quality, especially for the back and side view’s visual fidelity and texture coherence.",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x5.png",
                "caption": "Figure 6:Ablation Study.(a) demonstrates that using multi-view guidance significantly increases the geometry quality.\n(b) shows our Spatially Decoupled Attention effectively resolves the Janus problem, achieving high-fidelity texture coherence,\n(c) reveals our occlusion inpainting outperforms previous inpainting methods like UV-ControlNet, presented in Paint3D[59].",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2501.09433/extracted/6135590/figs/scalability.png",
                "caption": "Figure 7:Scalability of CaPa.\n(a) Original result of CaPa.\n(b) 3D inpainting result using text-prompt (“orange sofa, orange pulp”). CaPa’s texture generation extends smoothly to 3D inpainting, stylizing the generated asset.\n(c) CaPa w/ LoRA[14]adaptation. The model-agnostic approach allows CaPa to leverage pre-trained LoRA (balloon style) without additional 3D-specific retraining.",
                "position": 718
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADiscussions and Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09433/x6.png",
                "caption": "Figure 8:Result of the CaPa with PBR Understanding.We demonstrate CaPa’s capability for disentangling physically based rendering (PBR) materials.\nThe figure shows PBR-aware generation results under various lighting conditions: ‘city,’ ‘studio,’ and ‘night,’ using Blender’s default environment settings[7].\nAs shown, CaPa effectively adapts to different light environments, highlighting its potential for PBR-aware asset generation.",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x7.png",
                "caption": "Figure 9:Additional Image-to-3D Results of CaPa.\nCaPa can generate diverse objects from textual, and visual input.\nThe result demonstrates our diversity across the various categories, marking a significant advancement in practical 3D asset generation methodologies.",
                "position": 1595
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x8.png",
                "caption": "Figure 10:Additional Comparison of Image-to-3D Generation.\nCaPa significantly outperforms both geometry stability and texture quality, especially for the back and side view’s visual fidelity and texture coherence.",
                "position": 1602
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09433/x9.png",
                "caption": "Figure 11:Impact of Spatially Decoupled Cross Attention on Janus Artifacts.\nIn this additional figure, We demonstrate the capability of Janus prevention in the proposed spatially decoupled cross-attention mechanism.\nEach row depicts, (a) with spatially decoupled cross attention, (b) without spatially decoupled cross attention, and (c) a mesh rendering of the current view, respectively.",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x10.png",
                "caption": "Figure 12:Qualitative results for different occlusion inpainting methods.\n(a) shows results from our 3D-aware occlusion inpainting method, (b) uses automatic view selection, and (c) employs UV ControlNet.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2501.09433/x11.png",
                "caption": "Figure 13:Text-to-3D Results of CaPa.\nCaPa can generate diverse objects from textual, and visual input.\nThe result underscores CaPa’s strengths in generating high-resolution textures that align with well-defined geometries.",
                "position": 1716
            },
            {
                "img": "https://arxiv.org/html/2501.09433/extracted/6135590/figs/remeshing_draft.png",
                "caption": "Figure 14:Results of Our Remeshing Algorithm.\nWe employ a carefully designed remeshing scheme after geometry generation for better practical usage for broader applications.\n(a) shows the original polygonal mesh, (b) shows remeshed output of quadrilateral faces, and (c) shows remeshed output of triangular faces.",
                "position": 1844
            }
        ]
    },
    {
        "header": "Appendix CAdditional Details of CaPa",
        "images": []
    }
]