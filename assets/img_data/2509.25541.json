[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25541/fig/robot-1.png",
                "caption": "Figure 1:Vision-Zero Paradigm.(a) Supervised learning depends on human-curated reasoning trajectories; (b) Reinforcement Learning, although enabling models to autonomously learn reasoning processes via validated rewards, still relies heavily on expert-designed question-answer pairs.\n(c) In contrast, Vision-Zero is a novel self-improvement paradigm entirely independent of human experience. It constructs self-play games by leveraging image pairs that exhibit visual differences.\nThrough the interactive and strategic game, Vision-Zero continuously generates training data for VLMs, enabling the model to achieve scalable self-improvement.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2509.25541/fig/robot-2.png",
                "caption": "",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2509.25541/fig/robot-3.png",
                "caption": "",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2509.25541/x1.png",
                "caption": "Figure 2:Performance Comparison of Vision-Zero with SOTA post-training methods.All models were post-trained on Qwen2.5-VL-7B. The numbers on the horizontal axis represent the accuracy of Qwen2.5-VL-7B on different tasks, while the vertical axis represents the change in accuracy of the trained model. Vision-Zero outperforms baselines trained on expensive human-labeled datasets.",
                "position": 156
            }
        ]
    },
    {
        "header": "2Vision-Zero: A Generalizable Gamification Training Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25541/x2.png",
                "caption": "Figure 3:Overall Framework of Vision-Zero.Vision-Zero comprises three core components.Strategic Game Environment:Each role is required to exhibit strategic behavior tailored to diverse scenarios, thereby simultaneously necessitating multiple capabilities.Label-free and Domain-agnostic Data Input:Vision-Zero accepts arbitrary inputs to promote diversity and generalization. To verify this, we train Qwen2.5-VL-7B for 100 iterations on Gobang and our environment and evaluate on MathVision; results show that Vision-Zero effective generalization.Iterative-SPO:We introduce a novel two-stage training algorithm. In the clue stage, models are trained via Self-Play using a zero-sum reward inversely proportional to votes received. In the decision stage, models undergo RLVR training with group normalization, using rewards based on vote correctness.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2509.25541/x3.png",
                "caption": "Figure 4:Visualization of the datasets used in Vision-Zero.We employ three representative data in our experiments: (left) CLEVR-based data, (middle) Chart-based data, and (right) Real-world data. For visualization, difference parts are circled, which are not present in the SPY images in game.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2509.25541/x4.png",
                "caption": "Figure 5:Visualization of spy reasoning in Vision-Zero.A comparison of model responses to identical scenarios before and after training, as evaluated by GPT-based scoring, reveals substantial improvements in planning, retrieval, decomposition, strategy formulation, and logical reasoning.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2509.25541/fig/win2.png",
                "caption": "Figure 6:Evolution of win rate and token length during Vision-Zero training.Win rates are evaluated over 100 rounds (50 civilian, 50 spy) against corresponding untrained reference models; civilians win by correctly identifying the spy. Token length are collected across these rounds.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2509.25541/fig/clue2.png",
                "caption": "",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2509.25541/fig/decision2.png",
                "caption": "",
                "position": 612
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25541/x5.png",
                "caption": "Figure 7:Performance Comparison between Iterative-SPO and pure Self-play / pure RLVR training.(left) Winning Rate (right) Performance on LogicVista.\nWe evaluate under three settings:\n(1) Iterative-SPO;\n(2) Pure Decision: Clue stage frozen, training only Decision stage via RLVR;\n(3) Pure Clue: Decision stage frozen, training only Clue stage via Self-Play.",
                "position": 1061
            },
            {
                "img": "https://arxiv.org/html/2509.25541/x6.png",
                "caption": "",
                "position": 1068
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]