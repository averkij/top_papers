[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3TimE: Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/1_dataset_overview_v0.png",
                "caption": "Figure 1:An overview ofTimE. The top-left block illustrates three key challenges of real-world complexity and their corresponding dataset construction. The bottom-left quadrant depicts a three-level tasks. One data example fromTimE-Dialis shown on the right.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/2_data_pipeline_v0-2.png",
                "caption": "Figure 2:Dataset construction pipeline forTimE. In the process of QA synthesis for each sub-dataset, we first collect temporal facts (temporal knowledge graphs forTimE-Wiki, time points forTimE-News, fact bank forTimE-Dial). Then timelines are generated for QA data synthesis.",
                "position": 334
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/combined_task_vectors_correlation_heatmap.png",
                "caption": "Figure 3:Task correlation heatmap highlighting the relationship betweenExtractandLocalizationtasks and other temporal reasoning tasks. Note:Extracttask is excluded fromTimE-Lite-Newsevaluation.",
                "position": 1277
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_wiki_demo.png",
                "caption": "Figure 4:An overview of theTimE-Wikibenchmark construction pipeline. Beginning with Wikidata as the data source, temporal facts are parsed using SLING. These facts are then used to construct multi-hop temporal knowledge graphs. Timelines for link entities are generated from these graphs by sorting temporal facts. Finally, these timelines are used to synthesize question-answer (QA) pairs, and corresponding context is generated by concatenating and paraphrasing stories derived from the timelines, forming the final QA tasks.",
                "position": 2090
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_tcelongbench_demo.png",
                "caption": "Figure 5:One example of temporal complex events in dataset[56]",
                "position": 2671
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_tcelongbench_days_num_distribution_of_len_days.png",
                "caption": "Figure 6:Frequency distribution of length of days in 600 selected Temporal Complex Events (TCEs) inTimE-News",
                "position": 2717
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_dial_data_source_demo.png",
                "caption": "Figure 7:A long-term multi-turn conversation example from theLoCoModataset[28]. The interlocutors, Joanna and Nate, are each assigned distinct persona characteristics. The conversation consists of multiple session units, each timestamped and containing several dialogue turns. Throughout the interaction, the participants engage in image sharing and response behaviors. Notably, certain dialogue segments require the speakers to leverage previously established shared memory to facilitate conversational progression.RealTalk[23]maintains an identical data format to the example illustrated in this figure.",
                "position": 2738
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_dial_demo.png",
                "caption": "Figure 8:An overview of the construction pipeline ofTimE-Dial. We employ real-world long-term multi-turn conversations as evaluation contexts. First, we extract and summarize event graphs from the conversations. Then, through a combination of LLM extraction and manual verification, we identify explicit temporal information and infer implicit temporal relations, standardizing them into unified temporal expressions. Subsequently, we temporally order the event graphs to construct individual timelines for each speaker. Finally, we generate question-answer pairs covering 11 subtasks based on these timeline representations.",
                "position": 2794
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_misleading_generation.png",
                "caption": "Figure 9:An illustration of the misleading option generation process. This example demonstrates a TCE question fromTimE-Newsregarding Netanyahuâ€™s planned address to Congress in April 2015, which falls under the explicit reasoning task category. Based on the gold answer from the free-form QA format, we employ an LLM to generate misleading options following our predefined principles, resulting in a single-choice QA format.",
                "position": 2974
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/appendix_TIME_News_synthesize_QAs.png",
                "caption": "Figure 10:An illustration of the QA synthesis process based on time points inTimE-News. We first utilize an LLM to generate contexts for the same entity, which are then organized into timeline contexts. Subsequently, for each QA task generation, we select relevant time points and their corresponding contexts to populate the prompt template, thereby synthesizing the QAs.",
                "position": 2979
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/login_page.png",
                "caption": "Figure 11:The login page of the annotation website.",
                "position": 3173
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/overview_page.png",
                "caption": "Figure 12:The overview page of the annotation website.",
                "position": 3178
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/first_stage.png",
                "caption": "Figure 13:The first stage of data annotation.",
                "position": 3183
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/second_stage.png",
                "caption": "Figure 14:The second stage of data annotation.",
                "position": 3188
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/task_comparison_heatmap.png",
                "caption": "Figure 15:Task Type Comparison Heatmap: A comparison of annotation consistency across three distinct task types (TimE-Wiki,TimE-News, andTimE-Dial). The heatmap employs a color gradient from dark to light to visually represent the differences in average similarity and exact match rates among the task types.",
                "position": 3218
            },
            {
                "img": "https://arxiv.org/html/2505.12891/extracted/6451314/figures/similarity_vs_perfect_match.png",
                "caption": "Figure 16:Scatter Plot of Average Similarity vs. Exact Match Rate:\nThis plot illustrates the relationship between the average token-level similarity and the exact match rate for each file, with different colors and markers distinguishing different task types. A regression line is added to indicate the correlation between the two metrics.",
                "position": 3223
            }
        ]
    },
    {
        "header": "Appendix BBenchmark Details",
        "images": []
    },
    {
        "header": "Appendix CExperiment Details",
        "images": []
    },
    {
        "header": "Appendix DExtended Case Study",
        "images": []
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FSocietal Impacts and Ethical Considerations",
        "images": []
    }
]