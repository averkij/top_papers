[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05641/x1.png",
                "caption": "Figure 1:Comparison between existing multi-agent work and our work. (a) In prior work, a fixed set of task-level experts (e.g., Phi, Mistral, and Llama) is recruited to solve mathematical problems, where heterogeneous questions may differ in the skills required to solve them (e.g Q1 requires algebra, while Q2 focuses on probability).\nExpert models then engage in multiple rounds of discussion, making this approach resource-intensive.\n(b) In contrast,Symbolic-MoEadaptively recruits instance-level experts based on skills needed (‚Äò‚ÄòAlgebra\" experts for Q1 and a different set of ‚Äò‚ÄòProbability\" experts for Q2).\nBy generating only a single round of responses with a selected aggregator to synthesize the final output, our approach is both more performant and more efficient.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05641/x2.png",
                "caption": "Figure 2:Overview ofSymbolic-MoE. (a) Preprocessing: Given a validation set and a pool of agents, we create model profiles and select an aggregator.\n(b) Inference-Time: For each test example,Symbolic-MoEactivates the most relevant models (experts) based on skill-based routing, using model profiles determined during preprocessing.\nThese models generate CoT responses, which the aggregator (chosen based on its ability to select correct answers) synthesizes into a final answer.",
                "position": 226
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05641/x3.png",
                "caption": "Figure 3:Different approaches to achieving adaptiveness inSymbolic-MoE, which uses different models for each instance. In a naive setup (I),kùëòkitalic_kGPUs must be hosted simultaneously, allowing immediate access to outputs from each model. Another naive setup (II) requires only a single GPU but involves constant loading and offloading of models to obtain outputs from the corresponding model. Our scalable batch inference process (III) strikes a balance between (I) and (II). When models are assigned to problems, we group samples by model and sequentially load the corresponding LLM onto a single GPU to generate outputs efficiently. Moreover, this approach still allows us to parallelize across GPUs if they are available.",
                "position": 362
            }
        ]
    },
    {
        "header": "4Results and Analysis",
        "images": []
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset Statistics and Licenses",
        "images": []
    },
    {
        "header": "Appendix BModel Pool",
        "images": []
    },
    {
        "header": "Appendix CPerformance on the Validation Set",
        "images": []
    },
    {
        "header": "Appendix DPerformance of Each Model as an Aggregator",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05641/x4.png",
                "caption": "Figure 4:Distribution of the recruited experts across datasets. Top row: the distribution before trimming. Bottom row: the distribution after trimming and re-sampling.",
                "position": 2322
            }
        ]
    },
    {
        "header": "Appendix EDistribution of Experts",
        "images": []
    },
    {
        "header": "Appendix FAlgorithm",
        "images": []
    },
    {
        "header": "Appendix GPrompts",
        "images": []
    },
    {
        "header": "Appendix HSymbolic-MoEas a Sparse Mixture-of-Expert",
        "images": []
    }
]