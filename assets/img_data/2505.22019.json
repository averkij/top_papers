[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22019/x1.png",
                "caption": "Figure 1:Overall Framework of our Reinforcement Learning Framework.(a) demonstrates the interaction process between the model and the external environment, as well as the implementation of the GRPO algorithm.\n(b) shows the proposed visual perception action space which allows the model to extract information from a coarse-to-fine perspective.\n(c) is the specially designed reward for RAG, which combines outcome and retrieval performance across the entire sampling process.",
                "position": 142
            }
        ]
    },
    {
        "header": "2VRAG-RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22019/x2.png",
                "caption": "Figure 2:Comparison between our VRAG-RL and the traditional RAG in terms of perception methods.(a) Traditional methods lack effective perception, which easily leads to repetitive and ineffective retrieval calls and suboptimal outcomes. (b) Our VRAG-RL is efficient and accurate, enabling the model to perceive information-dense regions from a coarse-to-fine perspective.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2505.22019/x3.png",
                "caption": "Figure 3:Experiments on the impact of context length on model performance.",
                "position": 306
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22019/x4.png",
                "caption": "Figure 4:Retrieval performance of our approach.",
                "position": 827
            },
            {
                "img": "https://arxiv.org/html/2505.22019/x5.png",
                "caption": "Figure 5:Relative performance on MMLongBench.",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2505.22019/x6.png",
                "caption": "Figure 6:Latency Analysis on Generation.",
                "position": 887
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel-Based Reward",
        "images": []
    },
    {
        "header": "Appendix BThe implementation of the search engine",
        "images": []
    },
    {
        "header": "Appendix CReinforcement Learning Framework with GRPO",
        "images": []
    },
    {
        "header": "Appendix DExpert Trajectories Collection",
        "images": []
    },
    {
        "header": "Appendix EDataset Information",
        "images": []
    },
    {
        "header": "Appendix FCompared Baselines",
        "images": []
    },
    {
        "header": "Appendix GHyperparameters",
        "images": []
    },
    {
        "header": "Appendix HCase Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22019/x7.png",
                "caption": "Figure 7:Case 1 for VRAG-RL.",
                "position": 1914
            },
            {
                "img": "https://arxiv.org/html/2505.22019/x8.png",
                "caption": "Figure 8:Case 2 for VRAG-RL.",
                "position": 1918
            }
        ]
    },
    {
        "header": "Appendix IPrompts",
        "images": []
    }
]