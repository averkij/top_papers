[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00070/x1.png",
                "caption": "Figure 1:Illustration of theRobot-R1framework.(a)Robot-R1uses robot states and image observations from expert demonstrations to create a dataset.\n(b) These data are reformulated into three different multiple-choice question answering (MCQA) tasks: predicting next states, current states, and primitive movements.\n(c) During training, an LVLM solves MCQA tasks with reasoning which is then optimized using the GRPO algorithm[17]to reinforce reasoning pathways.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00070/x2.png",
                "caption": "Figure 2:Illustration of theRobot-R1Bench.(a)Robot-R1Bench consists of human-written questions paired with corresponding ground truth (reference) answers. (b) The LVLM under evaluation takes each question along with its associated image as input and generates an answer. (c) The generated answers are scored using GPT-4o, based on predefined rubrics and ground truth answers.",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x3.png",
                "caption": "Table 1:Example of theRobot-R1Bench question and response.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00070/x4.png",
                "caption": "Figure 3:Robot-R1Bench results.In embodied reasoning tailored for low-level control,Robot-R1outperforms all previously reported models.",
                "position": 584
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x5.png",
                "caption": "((a))Reward duringRobot-R1training",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x5.png",
                "caption": "((a))Reward duringRobot-R1training",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x6.png",
                "caption": "((b))Response length duringRobot-R1training",
                "position": 1143
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitation and Social Impact",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00070/x7.png",
                "caption": "Table 7:Example of the working process ofRobot-R1Bench",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x8.png",
                "caption": "((a))Avg reward change duringRobot-R1training",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x8.png",
                "caption": "((a))Avg reward change duringRobot-R1training",
                "position": 2465
            },
            {
                "img": "https://arxiv.org/html/2506.00070/x9.png",
                "caption": "((b))Response length change duringRobot-R1training",
                "position": 2470
            }
        ]
    },
    {
        "header": "Appendix CQualitative Results",
        "images": []
    }
]