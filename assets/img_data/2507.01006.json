[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01006/x1.png",
                "caption": "Figure 1:(A) GLM-4.1V-Thinking matches or outperforms the much larger Qwen2.5-VL-72B and the closed-source GPT-4o across a range of tasks. (B) Reinforcement learning substantially boosts the model’s performance, with gains of up to +7.3%.",
                "position": 111
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Overview and Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01006/x2.png",
                "caption": "Figure 2:The architecture of GLM-4.1V-Thinking.The proposed model consists of three components: (1) a ViT Encoder to process and encode images and videos, (2) an MLP Projector to align visual features to tokens, (3) a Large Language Model as a Language Decoder to process multimodal tokens and yield token completions. GLM-4.1V-Thinking can perceive images and videos as their native resolutions and aspect ratios. For video inputs, additional time index tokens are inserted behind each frame to enhance the model’s temporal understanding capability.",
                "position": 201
            }
        ]
    },
    {
        "header": "3Pre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/figs/pass_at_k.png",
                "caption": "Figure 3:Comparison of pass@k performance on a subset of MathVista consisting of non-multiple-choice questions.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/figs/recaption.png",
                "caption": "Figure 4:Examples of recaption model results.The recaptioning process eliminates noise and hallucinated content from the original data, while fully retaining factual knowledge.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Supervised Fine-Tuning",
        "images": []
    },
    {
        "header": "5Reinforcement Learning: What’s Challenging and What Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/figs/rewardhacking-curve.jpg",
                "caption": "Figure 5:Training reward curves (top) and evaluation metrics (bottom) when low-quality verifiers exist in some multimodal sub-domains. The STEM verifier is finely tuned, but the other-single-image and other-multi-image verifiers are not, causing: (a) Reward noise@other-single-image: The model tweaks outputs to drive rewards up without improving actual accuracy. (b) Reward hacking@other-multi-image: The model learns shortcuts that repeatedly fool the verifier, inflating rewards. After step 150, STEM reward growth stalls, the overall multimodal benchmark declines, and STEM-related benchmarks (MMMU, MathVista, AI2D) drop sharply.",
                "position": 526
            }
        ]
    },
    {
        "header": "6Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01006/x3.png",
                "caption": "Figure 6:Cross-domain generalization in reinforcement learning. We evaluate the SFT-stage models across five RL data settings: STEM, OCR & Chart, grounding, GUI agent, and a combined “Mix-all”. Each model is tested on five benchmark suites corresponding to these domains. The values in the grid show the average performance improvement per domain (negative values indicate a decline), and the cell colors are normalized within each domain.",
                "position": 2132
            }
        ]
    },
    {
        "header": "7Discussion: Limitations and Future Work",
        "images": []
    },
    {
        "header": "8Contribution",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/1.png",
                "caption": "Figure 7:A case showing the ability of generating the front-end code from a UI snapshot.",
                "position": 2919
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/2.png",
                "caption": "Figure 8:A case showing the ability of giving detailed and precise description for a video.",
                "position": 2927
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/3.png",
                "caption": "Figure 9:A case showing the ability of giving detailed and precise description for a video.",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/4.png",
                "caption": "Figure 10:A case showing the ability of answering a question related to a video using perception, knowledge and reasoning.",
                "position": 2943
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/5.png",
                "caption": "Figure 11:A case showing the ability of GUI recognition and operation.",
                "position": 2951
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/6.png",
                "caption": "Figure 12:A case showing the ability of chart understanding and question answering.",
                "position": 2959
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/7.png",
                "caption": "Figure 13:A case showing the ability of inferring the geographic position from a picture.",
                "position": 2967
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/8.png",
                "caption": "Figure 14:A case showing the integrated ability of code recognition, debugging and correction.",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/9.png",
                "caption": "Figure 15:A case showing the ability of solving Chemistry problem.",
                "position": 2983
            },
            {
                "img": "https://arxiv.org/html/2507.01006/extracted/6587500/appendix_figs/10.png",
                "caption": "Figure 16:A case showing the ability of solving complex Math problem.",
                "position": 2991
            }
        ]
    },
    {
        "header": "Appendix AQualitative Examples",
        "images": []
    }
]