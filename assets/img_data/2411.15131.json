[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15131/extracted/6016481/imgs/teaser_xyz.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15131/x1.png",
                "caption": "Figure 2:Overview of WildLMa models and robot setups.(a) WildLMa takes a frozen CLIP model to encode task-specific texts and visual observations; (b) Our robot platform is a Unitree B1 quadruped combined with a Unitree Z1 arm and a 3D-printed gripper, with two RGBD cameras and one lidar mounted on.",
                "position": 137
            }
        ]
    },
    {
        "header": "IIIMethod",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15131/x2.png",
                "caption": "Figure 3:Overview of WildLMa-planner. Given a constructed hierarchical scene graph, WildLMa-planner adopts a coarse-to-fine searching mechanism to determine node traversal and structured actions to take.",
                "position": 197
            }
        ]
    },
    {
        "header": "IVExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15131/x3.png",
                "caption": "Figure 4:Qualitative illustrations of some evaluated tasks.",
                "position": 599
            }
        ]
    },
    {
        "header": "VConclusion",
        "images": []
    }
]