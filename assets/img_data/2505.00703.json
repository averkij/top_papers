[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00703/x1.png",
                "caption": "Figure 1:The Illustration of CoT in Image Understand and Generation Tasks.In the image understanding task, the CoT is the textual reasoning process.\nIn the autoregressive visual generation task, we identify two levels of CoT: thesemantic-levelandtoken-levelCoT. Thesemantic-level CoTis the high-level planning prior to the image generation, in the form of text. Thetoken-level CoTis the intermediate patch-by-patch generation process, focusing on the local pixel details within a patch, in the form of image tokens.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2505.00703/x2.png",
                "caption": "Figure 2:Visualization of the Image Generation Process of T2I-R1.All the prompts need reasoning or contain an uncommon scenario. We observe that T2I-R1 successfully deduces the true intention behind the prompt or provides a sensible imagination for the uncommon scenario (highlighted in the text) to produce a satisfying result compared with the baseline model, Janus-Pro.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00703/x3.png",
                "caption": "Figure 3:Framework of BiCoT-GRPO.In step 1, we instruct the model to generate the semantic-level CoT based on the image prompt. In step 2, images are generated conditioned on both the image prompt andsemantic-level CoT, with the intermediate generation process serving astoken-level CoT. The resulting images are evaluated by an ensemble of vision experts to obtain rewards. We generateNùëÅNitalic_Nimages from each prompt to compute the group-relative reward and perform GRPO training.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2505.00703/x4.png",
                "caption": "Figure 4:Illustration of the Ensemble of Generation Rewards.We use GPT-4o mini to extract the objects and their attributes before training. Each specialized reward model receives customized information inputs for the reward calculation. We take the average of all the rewards as final reward.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2505.00703/x5.png",
                "caption": "Figure 5:Visualization Results.We provide the image generation results of the same prompt from four models: base model, the model with onlysemantic-level CoToptimized, the model with onlytoken-level CoToptimized, and the model with both levels of CoT optimized.",
                "position": 408
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.00703/x6.png",
                "caption": "Figure 6:Visualization Result of the Image Diversity of a Single Prompt.We showcase the result of only token-level CoT optimized and bothsemantic-levelandtoken-levelCoT optimized.",
                "position": 913
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]