[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03821/x1.png",
                "caption": "Figure 1:Prediction correctness across three categories of growing difficulty:scene understanding,spatial reasoning, andvisual perspective taking. Error bars represent95%percent9595\\%95 %confidence intervals (estimated using bootstrapping (10,000 iterations)). The random classifier is a baseline choosing an answer uniformly at random, seeAppendix A.3.",
                "position": 124
            }
        ]
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/1.jpg",
                "caption": "Figure 2:Sixteen tasks involving a single humanoid minifigure–object pair. Tasks vary by the object’s placement (left, right, front, back); the orientation of the humanoid minifigure (facing toward or away from the object); and camera angle (surface-level an bird’s-eye views). All images had the same dimensions, but some are enlarged here for presentation purposes.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/2.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/3.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/4.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/9.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/10.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/11.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/12.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/5.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/6.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/7.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/8.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/13.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/14.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/15.jpg",
                "caption": "",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2505.03821/extracted/6408150/images/dataset/16.jpg",
                "caption": "",
                "position": 137
            }
        ]
    },
    {
        "header": "3Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03821/x2.png",
                "caption": "Figure 4:Comparison of co-occurrence matrix for models (columns) across questions Q5 (top row) and Q7 (bottom row). For more details seeAppendix A.5.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2505.03821/x3.png",
                "caption": "Figure 5:Experiments to investigate GPT-4-Turbo’s persistent east bias in spatial reasoning (Q5). Top-left: Zoomed-in images of humanoid minifigures, testing the impact of increased visual detail. Bottom-left: Images with explicit cardinal direction labels (N, S, E, W) added. Top-right: Images with only a humanoid minifigure (no secondary objects). Bottom-right: Human figures replacing humanoid minifigures. Additionally, we permuted the cardinal directions in the prompt, testing all 24 possible orders of north, south, east, and west.",
                "position": 449
            }
        ]
    },
    {
        "header": "References and Notes",
        "images": []
    },
    {
        "header": "Appendix Appendix AData",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03821/x4.png",
                "caption": "Figure 6:Example model responses to Q5 and Q7 questions. Left: GPT-4 Turbo answersfront and slightly to the right(gold-standard answer: front). Right: Claude 3.5 Sonnet answerssoutheast(gold-standard answer: south).",
                "position": 774
            }
        ]
    },
    {
        "header": "Appendix Appendix BDirectional Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.03821/x5.png",
                "caption": "Figure 7:Illustration of the object removal process, used in investigating if the presence of contextual objects impacts the model’s orientation predictions (Q5). Top: visual tasks with objects. Bottom: corresponding visual tasks without objects. Left/Right: surface-level/bird’s-eye.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2505.03821/x6.png",
                "caption": "Figure 8:Examples of humanoid minifigure orientation tasks (Q5) at different zoom levels. Left to right: original image with 10% zoom, 30% zoom, and 50% zoom.",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2505.03821/x7.png",
                "caption": "Figure 9:Surface-level (left) and bird’s-eye (right) views of the visual task, showingN S E Wmarks on the image used for the on-visual task cardinal hints experiment.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2505.03821/x8.png",
                "caption": "Figure 10:Surface-level (top row) and bird’s-eye (bottom row) views of a real person used in the influence of subject type experiment.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2505.03821/x9.png",
                "caption": "Figure 11:Probability distributions for cardinal directions averaged over 69 images for each of 24 prompt permutations ordering the options.Eastdominates in 18/24 cases, confirming a strong bias. However, permutations starting witheastshow high, similar probabilities for botheastandsouth, revealing sensitivity to prompt structure alongside the baseline preference.",
                "position": 962
            },
            {
                "img": "https://arxiv.org/html/2505.03821/x10.png",
                "caption": "Figure 12:Comparison of VLM performance on visual perspective taking (Q6) with and without an explicit orientation hint (gold-standard Q5 answer), showing only marginal prediction correctness improvement.",
                "position": 978
            }
        ]
    },
    {
        "header": "Appendix Appendix COrientation vs Perspective Test",
        "images": []
    }
]