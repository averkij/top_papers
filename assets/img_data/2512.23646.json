[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23646/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23646/x2.png",
                "caption": "Figure 2:(a) End-to-end OmniLLMsimplicitly fuse modalities but suffer from high training costs, difficult alignment, and limited fine-grained reasoning.(b) Fixed workflow agentsrely on rigid pipelines, lacking the flexibility to allocate attention for fine-grained analysis adaptively.(c) Caption-based agentsincur high precomputation costs and noise sensitivity, often failing to capture comprehensive multimodal context.(d) Our OmniAgentemploys active perception reasoning and inquiry. Within an iterative reflective loop, the agent strategically calls on the ability of video and audio understanding.\nThis explicitly solves the cross-modal alignment difficulty and achieves fine-grained understanding.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23646/x3.png",
                "caption": "Figure 3:Overview of the OmniAgent framework.The system processes audio and video inputs through an iterativethinking-action-observe-reflectioncycle. The agent utilizes a comprehensive suite of perception tools (video, audio, and event) to gather fine-grained evidence, while the reflection module synthesizes observations to update the memory and decide whether to rethink or conclude the task.",
                "position": 186
            }
        ]
    },
    {
        "header": "3OmniAgent",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23646/x4.png",
                "caption": "Figure 4:Visualization of the responses and underlying reasoning processes generated by our OmniAgent and Gemini2.5-Flash to an audio-video understanding question.",
                "position": 264
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23646/x5.png",
                "caption": "Figure 5:Analysis of the behavior of OmniAgent with different core LLM models. We quantified tool utilization patterns by calculating both the proportion of invocations (call ratio) and the average number of reasoning steps per call. In the resulting visualization, the sector angle represents thetool call ratio, and the magnitude of the radius denotes the specific execution steps at which the tool was invoked.",
                "position": 956
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Release Notes",
        "images": []
    }
]