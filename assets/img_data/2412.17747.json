[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17747/x1.png",
                "caption": "Figure 1:Overview of the proposed architecture. The input sequence is processed by a frozen LLM, generating a kv-cache. This cache is then passed to a coprocessor, along with trainable soft tokens. The coprocessor outputs latent embeddings which are used to augment the original kv-cache before being fed back into the LLM for output generation.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2412.17747/x2.png",
                "caption": "Figure 2:Our coprocessor training framework. (a) Illustration of multi-position augmentation and ahead token prediction. For each selected augmentation position, latent embeddings are generated by the coprocessor and inserted after the corresponding token’s embedding. The target tokens for prediction (\"ahead tokens\") are then appended. A causal mask is applied to all sequences following these insertion points. (b) Structure of the modified input and attention mask for model training. We show an example of 1 latent embedding and 1 ahead token here for simplicity.",
                "position": 219
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17747/x3.png",
                "caption": "Figure 3:Validation perplexity of the baseline frozen Gemma-2 2B model and augmented models with varying numbers of latents (8, 16, 32, 64), when predicting the 1st and 32nd tokens following latent augmentation. Lower perplexity indicates better performance.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2412.17747/x4.png",
                "caption": "Figure 4:Finetuning the coprocessor from Gemma-2 2B pretrained weights significantly improves GSM8K accuracy compared to training from scratch. Lines represent the mean and shaded areas represent the 95% confidence interval, both estimated from the last 5 checkpoints.",
                "position": 597
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.17747/x5.png",
                "caption": "Figure 5:Scaling of GSM8K accuracy and validation perplexity with increasing training steps for the coprocessor (using 32 latent embeddings). The baseline performance of the frozen Gemma-2 2B model is shown for reference.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2412.17747/x6.png",
                "caption": "Figure 6:Accuracy on GSM8K’s test set after LoRA finetuning. Our augmented model shows a significant improvement compared to the baseline.",
                "position": 1529
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]