[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Shakti-SLMs Architecture",
        "images": []
    },
    {
        "header": "4Training and Fine-Tuning Methodologies",
        "images": []
    },
    {
        "header": "5Dataset Details",
        "images": []
    },
    {
        "header": "6Evaluation and Competitive Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01933/extracted/6246866/100M_benchmark_graph.png",
                "caption": "Figure 1:Comparison results on academic benchmarks for Shakti-100M, Boomer-634M[14], SmolLM-135M[12], SmolLM-360M[12], and AMD-Llama-135M[45], which are in the same parameter range.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2503.01933/extracted/6246866/250M_benchmark_graph.png",
                "caption": "Figure 2:Comparison results on academic benchmarks for Shakti-250M, Boomer-1B[13], Boomer-634M[14], Qwen2.5-0.5B[47], SmolLM-360M[12], and Llama 3.2 1B[46].",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2503.01933/extracted/6246866/500M_benchmark_graph.png",
                "caption": "Figure 3:Comparison results on academic benchmarks for Shakti-500M, Boomer-1B[13], Boomer-634M[14], Qwen2.5-0.5B[47], and Llama 3.2 1B[46].",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2503.01933/extracted/6246866/domain_specific_benchmark_250M.png",
                "caption": "Figure 4:Comparison results on medical and finance domain benchmarks for Shakti-250M, Phi-1.5-1.3B[48], Gemma-2B[49], and Opt-2.7B[50]models, specifically for the Medical domain.",
                "position": 568
            }
        ]
    },
    {
        "header": "7Shakti-SLMs Multilingual Capabilities",
        "images": []
    },
    {
        "header": "8Quantization",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01933/extracted/6246866/raw_quantized_model_size.png",
                "caption": "Figure 5:Model size comparison before and after quantization. FP32 represents the original model size, while Q8, Q5, and Q4 represent increasingly aggressive quantization levels. Note the substantial reduction in memory footprint, with Q4 models requiring approximately 8x less memory than their FP32 counterparts.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2503.01933/extracted/6246866/token_performance_hardware_v1.png",
                "caption": "Figure 6:Performance comparison (tokens per second) of Shakti models across different hardware platforms. The graph demonstrates how our models maintain high throughput even on resource-constrained devices compared to similar-sized competitors.",
                "position": 739
            }
        ]
    },
    {
        "header": "9Responsible AI",
        "images": []
    },
    {
        "header": "10Conclusion",
        "images": []
    },
    {
        "header": "11Future Scope",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]