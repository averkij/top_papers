[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03663/x1.png",
                "caption": "Figure 1:UniDoc-Benchoverview.",
                "position": 180
            },
            {
                "img": "https://arxiv.org/html/2510.03663/figures/symbols/image-1.png",
                "caption": "Table 1:Comparison of existing dataset withUniDoc-Bench.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2510.03663/figures/symbols/text-format.png",
                "caption": "",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2510.03663/figures/symbols/table.png",
                "caption": "",
                "position": 236
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03663/x2.png",
                "caption": "Figure 2:Data Construction pipeline. (a) We filter and tag PDFA documents to curate a high-quality database of7070k pages spanning88domains.\n(b) We parse documents into text, figures, and tables, then synthesize initial QA pairs covering four question types and three modalities using adapted templates.\n(c) We ground answers in supporting evidence, refine questions for human-intent and self-containment, and verify responses for factuality and completeness, yielding1,6001,600QA pairs. To ensure quality,20%20\\%of the dataset is validated by three independent human annotators.",
                "position": 422
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix BDataset Creation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03663/figures/exp-parse.png",
                "caption": "Figure 3:Example of PDF parsing with figure placeholders (<<fig-XXX>>).",
                "position": 2901
            }
        ]
    },
    {
        "header": "Appendix CHuman Annotation",
        "images": []
    },
    {
        "header": "Appendix DExamples",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03663/x3.png",
                "caption": "Figure 4:Image-retrieval system fails to extract factual facts and details.",
                "position": 4513
            },
            {
                "img": "https://arxiv.org/html/2510.03663/x4.png",
                "caption": "Figure 5:Image-retrieval system fails to extract factual facts and details in the image.",
                "position": 4517
            },
            {
                "img": "https://arxiv.org/html/2510.03663/x5.png",
                "caption": "Figure 6:Text-retrieval system fails to extract factual facts and details in the table.",
                "position": 4525
            },
            {
                "img": "https://arxiv.org/html/2510.03663/x6.png",
                "caption": "Figure 7:Text-retrieval system fails to extract factual facts and details in the table.",
                "position": 4529
            },
            {
                "img": "https://arxiv.org/html/2510.03663/x7.png",
                "caption": "Figure 8:MM RAG system handles multi-modality-evidence questions better.",
                "position": 4537
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix FAdditional Analysis",
        "images": []
    }
]