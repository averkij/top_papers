[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11799/x1.png",
                "caption": "Figure 1:Overview.Given a source input video and an edited keyframe obtained by manipulating various intrinsic properties, V-RGBX generates an edited video which propagates the edit in an intrinsic aware manner. V-RGBX is an end-to-end framework that understands intrinsic scene properties and uses them for generation to support tasks such as object retexturing, relighting, or material editing, etc.",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11799/x2.png",
                "caption": "Figure 2:The overall architecture of V-RGBX.Our framework consists of three parts: (1) Inverse RendererDD, which decomposes the input video into albedo, normal, material, and irradiance channels; (2) Intrinsic Conditioning SamplerSS, which interleaves edited keyframe intrinsics with non-conflicted random intrinsic frames to form a unified intrinsic conditioning video; and (3) Forward RendererRR, which integrates the intrinsic video, keyframe reference, and temporal-aware intrinsic embeddings to synthesize the output RGB video and consistently propagate intrinsic properties across time.",
                "position": 209
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11799/x3.png",
                "caption": "Figure 3:Qualitative comparison on intrinsic-aware editing.We show the results of keyframe-edit video propagation based on a single intrinsic channel. AnyV2V[ku2024anyv2v]and VACE[vace]suffer from property drifting, exhibiting unexpected evolution over time, and fail to achieve accurate disentanglement among different intrinsic channels. In contrast, our method consistently propagates the intrinsic-aware edits throughout the sequence. (Green arrows mark the edited regions in the original video’s keyframe, whereas red arrows and boxes denote artifacts generated by the baseline methods.)",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x4.png",
                "caption": "Figure 4:Qualitative comparison on relighting.We evaluate our method on light color and shadow editing tasks. AnyV2V exhibits geometry and appearance drifting as generation progresses, while VACE struggles to disentangle lighting effects, leading to unintended changes in other channels and even introducing new assets. Our approach naturally performs relighting in both tasks and produces results that closely match the ground truth.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x5.png",
                "caption": "Figure 5:Qualitative comparisons for the X→\\rightarrowRGB task.Pink frames highlight temporal or spatial inconsistencies observed in RGBX results, while yellow frames indicate inaccurate or missing shadow modeling produced by DiffusionRenderer.\nOur method achieves temporal coherence, reliable shadow and light modeling, and overall more faithful scene generation from X to video.",
                "position": 421
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview",
        "images": []
    },
    {
        "header": "Appendix BWorkflow & Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11799/x6.png",
                "caption": "Figure S1:Intrinsic-aware video editing workflow of V-RGBX.Given an input video and edited keyframes, we decompose them into intrinsic channels, and the intrinsic conditioning sampler uses these representations to produce an intrinsic video. The forward renderer then synthesizes the final edited sequence using both the intrinsic video and the appearance cues provided by the edited keyframes.",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x7.png",
                "caption": "Figure S2:Overview of RGB→X→RGB cycle workflow.An input RGB video is first decomposed into intrinsic components by our inverse renderer, then reconstructed by the forward renderer using the predicted intrinsic sequence and a first-frame keyframe. The decomposition and forward-rendering results illustrate the quality of our intrinsic predictions and the rendered video.",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x8.png",
                "caption": "Figure S3:Visual ablations on the Intrinsic Type Embedding (ITE) module and the reference condition.Columns show the base model, adding ITE, adding both ITE and the reference condition (ours), and the ground truth. The intrinsic maps (top) and reconstructed RGB frames (bottom) illustrate that ITE reduces temporal and modality inconsistencies, while the reference condition further improves reflections and color fidelity.",
                "position": 829
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.11799/x9.png",
                "caption": "Figure S4:RGB→X results on synthetic Evermotion scenes.Given an input RGB video, V-RGBX decomposes it into albedo, normal, material, and irradiance channels. Each pair of rows shows two frames from the same video, and the second to fifth columns visualize the corresponding intrinsic channels, demonstrating spatially coherent and temporally stable decompositions across diverse indoor scenes.",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x10.png",
                "caption": "Figure S5:RGB→X results on real-world RealEstate10K videos.Given an input RGB video, V-RGBX decomposes it into albedo, normal, material, and irradiance channels. Each pair of rows shows two frames from the same video, and the second to fifth columns visualize the corresponding intrinsic channels, demonstrating coherent and temporally stable decompositions under challenging and unseen real-world conditions, while also showing reasonable generalization to outdoor scenes.",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x11.png",
                "caption": "Figure S6:Comparison of RGB→X decomposition results with baselines.Each pair of rows shows two frames from the same input video (first column). For each method, the two columns visualize the predicted albedo and normal channels. Compared with RGBX and DiffusionRenderer, V-RGBX produces intrinsic decompositions with higher visual fidelity, more accurate albedo estimation, and more consistent normal predictions across frames.",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x12.png",
                "caption": "Figure S7:Comparison of irradiance decomposition with baselines.The figure shows two different videos, with each pair of rows representing two frames from the same video. For each frame, the second and third columns show irradiance predictions from V-RGBX and RGBX. V-RGBX produces more accurate illumination and shadow modeling, resulting in clearer and more plausible irradiance maps.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x13.png",
                "caption": "Figure S8:More qualitative comparisons for the X→RGB task.Each group of three columns shows three frames from the same video, while each row corresponds to a different method: intrinsic channels inputs, RGBX, DiffusionRenderer, our results, and the ground truth. The comparisons illustrate that our method performs better in scene appearance and temporal consistency across frames.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x14.png",
                "caption": "Figure S9:RGB→X→RGB cycle results on the synthetic dataset.Each row shows a different method (the first row is the input video as ground truth). Every three columns correspond to three frames from the same video. Our method produces reconstructions closest to the ground truth and better preserves scene appearance and structure throughout the sequence.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x15.png",
                "caption": "Figure S10:RGB→X→RGB cycle results on the real-world dataset.Each row shows a different method (the first row is the input video as ground truth). Every three columns correspond to frames from the same video. Our method gives a closer match to the ground truth.",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2512.11799/x16.png",
                "caption": "Figure S11:Intermediate results of the intrinsic-aware keyframe editing.We visualize intermediate results used by V-RGBX across the following editing types: (1) solid color, (2) texture, (3) material, (4) normal, (5) light color, and (6) shadow editing. For each case, we show the original video frames, the edited keyframe produced by the NanoBanana tool, and the corresponding modified intrinsic channels (albedo, material, normal, or irradiance) that serve as conditioning inputs. These processes reveal how keyframe edits are translated into intrinsic-space modifications, which are then reliably propagated by V-RGBX to generate the final temporally consistent edited video.",
                "position": 1015
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative Results",
        "images": []
    }
]