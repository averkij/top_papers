[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18130/x1.png",
                "caption": "Figure 1:Significant variations in model capabilities.Values are normalized to [0,1]. Models exhibit clear specialization: Qwen2.5-Coder leads in coding but lags in biomedical tasks; Qwen2.5-Math excels in mathematics but struggles elsewhere; Bio-Medical-Llama dominates in biomedical knowledge but performs poorly in math and coding; Gemma stands out in reasoning and reading. These distinct profiles make it feasible to predict model performance only based on specific user queries.",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2601.18130/x2.png",
                "caption": "Figure 2:Concept comparisonbetween our RouteMoA and previous MoA-based methods. (a) Classical MoAWanget al.(2024a)forwards all LLMs in each layer, and concatenates all outputs as the input of the next layer. (b) Sparse MoALiet al.(2024)introduces an LLM-based judge to select some good responses as the input of the next layer. This reduces the number of input tokens, but still needs to forward all LLMs and another LLM-based judge. (c) RouteMoA uses a lightweight router to select parts of LLMs for inference, significantly reducing computational cost.",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2601.18130/x3.png",
                "caption": "Figure 3:RouteMoA architecture.\nThe framework operates layer-wise (left). At each layerll, the router selects a subset of suitable LLMs, whose outputs are aggregated and passed to the next layer. The router (right) consists of two stages:b.1 Mixture of Judges, which includes a scorer (trained as ina. Scorer Training), self-assessment, and cross-assessment. The scorer predicts candidate performance in layer-1 using prior knowledge from the query; subsequent layers refine scores via self- and cross-assessment using posterior knowledge from model outputs.b.2 Model Rankingselects LLMs by balancing performance, cost, and latency.",
                "position": 152
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18130/x4.png",
                "caption": "Figure 4:Average values of three scorer assessment metrics(Top-1-Hit, Top-3-Hit, and Top-3-Agree) under different training hyperparameters (λ\\lambdaandα\\alpha).",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2601.18130/x5.png",
                "caption": "Figure 5:Case studyof adjusting wrong scorer predictions with self- and cross-assessment.",
                "position": 993
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAggregation Prompt, Self- and Cross-assessment Prompt",
        "images": []
    },
    {
        "header": "Appendix BPrompt for Training Dataset Generation",
        "images": []
    },
    {
        "header": "Appendix CClustering Details for Sample-Sample Loss",
        "images": []
    },
    {
        "header": "Appendix DTop-1-Hit, Top-3-Hit and Top-3-Agree Calculation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.18130/x6.png",
                "caption": "Figure 10:Top-1-Hit under different training hyperparameters (λ\\lambdaandα\\alpha) for the scorer module.",
                "position": 2049
            },
            {
                "img": "https://arxiv.org/html/2601.18130/x7.png",
                "caption": "Figure 11:Top-3-Hit under different training hyperparameters (λ\\lambdaandα\\alpha) for the scorer module.",
                "position": 2053
            },
            {
                "img": "https://arxiv.org/html/2601.18130/x8.png",
                "caption": "Figure 12:Top-3-Agree under different training hyperparameters (λ\\lambdaandα\\alpha) for the scorer module.",
                "position": 2057
            }
        ]
    },
    {
        "header": "Appendix EDataset Statistics for Scorer Training of Small-Scale Model Pool",
        "images": []
    },
    {
        "header": "Appendix FEvaluation Details on Large-Scale Model Pool",
        "images": []
    },
    {
        "header": "Appendix GModel & Data License and Intended Use Statement",
        "images": []
    }
]