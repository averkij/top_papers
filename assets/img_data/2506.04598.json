[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods & experimental setup",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04598/x1.png",
                "caption": "(a)",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x1.png",
                "caption": "(a)",
                "position": 252
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x2.png",
                "caption": "(b)",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x3.png",
                "caption": "(a)",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x3.png",
                "caption": "(a)",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x4.png",
                "caption": "(b)",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x5.png",
                "caption": "(a)",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x5.png",
                "caption": "(a)",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x6.png",
                "caption": "(b)",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x7.png",
                "caption": "(a)",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x7.png",
                "caption": "(a)",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x8.png",
                "caption": "(b)",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x9.png",
                "caption": "(a)",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x9.png",
                "caption": "(a)",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x10.png",
                "caption": "(b)",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x11.png",
                "caption": "(a)",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x11.png",
                "caption": "(a)",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x12.png",
                "caption": "(b)",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x13.png",
                "caption": "(a)",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x13.png",
                "caption": "(a)",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x14.png",
                "caption": "(b)",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x15.png",
                "caption": "(a)",
                "position": 420
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x15.png",
                "caption": "(a)",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x16.png",
                "caption": "(b)",
                "position": 429
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x17.png",
                "caption": "(a)",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x17.png",
                "caption": "(a)",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x18.png",
                "caption": "(b)",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x19.png",
                "caption": "Figure 10:Scaling law fit for ImageNet-1k 0-shot classification, comparing MaMMUT, CLIP and Cap (captioning only). Cap can be only evaluated via log-likelihood, which is more expensive as similarity based evaluation used by CLIP and MaMMUT, as Cap misses contrastive loss in its architecture, which makes it disadvantageous for 0-shot setting.",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x20.png",
                "caption": "Figure 11:Scaling law on DataComp evaluation suite (average over 35 tasks, 0-shot classification), openCLIP vs. openMaMMUT comparison on DataComp-1.4B",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x21.png",
                "caption": "(a)",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x21.png",
                "caption": "(a)",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x22.png",
                "caption": "(b)",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x23.png",
                "caption": "Figure 13:Scaling law for semantic segmentation.Downstream error rate (1 – mIoU) of openCLIP and openMaMMUT pre-trained on DataComp-1.4B and fine-tuned on ADE20K. MaMMUT shows higher performance than CLIP for segmentation at higher scales. Crossing point appears earlier around109superscript10910^{9}10 start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPTGFLOPS, which might be due to fine-tuning used for this task, as opposed to zero-shot evaluation applied everywhere else.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x24.png",
                "caption": "(a)",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x24.png",
                "caption": "(a)",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x25.png",
                "caption": "(b)",
                "position": 576
            }
        ]
    },
    {
        "header": "4Related work & limitations",
        "images": []
    },
    {
        "header": "5Discussion & conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEstimated parameters for scaling law fits",
        "images": []
    },
    {
        "header": "Appendix BMore details on scaling law derivation experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04598/x26.png",
                "caption": "Figure 15:Detailed version of the scaling law fit for ImageNet 0-shot classification error rate for DataComp-1.4B for openCLIP. Cooler colors indicate smaller models. Bigger models are bottlenecked by samples seen scale (require larger samples seen than the smaller ones) and smaller models saturate with increased data and compute scale (over-training regime). Pareto front is composed by taking for each compute budget the points corresponding to models reaching minimal error rate for the given compute. Fit is performed through points on Pareto front.",
                "position": 1858
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x27.png",
                "caption": "Figure 16:Detailed version of the scaling law fit for ImageNet 0-shot classification error rate for DataComp-1.4B for OpenMaMMUT. Cooler colors indicate smaller models. Bigger models are bottlenecked by samples seen scale (require larger samples seen than the smaller ones) and smaller models saturate with increased data and compute scale (overtraining regime). Pareto front is composed by taking for each compute budget the points corresponding to models reaching minimal error rate for the given compute. Fit is performed through points on Pareto front.",
                "position": 1861
            }
        ]
    },
    {
        "header": "Appendix CEvaluating scaling law fit quality",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04598/x28.png",
                "caption": "(a)",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x28.png",
                "caption": "(a)",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x29.png",
                "caption": "(b)",
                "position": 1894
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x30.png",
                "caption": "(a)",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x30.png",
                "caption": "(a)",
                "position": 1904
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x31.png",
                "caption": "(b)",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x32.png",
                "caption": "(a)",
                "position": 1920
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x32.png",
                "caption": "(a)",
                "position": 1923
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x33.png",
                "caption": "(b)",
                "position": 1929
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x34.png",
                "caption": "(a)",
                "position": 1936
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x34.png",
                "caption": "(a)",
                "position": 1939
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x35.png",
                "caption": "(b)",
                "position": 1945
            }
        ]
    },
    {
        "header": "Appendix DAdditional training details",
        "images": []
    },
    {
        "header": "Appendix EMore details on fine-tuning for segmentation and scaling laws",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04598/x36.png",
                "caption": "Figure 21:Detailed scaling law for downstream semantic segmentation performance of openCLIP pre-trained on DataComp-1.4B and fine-tuned on ADE20K. Error rate (1 – mIoU).",
                "position": 2825
            },
            {
                "img": "https://arxiv.org/html/2506.04598/x37.png",
                "caption": "Figure 22:Detailed scaling law for downstream semantic segmentation performance of openMaMMUT pre-trained on DataComp-1.4B and fine-tuned on ADE20K. Error rate (1 – mIoU).",
                "position": 2828
            }
        ]
    },
    {
        "header": "Appendix FBroader impact",
        "images": []
    },
    {
        "header": "Appendix GAuthor contributions",
        "images": []
    }
]