[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x1.png",
                "caption": "Figure 1:Comparison of our proposed LVD-2M dataset against previous datasets. Our dataset contains long-take videos with significant motion and temporally-dense captions (different colors represent captions for different frames), contrasting with short videos and sparse annotations in previous datasets like Panda-70M[1], HD-VG[2], and WebVid[3](shown as \"Others\").",
                "position": 91
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x2.png",
                "caption": "Figure 2:Video filtering process. Our video filtering process employs multiple criteria to select high-quality, dynamic, and long-take videos from four source datasets.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x3.png",
                "caption": "Figure 3:Hierarchical video captioning process. First, we split the long video into 30-second clips and compose them into image grids. Then, we use the LLaVA-1.6 model[20]to generate captions for each image grid. Finally, we use the Claude3-Haiku model[21]to refine and merge these captions into the final complete caption for the whole video.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x4.png",
                "caption": "Figure 4:Statistics of LVD-2M. LVD-2M consists of long video clips with detailed dense captions, and diverse categories.",
                "position": 328
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x5.png",
                "caption": "Figure 5:The distribution of human-rated dynamic degree score and human preference for caption quality, comparing LVD-2M with other video datasets.",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x6.png",
                "caption": "Figure 6:Human evaluation of generated videos by baseline v.s. fine-tuned models. We finetune both a diffusion-based I2V model and a LM-based T2V model on LVD-2M. Compared to the pretrained model, the finetuned models can generate more dynamic videos.",
                "position": 387
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x7.png",
                "caption": "Figure 7:After finetuning a T2V diffusion model on LVD-2M, the videos are more dynamic, and the actions and objects in the videos are more reasonable, in contrast to finetuning on WebVid-10M.",
                "position": 766
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations and Social Impacts",
        "images": []
    },
    {
        "header": "Appendix BExtending a Diffusion-based I2V Model for Longer Range on LVD-2M",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x8.png",
                "caption": "Figure 8:After fine-tuning the diffusion-based I2V model on LVD-2M, the camera perspective will present more translation, compared to WebVid-10M.",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x9.png",
                "caption": "Figure 9:After fine-tuning the diffusion-based I2V model on LVD-2M, the camera view rotates more often and will present more view points, compared to WebVid-10M.",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x10.png",
                "caption": "Figure 10:The problem of abrupt transition into black-white mask frames are less observed after fine-tuning the diffusion-based I2V model on LVD-2M.",
                "position": 1387
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x11.png",
                "caption": "Figure 11:fine-tuning the diffusion-based I2V model on LVD-2M will further improve the capability of the model to generate more dynamic content, compared to WebVid-10M.",
                "position": 1391
            }
        ]
    },
    {
        "header": "Appendix CQualitative Evaluation for Long Range Video Fine-tuning of LM-based Model on LVD-2M",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x12.png",
                "caption": "Figure 12:Fintuning the LM-based T2V model on LVD-2M vs. WebVid-10M. After fine-tuning, the model can generate richer content with larger motion. This shows that fine-tuning on LVD-2M can further improve the model’s capability to generate more dynamic content, compared to WebVid-10M.",
                "position": 1409
            }
        ]
    },
    {
        "header": "Appendix DStatistics of LVD-2M and Previous Datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x13.png",
                "caption": "Figure 13:The distribution of video clip duration.",
                "position": 1420
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x14.png",
                "caption": "Figure 14:The distribution of average optical flow magnitude. LVD-2M demonstrate significantly larger portion of dynamic (measured by optical flow) videos.",
                "position": 1427
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x15.png",
                "caption": "Figure 15:The distribution of caption word count.",
                "position": 1434
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x16.png",
                "caption": "Figure 16:LVD-2Mpresents desirable quality for training of long video generation in 5 dimensions.",
                "position": 1440
            }
        ]
    },
    {
        "header": "Appendix EPrompt Designs",
        "images": []
    },
    {
        "header": "Appendix FDiscussions on Using MLLM for Data Filtering",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x17.png",
                "caption": "Figure 17:The prompt used for evaluating video quality with PLLaVA[18].",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x18.png",
                "caption": "Figure 18:The prompt used for instructing LLaVA-v1.6-34B[20]to generate relatively coarse captions for video clips.",
                "position": 1476
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x19.png",
                "caption": "Figure 19:The prompt used for instructing Claude3-Haiku[21]to refine the single coarse caption from LLaVA-v1.6.",
                "position": 1479
            },
            {
                "img": "https://arxiv.org/html/2410.10816/x20.png",
                "caption": "Figure 20:The prompt used for instructing Claude3-Haiku[21]to compose the multiple coarse captions from LLaVA-v1.6.",
                "position": 1482
            }
        ]
    },
    {
        "header": "Appendix GAuthor Statements",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.10816/x21.png",
                "caption": "Figure 21:The UI for all the user studies conducted in this work.",
                "position": 1499
            }
        ]
    },
    {
        "header": "Appendix HAcknowledgement",
        "images": []
    }
]