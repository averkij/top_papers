[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/convergence_2.jpg",
                "caption": "Figure 1:We propose FAST, a simple yet effective approach for tokenization of robot action trajectories via time-series compression. FAST enables training of autoregressive VLAs that solve complex dexterous manipulation tasks and generalize broadly to new scenes. We use it to trainœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST, a generalist robot policy that matches the performance of the state-of-the-artœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTdiffusion VLA on dexterous and long-horizon manipulation tasks, while training 5x faster (top).",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x1.png",
                "caption": "Figure 2:Left: FAST tokenization enables training of autoregressive Transformers for dexterous robot control via simple next token prediction.Right: FAST outperforms popular binning tokenization schemes, e.g., used in OpenVLA[39], particularly for high-frequency robot data.",
                "position": 96
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIPreliminaries",
        "images": []
    },
    {
        "header": "IVCase Study: How Does Tokenization Affect VLA Training?",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/case_study.png",
                "caption": "Figure 3:Effect of sampling rate on prediction performance.We train a small autoregressive transformer model on a didactic interpolation task, in which the network must predict the black dashed curve given the four circles.\nWe find that models trained with the binning tokenization approach used in prior VLAs[10,39]produce increasingly poor predictions as we increase the sampling frequency of the underlying signal, due to strong correlation between consecutive tokens at high frequencies. Our FAST tokenization approach, based on the discrete cosine transform (DCT), addresses the problem and leads to high-quality predictions across all sampling rates.",
                "position": 153
            }
        ]
    },
    {
        "header": "VEfficient Action Tokenization via Time-Series Compression",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09747/x2.png",
                "caption": "Figure 4:Overview of the FAST¬†action tokenization pipeline.Given a normalized chunk of actions, we apply discrete cosine transform (DCT) to convert the signal to the frequency domain. We then quantize the DCT coefficients and use byte-pair encoding (BPE) to compress the flattened sequence of per-dimension DCT coefficients into the final action token sequence. SeeSectionV-Bfor a detailed description.",
                "position": 174
            }
        ]
    },
    {
        "header": "VIExperiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/environments.jpg",
                "caption": "Figure 5:Evaluation environments.We test FAST across 7¬†evaluation environments: 6¬†real-robot tasks and 1¬†simulation environment. The tasks are designed to test VLA performance on highly dexterous tasks, like folding cloths from a laundry basket (‚ÄúLaundry Folding‚Äù), and generalization, e.g., zero-shot table-top manipulation in unseen environments (‚ÄúDROID‚Äù).",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x3.png",
                "caption": "Figure 6:Comparison of policy performance using different tokenization approaches.We find that tokenization approaches that compress action targets (FAST, FSQ) lead to substantially more efficient training than the na√Øve binning tokenization used in prior VLAs. Overall, we find that FAST¬†leads to more effective policy training than FSQ, particularly on dexterous real-robot tasks. Our universal tokenizer, FAST+, matches the performance of dataset-specific tokenizers. We report mean and 95% CI.",
                "position": 451
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x4.png",
                "caption": "Figure 7:Evaluation environments of FAST policy trained on DROID[38].We find that the same policy checkpoint generalizes robustly, and performs various simple table-top taskszero-shotacross three university campuses.",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x5.png",
                "caption": "Figure 8:Universal tokenizer.We test the compression rate achieved by our FAST+ tokenizer vs. na√Øve tokenization across diverse robot datasets,unseenduring tokenizer training. We find that FAST is effective across a wide range of robot morphologies, action spaces and control frequencies.",
                "position": 478
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x6.png",
                "caption": "",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x7.png",
                "caption": "",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x8.png",
                "caption": "Figure 9:Comparison of diffusionœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT[7]to ourœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTmodel with FAST decoding on single-task training.On small datasets (Libero, T-Shirt Folding), both perform comparably. On large datasets (Table Bussing), FAST¬†converges faster. In DROID, we find that FAST¬†follows language instructions better. We report mean and 95% CI.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x9.png",
                "caption": "Figure 10:Rollout ofœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST on the laundry folding task.FAST tokenization enables autoregressive VLAs to perform complex, long-horizon, and dexterous tasks that were impossible with previous tokenization schemes.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x10.png",
                "caption": "Figure 11:Comparison ofœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST and diffusionœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT[7]generalist policies.œÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST matches the performance of diffusionœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTwhile requiring significantly less compute for training. Reported: mean and 95% CI.",
                "position": 542
            }
        ]
    },
    {
        "header": "VIIDiscussion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09747/x11.png",
                "caption": "Figure 12:Comparison of compression-reconstruction tradeoff on six training datsets. Any discretization method includes some hyperparameter that controls the tradeoff between reconstruction fidelity and compression level, represented here as number of tokens in the output (vocab size is held constant across all tokenizers). We sweep this hyperparameter (FAST: rounding scale; na√Øve tokenization: subsampling frequency; FSQ: number of latent tokens) and find that FAST performs well across a wide range of scales. In particular, although it is less efficient than VQ-based tokenizers at low fidelities, it exhibits much better scaling to higher reconstruction fidelity, making FAST much more applicable to fine-grained control problems. Specific instantiations of each tokenizer (FAST+, and na√Øve tokenization without subsampling) are also shown.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_bus.jpeg",
                "caption": "(a)Table Bussing",
                "position": 1865
            },
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_bus.jpeg",
                "caption": "(a)Table Bussing",
                "position": 1868
            },
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_shirt.jpeg",
                "caption": "(b)T-Shirt Folding",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_grocery.jpeg",
                "caption": "(c)Grocery Bagging",
                "position": 1880
            },
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_toast.jpeg",
                "caption": "(d)Toast out of Toaster",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_laundry.jpeg",
                "caption": "(e)Laundry Folding",
                "position": 1892
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x12.png",
                "caption": "Figure 14:Setups used for quantitative DROID evaluation.",
                "position": 2024
            },
            {
                "img": "https://arxiv.org/html/2501.09747/x13.png",
                "caption": "Figure 15:Comparison ofœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST andcompute-matcheddiffusionœÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT[7]generalist policies.œÄ0subscriptùúã0\\pi_{0}italic_œÄ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST clearly outperforms the diffusion VLA when trained with the same amount of training compute, due to its faster convergence. Reported: mean and 95% CI.",
                "position": 2176
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]