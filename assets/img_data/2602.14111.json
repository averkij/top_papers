[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/main.png",
                "caption": "Figure 1:Frozen SAE baselines and their performance.(Left) Conceptual diagrams: Frozen Decoder SAE (decoder weights fixed at random initialization), Soft-Frozen Decoder SAE (decoder weights initialized randomly and constrained to maintain CosineSim≥\\geq0.8 with their initial values throughout training), and Frozen Encoder SAE (encoder weights fixed at random initialization). (Right) For BatchTopK SAE (L0=160), these baselines remain competitive with fully trained SAE across four key evaluation metrics, challenging the assumption that strong performance indicates meaningful feature learning.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/main.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_k160/main_k160_btk_comparison_4variants.png",
                "caption": "",
                "position": 118
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Case Study #1: Toy Model Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/paired_plots/uniform/unit_coef_False/seed_0/paired_uniform_unit_coef_False_gt_max_cosine_hist_no_bars.png",
                "caption": "Figure 2:SAEs performance on constant probability setting.Both BatchTopK and JumpReLU SAEs achieve high reconstruction fidelity (Explained Variance = 0.67), yet recover almost none of the ground‑truth features in this simplest aligned setting.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/paired_plots/varying/unit_coef_False/seed_0/paired_varying_unit_coef_False_gt_max_cosine_vs_log10_prob_found_features_no_bars.png",
                "caption": "Figure 3:SAEs performance on variable probability setting.Both SAE architectures achieve high reconstruction fidelity (explained variance = 0.71), yet recover only the highest-frequency ground‑truth features.",
                "position": 300
            }
        ]
    },
    {
        "header": "4Case Study #2: Validating SAEs on LLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_vs_k/explained_variance_paired_vs_k_no_ci.png",
                "caption": "Figure 4:Explained Variance.Despite training with frozen components, naive baselines achieve high reconstruction performance, with Soft-Frozen SAEs matching fully-trained ReLU SAEs and losing only 6% relative to their original variants.",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_k160/autointerp_score_hist_k160_paired.png",
                "caption": "Figure 5:AutoInterp score distribution.For both SAE architectures, frozen baselines achieve high AutoInterp scores, with the Soft-Frozen variant matching original performance, suggesting interpretability can emerge without learned feature alignment.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x1.png",
                "caption": "Figure 6:Qualitative analysis: frozen SAEs produce interpretable latents.Sample activation contexts for latents from Soft-Frozen Decoder (rows 1-2) and Frozen Encoder (row 3) variants of BatchTopK SAE, with corresponding LLM-generated descriptions and AutoInterp scores. The features capture abstract concepts like“words expressing kindness”,“time-related expressions indicating future events”, and“names of sports teams”, demonstrating that high-level abstract interpretability can emerge without learned feature alignment.",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_vs_k/sae_top_1_test_accuracy_paired_vs_k_no_ci.png",
                "caption": "Figure 7:Sparse probing accuracy.Frozen-component variants of both BatchTopK and JumpReLU SAEs achieve accuracy comparable to their fully-trained counterparts when using single-top-latent probing.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_vs_k/disentanglement_score_paired_vs_k_no_ci.png",
                "caption": "Figure 8:RAVEL causal editing scores.SAEs with frozen components achieve RAVEL disentanglement scores equivalent to fully-trained SAEs, challenging the premise that SAEs learn meaningful features.",
                "position": 402
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASAEs: Claimed Benefits vs. Documented Limitations",
        "images": []
    },
    {
        "header": "Appendix BExtended Comparison for Toy Model Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/paired_plots_mat_topk/uniform/unit_coef_False/seed_0/paired_mat_topk_uniform_unit_coef_False_gt_max_cosine_hist_no_bars.png",
                "caption": "Figure 9:Toy model experiments: extended architectures.Surprisingly, simple TopK SAE successfully recovers nearly all ground truth features (99.99%), while Matryoshka SAE fails completely (0.03% recovery), performing equivalently to BatchTopK and JumpReLU in the constant probability setting. In the more realistic heavy-tailed variable setting, all architectures recover only high-frequency features (7 to 43% of total). TopK SAE achieves the best recovery, while Matryoshka’s hierarchical structure shifts recovery toward moderately frequent features at the expense of the very highest frequencies compared to BatchTopK and JumpReLU SAEs.",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/paired_plots_mat_topk/uniform/unit_coef_False/seed_0/paired_mat_topk_uniform_unit_coef_False_gt_max_cosine_hist_no_bars.png",
                "caption": "",
                "position": 1312
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/paired_plots_mat_topk/varying/unit_coef_False/seed_0/paired_mat_topk_varying_unit_coef_False_gt_max_cosine_vs_log10_prob_found_features_no_bars.png",
                "caption": "",
                "position": 1316
            }
        ]
    },
    {
        "header": "Appendix CFrozen Baselines for TopK SAE on Real Activations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_k160/main_k160_tk_comparison_4variants.png",
                "caption": "Figure 10:Frozen baseline performance for TopK SAE (L0=160) on Gemma-2-2B layer 12.",
                "position": 1339
            }
        ]
    },
    {
        "header": "Appendix DThe Soft-Frozen Decoder: Testing the Lazy Training Hypothesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/btk_jpr_init_to_ckpt_plots/jpr/k160/trainer_0/decoder/jumprelu__decoder__k160_trainer0_orig_first_cos_hist.png",
                "caption": "Figure 11:Decoder vectors remain near random initialization throughout training.Histograms of cosine similarity between decoder vectors at initialization and after 5% (left) and 10% (right) of training steps for a JumpReLU SAE. The strong concentration near 1.0 indicates minimal directional change even after reconstruction performance has plateaued, supporting the lazy training hypothesis and motivating the Soft-Frozen Decoder baseline.",
                "position": 1349
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/btk_jpr_init_to_ckpt_plots/jpr/k160/trainer_0/decoder/jumprelu__decoder__k160_trainer0_orig_first_cos_hist.png",
                "caption": "",
                "position": 1352
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/btk_jpr_init_to_ckpt_plots/jpr/k160/trainer_0/decoder/jumprelu__decoder__k160_trainer0_orig_second_cos_hist.png",
                "caption": "",
                "position": 1356
            }
        ]
    },
    {
        "header": "Appendix EFrozen Models Ablation",
        "images": []
    },
    {
        "header": "Appendix FAdditional Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_vs_k/ce_loss_with_sae_paired_vs_k_no_ci.png",
                "caption": "Figure 12:Cross-entropy loss comparison.Frozen baselines match fully-trained SAEs, showing reconstruction metrics alone don’t guarantee feature learning.",
                "position": 2135
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_vs_k/kl_div_with_sae_paired_vs_k_no_ci.png",
                "caption": "Figure 13:KL-divergence comparison.Frozen and fully-trained SAEs produce similar KL-divergence, indicating comparable model behavior without meaningful feature learning.",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2602.14111/figures/figures_vs_k/sae_top_5_test_accuracy_paired_vs_k_no_ci.png",
                "caption": "Figure 14:Sparse probing accuracy (top-5).Frozen baselines show larger gaps vs. fully-trained SAEs, suggesting learned representations better aggregate multiple features.",
                "position": 2141
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x2.png",
                "caption": "(a)Random",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x2.png",
                "caption": "(a)Random",
                "position": 2149
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x3.png",
                "caption": "(b)Trained",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x4.png",
                "caption": "(a)Random",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x4.png",
                "caption": "(a)Random",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x5.png",
                "caption": "(b)Trained",
                "position": 2169
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x6.png",
                "caption": "(a)Random",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x6.png",
                "caption": "(a)Random",
                "position": 2181
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x7.png",
                "caption": "(b)Trained",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x8.png",
                "caption": "(a)Random",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x8.png",
                "caption": "(a)Random",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2602.14111/x9.png",
                "caption": "(b)Trained",
                "position": 2201
            }
        ]
    },
    {
        "header": "Appendix GRandom SAEs on CLIP",
        "images": []
    }
]