[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10927/x1.png",
                "caption": "Figure 1:Illustration of motion automatically labeled using FoundationMotion. Our proposed FoundationMotion automatically detects and tracks moving objects, annotating their spatial movement (motion) in videos. We demonstrate the auto-labeled motion trajectories on diverse video domains, including autonomous driving, robotics, and human daily activities.",
                "position": 148
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3FoundationMotion Data Curation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10927/x2.png",
                "caption": "Figure 2:FoundationMotion Data Curation Pipeline. FoundationMotion is a fully automated pipeline for constructing large-scale motion datasets, enabling accurate detection, tracking, and understanding of object behavior. It leverages recognition models (e.g., Segment Anything) and understanding models (e.g., LLMs). Videos are first cropped to focus on motion, then objects such as cars and human-centric items (hands, bodies, persons) are detected and tracked. Their location changes are annotated into JSON files, which are summarized into captions. Finally, we design specific prompts for the LLM to generate questions and answers.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2512.10927/x3.png",
                "caption": "Figure 3:Examples of four zero-shot FoundationMotion evaluation benchmark.",
                "position": 312
            }
        ]
    },
    {
        "header": "4Fine-tuning with FoundationMotion for State-of-the-Art Motion Understanding",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10927/x4.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2512.10927/x5.png",
                "caption": "Figure 5:Dataset statistics. (a) correct answer distribution across options, (b) question length distribution measured in characters, and (c) video duration distribution in seconds.",
                "position": 561
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa1_videoframe_1.png",
                "caption": "",
                "position": 1183
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa1_videoframe_2.png",
                "caption": "",
                "position": 1186
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa1_videoframe_3.png",
                "caption": "",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa2_videoframe_1.png",
                "caption": "",
                "position": 1224
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa2_videoframe_2.png",
                "caption": "",
                "position": 1227
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa2_videoframe_3.png",
                "caption": "",
                "position": 1230
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa3_videoframe_1.png",
                "caption": "",
                "position": 1265
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa3_videoframe_2.png",
                "caption": "",
                "position": 1268
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa3_videoframe_3.png",
                "caption": "",
                "position": 1271
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa4_videoframe_1.png",
                "caption": "",
                "position": 1306
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa4_videoframe_2.png",
                "caption": "",
                "position": 1309
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa4_videoframe_3.png",
                "caption": "",
                "position": 1312
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa5_videoframe_1.png",
                "caption": "",
                "position": 1347
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa5_videoframe_2.png",
                "caption": "",
                "position": 1350
            },
            {
                "img": "https://arxiv.org/html/2512.10927/Figures/data_examples/qa5_videoframe_3.png",
                "caption": "",
                "position": 1353
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]