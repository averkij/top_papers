[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03264/x1.png",
                "caption": "Figure 1:We systematically inject reasoning-style data (ùíüres\\mathcal{D}_{\\mathrm{res}}) at different phases of training‚Äîpretraining versusSFT\\mathrm{SFT}‚Äîwhile varying itsdiversity, quantity, and quality. Our results show an asymmetric principle: diversity and scale matter most during pretraining, whereas quality dominates in SFT. This allocation strategy compounds through reinforcement learning (RL), yielding sustained gains across complex reasoning benchmarks.",
                "position": 206
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": []
    },
    {
        "header": "5Ablations",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BExperiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.03264/x2.png",
                "caption": "Figure 2:The model that saw the same high-quality data in both pretraining and SFT (‚Ñ≥SHQ\\mathcal{M}_{\\mathrm{SHQ}}) handily beats the baseline (‚Ñ≥b‚Äãa‚Äãs‚Äãe\\mathcal{M}_{base}) that only saw the data once.",
                "position": 2018
            }
        ]
    },
    {
        "header": "Appendix CAdditional Ablations",
        "images": []
    }
]