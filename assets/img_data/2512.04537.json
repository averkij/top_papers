[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04537/x1.png",
                "caption": "Figure 2:Our synthetic data creation pipeline. We leverage rich community assets (e.g., Fab marketplace) for characters, animations, and environments (blue) to create paired human-humanoid videos in three steps: First (red), we align the skeletons of different characters and animations to ensure compatibility across all assets. Second (green), with the help of compatible skeletons, we transfer the same animation to both human and humanoid characters. Finally (yellow), we place the animated human and humanoid in diverse scenes and record them using identical camera setups and movements to produce the paired data.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2512.04537/x2.png",
                "caption": "Figure 3:Visualization of sample pairs from our synthesized Human-Humanoid video dataset. To ensure diversity, the dataset encompasses a wide variety of scenes, character motions, and camera parameters (e.g., focal length, exposure). We intentionally include challenging conditions, such as occlusions by obstacles and partial-body or off-center framing, to improve model robustness.",
                "position": 146
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04537/x3.png",
                "caption": "Figure 4:The network architecture of our method. The original Wan 2.2 model does not accept video input. It only contains generation tokens (red), which the model denoises to generate a video. Parallel to these generation tokens, we encode the input human video into condition tokens (blue) and concatenate all tokens. During self-attention, we apply a mask to prevent condition tokens from attending to generation tokens. At the output, we only retain the generation tokens and decode them to produce the edited video.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2512.04537/x4.png",
                "caption": "Figure 5:Qualitative comparison to baselines. The videos are sampled at frames 15, 30, 45, 60, and 75. MoCha[mocha]generally maintains the original character’s motion, but it often produces incorrect details, such as the upper arm shape in (a) and the leg pose in (b). Kling[kling]generates the correct embodiment but fails on motion consistency, resulting in unsynchronized actions and undesirable scene alternations (e.g., dropping the green bag to the ground instead of putting it back in (a)). Runway Aleph[aleph]fails to produce both the correct robot embodiment (a) and the correct motion (b). In contrasts, our method successfully generates the correct embodiment and motions synchronized with the original human.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04537/x5.png",
                "caption": "Figure 6:Qualitative ablation of model architectures. Both VACE models fail to generate the correct robot embodiment, while the 14B version of our model suffers from lower overall video quality. Our 5B model performs the best on this video editing task.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2512.04537/x6.png",
                "caption": "Figure 7:Qualitative ablation on the inference prompt. The finetuning prompt (left) performs best. Any other prompts could lead to sub-optimal performances.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2512.04537/x7.png",
                "caption": "Figure 8:Visualization of robotizing in-the-wild videos. Our method successfully transforms the human protagonist into the correct humanoid embodiment while preserving motion, background, and overall video quality (e.g., middle-left vs. middle-right). It also robustly handles complex video effects such as pillarboxes (top-right), camera cuts (bottom-left), and motion blur (middle-right and bottom-right).",
                "position": 461
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "AImplementation Details of VACE Models",
        "images": []
    },
    {
        "header": "BQualitative Ablation on Finetuning Steps",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.04537/x8.png",
                "caption": "Figure 9:Qualitative ablation on finetuning steps. Too few steps (200) leads to incorrect occlusions (robot “overlayed” in front of the bicycle), while more steps (700) leads to overfitting to the synthetic data, causing degraded generation when editing real videos.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2512.04537/x9.png",
                "caption": "Figure 10:Qualitative comparison to baselines. The videos are sampled at frames 15, 30, 45, 60, and 75. MoCha achieves good motion consistency, but fails to generate the correct embodiment. Kling’s generated embodiment is mostly correct, but the motions (such as arm and leg poses) are not aligned with the original video. For both motion consistency and embodiment correctness, Aleph’s performance is between Kling and MoCha. In comparison, our method generates the most consistent motions with a correct Tesla Optimus embodiment.",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2512.04537/x10.png",
                "caption": "Figure 11:A failure case of our model, with comparison to baselines. The videos are sampled at frames 15, 30, 45, 60, and 75. During editing, our model may at times cause details or small objects near the robot to disappear; for example, in this video, the seat back, which is not clearly distinguished from the background color, disappears. Furthermore, for this scene, generating the leg poses under the table is also a challenge. Our method produces a slightly wrong occlusion near the knees of the humanoid, while among all methods, only Kling achieved a reasonable generation by changing the pose and location of the legs and feet under the table (although this hallucination is also undesired). This highlights that future work can focus on ensuring the correctness of details during the robotizing process.",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2512.04537/figures/appendix_ours_screenshots.jpg",
                "caption": "Figure 12:Visualization of all test set videos. This subset is randomly sampled, allowing for a broader inspection of our method’s editing performance and the usability of the output videos.",
                "position": 641
            }
        ]
    },
    {
        "header": "CMore on Baseline Comparison",
        "images": []
    },
    {
        "header": "DNetwork Inference Cost",
        "images": []
    }
]