[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01287/img/transformer_setup.png",
                "caption": "Figure 1:Setup for DQN meta-RL learning with a transformer. Tokens make up elements, which comprise episodes, which in turn make up task blocks. This is fed to the transformer which is trained to output the value of each action.",
                "position": 217
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.01287/img/bandit_reward_n30_X1024_gamma0_9.png",
                "caption": "Figure 2:Reward per episode in the 3-arm bandit environment, comparing meta-RL (n=30n=30italic_n = 30X=1024X=1024italic_X = 1024γe​p​i​s​o​d​e=0.9\\gamma_{episode}=0.9italic_γ start_POSTSUBSCRIPT italic_e italic_p italic_i italic_s italic_o italic_d italic_e end_POSTSUBSCRIPT = 0.9) to baseline strategies. Shaded areas denote 95% confidence intervals. The meta-RL agent exceeds Thompson Sampling andϵ\\epsilonitalic_ϵ-greedy baselines, demonstrating its ability to explore in the absence of explicit incentives.",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2508.01287/img/entropy.png",
                "caption": "Figure 3:Entropy of the action distribution derived from multiple trials with identical environment parameters, seeded with randomly generated episodes. The meta-RL agent’s (trained withγe​p​i​s​o​d​e=0\\gamma_{episode}=0italic_γ start_POSTSUBSCRIPT italic_e italic_p italic_i italic_s italic_o italic_d italic_e end_POSTSUBSCRIPT = 0) actions are initially stochastic (exploratory), becoming more deterministic (exploitative) as it accumulates experience, demonstrating that the agent can output pseudo-stochastic samples conditioned on contexts.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2508.01287/img/gridworld_reward_n30_X1024_gamma0_9.png",
                "caption": "Figure 4:Reward per episode in the Frozen Lake gridworld, comparing meta-RL (n=30n=30italic_n = 30X=1024X=1024italic_X = 1024γe​p​i​s​o​d​e=0.9\\gamma_{episode}=0.9italic_γ start_POSTSUBSCRIPT italic_e italic_p italic_i italic_s italic_o italic_d italic_e end_POSTSUBSCRIPT = 0.9) to random and oracle strategies. Shaded areas denote 95% confidence intervals. The meta-RL agent achieves significant gains, demonstrating that it gathers information (exploration) that is later used to gather rewards (exploitation).",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2508.01287/img/state_visitation_1.png",
                "caption": "Figure 5:State visitation heatmaps in the same gridworld (top: episodes 1–3; middle: 4–7; bottom: 8–30; over ten trials). Early episodes show wide exploration; later episodes focus on efficient exploitation of discovered paths to goals. This progression illustrates emergent exploration followed by exploitation.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2508.01287/img/state_visitation_2.png",
                "caption": "",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2508.01287/img/state_visitation_3.png",
                "caption": "",
                "position": 535
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]