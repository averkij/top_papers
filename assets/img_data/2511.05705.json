[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05705/x1.png",
                "caption": "Figure 1:Overview of our two-stage synthesis framework. First, we synthesize multiple-choice questions (MCQs) from dense captions and grounded object metadata, emphasizing scale and diversity while teaching basic cognitive behaviors (verification, backtracking, correction). Later, we harden questions by composing them into visual reasoning problems that requires decomposition and higher-order reasoning. For each stage, we also synthesize reasoning traces by first distilling CoTs from VLMs and then expanding them with reasoning LLMs, yielding traces that are in the distribution of VLM outputs yet richer in reasoning depth.",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05705/x2.png",
                "caption": "Figure 2:Scaling Behaviour of LPT vs Ours for SFT. We find that using additional metadata (here bounding boxes) in addition to highly details captions allows for more diverse and controlled generation of MCQ successfully scaling beyond 1M+ examples.",
                "position": 213
            },
            {
                "img": "https://arxiv.org/html/2511.05705/x3.png",
                "caption": "Figure 3:Reasoning trace comparison between our model (post-SFT and RL) and the vanilla base model.Both models initially fail to identify the dog in the image. The base model terminates with an incorrect answer based on this flawed premise. In contrast, our model demonstrates a non-linear reasoning process; it employs self-verification and backtracking to challenge and self-correct its initial assessment. This correction appears to stem from a trace where the model relies on captioning and grounding as a bridge between language and vision; notably grounding on the dog triggers the revised path on a second \"self-captioned\" verification structure. This behavior is notable as captions were not explicitly included in the training traces, perhaps suggesting captioning and grounding as part of the thinking process could be an emergent capability of training on our data.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2511.05705/figures/exp/correct_rollouts_stacked_percentage.png",
                "caption": "(a)Complexity estimation via multiple rollouts.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2511.05705/figures/exp/correct_rollouts_stacked_percentage.png",
                "caption": "(a)Complexity estimation via multiple rollouts.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2511.05705/figures/exp/cogitive_behavior.png",
                "caption": "(b)Analysis of Cognitive Behaviors in CoTs.",
                "position": 272
            }
        ]
    },
    {
        "header": "3Offline and Online Synthesis for RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05705/x4.png",
                "caption": "Figure 5:Quantitative and qualitative comparison of the post-training pipeline on our data vs pure RL on the base model.(Right)The graph illustrates the effect of scaling dataset size during online RL. The baseline (blue line), starting from an off-the-shelf model, exhibitsnegative scaling: performance peaks at 0.695 (10K samples) and degrades with more data. In contrast, our method (green line), which includes SFT on our high-quality data with complex reasoning traces, allows to scale online RL further. This suggests that without offline \"skill teaching\" via SFT, online RL fails to effectively utilize larger datasets.(Left)A qualitative example (from V* bench), using each model’s best checkpoint (indicated by a dot on the curve), highlights the resulting difference in reasoning. The baseline model fails to identify the partially obscured dog and answers incorrectly. Our model also initially expresses confusion but then self-corrects (\"Wait, I’m getting conflicting information…\"), showcasing a multi-step reasoning process to arrive at the correct answer. This self-correction capability, instilled with our data, is not observed in the baseline, indicating RL alone was insufficient to elicit this behavior.\nImage brightness was increased for illustration purposes.",
                "position": 488
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.05705/x5.png",
                "caption": "Figure 6:Stage 1 Generation Pipeline.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2511.05705/x6.png",
                "caption": "Figure 7:Additional qualitative example of a reasoning trace from the model post-trained on our data vs the base model.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2511.05705/x7.png",
                "caption": "Figure 8:Temporal reasoning improvement illustrated by a qualitative example of a reasoning trace from the Qwen-2.5 Omni model post-trained on our data, compared to the base Qwen-2.5 Omni model, on an unseen audio reasoning question involving joint speaking and sound events.",
                "position": 1644
            }
        ]
    },
    {
        "header": "Appendix ADetails on Data Generation Stages and Prompts",
        "images": []
    }
]