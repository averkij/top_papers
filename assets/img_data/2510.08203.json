[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08203/x1.png",
                "caption": "Figure 1:Function tokens can dynamically activate the most predictive features from the context to guide the next-token prediction. For example, the function token ‘in’ reactivates features ‘J.K. Rowling’ and ‘Location’ from context (while suppressing feature ’French’) and activates ‘England’ to predict ‘Britain’. In contrast, the content token ‘Harry’ activates feature ‘Harry Potter’.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08203/seed/fig1_zipf_law.png",
                "caption": "(a)Zip’f distribution of tokens on a log-log scale.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/fig1_zipf_law.png",
                "caption": "(a)Zip’f distribution of tokens on a log-log scale.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2510.08203/x2.png",
                "caption": "(b)The 15 most frequent tokens.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/token_distribution_of.png",
                "caption": "(a)Distribution of the function token ‘of’ across documents, showing uniform and dense coverage.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/token_distribution_of.png",
                "caption": "(a)Distribution of the function token ‘of’ across documents, showing uniform and dense coverage.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/token_distribution_Tokyo.png",
                "caption": "(b)Distribution of the content token ‘Tokyo’ across documents, showing sparse coverage.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/document_coverage.png",
                "caption": "(c)Document coverage versus token rank (ordered by frequency, log-log scale).",
                "position": 231
            }
        ]
    },
    {
        "header": "3Memory Retrieval through Function Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08203/x3.png",
                "caption": "Figure 4:Construction of the bipartite graph using token-feature activation pairs as edges. Nodes consist of tokens from the vocabulary and features from the SAE decomposition.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/gemma-9b-5M-token_degree_9.png",
                "caption": "(a)Layer 9",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/gemma-9b-5M-token_degree_9.png",
                "caption": "(a)Layer 9",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/gemma-9b-5M-token_degree_20.png",
                "caption": "(b)Layer 20",
                "position": 297
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/gemma-9b-5M-token_degree_31.png",
                "caption": "(c)Layer 31",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2510.08203/x4.png",
                "caption": "Figure 6:Function tokens can dynamically reactivate predictive features based on different contexts.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2510.08203/x5.png",
                "caption": "Figure 7:Response of Gemma2-9B-it when editing the activation at the final function token (‘\\n’) in the prompt. The Chinese terms shown in the table and their corresponding English translations are:日本(Japan),哈佛大学(Harvard University),故宫(The Forbidden City),英国(UK),牛津大学(Oxford University),伦敦眼(London Eye),俄罗斯(Russia),莫斯科国立大学(Moscow State University), and叶卡捷琳娜宫(Catherine Palace).",
                "position": 446
            }
        ]
    },
    {
        "header": "4Memory Consolidation through Function Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08203/seed/sae_learned_features.png",
                "caption": "(a)Number of learned features",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/sae_learned_features.png",
                "caption": "(a)Number of learned features",
                "position": 524
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/token_degree_by_ckpts_plot.png",
                "caption": "(b)Token degree by checkpoints",
                "position": 529
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/1.5b_token_group_loss.png",
                "caption": "(a)Grouped token loss trajectories during 1.5B model pre-training",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/1.5b_token_group_loss.png",
                "caption": "(a)Grouped token loss trajectories during 1.5B model pre-training",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/8b_token_group_loss.png",
                "caption": "(b)Grouped token loss trajectories during 8B model pre-training",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2510.08203/seed/func_as_next_token_loss.png",
                "caption": "(c)Next-token loss for typical function tokens in 1.5B model pretrain",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2510.08203/x6.png",
                "caption": "Figure 10:Next token predictions at different training steps. The first row shows the prompt. Each subsequent row shows the next-token predictions conditioned on all preceding tokens. For example, the third column uses ‘When young’ as input, and the fourth uses ‘When young children’ as input.",
                "position": 576
            }
        ]
    },
    {
        "header": "5Function Token Hypothesis",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion and Open Questions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Steering Method for Large Language Models",
        "images": []
    },
    {
        "header": "9Additional Case Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08203/x7.png",
                "caption": "Figure 11:Response of Gemma2-9B-it when editing the activation at the final function token (‘\\n’) in the prompt. The Chinese terms shown in the table and their corresponding English translations are:小雨(Xiaoyu, a common Chinese feminine nickname),啤酒(beer),意大利面(Carbonara),艾丽娅(Alia),威士忌(Whiskey),英国的鱼薯条(British fish and chips),安娜(Anna),伏特加(Vodka), and俄式肉丸子(Russian meatballs).",
                "position": 1503
            }
        ]
    },
    {
        "header": "10SAE Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08203/seed/sae_training_topk_1b_tok.png",
                "caption": "Figure 12:Cross-Entropy reconstruction scores under varyingλ\\lambdaValues",
                "position": 1510
            }
        ]
    },
    {
        "header": "11Function Token List",
        "images": []
    }
]