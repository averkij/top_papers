[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/Fin-R1-pipeline.png",
                "caption": "Figure 1:The pipeline for constructing Fin-R1. The diagram depicts the two-stage construction framework of Fin-R1: Data Generation (using DeepSeek-R1 for reasoning to generate CoT data, followed by quality filtering with the Qwen2.5-72B-Instruct) and Model Training (including SFT pretraining and GRPO optimization for Fin-R1). Additionally, the right side highlights the performance of Fin-R1 in financial code generation, professional knowledge, and business knowledge.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/data_construct.png",
                "caption": "Figure 2:Stage 1-The pipeline of data construction: (1) Data Distillation, (2) Answer Check, where an LLM evaluates the accuracy of responses generated by DeepSeek-R1, and (3) Reasoning Selection, where an LLM assesses and scores reasoning trajectories to ensure logical coherence and quality. \"Reasoning\" represents the reasoning output, while \"Thinking\" refers to the evaluation process of the judgment model.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/Data_distribution.jpg",
                "caption": "Figure 3:Composition structure of Fin-R1-Data: (1) Financial Code, (2) Financial Professional Knowledge, (3) Financial Reasoning Knowledge, and (4) Financial Non-Reasoning Knowledge.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/data_case.png",
                "caption": "Figure 4:Examples of high-quality and low-quality reasoning selections filtering",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/Fin-R1-GRPO.png",
                "caption": "Figure 5:Stage 2-The pipeline of training construction. During the SFT phase, the base model undergoes SFT using a structured reasoning-augmented dataset, focusing on enhancing its ability to perform financial reasoning. During the RL phase, we apply GRPO algorithm, which introduces a group computation mechanism to provide two reward signalsâ€”one for format correctness and one for content accuracy.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/decimal.png",
                "caption": "(a)Difference in decimal places.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/decimal.png",
                "caption": "(a)Difference in decimal places.",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/unit.png",
                "caption": "(b)Difference in expression.",
                "position": 572
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": []
    },
    {
        "header": "4Conclusion and Future work",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/data_distill_prompt.png",
                "caption": "Figure 7:The prompt of data distillation that we used",
                "position": 1294
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/reasoning_selection_prompt.png",
                "caption": "Figure 8:The Prompt for reasoning selection",
                "position": 1304
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/QwenHuman.png",
                "caption": "(a)Qwen2.5-72B-Instruct vs. Human",
                "position": 1310
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/QwenHuman.png",
                "caption": "(a)Qwen2.5-72B-Instruct vs. Human",
                "position": 1313
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/GPT4oHuman.png",
                "caption": "(b)GPT-4o vs. Human",
                "position": 1318
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/judge1.png",
                "caption": "Figure 10:The prompt for judging the model answer that we used.",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/judge2.png",
                "caption": "Figure 11:The prompt for judging the model answer that the content comes at the end.",
                "position": 1433
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/judge3.png",
                "caption": "Figure 12:The prompt for judging the model answer which is combined with the question.",
                "position": 1436
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/judge4.png",
                "caption": "Figure 13:The prompt for judging the model answer, which is combined with the question, comes at the end.",
                "position": 1439
            },
            {
                "img": "https://arxiv.org/html/2503.16252/extracted/6294171/judge5.png",
                "caption": "Figure 14:The Chinese prompt for judging the model answer.",
                "position": 1442
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]