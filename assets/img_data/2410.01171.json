[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01171/x1.png",
                "caption": "Figure 1:Left: a typical RAG setup proceeds in one language. Given a user query, an IR system retrieveskùëòkitalic_kmost relevant passages from a large database (Wikipedia). These passages are combined with the user query to form a prompt, and an LLM is queried for the answer.Right:xlRAGfollows the same overall pipeline, except passages are now in multiple languages, and retrieval can be done from several (or one) databases. For the given query, cross-lingual retrieval is especially interesting, as each document displays a different perspective, reflecting each culture‚Äôs take on the controversial issue. Here, the LLM was asked to cite supporting spans from each context.",
                "position": 154
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Cross-lingual Retrieval Augmented Generation",
        "images": []
    },
    {
        "header": "4TheBordIRlinesCross-lingual Retrieval Dataset",
        "images": []
    },
    {
        "header": "5Dataset Creation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01171/extracted/5894468/figures/WikiCollection4.png",
                "caption": "Figure 2:The data collection process for finding relevant Wikipedia articles. Given a query, territory, and languages, relevant multilingual passages are retrieved from Wikipedia and ranked by relevance.",
                "position": 337
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.01171/x2.png",
                "caption": "Figure 4:Illustrations for the 6 experimental settings, used for the case studies onxlRAG. For each prompt, we vary the languages in the passages (dashed line) and the prompt (solid line); where colors representEnglish,language 1, andlanguage 2.0: direct prompt without RAG.I: monolingual RAG.II: Two-languagexlRAG, with English queries.III: MultilingualxlRAG, with English queries and balanced languages used in the passages.IV: Two-language RAG, with English passages.V: MultilingualxlRAG, with native queries and balanced language passages.",
                "position": 420
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APer-Language Statistics forBordIRlinesPassages",
        "images": []
    },
    {
        "header": "Appendix BCase Study onxlRAG: Crimea",
        "images": []
    },
    {
        "header": "Appendix CCase Study for IR: Falkland Islands, Spanish",
        "images": []
    }
]