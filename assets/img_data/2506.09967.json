[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09967/extracted/6532785/figures/Resa-logo.png",
                "caption": "",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2506.09967/x1.png",
                "caption": "",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2506.09967/extracted/6532785/figures/wnb-logo.jpeg",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09967/x2.png",
                "caption": "Figure 1:Comparison between Example Resa Models and BaselinesThe Tina models correspond to the best checkpoints inWang etÂ al. (2025a). Resa-STILL and Resa-DeepScaleR correspond to Resa-STILL-v5 and Resa-DeepScaleR-v3 in Table4, respectively. For these Resa models, the required SAEs are trained from scratch (as shown in Section3.2) and both computational and time costs are total costs for training SAEs and models. Reasoning performance denotes the average zero-shot Pass@1 score across AIME24/25, AMC23, MATH500, GPQA Diamond, and Minerva benchmarks.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Resa: Transparent Reasoning Models via SAEs",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09967/x3.png",
                "caption": "Figure 2:Two-Stage Pipeline of SAE-TuningThe procedure begins with SAE training (Left), where an SAE is trained to capture reasoning features from a source model with a trigger dataset. During SAE-guided SFT (Right), the trained SAE is then frozen and inserted into a target model. An elicitation dataset is used to guide a SFT process to elicit the reasoning abilities in the target model. Notably, the trigger and elicitation datasets are usually the same CoT-free data. See Section2.1for a description of each component.",
                "position": 155
            }
        ]
    },
    {
        "header": "3Efficient Reasoning Ability Elicitation",
        "images": []
    },
    {
        "header": "4Hypothesis: Generalizable and Modular Reasoning Ability",
        "images": []
    },
    {
        "header": "5Hypothesis: Transparent Reasoning Feature Extraction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09967/x4.png",
                "caption": "Figure 3:Reasoning Feature Extraction(Left) This shows the layer-wise feature counts of the base R1-Distill model. (Middle) This shows the layer-wise feature counts of the Tina-STILL model. (Right) This shows the reasoning performance of the trained Resa models with different layer-wise SAEs when Tina-STILL is the source model.",
                "position": 1168
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFull Hyperparameter",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiment Results",
        "images": []
    }
]