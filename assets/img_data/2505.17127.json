[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/pixels_vs_priors.png",
                "caption": "Figure 1:Pixels Versus Priors Steering.We introduce a framework for controlling whether a vision-language model relies on visual input or memorized knowledge. Counterfactual visual evidence often overrides world knowledge priors.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Creating Visual CounterFact",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/counterfactuals_example.png",
                "caption": "Figure 2:Visual CounterFact.A new benchmark to study how VLMs utilize world knowledge compared to visual inputs. (Left) images created using color relations,\n(right) images created using size relations.",
                "position": 134
            }
        ]
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/task_vector_diagram.png",
                "caption": "Figure 3:Pixel Versus Prior steering vectors are created by contrasting representations of prompts that emphasize pixel-level information (“this”) versus priors (“most”), using the last hidden state.",
                "position": 211
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/early_decoding_llava-next.png",
                "caption": "Figure 4:Early decoding results on LLaVA-Next show a conflict between answering “world knowledge” using priors or answering “counterfact”.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/llava_attention_mass.png",
                "caption": "Figure 5:Effect of prompt changes and interventions on attention mass across layers for LLaVA-Next in the color and size tasks. Solid lines show changes when applying the steering vector; dashed lines show the effect of modifying the prompt. Green and purple lines represent attention shifts toward image and text tokens, respectively. The red shaded region highlights the layers where the intervention was applied (corresponding to Table3). We see that intervention has a much stronger effect than changing the prompt.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/steering_pca.png",
                "caption": "Figure 6:First two principal components of sentence embeddings of LLaVA-Next before and after steering from priors to pixels and from pixels to priors.",
                "position": 527
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACreating Visual CounterFact",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/more_examples.png",
                "caption": "Figure 7:Examples from our dataset. (Left) images created using color relations,\n(right) images created using size relations.",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/dataset_pipeline.png",
                "caption": "Figure 8:Construction pipeline for the Visual Counterfactual Dataset. We identify typical traits using semantic knowledge sources, retrieve realistic visual exemplars, and apply transformations to create perceptually plausible counterfactuals that conflict with language priors.",
                "position": 1213
            }
        ]
    },
    {
        "header": "Appendix BDataset Statistics and Examples",
        "images": []
    },
    {
        "header": "Appendix CAttention Mass Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/qwen_janus_steering.png",
                "caption": "Figure 9:First two principal components of sentence embeddings of Qwen2.5-VL and Janus-Pro before and after steering from priors to pixels and from pixels to priors.",
                "position": 1377
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/attention_mass_color.png",
                "caption": "Figure 10:Attention mass difference across layers for all models on the color task. Solid lines show changes from PvP steering vectors; dashed lines show prompt-only changes. Green represents attention to image tokens, purple to text tokens. Each subplot shows one model and intervention direction (WK→→\\rightarrow→CF or CF→→\\rightarrow→WK).",
                "position": 1380
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/attention_mass_size.png",
                "caption": "Figure 11:Attention mass difference across layers for all models on the size task. Solid lines show changes from PvP steering vectors; dashed lines show prompt-only changes. Each subplot shows one model and intervention direction (WK→→\\rightarrow→CF or CF→→\\rightarrow→WK).",
                "position": 1383
            }
        ]
    },
    {
        "header": "Appendix DEarly Decoding",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/early_decoding_janus.png",
                "caption": "Janus-Pro",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/early_decoding_janus.png",
                "caption": "Janus-Pro",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2505.17127/extracted/6464755/figures/early_decoding_qwen.png",
                "caption": "Qwen2.5-VL",
                "position": 1505
            }
        ]
    },
    {
        "header": "Appendix EPCA Results",
        "images": []
    }
]