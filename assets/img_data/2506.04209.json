[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04209/x1.png",
                "caption": "Figure 1:The qualitative comparisons between LIFT and CLIP[43]. The first line shows the caption or option selected by LIFT, and the second line shows the one selected by CLIP.\nIn every case, LIFT selects the correct one, while CLIP does not. We observe that LIFT compensates for CLIP‚Äôs shortcomings in tasks involving compositional information (e.g., spatial locations, object-attribute associations, object-object relations).",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x2.png",
                "caption": "Figure 2:The pipeline of LIFT, which adopts a dual-encoder architecture similar to CLIP[43]. LIFT uses an LLM-based text encoderftextsuperscriptùëìtextf^{\\text{text}}italic_f start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPTto pre-compute the embeddingzTsuperscriptùëßùëáz^{T}italic_z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTfor each text sampleTùëáTitalic_Toffline. During training, we solely update the image encoderfŒ∏imgsuperscriptsubscriptùëìùúÉimgf_{\\theta}^{\\text{img}}italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT img end_POSTSUPERSCRIPTand the projection headfœïheadsuperscriptsubscriptùëìitalic-œïheadf_{\\phi}^{\\text{head}}italic_f start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT start_POSTSUPERSCRIPT head end_POSTSUPERSCRIPTto align image embeddings with the pre-computed text embeddings by optimizing an alignment objective.",
                "position": 139
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04209/x3.png",
                "caption": "Figure 5:The original captions (top) and their negative counterparts (bottom) from two SugarCrepe[16]tasks:replace relation(left) andswap attribute(right).",
                "position": 235
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04209/x4.png",
                "caption": "Figure 6:The examples of syntactically similar but semantically different caption pairs from Recap-DataComp-1B[25]. The synthetic captions follow the template ‚ÄúA {Adj.} {Noun} with {N.¬†Phrase} {Verb} {Location}‚Äù. CLIP‚Äôs[43]text encoder often assigns higher scores to caption pairs with similar syntax, while LIFT better captures semantic differences and assigns lower ones. The scores are calculated based on the embeddings from LIFT‚Äôsftextsuperscriptùëìtextf^{\\text{text}}italic_f start_POSTSUPERSCRIPT text end_POSTSUPERSCRIPTand CLIP‚Äôs text encoder trained on 512M Recap-DataComp-1B samples.",
                "position": 629
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04209/x5.png",
                "caption": "Figure A1:The visualizations of theadd objecttask from SugarCrepe[16]. Each subfigure shows the captions selected by LIFT (top) and CLIP[43](bottom). In every case, LIFT selects the correct caption, while CLIP does not.",
                "position": 1547
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x6.png",
                "caption": "(a)Add Attribute",
                "position": 1551
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x6.png",
                "caption": "(a)Add Attribute",
                "position": 1554
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x7.png",
                "caption": "(b)Replace Object",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x8.png",
                "caption": "(a)Replace Attribute",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x8.png",
                "caption": "(a)Replace Attribute",
                "position": 1572
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x9.png",
                "caption": "(b)Replace Relation",
                "position": 1578
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x10.png",
                "caption": "(a)Swap Object",
                "position": 1587
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x10.png",
                "caption": "(a)Swap Object",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x11.png",
                "caption": "(b)Swap Attribute",
                "position": 1596
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x12.png",
                "caption": "Figure A5:The visualizations of theobject localizationsubtask from MMBench[31]. Each subfigure shows the options selected byLIFTandCLIP[43]. In every case, LIFT selects the correct option, while CLIP does not.",
                "position": 1612
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x13.png",
                "caption": "(a)Attribute Recognition",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x13.png",
                "caption": "(a)Attribute Recognition",
                "position": 1619
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x14.png",
                "caption": "(b)Physical Relation",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x15.png",
                "caption": "(a)Spatial Relation",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x15.png",
                "caption": "(a)Spatial Relation",
                "position": 1637
            },
            {
                "img": "https://arxiv.org/html/2506.04209/x16.png",
                "caption": "(b)Counting",
                "position": 1643
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]