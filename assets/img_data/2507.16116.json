[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16116/x1.png",
                "caption": "Figure 1:Overview of Pusa's performance and efficiency. Specifically, Pusa outperforms Wan-I2V on Vbench-I2V with only≤1/2500absent12500\\leq 1/2500≤ 1 / 2500dataset,≤1/200absent1200\\leq 1/200≤ 1 / 200training budget, and1/5151/51 / 5inference steps. Besides, Wan-I2V can only do image-to-video generation, while the same Pusa model has many other capabilities including: start-end frames, video extension, text-to-video, and so on.",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16116/x2.png",
                "caption": "Figure 2:Paradigm comparison between (c) Pusa and (b) Wan-I2V, both support image-to-video (I2V) generation, and are finetuned from a text-to-video model (a) Wan-T2V. Specifically, Wan-I2V modifies the model with an additional mask mechanism and adds a clip embedding of the condition image to enable I2V capability. However, this is a destructive adaptation of the original model that changes the model's input and internal calculation process, which indicates it cannot fully utilize the pretrained priors of the base model. In contrast, our proposed model, Pusa, only inflates the model's timestep variable from a scalar to a vector, which is a non-destructive adaptation. With this method, Pusa can fully utilize the pretrained priors and use much less resources to learn temporal dynamics. Regarding the I2V task, Pusa achieves unprecedented efficiency, surpassing Wan-I2V with≤1/2500absent12500\\leq 1/2500≤ 1 / 2500training data, revolutionizing the video diffusion paradigm.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/video_frames_comparison.png",
                "caption": "Figure 3:Image-to-video results, where our method generates a smooth and realistic animation of the first frame while Wan2.1-T2V generates completely different frames from the first frame, though aligned with text prompt, and Wan2.1-I2V generates frames with noticeable distortions. All generated with the same input image (Frame 0) and text prompt: \"A man in a black suit and a sombrero, shouting loudly\".",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/attention_comparison.png",
                "caption": "Figure 4:Visualization of attention maps corresponding to the generation of videos in Fig.3. Each value in the attention map represents frame-to-frame correlation/attention, a larger value means higher correlation. Zoom in for better view.",
                "position": 1395
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/parameter_analysis.png",
                "caption": "Figure 5:Analysis on finetuned model's parameter shifts. The cloumns, from left to right, represent average relative change of parameters by model component/modules, top 20 parameters with largest relative change, and average change by transformer blocks (in total 40 blocks from 0 to 39). Overall, our model has less variations to Wan2.1-T2V compared to Wan2.1-I2V, which indicates our model preserves the original distribution better. Note that for our model, we use our final configurations in Table1, i.e., Lora rank 512 with lora 1.4 at 900 steps. Zoom in for a better view.",
                "position": 1416
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/i2v_gen_comparison.png",
                "caption": "Figure 6:More image-to-video results. The first frames of each row are the given condition images extracted from Veo2 & Sora demos. Each generated video has 81 frames in total.",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/start_and_end_gen_comparison.png",
                "caption": "Figure 7:Zero-shot results w.r.t. start & end frames to video.\nThe first and last frames are given condition frames extracted from Veo2 & Sora demos. Each generated video has 81 frames in total.",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/start_and_end_gen_comparison_noise.png",
                "caption": "Figure 8:Zero-shot results w.r.t. start & end frames with noise.\nThe first and last frames are given conditions and added 30% and 70% noise during sampling to make the generated video more coherent. Condition frames are extracted from Veo2 & Sora demos. Each generated video has 81 frames in total.",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/start_and_4_end_gen_comparison.png",
                "caption": "Figure 9:Zero-shot results w.r.t. start & end frames to video.\nThe first and last 4 frames (encoded to one latent frame) are given condition frames extracted from Veo2 & Sora demos. Each generated video has 81 frames/21 latent frame in total.",
                "position": 1513
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/outputs_v2v_transition_0-2_18-20-min.png",
                "caption": "Figure 10:Zero-shot results w.r.t. video completion/transition.\nThe first 9 frames and the last 12 frames extracted from Veo2 demos are given as conditions and encoded to the first 3 latent frames and the last 3 latent frames, respectively. Each generated video has 81 frames/21 latent frames in total.",
                "position": 1517
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/outputs_v2v_extension_0_3-min.png",
                "caption": "Figure 11:Zero-shot results w.r.t. video extension.\nThe first 13 frames extracted from Veo2 demos are given as conditions and encoded to the first 4 latent frames. Each generated video has 81 frames/21 latent frames in total.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/outputs_v2v_extension_0_10-min.png",
                "caption": "Figure 12:Zero-shot results w.r.t. video extension.\nThe first 41 frames extracted from Veo2 demos are given as conditions and encoded to the first 11 latent frames. Each generated video has 81 frames/21 latent frames in total.",
                "position": 1525
            },
            {
                "img": "https://arxiv.org/html/2507.16116/extracted/6641773/figures/t2v_gen_comparison.png",
                "caption": "Figure 13:Text-to-video results. Prompts all from Vbench2.0.",
                "position": 1529
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]