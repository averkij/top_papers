[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16978/x1.png",
                "caption": "Figure 1:Construction pipeline of LongShOTBench.The pipeline begins with raw video data where speech, visuals, and audio cues are extracted. These are passed into multimodal processing to generate segment-wise aligned and fused metadata. Only the distilled information flows to question design, where scenarios and question types are mapped, followed by the generation of questions and conversational answers. Next, verifiable rubrics are created to evaluate correctness and difficulty. Finally, the core dataset, comprising Q&A pairs and tailored evaluation rubrics, is manually reviewed and corrected by human validators, ensuring a clean, reliable benchmark.",
                "position": 445
            }
        ]
    },
    {
        "header": "3Constructing LongShOTBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16978/x2.png",
                "caption": "Figure 2:LongShOTAgent Pipeline.The orchestrator agent (Qwen3-4B) receives a user query and video input, then calls the Preprocessor to extract multimodal signals, including Whisper-small speech transcription, scene-based frame sampling, SigLIP embeddings, OCR, and audio analysis. These features populate a vector database, which the Search tool queries to retrieve top-k relevant segments via semantic similarity. For deeper analysis, the orchestrator invokes Refiner tools such as Whisper-large-v3 for high-quality speech transcription, Audio-Flamingo-3 for detailed audio understanding, and a Video Refiner for dense caption generation. Beyond these core modules, LongShOTAgent can access external tools including activity detection, web search, and other APIs to expand reasoning and retrieve additional context when needed. By flexibly sequencing preprocessing, search, refinement, and external tool calls, the orchestrator integrates multimodal evidence and auxiliary knowledge to generate a coherent final answer, demonstrating adaptive and agentic coordination across heterogeneous capabilities.",
                "position": 591
            }
        ]
    },
    {
        "header": "4LongShOTAgent",
        "images": []
    },
    {
        "header": "5Benchmarking LongShOTBench",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Ethics Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Appendix ADetails of LongShOTBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.16978/figures/01_duration_distribution_per_video.png",
                "caption": "Figure 3:Distribution of video durations (minutes)in our validated sample set (n=157n=157).",
                "position": 1095
            },
            {
                "img": "https://arxiv.org/html/2512.16978/figures/06_category_distribution_topN_per_video.png",
                "caption": "Figure 4:Video category distribution.Each video may belong to multiple categories.",
                "position": 1489
            },
            {
                "img": "https://arxiv.org/html/2512.16978/x3.png",
                "caption": "Figure 5:Evaluation Examples of LongShOTBench. The data samples illustrate how we construct scenario context, model a userâ€™s thought process, generate diverse questions (single- and multi-turn), and apply criterion-weighted evaluation rubrics for interpretable scoring.",
                "position": 1492
            }
        ]
    },
    {
        "header": "Appendix BAdditional Details on the Agentic Pipeline",
        "images": []
    },
    {
        "header": "Appendix CPrompts for Dataset Generation Pipeline",
        "images": []
    }
]