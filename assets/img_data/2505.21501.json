[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21501/x1.png",
                "caption": "Figure 1:Effect of PH-Reg on open-vocabulary segmentation.For each image, we compare four methods: MaskCLIP which directly takes thevaluefeatures from the last attention layer; SCLIP which adds correlative self-attention; NACLIP which further enforces a locality bias; and our PH-Reg method with self-distilled registers. We utilize the sameOpenAI CLIP ViT-B/16weights for all three methods. For each method, we visualize the UMAP of the dense features and a heatmap of one text query. Our method yields noticeably cleaner dense features and high quality localizations, and requires only a small set of additional register parameters compared to the original network.",
                "position": 150
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21501/x2.png",
                "caption": "Figure 2:Learning framework of PH-Reg.(a)Our framework begins by creating two networks from the same set of weights. In the teacher, the weights are frozen and unmodified. In the student, the only additional parameters are learnable register tokens. The teacher creates a learning target using denoised representations.(b)An image‚Ñê‚Ñê\\mathcal{I}caligraphic_Iundergoes augmentation by functionTùëáTitalic_Twith random augmentation parameters consisting of random offsets and horizontal flips.(c)Given an RGB image, we utilize UMAP to visualize the features, and a heatmap using CLIP text query. Our method can produce significantly cleaner dense representations with minimal additional inference cost.",
                "position": 162
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21501/x3.png",
                "caption": "Figure 3:Denoising teacher representations with augmentations.For each model, we visualize the UMAP of dense features before and after applying test-time augmentation. The results show that our proposed method produces noticeably cleaner dense feature representations without requiring gradient-based learning. Please zoom in for details.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x4.png",
                "caption": "Figure 4:Visualization of Open Vocabulary Segmentation.We compare against MaskCLIP, SCLIP, NACLIP, and find that our method yields clean feature maps free of artifacts.",
                "position": 200
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21501/x5.png",
                "caption": "(a)Registers‚Äô behavior.This plot illustrates adding registers improve PH-Reg teacher performance. In the blue settings, only registers are unfreezed. The green settings represent the improvements when positional embeddings are unlocked additionally. The red settings represent performance of unlocking more layers.",
                "position": 599
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x5.png",
                "caption": "(a)Registers‚Äô behavior.This plot illustrates adding registers improve PH-Reg teacher performance. In the blue settings, only registers are unfreezed. The green settings represent the improvements when positional embeddings are unlocked additionally. The red settings represent performance of unlocking more layers.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x6.png",
                "caption": "(b)Augmentations improves cosine similarity.The plot illustrates how increasing the number of augmentations improves the alignment of the model‚Äôs predictions with the target of 200 augmentations‚Äô features, as measured by cosine similarity.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x7.png",
                "caption": "Figure 6:Comparison of original and PH-Reg features and norms.While prior work has noted artifact tokens in DINOv2 as having higher norm than other tokens, we observe this is not the case for all models. Some models have artifact tokens with lower magnitude.",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x8.png",
                "caption": "Figure 7:Patch Norms.This figure illustrate norms of patch tokens of different backbones. Our method effectively reduces the variance of token norms and reduces the outliers, regardless if the artifacts are lower/higher norm.",
                "position": 638
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix BImplementation details for Self-Distillation",
        "images": []
    },
    {
        "header": "Appendix CImplementation details for Quantitative Evaluation",
        "images": []
    },
    {
        "header": "Appendix DAdditional Qualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21501/x9.png",
                "caption": "Figure S.1:Open-vocabulary semantic segmantation qualitative comparision between different baseline models on ADE20K.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x10.png",
                "caption": "Figure S.2:Open-vocabulary semantic segmantation qualitative comparision between different baseline models on Pascal Context59.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x11.png",
                "caption": "Figure S.3:Open-vocabulary semantic segmantation qualitative comparision between different baseline models on COCO Obejct.",
                "position": 2006
            }
        ]
    },
    {
        "header": "Appendix EAdditional Qualitative Heatmaps for PH-Reg Zero-Shot",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21501/x12.png",
                "caption": "Figure S.4:Zero-shot heatmap results.Our results have fewer artifacts than other methods.",
                "position": 2014
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x13.png",
                "caption": "Figure S.5:Zero-shot heatmap results.Our results have fewer artifacts than other methods.",
                "position": 2017
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x14.png",
                "caption": "Figure S.6:Zero-shot heatmap results.Our results have fewer artifacts than other methods.",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2505.21501/x15.png",
                "caption": "Figure S.7:Zero-shot heatmap results.Our results have fewer artifacts than other methods.",
                "position": 2024
            }
        ]
    },
    {
        "header": "Appendix FOptimal Feature Aggregation",
        "images": []
    }
]