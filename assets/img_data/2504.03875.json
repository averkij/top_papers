[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03875/x1.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03875/x2.png",
                "caption": "Figure 2:LRASArchitecture.A. Quantization:We train a small, patch local, convolutional autoencoder with a 16 bit LFQ codebook.B. Serialization:We serialize the codes into sequences using the pointer-content representation, which allows us to arbitrarily order the patches during training and generation.C. Local Random Access Sequence Modeling: We train an LLM-like autoregressive transformer to predict the contents of the next patch, shuffled in random order.D. Sequence Design With Optical Flow: We design sequences of tokens that contain optical flow intermediates, to provide robust control over the generation. We train two models:LRASRGBsubscriptLRASRGB\\textbf{{LRAS}}_{\\textbf{{RGB}}}LRAS start_POSTSUBSCRIPT RGB end_POSTSUBSCRIPT, which is conditioned on a source RGB image and an optical flow describing the desired transformation to predict the next frame, andLRASFLOWsubscriptLRASFLOW\\textbf{{LRAS}}_{\\textbf{{FLOW}}}LRAS start_POSTSUBSCRIPT FLOW end_POSTSUBSCRIPT, which is conditioned on a source RGB image to predict a plausible optical flow field.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03875/x3.png",
                "caption": "Figure 3:3D Scene Editing Through Flow Field Manipulation:We perform 3D scene edits by constructing optical flow fields corresponding to the desired transformations - either camera or object motion in 3D.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03875/x4.png",
                "caption": "Figure 4:Novel view synthesis from a single image.The results show that our model performs controllable novel view synthesis with various camera motions in a diverse scenes. Compared to other models, the reconstructed images do not show abrupt change in object and scene identity. See supplementary for more results.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2504.03875/x5.png",
                "caption": "Figure 5:3D object manipulation from a single image.We show that our model can perform both 3D object translation and rotation. Compared to the other methods, our model preserves object identity on real world images, and produces more photorealisic generated images with accurate object edits. See supplementary for more results.",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2504.03875/x6.png",
                "caption": "Figure 6:Self-supervised monocular depth estimation.On static scenes, our model performs comparably well to existing self-supervised depth estimation methods. However, when there are dynamic objects in the scene, our model significantly outperforms geometric-consistency-based methods, demonstrating its robustness in handling moving objects. Yellow artifacts in ground truth depth maps are noise and excluded during the evaluation.",
                "position": 362
            }
        ]
    },
    {
        "header": "5Discussion & Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": []
    },
    {
        "header": "Appendix BNVS Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03875/x7.png",
                "caption": "Figure 7:Amodal Completion.We compare our self-supervised amodal depth reasoning with the inpainting method. The inpainting method struggles with underdetermined scene changes, as it lacks explicit control over object removal. In contrast, the flow-based physical scene editing approach conditions object removal more precisely, resulting in more reliable amodal reasoning.",
                "position": 1424
            }
        ]
    },
    {
        "header": "Appendix CAmodal Completion",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.03875/x8.png",
                "caption": "Figure 8:Additional results on novel view synthesis from a single image.The results show that our model performs controllable novel view synthesis with various camera motions in a diverse scenes. Compared to other models, the reconstructed images do not show abrupt change in object and scene identity.",
                "position": 1450
            },
            {
                "img": "https://arxiv.org/html/2504.03875/x9.png",
                "caption": "Figure 9:Additional results on 3D object manipulation from a single image.The results show that our model can perform both object translation and rotation in 3D. Compared to the other methods, our model does not change the object identity even for in-the-wild real world images.",
                "position": 1453
            }
        ]
    },
    {
        "header": "Appendix DAdditional Qualitative results on NVS and object manipulations",
        "images": []
    }
]