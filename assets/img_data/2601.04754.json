[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04754/fig1.png",
                "caption": "Figure 1:Overview ofProFuse.Left:A dense matcher supplies cross-view geometric and semantic correspondences.Top:Warped masks are grouped into 3D Context Proposals with a shared global feature.Bottom:Triangulated matches initialize a compact Gaussian scene, and proposal features are fused without render supervision for coherent open-vocabulary 3D semantics.",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04754/fig2.png",
                "caption": "Figure 2:Pre-registration.For each reference view we selectKKneighbors via view clustering, then apply a pre-trained dense matcher to obtain per-pixel warpsWj→iW_{j\\!\\to i}and confidencesαj→i\\alpha_{j\\!\\to i}.Bottom right:Given the warps of apixel pair, we triangulate a 3D seed point for Gaussian initialization.Top right:Warped IoU comparison on every reference–neighbormask pair; masks that pass the selection form edges of a bipartite graph.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04754/fig3.png",
                "caption": "Figure 3:From context proposal to global feature.Left: masks of the same entity are grouped into a 3D Context Proposal. Center: for a pixelpp, the renderer returns the top-KKGaussians with contributions{ωi,p,t}t=1K\\{\\omega_{i,p,t}\\}_{t=1}^{K}, from which themask massμ​(Mik)\\mu\\!\\left(M_{i}^{k}\\right)is computed. Right: a mass-weighted pool of member mask embeddings forms the proposal feature, which is registered to Gaussians via Eq. (8).",
                "position": 183
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.04754/fig4.png",
                "caption": "Figure 4:Qualitative comparison of object-level semantic queries on the LERF-OVS[15]dataset. Our method produces more accurate and cleaner object retrieval, showing sharper correspondence between the text query and the selected 3D content.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2601.04754/fig5.png",
                "caption": "Figure 5:Feature visualizations on the ScanNet[6]dataset using registration-based methods. Colors represent normalized language features transferred to mesh vertices and rendered via a fixed RGB projection. ProFuse produces cleaner regions with sharper boundaries and fewer speckles.",
                "position": 530
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]