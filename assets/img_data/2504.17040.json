[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/teaser_long.png",
                "caption": "",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/emojis/cross.png",
                "caption": "",
                "position": 147
            },
            {
                "img": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/emojis/check.png",
                "caption": "",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/method.png",
                "caption": "Figure 2:Method Overview.\\ours, is composed of two key ideas: Dynamic Token Merging (DToMe) and Virtual Token Unmerging (VTU).\nDToMe first determines per‚Äêlayer thresholds (left) by feeding a large batch of images into the vision transformer and computing bipartite token similarities. We rank these edges across theentire batchand choose the top-B‚Å¢rùêµùëüBritalic_B italic_r(r=ùëüabsentr=italic_r =desiredaveragenumber of tokens, batch sizeBùêµBitalic_B). This leads to more edges from simpler images (with more redundancy) being chosen, while complex images remain less merged. During inference, DToMe merges tokens on a per‚Äêimage basis using these pre-computed thresholds. We then apply VTU (right) in the self‚Äêattention layers of the pretrained VLM to efficiently expand the attention matrices to the standard token count‚Äîensuring the model‚Äôs original weights and outputs remain compatible‚Äîbefore re‚Äêmerging the tokens for the next layer. The overall process is training‚Äêfree and utilizes crucial image information by allocating the token budget more effectively for both simple and complex images.",
                "position": 527
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17040/x1.png",
                "caption": "Figure 3:Image Complexity vs Token Count and AccuracyThe scatter plot (left) demonstrates a strong correlation between DyMU‚Äôs token count and image complexity score‚Äîmore complex images naturally receive more tokens. On theright, MME accuracy at varying complexity levels is compared between ToMe (fixed-length) and DyMU (dynamic-length), highlighting the benefit of assigning additional tokens to complex images.",
                "position": 1479
            },
            {
                "img": "https://arxiv.org/html/2504.17040/x2.png",
                "caption": "Figure 4:Importance of Virtual Token Unmerging (VTU).We ablate the performance of LLaVA¬†1.5 with two token reduction methods applied to the visual encoder‚ÄîToMe (fixed‚Äêlength) and DToMe (variable‚Äêlength). We observe that applying VTU significantly improves performance on 8 out of 9 benchmarks, demonstrating robustness to varied token reduction methods.",
                "position": 1482
            },
            {
                "img": "https://arxiv.org/html/2504.17040/x3.png",
                "caption": "Figure 5:Comparing thresholds using LLaVA Instruct Data vs Pixmo-Cap. Although both methods use the same per‚Äêlayer merging hyperparameter (risubscriptùëüùëñr_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), the Pixmo‚Äêbased thresholds lead to fewer tokens (top)‚Äîlikely due to domain differences.\nHowever, performance across a range of benchmarks shows minimal drop (bottom),\nindicating the robustness of our threshold estimation.",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/qualitative_v2.png",
                "caption": "Figure 6:Controllable Visual Token Length.By dynamically allocating tokens based on image complexity,\\oursenables direct control over computational cost. In these examples, we combine\\ourswith additional vision tools‚Äîbackground removal, OCR, or object detection‚Äîto focus only on the relevant regions. As a result, token count is substantially reduced without degrading performance, showcasing the flexibility of\\oursto adapt token usage according to the task‚Äôs requirements.",
                "position": 1490
            }
        ]
    },
    {
        "header": "5Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVisualization of Variable Token Length",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17040/x4.png",
                "caption": "Figure 7:DToMe Token Count Across Benchmarks.For each dataset, we show three examples processed by our method‚Äîthose yielding the fewest tokens, the median number of tokens, and the most tokens. Observe that visually simple or nearly blank images consistently require fewer tokens, while more detailed, semantically complex or cluttered images produce more tokens. This demonstrates how DToMe effectively adapts to image complexity across diverse benchmarks, allocating fewer tokens to simpler content and preserving more tokens for complex scenes.",
                "position": 2167
            }
        ]
    },
    {
        "header": "Appendix BImpact of Token Merging Schedule",
        "images": []
    },
    {
        "header": "Appendix CFull Results for Figure4",
        "images": []
    },
    {
        "header": "Appendix DFull Results for Figure5",
        "images": []
    }
]