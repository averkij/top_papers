[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02659/x1.png",
                "caption": "Figure 1:Overview of the OmniDraft framework: during cross-vocabulary speculative decoding, the drafter (Llama-68M) generates multiple tokensdisubscriptùëëùëñd_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTwith corresponding distributionsqisubscriptùëûùëñq_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Cross-vocabulary translator then converts the drafter tokens into tokens in the vocabulary of the target model (Llama3-8B). In this example, tokend0subscriptùëë0d_{0}italic_d start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT(‚ÄôSnow‚Äô) andd4subscriptùëë4d_{4}italic_d start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT(‚Äôis‚Äô) are directly mapped to target tokenst0subscriptùë°0t_{0}italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandt2subscriptùë°2t_{2}italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, while tokend1subscriptùëë1d_{1}italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(‚Äôf‚Äô),d2subscriptùëë2d_{2}italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(‚Äôla‚Äô) andd3subscriptùëë3d_{3}italic_d start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT(‚Äôke‚Äô) are merged into a single target tokent1subscriptùë°1t_{1}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(‚Äôflake‚Äô), since there is a mapping item in the n-gram cache. The translated proposaltisubscriptùë°ùëñt_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTalong with combined probabilitiesqi‚Ä≤subscriptsuperscriptùëû‚Ä≤ùëñq^{\\prime}_{i}italic_q start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTis verified by the target model, resulting int0subscriptùë°0t_{0}italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTandt1subscriptùë°1t_{1}italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTbeing accepted whilet2subscriptùë°2t_{2}italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTbeing rejected and replaced byt2‚Ä≤subscriptsuperscriptùë°‚Ä≤2t^{\\prime}_{2}italic_t start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The target outputs tokens and their probabilitiespisubscriptùëùùëñp_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTare translated into drafter tokens and sent back to drafter for next round of drafting. The n-gram cache is updated by inserting a new unseen item (‚Äôst‚Äô,‚Äôamps‚Äô->‚Äôstamps‚Äô). Meanwhile, the accepted and corrected tokens from the target model are used to align the drafter through online cross-vocabulary distillation.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02659/extracted/6564826/images/training_curves_qwen.png",
                "caption": "Figure 2:Cross-vocabulary SpD online distillation on Llama-68M with Qwen2-7B as target",
                "position": 602
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02659/extracted/6564826/images/adaptive_drafting_training_curves.png",
                "caption": "Figure 3:Online Adaptive Drafting training plot of Llama-68M vs Vicuna-7B",
                "position": 1748
            },
            {
                "img": "https://arxiv.org/html/2507.02659/extracted/6564826/images/training_curves_llama3.png",
                "caption": "Figure 4:Cross-vocabulary SpD online distillation on Llama-68M with Llama3-8B as target",
                "position": 1842
            }
        ]
    },
    {
        "header": "Appendix CAdditional Algorithm Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.02659/extracted/6564826/images/ngram_freq_hist.png",
                "caption": "Figure 5:Cross-vocabulary N-gram Distributions of Llama-68M and Qwen2-7B",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2507.02659/extracted/6564826/images/ngram_training.png",
                "caption": "Figure 6:Evolution of n-gram tokens learning with Llama-68M and Llama3-8B onMBPP+HumanEvaldataset.N-gram hitrefers to the number of n-grams proposed by drafter in each response, averaged over query samples.Acceptance ratiorefers to the quantitymin‚Å°(1,p/q)1ùëùùëû\\min(1,p/q)roman_min ( 1 , italic_p / italic_q )in speculative decoding, it is averaged over proposal steps with an n-gram token and is indicative of the distribution alignment between the drafter and target. These two metrics show cross-vocabulary distillation learns to slowly align n-gram distributions to the target and utilize them more in inference.",
                "position": 2261
            }
        ]
    },
    {
        "header": "Appendix DCross-vocabulary N-gram Cache Ablations",
        "images": []
    }
]