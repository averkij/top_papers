[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/icon_point.png",
                "caption": "",
                "position": 48
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f1.jpg",
                "caption": "Figure 1:Overview ofPointArena.PointArena consists of three components:Point-Bench, a curated dataset for evaluating grounded pointing across five reasoning types;Point-Battle, a live platform for blind, pairwise model comparisons with user voting; andPoint-Act, real-world task involving manipulation via pointing-based language commands.",
                "position": 74
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3PointArena",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f2.png",
                "caption": "Figure 2:Overview of the five Point-Bench categories and the annotation UI.Point-Bench includes 982 image-query pairs grouped into five categories:Spatial(positional references),Affordance(functional part identification),Counting(attribute-based grouping),Steerable(relative pointing), andReasoning(open-ended visual inference). Each example shows a representative query and the corresponding target. On the right, we show the Gradio-based annotation interface used to collect and refine segmentation masks. Initial masks are generated using SAM and refined by annotators, followed by manual verification.",
                "position": 152
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f3.png",
                "caption": "Figure 3:Success rates of MLLMs on Point-Benchacross six task categories:Spatial,Affordance,Counting,Steerable,Reasoning, andAverage. Each bar represents the mean success rate (%) for a given model, with error bars indicating standard deviation across three evaluation runs. The ‚ÄúHuman‚Äù bar serves as an upper-bound reference. The results demonstrate substantial performance disparities, with top models (e.g., GPT-4o, Gemini-2.5-Pro, Molmo-72B) achieving near-human accuracy in select categories, while others (e.g., LLaVA, Grok, and Claude) consistently underperform.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f4.jpg",
                "caption": "Figure 4:Qualitative predictions across Point-Bench categories.Example model predictions are shown for each of the five Point-Bench categories:Spatial,Affordance,Counting,Steerable, andReasoning. Each colored dot corresponds to a prediction from a different MLLM, labeled by model name in the legend. These examples highlight the diversity of pointing behaviors and the variation in performance across models.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f5.jpg",
                "caption": "Figure 5:Insights drawn from Point-Battle and Point-Bench.(a) This figure shows Point-Bench performance (%) of MLLMs over time, grouped by model family. A sharp performance increase is observed in models released after the PixMo dataset (dashed line, December 2024). Notably,GPT4.1improves by 21.1 percentage points overGPT-4-Turbo, andGemini-2.0-Flashimproves by 45.9 points overGemini-1.5-Flash. These trends suggest that newer proprietary models may incorporate pointing supervision, potentially derived from or inspired by PixMo. (b) Linear regression of the five models common to Point-Battle and Point-Bench reveals a strong correlation (R2=0.85superscriptùëÖ20.85R^{2}=0.85italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.85), confirming close agreement between the two evaluation frameworks. (c) Performance of open-source models as a function of parameter count. While there is a slight upward trend, the performance gains with increasing model size are marginal, suggesting diminishing returns and limited sensitivity to scale within this range.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f6.jpg",
                "caption": "Figure 6:Performance on Human Preference Evaluation with Point-Battle.We collected over 4,500 votes from more than 100 global participants. Based on the Elo ratings derived from these votes, we observed a clear preference for outputs from open-source models such asMolmo-7B-DandQwen2.5-VL-7B, which consistently outperformed proprietary models in terms of human preference.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2505.09990/extracted/6440911/figure/f7.png",
                "caption": "Figure 7:Overview of the Point-Act system.(a) The Point-Act manipulation setup enables remote control of a real-world xArm 6 Lite robot via language instructions, allowing users to evaluate pointing MLLMs. (b) User-blind evaluations and SUS preference scores collected for each model.",
                "position": 386
            }
        ]
    },
    {
        "header": "5Limitations, Discussion, and Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]