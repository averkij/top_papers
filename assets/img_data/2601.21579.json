[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21579/x1.png",
                "caption": "Figure 1:Illustration of variants of manifold-constrained hyper-connections with a residual stream widthn=8n=8.(a) mHC: utilizes iterative Sinkhorn-Knopp (SK) algorithm to approximate a doubly stochastic residual matrix;(b) mHC-lite: builds the residual matrix as convex combinations ofn!n!permutation matrices, but becomes infeasible for a largenn;(c) KromHC (Ours): constructs the residual matrix as the Kronecker products of smaller (e.g.,2√ó22\\times 2) doubly stochastic matrices, thus guaranteeing double stochasticity while remaining parameter efficient.",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21579/x2.png",
                "caption": "Figure 2:Numerical stability analysis of the products of residual matrices. The plot compares the Mean Absolute Error (MAE) between the column sum of‚àèi=0L‚àí1ùêáL‚àíires\\prod_{i=0}^{L-1}\\mathbf{H}^{\\mathrm{res}}_{L-i}and11in an LLM withD=12D=12transformer blocks andL=24L=24layers of HC. The standard mHC architecture exhibits a MAE of around0.050.05, indicating potential training instabilities. The mHC-lite and KromHC have exact doubly stochastic matrices, thus yielding zero MAE.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2601.21579/x3.png",
                "caption": "Figure 3:The number of learnable parameters against the number of residual streams,nn, per hyper-connection in mHC, mHC-lite, and KromHC. We assume the feature dimension,CC, to be 512. Also,nnis factored into‚àèm=1log2‚Å°(n)2\\prod_{m=1}^{\\log_{2}(n)}2, i.e.,i1=i2=‚ãØ=iK=2i_{1}=i_{2}=\\cdots=i_{K}=2.",
                "position": 231
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Notation and Preliminaries",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21579/x4.png",
                "caption": "Figure 4:Training loss and validation BPB gaps of KromHC at different widths of the residual stream,nn, compared ton=4n=4. Exponential Moving Average (EMA) is applied to the raw loss before the calculation of the loss gap.",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2601.21579/x5.png",
                "caption": "Figure 5:Zoomed-in view of gradient norms from 5000 to 7000 steps during training. Trajectories are smoothed using EMA, with shaded regions indicating the EMA variance.",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2601.21579/x6.png",
                "caption": "Figure 6:SharingŒ±lres\\alpha_{l}^{\\text{res}}across all doubly stochastic matricesùêîlk\\mathbf{U}_{l}^{k}outperforms the use of matrix-specificŒ±lres,k\\alpha_{l}^{\\text{res},k}. The experiment was conducted with1212transformer blocks andn=4n=4residual streams.",
                "position": 1089
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AKronecker Product",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21579/x7.png",
                "caption": "Figure 7:Tensor network diagram of the proposed KromHC method.",
                "position": 1641
            }
        ]
    },
    {
        "header": "Appendix BProof for TheoremB.1",
        "images": []
    },
    {
        "header": "Appendix CDetails of the CORE Tasks",
        "images": []
    },
    {
        "header": "Appendix DTensor Network Diagram of KromHC",
        "images": []
    },
    {
        "header": "Appendix EParametrization of HC",
        "images": []
    },
    {
        "header": "Appendix FParametrization of mHC-lite",
        "images": []
    },
    {
        "header": "Appendix GNanochat",
        "images": []
    },
    {
        "header": "Appendix HHyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21579/x8.png",
                "caption": "Figure 8:Gradient norm dynamics across training. Raw gradient norm across 7000 training steps.",
                "position": 1908
            }
        ]
    },
    {
        "header": "Appendix IGrad Norm",
        "images": []
    }
]