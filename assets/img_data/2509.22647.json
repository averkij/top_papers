[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22647/x1.png",
                "caption": "Figure 1:(a) Existing Reward Models:Current LVLM-as-a-judge/reward models suffer from limitations like rewarding verbosity or brevity, leading to low-quality captions and reward hacking.(b) Our CapRL:CapRL uses a decoupled two-stage VQA approach to provide subjective rewards for captions.(c) CapRL’s Advantage:CapRL outperforms previous subjective reward methods, as shown by training curves and higher performance in the Prism(Qiao et al.,2024)evaluation setting.",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x2.png",
                "caption": "Figure 2:Illustration of the captioning capability improvement CapRL brings to Qwen2.5-VL-3B.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22647/x3.png",
                "caption": "Figure 3:Overview of CapRL.Unlike the standard single-stage RLVR, our CapRL performs a decoupled two-stage process.\nCaptions generated by the LVLM, paired with curated MCQs(b), are used to query an LLM, whose resulting accuracy becomes the objective reward for the LVLM(a).\nOur CapRL offers a scalable framework for applying RLVR to the open-ended image captioning task.",
                "position": 183
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22647/x4.png",
                "caption": "Figure 4:The scaling performance comparison between CapRL-1M and DenseFusion-1M. We use different amounts of pretraining data from the two datasets to observe the scaling trend.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x5.png",
                "caption": "Figure 5:(Left)CapRL demonstrates strong generalization even when trained on images from a single domain. CapRL-DocChart-20k refers to training conducted solely on document or chart images, while CapRL-Natural-20k is trained exclusively on natural images. Both models achieve significant improvements over the baseline on out-of-domain benchmarks, highlighting strong generalization capability.(Right)CapRL demonstrates promising scaling performance on QA training datasets.",
                "position": 1059
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x5.png",
                "caption": "",
                "position": 1062
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x6.png",
                "caption": "",
                "position": 1066
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACapRL Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22647/x7.png",
                "caption": "Figure 6:An illustrative example of CapRL-3B applied to infographic understanding.",
                "position": 2133
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x8.png",
                "caption": "Figure 7:An illustrative example of Qwen2.5-VL-3B applied to infographic understanding.",
                "position": 2137
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x9.png",
                "caption": "Figure 8:Chart understanding comparison between CapRL-3B and Qwen2.5-VL-3B.",
                "position": 2141
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x10.png",
                "caption": "Figure 9:Example of CapRL-3B chart understanding.",
                "position": 2145
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x11.png",
                "caption": "Figure 10:Example of Qwen2.5VL-as-Judge-3B chart understanding.",
                "position": 2149
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x12.png",
                "caption": "Figure 11:Example of Qwen2.5VL-as-Judge-3B chart understanding.",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x13.png",
                "caption": "Figure 12:Example of CapRL enhancing the captioning ability of Qwen2.5-VL-3B.",
                "position": 2157
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x14.png",
                "caption": "Figure 13:An illustrative example of CapRL applied to infographic understanding.",
                "position": 2161
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x15.png",
                "caption": "Figure 14:An illustrative example of CapRL applied to natural image understanding.",
                "position": 2165
            }
        ]
    },
    {
        "header": "Appendix BMore analysis experiments about CapRL",
        "images": []
    },
    {
        "header": "Appendix CPrompt Used",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22647/x16.png",
                "caption": "Figure 15:Prompt for LLM to answer questions based on Caption.",
                "position": 2239
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x17.png",
                "caption": "Figure 16:Prompt for Unified Reward Model as a Judge.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x18.png",
                "caption": "Figure 17:Prompt for Qwen2.5-VL-3B as a Judge.",
                "position": 2247
            },
            {
                "img": "https://arxiv.org/html/2509.22647/x19.png",
                "caption": "Figure 18:Prompt Qwen2.5-VL-72B to generate QA",
                "position": 2251
            }
        ]
    },
    {
        "header": "Appendix DData processing",
        "images": []
    },
    {
        "header": "Appendix EQA Processing",
        "images": []
    },
    {
        "header": "Appendix FPretraining Details",
        "images": []
    },
    {
        "header": "Appendix GCurrent Landscape and Future Directions of Multimodal Models",
        "images": []
    }
]