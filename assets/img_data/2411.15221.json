[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.15221/x1.png",
                "caption": "Figure 1:LLM Hackathon for Applications in Materials and Chemistry hybrid hackathon. Researchers were able to participate from both remote and in-person locations (purple pins).",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/learning_lobsters.png",
                "caption": "Figure 2:Schematic depicting the prompt for fine-tuning the LLM with Alpaca prompt format.",
                "position": 2326
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture11.png",
                "caption": "Figure 3:Model architecture and the schema of the second experiment. Material composition is encoded with Roost encoder, additional information extracted from cited paper with Llama3, and encoded with Mat(Sci)Bert. Composition and LLM embeddings are aggregated and passed through the Residual Net projection head to predict the property. At the inference stage, the average LLM embedding from 5 nearest neighbors in composition space is taken. Results show MAE for adding different types of context (top to bottom): adding random context; not adding context; adding consistently structured context for chemical and structural family (data extracted by humans); adding automatically extracted context for experimental conditions and structure-property relationship.",
                "position": 2439
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureMF1.png",
                "caption": "Figure 4:",
                "position": 2541
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureMF2.png",
                "caption": "Figure 5:",
                "position": 2553
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureMF3.png",
                "caption": "Figure 6:Model comparison of the pre-trained and fine-tuned T5-Chem on zero-point vibrational energy (ZPVE).",
                "position": 2564
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureMF4.png",
                "caption": "Figure 7:Model comparison of the pre-trained and fine-tuned ChemBERTa on zero-point vibrational energy (ZPVE).",
                "position": 2568
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureGeomG1.png",
                "caption": "Figure 8:Schematic representation of the training process for a regression model illustrating the novel string-based encoding of 3D molecular geometry for LLM-based energy prediction. The workﬂow involves (1) Computation of high-dimensional feature vectors representing the 3D molecular geometry of each conformer. (2) Dimensionality reduction to obtain a more compact representation. (3) Conversion of the reduced vectors into a histogram, where unique string characters are assigned to each bin. (4) Input of the resulting set of strings (one per conformer) into an LLM for energy prediction.",
                "position": 2615
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureGeomG2a.jpg",
                "caption": "Figure 9:Performance of an LLM in predicting total energies of benzene and ethanol structures, where the model was trained on a large dataset of MD-generated conﬁgurations. Each point represents a different molecular structure sampled from MD simulations[7].",
                "position": 2622
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureGeomG2b.jpg",
                "caption": "",
                "position": 2625
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture5.png",
                "caption": "Figure 10:Scheme of the system. Data is first converted into text format, with peak positions and intensities represented as (x,y) pairs. These are passed into an LLM prompt, which is tasked to use a scratchpad as it reasons about the data and the formula, before providing a final answer.",
                "position": 2739
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture2.png",
                "caption": "Figure 11:MC-Peptide: Pipeline implemented in this work. The example illustrates a user request, followed by retrieval from the Semantic Scholar API, and the creation of a knowledge base. MCPs and permeabilities are extracted from[7]. The pipeline finishes with an LLM proposing modifications to an MCP, increasing its permeability.",
                "position": 2860
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture6.png",
                "caption": "Figure 12:a) Workflow overview. The ReAct agent looks up guidelines for designing low band gap MOFs from research papers and suggests a new MOF (likely with lower band gap). It then checks validity of the new SMILES candidate and predicts band gap with uncertainty estimation using an ensemble of surrogate fine-tuned MOFormers. b) Band gap predictions for new MOF candidates as a function of agent iterations. Detailed analysis of this methodology applied to other materials and target properties can be found in reference[3].",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureLlama3MatSci.png",
                "caption": "Figure 13:LLM-based material design workflow (left) and diagram showing evaluation metric (right).",
                "position": 3210
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureLangSim.png",
                "caption": "Figure 14:",
                "position": 3281
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureLLMicroscopilot.png",
                "caption": "Figure 15:Schematic overview of the LLMicroscopilot assistant. The microscope user interface allows the user to input queries, which are then processed by the LLM. The LLM executes appropriate tools to provide domain-specific knowledge, support data analysis, or operate the microscope.",
                "position": 3496
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture16.png",
                "caption": "Figure 16:Retrieval Augmented Generation [RAG] architecture with LLM interface",
                "position": 3577
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/mat_agent1.jpg",
                "caption": "Figure 17:Workflow of a response generated to a user prompt by Materials Agent using a tool based on RDKit.",
                "position": 3677
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/mat_agent2.jpg",
                "caption": "Figure 18:Niche purpose tools with LLM. (a) Illustration of the RDF computing tool. (b) Code snippet to highlight the ease of transferability of LLMs with tool-calling capabilities.",
                "position": 3691
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/mat_agent3.jpg",
                "caption": "Figure 19:Illustration of RAG for interacting with a MSDS.",
                "position": 3702
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture8.png",
                "caption": "Figure 20:Workflow for integrating chemical encoders with large language models. Molecular data from SMILES is transformed into molecular tokens and combined with text embeddings for tasks such as property prediction, molecule editing, and generation.",
                "position": 3808
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture9.png",
                "caption": "Figure 21:Modified code of the function get_embeddings.",
                "position": 3881
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture10.png",
                "caption": "Figure 22:Modified forward function which allows for the molecular token to be added.",
                "position": 3884
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureLLMMyWay.png",
                "caption": "Figure 23:",
                "position": 4349
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureWaterLLM1.png",
                "caption": "Figure 24:WaterLLM approach: custom chatGPT with RAG from scientific papers, context and chain-of-thought allowed for interactive dialog with the user anchored to science.",
                "position": 4475
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureWaterLLM2.png",
                "caption": "Figure 25:Sample of the WaterLLM communication with the User.",
                "position": 4479
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture4.png",
                "caption": "Figure 26:The yeLLowhaMmer multimodal agent can be used for a variety of data management tasks. Here, it is shown automatically adding an entry into thedatalablab data management system based on an image of a handwritten lab notebook page.",
                "position": 4523
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture20.png",
                "caption": "Figure 27:Flowchart of the Query Reporter usage, including the back-end interaction with external resources, i.e., NOMAD and Llama. Intermediate steps managing hallucinations or token limits are marked in red and orange, respectively.",
                "position": 4659
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture17.png",
                "caption": "Figure 28:a) Part of a JSON Schema defining a data structure for a solution preparation. b) The schema converted to an ELN form in NOMAD[3].",
                "position": 4868
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture14.png",
                "caption": "Figure 29:Likelihood of accepting the hypothesis “LK-99 is a room-temperature superconductor” via three approaches, from April 15, 2023 to April 15, 2024.\nThe unadjusted initial probability (set to 50%) is shown in gray, the adjusted initial probability (set to 20%) is shown in black, and the probability according to the Manifold Markets online betting platform is shown in red.",
                "position": 5028
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture15.png",
                "caption": "Figure 30:Multi-Agent Hypothesis Generation and Verification Pipeline",
                "position": 5125
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureActiveScience1.png",
                "caption": "Figure 31:A Schematic illustration of ActiveScience architecture and its potential applications. Code snippet demonstrating the use of LangChain.",
                "position": 5420
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture3.png",
                "caption": "Figure 32:Performance of Gemini Pro, GPT-4 Turbo, and Claude3 Opus on text, visual, and text+visual representations. The plot shows that models achieve higher accuracy with combined text and visual inputs compared to visual-only inputs.",
                "position": 5634
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture12.png",
                "caption": "Figure 33:Summary of our LLM hackathon project.",
                "position": 5762
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture18.jpg",
                "caption": "Figure 34:KnowMat Workflow. The graphical abstract illustrates the KnowMat workflow, which begins with parsing input files in XML or PDF formats. The Large Language Model (LLM) powered by engineered prompts processes the reference text to extract key information, such as material composition, properties, and measurement conditions, and converts it into a structured JSON output.",
                "position": 5905
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture19.png",
                "caption": "Figure 35:",
                "position": 6056
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PicturePolymerRAG.png",
                "caption": "Figure 36:Creating Knowledge Graph Retrieval-Augmented Generation (KGRAG) for Polymer Simulation.",
                "position": 6112
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/Picture13.png",
                "caption": "Figure 37:Insightful machine learning for HEA hydrides",
                "position": 6220
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureChemSense.png",
                "caption": "Figure 38:Comparison of the alignment of the different LLMs with the SMILES (left) and IUPAC (right) molecular representations. For both representations, note that the random baseline is at a fixed value of 0.5 since all the questions were binary and the datasets are balanced.",
                "position": 6302
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/glossagen.jpeg",
                "caption": "Figure 39:Overview of (left) the graphical user interface (GUI) protoype and (right) the generated Neo4J knowledge graph (right).\nOur results demonstrate that LLMs can greatly accelerate glossary creation, increasing the likelihood that authors will include a helpful glossary without the need for tedious manual effort. Additionally, an analysis of our test case by a zeolite domain expert revealed that LLMs produce good results, with about 70% - 80% of explanations requiring little to no manual changes.",
                "position": 6398
            },
            {
                "img": "https://arxiv.org/html/2411.15221/extracted/6013725/images/PictureGGen2.png",
                "caption": "Figure 40:Overview of the GlossaryGenerator class, responsible for processing text chunks and extracting relevant, correctly formatted terms and definitions.",
                "position": 6412
            }
        ]
    },
    {
        "header": "Appendix: Individual Project Reports",
        "images": []
    }
]