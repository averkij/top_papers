[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08519/x1.png",
                "caption": "Figure 1:We proposeHuMo, a multimodal HCVG framework that supports flexible input compositions of text-image (first row), text-audio (second row), and text-image-audio (third row). HuMo generalizes to humans, humans with objects or animals, stylized humanoid artworks, and animations.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2509.08519/x2.png",
                "caption": "Figure 2:Prior HCVG methods comparison. Reference images as inputs for Phantom[22], HunyuanCustom[11], and HuMo. OmniHuman-1[19]takes the start frame as input, which is synthesized by an image generation model[8,29]. OmniHuman-1 suffers from weak text adherence, unable to generate subjects (e.g., a toy) absent in the start frame, while the subject preservation is precariously dependent on the preceding image generator. Phantom lacks audio-driven articulation to synchronize mouth movements with the spoken words in the input audio. HunyuanCustom delivers unbalanced performance on both fronts. HuMo excels in collaborative performance across video quality, subject consistency, audio-visual sync, and text controllability.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08519/x3.png",
                "caption": "Figure 3:Overview of our framework.HuMo model (left) is trained based on the proposed data processing pipeline (right). Built upon a DiT-based T2V backbone from Stage 0, the model progressively learns subject preservation and audio-visual sync capabilities in Stages 1 and 2. HuMo achieves collaborative generation across different modality compositions.",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2509.08519/x4.png",
                "caption": "Figure 4:The proposedtime-adaptive CFGbalances text guidance and identity preservation.",
                "position": 292
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08519/x5.png",
                "caption": "Figure 5:Qualitative comparison for the subject preservation task.Zoom in for details.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2509.08519/x6.png",
                "caption": "Figure 6:Qualitative comparison for audio-visual sync task.For the I2V-based methods, we use the first frame generated by MoCha[35]as the input start frame. For HunyuanCustom and Ours, the cropped face from the start frame is utilized as input. The official OmniHuman-1[19]website API does not support text input. Zoom in for details.",
                "position": 466
            },
            {
                "img": "https://arxiv.org/html/2509.08519/x7.png",
                "caption": "Figure 7:Qualitative ablation study.Zoom in for details.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2509.08519/x8.png",
                "caption": "Figure 8:Given the same reference subject reference image but different text prompts, our method achievescollaborative text-image controllability.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2509.08519/x9.png",
                "caption": "Figure 9:Re-creation of Game of Thrones (Season 3), named as Faceless Thrones.We extract captions[1]and audio from the original video and generate new videos with HuMo using two modes:text–audio (TA)andtext–image–audio (TIA). The reference identity image for TIA mode is displayed at the top-left corner.",
                "position": 692
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Ethical Considerations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]