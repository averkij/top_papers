[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08143/x1.png",
                "caption": "Figure 1:UniMMVSR is a unified framework that supports video super-resolution with multi-modal input conditions.By cooperating with the low-resolution multi-modal generative model, the proposed cascaded framework can effectively extend the controllable video generation to ultra-high-resolution (e.g., 4K) with high visual quality and subject consistency.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08143/x2.png",
                "caption": "Figure 2:Overview of UniMMVSR in the context of a cascaded generation framework. Upsampler denotes the sequential operations of VAE decoding, upscaling via bilinear interpolation, and VAE encoding.TCandCCdenote token concatenation and channel concatenation respectively. Texts are encoded by text encoder and then injected via cross-attention layers, which are omit for simplicity.",
                "position": 234
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08143/x3.png",
                "caption": "Figure 3:Qualitative comparisons ontext-to-video generation,text-guided video editingandmulti-ID image-guided text-to-video generationtasks from top to bottom.(Zoom-in for best view)",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x4.png",
                "caption": "Figure 4:Visual Comparisons of single-task and unified model.Zoom-in for best view.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x4.png",
                "caption": "Figure 4:Visual Comparisons of single-task and unified model.Zoom-in for best view.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x5.png",
                "caption": "Figure 5:Qualitative results of 4K multi-ID image-guided text-to-video generation.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x6.png",
                "caption": "Figure 6:Visual Comparisons of reference augmentation.Zoom-in for best view.",
                "position": 765
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08143/x7.png",
                "caption": "Figure 7:An overview of the architecture of our base model.",
                "position": 2193
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x8.png",
                "caption": "Figure 8:Qualitative comparisons of different inference settings. The text prompt is omitted.",
                "position": 2265
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x9.png",
                "caption": "Figure 9:Degradation pipeline for UniMMVSR.",
                "position": 2272
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x10.png",
                "caption": "Figure 10:Samples of light sdedit degradation.",
                "position": 2278
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x11.png",
                "caption": "Figure 11:Samples of heavy sdedit degradation.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x12.png",
                "caption": "Figure 12:Training loss curve of all three tasks.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x13.png",
                "caption": "Figure 13:Qualitative comparisons on text-to-video generation task.",
                "position": 2908
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x14.png",
                "caption": "Figure 14:Qualitative comparisons on multi-ID image-guided text-to-video generation task.",
                "position": 2911
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x15.png",
                "caption": "Figure 15:Qualitative comparisons on text-guided video editing task.",
                "position": 2914
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x16.png",
                "caption": "Figure 16:Qualitative comparisons with different components.",
                "position": 2924
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x17.png",
                "caption": "Figure 17:Additional 4K results on text-to-video generation task.Zoom-in for best view.",
                "position": 2934
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x18.png",
                "caption": "Figure 18:Additional 4K results on multi-ID image-guided text-to-video generation task.Zoom-in for best view.",
                "position": 2937
            },
            {
                "img": "https://arxiv.org/html/2510.08143/x19.png",
                "caption": "Figure 19:Additional 4K results on text-guided video editing task.Zoom-in for best view.",
                "position": 2940
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]