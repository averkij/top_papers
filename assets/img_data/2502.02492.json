[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02492/x1.png",
                "caption": "",
                "position": 96
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02492/x2.png",
                "caption": "Figure 2:Motion incoherence in video generation.Examples of incoherent generations by DiT-30B(Peebles & Xie,2023). The model struggles with (a) basic motion, e.g., jogging (stepping on the same leg repeatedly); (b) complex motion e.g., gymnastics; (c) physics, e.g., object dynamics (the hoop passes through the woman); and (d) rotational motion, failing to replicate simple repetitive patterns.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02492/x3.png",
                "caption": "Figure 3:Motivation Experiment.We compare the model‚Äôs loss before and after randomly permuting the video frames, using a ‚Äúvanilla‚Äù DiT (orange) and our fine-tuned model (blue). The original model isnearly invariantto temporal perturbations fort‚â§60ùë°60t\\leq 60italic_t ‚â§ 60.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2502.02492/x4.png",
                "caption": "Figure 4:VideoJAM Framework.VideoJAM is constructed of two units; (a)Training.Given an input videox1subscriptùë•1x_{1}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand its motion representationd1subscriptùëë1d_{1}italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, both signals are noised and embedded to asingle, jointlatent representation using a linear layer,Wi‚Å¢n+subscriptsuperscriptWùëñùëõ\\textbf{W}^{+}_{in}W start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT. The diffusion model processes the input, and two linear projection layers predict both appearance and motion from the joint representation. (b)Inference.We proposeInner-Guidance, where the model‚Äôs own noisy motion prediction is used to guide the video prediction at each step.",
                "position": 173
            }
        ]
    },
    {
        "header": "4VideoJAM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02492/x5.png",
                "caption": "Figure 5:Text-to-video results by VideoJAM-30B.VideoJAM enables the generation of a wide variety of motion types, from basic motion (e.g., running) to complex motion (e.g., acrobatics), and improved physics (e.g., jumping over a hurdle).",
                "position": 405
            },
            {
                "img": "https://arxiv.org/html/2502.02492/x6.png",
                "caption": "Figure 6:Qualitative comparisonsbetween VideoJAM-30B and the leading baselines- Sora, Kling, and DiT-30B on representative prompts from VideoJAM-bench. The baselines struggle with basic motion, displaying ‚Äúbackward motion‚Äù (Sora, 2nd row) or unnatural motion (Kling, 2nd row). The generated content defies the basic laws of physics e.g., people passing through objects (DiT, 1st row), or objects that appear or evaporate (Sora, DiT, 4th row). For complex motion, the baselines display static motion or deformations (Sora, Kling, 1st, 3rd row). Conversely, in all cases, VideoJAM produces temporally coherent videos that better adhere to the laws of physics.",
                "position": 408
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02492/x7.png",
                "caption": "Figure 7:Limitations.Our method is less effective for: (a) motion observed in ‚Äúzoom-out‚Äù (the moving object covers a small part of the frame). (b) Complex physics of object interactions.",
                "position": 676
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Impact Statements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACompositional Guidance vs. Inner-Guidance",
        "images": []
    },
    {
        "header": "Appendix BMotivation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02492/x8.png",
                "caption": "Figure 8:Qualitative motivation.We noise input videos to different timesteps (20,60,8020608020,60,8020 , 60 , 80) and continue the generation. By step60606060, the video‚Äôs coarse motion and structure are mostly determined.",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2502.02492/x9.png",
                "caption": "Figure 9:Additional text-to-video results using VideoJAM-30B.",
                "position": 1491
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DVideoJAM-bench: Automatic Metrics Breakdown and Prompts",
        "images": []
    },
    {
        "header": "Appendix EMovie Gen Benchmark",
        "images": []
    },
    {
        "header": "Appendix FVideoJAM-bench Prompts",
        "images": []
    }
]