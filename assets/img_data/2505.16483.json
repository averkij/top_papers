[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16483/x1.png",
                "caption": "Figure 1:Average score on 11 downstream tasks vs model size. With only 7B parameters,Canoealready exceeds state-of-the-art LLMs like GPT-4o and o1.",
                "position": 288
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16483/x2.png",
                "caption": "Figure 2:An overview ofCanoeframework.Canoefirst synthesizes easily verifiable short-form QA data and then proposes the Dual-GRPO with designed rule-based rewards to improve the faithfulness of LLMs.",
                "position": 351
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16483/x3.png",
                "caption": "Figure 3:Model performance comparison on FaithEval in a closed-book QA setting and counterfactual context setting.\nOur models are colored in orange.\nWe report the results from the chat version of LLaMA-3 and Qwen-2.5.",
                "position": 1263
            },
            {
                "img": "https://arxiv.org/html/2505.16483/x4.png",
                "caption": "Figure 4:The average perplexity score of 110 negative samples for each model from eleven datasets.",
                "position": 1298
            },
            {
                "img": "https://arxiv.org/html/2505.16483/x5.png",
                "caption": "Table 5:Case study from long-form QA task CLAPNQ. For different useful statements, we use different colors.",
                "position": 1384
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ATraining Data Details",
        "images": []
    },
    {
        "header": "Appendix BDual-GRPO Details",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16483/x5.png",
                "caption": "Figure 5:Human evaluation across four key dimensions.",
                "position": 2830
            }
        ]
    },
    {
        "header": "Appendix DImplementations Details",
        "images": []
    },
    {
        "header": "Appendix EHuman Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16483/x6.png",
                "caption": "Figure 6:The Avg EM results (%) on 11 datasets with different numbers of synthesized short-form training data. We conduct the experiments based on LLaMA-3-Instruct-8B models.",
                "position": 3386
            }
        ]
    },
    {
        "header": "Appendix FDiscussion",
        "images": []
    }
]