[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x1.png",
                "caption": "",
                "position": 110
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x2.png",
                "caption": "Figure 2:The SpeakerVid-5M curation pipeline.The process consists: (1) Source data collection from YouTube; (2) Multi-step audio-visual pre-processing; (3) Rich multi-modal annotation using models like Qwen-VL; (4) Rigorous quality filtering stage for data fidelity.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Dataset Statistics and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x3.png",
                "caption": "(a)Face and Hand Blur Score",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x3.png",
                "caption": "(a)Face and Hand Blur Score",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x4.png",
                "caption": "(b)Duration and Resolution",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x5.png",
                "caption": "(c)Topic and Year Distribution",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x6.png",
                "caption": "(d)Caption Word Cloud",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x7.png",
                "caption": "(e)Sync Conf Score Distribution",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x8.png",
                "caption": "(f)Caption Words Distribution",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x9.png",
                "caption": "(g)DOVER Quality Score Distribution",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x10.png",
                "caption": "Figure 4:Examples of dyadic dialogue and body composition in SpeakerVid-5M.The top rows illustrate a typical dyadic human generation sample (initiator and responder). The bottom rows demonstrate the variety of body compositions annotated in our dataset, including close-up headshots, half-body, and full-body views, which are critical for controllable generation.",
                "position": 570
            }
        ]
    },
    {
        "header": "5Autoregressive Talking Human Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x11.png",
                "caption": "Figure 5:Our autoregressive audio-visual generation method.",
                "position": 673
            }
        ]
    },
    {
        "header": "6Experimental Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x12.png",
                "caption": "Figure 6:Qualitative results of our dyadic generation model.From left to right, the input video of the initiator, the reference image providing the target identity, and the modeâ€™s generated audio-visual response.",
                "position": 693
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Limitation and Future Work",
        "images": []
    },
    {
        "header": "9Implementation Details",
        "images": []
    },
    {
        "header": "10Annotation File Usage",
        "images": []
    },
    {
        "header": "11Visualization of Pretrain and Finetune Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x13.png",
                "caption": "Figure 7:Impact of finetuning on generation quality.A comparison between the model after pretraining phase and after finetuning on our high-quality subset.",
                "position": 1075
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x14.png",
                "caption": "Figure 8:Visualization of multi-turn dialogue and listening scenarios.Top (Multi-turn Dialogue): We showcase a sequence of conversational turns between an initiator and a responder. By preserving temporal context, our dataset facilitates the training of models capable of coherent, long-form conversations. Bottom (Listening Case): A speaker is paired with a non-speaking listener.",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2507.09862/x15.png",
                "caption": "Figure 9:Diverse examples of dyadic pairs from SpeakerVid-5M.This figure highlights the diversity of subjects, environments, and interaction styles captured in our dataset.",
                "position": 1084
            }
        ]
    },
    {
        "header": "12Prompt Used in Annotation",
        "images": []
    },
    {
        "header": "13Failed Case and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.09862/x16.png",
                "caption": "Figure 10:Analysis of failure cases.These include generating facial artifacts or unnatural distortions and struggling with severe motion blur during rapid head or hand movements.",
                "position": 1384
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]