[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12375/x1.png",
                "caption": "",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video Depth Anything",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12375/x2.png",
                "caption": "Figure 2:Overall pipeline and the spatio-temporal head. Left: Our model is composed of a backbone encoder from Depth Anything V2 and a newly proposed spatio-temporal head. We jointly train our model on video data using ground-truth depth labels for supervision and on unlabeled images with pseudo labels generated by a teacher model. During training, only the head is learned. Right: Our spatiotemporal head inserts several temporal layers into the DPT head, while preserving the original structure of DPT head[28].",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2501.12375/x3.png",
                "caption": "Figure 3:Inference strategy for long videos.N𝑁Nitalic_Nis the video clip lenght consumed by our model. Each inference video clip is built byN−To−Tk𝑁subscript𝑇𝑜subscript𝑇𝑘N-T_{o}-T_{k}italic_N - italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTfuture frames,Tosubscript𝑇𝑜T_{o}italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPToverlapping/adjacent frames, andTksubscript𝑇𝑘T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTkey frames. The key frames are selected by taking everyΔksubscriptΔ𝑘\\Delta_{k}roman_Δ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT-th frame going backward. Then, the new depth predictions will be scale-shift-aligned to the previous frames based on theTksubscript𝑇𝑘T_{k}italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPToverlapping frames. We useN=32,To=8,Tk=2,Δk=12formulae-sequence𝑁32formulae-sequencesubscript𝑇𝑜8formulae-sequencesubscript𝑇𝑘2subscriptΔ𝑘12N=32,T_{o}=8,T_{k}=2,\\Delta_{k}=12italic_N = 32 , italic_T start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = 8 , italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 2 , roman_Δ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 12.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12375/extracted/6138791/figures/imgs/performance_comparison.png",
                "caption": "Figure 4:Video depth estimation accuracy for different frame length.We compare our model (VDA-L) with DepthCrafter[13]and DepthAnyVideo[40]from 110 to 500 frames on Bonn[24], Scannet[7], and NYUv2[22].",
                "position": 567
            },
            {
                "img": "https://arxiv.org/html/2501.12375/x4.png",
                "caption": "Figure 5:Qualitative comparison for real-world long video depth estimation.We compare our model with DAv2-L[42]and DepthCrafter[13]on 500-frame videos from Scannet[7]and Bonn[24].",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2501.12375/x5.png",
                "caption": "Figure 6:Qualitative comparison for in-the-wild short video depth estimation.We compare with Depth-Anything-V2[42], DepthCrafter[13]and DepthAnyVideo[40]on videos with less than 100 frames from DAVIS[26]. Red boxes show incorrect depth estimation while blue boxes show inconsistent depth estimation.",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2501.12375/x6.png",
                "caption": "Figure 7:Qualitative comparisons of different inference strategies.We compare overlap alignment (OA) with our proposed overlap interpolation and key-frame referencing (OI + KR) on a self-captured video with 7320 frames.",
                "position": 762
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1More Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12375/x7.png",
                "caption": "Figure 8:Qualitative comparison for static image depth estimation.We compare our model with Depth-Anything-V2[42], DepthCrafter[13], and Depth Any Video[40]on static image depth estimation. Our model demonstrates visualization results comparable to those of Depth-Anything-V2[42].",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2501.12375/x8.png",
                "caption": "Figure 9:Qualitative comparison for real-world long video depth estimation.We compare with Depth-Anything-V2[42]and DepthCrafter[13]on 500-frames videos from Scannet[7]and Bonn[24]. We show changes in color and depth over time at the vertical red line in videos. White boxes show inconsistent estimation. Blue boxes show our algorithm has higher accuracy.",
                "position": 1473
            }
        ]
    },
    {
        "header": "2Short video depth quantitative results",
        "images": []
    },
    {
        "header": "3Limitations and future work",
        "images": []
    },
    {
        "header": "4More Details of Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12375/x9.png",
                "caption": "Figure 10:Temporal layer.The feature shape is adjusted for temporal attention.",
                "position": 1574
            }
        ]
    },
    {
        "header": "5More Details of Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12375/x10.png",
                "caption": "Figure 11:3D Video Conversion.A video from the DAVIS dataset[26]is transformed into a 3D video using our model.",
                "position": 1651
            },
            {
                "img": "https://arxiv.org/html/2501.12375/x11.png",
                "caption": "Figure 12:Dense point cloud generation.We compare our model with DepthCrafter[13]and DepthAnyVideo[40]for dense point cloud generation on the KITTI dataset[11]. Our model generates a clean and regular point cloud from multiple frames spanning approximately 5 seconds. In contrast, the point cloud generated by DepthCrafter[13]contains several obvious discontinuous layers. DepthAnyVideo[40]produces a point cloud with numerous noisy outliers and noticeable distortion in distant views.",
                "position": 1654
            }
        ]
    },
    {
        "header": "6Applications",
        "images": []
    }
]