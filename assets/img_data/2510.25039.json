[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25039/x1.png",
                "caption": "Figure 1:BeTaLautomates the process of designing and adjustingdynamic benchmarksto meet target criteria.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/performance_gap_by_difficulty_combined_gpt.png",
                "caption": "Figure 2:Evaluation results on o4-mini withBeTaL(with GPT-5 as the designer model, and o4-mini as the target model during parameter search) perform robustly at different target difficulty levels, compared to baselines on Arithmetic Sequences, Spatial Reasoning, andτ\\tau-Bench. A similar performance is noted using Claude Opus 4.1 and Grok-4 as Designers, in Figure8in the Appendix.",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/combined_domains_convergence.png",
                "caption": "Figure 3:Convergence of iterative methods during parameter selection on Spatial Reasoning andτ\\tau-Bench benchmarks:BeTaLvs. RS+PPR. Performance gap ofBeTaLshrinks faster compared to RS+PPR, within 10 iterations, indicating LLMs are more efficient than competing iterative methods at finding favorable environment parameters for benchmark creation. Results are averaged over difficulty levels and designer models.",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/evaluation_analysis_grid_all_datasets.png",
                "caption": "Figure 4:Evaluation generalization across designer models and datasets. Observed versus target accuracy for o4-mini target trained by different designers (columns: GPT-5, Grok-4, Opus-4.1) on three benchmarks (rows: Arithmetic Sequence, Spatial Reasoning,τ\\tau-Bench). The black dashed line indicates perfect alignment.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/tau_bench/eval_performance_by_approach.png",
                "caption": "(a)Results averaged over the difficulty levels.",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/tau_bench/eval_performance_by_approach.png",
                "caption": "(a)Results averaged over the difficulty levels.",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/tau_bench/eval_betal_accuracies_by_difficulty.png",
                "caption": "(b)Results forBeTaLat different target difficulty levels.",
                "position": 1047
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/betal_param_space_comparison-2.png",
                "caption": "Figure 6:Performance ofBeTaLonτ\\tau-bench parameter space generated by Opus 4.1 versus by human.BeTaLon AI-generated parameter space is an acceptably small performance gap for medium and hard benchmarks, yet still generally underperforms to that generated by humans.",
                "position": 1090
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiments and Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25039/x2.png",
                "caption": "Figure 7:Illustration of particles and actions in spatial reasoning tasks. Here the board is 4x4 and initially oriented towards north (black arrow). There are two particlesP​1P1andP​2P2oriented towards west and south respectively. The first action moved the particleP​1P1forward by one step, second action rotated the particleP​2P2by 90 degrees and the last action shows rotation of the board by 90 degrees. The board rotations are w.r.t. to its center and when a board rotates or moves the particles on it also rotate and move along with it.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/performance_gap_by_difficulty_opus_grok_combined.png",
                "caption": "Figure 8:Evaluation results on o4-mini withBeTaL(with Claude Opus 4.1 or Grok-4 as the designer model, and o4-mini as the target model during parameter search) perform robustly at different target difficulty levels, compared to baselines on Arithmetic Sequences, Spatial Reasoning, andτ\\tau-Bench.",
                "position": 1763
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/ced_teacher_performance_gap_combined.png",
                "caption": "Figure 9:BeTaLperformance by the designer model during parameter search across three benchmark domains. Each panel represents one dataset (Arithmetic Sequence, Spatial Reasoning,τ\\tau-Bench) and compares three designer models (GPT-5, Grok-4, Opus-4.1) across four target performance levels: Hard (ρhard=0.25\\rho^{\\mathrm{hard}}=0.25), Medium (ρmedium=0.50\\rho^{\\mathrm{medium}}=0.50), Easy (ρeasy=0.75\\rho^{\\mathrm{easy}}=0.75), and Trivial (ρtrivial=0.90\\rho^{\\mathrm{trivial}}=0.90), shown as grouped bars. Bars show mean performance gap (difference between target and observed target performance), with o4-mini as target model, averaged over training iterations. Error bars show standard error.",
                "position": 1769
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/ced_vs_drplr_convergence_combined_averaged.png",
                "caption": "Figure 10:BeTaLvs. RS+PPR convergence during parameter search across datasets and target difficulty levels. Each panel shows the mean performance gap (difference between target and observed target performance) over training iterations for two design approaches:BeTaL(our method) and RS+PPR (baseline). Rows indicate target performance (Hard (ρhard=0.25\\rho^{\\mathrm{hard}}=0.25), Medium (ρmedium=0.50\\rho^{\\mathrm{medium}}=0.50), Easy (ρeasy=0.75\\rho^{\\mathrm{easy}}=0.75), and Trivial (ρtrivial=0.90\\rho^{\\mathrm{trivial}}=0.90)). Columns show three benchmark domains (Arithmetic Sequence, Spatial Reasoning,τ\\tau-Bench).BeTaLresults are averaged across designer models (GPT-5, Grok-4, Opus-4.1). All results use o4-mini as the target model, with shaded regions showing standard error across seeds.",
                "position": 1776
            },
            {
                "img": "https://arxiv.org/html/2510.25039/images/eval/comprehensive_teacher_performance_combined.png",
                "caption": "Figure 11:Designer model performance during parameter search across datasets and\ntarget difficulty levels. Each panel shows the mean performance gap (difference between\ntarget and observed target performance) for different design approaches:BeTaL, BoN-ML, and BoN-TM. Rows indicate target performance (Hard (ρhard=0.25\\rho^{\\mathrm{hard}}=0.25), Medium (ρmedium=0.50\\rho^{\\mathrm{medium}}=0.50), Easy (ρeasy=0.75\\rho^{\\mathrm{easy}}=0.75), and Trivial (ρtrivial=0.90\\rho^{\\mathrm{trivial}}=0.90)). Columns show three\nbenchmark domains (Arithmetic Sequence, Spatial Reasoning,τ\\tau-Bench). All results\nuse o4-mini as the target model, averaged over each iteration.",
                "position": 1789
            },
            {
                "img": "https://arxiv.org/html/2510.25039/x3.png",
                "caption": "Figure 12:Parameter evolution over iterations in the hard difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok-4, Opus-4-1). Row 1 showsboard_size(width),wrap_aroundandnumber_of_board_rotation. Row 2 showsnumber_of_board_movements,number_of_particle_rotationsandnumber_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of setsallowed_board_movementsandallowed_board_rotationsand PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see to obtain a hard configuration, models generally prefer a larger board size and a higher number of capabilities and actions. Among the models, GPT-5 does it more aggressively and achieves the lowest performance gap as well.",
                "position": 1800
            },
            {
                "img": "https://arxiv.org/html/2510.25039/x4.png",
                "caption": "Figure 13:Parameter evolution over iterations in the medium difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok-4, Opus-4-1). Row 1 showsboard_size(width),wrap_aroundandnumber_of_board_rotation. Row 2 showsnumber_of_board_movements,number_of_particle_rotationsandnumber_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of setsallowed_board_movementsandallowed_board_rotationsand PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see that, to obtain a medium difficulty configuration, models prefer much smaller board sizes and number and types of actions as compared to the hard setting in Figure12. Also consistent with the expectations, the models reduce the number of board actions close to 0 but allow a decent number of particle actions.",
                "position": 1803
            },
            {
                "img": "https://arxiv.org/html/2510.25039/x5.png",
                "caption": "Figure 14:Parameter evolution over iterations in the medium difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok-4, Opus-4-1). Row 1 showsboard_size(width),wrap_aroundandnumber_of_board_rotation. Row 2 showsnumber_of_board_movements,number_of_particle_rotationsandnumber_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of setsallowed_board_movementsandallowed_board_rotationsand PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see, to obtain a medium difficulty configuration, models prefer much smaller board sizes and number and types of actions as compared to the hard setting in Figure12. Also consistent with the expectations the models reduce the number of board actions close to 0 but allow a decent number of particles actions.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2510.25039/x6.png",
                "caption": "Figure 15:Parameter evolution over iterations in the trivial difficulty setting. The subplots show average values of the different design parameters at each iteration chosen by the designer models (GPT-5, Grok-4, Opus-4-1). Row 1 showsboard_size(width),wrap_aroundandnumber_of_board_rotation. Row 2 showsnumber_of_board_movements,number_of_particle_rotationsandnumber_of_particle_movements. Next row presents the sum of these number of actions (total actions), absolute performance gap as observed on the o4-mini target model and the number of enabled capabilities (types of rotations and movements), here BM, BR are the sizes of setsallowed_board_movementsandallowed_board_rotationsand PM, PR similarly reflect the sizes of action sets corresponding to the particles. We can see that, to obtain an easy difficulty configuration, models prefer smaller board sizes and number and types of actions as compared to the medium and easy settings in Figures13and14. Also consistent with the expectations, the models reduce the number of board actions close to 0, but allow a few actions on particles.",
                "position": 1809
            }
        ]
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    },
    {
        "header": "Appendix CTeacher Model Reasoning Traces",
        "images": []
    }
]