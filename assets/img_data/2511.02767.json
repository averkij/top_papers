[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Our Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02767/x1.png",
                "caption": "Figure 1:Scaling both the number of video frames and text captions at test time improves alignment.Given a paired videoviv_{i}and set of captionscic_{i}, leveraging rich multi-frame and multi-caption information at test time leads to improved alignment, measured in terms of mutualkk-NN.",
                "position": 114
            }
        ]
    },
    {
        "header": "4Data and Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02767/x2.png",
                "caption": "Figure 2:Video representations are strongly aligned with text.We measure alignment between modern vision encoders and Gemma 2 9B-it (subset of all vision models shown for clarity) on the VATEX video dataset using a single caption for each video.\nIn addition to the points, we plot a linear regression of alignment scores to representation strength for three different LLMs.\nOur takeaways are threefold:\n(1) The strongest models for both alignment and retrieval are large video models ()\n(2) Averaging over frames is a simple yet effective baseline for image models on video input (),\nwhile image-only models are limited to scores below 22% (),\nin line withHuh et al. (2024).\n(3) Recent text models have improved alignment with vision models, shown by the regression lines.",
                "position": 170
            }
        ]
    },
    {
        "header": "5Video-text alignment results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02767/x3.png",
                "caption": "Figure 3:Vision-text alignment scales strongly with the amount of visual and textual data available at test time.(left) Providing moreframesincreases vision-text alignment for both image and video models, with the latter being able to take advantage of more frames more effectively.\n(right) Providing morecaptionsto the text model also significantly boosts alignment with vision models, across all frame counts.",
                "position": 212
            }
        ]
    },
    {
        "header": "6Video-Text Alignment and Data Dependence",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02767/x4.png",
                "caption": "Figure 4:Correlation between video/text alignment and downstream video perception task performance, for SSL methods trained without text supervision.",
                "position": 266
            }
        ]
    },
    {
        "header": "7Cross-modal alignment and downstream performance",
        "images": []
    },
    {
        "header": "8Temporal analysis & Cross-model alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02767/x5.png",
                "caption": "Figure 5:Video and text eventually are aligned, but encode temporal information differently.While the video text alignment is largely perfect for all of the models oncek=3k=3, they differ heavily before that when looking atk=1,2k=1,2.\nText models are often bag-of-words, while video models differ.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2511.02767/x5.png",
                "caption": "Figure 5:Video and text eventually are aligned, but encode temporal information differently.While the video text alignment is largely perfect for all of the models oncek=3k=3, they differ heavily before that when looking atk=1,2k=1,2.\nText models are often bag-of-words, while video models differ.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2511.02767/x6.png",
                "caption": "Figure 6:Video embeddings are sensitive to temporal caption reordering.Video-text alignment drops against the temporal reorder negatives in VideoComp, with the most aligned video representations having the largest such drop (models towards the right).",
                "position": 313
            }
        ]
    },
    {
        "header": "9Conclusion, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "10Acknowledgements",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.02767/x7.png",
                "caption": "Figure 7:Multiple captions provides a consistent improvement in alignment.\nGoing from 1 to 10 captions improves alignment by60%60\\%, based on the best fit estimate.",
                "position": 1363
            },
            {
                "img": "https://arxiv.org/html/2511.02767/x8.png",
                "caption": "Figure 8:Test time scaling is effective for other datasets.We apply our test time scaling for PVD.\n(left) Scaling with more frames is generally helpful, but is especially important for video models to improve. Here, frames are sampled linearly spaced.\n(right) Scaling the number of captions at test time, even synthesized ones, outperforms using the original provided caption (dotted lines).",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2511.02767/figs/pvd_frames.png",
                "caption": "Figure 9:Example of video frames, the original human-provided caption, and LLM-synthesized captions for a video in the PVD dataset.",
                "position": 1577
            },
            {
                "img": "https://arxiv.org/html/2511.02767/figs/video-video_alignment.png",
                "caption": "Figure 10:Video encoders cluster into semantic and geometric specialities.\nWe find that video-video alignment exhibits two clusters of models: those which are closer aligned to language and semantic tasks (bottom right), and those which are closer to geometric abilities (top left).\nHowever, some models are able to span both axes, notably DINOv2 and VideoMAEv2 K710 finetuned.",
                "position": 1858
            },
            {
                "img": "https://arxiv.org/html/2511.02767/figs/vatex_all_knn_k10_cid-0.png",
                "caption": "Figure 11:We measure alignment between a large number of vision and text encoders on the VATEX dataset, using only one caption for alignment.",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2511.02767/figs/vatex_all_knn_k10_all.png",
                "caption": "Figure 12:We measure alignment between a subset of our highest scoring vision and text encoders in Figure11on the VATEX dataset, usingall ten captionsfor alignment.\nWe observe a significant boost when integrating information across multiple captions.\nNotably, our best results significantly exceed those reported inHuh et al. (2024), highlighting that the paucity of annotations in both visual space (images vs. videos) and text space (single caption vs. multiple descriptions) can help to explain the limited alignment observed in prior work.",
                "position": 1866
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]