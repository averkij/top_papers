[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03052/images/teaser.jpg",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03052/x1.png",
                "caption": "Figure 2:Illustration of test-time scaling in our model. The model is trained with up to 6,144 tokens, but is evaluated under different token counts at test time, showing notable improvements.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x2.png",
                "caption": "Figure 3:LATTICE system: At its core is a novel VoxSet representation, enabling scalable 3D modeling from 0.6B to 4.5B.",
                "position": 138
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Scalable 3D Generative Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03052/x3.png",
                "caption": "Figure 4:Illustrations of different latent representations and different query types.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x4.png",
                "caption": "Figure 5:LATTICE Model Architecture: it features a two-stage coarse-to-fine pipeline and a novel VoxSet VAE and DiT.",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x5.png",
                "caption": "Figure 6:Illustration of model/training and test scaling effects.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x6.png",
                "caption": "Figure 7:Illustrations of applications of LATTICE. Mesh refinement in the left and part refinement in the right.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x7.png",
                "caption": "Figure 8:Visual comparison of geometry generation against several state-of-the-art open-source methods.",
                "position": 277
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03052/x8.png",
                "caption": "Figure 9:Visual comparison against commercial models.",
                "position": 476
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x9.png",
                "caption": "Figure 10:Ablation study on the proposed voxel query and VoxSet VAE, by incrementally adding each component.",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x10.png",
                "caption": "Figure 11:User study of our method against competitors showing win rate (%) across Overall, Subject, and Scene categories.",
                "position": 486
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADiscussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03052/x11.png",
                "caption": "Figure 12:Illustration of the effect of model scaling (in parameters) on performance. VecSet models show limited improvement as parameters increase, whereas larger VoxSet models produce finer and more detailed results.",
                "position": 578
            },
            {
                "img": "https://arxiv.org/html/2512.03052/x12.png",
                "caption": "Figure 13:Illustration of the effect of test-time scaling (in shape tokens) on model performance. VecSet models exhibit limited gains as the number of tokens increases, showing early saturation. In contrast, VoxSet models consistently benefit from higher token counts, producing finer details and demonstrating stronger scaling capability. * indicates the token count used during training.",
                "position": 581
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CPost-Training",
        "images": []
    },
    {
        "header": "Appendix DUser Study Setting",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03052/x13.png",
                "caption": "Figure 14:More visual results for image-to-geometry generation of LATTICE.",
                "position": 639
            }
        ]
    },
    {
        "header": "Appendix EMore Results",
        "images": []
    }
]