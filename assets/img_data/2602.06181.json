[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06181/x1.png",
                "caption": "Figure 1:Paper Overview. We curate thePostTrainingBiasBench(85K questions) and evaluate 10 models in 5 quantized formats (60 models). Pre- and post-quantization responses are paired and analyzed for changes in uncertainty and bias. Under the null hypothesis, paired responses are interchangeable, motivating the use of permutation tests to determine if changes in aggregate metrics are statistically significant.",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3A Unified Framework for Measuring Changes in Social Bias",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06181/x2.png",
                "caption": "Figure 2:Low confidence predictions are more likely to change after quantization. Model uncertainty is measured by the normalized Shannon entropy across options for closed-ended datasets.(a)High model uncertainty is more associated with response changes (blue), rather than when a response doesn’t change (yellow).(b)Model confidence is similarly distributed across questions before and after quantization.(c)Changes in model confidence per question is greater with stronger quantization strength (purple).\nFor (a) and (c), the y-axis is the probability density across responses.\nNote: BBQ centers around entropy≈0.63\\approx 0.63due to near-zero probability for the \"unknown\" option, while SocialStigmaQA shows near-certainty (entropy≡\\equiv0) with<1<1% flipping.",
                "position": 263
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06181/x3.png",
                "caption": "Figure 3:Quantization can significantly alter social bias.(a)The measured effect of PTQ varies by dataset. The x-axis is computed as the number of dataset – social axes that resulted in significantly different aggregate metrics after quantization.(b)For aggregate metrics with significant changes, the effect sizes are centered around 0.(c)Even without significant changes to aggregate metrics, PTQ can cause response flipping in almost a fourth of responses.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2602.06181/x4.png",
                "caption": "Figure 4:Quantization-induced behavior flipping varies by dataset, quantization method and model.Behavior flipping is measured as the percentage of aggregate measures that significantly change for each dataset×\\timesquantization method or model.(a)8-bit quantization exhibits lesser behavioral changes compared to 4-bit quantization methods.(b)Scaling parameter size does not seem to mitigate quantization-induced behavioral changes.(c)Relative model rankings for social bias is not consistent post-quantization.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2602.06181/x5.png",
                "caption": "Figure 5:Quantization affects social groups asymmetrically.(a)Different questions display different rates of response flipping across all models.(b)Quantization can cause large swings in social bias for certain social groups with as much as 25% of responses flipping in bias. On BBQ, we show this for two social groups (short, male), aggregating responses in three ways: across models, across quantizations of the same model and for individual models.(c)Even for the same social group (male), the percentage of behavior-flipped responses can differ by dataset.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2602.06181/x6.png",
                "caption": "Figure 6:Response flipping rates are driven by uncertainty.(a)Preference tuning can decrease model uncertainty (SimPO; in blue), while maximizing entropy increases model uncertainty (EntropyMax; in red).(b-c)Changes in relative uncertainty (entropy) and absolute uncertainty (average token probability) directly affect rates of response flipping.",
                "position": 367
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.06181/x7.png",
                "caption": "Figure S1:Lower average token probability is associated with higher rates of response flipping.",
                "position": 2811
            },
            {
                "img": "https://arxiv.org/html/2602.06181/x8.png",
                "caption": "Figure S2:4-bit quantization leads to greater changes in choice probability and normalized entropy. Both the probability of initially chosen response and the entropy of model-assigned probabilities change unpredictably post-quantization but center around 0.",
                "position": 2819
            },
            {
                "img": "https://arxiv.org/html/2602.06181/x9.png",
                "caption": "Figure S3:Response length and structure are greatly affected with little change in language-related errors.(a)Response lengths change unpredictably post-quantization with changes are centered around 0.(b)Sentence structure in generated text changes moderately. Quantized models maintain only around 30-50% of sequential content in responses before quantization.(c)The number of language errors, identified by LanguageTool, are mostly similar before and after quantization.",
                "position": 3297
            },
            {
                "img": "https://arxiv.org/html/2602.06181/x10.png",
                "caption": "Figure S4:Quantized models deviate quickly from the original model’s response.Box plots show for each quantized model, the proportion of words in the original response until a word differs",
                "position": 3307
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendices and Supplementary Material",
        "images": []
    }
]