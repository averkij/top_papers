[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08001/x1.png",
                "caption": "Figure 1:Training dynamics of singular values of the same weight matrix in a LLaMA model. Standard training on the left strictly follows the common practice for training LLMs (direct optimization with AdamW). POET on the right uses the proposed approximation for large-scale LLM training. The slight (almost negligible) singular value changes in POET are due to numerical and approximation error.",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x2.png",
                "caption": "Figure 2:POET‚Äôs three learning phases. Left: illustration; Middle: angle; Right: loss and validation.",
                "position": 172
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x3.png",
                "caption": "Figure 3:Singular values of a weight matrix of size 512√ó\\times√ó1376, randomly generated by different initialization schemes.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x4.png",
                "caption": "Figure 4:Examples of the primitive orthogonal transformation matrixùëÆisubscriptùëÆùëñ\\bm{G}_{i}bold_italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTin different orthogonalizations (two examples for each method). Note that, blue blocks represent1111, light purple blocks denote00and deep purple blocks are the actual orthogonal parameterization to be learned.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x5.png",
                "caption": "Figure 5:Performance of POET under a constant total parameter budget onùëπ,ùë∑ùëπùë∑\\bm{R},\\bm{P}bold_italic_R , bold_italic_P.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x6.png",
                "caption": "Figure 6:Validation perplexity dynamics on LLaMA-350M and LLaMA-1.3B.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x6.png",
                "caption": "",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x7.png",
                "caption": "",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x8.png",
                "caption": "Figure 7:Validation perplexity dynamics of POET (FS,bùëèbitalic_b=1/2) and AdamW on Llama-60M. POET outperforms the AdamW trained with almost twice the number of seen tokens.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x9.png",
                "caption": "Figure 8:Approximation error of orthogonal matricesùëπùëπ\\bm{R}bold_italic_Randùë∑ùë∑\\bm{P}bold_italic_Pof a weight matrix.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x10.png",
                "caption": "Figure 9:Cosine similarity for vector probing ofùë∑ùë∑\\bm{P}bold_italic_Pacross the self-attention components (query, key, value, and output projections) and feed-forward network components (up-, down-, and gate-projections) in all transformer blocks of a POET-trained Llama 60M model.",
                "position": 3037
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x10.png",
                "caption": "",
                "position": 3040
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x11.png",
                "caption": "",
                "position": 3044
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x12.png",
                "caption": "",
                "position": 3048
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x13.png",
                "caption": "",
                "position": 3053
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x14.png",
                "caption": "",
                "position": 3057
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x15.png",
                "caption": "",
                "position": 3061
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x16.png",
                "caption": "",
                "position": 3066
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x17.png",
                "caption": "Figure 10:Cosine similarity for vector probing ofùëπùëπ\\bm{R}bold_italic_Racross the self-attention components (query, key, value, and output projections) and feed-forward network components (up-, down-, and gate-projections) from all transformer blocks of a POET-trained Llama 60M model.",
                "position": 3072
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x17.png",
                "caption": "",
                "position": 3075
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x18.png",
                "caption": "",
                "position": 3079
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x19.png",
                "caption": "",
                "position": 3083
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x20.png",
                "caption": "",
                "position": 3088
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x21.png",
                "caption": "",
                "position": 3092
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x22.png",
                "caption": "",
                "position": 3096
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x23.png",
                "caption": "",
                "position": 3101
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x24.png",
                "caption": "Figure 11:Trace ofùë∑ùë∑\\bm{P}bold_italic_Pacross the self-attention components (query, key, value, and output projections) and feed-forward network components (up-, down-, and gate-projections) from all transformer blocks of a POET-trained Llama 60M model.",
                "position": 3107
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x24.png",
                "caption": "",
                "position": 3110
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x25.png",
                "caption": "",
                "position": 3114
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x26.png",
                "caption": "",
                "position": 3118
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x27.png",
                "caption": "",
                "position": 3123
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x28.png",
                "caption": "",
                "position": 3127
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x29.png",
                "caption": "",
                "position": 3131
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x30.png",
                "caption": "",
                "position": 3136
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x31.png",
                "caption": "Figure 12:Trace ofùëπùëπ\\bm{R}bold_italic_Racross the self-attention components (query, key, value, and output projections) and feed-forward network components (up-, down-, and gate-projections) from all transformer blocks of a POET-trained Llama 60M model.",
                "position": 3142
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x31.png",
                "caption": "",
                "position": 3145
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x32.png",
                "caption": "",
                "position": 3149
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x33.png",
                "caption": "",
                "position": 3153
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x34.png",
                "caption": "",
                "position": 3158
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x35.png",
                "caption": "",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x36.png",
                "caption": "",
                "position": 3166
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x37.png",
                "caption": "",
                "position": 3171
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x38.png",
                "caption": "(a)POET-BS (b=8ùëè8b=8italic_b = 8)",
                "position": 3189
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x38.png",
                "caption": "(a)POET-BS (b=8ùëè8b=8italic_b = 8)",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x39.png",
                "caption": "(b)POET-BS (b=16ùëè16b=16italic_b = 16)",
                "position": 3197
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x40.png",
                "caption": "(c)POET-BS (b=32ùëè32b=32italic_b = 32)",
                "position": 3202
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x41.png",
                "caption": "(a)POET-FS (b=1/8ùëè18b=1/8italic_b = 1 / 8)",
                "position": 3209
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x41.png",
                "caption": "(a)POET-FS (b=1/8ùëè18b=1/8italic_b = 1 / 8)",
                "position": 3212
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x42.png",
                "caption": "(b)POET-FS (b=1/4ùëè14b=1/4italic_b = 1 / 4)",
                "position": 3217
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x43.png",
                "caption": "(c)POET-FS (b=1/2ùëè12b=1/2italic_b = 1 / 2)",
                "position": 3222
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x44.png",
                "caption": "Figure 15:Training dynamics of the singular values of weight matrices\nwithin Blocks 0‚Äì1 (theiùëñiitalic_i-th row represents Blockiùëñiitalic_i) of a 60M Llama Transformer trained\nwithAdamW.",
                "position": 3237
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x45.png",
                "caption": "",
                "position": 3240
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x46.png",
                "caption": "",
                "position": 3241
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x47.png",
                "caption": "",
                "position": 3243
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x48.png",
                "caption": "",
                "position": 3244
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x49.png",
                "caption": "",
                "position": 3245
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x50.png",
                "caption": "",
                "position": 3247
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x51.png",
                "caption": "",
                "position": 3248
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x52.png",
                "caption": "",
                "position": 3249
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x53.png",
                "caption": "",
                "position": 3251
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x54.png",
                "caption": "",
                "position": 3252
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x55.png",
                "caption": "",
                "position": 3253
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x56.png",
                "caption": "Figure 16:Training dynamics of the singular values of weight matrices\nwithin Blocks 2‚Äì4 (theiùëñiitalic_i-th row represents Blockiùëñiitalic_i) of a 60M Llama Transformer trained\nwithAdamW.",
                "position": 3260
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x57.png",
                "caption": "",
                "position": 3263
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x58.png",
                "caption": "",
                "position": 3264
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x59.png",
                "caption": "",
                "position": 3266
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x60.png",
                "caption": "",
                "position": 3267
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x61.png",
                "caption": "",
                "position": 3268
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x62.png",
                "caption": "",
                "position": 3270
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x63.png",
                "caption": "",
                "position": 3271
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x64.png",
                "caption": "",
                "position": 3272
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x65.png",
                "caption": "",
                "position": 3274
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x66.png",
                "caption": "",
                "position": 3275
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x67.png",
                "caption": "",
                "position": 3276
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x68.png",
                "caption": "",
                "position": 3278
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x69.png",
                "caption": "",
                "position": 3279
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x70.png",
                "caption": "",
                "position": 3280
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x71.png",
                "caption": "",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x72.png",
                "caption": "",
                "position": 3283
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x73.png",
                "caption": "",
                "position": 3284
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x74.png",
                "caption": "Figure 17:Training dynamics of the singular values of weight matrices\nwithin Blocks 5‚Äì7 (theiùëñiitalic_i-th row represents Blockiùëñiitalic_i) of a 60M Llama Transformer trained\nwithAdamW.",
                "position": 3291
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x75.png",
                "caption": "",
                "position": 3294
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x76.png",
                "caption": "",
                "position": 3295
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x77.png",
                "caption": "",
                "position": 3297
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x78.png",
                "caption": "",
                "position": 3298
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x79.png",
                "caption": "",
                "position": 3299
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x80.png",
                "caption": "",
                "position": 3301
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x81.png",
                "caption": "",
                "position": 3302
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x82.png",
                "caption": "",
                "position": 3303
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x83.png",
                "caption": "",
                "position": 3305
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x84.png",
                "caption": "",
                "position": 3306
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x85.png",
                "caption": "",
                "position": 3307
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x86.png",
                "caption": "",
                "position": 3309
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x87.png",
                "caption": "",
                "position": 3310
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x88.png",
                "caption": "",
                "position": 3311
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x89.png",
                "caption": "",
                "position": 3313
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x90.png",
                "caption": "",
                "position": 3314
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x91.png",
                "caption": "",
                "position": 3315
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x92.png",
                "caption": "Figure 18:This plot illustrates the singular value training dynamics for individual weight matrices within Blocks 0-2 of a 60M Llama transformer model trained withPOET. For each block, the dynamics are shown for the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection).",
                "position": 3322
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x93.png",
                "caption": "",
                "position": 3325
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x94.png",
                "caption": "",
                "position": 3326
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x95.png",
                "caption": "",
                "position": 3328
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x96.png",
                "caption": "",
                "position": 3329
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x97.png",
                "caption": "",
                "position": 3330
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x98.png",
                "caption": "",
                "position": 3332
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x99.png",
                "caption": "",
                "position": 3333
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x100.png",
                "caption": "",
                "position": 3334
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x101.png",
                "caption": "",
                "position": 3336
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x102.png",
                "caption": "",
                "position": 3337
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x103.png",
                "caption": "",
                "position": 3338
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x104.png",
                "caption": "",
                "position": 3340
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x105.png",
                "caption": "",
                "position": 3341
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x106.png",
                "caption": "",
                "position": 3342
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x107.png",
                "caption": "",
                "position": 3344
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x108.png",
                "caption": "",
                "position": 3345
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x109.png",
                "caption": "",
                "position": 3346
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x110.png",
                "caption": "Figure 19:This plot illustrates the singular value training dynamics for individual weight matrices within Blocks 3-5 of a 60M Llama transformer model trained withPOET. For each block, the dynamics are shown for the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection).",
                "position": 3351
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x111.png",
                "caption": "",
                "position": 3354
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x112.png",
                "caption": "",
                "position": 3355
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x113.png",
                "caption": "",
                "position": 3357
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x114.png",
                "caption": "",
                "position": 3358
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x115.png",
                "caption": "",
                "position": 3359
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x116.png",
                "caption": "",
                "position": 3361
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x117.png",
                "caption": "",
                "position": 3362
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x118.png",
                "caption": "",
                "position": 3363
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x119.png",
                "caption": "",
                "position": 3365
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x120.png",
                "caption": "",
                "position": 3366
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x121.png",
                "caption": "",
                "position": 3367
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x122.png",
                "caption": "",
                "position": 3369
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x123.png",
                "caption": "",
                "position": 3370
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x124.png",
                "caption": "",
                "position": 3371
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x125.png",
                "caption": "",
                "position": 3373
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x126.png",
                "caption": "",
                "position": 3374
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x127.png",
                "caption": "",
                "position": 3375
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x128.png",
                "caption": "Figure 20:This plot illustrates the singular value training dynamics for individual weight matrices within Blocks 6-7 of a 60M Llama transformer model trained withPOET. For each block, the dynamics are shown for the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection).",
                "position": 3380
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x129.png",
                "caption": "",
                "position": 3383
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x130.png",
                "caption": "",
                "position": 3384
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x131.png",
                "caption": "",
                "position": 3386
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x132.png",
                "caption": "",
                "position": 3387
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x133.png",
                "caption": "",
                "position": 3388
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x134.png",
                "caption": "",
                "position": 3390
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x135.png",
                "caption": "",
                "position": 3391
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x136.png",
                "caption": "",
                "position": 3392
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x137.png",
                "caption": "",
                "position": 3394
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x138.png",
                "caption": "",
                "position": 3395
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x139.png",
                "caption": "",
                "position": 3396
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x140.png",
                "caption": "Figure 21:For the transformer block 0, we show approximation error of orthogonal matrixùëπùëπ\\bm{R}bold_italic_Rfor the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection).",
                "position": 3410
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x140.png",
                "caption": "",
                "position": 3413
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x141.png",
                "caption": "",
                "position": 3417
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x142.png",
                "caption": "",
                "position": 3421
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x143.png",
                "caption": "",
                "position": 3426
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x144.png",
                "caption": "",
                "position": 3430
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x145.png",
                "caption": "",
                "position": 3434
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x146.png",
                "caption": "",
                "position": 3439
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x147.png",
                "caption": "Figure 22:For the transformer block 0, we show approximation error of orthogonal matrixùë∑ùë∑\\bm{P}bold_italic_Pfor the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection).",
                "position": 3445
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x147.png",
                "caption": "",
                "position": 3448
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x148.png",
                "caption": "",
                "position": 3452
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x149.png",
                "caption": "",
                "position": 3456
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x150.png",
                "caption": "",
                "position": 3461
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x151.png",
                "caption": "",
                "position": 3465
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x152.png",
                "caption": "",
                "position": 3469
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x153.png",
                "caption": "",
                "position": 3474
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x154.png",
                "caption": "Figure 23:The approximation error of orthogonal matrixùë∑ùë∑\\bm{P}bold_italic_Pin a randomly selected down-projection layer after training 10000 steps.",
                "position": 3484
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x155.png",
                "caption": "(a)Llama 60M",
                "position": 3495
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x155.png",
                "caption": "(a)Llama 60M",
                "position": 3498
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x156.png",
                "caption": "(b)Llama 130M",
                "position": 3503
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x157.png",
                "caption": "(c)Llama 350M",
                "position": 3509
            },
            {
                "img": "https://arxiv.org/html/2506.08001/x158.png",
                "caption": "(d)Llama 1.3B",
                "position": 3514
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]