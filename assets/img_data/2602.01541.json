[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2CogSense Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01541/x1.png",
                "caption": "Figure 1:CogSense-Dataset Examples.Samples across each category from the CogSense-Dataset. CogSense-Dataset comprises various visual cognitive questions classified into five categories: Fluid Intelligence, Crystallized Intelligence, Visuospatial Cognition, Mental Simulation, and Visual Routines, which require visual imagery and cognitive supersensing with deep thinking and reasoning.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2602.01541/x2.png",
                "caption": "Figure 2:CogSense-Dataset Distribution.The data distribution of our CogSense-Dataset-105K.",
                "position": 205
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01541/x3.png",
                "caption": "Figure 3:The framework of Cognitive Surpersensing.Left:Architecture Overview.CogSense-8B is a VLM that takes images and prompts as input with a text decoder to generate the answer and a Latent Visual Imagery Prediction (LVIP) head to generate a latent visual imagery of the option-image in parallel.Right:Method Overview.To train CogSense-8B, we (1) generate reasoning paths via LLMs, (2) implement SFT to jointly optimize the LVIP head and the model weights, and (3) implement RL to further optimize reasoning paths with Latent Rationales.",
                "position": 261
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01541/x4.png",
                "caption": "Figure 4:Qualitative Example of Visual Cognition Reasoning Across Models.Weunderlinedecisive sentences in the reasoning chain. CogSense-8B demonstrates a coherent, multi-step logical chain that closely matches the ground truth, while other models exhibit less precise or less interpretable reasoning paths",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2602.01541/x5.png",
                "caption": "Figure 5:EMMA Benchmark Sample Problems.",
                "position": 689
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AStatistics and Samples of Dataset and Benchmark",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details for Data Pipeline",
        "images": []
    },
    {
        "header": "Appendix CHuman Study Design and Setup",
        "images": []
    },
    {
        "header": "Appendix DMore Qualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01541/x6.png",
                "caption": "Figure D1:More Qualitative Examples of Visual Cognition Reasoning Across Models.CogSense-8B demonstrates a coherent, multi-step logical chain that closely matches the ground truth, while other models exhibit less precise or less interpretable reasoning paths",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2602.01541/x6.png",
                "caption": "",
                "position": 1042
            },
            {
                "img": "https://arxiv.org/html/2602.01541/x7.png",
                "caption": "",
                "position": 1047
            }
        ]
    },
    {
        "header": "Appendix ERelated Works",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]