[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1INTRODUCTION",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16151/x1.png",
                "caption": "Figure 1:Non-reasoning MLLMs lack reasoning and reflection abilities, while reasoning LLMs are unable to perceive visual information. We propose a training-free, closed-form layerwise fusion method that combines visual perception and language reasoning strengths, substantially enhancing overall reasoning capability in multimodal settings.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16151/x2.png",
                "caption": "Figure 2:Layer-wise visual attention of NVIL-15B.\nEach curve shows the average attention from a text token to all visual tokens across layers.\nShallow layers assign significantly higher attention to visual tokens, while attention in deeper layers approaches zero and rapidly descends indicating a shift from perception to language reasoning.\nThis supports our use of an exponential decay prior to the fusion process.",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x3.png",
                "caption": "Figure 3:Cosine similarity between task vectors of vision-finetuned (NVIL-15B) and reasoning-finetuned (DeepSeekDistil-Qwen2.5-14B) models at each decoder block. The task vector at each block is computed by flattening the weight deltas with respect to the base model. The similarity remains close to 0 across all layers, indicating strong near-orthogonality.",
                "position": 419
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16151/x4.png",
                "caption": "Figure 4:Average output length of the FRANK on the MMMU benchmark, stratified by task difficulty.",
                "position": 1092
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x5.png",
                "caption": "Figure 5:Output examples from FRANK-8B and the non-reasoning baseline model Idecifics3-8B. Here,<think>and</think>denote R1-like reasoning processes, while blue text indicates reflection tokens.",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x6.png",
                "caption": "Figure 6:Output examples from FRANK-15B and the non-reasoning baseline model NVIL-15B.",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x7.png",
                "caption": "Figure 7:Output examples from FRANK-38B and the non-reasoning baseline model InternVL2.5-38B.",
                "position": 1126
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16151/x8.png",
                "caption": "Figure 8:Layer-wise visual attention of Idefics3-8B.",
                "position": 2292
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x9.png",
                "caption": "Figure 9:Cosine similarity between task vectors of vision-finetuned (Idefics3-8B) and reasoning-finetuned (DeepSeekDistil-LLaMA3-8B) models at each decoder block.",
                "position": 2295
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x10.png",
                "caption": "Figure 10:Layer-wise visual attention of InternVL2.5-38B.",
                "position": 2387
            },
            {
                "img": "https://arxiv.org/html/2505.16151/x11.png",
                "caption": "Figure 11:Cosine similarity between task vectors of vision-finetuned (InternVL2.5-38B) and reasoning-finetuned (QwQ-32B) models at each decoder block.",
                "position": 2390
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]