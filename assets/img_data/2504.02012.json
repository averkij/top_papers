[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02012/x1.png",
                "caption": "Figure 1:Our approach integrates a VQ-VAE autoencoder (ùêÑùêÑ\\mathbf{E}bold_E‚ÄìùêÉùêÉ\\mathbf{D}bold_D) with a transformer prior. First, the VQ-VAE encodes vectorized network parameters (see Section2.2), and then the transformer is trained on the resulting codebook (see Section3). Additionally, prompts‚Äîincluding data, task, or architecture details‚Äîare processed using multimodal or language modeling techniques (see Section3), with an example training simplified prompt template provided in Remark1.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Instruction-Guided Parameters Generation",
        "images": []
    },
    {
        "header": "3Autoregressive Parameter Generation",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02012/x2.png",
                "caption": "Figure 2:Transfer learning evaluation on novel datasets: CIFAR100, CIFAR10, Aircraft30, and PETS10 compared to random initialization.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2504.02012/x3.png",
                "caption": "Figure 3:Performance evaluation with seen and unseen ResNet architectures on CIFAR-10 against models pretrained on CIFAR-100 and Random Initialization.",
                "position": 591
            },
            {
                "img": "https://arxiv.org/html/2504.02012/x4.png",
                "caption": "(a)CIFAR10",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2504.02012/x4.png",
                "caption": "(a)CIFAR10",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2504.02012/x5.png",
                "caption": "(b)CIFAR100",
                "position": 602
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results and Tables",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02012/x6.png",
                "caption": "Figure 5:Parameters distribution of diverse architectures pretrained on CIFAR-10 and CIFAR100 all jointly encoded by IGPG",
                "position": 1630
            }
        ]
    },
    {
        "header": "Appendix CRelated Work",
        "images": []
    },
    {
        "header": "Appendix DAblation Study",
        "images": []
    }
]