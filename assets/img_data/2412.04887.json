[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04887/extracted/6050107/images/teaser.png",
                "caption": "",
                "position": 106
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04887/x1.png",
                "caption": "Figure 2:Comparison of three approaches for using hybrid representations to reconstruct large-scale scenes in a divide-and-conquer manner.Examples with two blocks: (a) Independent training of each block, resulting in separate models that cannot be merged due to independent Gaussian Decoders, complicating rendering; (b) Parallel training with a shared Gaussian decoder, allowing merged output but limited by GPU availability; (c) Our approach with a Momentum Gaussian Decoder, providing global guidance to each block and improving consistency across blocks.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04887/x2.png",
                "caption": "Figure 3:Overview of the proposed Momentum-GS.Our method begins by dividing the scene into multiple blocks (left), periodically sampling a subset of blocks (e.g., 4 blocks) and assigning them to available GPUs for parallel processing. The momentum Gaussian decoder provides stable global guidance to each block, ensuring consistency across blocks. To align the online Gaussians with the momentum Gaussian decoder, a consistency loss is applied. During splatting, predicted images are compared with ground truth images, and the resulting reconstruction loss is used to update the shared online Gaussian decoder. Additionally, reconstruction-guided block weighting dynamically adjusts the emphasis on each block, prioritizing underperforming blocks to enhance overall scene consistency.",
                "position": 190
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04887/x3.png",
                "caption": "Figure 4:Qualitative comparisons of our Momentum-GS and prior methods across four large-scale scenes.Redinsets highlight patches that reveal notable visual differences between these methods. Our method (d) demonstrates better fidelity in capturing fine details, maintaining structural consistency, and accurately representing textures. This results in visual reconstructions that are closer to the ground truth (e) compared to Mega-NeRF (a), 3D-GS (b), and CityGaussian (c), which exhibit artifacts, blurring, or inconsistencies in these areas.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2412.04887/x4.png",
                "caption": "Figure 5:Qualitative comparisons of our Momentum-GS and other methods on a large-scale urban scene MatrixCity. Our method (c) demonstrates greater detail preservation in challenging regions such as building facades and edges, closely matching the ground truth (d). In contrast, 3D-GS (a) and CityGaussian (b) exhibit obvious artifacts, blurring, or loss of structural detail in these areas.",
                "position": 580
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Quantitative Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04887/x5.png",
                "caption": "Figure 6:Qualitative comparisons of our Momentum-GS and prior methods across four large-scale scenes.",
                "position": 1781
            }
        ]
    },
    {
        "header": "7More Visual Comparisons",
        "images": []
    }
]