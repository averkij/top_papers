[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/4D_Spatial_Intelligence.png",
                "caption": "Figure 1:Classification of 4D spatial intelligence by level.Specifically, in this survey, we categorize the methods of reconstructing 3D spatial intelligence from video into five levels: (1) low-level 3D cues, (2) 3D scene components, (3) 4D dynamic scenes, (4) modeling of interactions among scene components, and (5) incorporation of physical laws and constraints.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Level 1 – Low-level 3D cues",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level1.png",
                "caption": "Figure 2:The paradigms of methods for reconstructing low-level cues from video input.(I) Video-based depth reconstruction methods recently leverage the diffusion model to obtain the depth maps; (II) Methods for reconstructing camera pose from video input typically employ the neural network to infer the camera pose based on the encoded image features; (III) 3D tracking methods uses point tracker and transformers to achieve 3D tracking from video input; (IV) Recent methods, such as VGGT, apply DINO to extract the features and then train transformer-based DPT heads to infer the unified 3D attributes. “Enc.”, “Dec.”, “Spt. Grid”, “Qry. Points”, and “Cam.” denote “Encoder”, “Decoder”, “Supporting Grid”, “Query Points”, and “Camera Head” correspondingly.",
                "position": 239
            }
        ]
    },
    {
        "header": "3Level 2 – 3D scene components",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level2.png",
                "caption": "Figure 3:The paradigms of methods for reconstructing 3D scene components from video input.3D reconstruction methods for small-scale and large-scale scenes often share similar architectures, differing primarily in the spatial extent they handle. As shown in the left panel (Image source: MipNeRF360[255]), small-scale scenes correspond to the unaffected domain. large-scale scenes additionally incorporate a contracted domain. Examples illustrating both scene types are provided in the right panel.",
                "position": 392
            }
        ]
    },
    {
        "header": "4Level 3 – 4D dynamic scenes",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level3.png",
                "caption": "Figure 4:The paradigms of methods for reconstructing dynamic scenes from video input.Methods in this domain typically adopt one of two strategies for temporal modeling: (I) explicitly incorporating time as an additional input to extend a static 3D representation, or (II) reconstructing a canonical 3D space and learning its deformation over time. “Def.” denotes “Deformation module”.",
                "position": 484
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level3-human.png",
                "caption": "Figure 5:The illustrations of methods for reconstructing 4D dynamic humans from video input.Human-centric dynamic modeling approaches are generally categorized based on their representations: (I) methods that apply SMPL parametric model as their representation to derive the human pose and shape parameters (image source: Neural Body Fitting[371]), (II) methods that similarly apply SMPL but focus more on the prediction based on egocentric videos (image source: EgoAllo[372]), and (III) appearance-rich non-parametric methods that are capable of reconstructing the textured topologies, such as garments and accessories, from video data (image source: Neural Body[373]).",
                "position": 526
            }
        ]
    },
    {
        "header": "5Level 4 – Interactions among scene components",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level4-smpl.png",
                "caption": "Figure 6:Examples of methods for modeling SMPL-based human-centric interaction.Image source: InterDreamer[439], CIRCLE[440], and BUDDI[441].",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level4-textured-human.png",
                "caption": "Figure 7:The paradigms of methods for reconstructing appearance-rich human-centric interaction.These methods generally build on SMPL-based linear blend skinning (LBS) deformation, extending the human body skeleton to include interacted objects. An example result is shown in the figure below. (image source: HOSNeRF[84]). “H. Def.”, “O. Def.”, and “Ext. LBS” denote “Human Deformation”, “Object Deformation”, and “Extended SMPL-based LBS” correspondingly.",
                "position": 687
            }
        ]
    },
    {
        "header": "6Level 5 – Incorporation of physical laws and constraints",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/survey-level5.png",
                "caption": "Figure 8:The paradigms of methods for inferring physically grounded 3D spatial understanding from videos.(I) Physical dynamic human modeling methods learn motion policies from real-world captures of human-object interactions, enabling deployment in simulators and transfer to humanoid robotics (image source: SkillMimic[469]). (II) Physically plausible 3D scene reconstruction mitigates missing geometry artifacts prevalent in traditional approaches, producing simulation-ready environments (image source: PhyRecon[470]).",
                "position": 720
            }
        ]
    },
    {
        "header": "7Challenges and future directions",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/yukang-cao.jpg",
                "caption": "",
                "position": 4473
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/jiahao-lu.jpg",
                "caption": "",
                "position": 4490
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/zhisheng-huang.jpg",
                "caption": "",
                "position": 4504
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/zhuowen-shen.jpg",
                "caption": "",
                "position": 4518
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/chengfeng-zhao.jpg",
                "caption": "",
                "position": 4532
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/fangzhou-hong.jpg",
                "caption": "",
                "position": 4547
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/zhaoxi-chen.jpg",
                "caption": "",
                "position": 4562
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/xin-li.jpg",
                "caption": "",
                "position": 4579
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/wenping-wang.jpg",
                "caption": "",
                "position": 4595
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/yuan-liu.jpg",
                "caption": "",
                "position": 4613
            },
            {
                "img": "https://arxiv.org/html/2507.21045/images/authors/ziwei-liu.jpg",
                "caption": "",
                "position": 4628
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]