[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Diversified Multiplet Upcycling for CLIP",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19291/x1.png",
                "caption": "Figure 1:Overview of Diversified Multiplet Upcycling: Our approach involves three key steps. (a) Fine-tuning the base CLIP model using the MCL framework while freezing all parameters except for the FFN layers. This process yields a new set of FFN layers at each stage of MCL. (b) Using the obtained FFN layers as experts to initialize a CLIP-MoE. (c) Continuously fine-tuning the CLIP-MoE using both contrastive learning loss and a router balancing loss to optimize the routers. The terms ‘color’, ‘shape’, and ‘texture’ are metaphorical representations of abstract features.",
                "position": 225
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.19291/x2.png",
                "caption": "Figure 2:Proportion of tokens assigned to each expert on the COCO and ImageNet validation dataset. Here, we consider experts that are either selected as a first or second choice by the router.",
                "position": 809
            },
            {
                "img": "https://arxiv.org/html/2409.19291/x3.png",
                "caption": "Figure 3:Example cases comparing the performance of CLIP-MoE and OpenAI CLIP on the MMVP-VLM Benchmark, illustrating differences in their ability to capture fine-grained semantic information.",
                "position": 819
            }
        ]
    },
    {
        "header": "6Conclusion & Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]