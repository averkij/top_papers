[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/logo_crop.png",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12513/x1.png",
                "caption": "Figure 1:Multimodal interleaved documents are unsuitable for vision-language representation learning. We construct distinct image-text pairs from such documents via retrieval and generation.",
                "position": 125
            },
            {
                "img": "https://arxiv.org/html/2502.12513/x2.png",
                "caption": "Figure 2:The Real-World Data Extraction pipeline to extract high-quality images and texts from interleaved image-text documents.",
                "position": 134
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3RealSynDataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12513/x3.png",
                "caption": "Figure 3:The architecture of our proposed framework, which constructs distinct image-text pairs from real-world data extracted from interleaved documents via retrieval and generation.",
                "position": 216
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12513/x4.png",
                "caption": "Figure 4:A T-SNEVan¬†der Maaten and Hinton (2008)projection of LDABlei et¬†al. (2003)topic cluster from a randomly selected 1M samples fromRealSyn.",
                "position": 1099
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_distribution/data_consine_similarity_analysis.png",
                "caption": "(a)Richness assessment comparison",
                "position": 1110
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_distribution/data_consine_similarity_analysis.png",
                "caption": "(a)Richness assessment comparison",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_distribution/data_token_len.png",
                "caption": "",
                "position": 1116
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_richness/diversity_analysis.png",
                "caption": "(b)Diversity assessment comparison",
                "position": 1123
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/model_scaling/model_scaling_combined.png",
                "caption": "Figure 6:Model scaling capability. We compare the models pre-trained on LAION30M andRealSyn30M.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/image_captioning/coco2017_name.png",
                "caption": "Figure 7:Image captioning comparisons on COCO2017 and Flickr30k. B4, MT., RL. and Cd. represent the metric of BLEU, METEOR, ROUGE-L, and Cider.",
                "position": 1148
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/image_captioning/coco2017_name.png",
                "caption": "",
                "position": 1151
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/image_captioning/flickr30K_name.png",
                "caption": "",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/concept_balance/Image_concept_balance_resize.png",
                "caption": "Figure 8:Clustering distribution of 15M data obtained from random sampling and semantic balanced sampling.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2502.12513/x5.png",
                "caption": "Figure 9:Visualization of the raw interleaved document, the retrieved realistic text, and synthetic text. Image semantic-related information is highlighted ingreen.",
                "position": 1384
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetail Experiment Settings",
        "images": []
    },
    {
        "header": "Appendix BDetail External Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12513/extracted/6212411/Figures/data_scaling_law.png",
                "caption": "Figure 10:Data Scaling Analysis. Pre-training ViT-B/32 onRealSynin different data scales.",
                "position": 3303
            },
            {
                "img": "https://arxiv.org/html/2502.12513/x6.png",
                "caption": "Figure 11:Visualization of image-text pairs in our proposedRealSyndataset.TrksubscriptsuperscriptùëáùëòùëüT^{k}_{r}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT: thekùëòkitalic_k-th retrieved realistic text.Tsksubscriptsuperscriptùëáùëòùë†T^{k}_{s}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT: the image semantic augmented synthetic text forTrksubscriptsuperscriptùëáùëòùëüT^{k}_{r}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. Image semantic-related information is highlighted ingreen.",
                "position": 3653
            },
            {
                "img": "https://arxiv.org/html/2502.12513/x7.png",
                "caption": "Figure 12:Visualization of image-text pairs in our proposedRealSyndataset.TrksubscriptsuperscriptùëáùëòùëüT^{k}_{r}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT: thekùëòkitalic_k-th retrieved realistic text.Tsksubscriptsuperscriptùëáùëòùë†T^{k}_{s}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT: the image semantic augmented synthetic text forTrksubscriptsuperscriptùëáùëòùëüT^{k}_{r}italic_T start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT. Image semantic-related information is highlighted ingreen.",
                "position": 3656
            }
        ]
    },
    {
        "header": "Appendix CFurther Analysis ofRealSyn",
        "images": []
    }
]