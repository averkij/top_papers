[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17192/x1.png",
                "caption": "Figure 1:(a) PaperCoder overview. The proposed PaperCoder aims to transform given scientific papers (in machine learning domains) into code repositories, which consists of three sequential steps: planning, analyzing, and coding. (b) Code availability. The availability of the code repositories, where blue bars indicate the total number of accepted papers and orange regions represent the subset of papers with officially released code.",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2504.17192/x2.png",
                "caption": "",
                "position": 149
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17192/x3.png",
                "caption": "Figure 2:PaperCoder framework. (Left) The naive approach, where a model directly generates code from the paper. (Right) PaperCoder framework, which decomposes the task into three stages: (1) Planning, where a high-level implementation plan is constructed based on the paper’s content, including overall plan, architectural design, logic design, and configuration files; (2) Analyzing, where the plan is translated into detailed file-level specifications; and (3) Coding, where the final codes are generated to implement the paper’s methods and experiments.",
                "position": 197
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.17192/x4.png",
                "caption": "Table 1:Results on our experimental Paper2Code benchmark. We report average model-based evaluation scores for each conference. Oracle denotes the official repository released by the paper’s authors. Reference-based evaluation assesses correctness (on a 1–5 scale) by comparing the generated repository against both the paper and the official implementation, while reference-free evaluation relies solely on the paper. We report statistics on the average number of tokens, files, and functions per repository. The best scores are highlighted in bold.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2504.17192/x4.png",
                "caption": "Figure 3:Correlation between reference-based and reference-free model-based evaluations.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2504.17192/x5.png",
                "caption": "",
                "position": 515
            }
        ]
    },
    {
        "header": "5Experimental Results and Analysis",
        "images": []
    }
]