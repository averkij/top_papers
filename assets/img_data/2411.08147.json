[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Understanding the Potential of LLMs in Long-context Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08147/extracted/5995567/figures/impact_of_exploration.png",
                "caption": "Figure 1:Scaling up the number of sampled outputs improves the performance of both the oracle sample and MBR decoding (ยง3.1). The results are based on Llama-3.1-8B-Instruct.",
                "position": 233
            }
        ]
    },
    {
        "header": "3SeaLong",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08147/x1.png",
                "caption": "Figure 2:SeaLongconsists of two stages: self-supervision creation and fine-tuning. Given a long context and a corresponding query, multiple outputs are sampled, each assigned a score based on Minimum Bayes Risk. Fine-tuning is then conducted using either the highest-scoring output for supervised fine-tuning or both high-scoring and low-scoring outputs for preference optimization.",
                "position": 293
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08147/extracted/5995567/figures/impact_of_the_number_of_training_examples.png",
                "caption": "Figure 3:Long-context performance ofSeaLongwith varying numbers of synthetic training examples, evaluated based on Llama-3.1-8B-Instruct fine-tuned on the corresponding dataset.",
                "position": 914
            },
            {
                "img": "https://arxiv.org/html/2411.08147/extracted/5995567/figures/impact_of_sampling_quantity.png",
                "caption": "Figure 4:Long-context performance ofSeaLongwith varying numbers of samples per example during data synthesis, evaluated based on Llama-3.1-8B-Instruct fine-tuned on the corresponding dataset.",
                "position": 925
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Details",
        "images": []
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    }
]