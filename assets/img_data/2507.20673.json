[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20673/figs/gmpo_teaser.png",
                "caption": "Figure 1:Comparison between vanilla Group Relative Policy Optimization (GRPO) and our proposed GMPO. GRPO optimizes the arithmetic mean of token-level rewards, while GMPO uses the geometric mean (left).\nFor GRPO, extreme importance sampling ratios emerge frequently during training, which implies unstable policy updates. In contrast, GMPO results in more stable importance sampling ratios with less extreme values and narrower range (right).",
                "position": 65
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Geometric-Mean Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20673/figs/ratio_with_different_clip.png",
                "caption": "Figure 2:The range of importance sampling ratio with respect to different clip range and training steps. A wider range indicates less stable policy updates. Compared to GRPO with a clip range of (0.8, 1.2), GMPO demonstrates greater stability, even with a larger clip range of (e−0.4,e0.4e^{-0.4},e^{0.4}italic_e start_POSTSUPERSCRIPT - 0.4 end_POSTSUPERSCRIPT , italic_e start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT). All curves are smoothed for clarity.",
                "position": 352
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20673/figs/gmpo_analysis.png",
                "caption": "Figure 3:Analysis of entropy, KL divergence and reward over training steps. (a–b) GMPO maintains higher entropy than GRPO, whether trained on MATH Level 3–Level 5 or DeepScaleR dataset.\n(c) GMPO maintains a smaller KL divergence from the pre-RL model than GRPO, suggesting more stable learning and reduced overfitting risk.\n(d-f) GMPO achieves competitive rewards with GRPO on simpler dataset like MATH Level 3–Level 5, and outperforms GRPO in training rewards on more challenging datasets, such as DeepScaleRdeepscaler2025and Geometry3Kgeometry3k.",
                "position": 773
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]