[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23500/x1.png",
                "caption": "Figure 1:Accuracy correlation with different metrics for the 760M model. Traditional outlier-sensitive metrics like MMR (Left) and kurtosis (Center) show little to no correlation (measured byρ\\rho) with model accuracy, whereas our proposed metric (Right) correlates strongly with the model’s zero-shot performance. MMR and kurtosis are computed row-wise, on the output of the last transformer block.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Background and Experimental Setup",
        "images": []
    },
    {
        "header": "3Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23500/x2.png",
                "caption": "Figure 2:The effect of changing learning-rateη\\etaon the validation loss and MMR in different optimizers for 760M model. We average the MMR over the rows in the input tensor of the last linear layer (before head).",
                "position": 400
            },
            {
                "img": "https://arxiv.org/html/2509.23500/x3.png",
                "caption": "Figure 3:ABC decomposition for the760760M models. The x-axis shows the module indexℓ\\ell. Here is the only exception where we use the truncated average (average after we ignore the top 1% of the values) as our summary statistic.Trunc⁡(Rℓ)\\operatorname{Trunc}(R_{\\ell})andAvg⁡(Rℓ)\\operatorname{Avg}(R_{\\ell})only differ non-negligibly for a single intermediate layer for Shampoo, where a spike inAvg⁡(Rℓ)\\operatorname{Avg}(R_{\\ell})would force us to use log-scale.",
                "position": 516
            },
            {
                "img": "https://arxiv.org/html/2509.23500/x4.png",
                "caption": "Figure 4:Gain decomposition for the760760M models with different optimizers. The x-axis shows the module indexℓ\\ell.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2509.23500/x5.png",
                "caption": "Figure 5:Scaling Laws for each optimizer, for full precision (BF16) and QAT (W4A4). For each optimizer, we report the parameter efficiencyρ\\rhoof 4-bit QAT in the subplot title. Shampoo has the highest parameter efficiency, followed by AdamW.",
                "position": 710
            }
        ]
    },
    {
        "header": "4Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]