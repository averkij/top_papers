[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04996/tmlr-style-file-main/figs/ada_code.jpeg",
                "caption": "Figure 1:Plug-and-play usage. Left: a one-line swap of the generation API inverl(generate_sequences→\\rightarrowgenerate_multi_round_adaptive_downsampling). Right: with no other changes, training attains faster reward growth and a higher asymptote than GRPO.",
                "position": 90
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x1.png",
                "caption": "",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04996/x2.png",
                "caption": "Figure 2:Pass@k curves (left) and the ratio of prompts with all-correct responses (right) for two models on a subset of the Open-R1 prompt set.\nThe models tested are the Qwen2.5-Math-1.5B base model and an intermediate checkpoint from its RL training. The percentage of prompts yielding all-correct/all-incorrect responses is high for smallkkbut drops significantly askkincreases. This suggests that signal loss is often a statistical artifact of small sample groups.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x3.png",
                "caption": "",
                "position": 149
            }
        ]
    },
    {
        "header": "2Reinforce-Ada: Reinforce with Adaptive Sampling",
        "images": []
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04996/x4.png",
                "caption": "Figure 3:Training reward vs. steps for GRPO andReinforce-Adaacross backbones:\nQwen2.5-Math-1.5B, Qwen2.5-Math-7B, and Llama-3.2-3B-it, Qwen3-4B. Curves are smoothed with a 20-step moving average. In all cases,Reinforce-Adalearns faster and reaches a higher reward than GRPO, with theBalancevariant typically achieving the highest asymptote.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x5.png",
                "caption": "",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x6.png",
                "caption": "",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x7.png",
                "caption": "",
                "position": 376
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x8.png",
                "caption": "Figure 4:Reward–entropy trade-off (left and mid) and Pass@k (right) on the test benchmarks.Reinforce-Adashifts the frontier outward: higher reward at fixed entropy and higher entropy at fixed reward and converts this into stronger Pass@k at small, practical budgets (k≤8k\\leq 8), withReinforce-Ada-balancetypically best.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x9.png",
                "caption": "",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x10.png",
                "caption": "",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x11.png",
                "caption": "Figure 5:Sampling dynamics with the Qwen2.5-Math-1.5B model. Left: additional samples generated in later rounds compared to standard GRPO. Middle: number of prompts that remain active after multi-round adaptive sampling with theReinforce-Ada-balancevariant. Right: number of prompts that satisfy the stopping criteria within the first two rounds with theReinforce-Ada-balancevariant. All curves are smoothed using a moving average with a window size of2020.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x12.png",
                "caption": "",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x13.png",
                "caption": "",
                "position": 657
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x14.png",
                "caption": "Figure 6:Ablation studies on the prompt set difficulty. Left: prompt set with moderate difficulty. Right: challenging prompt set. The benefit of adaptive sampling is more obvious with challenging prompt set.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2510.04996/x15.png",
                "caption": "",
                "position": 693
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion and End Note",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAuthorship and Credit Attribution",
        "images": []
    }
]