[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x1.png",
                "caption": "",
                "position": 324
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIInfrastructure: MetaSim",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x2.png",
                "caption": "Figure 2:RoboVerseconsists of a simulation platform, a large-scale, high-quality dataset, and unified benchmarks. At the core of the simulation platform isMetaSim, the infrastructure ofRoboVerse. Powered byMetaSim, the simulation platform facilitates dataset creation and benchmark construction.",
                "position": 443
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x3.png",
                "caption": "Figure 3:MetaSimprovides a universal configuration system, aligned simulator backends, and a Gym[115]environment wrapper. This three-layer architecture abstracts simulation environments into simulator-agnostic specifications and aligns simulator backends, enabling three key capabilities: cross-simulator integration, hybrid simulation and cross-embodiment transfer. Based onMetaSim, we build a pipeline to collect tasks, assets and trajectories from diverse public sources in a unified format, employ data augmentation methods, and ultimately generate a large-scale high-quality dataset along with unified benchmarks. This data pipeline forms the foundation ofRoboVerse, facilitating the generation of large-scale datasets and construction of unified benchmarks.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x4.png",
                "caption": "Figure 4:TheMetaConfigis a nested dataclass that abstracts the core components in any simulation environment in a simulator-agnostic way.",
                "position": 471
            }
        ]
    },
    {
        "header": "IVRoboVerse Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x5.png",
                "caption": "Figure 5:Teleoperation System.RoboVersesupports\nvarious user-friendly teleoperation approaches. Currently, it\nenables teleoperation via a phone app (second row), motion capture (middle),\nVR devices (bottom left), as well as keyboard and joystick (bottom\nright). These methods allow control of robotic arms, dexterous hands,\nand bimanual systems across different simulators.",
                "position": 805
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x6.png",
                "caption": "Figure 6:AI-Assisted Task Generation.RoboVersesupports an\nAI-assisted task generation framework that leverages large generative\nmodels’ extrapolation capabilities to generate non-trivial and\nsemantically rich tasks. Combined with our teleoperation system, it\nenables the generation of diverse and high-quality data.",
                "position": 813
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x7.png",
                "caption": "Figure 7:Real-to-Sim Tools.We use a mobile device to\ncapture multi-view images, reconstruct a high-quality mesh, build a\nURDF using VLM, and then perform actions in bothRoboVerseand the real\nworld.",
                "position": 821
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x8.png",
                "caption": "Figure 8:Dataset Comparison and Gallery. Left: other representative\nsynthetic robotics datasets. Right: theRoboVersedataset.",
                "position": 1043
            }
        ]
    },
    {
        "header": "VRoboVerse Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x9.png",
                "caption": "Figure 9:Benchmark Protocol:We define a four-level generalization benchmarking\nprotocol, allocating 90% of the data for training and 10% for\ngeneralization evaluation. From left to right, Levels00to3333corresponds\nto task space generalization, environment radomization, camera randomization,\nlighting and reflection randomization, respectively.",
                "position": 1093
            }
        ]
    },
    {
        "header": "VIExperimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/extracted/6391426/fig/data_aug.png",
                "caption": "Figure 10:Effectiveness of Trajectory Augmentation.Success rates of policy trained with augmented dataset and source dataset.",
                "position": 1421
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x10.png",
                "caption": "Figure 11:Ablation Study of Action-conditioned World Model Learning.We compare the qualitative results of an action-conditioned world model trained on pure DROID and DROID-RoboVerse datasets, with evaluations sampled from the DROID dataset.",
                "position": 1430
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x11.png",
                "caption": "Figure 12:Sim-to-Real and Sim-to-Sim-to-Real Experiment Results.We demonstrate that learning within theRoboVerseframework enables seamless direct Sim-to-Real transfer for manipulating unseen objects in new environments (imitation learning) and Sim-to-Sim-to-Real transfer for whole-body humanoid control (reinforcement learning).",
                "position": 1449
            },
            {
                "img": "https://arxiv.org/html/2504.18904/extracted/6391426/fig/real_3.png",
                "caption": "Figure 13:Generalization of Sim-to-Sim-to-Real.This figure shows the in-the-wild generalization ability of our lower-body RL policy with upper-body PD control by the sim-to-sim-to-real approach.",
                "position": 1452
            }
        ]
    },
    {
        "header": "VIILimitations",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "VIIISimulators Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x12.png",
                "caption": "(a)Momentum",
                "position": 3792
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x12.png",
                "caption": "(a)Momentum",
                "position": 3795
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x13.png",
                "caption": "(b)Angular",
                "position": 3800
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x14.png",
                "caption": "(c)Kinetic Energy",
                "position": 3805
            }
        ]
    },
    {
        "header": "IXThe MetaSim Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/extracted/6391426/fig/infra_other.png",
                "caption": "Figure 15:Comparison between theMetaSimand the other simulation environments. Left: Other simulator and benchmark, using self-defined data format, simulator-associated assets, simulator-dependent task definition, and scripts. Right: TheMetaSim, decoupling all components to be agnostic to specific simulators or benchmark environments.",
                "position": 3824
            },
            {
                "img": "https://arxiv.org/html/2504.18904/extracted/6391426/fig/infra_metasim.png",
                "caption": "",
                "position": 3827
            }
        ]
    },
    {
        "header": "XAsset Conversion",
        "images": []
    },
    {
        "header": "XITask and Data Migration",
        "images": []
    },
    {
        "header": "XIITask Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x15.png",
                "caption": "Figure 16:Illustration of the two-phase generation protocol. A user prompt guides the LLM to propose an overall task and item list. The system then refines object positions and merges them into a final initial state.",
                "position": 4634
            }
        ]
    },
    {
        "header": "XIIITeleoperation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x16.png",
                "caption": "Figure 17:Sequential demonstration of smartphone-based control for stack cube and close box tasks.",
                "position": 4643
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x17.png",
                "caption": "Figure 18:Visualization of the smartphone’s local coordinate system, world-frame orientation, and app functionality: six buttons control translation, and two switches toggle orientation control and gripper state.",
                "position": 4670
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x18.png",
                "caption": "Figure 19:The smartphone app enables 6-DoF control using orientation sensing and multi-touch buttons for translation commands, while the simulated robot’s movements are visualized in real-time on the workstation.",
                "position": 4675
            }
        ]
    },
    {
        "header": "XIVReal2Sim Toolset for Asset and Task Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x19.png",
                "caption": "Figure 20:Visualization of our real2sim pipeline for robotic grasping.",
                "position": 4804
            }
        ]
    },
    {
        "header": "XVDomain Randomization",
        "images": []
    },
    {
        "header": "XVINavigation and Locomotoin Tasks",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/x20.png",
                "caption": "Figure 21:Navigation gallery. We deploy the Unitree Go2 robot within Matterport 3D environments. The robot is tasked with navigating the environment based on provided instructions.",
                "position": 4867
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x21.png",
                "caption": "Figure 22:Learning curves of RL algorithms on HumanoidBench task migratation: We also run PPO in the Isaac Sim handler in RoboVerse, but it is not visible in the plot since it only achieves very low returns.",
                "position": 4895
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x22.png",
                "caption": "Figure 23:Demonstration of TD-MPC2 policys trained in the RoboVerse MuJoCo simulator on the Walk and Stand tasks migrated from the HumanoidBench benchmark",
                "position": 4898
            }
        ]
    },
    {
        "header": "XVIIRoboVerse Benchmark Set up Details",
        "images": []
    },
    {
        "header": "XVIIIPolicy Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.18904/extracted/6391426/fig/sim2real_RL.jpg",
                "caption": "Figure 24:Visualization of Sim-to-Sim-to-Real Experiments.",
                "position": 5042
            },
            {
                "img": "https://arxiv.org/html/2504.18904/x23.png",
                "caption": "Figure 25:Visualization of ground truth and predicted frames by models conditioned on cartesian position (plus orientation) and joint position.",
                "position": 5078
            }
        ]
    },
    {
        "header": "XIXWorld Model Details",
        "images": []
    }
]