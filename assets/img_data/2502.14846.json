[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14846/x1.png",
                "caption": "Figure 1:Given a novel task (e.g., answering questions about nutrition facts), our code-guided generation system can produce targeted synthetic data to enhance the performance of VLMs on that specific task.",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14846/x2.png",
                "caption": "Figure 2:The overview of ourCode GuidedSynthetic data generation system (CoSyn), which has 20 generation pipelines based on 11 render tools. Given a user query, e.g., “book cover,” CoSyn selects the appropriate pipelines and starts with generating diverse topics conditioned on personas, then synthesizes detailed data for code generation. The code renders the image and is also fed as context for an LLM to construct instruction-tuning data.",
                "position": 151
            }
        ]
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4CoSyn System",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14846/x3.png",
                "caption": "Figure 3:Our CoSyn-400K dataset consists of 9 categories of text-rich images with 2.7M instruction-tuning data. More qualitative examples, along with question-answer annotations, are available in Figure12-18in AppendixC.",
                "position": 208
            }
        ]
    },
    {
        "header": "5Experimental Setup",
        "images": []
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14846/x4.png",
                "caption": "Figure 4:Ablation on training data selection.Aux, Syn, and Eval stand for auxiliary, synthetic, and evaluation datasets, respectively. We report the average score on eight benchmarks. The detailed performance breakdown on each benchmark is in Table7.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x5.png",
                "caption": "Figure 5:Zero shot performance on NutritionQA.The x-axis denotes the number of training examples used for the instruction-tuning stage. The models on the upper left side demonstrate better data efficiency.",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x6.png",
                "caption": "Figure 6:Ablation of using Chain-of-Thought reasoning.Short Answer represents prompting model to output the answer as short as possible.+++CoT stands for providing Chain-of-Thought reasoning before giving the final answer. Results on all datasets are in Table6.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x7.png",
                "caption": "Table 2:Results on human and machine-generated questions of ChartQA.The pie charts above display the percentage distribution of two question types in training and testing.ΔΔ\\Deltaroman_Δ(↓↓\\downarrow↓lower is better) denotes the performance gap between human and machine questions.",
                "position": 553
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x8.png",
                "caption": "Figure 7:The overview of enabling VLMs to point through synthetic data.(a) We synthesize pointing data by prompting an LLM to generate pointing questions and edit the code to draw the answer points explicitly. (b) We demonstrate that the VLM trained on synthetic pointing data can be generalized to real agentic tasks.",
                "position": 609
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitation",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14846/x9.png",
                "caption": "Figure 9:Scaling the Size of Synthetic Data.We evaluate thezero-shotperformance on ChartQA of models fine-tuned on increasing numbers of synthetic images.",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x10.png",
                "caption": "Figure 10:Examples from our newly collectedNutritionQAdataset.",
                "position": 2196
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x11.png",
                "caption": "Figure 11:Examples from our newly collectedDocPointQAdataset.",
                "position": 2199
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x12.png",
                "caption": "Figure 12:Randomly selected examples from our syntheticchartdata.",
                "position": 2202
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x13.png",
                "caption": "Figure 13:Randomly selected examples from our syntheticdocumentdata.",
                "position": 2206
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x14.png",
                "caption": "Figure 14:Randomly selected examples from our synthetictabledata.",
                "position": 2209
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x15.png",
                "caption": "Figure 15:Randomly selected examples from our syntheticmathdata.",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x16.png",
                "caption": "Figure 16:Randomly selected examples from our syntheticdiagramdata.",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x17.png",
                "caption": "Figure 17:Randomly selected examples from our syntheticvector graphicdata.",
                "position": 2219
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x18.png",
                "caption": "Figure 18:Randomly selected examples from our syntheticsheet music,circuitsandchemical structures.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2502.14846/x19.png",
                "caption": "Figure 19:Randomly selected examples from our syntheticpointingdata.",
                "position": 2225
            }
        ]
    },
    {
        "header": "Appendix CQualitative Examples",
        "images": []
    }
]