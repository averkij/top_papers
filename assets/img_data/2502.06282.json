[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06282/x1.png",
                "caption": "",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06282/x2.png",
                "caption": "Figure 1:Comparison of different speculative decoding methods.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2502.06282/x3.png",
                "caption": "Figure 2:Speedup ratio of Vicuna, LLaMA2-chat, and LLaMA3-instruct models inference latency on the MT-bench for non-greedy (Temperature=1) settings. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs on an A100-40G GPU. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.",
                "position": 118
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Jakiro",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06282/x4.png",
                "caption": "Figure 3:Comparison of different building methods of draft tree.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2502.06282/x5.png",
                "caption": "Figure 4:The building process of tree attention mask mechanism.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2502.06282/x6.png",
                "caption": "Figure 5:Efficient integration of contrastive mechanism.",
                "position": 337
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06282/x7.png",
                "caption": "Figure 6:Average acceptance length of Vicuna, LLaMA2-chat, and LLaMA3-instruct models inference on the MT-bench for non-greedy (Temperature=1) settings. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs on an A100-40G GPU. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2502.06282/x8.png",
                "caption": "Figure 7:Speedup ratios of Vicuna 7B inference on all tasks under different devices. Where the suffix “T0” represents greedy mode sampling, while suffix “T1” represents non-greedy sampling. The above reproduction results are based on the open-source code from the original paper and are averaged over four inference runs. In this paper, we only compare with speculative sampling based methods that do not need to finetune the backbone models, ensuring the output text distribution remains constant.",
                "position": 2011
            }
        ]
    },
    {
        "header": "Appendix ASupplementary experiments",
        "images": []
    }
]