[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/teaser.png",
                "caption": "Figure 1:OST-Benchis designed from the perspective of an embodied agent dynamically exploring static indoor environments, with a focus ononlineandspatio-temporalunderstanding. Compared to the conventional offline setting (top right), which answers questions based on a fixed-length video of the scene, the bottom section illustrates our online setting: for the same question, the agent’s answers evolve as it explores the scene, changing from blue (t1) to red (t2) to green (t3), reflecting its continuously updated understanding.",
                "position": 102
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3OST-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/benchmark_samples.png",
                "caption": "Figure 2:OST-Bench categorizes questions into three main categories. Each main category includes several subtypes; in total, the benchmark comprises 15 fine-grained question subtypes.",
                "position": 287
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/turn_scores.png",
                "caption": "Figure 3:Model performance over exploration time.The right side shows a general decline in answer accuracy for all models; the left side illustrates the accuracy trends across three main categories for InternVL-2.5-38B and GPT-4.1.",
                "position": 903
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/error_analysis.png",
                "caption": "Figure 4:Distribution of three error types across the three task categories in OST-Bench.",
                "position": 926
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/error_analysis.png",
                "caption": "Figure 4:Distribution of three error types across the three task categories in OST-Bench.",
                "position": 929
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/shortcut.png",
                "caption": "Figure 5:An example of Spatio-temporal Reasoning Shortcut, the green text indicates correct reasoning by the model, while the red text highlights wrong reasoning.",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/cross_view.png",
                "caption": "Figure 6:Single- vs. multi-step spatial connection settings. Target objects and spatial clues are highlighted.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/cross_view.png",
                "caption": "Figure 6:Single- vs. multi-step spatial connection settings. Target objects and spatial clues are highlighted.",
                "position": 963
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/cross_view_results.png",
                "caption": "Figure 7:Model performance across four task settings: keyframe- vs. sequence-based context, and single- vs. multi-step spatial connection.",
                "position": 968
            }
        ]
    },
    {
        "header": "5Limitations and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/route_generation.png",
                "caption": "Figure 8:Illustration of the route generation process.The radial arrows represent multiple different viewing angles at a position, and the red edges denote connections generated by the MST algorithm. The right part shows the captured images for each viewing angle of two adjacent nodes. The agent can only move along the edges of the tree, and adjacent frames are required to have a certain amount of overlap.",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/template.png",
                "caption": "Figure 9:Rule-based generation templates for all subtypes in OST-Bench.Placeholders to be filled with specific content are marked in red, and question focal points are highlighted in blue. \"JUD.\"/ \"CNT.\" / \"TEMP.\" / \"EST.\" are abbreviations for \"judgement\",\"counting\",\"temporal-localization\", and \"estimation\"; \"Q\" and \"O\" denote \"Question\" and \"Options\"",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/sample_distribution.png",
                "caption": "Figure 10:Distribution of sample counts across different subtypes in OST-Bench.",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/sample_distribution.png",
                "caption": "Figure 10:Distribution of sample counts across different subtypes in OST-Bench.",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/turn_distribution.png",
                "caption": "Figure 11:Word cloud (top) and dialogue length distribution (bottom) of OST-Bench.",
                "position": 1758
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/prompt_ex.png",
                "caption": "Figure 12:Model input content, including the system prompt and inputs for each round. Text placeholders to be filled are highlighted in red, while the green <image> token represent image placeholders to be filled.",
                "position": 1790
            }
        ]
    },
    {
        "header": "Appendix CExperiment Analysis Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/error_examples.png",
                "caption": "Figure 13:Illustrative Examples of the Three Error Types: Prompt Analysis Error, Perception Error, and Reasoning Error.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/shortcut_append.png",
                "caption": "Figure 14:More examples of Spatio-temporal Reasoning Shortcuts.Green text marks correct reasoning; red indicates errors. For clarity, only key video frames relevant to each question are shown, with temporal references replaced by t1, t2, and t3.",
                "position": 1821
            }
        ]
    },
    {
        "header": "Appendix DInference Time of the Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/latency.png",
                "caption": "Figure 15:The trend of the model’s inference time per question as the duration of exploration increases.",
                "position": 1867
            }
        ]
    },
    {
        "header": "Appendix ESocial Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/sample_example1.png",
                "caption": "Figure 16:Example 1 of OST-Bench data samples.Each row represents the newly added observations in each round, with images input from left to right within each round. The example shows the question-answer pairs from the first six rounds.",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2507.07984/extracted/6612751/figures/sample_example2.png",
                "caption": "Figure 17:Example 2 of OST-Bench data samples.Each row represents the newly added observations in each round, with images input from left to right within each round. The example shows the question-answer pairs from the first six rounds.",
                "position": 1904
            }
        ]
    },
    {
        "header": "Appendix FLicense and Acess",
        "images": []
    }
]