[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04606/x1.png",
                "caption": "Figure 1:The rate-distortion curve illustrates how visual distortion decreases as the number of transmitted bits increases. With just a small number of bits representing high-level semantic features, we can already achieve relatively low visual distortion. Building on this information-theoretic insight, LanDiff combines the strengths of both paradigms: LLMs efficiently generate compact semantic features in the first stage, followed by diffusion models that add perceptual details in the second stage, before final decoding to pixels via VAE. Data fromHo etÂ al. (2020), illustration is conceptual.",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x2.png",
                "caption": "Figure 2:The architecture of LanDiff. Given text inputs, we first extract text embeddings and employ an LLM to generate semantic tokens in the first stage. Subsequently, we utilize a diffusion model to synthesize perceptual features conditioned on these semantic tokens, followed by a VAE decoder that transforms these features into the final video frames.",
                "position": 243
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04606/x3.png",
                "caption": "Figure 3:Proposed architecture of the video semantic tokenizer. We use query tokens to compress the semantic sequence length. Furthermore, we group the frames into groups (3 frames in a group in this figure). In a group, the first frame is the IFrame and the rest frames are PFrames. We use different query token numbers for them. The attention mask design is shown in the right.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x4.png",
                "caption": "Figure 4:Proposed diffusion model structure. We use a ControlNet-style control module to guide the model to generate perceptual feature based on semantic features.",
                "position": 374
            }
        ]
    },
    {
        "header": "3Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04606/x5.png",
                "caption": "Figure 5:Qualitative comparison of long video generation results between LanDiff and other state-of-the-art models (FreeNoise, StreamingT2V and OpenSora-V1.2).",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x6.png",
                "caption": "Figure 6:Comparison of qualitative results for text-to-video generation.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x7.png",
                "caption": "Figure 7:Visualization results of video reconstruction using video tokenizer.",
                "position": 1175
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x8.png",
                "caption": "Figure 8:Training loss comparison of the video tokenizer. The plot illustrates the reconstruction loss trajectories for IFrame and PFrame components over training iterations. Despite the different token allocation strategies (330 tokens for IFrame vs. 74 tokens for PFrame), both frame types achieve comparable reconstruction quality.",
                "position": 1178
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.04606/x9.png",
                "caption": "Figure 9:Radar chart visualization of performance comparison across different dimensions on VBench.The plot compares LanDiff against five competitive baselines: Sora, Hailuo, HunyuanVideo, Kling, and CogVideoX-5B.",
                "position": 2906
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x10.png",
                "caption": "Figure 10:Examples of text to videos generation of LanDiff and CogVideoX-5B.",
                "position": 2926
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x11.png",
                "caption": "Figure 11:Examples of text to videos generation of LanDiff and CogVideoX-5B.",
                "position": 2929
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x12.png",
                "caption": "Figure 12:Examples of text to videos generation of LanDiff and CogVideoX-5B.",
                "position": 2932
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x13.png",
                "caption": "Figure 13:Examples of text to videos generation of LanDiff and CogVideoX-5B.",
                "position": 2935
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x14.png",
                "caption": "Figure 14:Examples of text to videos generation of LanDiff and CogVideoX-5B.",
                "position": 2938
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x15.png",
                "caption": "Figure 15:Examples of text to videos generation of LanDiff and CogVideoX-5B.",
                "position": 2941
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x16.png",
                "caption": "Figure 16:Examples of text to long video generation.",
                "position": 2944
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x17.png",
                "caption": "Figure 17:Visualization results of video reconstruction using video\ntokenizer.",
                "position": 2947
            },
            {
                "img": "https://arxiv.org/html/2503.04606/x18.png",
                "caption": "Figure 18:Visualization results of video reconstruction using video\ntokenizer.",
                "position": 2951
            }
        ]
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    }
]