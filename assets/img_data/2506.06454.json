[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3DeepEDM for Time Series Forecasting",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06454/x1.png",
                "caption": "Figure 1:(a)Takens‚Äô theorem in action. The state space of an unknown nonlinear dynamical system is reconstructed using time-delayed embeddings from observed time series measurements (noise free). (b)Overview of DeepEDM. Time-delayed embeddings are constructed to model the system‚Äôs underlying state space. These embeddings are then mapped into a learned latent space that is robust to measurement noise. Forecasting is performed via kernel regression followed by a learned decoder, where soft nearest neighbors for regression are defined in the latent space. This model, resembling the key idea of EDM, is fully differentially and thus can be learned from end-to-end.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2506.06454/x2.png",
                "caption": "Figure 2:DeepEDM blockcan be stacked, with each subsequent block iteratively refines the prediction of the previous one.",
                "position": 365
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06454/x3.png",
                "caption": "Figure 3:Results with synthetic data from Lorenz systems. We plot MSE under varying prediction lengths on non-Chaotic (top) and chaotic (bottom) Lorenz. DeepEDM significantly outperforms baselines in both chaotic and non-chaotic regimes.",
                "position": 387
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.06454/x4.png",
                "caption": "Figure 4:Impact of lookback length on forecast accuracy. The prediction horizon is fixed atH=96ùêª96H=96italic_H = 96, while the input sequence lengthT‚àà{192,288,336,512,720}ùëá192288336512720T\\in\\{192,288,336,512,720\\}italic_T ‚àà { 192 , 288 , 336 , 512 , 720 }is varied to assess its effect on forecasting performance. Increasing the lookback window generally improves accuracy up to a certain point; however, excessively long lookbacks can introduce irrelevant information or noise, ultimately degrading performance.",
                "position": 13681
            },
            {
                "img": "https://arxiv.org/html/2506.06454/extracted/6520184/figures/lorenz/attractor/lorenz_attractor_new_resized.png",
                "caption": "Figure 5:Visualization of the synthetic datasets: Non-chaotic Lorenz (top row), chaotic Lorenz (middle row), and chaotic R√∂ssler (bottom row) systems. As the noise level increases (from left to right), forecasting future states becomes progressively more challenging. Pink dots indicate the new attractor under the current regime, while the light blue denotes the original attractor for reference.",
                "position": 14973
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]