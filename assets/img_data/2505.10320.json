[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10320/x1.png",
                "caption": "Figure 1:Comparison ofPairwise-J1with state-of-the-art LLM-as-a-Judge models on five reward modeling benchmarks at 8B and 70B scale.Pairwise-J1outperforms all baselines despite being trained only on synthetic preference pairs.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2505.10320/x1.png",
                "caption": "",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2505.10320/x2.png",
                "caption": "",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2505.10320/extracted/6442142/assets/J1-Example.png",
                "caption": "Figure 2:Illustration of the thinking patterns ofPairwise-J1andPointwise-J1models during RL training: it learns to outline evaluation criteria, generate reference answers, re-evaluate for correctness, and compare between responses. WhilePairwise-J1outputs a final verdict, indicating the better response,Pointwise-J1generates a real-valued score, where a higher score indicates a better response.",
                "position": 139
            }
        ]
    },
    {
        "header": "2J1: Thinking-LLM-as-a-Judge via Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10320/extracted/6442142/assets/J1-Method.png",
                "caption": "Figure 3:Reinforcement Learning recipes for trainingPairwise-J1andPointwise-J1models. We generate synthetic preference pairs for both verifiable and non-verifiable tasks to create position-agnostic training batches. Rewards based on verdict correctness, consistency, and score alignment jointly optimize evaluation thoughts and verdicts using online RL (GRPO).Pointwise-J1is trainedonlyvia distant supervision from pairwise labels.",
                "position": 166
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": []
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10320/x3.png",
                "caption": "Figure 4:Test-time scaling ofPairwise-J1andPointwise-J1on the PPE Correctness benchmark. We show greedy decoding results in dotted lines for comparison. As we sample moreN𝑁Nitalic_N, (i) position-consistent accuracy increases and (ii) tie rate decreases, for both pairwise and pointwise models, at both 8B and 70B scales.",
                "position": 835
            },
            {
                "img": "https://arxiv.org/html/2505.10320/x4.png",
                "caption": "Figure 5:Distribution of Absolute Scores andΔ⁢S⁢c⁢o⁢r⁢e⁢(C⁢h⁢o⁢s⁢e⁢n−R⁢e⁢j⁢e⁢c⁢t⁢e⁢d)Δ𝑆𝑐𝑜𝑟𝑒𝐶ℎ𝑜𝑠𝑒𝑛𝑅𝑒𝑗𝑒𝑐𝑡𝑒𝑑\\Delta Score\\leavevmode\\nobreak\\ (Chosen-Rejected)roman_Δ italic_S italic_c italic_o italic_r italic_e ( italic_C italic_h italic_o italic_s italic_e italic_n - italic_R italic_e italic_j italic_e italic_c italic_t italic_e italic_d )generated by the 8BPairwise-J1(w/ Scores) andPointwise-J1models on the PPE Correctness benchmark.Pairwise-J1exhibits sparser score distribution and larger score differences between Chosen and Rejected (ground-truth) responses.",
                "position": 846
            },
            {
                "img": "https://arxiv.org/html/2505.10320/x4.png",
                "caption": "",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2505.10320/x5.png",
                "caption": "",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2505.10320/x6.png",
                "caption": "Figure 6:Reward and average generation length during training for differentJ1-Llama-8Bmodels.Pointwise-J1is trained viadistant supervisionderived from pairwise preference data.",
                "position": 961
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Templates",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]