[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04335/x1.png",
                "caption": "",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/palette.png",
                "caption": "Figure 1:GazeGen. (1)User’s View: Overview of the user’s view, setting the context for gaze estimation (input: user’s eye images) and visual editing (inputs: user’s view and predicted gaze point). (2)Real-Time Gaze Estimation: The DFT Gaze Agent (281KB storage) predicts the user’s gaze point (green) aligned with the ground-truth gaze (red). (3)Gaze-Driven Visual Content Generation/Detection: Predicted gaze is used for editing () objects, detecting () objects, or creating animations () based on the user’s focus (). GazeGen sets a new standard for gaze-driven visual content generation, enhancing user experience and positioning users as visual creators.",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/mag.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/clapper.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/eye.png",
                "caption": "",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2411.04335/x2.png",
                "caption": "Figure 2:Extended applications of gaze-driven interaction with GazeGen.\n(1)Real-Time Gaze Estimation: Continuous tracking of eye movements for precise gaze estimation.\n(2)Gaze-Driven Detection: Detecting and identifying objects based on where the user is looking.\n(3)Gaze-Driven Image Editing: Dynamic editing tasks such asAddition(adding objects based on the user’s gaze),Deletion/Replacement(removing or replacing objects based on the user’s gaze),Reposition(move objects by first gazing at the initial position, then the new position), andMaterial Transfer(change an object’s style or texture by first gazing at a reference object, then applying the style to the target object).\n(4)Gaze-Driven Video Generation: Creating and manipulating video content driven by the user’s gaze.",
                "position": 90
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3GazeGen",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04335/x3.png",
                "caption": "Figure 3:Gaze-driven visual content generation. This diagram shows the process starting from the user’s eye, where the gaze estimation agent determines the gaze point. The gaze point is used to get the editing region, which can be toggled to use either a box or a mask. The T2I (Text-to-Image) and T2V (Text-to-Video) modules then generate visual content based on the selected editing region. The On/Off switches indicate whether the box or mask is used for gaze-driven editing.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2411.04335/x4.png",
                "caption": "Figure 4:Self-supervised distillation for a compact model. Using ConvNeXt V2-A(Woo et al.2023)as the teacher network, we create a downsized student network. The first stage of the student model inherits weights from the teacher, while stages 2 to 4 reduce the channel dimensions to one-fourth. Distinct decoders are used to reconstruct both input images and the teacher’s intermediate features. The student processes masked inputs, allowing it to emulate the teacher’s deep understanding of visual data and align with how the teacher perceives and interprets these images. For simplicity, the diagram only illustrates the reconstruction of the teacher’s features to emulate knowledge.",
                "position": 246
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04335/x5.png",
                "caption": "Figure 6:Qualitative results for gaze-driven image editing. The tasks include:Addition(first row): Adding objects like a lantern, basket, or photo.Deletion/Replacement(second row): Replacing objects with items like a curtain, aquarium, or galaxy.Reposition(third row): Moving objects such as a wall decoration to the upper left corner, books to the lower left corner, or a phone upward.Material Transfer(last row): Changing an object’s style, such as polished wood to the fridge, woven wicker to the washing machine, or polished metal to the chopping board. All edits are based on the user’s gaze.",
                "position": 433
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/latency_plot.png",
                "caption": "Figure 8:Model latency comparison on Raspberry Pi 4. The figure compares the latency of two gaze estimation models: ConvNeXt V2-A (Teacher) and DFT Gaze (Student). ConvNeXt V2-A shows a latency of 928.84 ms, while DFT Gaze reduces latency to 426.66 ms, demonstrating its efficiency for real-time applications on edge devices.",
                "position": 738
            }
        ]
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/loc1_script2_seq8_rec1_eyetracking_00929.png",
                "caption": "(a) Lighting conditions",
                "position": 784
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/loc1_script2_seq8_rec1_eyetracking_00929.png",
                "caption": "(a) Lighting conditions",
                "position": 787
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/loc3_script2_seq3_rec2_eyetracking_00018.png",
                "caption": "(b) Closed eyes",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2411.04335/extracted/5982601/figs/limitations/failure_gaze_replacement.png",
                "caption": "Figure 10:Visual content generation limitation. This figure illustrates the limitations of gaze-driven visual content generation. The replaced objects do not accurately reflect the original object’s 3D angle or orientation, causing visual inconsistencies.",
                "position": 801
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]