[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Model architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01743/x1.png",
                "caption": "Figure 1:A overview of the Multimodal architecture forPhi-4-Multimodal",
                "position": 221
            }
        ]
    },
    {
        "header": "3Data and training details",
        "images": []
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.01743/extracted/6234065/figures/phio_vision_demo_case.png",
                "caption": "Figure 2:One demo case to show the vision-language understanding and reasoning capability ofPhi-4-Multimodal.",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2503.01743/extracted/6234065/figures/phio_speech_demo.png",
                "caption": "Figure 3:An example to showcase the understanding capabilities forPhi-4-Multimodal, including audio understanding, summarization, ASR, and AST.",
                "position": 2182
            }
        ]
    },
    {
        "header": "5Safety",
        "images": []
    },
    {
        "header": "6Weaknesses and limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt for GPT-4 as a Judge on speech benchmarks",
        "images": []
    },
    {
        "header": "Appendix BAuthors (alphabetical)",
        "images": []
    }
]