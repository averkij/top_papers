[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/Teaser_final_grid.jpg",
                "caption": "Figure 1:Reconstruction of seen images from fMRI using“Brain-IT”.(a)Image reconstructions using the full NSD dataset (40 hours per subject).(b)Efficient Transfer-learning to new subjects with very little data: Meaningful reconstructions are obtained with only 15 minutes of fMRI recordings. (Results on Subject 1)",
                "position": 167
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/Overview9.png",
                "caption": "Figure 2:Overview ofBrain-IT.(a) Brain Interaction Transformer (BIT)transforms fMRI signals intoSemanticandVGGfeatures using the Voxel-to-Cluster (V2C) mapping. Two branches are\napplied: (i) the Low-Level branch reconstructs a coarse image from VGG features, used to initialize\nthe (ii) Semantic branch, which uses semantic features to guide the diffusion model.(b) Voxel-to-Cluster mapping (V2C):each voxel from every subject is mapped to a functional cluster shared across subjects.(c) Low-level branch:VGG-predicted features are inverted using Deep Image Prior (DIP)\nto reconstruct a coarse image layout.",
                "position": 292
            }
        ]
    },
    {
        "header": "3Brain-IT Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/40h_corrected.jpg",
                "caption": "Figure 3:Comparing methods on 40-hour data (for Subject 1).Brain-ITis compared to 3 leading\nmethods, yielding reconstructions that better preserve both semantic content and low-level visual properties.Brain-ITbetter reconstructs the correct objects with relevant structural details (e.g., orientation, color), providing reconstructions more faithful to the seen images.\nSee many more examples in AppendixFig.S2",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/Architecture10.png",
                "caption": "Figure 4:Architecture of the Brain-Interaction-Transformer (BIT).(Sec.4)",
                "position": 373
            }
        ]
    },
    {
        "header": "4Brain Interaction Transformer (BIT)",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/1h_final.jpg",
                "caption": "Figure 5:Reconstruction with limited amount of subject-specific data (1 hour).We compareBrain-ITagainst 2 leading approaches which provide also 1-hour reconstructions (MindEye2 & MindTuner) for Subj1.Brain-ITdemonstrates greater fidelity to the seen image. See many more examples in App.Fig.S6",
                "position": 613
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/low_level_short.jpg",
                "caption": "Figure 6:Two-Branch Contribution Reconstructions.Sample results of the semantic branch, low-level branch, and both combined.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/low_level_short.jpg",
                "caption": "Figure 6:Two-Branch Contribution Reconstructions.Sample results of the semantic branch, low-level branch, and both combined.",
                "position": 688
            }
        ]
    },
    {
        "header": "6Ablations and Analysis",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAblations and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/refinement_fig.png",
                "caption": "Figure S1:Refinement stage using a pre-trained SDXL diffusion model.The SDXL model enhances global realism, smooths textures, and corrects fine details such as the cat eyes, resulting in more natural and visually coherent reconstructions.",
                "position": 1790
            }
        ]
    },
    {
        "header": "Appendix BAdditional Quantitative Results",
        "images": []
    },
    {
        "header": "Appendix CAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_40h_1.jpg",
                "caption": "Figure S2:Qualitative reconstruction results for subject 1.Comparison to prominent previous methods on a large amount of images demonstrates the visual faithfulness of Brain-IT, alongside semantic accuracy.",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_40h_2.jpg",
                "caption": "Figure S3:Qualitative reconstruction results for subject 1- Part B",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_40h_3.jpg",
                "caption": "Figure S4:Qualitative reconstruction results for subject 1- Part C",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_40h_4.jpg",
                "caption": "Figure S5:Qualitative reconstruction results for subject 1- Part D",
                "position": 2011
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_1h_1.jpg",
                "caption": "Figure S6:Qualitative reconstruction results for transfer learning to subject 1 with limited data.Brain-IT reconstructions using 1 hours and even 30 minutes are comparable to reconstructions of previous methods generated using 40 hours of data.",
                "position": 2020
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_1h_2.jpg",
                "caption": "Figure S7:Qualitative reconstruction results for transfer learning with limited data- Part B",
                "position": 2023
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_1h_3.jpg",
                "caption": "Figure S8:Qualitative reconstruction results for transfer learning with limited data- Part C",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_1h_4.jpg",
                "caption": "Figure S9:Qualitative reconstruction results for transfer learning with limited data- Part D",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_allsubs_1.jpg",
                "caption": "Figure S10:Image reconstruction results for subjects 1, 2, 5 and 7.Due to its use of shared functional clusters and network modules, Brain-IT successfully leverages cross-subject information to produce effective reconstructions for all four evaluated subjects",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_allsubs_2.jpg",
                "caption": "Figure S11:Image reconstruction results for subjects 1, 2, 5 and 7- Part B",
                "position": 2041
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/app_dual_branch.jpg",
                "caption": "Figure S12:Reconstructions via the two generation branches of Brain-IT. The combination of both the low-level and the semantic branch ensures both semantically and visually accurate reconstructions. Importantly, the low-level branch often leads to accurate modifications of the semantic content, while the semantic branch often carries important visual information.",
                "position": 2050
            },
            {
                "img": "https://arxiv.org/html/2510.25976/figures/failures.png",
                "caption": "Figure S13:Failure cases. For certain fMRI samples, Brain-IT will predict either the semantic content or the visual layout of the image relatively accurately, but will fail to predict the other accurately, leading to incorrect reconstructions.",
                "position": 2058
            }
        ]
    },
    {
        "header": "Appendix DAdditional Technical Details",
        "images": []
    }
]