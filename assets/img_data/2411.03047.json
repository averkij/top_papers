[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_teaser_1016_v6.png",
                "caption": "Figure 1.We propose a hierarchical framework to recover different levels of garment details by leveraging the garment shape and deformation priors from the GarVerseLOD dataset. Given a single clothed human image, our approach is capable of generating high-fidelity 3D standalone garment meshes that exhibit realistic deformation and are well-aligned with the input image. Original images courtesy of licensed photos and Stable Diffusion(Rombach etÂ al.,2022). The images with a gray background are synthesized, while the rest are licensed photos.",
                "position": 111
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_pipeline3.png",
                "caption": "Figure 2.The pipeline of our novel strategy for constructing a progressive garment dataset with levels of details. (a) Each case shows the reference image and the artist-crafted T-pose coarse garment inGarment Style Database. (b) A example of the reference image and the artist-crafted detail-pair inLocal Detail Database. (c) A example of the reference image and the artist-crafted deformation-pair inGarment Deformation Database. (d) To obtain an T-pose garment with geometric details, we first sample a shapeMCsubscriptğ‘€ğ¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTfrom the Garment Style Database and a â€œLocal Detail Pairâ€ (LCsubscriptğ¿ğ¶L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT,LFsubscriptğ¿ğ¹L_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) from the Local Detail Database. Then we transfer the geometric details depicted by (LCsubscriptğ¿ğ¶L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT,LFsubscriptğ¿ğ¹L_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) toMCsubscriptğ‘€ğ¶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTto obtainMLsubscriptğ‘€ğ¿M_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT. (e) The deformation depicted by a sampled â€œGarment Deformation Pairâ€ (DTsubscriptğ·ğ‘‡D_{T}italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT,DFsubscriptğ·ğ¹D_{F}italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) is transferred toMLsubscriptğ‘€ğ¿M_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTto obtain the fine garmentMDsubscriptğ‘€ğ·M_{D}italic_M start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT, which contains fine-grained geometric details and complex deformations (Fine Garment Dataset). Original images courtesy of licensed photos.",
                "position": 281
            }
        ]
    },
    {
        "header": "3.Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_gallery.png",
                "caption": "Figure 3.Left: Our novel strategy for generating extensive photorealistic paired images. We acquire rendered images of 3D garments with random camera views. These rendered images are processed through Canny-Conditional Stable Diffusion(Rombach etÂ al.,2022; Mou etÂ al.,2023; Zhang etÂ al.,2023a)to produce photorealistic images. Right: (a) The garment sampled from Fine Garment Dataset; (b) The synthesized image; (c) The pixel-aligned mask; (d) The normal map rendered using (a); (e) The garment mask rendered by (a); (f) The counterpart T-pose coarse garment of (a). In Sec.4, (b, f) is used to train the coarse garment estimator, while (b,c,d) is adopted to train the normal estimator. (d, e, a) is utilized to train the fine garment estimator and the geometry-aware boundary predictor. Synthesized images courtesy of Stable Diffusion.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_method2.png",
                "caption": "Figure 4.The pipeline of our proposed method. Given an RGB image, our method first estimates the T-pose garment shapeGâ¢(Î±)ğºğ›¼G({{\\alpha}})italic_G ( italic_Î± )(Eq.4) and computes its pose-related deformationMPâ¢(Î±,Î²,Î¸)subscriptğ‘€ğ‘ƒğ›¼ğ›½ğœƒM_{P}(\\alpha,\\beta,\\theta)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_Î± , italic_Î² , italic_Î¸ )with the help of the predicted SMPL body (Eq.7, Eq.10). Then a pixel-aligned network is used to reconstruct implicit fine garmentMIsubscriptğ‘€ğ¼M_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPTand the geometry-aware boundary estimator is adopted to predict the garment boundary. Finally, we registerMPâ¢(â‹…)subscriptğ‘€ğ‘ƒâ‹…M_{P}(\\cdot)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( â‹… )toMIsubscriptğ‘€ğ¼M_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPTto obtain the final meshMFsubscriptğ‘€ğ¹M_{F}italic_M start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, which has fine topology and open-boundaries. Images courtesy of Stable Diffusion.",
                "position": 374
            }
        ]
    },
    {
        "header": "4.Method",
        "images": []
    },
    {
        "header": "5.Experiments",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_result_gallery_1016_v6.png",
                "caption": "Figure 5.Result gallery of our method. Each image is followed by the reconstructed garment mesh. As illustrated, our method can effectively reconstruct garments with intricate deformations and fine-grained surface details. To support the modeling of folded structures, such as collars, we assembled a repository of diverse real-world collars that were crafted based on our topologically-consistent garments. A lightweight classification network was trained to select the collar that best matches the given image in terms of appearance(Zhu etÂ al.,2022). Original images courtesy of licensed photos and Stable Diffusion. The images with a gray background are synthesized, while the rest are licensed photos.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_compare_1016_v6.png",
                "caption": "Figure 6.Qualitative comparison between ours and the state of the arts. For each row, the input image is followed by the results generated by BCNet(Jiang etÂ al.,2020), ClothWild(Moon etÂ al.,2022), Deep Fashion3D(Zhu etÂ al.,2020), ReEF(Zhu etÂ al.,2022)and our method. Input images courtesy of Stable Diffusion.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_boundary_1016_v6.png",
                "caption": "Figure 7.Qualitative comparison between our method and the alternative strategy for predicting garment boundary from in-the-wild images. The input image (a) is followed by the boundaries generated by (b) ReEFâ€™s strategy and (c) our geometry-aware estimator. ReEF fails to accurately predict boundaries with complex poses and deformations, leading to discontinuous boundaries. Our geometry-aware boundary prediction outperforms ReEF in reconstructing complex garment boundaries that are well-aligned with the garment shape. Input images courtesy of Stable Diffusion.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_data_1016_v6.png",
                "caption": "Figure 8.Qualitative comparison on different data. The input image (a) is followed by the results generated by networks trained with (b) ReEFâ€™s data and (c) our GarVerseLOD. Input images courtesy of Stable Diffusion.",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_coarse_1016_v6.png",
                "caption": "Figure 9.Qualitative comparison between our method and the alternative strategy for obtaining coarse garment template. (a) the input image; (b) the template (black part) cropped from SMPL; (c) the registration result using (b); (d) the coarse garment estimated by our coarse garment estimator; and (e) the registration result using (d). Input images courtesy of Stable Diffusion.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_implicit_udf_1016_v6.png",
                "caption": "Figure 10.Qualitative comparison on different representation. The input image (a) is followed by the result generated by (b) UDF, (c) registering to (b), (d) occupancy field and (e) registering to (d). Input images courtesy of Stable Diffusion.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_limitation_1016_v6.png",
                "caption": "Figure 11.Failure cases. Our framework may struggle to reconstruct garments with complex topology, such as those multi-layered structures (a) or featuring slits (b). Images courtesy of licensed photos and Stable Diffusion.",
                "position": 766
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Results and Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_template_f322.png",
                "caption": "Figure 12.Predefined templates for each garment category, including (a) dress, (b) skirt, (c) top, (d) pant, and (e) coat.",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_deformation_craft.png",
                "caption": "Figure 13.Given a â€œCollected Imageâ€, we utilize PyMAF(Zhang etÂ al.,2021,2023b)to estimate SMPL body. Eight artists are then tasked with creating â€œT-pose Garmentâ€ shapes by deforming a predefined â€œTemplateâ€ to match the T-pose body predicted by PyMAF. Then the SMPLâ€™s Linear Blend Skinning (LBS) is extended to the T-pose garment to obtain the â€œPosed Garmentâ€. Finally, the artists are further instructed to refine the posed garment to get the â€œCrafted Garmentâ€ while ensuring that garment deformations closely match the collected images. â€œPosed Garmentâ€ represent the shape of clothing influenced by human pose, while â€œCrafted Garmentâ€ capture the state of garments affected by various complex factorsâ€”not only pose but also other environmental influences, such as garment-environment interactions and external forces like wind.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_0.jpg",
                "caption": "Figure 14.More Results on Loose-fitting Garments.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_1.jpg",
                "caption": "Figure 15.More Results on Loose-fitting Garments.",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_2.jpg",
                "caption": "Figure 16.More Results on Loose-fitting Garments.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_3.jpg",
                "caption": "Figure 17.More Results on Loose-fitting Garments.",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_0.png",
                "caption": "Figure 18.An illustration of ourGarment Style Database.",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_1.png",
                "caption": "Figure 19.An illustration of ourLocal Detail Database.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_2.png",
                "caption": "Figure 20.An illustration of ourGarment Deformation Database.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_3.png",
                "caption": "Figure 21.An illustration of ourFine Garment Dataset.",
                "position": 1718
            }
        ]
    },
    {
        "header": "Appendix BDetails of GarVerseLOD",
        "images": []
    }
]