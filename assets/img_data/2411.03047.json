[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_teaser_1016_v6.png",
                "caption": "Figure 1.We propose a hierarchical framework to recover different levels of garment details by leveraging the garment shape and deformation priors from the GarVerseLOD dataset. Given a single clothed human image, our approach is capable of generating high-fidelity 3D standalone garment meshes that exhibit realistic deformation and are well-aligned with the input image. Original images courtesy of licensed photos and Stable Diffusion(Rombach et al.,2022). The images with a gray background are synthesized, while the rest are licensed photos.",
                "position": 111
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_pipeline3.png",
                "caption": "Figure 2.The pipeline of our novel strategy for constructing a progressive garment dataset with levels of details. (a) Each case shows the reference image and the artist-crafted T-pose coarse garment inGarment Style Database. (b) A example of the reference image and the artist-crafted detail-pair inLocal Detail Database. (c) A example of the reference image and the artist-crafted deformation-pair inGarment Deformation Database. (d) To obtain an T-pose garment with geometric details, we first sample a shapeMCsubscript𝑀𝐶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTfrom the Garment Style Database and a “Local Detail Pair” (LCsubscript𝐿𝐶L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT,LFsubscript𝐿𝐹L_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) from the Local Detail Database. Then we transfer the geometric details depicted by (LCsubscript𝐿𝐶L_{C}italic_L start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT,LFsubscript𝐿𝐹L_{F}italic_L start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) toMCsubscript𝑀𝐶M_{C}italic_M start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPTto obtainMLsubscript𝑀𝐿M_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT. (e) The deformation depicted by a sampled “Garment Deformation Pair” (DTsubscript𝐷𝑇D_{T}italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT,DFsubscript𝐷𝐹D_{F}italic_D start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT) is transferred toMLsubscript𝑀𝐿M_{L}italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTto obtain the fine garmentMDsubscript𝑀𝐷M_{D}italic_M start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT, which contains fine-grained geometric details and complex deformations (Fine Garment Dataset). Original images courtesy of licensed photos.",
                "position": 281
            }
        ]
    },
    {
        "header": "3.Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_dataset_gallery.png",
                "caption": "Figure 3.Left: Our novel strategy for generating extensive photorealistic paired images. We acquire rendered images of 3D garments with random camera views. These rendered images are processed through Canny-Conditional Stable Diffusion(Rombach et al.,2022; Mou et al.,2023; Zhang et al.,2023a)to produce photorealistic images. Right: (a) The garment sampled from Fine Garment Dataset; (b) The synthesized image; (c) The pixel-aligned mask; (d) The normal map rendered using (a); (e) The garment mask rendered by (a); (f) The counterpart T-pose coarse garment of (a). In Sec.4, (b, f) is used to train the coarse garment estimator, while (b,c,d) is adopted to train the normal estimator. (d, e, a) is utilized to train the fine garment estimator and the geometry-aware boundary predictor. Synthesized images courtesy of Stable Diffusion.",
                "position": 314
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_method2.png",
                "caption": "Figure 4.The pipeline of our proposed method. Given an RGB image, our method first estimates the T-pose garment shapeG⁢(α)𝐺𝛼G({{\\alpha}})italic_G ( italic_α )(Eq.4) and computes its pose-related deformationMP⁢(α,β,θ)subscript𝑀𝑃𝛼𝛽𝜃M_{P}(\\alpha,\\beta,\\theta)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( italic_α , italic_β , italic_θ )with the help of the predicted SMPL body (Eq.7, Eq.10). Then a pixel-aligned network is used to reconstruct implicit fine garmentMIsubscript𝑀𝐼M_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPTand the geometry-aware boundary estimator is adopted to predict the garment boundary. Finally, we registerMP⁢(⋅)subscript𝑀𝑃⋅M_{P}(\\cdot)italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( ⋅ )toMIsubscript𝑀𝐼M_{I}italic_M start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPTto obtain the final meshMFsubscript𝑀𝐹M_{F}italic_M start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT, which has fine topology and open-boundaries. Images courtesy of Stable Diffusion.",
                "position": 374
            }
        ]
    },
    {
        "header": "4.Method",
        "images": []
    },
    {
        "header": "5.Experiments",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_result_gallery_1016_v6.png",
                "caption": "Figure 5.Result gallery of our method. Each image is followed by the reconstructed garment mesh. As illustrated, our method can effectively reconstruct garments with intricate deformations and fine-grained surface details. To support the modeling of folded structures, such as collars, we assembled a repository of diverse real-world collars that were crafted based on our topologically-consistent garments. A lightweight classification network was trained to select the collar that best matches the given image in terms of appearance(Zhu et al.,2022). Original images courtesy of licensed photos and Stable Diffusion. The images with a gray background are synthesized, while the rest are licensed photos.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_compare_1016_v6.png",
                "caption": "Figure 6.Qualitative comparison between ours and the state of the arts. For each row, the input image is followed by the results generated by BCNet(Jiang et al.,2020), ClothWild(Moon et al.,2022), Deep Fashion3D(Zhu et al.,2020), ReEF(Zhu et al.,2022)and our method. Input images courtesy of Stable Diffusion.",
                "position": 750
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_boundary_1016_v6.png",
                "caption": "Figure 7.Qualitative comparison between our method and the alternative strategy for predicting garment boundary from in-the-wild images. The input image (a) is followed by the boundaries generated by (b) ReEF’s strategy and (c) our geometry-aware estimator. ReEF fails to accurately predict boundaries with complex poses and deformations, leading to discontinuous boundaries. Our geometry-aware boundary prediction outperforms ReEF in reconstructing complex garment boundaries that are well-aligned with the garment shape. Input images courtesy of Stable Diffusion.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_data_1016_v6.png",
                "caption": "Figure 8.Qualitative comparison on different data. The input image (a) is followed by the results generated by networks trained with (b) ReEF’s data and (c) our GarVerseLOD. Input images courtesy of Stable Diffusion.",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_coarse_1016_v6.png",
                "caption": "Figure 9.Qualitative comparison between our method and the alternative strategy for obtaining coarse garment template. (a) the input image; (b) the template (black part) cropped from SMPL; (c) the registration result using (b); (d) the coarse garment estimated by our coarse garment estimator; and (e) the registration result using (d). Input images courtesy of Stable Diffusion.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_ablation_implicit_udf_1016_v6.png",
                "caption": "Figure 10.Qualitative comparison on different representation. The input image (a) is followed by the result generated by (b) UDF, (c) registering to (b), (d) occupancy field and (e) registering to (d). Input images courtesy of Stable Diffusion.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/images/fig_limitation_1016_v6.png",
                "caption": "Figure 11.Failure cases. Our framework may struggle to reconstruct garments with complex topology, such as those multi-layered structures (a) or featuring slits (b). Images courtesy of licensed photos and Stable Diffusion.",
                "position": 766
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Results and Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_template_f322.png",
                "caption": "Figure 12.Predefined templates for each garment category, including (a) dress, (b) skirt, (c) top, (d) pant, and (e) coat.",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/fig_deformation_craft.png",
                "caption": "Figure 13.Given a “Collected Image”, we utilize PyMAF(Zhang et al.,2021,2023b)to estimate SMPL body. Eight artists are then tasked with creating “T-pose Garment” shapes by deforming a predefined “Template” to match the T-pose body predicted by PyMAF. Then the SMPL’s Linear Blend Skinning (LBS) is extended to the T-pose garment to obtain the “Posed Garment”. Finally, the artists are further instructed to refine the posed garment to get the “Crafted Garment” while ensuring that garment deformations closely match the collected images. “Posed Garment” represent the shape of clothing influenced by human pose, while “Crafted Garment” capture the state of garments affected by various complex factors—not only pose but also other environmental influences, such as garment-environment interactions and external forces like wind.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_0.jpg",
                "caption": "Figure 14.More Results on Loose-fitting Garments.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_1.jpg",
                "caption": "Figure 15.More Results on Loose-fitting Garments.",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_2.jpg",
                "caption": "Figure 16.More Results on Loose-fitting Garments.",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/stitched_image_group_3.jpg",
                "caption": "Figure 17.More Results on Loose-fitting Garments.",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_0.png",
                "caption": "Figure 18.An illustration of ourGarment Style Database.",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_1.png",
                "caption": "Figure 19.An illustration of ourLocal Detail Database.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_2.png",
                "caption": "Figure 20.An illustration of ourGarment Deformation Database.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2411.03047/extracted/5979162/supp_images/LOD_3.png",
                "caption": "Figure 21.An illustration of ourFine Garment Dataset.",
                "position": 1718
            }
        ]
    },
    {
        "header": "Appendix BDetails of GarVerseLOD",
        "images": []
    }
]