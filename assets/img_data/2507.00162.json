[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00162/x1.png",
                "caption": "Figure 1:Results of Short and Longer Videos. The first row of each case shows short videos\ngenerated using short video diffusion models (81 frames for Wan-2.1[1]and 121 frames for LTX-Video[2]). Directly extending these models to longer videos, like those with 4√ó\\times√ó(324 frames and 484 frames), preserves temporal consistency but lacks\nfine spatial-temporal details. In contrast, our proposed FreeLong and FreeLong++ adapts short video diffusion models\nto create consistent long videos with high fidelity.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2507.00162/x2.png",
                "caption": "Figure 2:Ratio of short video SNR on high¬†(0.25œÄùúã\\piitalic_œÄ-1.0œÄùúã\\piitalic_œÄ)/low¬†(0.0œÄùúã\\piitalic_œÄ-0.25œÄùúã\\piitalic_œÄ) frequency to longer videos.Our findings reveal that when direct extend short video diffusion model to generate longer videos, the SNR of high-frequency components in the space-time frequency domain degrades significantly as video length increases.",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2507.00162/x3.png",
                "caption": "Figure 3:Attention Visualization.We visualize the attention by average across all layers and time steps from Wan2.1[1]. The attention maps for 81-frame videos exhibit a diagonal-like pattern, indicating a high correlation with adjacent frames, which helps preserve high-frequency details and motion patterns when generating new frames. In contrast, attention maps for longer videos are less structured, such as 648 frames¬†(8√ó\\times√ó), making the model struggle to identify and attend to the relevant information across distant frames. This lack of structure in the attention maps results in the distortion of high-frequency components of long videos, which results in the degradation of fine spatial-temporal details.",
                "position": 111
            },
            {
                "img": "https://arxiv.org/html/2507.00162/x4.png",
                "caption": "Figure 4:Fine-grained frequency analysis on longer video generation.(a) As video length increases, both the range and severity of frequency distortion grow substantially.\n(b) We define available frequency bands as those with a relative SNR above 0.9. As shown, the number of available bands drops significantly when the video length increases from 2√ó\\times√óto 4√ó\\times√ó, indicating that a fixed two-branch structure in FreeLong is insufficient for modeling motion dynamics in longer sequences.\n(c) High-frequency distortion correlates with attention window size: larger window sizes introduce more severe distortion in the high-frequency components.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00162/x5.png",
                "caption": "Figure 5:Overview of FreeLong.FreeLong facilitates consistent and high-fidelity video generation using SpectralBlend Attention. SpectralBlend effectively blends low-frequency global video features with high-frequency local video features through a two-step process: local-global attention decoupling and spectral blending. Local video features are obtained by masking temporal attention to concentrate on fixed-length adjacent frames, while global temporal attention encompasses all frames. During spectral blending, 3D FFT projects features into the frequency domain, where high-frequency local components and low-frequency global components are merged. The resulting blended feature, transformed back to the time domain via IFFT, is then utilized in the subsequent block for refined video generation.",
                "position": 239
            },
            {
                "img": "https://arxiv.org/html/2507.00162/x6.png",
                "caption": "Figure 6:Overview of FreeLong++.The FreeLong++ framework extends FreeLong by introducing Multi-band SpectralFusion Attention. Multi-scale temporal branches with varying window sizes capture motion dynamics at different frequency bands. Each branch is processed in the frequency domain and selectively fused via scale-specific filters, enhancing long-range consistency while preserving fine-grained motion.",
                "position": 351
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.00162/x7.png",
                "caption": "Figure 7:Qualitative comparison across models.All methods generate videos that are 4√ó the original length, based on the Wan2.1[1]model.",
                "position": 773
            },
            {
                "img": "https://arxiv.org/html/2507.00162/x8.png",
                "caption": "Figure 8:Results of Multi-Prompt Video Generation.Our method ensures coherent visual continuity and motion consistency across different video segments.",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2507.00162/x9.png",
                "caption": "Figure 9:Long Control Sequence.Long-range video generation under pose (left) and depth (right) guidance. FreeLong++ produces more temporally consistent and semantically faithful outputs than direct generation.",
                "position": 823
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]