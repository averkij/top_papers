[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04804/figures/assets/0_icon_with_bg.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04804/x1.png",
                "caption": "Figure 1:Performance comparison across five audio–video benchmarks.\nResults are obtained using Qwen2.5-Omni-7B with a 35% token retained ratio, comparing OmniSIFT against three baseline token compression methods and the full-token baseline.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2602.04804/x2.png",
                "caption": "Figure 2:Compression paradigm comparison for Omni-LLMs.\nToken compression for Omni-LLMs can be categorized into three paradigms:\n(a) modality-decoupled compression (left top), which applies audio and video compression independently;\n(b) modality-symmetric compression (right top), which treats the two modalities equally informative;\nand (c) modality-asymmetric compression (bottom, ours), which first prunes visual redundancy and then performs visually guided audio compression.",
                "position": 148
            },
            {
                "img": "https://arxiv.org/html/2602.04804/x3.png",
                "caption": "Figure 3:Architecture of OmniSIFT, a modality-asymmetric compression framework.\nThe framework operates in two stages.\nIn the first stage, STVP removes spatial and temporal redundancy in video tokens to obtain a compact set of visual anchors.\nIn the second stage, VGAS selects audio tokens conditioned on these visual anchors.\nThe resulting compressed multimodal sequence is then fed into the LLM backbone for downstream reasoning.",
                "position": 201
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04804/x4.png",
                "caption": "Figure 4:Ablation results for video and audio compression ratios, evaluated on the Qwen2.5-Omni-7B model using the WorldSense benchmark.Left: Varying the video compression ratioρv\\rho_{v}with audio compression ratioρa=0.5\\rho_{a}=0.5;Right: Varying the audio compression ratioρa\\rho_{a}with video compression ratioρv=0.8\\rho_{v}=0.8.",
                "position": 1555
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04804/x5.png",
                "caption": "Figure 5:Ablation results for OmniSIFT’s architecture.w/o Spatial Component: all visual tokens are selected using temporal saliency only.w/o Temporal Component: all visual tokens are selected based on spatial saliency only.Audio-Only Selector: audio tokens are selected solely based on intra-audio self-attention without any visual guidance.",
                "position": 1758
            },
            {
                "img": "https://arxiv.org/html/2602.04804/x6.png",
                "caption": "Figure 6:Visualization of token compression methods for Omni-LLMs.White blocks denote discarded video and audio tokens. The vertical amplitude of the waveform reflects the audio information density. As illustrated, OmniZip prunes critical visual features and audio cues, leading to an erroneous interpretation of the score change. In contrast, OmniSIFT preserves both the salient visual dynamics and the informative audio segments required for accurate event reasoning.",
                "position": 1766
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExpanded Benchmark Details",
        "images": []
    },
    {
        "header": "Appendix BExpanded Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CComputing Cost Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04804/x7.png",
                "caption": "Figure 10:Attention score distribution maps for layers 15 and 27 of the LLM decoder in the Qwen2.5-Omni-7B model.",
                "position": 2640
            },
            {
                "img": "https://arxiv.org/html/2602.04804/x8.png",
                "caption": "Figure 11:Comparison of peak GPU memory and end-to-end latency between OmniSIFT and full-token baseline using Qwen2.5-Omni-7B on WorldSense videos of varying durations.",
                "position": 2650
            },
            {
                "img": "https://arxiv.org/html/2602.04804/x9.png",
                "caption": "Figure 12:Results of extended ablation experiments on the architecture of OmniSIFT, conducted using the Qwen2.5-Omni-7B model at a 35% retention ratio.Visual Random Pruning: replacing the STVP module with random selection for video tokens;Audio Random Pruning: replacing the VGAS module with random selection for audio tokens;SSM Selector: utilizing a State Space Model as the selector for audio tokens.",
                "position": 2725
            }
        ]
    },
    {
        "header": "Appendix DMore Experimental Results",
        "images": []
    }
]