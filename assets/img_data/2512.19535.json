[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19535/x1.png",
                "caption": "Figure 1:Token insertionvs.cross-attention (CA)vs.CASA.In a controlled experiment, we compare various cross-attention (vanilla,+gating,+registers) models with the insertion of visual tokens directly in the language model‚Äôs text tokens input. We find that while CA methods achieve comparable performance on general visual-question-answering tasks, they significantly lag behind on fine-grained benchmarkssuch as chart or document understanding.Interestingly, we find this trend to be closely mirrored in the literature when comparing current state-of-the-art cross-attention- and insertion-based models, seeAppendixC.1.Based on a detailed analysis, we proposeCross-Attention viaSelf-Attention (CASA), which largely closes the gap to token insertion.",
                "position": 113
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x1.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x2.png",
                "caption": "",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19535/x3.png",
                "caption": "Figure 2:CASA (Cross-Attention via Self-Attention)injects visual information through cross-attention layers in which the current text tokens attend tothe concatenationof themselves and image tokens, in a causal manner. This provides a natural gating mechanism that outperforms standard cross-attention-based VLM architectures.\nDuring training, CASA leverages recent blockwise attention mechanism to remain efficient. At inference, CASA benefits from the same practical advantages as cross-attention and can handle long interleaved image-text sequences without affecting the KV cache and memory usage of the underlying LLM.",
                "position": 221
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x3.png",
                "caption": "",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x4.png",
                "caption": "",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x5.png",
                "caption": "Figure 3:CASAvs.SA.(Left) Instead of insertingimage tokensdirectly into the stream oftext tokensat the desired position (dashed lines,bottom), CASA (top) uses block-wise attention to augment groups of tokens with visual information. Compared to token insertion, the cost of self-attention (SA) thus reduces fromùí™‚Äã(T+N)\\mathcal{O}(T+N)toùí™‚Äã(max‚Å°(Twindow+Nwindow,N))\\mathcal{O}(\\max(T_{\\text{window}}+N_{\\text{window}},N)), forTTtext andNNimage tokens.\nTo illustrate this, we show which tokens a query at a given text position attends to, showing masked tokens at lower opacity. (Right) Attention matrices for CASA and SA layers: while in SA layers text only attends to text, CASA layers apply block-wise attention between text and images.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x5.png",
                "caption": "",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x6.png",
                "caption": "",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x7.png",
                "caption": "",
                "position": 392
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19535/x8.png",
                "caption": "Figure 4:Live captioning.\nWe display captions generated by ourCASAQwen2.5-VL‚äï\\text{CASA}^{\\scriptstyle\\oplus}_{\\text{\\scriptsize Qwen2.5-VL}}-LiveCC model.\nEach text span is annotated with the corresponding frame‚Äôs timestamp (top) and the model‚Äôs delay as[timestamp||delay].\nAs shown inFigure5, CASA‚Äôs outputs are generated in real-time and with little memory building up over time; for further qualitative examples, including insertion-based VLMs, seeApp.D.",
                "position": 1221
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x9.png",
                "caption": "Figure 5:Real-time memory usage(reported in MB, log scale) of CASA compared to token insertion techniques with different levels of token compression (via a Q-Former), measured on a single H100 GPU.\nCASA memory usage builds up more slowly than the token-insertion-based techniques, even with token compression.",
                "position": 1254
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x10.png",
                "caption": "Figure 6:Walltime in streaming inference.We record the wall-time as a function of the number of frames inserted in a streaming captioning scenario, for CASA and token-compression techniques (Q-Former with different numbers of query tokens).\nWhile token compression mitigates the computational cost of token insertion for short videos, CASA maintains low latency inference for longer times. Note that for better readability, we only plot a subset of markers, although the plotted measurements occur at every frame.",
                "position": 1361
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Appendix AImplicit Gating in CASA",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19535/x11.png",
                "caption": "Figure 7:Average attention scores in CASA layers (log scale).Complementing our ablation study inTable8, we visualize the average CASA attention pattern found in a CASA layer for a trainedCASAQwen2.5-VL‚äï\\text{CASA}^{\\scriptstyle\\oplus}_{\\text{\\scriptsize Qwen2.5-VL}}model. The attention scores are averaged across heads, layers, and for multiple input images of resolution 896√ó\\times896. In detail, we show the attention of query tokens to1)the vision start and end tokens of the Qwen2.5-VL model,2)image tokens, grouped and averaged by 16,3)the average across other text tokens, and4)the query itself. The attention of a query to itself is by far the strongest, consistent with the detrimental effect we found when ablating this component inTable8.\nThe average attention of the query to other text tokens is also higher than to any image tokens, further highlighting the implicit balancing of image and text tokens contributions naturally occurring in the attention operation of CASA layers.",
                "position": 1387
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x12.png",
                "caption": "Figure 8:Insertionvs.Cross-Attentionvs.CASA.Similar to our ablation experiments (right, same asFig.1), we find (left) that the current state-of-the-art cross-attention-based models (mPlug-Owl3) lag behind insertion-based models (here, Qwen2.5 3B), particularly on tasks that require a high degree of visual detail. This holds true even when significantly scaling up the mPlug-Owl3 model (2B‚Üí\\rightarrow8B).",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x12.png",
                "caption": "",
                "position": 1399
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x13.png",
                "caption": "",
                "position": 1403
            }
        ]
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19535/x14.png",
                "caption": "Figure 9:Performance across trainingforCASAQ2.5-VL‚äï\\text{CASA}^{\\scriptstyle\\oplus}_{\\text{\\scriptsize Q2.5-VL}}for the different benchmark groupsHRE, OCR, andVQA.",
                "position": 1893
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x15.png",
                "caption": "Figure 10:Live captioning examples ‚Äì Q-64. Similar toFigure4we show excerpts of video generated by a Q-Former (with 64 tokens per image) insertion based model. For full video samples, see theproject page.",
                "position": 1926
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x16.png",
                "caption": "",
                "position": 1965
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x17.png",
                "caption": "",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x18.png",
                "caption": "Figure 11:Live captioning examples ‚Äì CASA. Similar toFigure4we show excerpts of video generated by a CASA model. For full video samples, see theproject page.",
                "position": 2005
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x19.png",
                "caption": "",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2512.19535/x20.png",
                "caption": "",
                "position": 2077
            }
        ]
    },
    {
        "header": "Appendix DLive Video Captioning",
        "images": []
    }
]