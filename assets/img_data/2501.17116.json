[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17116/x1.png",
                "caption": "Figure 1:Directly casting to FP4 results in significantly higher training loss, whereas our proposed FP4 method achieves accuracy comparable to the BF16 baseline. These results are based on experiments with a 400M LLaMA2 model.",
                "position": 94
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17116/x2.png",
                "caption": "Figure 2:The structure of the proposed FP4 training scheme during the forward pass of a linear layer. A high-precision tensor, such as BF16, is quantized into the FP4 format using look-up table quantization. During the GeMM computation, both weight and activation tensors are quantized into FP4 to leverage the FP4 tensor cores. Two scaling factors are then applied to the final result to ensure computational correctness.",
                "position": 123
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17116/x3.png",
                "caption": "Figure 3:Visualization of theDifferentiableGradientEstimator (DGE). (a) Comparison of three quantization methods: hard quantization, differentiable quantization, andSTEquantization, demonstrated on a single quantization step. (b) The full quantization curve for E2M1 quantization within its dynamic range[‚àí6.0,6.0]6.06.0[-6.0,6.0][ - 6.0 , 6.0 ]. (c) The derivative curves for the three methods, highlighting that hard quantization has a gradient off‚Ä≤‚Å¢(x)‚â°0superscriptùëì‚Ä≤ùë•0f^{\\prime}(x)\\equiv 0italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) ‚â° 0, whileSTEassumes a constant gradient off‚Ä≤‚Å¢(x)‚â°1superscriptùëì‚Ä≤ùë•1f^{\\prime}(x)\\equiv 1italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) ‚â° 1.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x4.png",
                "caption": "Figure 4:Visualization of the outlier clamping method, based on the first transformer layer‚Äôs output of the LLaMA 1.3B model after 30,000 training iterations. Up: Quantization performed without outlier clamping, leading to severe loss of information. Down: Quantization after applying outlier clamping, effectively preserving tensor structure.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x5.png",
                "caption": "Figure 5:Training curves for BF16 models and FP4 models under different model sizes. (a) Training curves for 1.3B LLaMA model. (b) Training curves for 7B LLaMA model. (c) Training curves for 13B LLaMA model.",
                "position": 464
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17116/x6.png",
                "caption": "Figure 6:Ablation studies. (a) Training curves under different precision frameworks. (b) The effect of proposedDifferentiableGradientEstimator (DGE). (c) The effect of proposedOutlierClamping andCompensation method (OCC). Note that directly casting activation into 4-bit leads to divergence, and the loss value turn into NaN (Not a Number). (d) Training curves under different quantization granularities of FP4.",
                "position": 505
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Limitation",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation of FP4 quantizaiton",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17116/x7.png",
                "caption": "Figure 7:Visualization of all representable numbers in different FP4 formats.",
                "position": 1450
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Proof for Differentiable Quantization Estimator",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.17116/x8.png",
                "caption": "Figure 8:Visualization of theweighttensors in the dense projection layers of the self-attention module.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x9.png",
                "caption": "Figure 9:Visualization of theweighttensors in the up-projection linear layers of the MLP module.",
                "position": 1722
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x10.png",
                "caption": "Figure 10:Visualization of theweighttensors in the down-projection linear layers of the MLP module.",
                "position": 1726
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x11.png",
                "caption": "Figure 11:Visualization of theactivationtensors from the core attention output.",
                "position": 1731
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x12.png",
                "caption": "Figure 12:Visualization of theactivationtensors from the post-attention layer normalization output.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2501.17116/x13.png",
                "caption": "Figure 13:Visualization of theactivationtensors from the MLP down-projection layer output.",
                "position": 1739
            },
            {
                "img": "https://arxiv.org/html/2501.17116/extracted/6153012/figures/layer_0_mlp.activation_func_activation_heatmap.png",
                "caption": "Figure 14:Heatmap visualization of the activation function (GeLU) output from the first transformer layer on the 30,000 training iteration of the LLaMA 1.3B model. The vertical light lines in the heatmap correspond to specific channel dimensions in the activation tensor, highlighting the channel-wise distribution of outliers.",
                "position": 1755
            }
        ]
    },
    {
        "header": "Appendix CAnalyzing Quantization Difficulty Through Tensor Distribution",
        "images": []
    }
]