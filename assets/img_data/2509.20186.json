[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20186/x1.png",
                "caption": "Figure 1:(a) The average few-shot accuracy scores on the GSM8k and MATH datasets with respect to total training tokens.\nBoth models are pre-trained from scratch with88B parameters.\nOne model employed a vanilla next-token prediction objective, while the other utilized thinking-augmented pre-training.\n(b) Illustration of a thinking augmented data sample.\nThe token in red, “890”, is both correct and valuable, yet it is difficult to learn directly.\nThe complete text is provided in Appendix Table8.",
                "position": 80
            },
            {
                "img": "https://arxiv.org/html/2509.20186/x1.png",
                "caption": "",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2509.20186/x2.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Thinking Augmented Pre-training",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20186/x3.png",
                "caption": "Figure 2:Pre-training loss curves and aggregated scores on55tasks with respect to total training tokens (8B model).\nBoth models are trained from scratch on100100B tokens.\nThe loss values are not directly comparable due to differences in data distributions,\nbut we demonstrate how thinking augmentation reduces data noise and enhances learnability.\nThe final scores of both models are detailed in Appendix Table1.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2509.20186/x4.png",
                "caption": "Figure 3:Task scores with respect to total training tokens (8B model).\nThe tokens in raw documents are constrained to1010B via random sampling.\nThe final scores are detailed in Appendix Table7.",
                "position": 493
            }
        ]
    },
    {
        "header": "4Analysis of Thinking Patterns",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20186/x5.png",
                "caption": "Figure 4:The average number of thinking tokens, categorized by domain, target audience, and reasoning intensity.\nThe figure lists only the top-1010domains that exhibit the longest thinking trajectories.",
                "position": 803
            }
        ]
    },
    {
        "header": "5Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.20186/x6.png",
                "caption": "Figure 5:Task scores with respect to the mid-training token budget.\nThe “0B” data point corresponds to direct SFT without thinking augmented mid-training.",
                "position": 956
            },
            {
                "img": "https://arxiv.org/html/2509.20186/x7.png",
                "caption": "Figure 6:Task scores with respect to SFT epochs.\nThe “w/o mid-training” variant is initialized from the LLaMA-3.2-3B-Base checkpoint.",
                "position": 977
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]