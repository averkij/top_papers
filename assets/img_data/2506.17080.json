[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower_gray.png",
                "caption": "Figure 1:Translation and general capabilities performance of state-of-the-art translation-specific (circles) and general-purpose LLMs (logos) of varying sizes (from 9B to 72B). For their size,Tower+models outperform or match open-weight models on both axes, andTower+72B is competitive to state-of-the-art closed models. We omit 2B models for better visualization; we show detailed results in Table2.4. Gray lines connect eachTower+model to their respective “instruction-tuned” counterpart.",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/meta.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/qwen.png",
                "caption": "",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/gemini.png",
                "caption": "",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/openai.png",
                "caption": "",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/anthropic.png",
                "caption": "",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2506.17080/x1.png",
                "caption": "Figure 2:Process for creating and curating data for our final dataset for SFT.",
                "position": 214
            }
        ]
    },
    {
        "header": "2TowerPost-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17080/x2.png",
                "caption": "Figure 3:Process for creating and curating data for our final dataset for PO.",
                "position": 250
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower.png",
                "caption": "Table 1:Results of several translation-specific (††\\dagger†) and general-purpose open-weight and closed API models across M-ArenaHard, IFEval,WMT24++, and IF-MT (English→→\\rightarrow→Chinese). We consider two evaluation dimensions on IF-MT: instruction-following (IF) and raw MT quality (MT). ForWMT24++we reportxComet-xxland we split the language pairs into three categories: (1) seven high-resource languages, (2) the 15 languages fromTower-v2(our submission to WMT24), and (3) all languages supported by our new models. This categorization enables a more equitable comparison with other systems, which, in all cases, support at least the seven high-resource languages. We boldface the best overall system, and the best open-weight system if the former is proprietary.",
                "position": 310
            }
        ]
    },
    {
        "header": "3Experimental Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower.png",
                "caption": "Figure 5:Performance comparison across progressive training stages usingGemma 2 9Bas the foundation model. We represent a total of 4 training setups: (1) SFT (base): Supervised Fine-Tuning directly on the base model; (2) CPT+SFT: Continued Pre-Training followed by SFT; (3) CPT+SFT+WPO: Addition of Preference Optimization; and (4) CPT+SFT+WPO+GRPO: Integration of GRPO with verifiable rewards. The left plot omits (3) because performance is identical to (4) on M-ArenaHard; GRPO brings gains on IFEval.",
                "position": 586
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower.png",
                "caption": "",
                "position": 589
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower.png",
                "caption": "",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/qwen_gray.png",
                "caption": "Figure 6:Comparison ofGemma 2 9BandQwen 2.5 7B/14Bon translation quality (WMT24++) and general chat capabilities (M-ArenaHard). Arrows show the performance drop when including all languages vs high-resource ones.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/gemini_gray.png",
                "caption": "",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower_pink_gray.png",
                "caption": "",
                "position": 665
            },
            {
                "img": "https://arxiv.org/html/2506.17080/extracted/6558376/latex/logos/tower_pink.png",
                "caption": "",
                "position": 665
            }
        ]
    },
    {
        "header": "4Conclusions",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ACovered languages",
        "images": []
    },
    {
        "header": "Appendix BTranslation Templates used in CPT",
        "images": []
    },
    {
        "header": "Appendix CPrompts used to clean SFT data",
        "images": []
    },
    {
        "header": "Appendix DGeneration of Verifiable Translation Instructions for Training",
        "images": []
    },
    {
        "header": "Appendix EProblems Found in Tülu RLVR Datasets",
        "images": []
    },
    {
        "header": "Appendix FIF-MT: Prompts, examples, and additional results.",
        "images": []
    }
]