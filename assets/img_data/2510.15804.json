[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3The Truth Co-occurrence Hypothesis",
        "images": []
    },
    {
        "header": "4Analysis on a Toy Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15804/x1.png",
                "caption": "Figure 1:Visualization of the value matrix for the one-layer model at different training steps. We see that theex→ug​(x)e_{x}\\to u_{g(x)}block is learned first, along with thept→u¯p_{t}\\to\\bar{u}block. Later theex→−exe_{x}\\to-e_{x}andey→−uye_{y}\\to-u_{y}blocks, and finally theey→eg−1​(y)e_{y}\\to e_{g^{-1}(y)}block.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x2.png",
                "caption": "",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x3.png",
                "caption": "",
                "position": 324
            },
            {
                "img": "https://arxiv.org/html/2510.15804/plots/ov_matrices/annotated.png",
                "caption": "Figure 2:Visualization of representations on true (top) and false (bottom) sequences. The plots show representations before (left) and after (center) layer-norm, as well as predicted probabilities (right).",
                "position": 388
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15804/x4.png",
                "caption": "(a)Truth classification results, second subjectxx",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x4.png",
                "caption": "(a)Truth classification results, second subjectxx",
                "position": 504
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x5.png",
                "caption": "(b)P​[g​(x′)]P[g(x^{\\prime})]onfalsesequences, for whichy′≠g​(x′)y^{\\prime}\\neq g(x^{\\prime}))",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x6.png",
                "caption": "(a)PCA of layer-1 representations on the tokenx′x^{\\prime}",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x6.png",
                "caption": "(a)PCA of layer-1 representations on the tokenx′x^{\\prime}",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x7.png",
                "caption": "(b)Attention pattern (1-layer model)",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x8.png",
                "caption": "(c)PCA of input embeddingsXXversusYY",
                "position": 559
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x9.png",
                "caption": "(a)Truth classification results",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x9.png",
                "caption": "(a)Truth classification results",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x10.png",
                "caption": "(b)P​[correct attribute]P[\\text{correct attribute}]onfalsesequences, for which the observed attribute isnotthe correct one.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2510.15804/plots/Llama-context-dependency-layer-11_v2.png",
                "caption": "(a)",
                "position": 624
            }
        ]
    },
    {
        "header": "6Discussion and Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADetailed MAVEN-FACT Analysis",
        "images": []
    },
    {
        "header": "Appendix BEntropy incentive",
        "images": []
    },
    {
        "header": "Appendix CExperimental Setup",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15804/x11.png",
                "caption": "(a)",
                "position": 1460
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x11.png",
                "caption": "(a)",
                "position": 1463
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x12.png",
                "caption": "(b)",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x13.png",
                "caption": "(a)",
                "position": 1475
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x13.png",
                "caption": "(a)",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x14.png",
                "caption": "(b)",
                "position": 1483
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x15.png",
                "caption": "Figure 9:attention patterns of a 3-layer model.",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x16.png",
                "caption": "(a)",
                "position": 1496
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x16.png",
                "caption": "(a)",
                "position": 1499
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x17.png",
                "caption": "(b)",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x18.png",
                "caption": "(a)E​V​O​E⊤EVOE^{\\top}with frozen dense embeddings",
                "position": 1520
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x18.png",
                "caption": "(a)E​V​O​E⊤EVOE^{\\top}with frozen dense embeddings",
                "position": 1523
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x19.png",
                "caption": "(b)E​V​O​E⊤EVOE^{\\top}with trainable dense embeddings",
                "position": 1528
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x20.png",
                "caption": "(a)E​V​O​E⊤EVOE^{\\top}with frozen dense embeddings (sub-sampled and sorted)",
                "position": 1535
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x20.png",
                "caption": "(a)E​V​O​E⊤EVOE^{\\top}with frozen dense embeddings (sub-sampled and sorted)",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x21.png",
                "caption": "(b)E​V​O​E⊤EVOE^{\\top}with trainable dense embeddings (sub-sampled and sorted)",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2510.15804/plots/learned_input_emb_small.jpg",
                "caption": "Figure 13:Visualization of learned embeddings and value matrix for a model as in Section4with learned embeddings, initialized to one-hot.",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2510.15804/plots/ov_matrix_learned_onehot.png",
                "caption": "",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x22.png",
                "caption": "Figure 14:Structure of the value matrixWWwhen training without positional embeddings.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2510.15804/x23.png",
                "caption": "Figure 15:Structure of the value matrixWWwhen training with euclidean normalization.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2510.15804/plots/2d_rmsnorm_side_by_side_biglegendxlabel.png",
                "caption": "Figure 16:Illustration for a LN-induced linear separability.",
                "position": 2359
            }
        ]
    },
    {
        "header": "Appendix ETheoretical analysis",
        "images": []
    }
]