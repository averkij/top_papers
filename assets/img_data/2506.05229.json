[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05229/x1.png",
                "caption": "Figure 1:Diagonal Batching enables the Recurrent Memory Transformers (ARMT) to process 128k tokens sequences 3.3x faster than the LLama-3.2-1B model, with 167.1x memory savings.These results were obtained using an A100 GPU, and the segment size for the ARMT was set to 1,024 tokens.",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x2.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05229/x3.png",
                "caption": "Figure 2:Unlocking Parallelism in Recurrent Memory Transformers (RMT) with Diagonal Batching.Left:Standard RMT splits long sequences and processes segments sequentially. Each layer updates a memory state (mem0subscriptmem0\\mathrm{mem}_{0}roman_mem start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT,mem1subscriptmem1\\mathrm{mem}_{1}roman_mem start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,……\\dots…) and the final-layer memory state is fed as input to the next segment; red arrows highlight the recurrent dependencies that force strictly sequential execution.Center:Parallel RMT generalizes a family of models withlayer-level memory. Each layer maintains its own memory state and passes it horizontally to the same layer in the next segment. This eliminates inter-layer memory flow, yet still requires processing segments in order within each layer, thereby creating layer-wise recurrence.Right:Diagonal Batching rearranges the 2D grid of layers (rows) and segments (columns) into independent \"diagonals\" (same colored blocks). This allows all operations on one diagonal (up to N_Layers) to execute concurrently on the GPU, thus eliminating the sequential bottleneck while preserving all layer-level recurrence.",
                "position": 229
            }
        ]
    },
    {
        "header": "3Diagonal Batching method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05229/x4.png",
                "caption": "(a)Baseline compute scheme.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x4.png",
                "caption": "(a)Baseline compute scheme.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x5.png",
                "caption": "(b)Diagonal Batching: grouped compute scheme.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05229/x6.png",
                "caption": "(a)a100",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x6.png",
                "caption": "(a)a100",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x7.png",
                "caption": "(b)h100",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x8.png",
                "caption": "(a)a100",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x8.png",
                "caption": "(a)a100",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x9.png",
                "caption": "(b)h100",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x10.png",
                "caption": "Figure 6:Ideal batch-size scaling vs grouped batching on Nvidia A100 for Llama models, time per segment in batch (group)",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2506.05229/x11.png",
                "caption": "",
                "position": 727
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AEvaluating Models with Diagonal Batching",
        "images": []
    },
    {
        "header": "Appendix BAdditional measurements",
        "images": []
    }
]