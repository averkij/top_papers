[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10534/x1.png",
                "caption": "Figure 1:An example of IMO-level geometry problems.(a) The configuration in IMO 2018 Problem 6 appears simple, but it is difficult to prove. Its solution relies on sophisticated constructions, as illustrated in (b).",
                "position": 85
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10534/x2.png",
                "caption": "Figure 2:An overview of InternGeometry and Complexity-Boosting Reinforcement Learning (CBRL).(a) InternGeometry performs natural-language reasoning (Think), outputs a structured action in a domain-specific language (Action), and receives execution results (Feedback) in each turn. A dynamic memory moduleùîö\\mathfrak{W}compresses the multi-turn interaction history to preserve essential actions and outcomes. (b) CBRL optimizes the agent policy by generating synthetic training data with controllable difficulty, assigning binary rewards to effective steps and successful outcomes, and optimizing policy through iterative reinforcement learning.",
                "position": 107
            }
        ]
    },
    {
        "header": "3Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.10534/x3.png",
                "caption": "(a)",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2512.10534/x3.png",
                "caption": "(a)",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2512.10534/x4.png",
                "caption": "(b)",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2512.10534/x5.png",
                "caption": "(a)",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2512.10534/x5.png",
                "caption": "(a)",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2512.10534/x6.png",
                "caption": "(b)",
                "position": 851
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AThe Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix BImprovements in InternGeometry-DDAR",
        "images": []
    },
    {
        "header": "Appendix CInteraction between InternGeometry and InternGeometry-DDAR",
        "images": []
    },
    {
        "header": "Appendix DDetails of Complexity Boosting Reinforcement Learning (CBRL)",
        "images": []
    },
    {
        "header": "Appendix EDetails of Training Token",
        "images": []
    },
    {
        "header": "Appendix FFailed Cases",
        "images": []
    },
    {
        "header": "Appendix GDiscussion of Inference Cost",
        "images": []
    }
]