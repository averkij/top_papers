[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2MarDini: An Efficient and Asymmetric Video Diffusion Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20280/x1.png",
                "caption": "Figure 1:MarDini Training Pipeline Overview.A latent representation is computed for unmasked frames that serve as a conditional signal to a generative process. On the first hand, we have a planning model that autoregressively encodes global conditioning signals from a low-resolution version of the unmasked latent inputs. On the other hand, the planning signals are fed to the diffusion-based generation model through cross-attention layers. A high-resolution version of the input conditions is also ingested by the diffusion model, enabling generation with a coherent temporal structure and a direct mechanism to attend to fine-grained details of the unmasked frames. MarDini is trained end-to-end via masked frame-level diffusion loss.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x2.png",
                "caption": "Figure 2:MarDini Design Details.MarDini employs a transformer architecture for both the planning and generation models, incorporating a DiT-style block for the generation model and a Llama-style block for the planning model. We setL1‚â´L2much-greater-thansubscriptùêø1subscriptùêø2L_{1}\\gg L_{2}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚â´ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, whereL1subscriptùêø1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTandL2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTrefer to the number of layers in the planning and generation model respectively.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x3.png",
                "caption": "Figure 3:Identity Attention Design Details in DM.In this setup,[REF]tokens only attend to themselves, while[NOISE]tokens attend to all other tokens across different frames.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x4.png",
                "caption": "",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x5.png",
                "caption": "Figure 4:MarDini Training Manual.We list the mask ratios, frame rate (FPS), number of frames, and the size of training data for each training stage. This training manual applies to both small (MarDini-S) and large (MarDini-L) models. Note that the total training data refers to the amount of data observed by the model for gradient updates, rather than the vanilla size of the training dataset. Our final model checkpoints are highlighted ingray.",
                "position": 431
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20280/x6.png",
                "caption": "Table 2:Effectiveness of MAR and DM design.The reported results are FVD on VIDIM-Bench. All experiments are evaluated at a resolution of[256√ó256]delimited-[]256256[256\\times 256][ 256 √ó 256 ]using DDIM scheduler with 25 steps.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x6.png",
                "caption": "Table 2:Effectiveness of MAR and DM design.The reported results are FVD on VIDIM-Bench. All experiments are evaluated at a resolution of[256√ó256]delimited-[]256256[256\\times 256][ 256 √ó 256 ]using DDIM scheduler with 25 steps.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x6.png",
                "caption": "Figure 5:MarDini‚Äôs generations with and without the planning model.Here we show video frames generated when conditioning on the middle frame. Without MAR‚Äôs planning signal, DM generates degraded motion, such as pixel distortions (highlighted inred, left) or incorrect motions (highlighted inblue, right).",
                "position": 614
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x7.png",
                "caption": "",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x8.png",
                "caption": "(a)Video interpolation results with varying inference steps.",
                "position": 621
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x8.png",
                "caption": "(a)Video interpolation results with varying inference steps.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x9.png",
                "caption": "(b)Relationship between video interpolation and image-to-video generation.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x10.png",
                "caption": "(c)Training loss of\nMarDini w and w/o Identity Attention.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/bag_1.png",
                "caption": "Figure 7:Visualization of novel view synthesis conditioned on the two views.Starting with two views of an object, MarDini generates the intermediate ‚Äúframes‚Äù, effectively creating novel views. Notably, MarDini is trained without any 3D data but still manages to capture spatial information through video. The data used for this task is sourced from publicly available research datasets(Downs et¬†al.,2022).",
                "position": 1172
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/bag_2.png",
                "caption": "",
                "position": 1185
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/bag_gen1.png",
                "caption": "",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/bag_gen2.png",
                "caption": "",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/bag_gen3.png",
                "caption": "",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/bag_gen4.png",
                "caption": "",
                "position": 1201
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/turtle_1.png",
                "caption": "",
                "position": 1207
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/turtle_2.png",
                "caption": "",
                "position": 1211
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/turtle_gen1.png",
                "caption": "",
                "position": 1215
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/turtle_gen2.png",
                "caption": "",
                "position": 1219
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/turtle_gen3.png",
                "caption": "",
                "position": 1223
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/turtle_gen4.png",
                "caption": "",
                "position": 1227
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/flower_1.png",
                "caption": "Figure 8:Visualization of Video Expansion.The model is conditioned on a sequence of 16 consecutive frames to predict the subsequent 12 frames. The video data used for visualization is sourced from publicly available research dataset(Nan et¬†al.,2024).",
                "position": 1235
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/flower_2.png",
                "caption": "",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/flower_gen1.png",
                "caption": "",
                "position": 1252
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/flower_gen2.png",
                "caption": "",
                "position": 1256
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/flower_gen3.png",
                "caption": "",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/flower_gen4.png",
                "caption": "",
                "position": 1264
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/grass_1.png",
                "caption": "",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/grass_2.png",
                "caption": "",
                "position": 1274
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/grass_gen1.png",
                "caption": "",
                "position": 1278
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/grass_gen2.png",
                "caption": "",
                "position": 1282
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/grass_gen3.png",
                "caption": "",
                "position": 1286
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_extension/grass_gen4.png",
                "caption": "",
                "position": 1290
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Limitations and Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Reconstruction metrics in Video Interpolation.",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20280/x11.png",
                "caption": "(a)Original Video",
                "position": 2670
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x11.png",
                "caption": "(a)Original Video",
                "position": 2673
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x12.png",
                "caption": "(b)LDMVFI",
                "position": 2678
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x13.png",
                "caption": "(c)Ours",
                "position": 2683
            }
        ]
    },
    {
        "header": "8MarDini Training Strategies",
        "images": []
    },
    {
        "header": "9Visualization of Video Interpolation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_1.png",
                "caption": "Figure 10:Visualization of video interpolation methods conditioned on the first and last frames.We present the generated frames from FILM(Reda et¬†al.,2022), LDMVFI(Danier et¬†al.,2024), VIDIM(Jain et¬†al.,2024), and MarDini. The comparison results for these methods are sourced fromJain et¬†al.(2024). We have included additional samples in the supplementary materials.",
                "position": 2704
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_2.png",
                "caption": "",
                "position": 2747
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_film.png",
                "caption": "",
                "position": 2751
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_ldmvfi.png",
                "caption": "",
                "position": 2755
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_vidim.png",
                "caption": "",
                "position": 2759
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_mardini.png",
                "caption": "",
                "position": 2763
            },
            {
                "img": "https://arxiv.org/html/2410.20280/extracted/5949981/images/video_interpolation/interp_gt.png",
                "caption": "",
                "position": 2767
            },
            {
                "img": "https://arxiv.org/html/2410.20280/x14.png",
                "caption": "Figure 11:Visualization of MarDini using hierarchical auto-regressive generation.Starting with an initial 4 frames, MarDini auto-regressively generates a complete 128-frame video, demonstrating its capability to extend beyond the training window size (32 frames here).",
                "position": 2789
            }
        ]
    },
    {
        "header": "10Benchmarks",
        "images": []
    }
]