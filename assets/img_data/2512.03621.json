[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03621/x1.png",
                "caption": "",
                "position": 70
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Cross-trajectory Data Curation and ParaDriving Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03621/x2.png",
                "caption": "Figure 2:(a–b) Comparison of training and inference camera-transformation patterns. (c) Our training and inference data strategy. (Trans.: Transformation; Traj.: Trajectory)",
                "position": 145
            }
        ]
    },
    {
        "header": "4ReCamDriving",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03621/x3.png",
                "caption": "Figure 3:Overview of our framework. We adopt a two-stage training scheme for precise and structurally consistent novel-trajectory video generation. InStage 1,ReCamDrivingtrains DiT blocks conditioned on the source trajectory video and relative camera (cam.) pose. When switching toStage 2, the original DiT parameters are frozen, and additional attention modules are introduced to integrate 3DGS renderings for fine-grained view control and structural guidance. Shared modules between stages are connected with blue dashed lines.",
                "position": 156
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03621/x4.png",
                "caption": "Figure 4:Qualitative comparison results on NuScenes[7].",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x5.png",
                "caption": "Figure 5:Qualitative comparison results on WOD[40]. Our method and Difix3D+[53]use novel-trajectory renderings from DriveStudio[9]for camera control and restoration, respectively. Note that the officially released FreeVS[46]model is trained and tested on a cropped resolution that excludes sky regions to reduce computation and avoid LiDAR-sparse areas.",
                "position": 546
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x6.png",
                "caption": "Figure 6:Qualitative ablation of camera conditions on WOD[40].",
                "position": 633
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x7.png",
                "caption": "Figure 7:Qualitative ablation of training paradigms on WOD[40].",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x8.png",
                "caption": "Figure 8:Qualitative ablation on our two-stage training strategy.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x9.png",
                "caption": "Figure 9:Ablation on different source inputs at inference.",
                "position": 790
            }
        ]
    },
    {
        "header": "6Conclusion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Details of Data Construction",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Dataset",
        "images": []
    },
    {
        "header": "Appendix CMore Comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03621/x10.png",
                "caption": "Figure 10:Visualization of training pairs. During training, we use the 3DGS renderings of novel trajectories at 30,000 iterations as the source inputs, the 3DGS renderings of the original trajectory at 100, 500, or 1,000 iterations as the camera conditions, and the clean recorded videos of the original trajectory as the supervision.",
                "position": 1907
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x11.png",
                "caption": "Figure 11:Qualitative results of our method on multiple novel-trajectory generations.",
                "position": 1910
            },
            {
                "img": "https://arxiv.org/html/2512.03621/x12.png",
                "caption": "Figure 12:Qualitative novel-trajectory results and 3DGS rendering conditions, with the two scenes shifted right by 3 m and 4 m.",
                "position": 1913
            }
        ]
    },
    {
        "header": "Appendix DMore Qualitative Results",
        "images": []
    }
]