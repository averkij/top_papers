[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12782/artifacts_example4.png",
                "caption": "Figure 1:A section of the HeroBench virtual environment.\nA grid-based RPG-inspired world where agents must navigate, gather resources, craft equipment, and defeat enemies. Each location encodes specific environmental elements, forming the foundation for generating complex, structured tasks that challenge long-horizon reasoning and planning abilities of language models.",
                "position": 85
            },
            {
                "img": "https://arxiv.org/html/2508.12782/task_example2.png",
                "caption": "Figure 2:Example of a task in the environment: the agent’s ultimate goal is to defeat the target monster. To achieve this, agent must calculate the optimal gear by considering both its own and the monster’s stats, and acquire all the necessary ingredients.",
                "position": 89
            }
        ]
    },
    {
        "header": "HeroBench",
        "images": []
    },
    {
        "header": "Benchmark Task Generation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12782/BA.png",
                "caption": "Figure 3:Two agent architectures were evaluated in the benchmark. A: A-1, operates in two phases: in the first phase, it generates a high-level plan; in the second, it decomposes this plan into executable actions. B: Agentic system A-2, is a modification of A-1 based on the idea of assigning each agent a single, one-bite task.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2508.12782/success_rate_improved1.png",
                "caption": "Figure 4:Success rate of LLMs across nine base-task difficulty levels. Solid lines correspond to reasoning-enabled (thinking) models, while dashed lines represent standard (non-thinking) variants.",
                "position": 209
            }
        ]
    },
    {
        "header": "Multi-Agent systems architecture",
        "images": []
    },
    {
        "header": "Results",
        "images": []
    },
    {
        "header": "Related work",
        "images": []
    },
    {
        "header": "Conclusions",
        "images": []
    },
    {
        "header": "Future work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.12782/average_score_improved.png",
                "caption": "Figure 5:Score of LLMs across nine base-task difficulty levels. Solid lines correspond to reasoning-enabled (thinking)\nmodels, while dashed lines represent standard (non-thinking) variants.",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2508.12782/average_tokens_improved.png",
                "caption": "Figure 6:Token use of LLMs across nine base-task difficulty levels. Solid lines correspond to reasoning-enabled (thinking)\nmodels, while dashed lines represent standard (non-thinking) variants.",
                "position": 1385
            },
            {
                "img": "https://arxiv.org/html/2508.12782/bar_mean_score_improved.png",
                "caption": "Figure 7:Mean score of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking)\nmodels, while hatched bars represent standard (non-thinking) variants.",
                "position": 1389
            },
            {
                "img": "https://arxiv.org/html/2508.12782/bar_mean_success_improved.png",
                "caption": "Figure 8:Mean success rate of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking)\nmodels, while hatched bars represent standard (non-thinking) variants.",
                "position": 1393
            },
            {
                "img": "https://arxiv.org/html/2508.12782/bar_norm_score_improved.png",
                "caption": "Figure 9:Mean score per token of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking) models, while hatched bars represent standard (non-thinking) variants.",
                "position": 1397
            },
            {
                "img": "https://arxiv.org/html/2508.12782/bar_norm_success_improved.png",
                "caption": "Figure 10:Mean success rate per token of LLMs over nine base-task difficulty levels. Solid bars correspond to reasoning-enabled (thinking) models, while hatched bars represent standard (non-thinking) variants.",
                "position": 1400
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]