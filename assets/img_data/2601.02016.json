[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02016/LUPI_Front_Page12.png",
                "caption": "Figure 1:Visual comparison of baseline object detection predictions and those of a LUPI-trained student model, showing improved accuracy while keeping the same architecture. The figure also illustrates the LUPI training pipeline, including privileged information, teacher models, and knowledge distillation, with these boosts arising solely from the bolstered learning process.",
                "position": 109
            }
        ]
    },
    {
        "header": "IIRelated Work",
        "images": []
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02016/LUPI_Journal_Architecture.png",
                "caption": "Figure 2:Detailed architecture of the training setup. The teacher network receives both RGB images and privileged input channels, producing richer intermediate representations. The student network only processes RGB images, but is trained with additional supervision through knowledge distillation from the teacher. A baseline RGB-only model is included for comparison. The student demonstrates refined predictions relative to the baseline.",
                "position": 165
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Journal_Privileged_Info.png",
                "caption": "Figure 3:Investigation of different forms of privileged information using the RetinaNet model on the SODA 1-metre dataset. The comparison includes saliency, depth, fusion, and bounding box mask representations. The bounding box mask yielded the highest improvement in detection accuracy.",
                "position": 271
            }
        ]
    },
    {
        "header": "IVImplementation",
        "images": []
    },
    {
        "header": "VEvaluation Strategy",
        "images": []
    },
    {
        "header": "VIResults and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02016/baseline_vs_student_radars.png",
                "caption": "Figure 4:Comparative analysis of baseline and best LUPI-trained student models across all datasets for the five architectures, shown as a multi-radar graph. The figure highlights notable improvements in strict mAP and F1 score, with the largest boosts observed in within-dataset evaluations, while other datasets show smaller yet meaningful improvements using identical architectures.",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2601.02016/COCO_All_Models_All_Datasets.png",
                "caption": "Figure 5:Ablation study results across all datasets and experiments using COCO metrics. Baseline model corresponds toα=0\\alpha=0; other lines represent student models. Red downward arrows indicate top performance in the strict map@50–95 metric. Best results are generally observed forα=0.25\\alpha=0.25and 0.5, withα=0.75\\alpha=0.75also performing well, whileα=1\\alpha=1shows lower average performance.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Journal_GradCAM2.png",
                "caption": "Figure 6:Visual comparison of model predictions and interpretability results on the SODA 1-metre dataset experiment. (a) Baseline detection results. (b) Baseline Grad-CAM visualisation. (c) Best LUPI-trained student detection results. (d) Best Student Grad-CAM visualisation. The LUPI-trained student produces more accurate litter predictions than the baseline. For the Grad-CAM visualisations applied to the respective distillation layers, the student’s attention is more concentrated on litter objects, whereas the baseline exhibits more diffuse activation across the background.",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2601.02016/training_times_pascal_voc_2012_dataset.jpg",
                "caption": "Figure 7:Comparison of training times on the Pascal VOC 2012 dataset, highlighting the increased duration for LUPI teacher–student training.",
                "position": 883
            }
        ]
    },
    {
        "header": "VIIPractical Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/Author_Image.jpg",
                "caption": "",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/dylan.jpg",
                "caption": "",
                "position": 1010
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/Gabriel.jpeg",
                "caption": "",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/Prof_Matthew_Montebello.jpg",
                "caption": "",
                "position": 1035
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/Carl.jpg",
                "caption": "",
                "position": 1048
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/IMG_3751.jpg",
                "caption": "",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2601.02016/Authors/km.jpeg",
                "caption": "",
                "position": 1075
            }
        ]
    },
    {
        "header": "VIIIConclusion",
        "images": []
    }
]