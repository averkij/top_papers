[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23628/x1.png",
                "caption": "Figure 1:Examples of generated 3D shapes that illustrate memorizationvs.generalization relative to the training shapes.In this paper, we propose a framework to evaluate memorization in 3D shape generation, use it to quantify memorization in existing methods, and conduct controlled experiments to study how data and modeling designs impact memorization.",
                "position": 235
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23628/x2.png",
                "caption": "Table 2:Evaluating memorization on thechaircategory in ShapeNet.We reportZUZ_{U}for each model with respect to thechairsubset of ShapeNet’s training and test sets.\nThe last row shows, for each model, generated shapes at the60th60^{\\text{th}}percentile ranked by LFD distance to their nearest training shapes.\nAsZUZ_{U}increases, generated samples transition from near-identical replicas of training shapes to clearly novel shapes.\nFor the text-conditional model Michelangelo, we use the prompt “A 3D model of a chair” to sample, following its training recipe.",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x3.png",
                "caption": "",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x4.png",
                "caption": "",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x5.png",
                "caption": "",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x6.png",
                "caption": "",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x7.png",
                "caption": "",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x8.png",
                "caption": "",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x9.png",
                "caption": "",
                "position": 389
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x10.png",
                "caption": "",
                "position": 392
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x11.png",
                "caption": "",
                "position": 393
            }
        ]
    },
    {
        "header": "4Evaluating Existing Models",
        "images": []
    },
    {
        "header": "5Impact of Data on Memorization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/10th.png",
                "caption": "Figure 2:Generated samples from the baseline modelat each decile, ranked by LFD to the nearest training shape.",
                "position": 523
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/20th.png",
                "caption": "",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/30th.png",
                "caption": "",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/40th.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/50th.png",
                "caption": "",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/60th.png",
                "caption": "",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/70th.png",
                "caption": "",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/80th.png",
                "caption": "",
                "position": 581
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/baseline_grid/90th.png",
                "caption": "",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/ref.png",
                "caption": "Table 4:Qualitative comparison between 3D and image models for memorization.For each prompt, we show the training example, the generated 3D shape and its top-1 nearest training shapes under SSCD and LFD, together with the generated image and its top-1 nearest training image under SSCD.\nImage generation tends to replicate training samples, whereas the 3D generator produces more novel shapes.",
                "position": 600
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/3DGen1.png",
                "caption": "",
                "position": 638
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/3D1Top1NN_SSCD.png",
                "caption": "",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/3D1Top1NN_LFD.png",
                "caption": "",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/Gen1.png",
                "caption": "",
                "position": 659
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/Img1Top1NN_SSCD.png",
                "caption": "",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/3D2Top1NN_LFD.png",
                "caption": "",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/3DGen2.png",
                "caption": "",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/3D2Top1NN_SSCD.png",
                "caption": "",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/Gen2.png",
                "caption": "",
                "position": 710
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Img_vs_3D/Img2Top1NN_SSCD.png",
                "caption": "",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x12.png",
                "caption": "Figure 3:Training dynamics of the baseline model.As training progresses,ZUZ_{U}and training FD simultaneously decrease, whereas test FD decreases initially before plateauing at around 200K steps.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x13.png",
                "caption": "(a)Class-conditional models.",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x13.png",
                "caption": "(a)Class-conditional models.",
                "position": 781
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x14.png",
                "caption": "(b)Text-conditional models.",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x15.png",
                "caption": "Figure 5:Higher data diversity increases memorization.With a fixed training set size, increasing the number of classes from 16 to 100 leads to a moderate increase in memorization.",
                "position": 851
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x16.png",
                "caption": "(a)Base model",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x16.png",
                "caption": "(a)Base model",
                "position": 883
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x17.png",
                "caption": "(b)Large model",
                "position": 888
            }
        ]
    },
    {
        "header": "6Impact of Modeling on Memorization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/Uncond/gen/gen.png",
                "caption": "Table 6:Guidance scale case study for the prompt “table lamp with conical lampshade and cylindrical base”.Memorization is strongest at guidance scalew=3w=3, where the model most closely reproduces the training lamp. In contrast, samples atw=0w=0orw=1w=1either ignore the prompt or only weakly follow it, and samples at largerwwemphasize sub-phrases such as the base or the shade, rather than reproducing the full shape.",
                "position": 957
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/Uncond/gen/ref.png",
                "caption": "",
                "position": 979
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-1/gen.png",
                "caption": "",
                "position": 1005
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-1/Ref.png",
                "caption": "",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-3/gen/view_0002.png",
                "caption": "",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-3/LFD/view_0002.png",
                "caption": "",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-5/gen/view_0002.png",
                "caption": "",
                "position": 1067
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-5/Uni3D/view_0002.png",
                "caption": "",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-7/gen/view_0002.png",
                "caption": "",
                "position": 1098
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-7/Uni3D/view_0002.png",
                "caption": "",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-10/gen/view_0002.png",
                "caption": "",
                "position": 1129
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/GuidanceScale/GS-Search/GS-10/LFD/view_0002.png",
                "caption": "",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x18.png",
                "caption": "Figure 7:Increasing Vecset sequence length helps generalization.Generation quality improves, and memorization decreases.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/LatentLength/LengthRef.png",
                "caption": "Figure 8:Shapes generated from models trained with different Vecset lengths for the prompt “blocky stylized rabbit figure”.Models trained with longer Vecset lengths (1024 and 1280) generate high-quality shapes that are well aligned with the prompt, while exhibiting novel features not present in the training shape.",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/LatentLength/L768_Gen.png",
                "caption": "",
                "position": 1199
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/LatentLength/L1024_Gen.png",
                "caption": "",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/QualitativeResults/LatentLength/L1280_Gen.png",
                "caption": "",
                "position": 1201
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x19.png",
                "caption": "Figure 9:Training dynamics with yaw rotation augmentation.Compared to the baseline, test FD converges more slowly, yet after convergence, memorization (as reflected byZUZ_{U}) is much lower.",
                "position": 1228
            }
        ]
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMetrics Definition and Implementation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/1_memorization.png",
                "caption": "(a)Memorization",
                "position": 1482
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/1_memorization.png",
                "caption": "(a)Memorization",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/2_generalization.png",
                "caption": "(b)Generalization",
                "position": 1491
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x20.png",
                "caption": "Figure 11:Baseline model loss curves.The training and validation losses of the baseline model throughout the training process.",
                "position": 1510
            }
        ]
    },
    {
        "header": "Appendix BExisting Models’ Retrieval Visualization",
        "images": []
    },
    {
        "header": "Appendix CDetailed Experimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23628/x21.png",
                "caption": "Figure 14:Category distributionof our customized 100-category subset of Objaverse-XL.",
                "position": 1721
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x22.png",
                "caption": "Figure 15:Larger datasets reduce memorization.With the same diffusion model, increasing the dataset size decreasesZUZ_{U}.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x23.png",
                "caption": "Figure 16:Larger models strengthen memorization.With the same training dataset, increasing the model size decreasesZUZ_{U}.",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x24.png",
                "caption": "Figure 18:Coarse-grained category distribution.We use six categories (animals, daily-use, electronics, furniture, human-shape, and transportation) from the GObjaverse taxonomy.",
                "position": 1935
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x25.png",
                "caption": "Figure 19:Caption length distribution across granularities.The distribution of caption length for phrase, sentence, and paragraph captions demonstrates distinct levels of detail.",
                "position": 1944
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x26.png",
                "caption": "Figure 21:Moderate guidance scales lead to the strongest memorization (class-conditional checkpoint), while further increasing the guidance scale reduces memorization, consistent with Section6.1.\nWeak guidance also yields low memorization.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/GS-1.png",
                "caption": "Table 10:Guidance scale case study for the prompt “a geometric sculpture with a trapezoidal base and a rectangular prism suspended above it by rods”.The model reproduces the training shape at lower scales (w∈{1,3,5}w\\in\\{1,3,5\\}), and emphasizes sub-phrases such as “trapezoidal base” and “rods”, but fails to generate the rectangular prism at largerww.",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/Ref.png",
                "caption": "",
                "position": 2029
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/GS-3.png",
                "caption": "",
                "position": 2057
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/GS-5.png",
                "caption": "",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/GS-7.png",
                "caption": "",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/GS-10-Ref.png",
                "caption": "",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Basket/GS-10.png",
                "caption": "",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-1.png",
                "caption": "Table 11:Guidance scale case study for the prompt “blocky humanoid figure standing on a base”.The model generates a similar but lower-quality blocky figure with no guidance, reproduces the training shape whenw=3w=3, and emphasizes sub-phrases such as “figure standing on a base”, but fails to generate the blocky head with largerww.",
                "position": 2173
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-1-Ref.png",
                "caption": "",
                "position": 2195
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-3.png",
                "caption": "",
                "position": 2217
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/Ref.png",
                "caption": "",
                "position": 2224
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-5.png",
                "caption": "",
                "position": 2248
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-5-Ref.png",
                "caption": "",
                "position": 2255
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-7.png",
                "caption": "",
                "position": 2279
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-7-10-Ref.png",
                "caption": "",
                "position": 2286
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/GS-Cases/Figurine/GS-10.png",
                "caption": "",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Rifle/Ref.png",
                "caption": "Figure 22:Shapes generated with different Vecset lengths for the prompt “an assault rifle with a stock, foregrip, and pistol grip; a body; and a long barrel”.Vecsets of longer sequence lengths (1024 and 1280) generate high-quality shapes aligned with the training prompt, while exhibiting novel features (e.g., stocks and foregrips) that are different from the training shape.",
                "position": 2405
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Rifle/L768.png",
                "caption": "",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Rifle/L1024.png",
                "caption": "",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Rifle/L1280.png",
                "caption": "",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Chair/Ref.png",
                "caption": "Figure 23:Shapes generated with different Vecset lengths for the prompt “wing chair”.Vecsets of longer sequence lengths (1024 and 1280) generate high-quality shapes that are well aligned with the training prompt, while exhibiting novel features (e.g., arms and wings) that are different from the training shape.",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Chair/L768.png",
                "caption": "",
                "position": 2432
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Chair/L1024.png",
                "caption": "",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2512.23628/figs/Appendix/LatentLengthCases/Chair/L1280.png",
                "caption": "",
                "position": 2434
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x27.png",
                "caption": "Figure 24:Robustness of distance metrics to rotation.Mean and standard deviation of LFD (left) and Uni3D (right) distances between original and randomly rotated objects. LFD exhibits significant variation across all rotation axes, indicating that it is not rotation-invariant. Uni3D demonstrates strong robustness (i.e., low distances) specifically under yaw rotation.",
                "position": 2459
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x28.png",
                "caption": "Figure 25:Qualitative retrieval results on ShapeNet’schaircategory.We generate 100 chairs for each model and visualize the nearest training shapes for generated samples at the1s​t1^{st},20t​h20^{th},40t​h40^{th},60t​h60^{th},80t​h80^{th}, and90t​h90^{th}percentiles of the LFD distance distribution for each model.\nNFD, unconditional LAS-Diffusion, and Wavelet Generation exhibit strong memorization: even generated shapes at the60t​h60^{th}-80t​h80^{th}percentiles remain very close to their nearest training shapes.\nIn contrast, conditional LAS-Diffusion, 3DShape2VecSet, and Michelangelo show novel geometric features even for generated samples with lower nearest-neighbor distances, indicating stronger generalization.",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2512.23628/x29.png",
                "caption": "Figure 26:Qualitative retrieval results on the entire training sets.We generate 100 samples for each model and visualize the nearest training shapes for generated samples at the1s​t1^{st},20t​h20^{th},40t​h40^{th},60t​h60^{th},80t​h80^{th}, and90t​h90^{th}percentiles of the nearest-neighbor LFD distance distribution for each model trained on large datasets.\nAcross all models, the retrieved training shapes already become novel at moderate percentiles (e.g.,20t​h20^{th}-60t​h60^{th}).\nAlthough LFD is category-sensitive and may not always retrieve visually near-identical shapes, the overall trends suggest that these models trained on large datasets primarily generalize rather than copy individual training examples. We note that 3DTopia-XL does not generalize well and often degenerates, producing low-quality shapes even when using training prompts.",
                "position": 2467
            }
        ]
    },
    {
        "header": "Appendix DRendering Details",
        "images": []
    }
]