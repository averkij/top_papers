[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x1.png",
                "caption": "",
                "position": 182
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x2.png",
                "caption": "Figure 2:An example of annotation in VRBench. For each question, VRBench provides the question-answer pair, multi-step reasoning chain, question type, and the start-to-end timestamps of the entire question as well as each reasoning step.",
                "position": 208
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x3.png",
                "caption": "Figure 3:Statistics of VRBench. We provide the detailed distribution of videos and annotations of VRBench, including video languages and durations, steps, types, and temporal duration of questions, as well as the token numbers of answers and the reasoning process.",
                "position": 584
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x4.png",
                "caption": "Figure 4:Human preference alignment results.For each plot, we show the win ratios of three different tested LLMs evaluated by human experts and VRBench. We then fit them with a straight line and quantify the correlation by calculating the Spearman correlation coefficient.",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x5.png",
                "caption": "Figure 5:Test-Time Scaling Results. We report the average accuracy of outcome-level MCQ and process-level open-ended ratings.",
                "position": 1189
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "FQuestion Exemplar",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x6.png",
                "caption": "Figure A6:Event attribution: Determine causal origins or underlying motivations of video events.",
                "position": 2421
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x7.png",
                "caption": "Figure A7:Event prediction: Forecast subsequent events in the video timeline.",
                "position": 2424
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x8.png",
                "caption": "Figure A8:Implicit inference: Extract unstated temporal, emotional, or relational context from visual cues.",
                "position": 2427
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x9.png",
                "caption": "Figure A9:Information synopsis: Condense critical information across multimodal inputs.",
                "position": 2430
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x10.png",
                "caption": "Figure A10:Hypothetical reasoning: Deduce plausible scenario outcomes from stated premises.",
                "position": 2433
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x11.png",
                "caption": "Figure A11:Logical linkage: Establish event-mediated connections between visual/narrative elements.",
                "position": 2436
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x12.png",
                "caption": "Figure A12:Counting problems: Quantify state changes through arithmetic/combinatorial analysis.",
                "position": 2439
            }
        ]
    },
    {
        "header": "GManual Filtering Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x13.png",
                "caption": "Figure A13:Interface of manual video filtering.",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x14.png",
                "caption": "Figure A14:Interface of manual video labeling.",
                "position": 2497
            }
        ]
    },
    {
        "header": "HHuman Annotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x15.png",
                "caption": "Figure A15:Interface of video-level evaluation.",
                "position": 2885
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x16.png",
                "caption": "Figure A16:Interface of question-level evaluation.",
                "position": 2888
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x17.png",
                "caption": "Figure A17:Prompts of generating the first video summary.",
                "position": 2912
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x18.png",
                "caption": "Figure A18:Prompts of generating other video summaries.",
                "position": 2915
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x19.png",
                "caption": "Figure A19:Prompts of generating GPT pre-annotation.",
                "position": 2925
            }
        ]
    },
    {
        "header": "IHuman Validation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x20.png",
                "caption": "Table A3:Details of evaluated multimodal foundation models. The ”Input Frames” column represents the default number of input frames, chosen from 4, 8, 16, 32, 64, 128, 256, 512, based on the maximum value that does not exceed the model’s context window and the constraints of GPU memory. “HF” means “Hugging Face”.",
                "position": 3028
            }
        ]
    },
    {
        "header": "JEvaluation Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10857/x20.png",
                "caption": "(a)System prompt for LLM",
                "position": 3453
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x20.png",
                "caption": "(a)System prompt for LLM",
                "position": 3456
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x21.png",
                "caption": "(b)System prompt for LLM",
                "position": 3462
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x22.png",
                "caption": "(c)User prompt template",
                "position": 3468
            },
            {
                "img": "https://arxiv.org/html/2506.10857/x23.png",
                "caption": "Figure A21:System prompt for a question having a unique answer",
                "position": 3482
            }
        ]
    },
    {
        "header": "KEthical Discussion",
        "images": []
    }
]