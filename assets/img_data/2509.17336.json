[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17336/x1.png",
                "caption": "Figure 1:Overview of the Mano framework. The left part illustrates theExploration Module, which operates in simulated browsers and desktop environments to collect interaction elements and candidate goals, generating diverse trajectories and login assistance data for training. The center shows theInference Process Pipeline, where the model follows a structured “think–act–verify” loop: interpreting GUI states, producing action descriptions (e.g., clicks or type), executing them, and validating outcomes through a verifier. The right part depicts theOptimize Process, a progressive pipeline of SFT, offline RL, and online RL, which systematically strengthens reasoning, adaptability, and end-to-end decision-making in dynamic GUI environments.",
                "position": 140
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17336/x2.png",
                "caption": "Figure 2:Overall fine-tuning framework ofManofor GUI-oriented tasks. The pipeline consists of three progressive stages: (i)SFTon offline demonstrations; (ii)Offline RLleveraging static trajectories with reward decomposition; and (iii)Online RLwith active environment interaction. The system incorporates step-level reasoning, explicit action description, and operation type selection (e.g., click, drag, type, scroll), while final performance is evaluated through structured outputs and multi-dimensional rewards combining format accuracy, operation correctness, and task completion.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2509.17336/x3.png",
                "caption": "Figure 3:Overall framework of online RL in Mano.\nThe Mano model interacts with multiple parallelPlaywrightinstances, each representing a GUI environment.\nFor every step, the model fetches the status and screenshot, performs inference to generate thought and action, and then executes the action within the corresponding environment. The loop continues until the task is completed, while memory traces are recorded and trajectories are exported for further training and analysis.",
                "position": 319
            },
            {
                "img": "https://arxiv.org/html/2509.17336/x4.png",
                "caption": "Figure 4:The operational workflow ofMano-parking, which illustrates its autonomous data extraction pipeline. The process begins with request reception and function registry lookup, followed by either direct execution of pre-validated functions or initiation of a multi-phase extraction synthesis. In the latter case, simplified HTML structures are obtained through browser automation and cleaning algorithms, combined with user-defined attribute specifications to generate customized extraction functions. These functions undergo a three-tier validation—field completeness, semantic consistency, and structural integrity—before being executed and stored for reuse. Furthermore, Mano-parking incorporates continuous monitoring and a self-healing mechanism, enabling adaptive regeneration of extraction logic when website structures evolve. This design ensures robustness, efficiency, and minimal human intervention across diverse web environments.",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2509.17336/x5.png",
                "caption": "Figure 5:Mano-cipher is a specialized authentication GUI model. This GUI model facilitates automated login operations across diverse systems by handling various captcha types—including alphanumeric, image-based sliding, rotation, content recognition, and logical reasoning challenges. Upon successful verification, system control is returned to the Mano for subsequent tasks.",
                "position": 425
            }
        ]
    },
    {
        "header": "3Data Cycling System",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17336/x6.png",
                "caption": "Figure 6:The schematic representation of the automation engine’s architecture for collecting web interactions. The process begins with the generation of various URLs by a large language model (LLM). These URLs serve as inputs to identify common operation targets specific to the current website. Subsequently, the system fetches interactive elements on webpages usingMano-C, while also gathering descriptions of current webpage elements along with Document Object Model (DOM) information. This data facilitates the selection of candidate elements for subsequent exploration and operations. The iterative process is governed by a condition that checks whether the maximum exploration depth has been reached. If not, the cycle continues; otherwise, the exploration for the current candidate is deemed complete.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2509.17336/x7.png",
                "caption": "Figure 7:Attention heatmap of action tokens over prompt, historical/current images, thinking and summary. Note that due to the significantly longer sequence of image tokens compared to the summary and thinking process, the image tokens were compressed via max-pooling-based down-sampling. The vertical axis represents all tokens of the action sequence. Each row corresponds to the attention distribution of one action token over the context. Brighter color indicates higher attention weight.",
                "position": 778
            },
            {
                "img": "https://arxiv.org/html/2509.17336/x8.png",
                "caption": "Figure 8:Illustration of the data cycling. We sample from the dataset for SFT. Then, complete trajectories are picked to fine-tune the model, which is later used in a simulated environment for RL. During this process, high-value trajectories are retained and are refined through LLM assistance and human correction before being added back to the original dataset.",
                "position": 787
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.17336/x9.png",
                "caption": "Figure 9:Illustrative examples of Mano’s reasoning and execution process across 3 distinct scenarios. Each row corresponds to one task instance: the left column records the historical trace of executed actions with verification marks, while the right column displays the current reasoning state, including generatedThought, action description, and executable function. Row 1 demonstrates environment extension by scrolling a dropdown to reveal a hidden option; Row 2 shows exception handling and generalization when facing an unexpected popup; Row 3 highlights self-analysis and correction after an erroneous selection. Together, these cases illustrate Mano’s robustness, adaptability, and error-aware reasoning in complex interactive environments.",
                "position": 1267
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]