[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15216/x1.png",
                "caption": "",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2412.15216/x2.png",
                "caption": "Figure 2:Examples of biases introduced by Prompt-to-Prompt in the InstructPix2Pix dataset.Each example shows an input image and its corresponding edited image (generated by Prompt-to-Prompt) along with the associated edit instruction.(a) Attribute-entangled edits: modifying the lady’s dress also unintentionally changes the background.(b) Scene-entangled edits:transforming the cottage into a castle affects surrounding elements.(c) Global scene changes:converting the image to black and white alters the entire scene.",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15216/x3.png",
                "caption": "Figure 3:Overview of the UIP2P training framework.The model learns instruction-based image editing by utilizing forward and reverse instructions. Starting with an input image and a forward instruction, the model generates an edited image using IP2P. A reverse instruction is then applied to reconstruct the original image, enforcing Cycle Edit Consistency (CEC).",
                "position": 244
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15216/x4.png",
                "caption": "Figure 4:Qualitative Examples.UIP2P performance is shown across various tasks and datasets, compared to InstructPix2Pix, MagicBrush, HIVE, MGIE, and SmartEdit. Our method demonstrates either comparable or superior results in terms of accurately applying the requested edits while preserving visual consistency.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2412.15216/x5.png",
                "caption": "(a)Zero-shot Quantitative Comparison on MagicBrush[50]test set.Instruction-based editing methods that are not fine-tuned on MagicBrush are presented. In the multi-turn setting, target images are iteratively edited from the initial images.",
                "position": 686
            },
            {
                "img": "https://arxiv.org/html/2412.15216/x5.png",
                "caption": "(b)Evaluation on the IP2P test dataset.UIP2P outperforms IP2P in both CLIP image similarity and CLIP text-image similarity metrics, demonstrating better visual fidelity and instruction alignment.",
                "position": 811
            },
            {
                "img": "https://arxiv.org/html/2412.15216/x6.png",
                "caption": "Figure 6:Ablation study on the number of steps.UIP2P achieves high fidelity edits on the input image with fewer steps, whereas IP2P struggles to maintain quality.",
                "position": 904
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15216/x7.png",
                "caption": "Figure 7:Qualitative comparison of our method with baseline models for various editing instructions. From left to right: Input image, edit instruction, and results from InstructPix2Pix, MagicBrush, HIVE, MGIE, SmartEdit, and our method. Our approach demonstrates superior fidelity and alignment with the provided instructions across diverse tasks, such as expression changes, color adjustments, object transformations, and creative edits.",
                "position": 1645
            },
            {
                "img": "https://arxiv.org/html/2412.15216/x8.png",
                "caption": "Figure 8:Forward and reverse edits are applied sequentially.",
                "position": 1900
            },
            {
                "img": "https://arxiv.org/html/2412.15216/extracted/6082894/figures/user_study_setting.png",
                "caption": "Figure 9:User Study Setup.The input image is shown alongside randomly ordered edited images generated by different methods (a)-(f) based on the edit instruction, “make the face happy.” Participants are asked to select the best two methods that match the editing effect and those that best preserve irrelevant instruction regions.",
                "position": 2201
            }
        ]
    },
    {
        "header": "7Appendix",
        "images": []
    }
]