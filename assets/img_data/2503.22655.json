[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22655/x1.png",
                "caption": "Figure 1:Unlike traditional image-text data synthesis frameworks, Unicorn removes the dependency on real image data, offering a more efficient and scalable solution by cutting down API costs, synthesis time, and storage requirements.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2503.22655/x2.png",
                "caption": "Figure 2:Unicorn’s text-only data synthesis pipeline, comprising three cross-integrated stages, (1) Diverse Caption Data Synthesis, (2) Instruction-Tuning Data Synthesis, and (3) Modality Presentation Transfer, ultimately yields two synthetic datasets: Unicorn-1.2M and Unicorn-471K-Instruction. Notably, unlike traditional approaches that rely on real images, Unicorn generates image representations paired with text, entirely free of real image data.",
                "position": 170
            }
        ]
    },
    {
        "header": "2Backgrounds",
        "images": []
    },
    {
        "header": "3Data Synthesis Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22655/x3.png",
                "caption": "Figure 3:Data formats for the three instruction-tuning tasks. Each dialogue’s answer is derived from the textual content of diverse caption.",
                "position": 211
            },
            {
                "img": "https://arxiv.org/html/2503.22655/x4.png",
                "caption": "Figure 4:Training and inference processes of Unicorn-8B. Training aligns synthetic image representations with LLM embeddings, while inference adjusts real image embeddings as input.",
                "position": 243
            }
        ]
    },
    {
        "header": "4Unicorn-8B",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.22655/x5.png",
                "caption": "Figure 5:Comparison of the data length distributions between Unicorn-1.2M and ShareGPT4V.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2503.22655/x6.png",
                "caption": "Figure 6:Performance of Unicorn-8B on the MMECand ScienceQA benchmarks across different training data scales.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2503.22655/x7.png",
                "caption": "Figure 7:Examples of Unicorn-8B’s correct responses on the ScienceQA and iNaturallist-VQA benchmarks.",
                "position": 691
            }
        ]
    },
    {
        "header": "6Conclusion and Limitation",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]