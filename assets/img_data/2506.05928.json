[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05928/extracted/6518598/figures/MOA_version4.png",
                "caption": "Figure 1:MoA architecture with heterogeneous PEFT adapters. It is worth noting that in Sparse MoA, the Prompt Tuning module is deactivated due to its non-token-level activation mechanism.",
                "position": 173
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05928/x1.png",
                "caption": "Figure 2:Comparison of different models under identical batch sizes (1* 48G GPU): (a) Training time per epoch, (b) GPU memory consumption during training, and (c) Average inference time per sample.",
                "position": 586
            }
        ]
    },
    {
        "header": "6In-depth Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05928/x2.png",
                "caption": "Figure 3:Comparison of router weight distributions between Soft MoA and AdaMoLE under different random seeds. The MoA method exhibits strong consistency, whereas AdaMoLE does not.",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2506.05928/x3.png",
                "caption": "Figure 4:Visualization of average router weights per layer for Soft MoA (left) and Sparse MoA (right) on ARC-Challenge, averaged over tokens within 50 samples. Sparse MoA also includes the average per-layer threshold and the average count of activated experts. The average count of activated experts across layers in Sparse MoA is 3.55.",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2506.05928/x4.png",
                "caption": "",
                "position": 762
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APEFT Methods And The Unified Form",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05928/x5.png",
                "caption": "Figure 5:Visualization of average router weights per layer for Soft MoA (left) and Sparse MoA (right) on gsm8k, averaged over tokens within 50 samples. The Sparse MoA plot (right) also includes the average per-layer threshold and the average count of activated experts. The average count of activated experts across layers in Sparse MoA is 4.13.",
                "position": 1644
            },
            {
                "img": "https://arxiv.org/html/2506.05928/x6.png",
                "caption": "",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2506.05928/x7.png",
                "caption": "Figure 6:Router Weights of an example of at Layer 14 in Soft MoA.",
                "position": 1658
            },
            {
                "img": "https://arxiv.org/html/2506.05928/x8.png",
                "caption": "Figure 7:Router Weights of an example at Layer 14 in Sparse MoA. \"Threshold\" denotes the dynamic value of the threshold function for Sparse MoA. A router weight of \"0.00\" for an expert indicates that the expertâ€™s computation is skipped. Sparse MoA avoids unnecessary computations from the low-contribution expert for each token.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2506.05928/x9.png",
                "caption": "Figure 8:Average router weights across layers. Soft MoA exhibits strong consistency under different random seeds in training, whereas AdaMoLE does not.",
                "position": 1664
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experiments and Analyses",
        "images": []
    }
]