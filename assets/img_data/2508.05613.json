[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05613/figures/cooper_logo.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05613/x1.png",
                "caption": "Figure 1:Model-based rewards are generally robust to variations in model outputs, but they are susceptible to being hacked by the policy model. In contrast, rule-based rewards are less prone to hacking but often lack robustness. We introduceCooper, a reinforcement learning framework that achieves both high robustness and resistance to reward hacking. In this figure, the black arrows indicate the rollout process, the blue arrows represent the reward assignment process, and the brown arrows denote the update process for the reward model.",
                "position": 165
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05613/x2.png",
                "caption": "Figure 2:An overview of the Cooper training framework. Each training step in Cooper consists of two stages: policy model optimization (blue area) and reward model optimization (green area).",
                "position": 254
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05613/x3.png",
                "caption": "(a)Test set accuracy (%) across MATH500.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2508.05613/x3.png",
                "caption": "(a)Test set accuracy (%) across MATH500.",
                "position": 704
            },
            {
                "img": "https://arxiv.org/html/2508.05613/x4.png",
                "caption": "(b)Train set reward during RL training.",
                "position": 709
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05613/x5.png",
                "caption": "Figure 4:Accuracy of RM across training steps.",
                "position": 736
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AData Source",
        "images": []
    },
    {
        "header": "Appendix BLLM Usage",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05613/x6.png",
                "caption": "Figure 5:Prompt template for LLM-as-a-judge used in hybrid annotation.",
                "position": 1810
            },
            {
                "img": "https://arxiv.org/html/2508.05613/x7.png",
                "caption": "Figure 6:Prompt template for generating negative response.",
                "position": 1820
            },
            {
                "img": "https://arxiv.org/html/2508.05613/x8.png",
                "caption": "Figure 7:Prompt template for VerifyRM showing the problem-reference-completion triple format.",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix DDetails of Hybrid Annotation",
        "images": []
    }
]