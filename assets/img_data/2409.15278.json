[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15278/x1.png",
                "caption": "Figure 1:Task Overview of our Omni Pixel-to-Pixel Instruction-tuning Dataset for PixWizard.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Omni Pixel-to-Pixel Instruction-Tuning Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15278/x2.png",
                "caption": "Figure 2:Overall framework of PixWizard.",
                "position": 177
            }
        ]
    },
    {
        "header": "3PixWizard",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15278/x3.png",
                "caption": "Figure 3:t-SNE visualization of the global text embeddings. Each dot represents a human instruction.",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x4.png",
                "caption": "Figure 4:The schematic illustrations of PixWizard Block and Task-Aware Dynamic Sampler.",
                "position": 222
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15278/x5.png",
                "caption": "Figure 5:Qualitative Evaluation of Instruction-based Image Restoration.",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x6.png",
                "caption": "Figure 6:Qualitative Results of Instruction-based Image Grounding.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x7.png",
                "caption": "Figure 7:Visualizations of dense image prediction examples.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x8.png",
                "caption": "Figure 8:Qualitative examples comparing PixWizard with other editing approaches.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x9.png",
                "caption": "Figure 9:Visualization examples under different conditions.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x10.png",
                "caption": "Figure 10:Visualization results ofInpaintingandOutpainting.",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x11.png",
                "caption": "Figure 11:Visualization results ofRemove,ReplaceandAddAnything.",
                "position": 959
            },
            {
                "img": "https://arxiv.org/html/2409.15278/x12.png",
                "caption": "Figure 12:Text-to-image samples generated by PixWizard.",
                "position": 971
            }
        ]
    },
    {
        "header": "5Discussion and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BMore details for the Omni pexel-to-pexel instruction-tuning dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.15278/x13.png",
                "caption": "Figure 13:Examples of GPT4o-paraphrase user prompts for different task.",
                "position": 3456
            }
        ]
    },
    {
        "header": "Appendix CMore Detailed Descriptions for the Architecture",
        "images": []
    },
    {
        "header": "Appendix DMore Experimental Results",
        "images": []
    }
]