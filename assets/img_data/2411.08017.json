[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08017/x1.png",
                "caption": "Figure 1:We propose a new 3D generative model, called WaLa, that can generate shapes from conditions such as sketches, text, single-view images, low-resolution voxels, point clouds & depth-maps.",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08017/x2.png",
                "caption": "Figure 2:WaLa generates 3D shapes across various input modalities (see appendix for more).",
                "position": 95
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08017/x3.png",
                "caption": "Figure 3:Overview of the WaLa¬†network architecture and 2-stage training process and inference method. Top Left: Stage 1 autoencoder training, compressing diffusible wavelet tree (WùëäWitalic_W) shape representation into a compact latent space.\nTop Right: Conditional/unconditional diffusion training.\nBottom: Inference pipeline, illustrating sampling from the trained diffusion model and decoding the sampled latent into a Wavelet Tree (WùëäWitalic_W), then into a mesh.",
                "position": 225
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08017/x4.png",
                "caption": "Figure 4:Qualitative comparison with other methods for single-view (top-left), multi-view (top-right), voxels (bottom-left), and point cloud (bottom-right) conditional input modalities.Hui et¬†al. (2024); He & Wang (2024); Tochilkin et¬†al. (2024); Xu et¬†al. (2024); Tang et¬†al. (2024); Chen et¬†al. (2024b); Nichol et¬†al. (2022c)",
                "position": 348
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Results and Details",
        "images": []
    },
    {
        "header": "Appendix BArchitecture details",
        "images": []
    },
    {
        "header": "Appendix CAblation Studies",
        "images": []
    },
    {
        "header": "Appendix DSketch data generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08017/extracted/5995222/figures/sketches.png",
                "caption": "Figure 5:The 6 different sketch types. From left to right: Grease Pencil, Canny, HED, HED+potrace, HED+scribble, CLIPaasso, and a depth map for reference. Mesh taken from(Fu et¬†al.,2021).",
                "position": 2913
            },
            {
                "img": "https://arxiv.org/html/2411.08017/extracted/5995222/figures/sketch-views.png",
                "caption": "Figure 6:The 8 different views for which sketches were generated. Images created using the Grease Pencil technique on a mesh taken fromFu et¬†al. (2021). The CLIPasso technique was only used on the first, fifth, and sixth views from the left.",
                "position": 2928
            }
        ]
    },
    {
        "header": "Appendix EText-to-3D details",
        "images": []
    },
    {
        "header": "Appendix FModel sizes",
        "images": []
    },
    {
        "header": "Appendix GScale and timesteps for different models",
        "images": []
    },
    {
        "header": "Appendix HMore Visual Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.08017/x5.png",
                "caption": "Figure 7:This figure presents more results from the text-to-3D generation task. Each row corresponds to a unique text prompt, with the resulting 3D renderings highlighting the model‚Äôs capability to produce detailed and varied shapes from these inputs.",
                "position": 3082
            },
            {
                "img": "https://arxiv.org/html/2411.08017/x6.png",
                "caption": "Figure 8:Here, for each caption, four different 3D variations are displayed. This figure emphasizes the model‚Äôs flexibility in generating multiple distinct outputs for the same text description while maintaining thematic consistency.",
                "position": 3086
            },
            {
                "img": "https://arxiv.org/html/2411.08017/x7.png",
                "caption": "Figure 9:Here, for each caption, four different 3D variations are displayed. This figure emphasizes the model‚Äôs flexibility in generating multiple distinct outputs for the same text description while maintaining thematic consistency.",
                "position": 3090
            }
        ]
    },
    {
        "header": "Appendix IContributions",
        "images": []
    }
]