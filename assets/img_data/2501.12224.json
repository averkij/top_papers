[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x1.png",
                "caption": "",
                "position": 126
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x2.png",
                "caption": "Figure 2:Directions in the global modulation space (‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic_M) and our per-token modulation space (‚Ñ≥+superscript‚Ñ≥\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT).Given a generated image (top row), we modify it using text-driven directions in both‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic_Mand‚Ñ≥+superscript‚Ñ≥\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPTspaces.\n(a) Adding a direction to the vector that is used to modulate all the text and image tokens (i.e.a direction in the space‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic_M) can be used to effectively modify desired concepts in the generated image. Yet, this often results in non-local changes that also affect other concepts in the generated image. (b)¬†Adding a direction only to the modulation vector of a specific text token, like ‚Äúdog‚Äù or ‚Äúball‚Äù (i.e.a direction in the space‚Ñ≥+superscript‚Ñ≥\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT) leads to a localized modification that mostly affects the concept of interest.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Preliminaries: Diffusion transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x3.png",
                "caption": "Figure 3:TokenVerse overview.(a) A pre-trained text-to-image DiT model processes both image and text tokens via a series of DiT blocks. Each block consists of modulation, attention and feed-forward modules. We focus on the modulation block, in which the tokens are modulated via a vectoryùë¶yitalic_y, which is derived from a pooled text embedding. (b) Given a concept image and its corresponding caption, TokenVerse learns a personalized modulation vector offsetŒîŒî\\Deltaroman_Œîfor each text token. These offsets represent personalized directions in the modulation space and are learned using a simple reconstruction objective. (c) At inference, the pre-learned direction vectors are used to modulate the text tokens, enabling the injection of personalized concepts into the generated images.",
                "position": 236
            }
        ]
    },
    {
        "header": "4The‚Ñ≥+superscript‚Ñ≥\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPTspace",
        "images": []
    },
    {
        "header": "5Disentangled concept learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x4.png",
                "caption": "Figure 4:Concept isolation loss.When training Concept-Mod we apply an additionalconcept isolation lossin 50% of the training steps. This loss encourages learning directions that do not interfere with other images by enforcing that the parts in the image that should not be affected by the directions remain similar.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x5.png",
                "caption": "Figure 5:Qualitative results.Each row begins with a bank of four source images, from which our method independently extracts concepts. To the right, three generated images are shown, demonstrating the seamless combination of these concepts into new, coherent outputs.",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x6.png",
                "caption": "Figure 6:Extreme multi-concept personalization.Our method has no technical constraint on the number of concepts that can be combined in an image. As can be seen, TokenVerse can generate images composing a significant number of concepts.",
                "position": 328
            }
        ]
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x7.png",
                "caption": "Figure 7:Concepts beyond objects.We demonstrate the composition of three types of personalized concepts: object (the bear; concept image not shown), pose (left column) and lighting (top row).\nTokenVerse successfully learns the pose and lighting without overfitting to the identity of the poser or the specific lit scene.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x8.png",
                "caption": "Figure 8:Qualitative comparisons.Each row depicts two concept images (left) and images containing a combination of those concepts, generated by ConceptExpress[14], BAS[8], DreamBooth[32], OMG[23]and our method.\nThe concepts associated with the green and blue words are taken from the left and right concept images, respectively. As can be seen, our method best composes the two concepts while preserving concept fidelity.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x9.png",
                "caption": "Figure 9:Quantitative comparison.We compare our method to other baselines on concept preservation and prompt fidelity (higher is better) using DreamBench++ and a user study. (a) We compare three different settings:(i)ùëñ(i)( italic_i )composing two concepts from different images (concept composition),(i‚Å¢i)ùëñùëñ(ii)( italic_i italic_i )decomposing two concepts from the same image (concept decomposition), and(i‚Å¢i‚Å¢i)ùëñùëñùëñ(iii)( italic_i italic_i italic_i )the combination of the two (full task). (b) We conduct a user study, comparing our method to existing methods on our full task.\nOur method consistently scores best in terms of concept preservation while maintaining high prompt fidelity scores. See\nApp.C.2for the exact metrics.",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x10.png",
                "caption": "Figure 10:Ablations.The left pane shows all the concepts used to generate the result images. Columns (a) to (d) shows the results of our method as additional components are progressively integrated.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x11.png",
                "caption": "Figure 11:Limitations.Concept images are shown in the top row, with the generated images using TokenVerse below in each case. While our method supports both disentangled learning and multi-concept composition, limitations remain. (a) Rare blending can occur in specific combinations due to independent training of concepts; We provide analysis and mitigations in\nApp.F.\n(b)¬†Challenges arise with concepts sharing the same name identifier, which can be mitigated by using distinct terms. (c) Certain incompatible combinations, such as a doll with tiny limbs in a complex pose, may result in undesired outputs.",
                "position": 546
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAdditional training details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x12.png",
                "caption": "Figure 12:Augmentations Ablation.The top row shows the concepts used to generate the result images. Column (a) displays the results of our full method, while column (b) shows the results without text and image augmentations.",
                "position": 1032
            }
        ]
    },
    {
        "header": "Appendix BAdditional qualitative results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x13.png",
                "caption": "Figure 13:Progressive composition of concepts.TokenVerse can be used to progressively add concepts into a generated image, while controlling all other aspects of the generated images via text. In each row, the object, pose, lighting, and hair are personalized, while the background is described by text (e.g.‚ÄúNY city‚Äù, ‚Äúgarden‚Äù, and ‚ÄúMars‚Äù for the top row.)",
                "position": 1043
            }
        ]
    },
    {
        "header": "Appendix CQuantitative evaluation",
        "images": []
    },
    {
        "header": "Appendix DUser study",
        "images": []
    },
    {
        "header": "Appendix EApplication to styorytelling",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12224/x14.png",
                "caption": "Figure 14:Limitation ‚Äì highly similar modulated tokens.(a)¬†The common scenario of combining two distinct objects, such as a doll and a dog, into a single image. (b)¬†A failure case where independent training of concepts leads to the creation of hybrid objects. (c)¬†A potential mitigation for this issue by employing joint training on both concepts.",
                "position": 1305
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x15.png",
                "caption": "Figure 15:Limitations ‚Äì colliding captions.Our method may fail when handling cases of colliding identifiers, such as two dolls (a). This issue can be easily resolved by assigning distinct identifiers to each object during the initial training (b).",
                "position": 1310
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x16.png",
                "caption": "Figure 16:Qualitative results.Each row contains two result images and the source images of the concepts that they contain.",
                "position": 1321
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x17.png",
                "caption": "Figure 17:Qualitative results.Each row contains two result images and the source images of the concepts that they contain.",
                "position": 1326
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x18.png",
                "caption": "Figure 18:Qualitative results.Each row contains two result images and the source images of the concepts that they contain.",
                "position": 1331
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x19.png",
                "caption": "Figure 19:Storytelling results.Demonstration of our method‚Äôs usability for storytelling applications. All the characters, scenes, and poses featured in the story are shown on the left. On the right is the story itself, generated by a language model (LLM). This story was then reprocessed by the LLM to generate prompts, which were used to create the accompanying images.",
                "position": 1336
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x20.png",
                "caption": "Figure 20:An example of the questions asked in the user study. Given a generated image the users are asked about its alignment with both the text and the input concepts",
                "position": 1341
            },
            {
                "img": "https://arxiv.org/html/2501.12224/x21.png",
                "caption": "Figure 21:Generated images used for concept isolation loss.The images were generated with the base Flux model according to the accompanying prompts.",
                "position": 1344
            }
        ]
    },
    {
        "header": "Appendix FLimitation Analysis",
        "images": []
    }
]