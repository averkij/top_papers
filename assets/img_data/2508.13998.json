[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13998/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13998/x2.png",
                "caption": "Figure 1:The Embodied-R1 framework for zero-shot robotic manipulation through “pointing”.Embodied-R1 takes visual and textual instructions, performs explicit reasoning, and then generates a visual trace as a universal command. The other panel showcases our comprehensive evaluation, including spatial reasoning, embodied pointing benchmarks, and real-world robot tasks.",
                "position": 86
            },
            {
                "img": "https://arxiv.org/html/2508.13998/x3.png",
                "caption": "Figure 2:Overview of four embodied pointing abilities.",
                "position": 93
            }
        ]
    },
    {
        "header": "2Embodied-R1: Advancing Embodied Reasoning from RFT",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13998/x4.png",
                "caption": "Figure 3:Overview of training data: In stage 1, we focus on improving the model’s spatial reasoning capability, while incorporating a small amount of general reasoning data. In stage 2, we train the model’s embodied pointing capabilities, which comprise four distinct capability items.",
                "position": 127
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13998/x5.png",
                "caption": "Figure 4:Visualizing Embodied-R1’s Performance on Various Pointing Tasks.The model can follow diverse text instructions and generalize its capabilities to novel, unseen environments.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2508.13998/x6.png",
                "caption": "Figure 5:The process of Embodied-R1 performing real-world tasks.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2508.13998/x7.png",
                "caption": "Figure 6:The process of Embodied-R1 performing Task 6 under different visual disturbances.",
                "position": 1127
            },
            {
                "img": "https://arxiv.org/html/2508.13998/x8.png",
                "caption": "Figure 7:Case Analysis:Embodied-R1 possesses embodied reasoning capabilities. It can progressively locate relevant objects and infer spatial relationships according to task instructions, and ultimately provide coordinates through pointing based on embodied scene analysis.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2508.13998/x9.png",
                "caption": "Figure 8:Embodied-R1 exhibits strong generalization capabilities.We specifically designed tests using VTG tasks in entirely unseen scenarios (simulation, novel embodiment, and hand-drawn sketches), where the model must reason about objects in the images based on common knowledge:(left)LIBERO and ManiSkill simulator tasks,(middle)AhaRobot dual-arm robot tasks, and(right)human-drawn sketches.",
                "position": 1175
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAutomatic Data Generation Pipeline",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details of Embodied-R1",
        "images": []
    },
    {
        "header": "Appendix CEmbodied-R1 Prompts for Each Task",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13998/x10.png",
                "caption": "Figure 9:Qualitative comparison of Embodied-R1 and the SFT baseline. Our model Embodied-R1, leverages chain-of-thought reasoning (middle column) to generate a successful visual trace (left column). In contrast, the SFT baseline, which lacks an explicit reasoning process, produces incorrect trajectories (right column) for the same tasks.",
                "position": 2486
            },
            {
                "img": "https://arxiv.org/html/2508.13998/x11.png",
                "caption": "Figure 10:Visualizing Embodied-R1’s Prediction on VTG Tasks across Various Scenarios",
                "position": 2492
            }
        ]
    },
    {
        "header": "Appendix ELimitation and Future Work",
        "images": []
    }
]