[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01426/x1.png",
                "caption": "Figure 1:Different visual experts exhibit individual strengths.We show some examples where one single encoder model is the only model to correctly answer Perception Test questions(Pătrăucean et al.,2023).",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3MERV: Multi-Encoder Representation of Videos",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01426/x2.png",
                "caption": "Figure 2:Overview of MERV, a Multi-Encoder Representation of Videos.MERV proceeds in three main stages.\nFirst, we feed in our input video into each of visual encoders to get different representations.\nThey are then spatio-temporally aligned before being fused by a cross-attentive mixer.\nThe output is a visual embedding with an additive mix of information from all of the encoders, which is combined with the text query to produce our final generation.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01426/x3.png",
                "caption": "Figure 3:Extra encoders incur minimal step time overhead.\nHere we add encoders in the order of DINOv2, LanguageBind, SigLIP, ViViT,\nplotted alongside the slowest single encoder in each group.",
                "position": 894
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01426/x4.png",
                "caption": "(a)Visual Encoder Subsets.MERV outperforms single-encoder VideoLLMs, with our feature projectors unlocking more computational effiency. Removing any encoder also reduces MERV performance.",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x4.png",
                "caption": "(a)Visual Encoder Subsets.MERV outperforms single-encoder VideoLLMs, with our feature projectors unlocking more computational effiency. Removing any encoder also reduces MERV performance.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x5.png",
                "caption": "(b)SSv2-MCQ and Temporal.Temporal denotes performance on 12 selected classes where actions are indistinguishable if played in reverse.",
                "position": 951
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x6.png",
                "caption": "Figure 6:Single-Encoder Performance Difference in Something-Something v2 - MCQ. ViViT shows better performance on tasks where temporal understanding is crucial, while LanguageBind and SigLIP show better performance where task can be solved from single-frame understanding.",
                "position": 1026
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix / supplemental material",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01426/x7.png",
                "caption": "Figure 7:Samples of MERV in video understanding",
                "position": 2106
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x8.png",
                "caption": "",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x9.png",
                "caption": "",
                "position": 2110
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x10.png",
                "caption": "",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x11.png",
                "caption": "",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x12.png",
                "caption": "",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x13.png",
                "caption": "Figure 8:MERV examples. MERV tend to show improved understanding in temporal-heavy videos as in Something-Something v2 dataset(Goyal et al.,2017)(Top Row), while retaining the performance on scenic understanding, seen from popular video benchmarks(Xu et al.,2017; Yu et al.,2019)(Bottom Row).",
                "position": 2121
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x14.png",
                "caption": "",
                "position": 2125
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x15.png",
                "caption": "",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x16.png",
                "caption": "",
                "position": 2130
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x17.png",
                "caption": "",
                "position": 2135
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x18.png",
                "caption": "",
                "position": 2135
            }
        ]
    },
    {
        "header": "Appendix BTraining Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.01426/x19.png",
                "caption": "Figure 9:Example video of Something-Something V2. We see that ViViT show better performance in classes where temporal movement is critical for solving the task (Top row), while SigLIP performs better when the action can be inferred from the image without temporal information (Bottom row).",
                "position": 2944
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x20.png",
                "caption": "Figure 10:Samples of MERV in SSv2.Due to our design, our method shows better temporal action understanding than other VideoLLMs. (Top two rows) However, due to the difficulty of the task, we see failure cases for VideoLLMs. (Bottom two rows)",
                "position": 2948
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x21.png",
                "caption": "",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x22.png",
                "caption": "",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x23.png",
                "caption": "",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x24.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x25.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x26.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x27.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x28.png",
                "caption": "Figure 11:Example VideoLLM output on Something-Something v2.\nWhile SigLIP performs better on object and scene recognition, it fails to understand temporal actions. ViViT fails on the details of object recognition, but has better understanding in temporal movements.",
                "position": 3122
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x29.png",
                "caption": "",
                "position": 3131
            },
            {
                "img": "https://arxiv.org/html/2501.01426/x30.png",
                "caption": "",
                "position": 3136
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_0.png",
                "caption": "Figure 12:Videos that give the highest attention weight for each of the encoders.The right-most column shows the average frame of the video.",
                "position": 3156
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_1.png",
                "caption": "",
                "position": 3159
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_2.png",
                "caption": "",
                "position": 3160
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_3.png",
                "caption": "",
                "position": 3161
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_4.png",
                "caption": "",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_5.png",
                "caption": "",
                "position": 3163
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_6.png",
                "caption": "",
                "position": 3165
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_7.png",
                "caption": "",
                "position": 3166
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_8.png",
                "caption": "",
                "position": 3167
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_9.png",
                "caption": "",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_10.png",
                "caption": "",
                "position": 3169
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_11.png",
                "caption": "",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_12.png",
                "caption": "",
                "position": 3172
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_13.png",
                "caption": "",
                "position": 3173
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_14.png",
                "caption": "",
                "position": 3174
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_15.png",
                "caption": "",
                "position": 3175
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_16.png",
                "caption": "",
                "position": 3176
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_17.png",
                "caption": "",
                "position": 3177
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_18.png",
                "caption": "",
                "position": 3179
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_19.png",
                "caption": "",
                "position": 3180
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_20.png",
                "caption": "",
                "position": 3181
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_21.png",
                "caption": "",
                "position": 3182
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_22.png",
                "caption": "",
                "position": 3183
            },
            {
                "img": "https://arxiv.org/html/2501.01426/extracted/6102918/figures/appendix/attention_weight/aw_23.png",
                "caption": "",
                "position": 3184
            }
        ]
    },
    {
        "header": "Appendix CAdditional Analysis",
        "images": []
    }
]