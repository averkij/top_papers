[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00999/x1.png",
                "caption": "Figure 1:MergeVQ learning paradigms.(a)The MergeVQ Tokenizer extractsKùêæKitalic_Ksemantic tokens with decoupled positional information (retained in the source matrix) by ToMe[7]while quantizing spatial details by LFQ[49,73], which will be recovered and reconstructed correspondingly.(b)MergeVQ with random-order Generator[51]generatesKùêæKitalic_Kdiscrete tokens with associated position instructions while trained Source Prediction and decoder restore position details.(c)MergeAR Generator predictsLùêøLitalic_Ltokens efficiently in a raster-order with tailored KV Cache compression to remove the redundancy within Next-token Prediction (NTP)[57].",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00999/x2.png",
                "caption": "Figure 2:Overview of MergeVQ framework, which contains two stages and three groups of subtasks (Sec.3.1).(a)As for representation learning (Sec.3.2),KùêæKitalic_Ksemantic tokens are extracted by the encoder with self-attention and token merging[7], which can be aligned globally with a pre-trained teacher while learning contextual information by predicting the source matrix.(b)As for reconstruction (Sec.3.3), takingKùêæKitalic_Kmerged and quantized tokens as the input, the positional information can be retained by the Source Recovery module, and then high-quality details will be reconstructed.(c)As for generation (Sec.4), we utilize the source matrix to construct a causal mask for training and leverage the KV cache to prune repeated tokens during inference for efficient generation.",
                "position": 155
            }
        ]
    },
    {
        "header": "3MergeVQ Learning Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00999/x3.png",
                "caption": "Figure 3:Analysis of kept tokens in reconstruction and representation learning. Three MergeVQ tokenizers are trained with128128128128resolution for 30 epochs on ImageNet-1K.\nThey keep 256, 144, and 36 tokens with ToMe[7]in the encoder during training. In inference, we evaluate rFID and linear probing top-1 accuracy with diverse merge ratios to show the trade-off between generation and representation. Please view Sec.5and AppendixBfor details.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2504.00999/x4.png",
                "caption": "Figure 4:Visualization of MergeVQ (G+R) reconstruction. With the kept tokens varying from 64 to 256, clustering maps of ToMe Attention indicate that MergeVQ can extract discriminative semantic tokens while recovering contextual positions and details.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2504.00999/x5.png",
                "caption": "Figure 5:Distribution of merge ratios sampling in training. (a) With 256 tokens in total, MergeVQ (R) and (G+R) sample the square number as kept token numbers in[36,100]36100[36,100][ 36 , 100 ]and[121,225]121225[121,225][ 121 , 225 ]with exponential and Gaussian distributions for stage-1 training, while the G+R version sampling from[144,256]144256[144,256][ 144 , 256 ]for stage-2 training.\n(b) With 1024 tokens in total, MergeVQ (G) samples the square kept number in[225,400]225400[225,400][ 225 , 400 ]and[256,1024]2561024[256,1024][ 256 , 1024 ]with Gaussian and exponential distributions in both stage-1 and stage-2 training.",
                "position": 364
            }
        ]
    },
    {
        "header": "4MergeVQ for Efficient Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00999/x6.png",
                "caption": "Figure 6:Illustration of MergeAR pipeline.(a)MergeAR training with the source matrix andKùêæKitalic_K-sparse target sequences from the MergeVQ tokenizer to build a causal mask with duplicated tokens masked out, taking a class token and a merge instruction token as the starting conditions.(b)MergeAR inference that generatesLùêøLitalic_Ltokens in the raster order with duplicated tokens detected and removed in the position and KV Caches.",
                "position": 404
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.00999/x7.png",
                "caption": "Figure A1:Visualization of tokenizer reconstruction on ImageNet-1K.We conducted reconstruction experiments with our G version using 1024, 576, 400, 256, and 144 tokens and with our G+R version using 256, 196, 144, 100, 64, and 36 tokens. The reconstruction results are shown in the figure. As the number of retained tokens increases, the reconstruction becomes more realistic.",
                "position": 3282
            },
            {
                "img": "https://arxiv.org/html/2504.00999/x8.png",
                "caption": "Figure A2:Visualization of class conditional generationwith MergeVQ variants on ImageNet-1K. The G version performs generation on 256 tokens, and the G+R version performs generation on 144 tokens.",
                "position": 3287
            }
        ]
    },
    {
        "header": "Appendix BMore Experiment Results",
        "images": []
    }
]