[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07316/x1.png",
                "caption": "Figure 1:Overview of our training data construction: Raw code files are gathered from various sources and converted into a unified format. Input-output pairs are then generated by executing the code, while natural language CoTs for predictions are collected from DeepSeek-V2.5. The verified CoTs can undergo optional revisions to further enhance reasoning chains.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2502.07316/x2.png",
                "caption": "Figure 2:Two examples for the collected responses for input and output prediction respectively.",
                "position": 158
            }
        ]
    },
    {
        "header": "2CodeI/O",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07316/x3.png",
                "caption": "Figure 3:Average scores of Stage 1 training onCodeI/O, a 3.5M WebInstruct subset (WI) and an enhanced version distilled from DeepSeek-V2.5 Directly (WI-DS25).",
                "position": 1037
            },
            {
                "img": "https://arxiv.org/html/2502.07316/x4.png",
                "caption": "(a)Size of randomly sampled subset.",
                "position": 1051
            },
            {
                "img": "https://arxiv.org/html/2502.07316/x4.png",
                "caption": "(a)Size of randomly sampled subset.",
                "position": 1054
            },
            {
                "img": "https://arxiv.org/html/2502.07316/x5.png",
                "caption": "(b)Ratio of testcases per sample compared to the full set.",
                "position": 1059
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07316/x6.png",
                "caption": "Figure 5:Average benchmark scores from training on data from different turns of revision.",
                "position": 1244
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Checking Execution Complexity",
        "images": []
    },
    {
        "header": "Appendix BDetails of Selected Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CDetails of Processing Different Data Sources",
        "images": []
    },
    {
        "header": "Appendix DDetailed Statistics in Multi-turn Revision",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.07316/x7.png",
                "caption": "(a)Input prediction revision.",
                "position": 2383
            },
            {
                "img": "https://arxiv.org/html/2502.07316/x7.png",
                "caption": "(a)Input prediction revision.",
                "position": 2386
            },
            {
                "img": "https://arxiv.org/html/2502.07316/x8.png",
                "caption": "(b)Output prediction revision.",
                "position": 2391
            }
        ]
    },
    {
        "header": "Appendix ETraining Hyper-parameters",
        "images": []
    },
    {
        "header": "Appendix FThe Effect of Using Other Instruction-Tuning Data",
        "images": []
    },
    {
        "header": "Appendix GExamples Mentioned in the Main Text",
        "images": []
    }
]