[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/control_llm_sota_comparison.png",
                "caption": "Figure 1:Comparison: Ours vs SOTA Llama-tuned models.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/catastrophic_forgetting_openmath.png",
                "caption": "Figure 2:[Result] Comparison of CF - our method vs others on open-source datasets:(left)OpenMath,(right)OpenCoder.",
                "position": 163
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/catastrophic_forgetting_openmath.png",
                "caption": "",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/catastrophic_forgetting_opencoder.png",
                "caption": "",
                "position": 170
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_comparison.png",
                "caption": "Figure 3:[Why] Hidden State Alignment Comparison:Best Alignment(Control LLM)vsWorst Alignment(Full-Parameter Tuning).",
                "position": 604
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/control_llm_architecture.png",
                "caption": "Figure 4:[How] Control LLM Architecture.(a)Expanded blocks added everyN‚àí1ùëÅ1N-1italic_N - 1layers connect to frozen blocks via interpolators.(b)Interpolators align hidden-states to produce final representations.(c)Different interpolation strategies are explored.",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/control_llm_structure_analysis.png",
                "caption": "Figure 5:[How] Structure analysis:(concat)the default dual structure.(stack)stack the expanded block following LLaMA Pro.(hybrid)hybrid structure of concat and stack.",
                "position": 858
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Discussion, Future Work, and Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe effectiveness of Control LLM in addressing Catastrophic Forgetting",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Math.png",
                "caption": "Figure 6:[OpenMath + Llama-3.1-8B-Instruct]Benchmark comparison of training methods.(a)Full Parameter Tuning.(b)Partial Parameter Tuning: Freeze all except 1 of every 4 transformer layers (8 total).(c)Stack Expansion: Add 8 transformer layers (1 per 4) while freezing originals.(d)Concat-Lerp Expansion: Add 8 transformer layers connected via interpolator with MSE divergence loss.",
                "position": 2201
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Code.png",
                "caption": "Figure 7:[OpenCoder SFT Phase2 + Llama-3.1-8B-Instruct]Benchmark comparison of training methods.(a)Full Parameter Tuning.(b)Partial Parameter Tuning: Freeze all except 1 of every 4 transformer layers (8 total).(c)Stack Expansion: Add 8 transformer layers (1 per 4) while freezing originals.(d)Concat-Lerp Expansion: Add 8 transformer layers connected via interpolator with MSE divergence loss.",
                "position": 2204
            }
        ]
    },
    {
        "header": "Appendix BHidden State Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_pretrain.png",
                "caption": "(a)Pre-trained Model - Math Hard 0.237 - MMLU 0.724",
                "position": 2449
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_pretrain.png",
                "caption": "(a)Pre-trained Model - Math Hard 0.237 - MMLU 0.724",
                "position": 2452
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_best.png",
                "caption": "(b)Lerp with MSE - Math Hard 0.360 - MMLU 0.716",
                "position": 2457
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_good.png",
                "caption": "(c)Dlerp without Divergence Loss - Math Hard 0.357 - MMLU 0.66",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_cosine_loss_euclidean.png",
                "caption": "(d)Lerp with Cosine Alignment - Math Hard 0.362 - MMLU 0.54",
                "position": 2468
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_worse.png",
                "caption": "(e)Lerp without Divergence Loss - Math Hard 0.359 - MMLU 0.41",
                "position": 2474
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/alignment_worst.png",
                "caption": "(f)Full Parameter Training - Math Hard 0.368 - MMLU 0.07",
                "position": 2479
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Ablation_Study_Math.png",
                "caption": "Figure 9:[OpenMath2 + Llama-3.1-8B-Instruct]Comparison of benchmark results of different ablation study settings every 1K steps during training.(a)Lerp Interporation Strategy with MSE loss.(b)Lerp Interporation Strategy without Divergence Loss.(c)Dlerp Interporation Strategy with MSE loss.(d)Dlerp Interporation Strategy without Divergence Loss.(e)Hybrid Expansion Strategy.(e)Plerp Interporation Strategy.(f)DlerpIn Interporation Strategy.(e)MoE gating.",
                "position": 3017
            },
            {
                "img": "https://arxiv.org/html/2501.10979/extracted/6135550/ControlLLM_CF_Plot_Ablation_Study_Code.png",
                "caption": "Figure 10:[OpenCoder + Llama-3.1-8B-Instruct]Comparison of benchmark results of different ablation study settings during training.(a)Lerp Interporation Strategy with MSE Loss.(b)Lerp Interporation Strategy with MSE Loss. Trained 3X longer than (a)(c)Hybrid Expansion Strategy.(d)MoE gating.",
                "position": 3020
            }
        ]
    },
    {
        "header": "Appendix EAcknowledgments",
        "images": []
    }
]