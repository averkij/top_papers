[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22047/logo/tongyi.jpg",
                "caption": "",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x1.png",
                "caption": "Figure 1:MAI-UI achieves SOTA performance across GUI grounding and navigation benchmarks.",
                "position": 236
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2MAI-UI",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22047/x2.png",
                "caption": "Figure 2:Example trajectory of MAI-UI. MAI-UI completes GUI agent tasks via both UI operations and extended actions, including agent-user interaction and MCP tool use, and integrates a native device-cloud collaboration system.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x3.png",
                "caption": "Table 1:Action Space in MAI-UI.",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x3.png",
                "caption": "Figure 3:Overview of grounding and perception data pipeline",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x4.png",
                "caption": "Figure 4:Overview of the self-evolving data pipeline for trajectory synthesis. The pipeline comprises task generation, trajectory construction via human annotation and autonomous agent rollouts, and iterative rejection sampling that jointly evolve the model and the training corpus.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x5.png",
                "caption": "Figure 5:Overview of the agentic reinforcement learning framework. The framework alternates between rollout phases where the latest policy interacts with online mobile environments to generate trajectories and training phases that progressively improve the policy using trajectory-level rewards.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x6.png",
                "caption": "Figure 6:Detailed rollout process within the MobileAgentLoop. The agent loop asynchronously calls inference servers to generate actions and executes them in stateful environments across multiple turns, with hybrid verifiers evaluating complete trajectories to produce final rewards.",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x7.png",
                "caption": "Figure 7:Overview of device-cloud collaboration architecture. The system adaptively routes computation between device and cloud models based on task context and data sensitivity.",
                "position": 707
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22047/x8.png",
                "caption": "(a)",
                "position": 5410
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x8.png",
                "caption": "(a)",
                "position": 5413
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x9.png",
                "caption": "(b)",
                "position": 5419
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x10.png",
                "caption": "Figure 9:A case study of agent user interaction. The user instruction is:“In the Downloads folder, locate resume file(s) downloaded within one month and send them to my HR colleague with the subject “candidates_cv.”.",
                "position": 5429
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x11.png",
                "caption": "Figure 10:The native device–cloud collaboration (DCC) capability significantly improves the on-device model’s online performance, surpassing the random switch (RS) baseline, w/o error summary (ES) baseline, and several larger pure‑cloud models by a substantial margin. Our native DCC system also improves efficiency, executing 42.7% of steps locally and completing 40.5% of tasks entirely on‑device, thereby reducing cloud calls.",
                "position": 5447
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x12.png",
                "caption": "Figure 11:A pilot study demonstrating privacy protection in the device-cloud collaboration system. At step 3, the local agent deviates, as it repeatedly tapping the Login button without entering a password. The trajectory monitor flags the misalignment and proposes switch to the cloud agent. However, the privacy detection module detects sensitive credentials and blocks the switch, keeping execution on device. The local monitor ultimately corrects the trajectory and completes the task.",
                "position": 5450
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x13.png",
                "caption": "(a)",
                "position": 5551
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x13.png",
                "caption": "(a)",
                "position": 5554
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x14.png",
                "caption": "(b)",
                "position": 5559
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x15.png",
                "caption": "(a)",
                "position": 5598
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x15.png",
                "caption": "(a)",
                "position": 5601
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x16.png",
                "caption": "(b)",
                "position": 5606
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22047/x17.png",
                "caption": "Figure 14:MAI-UI shows robustness in recovering from recovering from failures: the agent initially navigates to the wrong application, but MAI-UI correct the trajectory and complete the task.",
                "position": 5650
            },
            {
                "img": "https://arxiv.org/html/2512.22047/x18.png",
                "caption": "Figure 15:Grounding case studies across different operating systems.",
                "position": 5659
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAdditional Grounding Results",
        "images": []
    }
]