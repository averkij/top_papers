[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09245/x1.png",
                "caption": "Figure 1:Training loss per FLOPs for Llama, Static LIMe, and Dynamic LIMe. LIMe has a substantially lower loss with a similar amount of FLOPs. See Section5.1for more details.",
                "position": 146
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09245/x2.png",
                "caption": "Figure 2:Mean retrieval weight for each representation (mùëömitalic_m) among later layers (rùëüritalic_r). In both cases, in the last layers, models tend to retrieve information from previous layers rather than from the current one. In the case of Dynamic LIMe, there is a clear bump in retrieving from the first layer. See Section5.2for more details.",
                "position": 429
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09245/x3.png",
                "caption": "Figure 3:Self Retrieval weights for each head of Static and Dynamic LIMe. Both models assign higher weights to the latest representation in the middle layers, but tend to retrieve lower-level features later. The depicted weights decrease significantly in almost all heads, although some of them still use self-retrieval paths, suggesting the outputs‚Äô refinement stage. Moreover, we can see that Dynamic LIMe‚Äôs first layers heavily rely on low-level features due to their sequence conditioning. See Section5.2for more details.",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x4.png",
                "caption": "Figure 4:Expected retrieved representation for each LIMe layer (rùëüritalic_r). Both static and dynamic variants tend to retrieve information from early layers. See Section5.2for more details.",
                "position": 472
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x5.png",
                "caption": "Figure 5:Values‚Äô matrix entropy on FineWeb Edu subset by layers. Both Dynamic and Static LIMe have more diverse values than LLaMa, which indicates more information stored in LIMe.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x6.png",
                "caption": "Figure 6:t-SNE of similar tokens‚Äô values among layers. Values obtained from LIMe are separable in later layers, while values in LLaMA become mixed and lose information about tokens. See Section5.3for more details.",
                "position": 510
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x7.png",
                "caption": "Figure 7:Values classification accuracy measured with standard deviation over 5 cross-validation folds. Values in later layers obtained from LIMe can be linearly separated with nearly 1.0 accuracy, while accuracy for values from LLaMA is much lower. See Section5.3for more details.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x8.png",
                "caption": "Figure 8:Training losses for deep architectures. The LIMe architecture significantly outperforms the baseline, especially in the case of128128128128layers. See Section5.4for more details.",
                "position": 648
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x9.png",
                "caption": "Figure 9:Retrieval weights statistics for a 128-layer LIMe model trained with top-pùëùpitalic_ppruning. The mean retrieval weight from subsequent layers (green curve) displays several distinct peaks, indicating that the model acquires multiple information streams in a self-supervised fashion. The mean self-retrieval weight (orange curve), where 1.0 denotes self-attention, decreases across later layers, forming three consecutive layer groups with different information-processing patterns. See Section5.4for further details.",
                "position": 668
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "7Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Setup",
        "images": []
    },
    {
        "header": "Appendix BPruning details",
        "images": []
    },
    {
        "header": "Appendix CAdditional LIMe Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.09245/x10.png",
                "caption": "Figure 10:Self Retrieval weights averaged across heads for each LIMe layer.",
                "position": 1191
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x11.png",
                "caption": "Figure 11:Hiddens‚Äô matrix entropy on FineWeb Edu subset by layers. We can see that hidden states in LIMe can be not very diverse for the model to provide better performance on language tasks. For details, see Section5.3.",
                "position": 1194
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x12.png",
                "caption": "Figure 12:Hidden states classification accuracy measured with standard deviation over 5 cross-validation folds. Although LLaMa‚Äôs deeper layers maintain stronger linear separability, LIMe‚Äôs hidden states become slightly harder to cluster in later layers due to its ability to smoothly move on to predicting the next token using the full hidden states‚Äô dimensionality.",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x13.png",
                "caption": "Figure 13:Learned static weights and dynamic prior distribution calculated on a subset of Fineweb Edu. Each cell represents retrieval probability for each layer in the specific head.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x14.png",
                "caption": "Figure 14:All weights for deep static LIMe with 128 layers. We can see explicitly the repeated routing patterns resembling a refinement process.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2502.09245/x15.png",
                "caption": "Figure 15:t-SNE of similar tokens‚Äô hidden states among layers. Although hidden states are not separable in later layers for both models, unlike LLaMA, LIMe can make updates attending to the previous representations, which leads to high values‚Äô separability. See Section5.3for more details.",
                "position": 1206
            }
        ]
    },
    {
        "header": "Appendix DEfficiency",
        "images": []
    },
    {
        "header": "Appendix ELIMe Pseudocode",
        "images": []
    }
]