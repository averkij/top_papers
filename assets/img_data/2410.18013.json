[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18013/x1.png",
                "caption": "Figure 1:Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improvesprompt followingandvisual qualityfor SDXL(Podell et¬†al.,2023)and SD3-Medium(Esser et¬†al.,2024), without requiring any manual annotations.",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18013/x2.png",
                "caption": "Figure 2:Overview of our two novel components: (A)Syn-Picand (B)RankDPO.Leftillustrates the pipeline to generate a synthetically ranked preference dataset. It starts by collecting prompts and generating images using the same prompt for different T2I models. Next, we calculate the overall preference score using Reward models (e.g., PickScore, ImageReward). Finally, we rank these images in the decreasing order of preference scores.Right: Given true preference rankings for generated images per prompt, we first obtain predicted ranking by current model checkpoint using scoresùê¨isubscriptùê¨ùëñ{\\mathbf{s}}_{i}bold_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT(see Eq.5). In this instance, although the predicted ranking is inverse of the true rankings, the ranks(1,4)14(1,4)( 1 , 4 )obtains a larger penalty than the ranks(2,3)23(2,3)( 2 , 3 ). This penalty is added to our ranking loss through DCG weights (see Eq.6). Thus, by optimizingùúΩùúΩ\\bm{\\theta}bold_italic_Œ∏with Ranking Loss (see Eq.7), the updated model addresses the incorrect rankings(1,4)14(1,4)( 1 , 4 ). This procedure is repeated over the training process, where the rankings induced by the model aligns with the labelled preferences.",
                "position": 236
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18013/x3.png",
                "caption": "Figure 3:Win rates of our approach compared to DPO-SDXL and SDXL on human evaluation.",
                "position": 565
            },
            {
                "img": "https://arxiv.org/html/2410.18013/x4.png",
                "caption": "Figure 4:Comparison among different preference optimization methods andRankDPOfor SDXL. The results illustrate that we generate images with better prompt alignment and aesthetic quality.",
                "position": 800
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18013/x5.png",
                "caption": "Figure 5:Comparison among different preference optimization methods andRankDPOfor SDXL. The results illustrate that we generate images with better prompt alignment and visual quality.",
                "position": 2361
            },
            {
                "img": "https://arxiv.org/html/2410.18013/x6.png",
                "caption": "Figure 6:Qualitative comparison between SDXL, before and after our preference-tuning. The results show that our method generates images with better prompt alignment and aesthetic quality.",
                "position": 2364
            },
            {
                "img": "https://arxiv.org/html/2410.18013/x7.png",
                "caption": "Figure 7:Qualitative comparison between SD3-Medium, before and after our preference-tuning. The results show that our method generates images with better prompt alignment and aesthetic quality.",
                "position": 2367
            }
        ]
    },
    {
        "header": "Anexo AAppendix",
        "images": []
    }
]