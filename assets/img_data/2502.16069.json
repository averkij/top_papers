[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16069/x1.png",
                "caption": "Figure 1:Curie overview.",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/case_study.png",
                "caption": "Figure 2:Case Study.Curiecan help researchers validate, expand, and critique existing research on the benefits of repeated sampling in LLM reasoning(Brown et al.,2024).\nThe first panel (Original Finding) presents a result from the original paper.\nThe second panel (Reproduce) hasCurieconfirming this finding through rigorous experimentation.\nThe third panel (Extend) hasCurieexploring the impact of sampling temperature on repeated sampling.\nThe final panel (Challenge) showsCurieidentifying a limitation in the original methodology, suggesting an avenue for future research.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/workflow.png",
                "caption": "Figure 3:Curieworkflow with an example task in LLM reasoning.\nThe Architect is responsible for designing high-level plans and reflects on the new findings.\nThe Technician is responsible for implementing and executing the experiments based on the plans.\nWhenever an agent completes its action (step1,2,3,4,5), the Experimental Rigor Engine (stepsA⇀⇀\\rightharpoonup⇀B⇀⇀\\rightharpoonup⇀C)\nvalidates the action, determines next steps, assigns tasks and maintains interpretable experimental progress, ensuring rigor throughout the entire process.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Curie: Rigorous Experimentation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/intra-arm.png",
                "caption": "Figure 4:Intra-ARMsetup validation high-level workflow.",
                "position": 299
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/setup-validator-examples.png",
                "caption": "(a)Example errors that can be captured by the setup validator.",
                "position": 302
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/setup-validator-examples.png",
                "caption": "(a)Example errors that can be captured by the setup validator.",
                "position": 305
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/exec-validator-examples.png",
                "caption": "(b)Example errors that can be captured by the execution validator.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2502.16069/x2.png",
                "caption": "Figure 6:SimplifiedInter-ARMworkflow with a partition state snapshot. Partition, control flow, and scheduling policies are customizable.",
                "position": 329
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/time-machine.png",
                "caption": "Figure 7:Simplified partial snapshot of an example Time Machine.",
                "position": 399
            }
        ]
    },
    {
        "header": "4Experimentation Benchmark",
        "images": []
    },
    {
        "header": "5Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16069/x3.png",
                "caption": "Figure 8:Average scores across different complexity dimensions at varying difficulty levels forCurie, OpenHands, and Magentic.Curieoutperforms the others consistently, with performance generally dropping as complexity increases.",
                "position": 741
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACurie Benchmark Complexity Explanation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/q6.png",
                "caption": "(a)Question 6: “Does the optimal number of reasoning steps vary across different LLMs?”",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/q6.png",
                "caption": "(a)Question 6: “Does the optimal number of reasoning steps vary across different LLMs?”",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2502.16069/extracted/6224150/figures/q8.png",
                "caption": "(b)Question 8: “What is the relationship between the complexity of a task (e.g., as measured by the number of logical inferences or mathematical operations needed) and the optimal length of the reasoning chain?”",
                "position": 1779
            }
        ]
    },
    {
        "header": "Appendix BCase Studies forCurie",
        "images": []
    },
    {
        "header": "Appendix CExtended Evaluation: Fine-grained Performance Breakdown by Individual Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.16069/x4.png",
                "caption": "Figure 10:Average alignment scores across different complexity dimensions at varying difficulty levels forCurie, OpenHands, and Magentic.Curieoutperforms the others consistently, with performance generally dropping as complexity increases.",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2502.16069/x5.png",
                "caption": "Figure 11:Average conclusion scores across different complexity dimensions at varying difficulty levels forCurie, OpenHands, and Magentic.Curieoutperforms the others consistently, with performance generally dropping as complexity increases.",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2502.16069/x6.png",
                "caption": "Figure 12:Average design scores across different complexity dimensions at varying difficulty levels forCurie, OpenHands, and Magentic.Curieoutperforms the others consistently, with performance generally dropping as complexity increases.",
                "position": 1821
            }
        ]
    },
    {
        "header": "Appendix DBenchmark Details.",
        "images": []
    },
    {
        "header": "Appendix EExperimental Setup Details",
        "images": []
    }
]