[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01720/x1.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01720/x2.png",
                "caption": "Figure 1:Overview of our Data Construction Pipeline. Our pipeline has two parallel tracks.Left: The local editing trackperforms object Swap and Removal. For swapping, we use target objects and captions from the source video to generate edits with erosion masks, followed by a quality filtering step. For removal, captions are constructed and paired with bounding-box masks to generate the edited videos. Notably, filtered samples are used to refine our VACE[11]model, which then regenerates the entire removal subset for higher quality (Sec.3.1).Right: The global stylization trackfirst generates source videos from images using Wan-I2V. It then combines these source videos, style reference images, and corresponding depth videos to produce high-fidelity stylized results (Sec.3.2).",
                "position": 183
            }
        ]
    },
    {
        "header": "3Scalable FFP Data Construction Pipeline",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01720/x3.png",
                "caption": "Figure 2:Overview of training paradigm.Left: The source video and edited frame are encoded. The source latent informs our AST-RoPE module for adaptive spatio-temporal scaling. Right: The target video is processed identically to extract a latent DiT embedding, which is used to align the generation process.",
                "position": 291
            }
        ]
    },
    {
        "header": "5Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01720/x4.png",
                "caption": "Figure 3:Qualitative comparison.We Choose top three method in quantitative comparison to compare with our visual results across four representative video editing tasks. Red boxes highlight the unreasonable generated contents. The gray placeholder denotes these methods cannot generate such long videos. Our method generally enjoys better editing fidelity, temporal consistency and visual quality.",
                "position": 653
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Additional Information about FFP-300K",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01720/figures/object_wordcloud.png",
                "caption": "Figure 4:Word cloud of edited objects of the local editing subst of FFP-300K.",
                "position": 1552
            },
            {
                "img": "https://arxiv.org/html/2601.01720/figures/scene_distribution_histogram.png",
                "caption": "Figure 5:Scene distribution of the local editing subset of FFP-300K",
                "position": 1560
            }
        ]
    },
    {
        "header": "8Additional Method Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.01720/x5.png",
                "caption": "Figure 6:The visualization of local editing track in FFP-300K.",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2601.01720/x6.png",
                "caption": "Figure 7:The visualization of global stylization track in FFP-300K.",
                "position": 1749
            },
            {
                "img": "https://arxiv.org/html/2601.01720/x7.png",
                "caption": "Figure 8:More results of local editing and global stylization tasks on EditVerseBench.",
                "position": 1768
            },
            {
                "img": "https://arxiv.org/html/2601.01720/x8.png",
                "caption": "Figure 9:More visual results in the landscape orientation on EditVerseBench.",
                "position": 1771
            },
            {
                "img": "https://arxiv.org/html/2601.01720/x9.png",
                "caption": "Figure 10:More visual results in the portrait orientation on EditVerseBench.",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2601.01720/x10.png",
                "caption": "Figure 11:Qualitative Comparison.We choose top three methods in quantitative comparison to compare with our-33f visual results across local editing and global stylization tasks.",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2601.01720/x11.png",
                "caption": "Figure 12:Visualization results of our method and UNIC on UNICBench for FFP-based video editing.",
                "position": 1780
            }
        ]
    },
    {
        "header": "9Additional Experiment Results",
        "images": []
    }
]