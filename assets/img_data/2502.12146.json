[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12146/x1.png",
                "caption": "Figure 1:Comparison of Three Diffusion-Based Methods for Reward-Driven Optimization: (i) Diffusion Reinforcement Learning, (ii) Diffusion Sampling Trajectory Optimization, and (iii) Diffusion Sharpening.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12146/x2.png",
                "caption": "Figure 2:Overview of Our Diffusion Sharpening Framework: (i) Training, (ii) Inference, and (iii) Reward Model Selection",
                "position": 221
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12146/x3.png",
                "caption": "Figure 3:Qualitative results comparing Diffusion Sharpening methods using different reward models. The images show the generated results with CLIP Score, Compositional Reward, MLLM, and Human Preferences as reward models, showcasing the effectiveness of SFT Diffusion Sharpening and RLHF Diffusion Sharpening in diffusion finetuning.",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2502.12146/x4.png",
                "caption": "Figure 4:SDXL Finetuning Loss across Difference Datasets. Here ”Diffusion-Sharpening” represents SFT Diffusion-Sharpening specifically.",
                "position": 749
            },
            {
                "img": "https://arxiv.org/html/2502.12146/x5.png",
                "caption": "Figure 5:Inference Performance of Diffusion Sharpening.",
                "position": 775
            },
            {
                "img": "https://arxiv.org/html/2502.12146/x6.png",
                "caption": "Figure 6:Diffusion Sharpening Fine-tuning Reward Curve.",
                "position": 787
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplemantation Details",
        "images": []
    },
    {
        "header": "Appendix BUser Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12146/x7.png",
                "caption": "Figure 7:User Study about Comparision with Other Methods",
                "position": 1608
            }
        ]
    },
    {
        "header": "Appendix CMLLM Grader Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12146/x8.png",
                "caption": "Figure 9:More Qualitative Results for SFT Diffusion-Sharpening.",
                "position": 1652
            },
            {
                "img": "https://arxiv.org/html/2502.12146/x9.png",
                "caption": "Figure 10:More Qualitative Results for RLHF Diffusion-Sharpening.",
                "position": 1655
            }
        ]
    },
    {
        "header": "Appendix DMore Qualitative Results",
        "images": []
    }
]