[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2TheEO-1Model and Training Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21112/x1.png",
                "caption": "Figure 1:EO-1 Model Architecture.EO-1model is a Vision-Language-Action (VLA) model that adopts a single unified decoder-only transformer, equipping with discrete language-modeling head for multimodal embodied reasoning and continuous flow-matching head for robot action generation. The language instruction, image observations, robot state, and noisy action are encoded into an interleaved token sequence of tokens to be processed by the shared transformer backbone, whose weights are initialized from Qwen2.5-VL. The model is trained on interleaved vision-text-action data with a combination of flow-matching objective and next-token-prediction objective and capable of seamless embodied reasoning and acting.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x2.png",
                "caption": "Figure 2:Interleaved rectifying sampling strategy.Our method samples variable-length subsequences from robot action generation segments, enabling efficient training of mixed-modality generation while preserving causal relationships.",
                "position": 333
            }
        ]
    },
    {
        "header": "3Dataset and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21112/x3.png",
                "caption": "Figure 3:(a) Statistics of EO-Robotics Dataset (EO-Data1.5M) and Benchmark (EO-Bench). (b) Dataset curation pipeline on robot episodes. (c) Interleaved Vision-Text-Action Data Example, including three interleaved formats to concatenate embodied reasoning QA pairs and robot control data along temporal order.",
                "position": 489
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x4.png",
                "caption": "Figure 4:Examples ofEO-Bench, including Multiview Pointing, Physical Common Sense, Trajectory Prediction, Process Verification, Task Planning, and Robot Affordance.",
                "position": 555
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21112/x5.png",
                "caption": "Figure 5:Example of real-world evaluation tasks in diverse robots, including Agibot G-1 Long-horizon Dexterous (row 1-4), Franka Panda Pick-and-Place (row 5), WidowX 250 S Out-of-Box (row 6), and Embodied Reasoning Control in Franka, Agibot G-1, and Lerobot SO100 (row 1,2,7,8).",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x6.png",
                "caption": "Figure 6:Performance comparison on diverse robot platforms and task categories.",
                "position": 1514
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x7.png",
                "caption": "Figure 7:Long-horizon dexterity completion rate comparison on the AgiBot G-1 platform.",
                "position": 1590
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x8.png",
                "caption": "Figure 8:Instruction-following in open-world settings.Left: Example scenes from three task types:Instructed Visual Rearrangement(Franka Panda),Sort Grocery Items(Agibot G-1), andWidowX Out-of-Box. Right: Success rates over 18 tasks.",
                "position": 1621
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x9.png",
                "caption": "(a)Examples of the three generalization axes.Visual:Changes in object appearance, background, and lighting.Action:Altered object positions, novel viewpoints, or new object instances.Language:Typos, rephrasings, and fuzzy descriptions.",
                "position": 1636
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x9.png",
                "caption": "(a)Examples of the three generalization axes.Visual:Changes in object appearance, background, and lighting.Action:Altered object positions, novel viewpoints, or new object instances.Language:Typos, rephrasings, and fuzzy descriptions.",
                "position": 1639
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x10.png",
                "caption": "(b)Generalization performance breakdown.Success rates forEO-1, GR00T-N1.5, andùùÖùüé\\pi_{0}across visual, action, and language variations.",
                "position": 1645
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x11.png",
                "caption": "Figure 10:Qualitative rollouts for unified reasoning and control.Each sequence shows howEO-1perceives the scene, reasons about next steps, and executes precise actions under a single interleaved policy: (a)Tic-Tac-Toe, (b)Visual Rearrangement, (c)Roast Beef Steak, (d)Make Breakfast Sandwich. Planning and execution remain aligned throughout, avoiding plan‚Äìact mismatches common in hierarchical pipelines.",
                "position": 1726
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x12.png",
                "caption": "Figure 11:Success rates on the Reasoning‚Äìcontrol benchmark.EO-1outperforms both hierarchical baselines on all tasks and on average, with the largest gains inTic-Tac-ToeandRoast Beef Steak.",
                "position": 1745
            },
            {
                "img": "https://arxiv.org/html/2508.21112/figures/ablation_demonstration.png",
                "caption": "Figure 12:WidowX generalization performance across data scales forEO-1(base),EO-1(interleaved), andEO-1(fast) with interleaved vision‚Äìtext‚Äìaction data.",
                "position": 1823
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContributors and Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix BTraining Dataset Statistics",
        "images": []
    },
    {
        "header": "Appendix CEO-1Reasoning Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.21112/x13.png",
                "caption": "Figure 13:Object pointing examples.",
                "position": 3094
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x14.png",
                "caption": "Figure 14:Trajectory prediction examples.",
                "position": 3097
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x15.png",
                "caption": "Figure 15:Object referring examples.",
                "position": 3100
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x16.png",
                "caption": "Figure 16:Multiview reasoning examples.",
                "position": 3103
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x17.png",
                "caption": "Figure 17:Task planning examples.",
                "position": 3106
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x18.png",
                "caption": "Figure 18:Subtask QA examples.",
                "position": 3109
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x19.png",
                "caption": "Figure 19:Episode caption examples.",
                "position": 3112
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x20.png",
                "caption": "Figure 20:Affordance QA examples.",
                "position": 3115
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x21.png",
                "caption": "Figure 21:Physical common sense examples.",
                "position": 3118
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x22.png",
                "caption": "Figure 22:Failure detection examples.",
                "position": 3121
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x23.png",
                "caption": "Figure 23:Process verification examples.",
                "position": 3124
            },
            {
                "img": "https://arxiv.org/html/2508.21112/figures/web_screenshot_cluster.jpg",
                "caption": "Figure 24:Web interface for video filtering and clustering.We extract features using a pretrained backbone and apply K-means clustering to group visually similar videos. Human verification is then used to remove redundant samples and ensure diversity.",
                "position": 3141
            },
            {
                "img": "https://arxiv.org/html/2508.21112/figures/web_screenshot_mulvw_selection.png",
                "caption": "Figure 25:Manual selection of multiview videos.All observations from different datasets are displayed, and the interface allows selecting samples with sufficient viewpoint diversity for multiview reasoning tasks.",
                "position": 3147
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x24.png",
                "caption": "Figure 26:Video caption prompt.",
                "position": 3157
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x25.png",
                "caption": "Figure 27:Clip caption prompt.",
                "position": 3160
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x26.png",
                "caption": "Figure 28:Task planning prompt.",
                "position": 3176
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x27.png",
                "caption": "Figure 29:Physical question prompt.",
                "position": 3179
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x28.png",
                "caption": "Figure 30:Trajectory prediction prompt.",
                "position": 3189
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x29.png",
                "caption": "Figure 31:Object pointing prompt.",
                "position": 3192
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x30.png",
                "caption": "Figure 32:Object referring prompt.",
                "position": 3195
            },
            {
                "img": "https://arxiv.org/html/2508.21112/x31.png",
                "caption": "Figure 33:Semantic deblurring prompt.",
                "position": 3203
            }
        ]
    },
    {
        "header": "Appendix DData Annotation Pipeline Details",
        "images": []
    }
]