[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10589/x1.png",
                "caption": "Figure 1:We propose Long Context Tuning (LCT) to expand the context window of pre-trained single-shot video diffusion models.\nA direct application of LCT is scene-level video generation for short film production, as shown in the top example.\nWe also show several emerging capabilities offered by LCT, such as interactive multi-shot direction and single shot extension, as well as zero-shot compositional generation, despite the model having never been trained on such tasks.We recommend referring to ourProject Pagefor better visualization.",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10589/x2.png",
                "caption": "Figure 2:Scene-level Video Data Example.Global promptcontains shared elements likecharacter,environment, andstory overview, whileper-shot promptdetails events in each shot.",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2503.10589/x3.png",
                "caption": "Figure 3:Architecture Designs.(a)Long-context MMDiT block.\nWe expand the attention operation to all text and video tokens within a scene, and apply independent noise levels to individual shots.\nThe interleaved 3D RoPE assigns distinct coordinates for each shot.\n(b)Interleaved 3D RoPE coordinates.At shot-level, text tokens precede video tokens along the space diagonal.\nAt scene-level, tokens are arranged shot by shot, forming an interleaved “[text]-[video]-[text]-...” pattern along the space diagonal.",
                "position": 241
            },
            {
                "img": "https://arxiv.org/html/2503.10589/x4.png",
                "caption": "Figure 4:Inference Modes.(a)Bidirectionalmodel enables (a.1) joint or (a.2) visual-conditioned generation, while (b)context-causalmodel supports auto-regressive generation.",
                "position": 309
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10589/x5.png",
                "caption": "Figure 5:Qualitative Comparisons.We show stacked video frames synthesized by all methods, and expand two shots to illustrate the “reappearance” issue discussed inSec.4.1.\nThe simplified prompts for each shot can be found in the subtitle in “Ours”.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2503.10589/x6.png",
                "caption": "Figure 6:Fidelity to History Condition.\nThe video background generated by the causal model exhibits superior fidelity to the history condition, as evidenced by the street lights’ layout.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2503.10589/x7.png",
                "caption": "Figure 7:Effects of Conditioning Timestep.Large timesteps sacrifice fidelity to the condition.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2503.10589/x8.png",
                "caption": "Figure 8:Effects of History Timestep.Large timesteps mitigate “error accumulation” issue at the cost of history fidelity.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2503.10589/x9.png",
                "caption": "Figure 9:Causal Adaptation.After 1K updates from bidirectional weights, the causal architecture shows excellent consistency.",
                "position": 579
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]