[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07493/x1.png",
                "caption": "Figure 1:Given a PDF document, we create VisR-Bench by extracting text into Markdown files and visual elements into separate images for each page. The extracted content is used to generate queries for retrieving the corresponding evidence page. During testing, a retrieval model identifies relevant pages, and a question answering model then uses these page images to answer the queries.",
                "position": 315
            },
            {
                "img": "https://arxiv.org/html/2508.07493/x2.png",
                "caption": "Figure 2:Distribution of language and document types in VisR-Bench. The blue colors represent the English multimodal split, which can further be categorized into ten document types. Other colors represent the multilingual multimodal split, containing documents in fifteen non-English languages.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2508.07493/x3.png",
                "caption": "Figure 3:Boxplot of document length distribution. Each box represents the inter-quartile range (IQR), covering the middle50%50\\%of the data. The horizontal line inside each box indicates the median document length, while the whiskers extend to the minimum and maximum values within1.51.5times the IQR. The dashed vertical line separates English multimodal split (left) from multilingual multimodal documents (right).",
                "position": 548
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3VisR-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07493/x4.png",
                "caption": "Figure 4:Overview of our QA data generation pipeline.Text and visual content are first extracted from documents, with regular text and tables saved as Markdown—tables are preserved in structured text format using Markdown table syntax—while figures are saved as separate image files. For table- and text-based QA, we prompt GPT-4o using the extracted Markdown content. For figure-based QA, we first filter out decorative figures using a CLIP-based classifier, then generate figure-centered questions by prompting GPT-4o with only the image. To ensure that figures are truly required for answering, we revise the QA pairs by incorporating surrounding text and apply a heuristic filtering step: any question that GPT-4o can already answer using only the Markdown text is discarded. This ensures that the final figure-based QA pairs require both visual and textual information for accurate retrieval and comprehension.",
                "position": 587
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADemo figure-based retrieval",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.07493/x5.png",
                "caption": "Figure A.1:Qualitative error analysis for figure-based question answering.This figure presents two examples where the GME model fails to retrieve the correct evidence page, while ColPali-v1.2 successfully identifies it. The incorrect pages retrieved by GME are shown as hard negatives. Notably, these hard negatives are highly similar to the correct evidence: in the top example, both pages are architectural blueprints containing references to door types and numbers such as “5” and “6”; in the bottom example, both figures depict grain yield advantage plots and contain the keyword “hybrid”. These visually and semantically similar distractors demonstrate typical failure cases for GME and highlight the improved discriminative ability of ColPALI-v1.2 in retrieving the truly relevant figure.",
                "position": 2895
            }
        ]
    },
    {
        "header": "Appendix BSystem Prompts",
        "images": []
    }
]