[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x1.png",
                "caption": "",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x2.png",
                "caption": "",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x3.png",
                "caption": "",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x4.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x5.png",
                "caption": "Figure 1:PixelRefer, a unified region-level MLLM, supports a broad range of tasks at bothobject-levelandscene-level, spanningspatial(images) andtemporal(videos) domains. It enables fine-grained spatiotemporal reasoning over user-specified region with arbitrary semantic granularity, while preserving general-purpose capabilities for holistic visual understanding.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x6.png",
                "caption": "Figure 2:Quantitative Evaluation and Efficiency Analysis. (a)Performance Comparison: PixelRefer and PixelRefer-Lite  consistently outperform state-of-the-art object-level MLLMs across diverse image (LVISYuan et al. [2024], PACOYuan et al. [2024], DLC-BenchLian et al. [2025]) and video (VideoRefer-Bench, HC-STVGTang et al. [2021]) benchmarks. (b)Data Efficiency: Our method achieves leading performance with fewer training samples compared to existing methods. (c)Runtime and Memory Efficiency: PixelRefer-Lite notably reduces inference time and memory usage, clearly demonstrating its efficiency.",
                "position": 204
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x7.png",
                "caption": "Figure 3:Visualization of attention maps across different layers (Layer 1, 7, 14 and 28) of the LLM. The input sequence includes system tokens (sys), global image token (vision), text prompts (text), object-level tokens (object), and answer tokens (ans). For clarity, image tokens are average pooled by a factor of 8. The figure showcases how attention patterns evolve across layers over different tokens.",
                "position": 257
            }
        ]
    },
    {
        "header": "3How Do MLLMs Understand Object Tokens?",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x8.png",
                "caption": "Figure 4:Visualization of answer-to-image attention heatmaps for different query regions. The model adaptively focuses on relevant objects while incorporating contextual cues from the surrounding areas.",
                "position": 328
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x9.png",
                "caption": "Figure 5:Frameworks of two complementary paradigms\nfor region-level representations\nin our approach: (a) illustratesVision-Object Framework, while (b) presentsObject-Only Framework.",
                "position": 348
            }
        ]
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x10.png",
                "caption": "Figure 6:Architecture of our proposedScale-Adaptive Object Tokenizer. For an input image and a given object, we first performDynamic Object Processingto adaptively scale the objects. Subsequently, vision features are extracted from the cropped and expanded sections of the image. To address redundancy prevalent in large objects, we further introduceAbundant Feature Aggregationfor efficient feature integration.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x11.png",
                "caption": "Figure 7:Cosine similarity analysis of object tokens. Left: Pairwise similarity matrix of 50 object tokens randomly sampled from a single object.\nRight: Histograms of pairwise similarities before (ùêÖ‚Ñ≥‚Äãùí´\\mathbf{F}_{\\mathcal{MP}}) and after (ùêÖ‚Ñ≥‚Äãùí´‚Ä≤\\mathbf{F}_{\\mathcal{MP}^{\\prime}})Abundant Feature Aggregationfor objects with 50 tokens (top) and 100 tokens (bottom), respectively. Aggregated tokens show reduced intra-object similarity, indicating effective redundancy reduction and improved representational compactness.",
                "position": 458
            }
        ]
    },
    {
        "header": "5Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x12.png",
                "caption": "Figure 8:Overview of datasets used for model training. Left: Data distribution for Foundational Object Perception training (1.4M samples). Right: Data used for Visual Instruction Tuning (0.8M samples).",
                "position": 595
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.23603/x13.png",
                "caption": "Figure 9:(a)Multi-perspective object understanding with PixelRefer.The model generates diverse responses according to different prompts. (b)Granular visual understanding with PixelRefer.PixelRefer yields distinct detailed descriptions at part-level and object-level based on specified region granularity.",
                "position": 893
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x14.png",
                "caption": "Figure 10:Effects of object token scaling across four typical benchmarks, including PACO, DLC-Bench, VideoRefer-BenchD, and HC-STVG. We evaluate the impact of varying object token numbers (1, 8, 16, 32) under two model configurations: Vision-Object and Object-Only.",
                "position": 1653
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x15.png",
                "caption": "Figure 11:Visualization of attention map between object tokens and image tokens with 16 tokens.",
                "position": 1847
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x16.png",
                "caption": "Figure 12:Performance comparisons between the proposed Scale-Adaptive Object Tokenizer (SAOT) and MaskPooling on LVIS and DLC-Bench with large and small regions. Our SAOT demonstrates consistently strong performance across both region sizes, while MaskPooling suffers significant degradation on small regions, highlighting the importance of scale-aware object representation.",
                "position": 1941
            },
            {
                "img": "https://arxiv.org/html/2510.23603/x17.png",
                "caption": "Figure 13:Left:Versatile video referring with PixelRefer.PixelRefer handles diverse video referring tasks, including video object captioning, multi-object question answering, and zero-shot spatial understanding. Right:Comparing  PixelRefer with Qwen2.5-VLBai et al. [2025], DAMLian et al. [2025]on video object referring task.PixelRefer exhibits the ability to accurately identify specific objects while also comprehending the overall context of the video.",
                "position": 1966
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]