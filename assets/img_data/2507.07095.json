[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07095/x1.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3MotionMillion Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07095/x2.png",
                "caption": "Figure 2:Data Construction Pipeline of MotionMillion. We can obtain high-quality human motion from a monocular video via our six processing stages, i.e. Shot Segmentation, Human Detection, Video Filtering, SMPL Motion Estimation and Motion Filtering.",
                "position": 185
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x3.png",
                "caption": "Figure 3:Overview of MotionMillion. This dataset exhibits extensive semantic and pose diversity, encompassing a broad spectrum of indoor and outdoor human motions.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x4.png",
                "caption": "Figure 4:Jerk comparison across MotionMillion, MotionX, and HumanML3D. Our MotionMillion exhibits the lowest jerk values, indicating that it produces smoother motion.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x5.png",
                "caption": "Figure 5:Overview of our scalable model architecture, which utilize FSQ as a motion tokenizer and an autoregressive transformer to generate the motion from the given text.",
                "position": 380
            }
        ]
    },
    {
        "header": "4Architecture",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07095/x6.png",
                "caption": "Figure 6:Our model demonstrates robust performance in generating coherent motions from complex compositional textual descriptions.",
                "position": 593
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Overview",
        "images": []
    },
    {
        "header": "8Data Distribution and Prompts",
        "images": []
    },
    {
        "header": "9Scoring Criteria Details of MotionMillion-Eval",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07095/x7.png",
                "caption": "Figure 7:Data Distributions of MotionMillion.",
                "position": 2243
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x8.png",
                "caption": "Figure 8:Prompt used during captioning the motions in the web-scale human videos and text rewrite in inference stage.",
                "position": 2246
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x9.png",
                "caption": "",
                "position": 2249
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x10.png",
                "caption": "",
                "position": 2251
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x11.png",
                "caption": "",
                "position": 2253
            },
            {
                "img": "https://arxiv.org/html/2507.07095/x12.png",
                "caption": "",
                "position": 2255
            }
        ]
    },
    {
        "header": "10Prompts in MotionMillion-Eval",
        "images": []
    }
]