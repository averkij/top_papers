[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/teaser.png",
                "caption": "Figure 1.Fashion-VDM. Given an input garment image and a person video, Fashion-VDM generates a video of the person virtually trying on the given garment, while preserving their original identity and motion.",
                "position": 175
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/architecture.png",
                "caption": "Figure 2.Fashion-VDM Architecture. Given a noisy videoztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTat diffusion timesteptğ‘¡titalic_t, a forward pass of Fashion-VDM computes a single denoising step to get the denoised videoztâˆ’1â€²subscriptsuperscriptğ‘§â€²ğ‘¡1z^{\\prime}_{t-1}italic_z start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Noisy videoztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis preprocessed into person posesJpsubscriptğ½ğ‘J_{p}italic_J start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTand clothing-agnostic framesIasubscriptğ¼ğ‘I_{a}italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, while the garment imageIgsubscriptğ¼ğ‘”I_{g}italic_I start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTis preprocessed into the garment segmentationSgsubscriptğ‘†ğ‘”S_{g}italic_S start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTand garment posesJgsubscriptğ½ğ‘”J_{g}italic_J start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT(Section3.3). The architecture follows(Zhu etÂ al.,2024), except the main UNet contains 3D-Conv and temporal attention blocks to maintain temporal consistency. Additionally, we inject temporal down/upsampling blocks during 64-frame temporal training. Noisy videoztsubscriptğ‘§ğ‘¡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis encoded by the main UNet and the conditioning signals,Sgsubscriptğ‘†ğ‘”S_{g}italic_S start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTandIasubscriptğ¼ğ‘I_{a}italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, are encoded by separate UNet encoders. In the 8 DiT blocks at the lowest resolution of the UNet, the garment conditioning features are cross-attended with the noisy video features and the spatially-aligned clothing-agnostic featureszasubscriptğ‘§ğ‘z_{a}italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTand noisy video features are directly concatenated.Jgsubscriptğ½ğ‘”J_{g}italic_J start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTandJpsubscriptğ½ğ‘J_{p}italic_J start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTare encoded by single linear layers, then concatenated to the noisy features in all UNet 2D spatial layers.",
                "position": 209
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/split_cfg.png",
                "caption": "Figure 3.Split-CFG Ablation. We compare different split-cfg weights, where(wâˆ…,wp,wg,wfull)subscriptğ‘¤subscriptğ‘¤psubscriptğ‘¤gsubscriptğ‘¤full(w_{\\emptyset},w_{\\text{p}},w_{\\text{g}},w_{\\text{full}})( italic_w start_POSTSUBSCRIPT âˆ… end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT p end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT g end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT full end_POSTSUBSCRIPT )correspond to the unconditional guidance, person-only guidance, person and cloth guidance, and full guidance terms, respectively.",
                "position": 287
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/joint_training.png",
                "caption": "Figure 4.Joint Training Ablation. Joint image and video training improves the realism of occluded views.",
                "position": 428
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/garment_fidelity_ablations.png",
                "caption": "Figure 5.Garment Fidelity Ablations. We compare our full model with ablated versions without split-CFG and without joint image-video training in terms of garment fidelity. Both split-CFG and joint image-video training improve fine-grain garment details (top row) and novel view generation (bottom row).",
                "position": 554
            }
        ]
    },
    {
        "header": "5.Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/temporal_smoothness_ablations.png",
                "caption": "Figure 6.Temporal Smoothness Ablations. We compare video frames generated by our ablated model without temporal blocks (top row) and without progressive training (middle row) to our full model (bottom row). Both ablated versions exhibit large frame-to-frame inconsistencies and artifacts.",
                "position": 594
            }
        ]
    },
    {
        "header": "6.Comparisons to State-of-the-Art",
        "images": []
    },
    {
        "header": "7.Limitations and Future Work",
        "images": []
    },
    {
        "header": "8.Discussion",
        "images": []
    },
    {
        "header": "9.Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/qualitative_main.png",
                "caption": "Figure 7.Qualitative Results. We showcase video try-on results generated by Fashion-VDM using randomly paired person-garment test videos from the UBC dataset(Zablotskaia etÂ al.,2019)and our own collected test dataset. Note that the input garment image and input person frames come from different videos.",
                "position": 1816
            },
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/sota_comparison.png",
                "caption": "Figure 8.Qualitative Comparisons. Fashion-VDM outperforms past methods in garment fidelity and realism. Especially in cases of large disocclusion, our method synthesizes more realistic novel views.",
                "position": 1819
            }
        ]
    },
    {
        "header": "Appendix AProgressive Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/progressive_training.png",
                "caption": "Figure 9.Progressive Training Strategy. Fashion-VDM is trained in multiple phases of increasing frame length. We first pretrain an image model, by training only the spatial layers on our image dataset. In subsequent phases, we train temporal and spatial layers on increasingly long batches of consecutive frames from our video dataset.",
                "position": 1829
            },
            {
                "img": "https://arxiv.org/html/2411.00225/x1.png",
                "caption": "Figure 10.Failure Cases.Errors in the person segmentation may lead to artifacts (top row). Fashion-VDM may incorrectly represent body shape (bottom row).",
                "position": 1845
            }
        ]
    },
    {
        "header": "Appendix BExamples of Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/ubc_only_split_cfg.png",
                "caption": "Figure 11.Split-CFG Ablation with UBC-Only Model.When Fashion-VDM is trained on the limited UBC dataset only, we observe overfitting to the largely plain garments in the UBC train dataset. However, we find that increasing garment image guidance (wgsubscriptğ‘¤ğ‘”w_{g}italic_w start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT) in split-CFG significantly increases garment details.",
                "position": 2023
            },
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/qualitative_ubc_only.png",
                "caption": "Figure 12.Qualitative Results for UBC-Only Model.Our model trained only on UBC data generates temporally consistent, smooth try-on videos for plain and simple patterned garments, but struggles to preserve intricate patterns and complex garment shapes.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2411.00225/x2.png",
                "caption": "Figure 13.Additional Qualitative Results.We showcase video try-on results generated by Fashion-VDM using swapped test videos from the UBC dataset(Zablotskaia etÂ al.,2019)and our own collected test dataset. Note that the input garment image and input person frames come from different videos.",
                "position": 2038
            }
        ]
    },
    {
        "header": "Appendix CSplit-CFG Weights Ablations",
        "images": []
    }
]