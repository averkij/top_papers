[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/teaser.png",
                "caption": "Figure 1.Fashion-VDM. Given an input garment image and a person video, Fashion-VDM generates a video of the person virtually trying on the given garment, while preserving their original identity and motion.",
                "position": 175
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/architecture.png",
                "caption": "Figure 2.Fashion-VDM Architecture. Given a noisy videoztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTat diffusion timestept𝑡titalic_t, a forward pass of Fashion-VDM computes a single denoising step to get the denoised videozt−1′subscriptsuperscript𝑧′𝑡1z^{\\prime}_{t-1}italic_z start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT. Noisy videoztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis preprocessed into person posesJpsubscript𝐽𝑝J_{p}italic_J start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTand clothing-agnostic framesIasubscript𝐼𝑎I_{a}italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, while the garment imageIgsubscript𝐼𝑔I_{g}italic_I start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTis preprocessed into the garment segmentationSgsubscript𝑆𝑔S_{g}italic_S start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTand garment posesJgsubscript𝐽𝑔J_{g}italic_J start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT(Section3.3). The architecture follows(Zhu et al.,2024), except the main UNet contains 3D-Conv and temporal attention blocks to maintain temporal consistency. Additionally, we inject temporal down/upsampling blocks during 64-frame temporal training. Noisy videoztsubscript𝑧𝑡z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis encoded by the main UNet and the conditioning signals,Sgsubscript𝑆𝑔S_{g}italic_S start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTandIasubscript𝐼𝑎I_{a}italic_I start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, are encoded by separate UNet encoders. In the 8 DiT blocks at the lowest resolution of the UNet, the garment conditioning features are cross-attended with the noisy video features and the spatially-aligned clothing-agnostic featureszasubscript𝑧𝑎z_{a}italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPTand noisy video features are directly concatenated.Jgsubscript𝐽𝑔J_{g}italic_J start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPTandJpsubscript𝐽𝑝J_{p}italic_J start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPTare encoded by single linear layers, then concatenated to the noisy features in all UNet 2D spatial layers.",
                "position": 209
            }
        ]
    },
    {
        "header": "2.Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/split_cfg.png",
                "caption": "Figure 3.Split-CFG Ablation. We compare different split-cfg weights, where(w∅,wp,wg,wfull)subscript𝑤subscript𝑤psubscript𝑤gsubscript𝑤full(w_{\\emptyset},w_{\\text{p}},w_{\\text{g}},w_{\\text{full}})( italic_w start_POSTSUBSCRIPT ∅ end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT p end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT g end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT full end_POSTSUBSCRIPT )correspond to the unconditional guidance, person-only guidance, person and cloth guidance, and full guidance terms, respectively.",
                "position": 287
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/joint_training.png",
                "caption": "Figure 4.Joint Training Ablation. Joint image and video training improves the realism of occluded views.",
                "position": 428
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/garment_fidelity_ablations.png",
                "caption": "Figure 5.Garment Fidelity Ablations. We compare our full model with ablated versions without split-CFG and without joint image-video training in terms of garment fidelity. Both split-CFG and joint image-video training improve fine-grain garment details (top row) and novel view generation (bottom row).",
                "position": 554
            }
        ]
    },
    {
        "header": "5.Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/temporal_smoothness_ablations.png",
                "caption": "Figure 6.Temporal Smoothness Ablations. We compare video frames generated by our ablated model without temporal blocks (top row) and without progressive training (middle row) to our full model (bottom row). Both ablated versions exhibit large frame-to-frame inconsistencies and artifacts.",
                "position": 594
            }
        ]
    },
    {
        "header": "6.Comparisons to State-of-the-Art",
        "images": []
    },
    {
        "header": "7.Limitations and Future Work",
        "images": []
    },
    {
        "header": "8.Discussion",
        "images": []
    },
    {
        "header": "9.Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/qualitative_main.png",
                "caption": "Figure 7.Qualitative Results. We showcase video try-on results generated by Fashion-VDM using randomly paired person-garment test videos from the UBC dataset(Zablotskaia et al.,2019)and our own collected test dataset. Note that the input garment image and input person frames come from different videos.",
                "position": 1816
            },
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/sota_comparison.png",
                "caption": "Figure 8.Qualitative Comparisons. Fashion-VDM outperforms past methods in garment fidelity and realism. Especially in cases of large disocclusion, our method synthesizes more realistic novel views.",
                "position": 1819
            }
        ]
    },
    {
        "header": "Appendix AProgressive Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/progressive_training.png",
                "caption": "Figure 9.Progressive Training Strategy. Fashion-VDM is trained in multiple phases of increasing frame length. We first pretrain an image model, by training only the spatial layers on our image dataset. In subsequent phases, we train temporal and spatial layers on increasingly long batches of consecutive frames from our video dataset.",
                "position": 1829
            },
            {
                "img": "https://arxiv.org/html/2411.00225/x1.png",
                "caption": "Figure 10.Failure Cases.Errors in the person segmentation may lead to artifacts (top row). Fashion-VDM may incorrectly represent body shape (bottom row).",
                "position": 1845
            }
        ]
    },
    {
        "header": "Appendix BExamples of Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/ubc_only_split_cfg.png",
                "caption": "Figure 11.Split-CFG Ablation with UBC-Only Model.When Fashion-VDM is trained on the limited UBC dataset only, we observe overfitting to the largely plain garments in the UBC train dataset. However, we find that increasing garment image guidance (wgsubscript𝑤𝑔w_{g}italic_w start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT) in split-CFG significantly increases garment details.",
                "position": 2023
            },
            {
                "img": "https://arxiv.org/html/2411.00225/extracted/5977110/Figures/qualitative_ubc_only.png",
                "caption": "Figure 12.Qualitative Results for UBC-Only Model.Our model trained only on UBC data generates temporally consistent, smooth try-on videos for plain and simple patterned garments, but struggles to preserve intricate patterns and complex garment shapes.",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2411.00225/x2.png",
                "caption": "Figure 13.Additional Qualitative Results.We showcase video try-on results generated by Fashion-VDM using swapped test videos from the UBC dataset(Zablotskaia et al.,2019)and our own collected test dataset. Note that the input garment image and input person frames come from different videos.",
                "position": 2038
            }
        ]
    },
    {
        "header": "Appendix CSplit-CFG Weights Ablations",
        "images": []
    }
]