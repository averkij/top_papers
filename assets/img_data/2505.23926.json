[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23926/x1.png",
                "caption": "Figure 1:Overview of cross-domain training architectures.Point clouds exhibit diverse characteristics across domains.(a)Naively training Point Transformer V3 (PTv3)[78]on multi-domain data leads to degraded performance within each domain.(b)Point Prompt Training (PPT)[80]addresses this by adding domain-aware normalization parameters.(c)Our proposed Point-MoE tackles this challenge with Mixture-of-Experts (MoE), enabling dynamic expert specialization across domains.\nDomain labels can optionally be provided to the router to guide the expert selection.",
                "position": 578
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Point-MoE for Cross-Domain Point Cloud Understanding",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23926/x2.png",
                "caption": "Figure 2:Training Loss and Validation mIoU over time.Left: training loss curves for joint training across multiple domains. Right: validation mIoU on indoor datasets. Point-MoE-L achieves competitive performance to PPT, and both significantly outperform the baseline PTv3-L.",
                "position": 2203
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x3.png",
                "caption": "Figure 3:Expert Choices Visualization.Different color represents a different expert. (a) shows a specific expert focusing on edges across the scene, revealing spatially coherent routing.\n(b) and (c) highlight expert selections in indoor scenes, where semantically related regions (e.g., chair, desk) are consistently routed to the same expert.\n(d) displays an outdoor scene with sparse LiDAR points, yet the model still demonstrates meaningful routing despite limited geometry.",
                "position": 2215
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x4.png",
                "caption": "Figure 4:Expert routing behavior across domains.The left panel shows the most frequently used expert paths through each encoder (E-) and decoder (D-) layer, with feature channel sizes marked in parentheses, illustrating the variation in expert selection for tokens from different datasets.\nThe router tends to assign experts based on the input domain, particularly in the decoder MoE layers.\nTo quantify this, we compute the Jensen-Shannon Divergence (JSD) between the expert selection distributions of different datasets at each MoE layer.",
                "position": 2225
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations, Licenses and Risks",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.23926/x5.png",
                "caption": "(a)Detector trained on indoor domains",
                "position": 4469
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x5.png",
                "caption": "(a)Detector trained on indoor domains",
                "position": 4472
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x6.png",
                "caption": "(b)Detector trained on indoor-outdoor domains",
                "position": 4477
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x7.png",
                "caption": "Figure 6:t-SNE Visualization.The first three columns illustrate feature similarities and differences across PTv3, PPT, and Point-MoE for both encoder and decoder layers. The rightmost column highlights how Point-MoE generalizes to unseen domains, with Matterport3D features aligning closely with the most relevant seen domains—demonstrating robust cross-domain generalization.",
                "position": 5178
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x8.png",
                "caption": "Figure 7:Expert Specialization Word Cloud.All classes’ frequencies of selecting their top-1 expert are measured on the validation set. Colors indicate the originating dataset of the class (e.g., ScanNet, S3DIS, etc.), providing insight into cross-domain consistency.",
                "position": 5204
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x9.png",
                "caption": "Figure 8:Expert Choices Visualization for Decoder Layer 0.",
                "position": 5216
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x10.png",
                "caption": "Figure 9:Expert Choices Visualization for Decoder Layer 1.",
                "position": 5219
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x11.png",
                "caption": "Figure 10:Expert Choices Visualization for Decoder Layer 2.",
                "position": 5222
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x12.png",
                "caption": "Figure 11:Expert Choices Visualization for Decoder Layer 3.",
                "position": 5225
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x13.png",
                "caption": "Figure 12:JSD Dynamics During Training.We visualize the Jensen-Shannon Divergence (JSD) between expert assignment distributions across datasets to study how expert specialization emerges over time. The plots illustrate JSD dynamics across training iterations for both encoder and decoder layers.",
                "position": 5233
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x14.png",
                "caption": "Figure 13:Point-MoE’s Expert Choice Visualization on ScanNet.Each point is colored by its assigned expert across encoder and decoder blocks. Early layers show structured expert usage, while deeper layers exhibit more mixed routing, reflecting the evolving representation dynamics throughout the network.",
                "position": 5288
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x15.png",
                "caption": "Figure 14:Point-MoE’s Expert Choice Visualization on S3DIS.Each point is color-coded based on the expert it was routed to across both encoder and decoder blocks. The earlier layers demonstrate clear structure in expert assignments, whereas the later layers reveal more diverse routing patterns, indicating evolving feature representations.",
                "position": 5291
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x16.png",
                "caption": "Figure 15:Point-MoE’s Expert Choice Visualization on Structured3D.Points are colored by their designated experts in the encoder and decoder. While the initial layers display consistent expert allocation, the deeper layers show a blend of routing decisions, highlighting the network’s shifting representational strategy.",
                "position": 5294
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x17.png",
                "caption": "Figure 16:Point-MoE’s Expert Choice Visualization on Matterport3D.Color represents the expert assigned to each point throughout the encoder and decoder. Structured expert usage is visible in early stages, transitioning to more varied routing in deeper layers, signaling changes in how features are processed.",
                "position": 5297
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x18.png",
                "caption": "Figure 17:Point-MoE’s Expert Choice Visualization on SemanticKITTI.The expert responsible for each point, shown through color, spans the encoder and decoder blocks. Initially, expert usage is highly organized, but becomes more dispersed in later layers, capturing the transformation in representation over depth.",
                "position": 5300
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x19.png",
                "caption": "Figure 18:Point-MoE’s Expert Choice Visualization on NuScene.Each point is colored according to its corresponding expert across the network’s layers. Early encoder and decoder blocks exhibit distinct expert patterns, while deeper layers demonstrate a more distributed and dynamic routing behavior.",
                "position": 5303
            },
            {
                "img": "https://arxiv.org/html/2505.23926/x20.png",
                "caption": "Figure 19:Point-MoE’s Expert Choice Visualization on Waymo.Color indicates the expert each point is assigned to across encoder and decoder stages. Early network layers maintain structured expert routes, but this structure relaxes in deeper layers, illustrating evolving internal representations.",
                "position": 5306
            }
        ]
    },
    {
        "header": "Appendix CAdditional Visualization and Discussions on Point-MoE",
        "images": []
    }
]