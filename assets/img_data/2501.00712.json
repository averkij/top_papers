[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Our Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00712/x1.png",
                "caption": "Figure 1:Overview of our proposed TAPE in standard decoder-only Transformer architecture.",
                "position": 278
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00712/x2.png",
                "caption": "Figure 2:Accuracy on addition task between different methods on 2√ó\\times√ócontext length. Models are trained on sequence with length up to 40 while test on sequence with length up to 80. The average accuracy across the heatmap is 26.32%, 26.56%, 22.45%, 26.98% and 32.82% respectively for RoPE, RandPE, NoPE, FIRE and TAPE.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2501.00712/x3.png",
                "caption": "Figure 3:Accuracy on passkey retrieval from 1k to 8k context length between Llama2 7B with different fine-tuning methods.",
                "position": 705
            }
        ]
    },
    {
        "header": "5More Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof of Proposition1",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00712/x4.png",
                "caption": "Figure 4:Visualization of TAPE‚Äôs operations. The channel dimension is omitted for simplicity as all operations can be channel-wise. In the attention layer, the input token embeddings have a shape ofN√óBùëÅùêµN\\times Bitalic_N √ó italic_B, and the position embeddings have a shape ofN√óL√óRùëÅùêøùëÖN\\times L\\times Ritalic_N √ó italic_L √ó italic_R. For the feed-forward layer, theNùëÅNitalic_Ndimension is omitted as its operations are position-wise. The input token embeddings then have a shape ofBùêµBitalic_B(orB√ó1ùêµ1B\\times 1italic_B √ó 1), and the position embeddings have a shape ofL√óRùêøùëÖL\\times Ritalic_L √ó italic_R.",
                "position": 2295
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.00712/x5.png",
                "caption": "Figure 5:Accuracy on addition task\ntrained with length 20 test on 2√ó\\times√ócontext length. The average accuracy across the heatmap is 26.12%, 26.12%, 39.44% and 41.42% respectively for RoPE, RandPE, FIRE and TAPE.",
                "position": 2666
            },
            {
                "img": "https://arxiv.org/html/2501.00712/x6.png",
                "caption": "Figure 6:Accuracy on addition task on 2√ó\\times√ócontext length. The average accuracy is 26.98%, 32.82% and 33.92% respectively for FIRE, TAPE and TAPE + YaRN.",
                "position": 2670
            },
            {
                "img": "https://arxiv.org/html/2501.00712/extracted/6105208/fig/vis_dp.png",
                "caption": "(a)Dot-product patterns of positional embeddings of TAPE and RoPE.",
                "position": 2693
            },
            {
                "img": "https://arxiv.org/html/2501.00712/extracted/6105208/fig/vis_dp.png",
                "caption": "(a)Dot-product patterns of positional embeddings of TAPE and RoPE.",
                "position": 2696
            },
            {
                "img": "https://arxiv.org/html/2501.00712/extracted/6105208/fig/vis_attn_diff.png",
                "caption": "(b)Difference between TAPE and RoPE",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2501.00712/x7.png",
                "caption": "Figure 8:Dot-product patterns of positional embeddings in layers 1, 4, 8, and 12 (last) of TAPE.",
                "position": 2708
            }
        ]
    },
    {
        "header": "Appendix DFurther Illustrations",
        "images": []
    }
]