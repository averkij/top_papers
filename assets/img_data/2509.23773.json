[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23773/Figure/motivation_new.png",
                "caption": "Figure 1.LLM is prompted on individual triple facts\n, which are aggregated to obtain entity-level knowledgeability scores. When visualized, these scores reveal the knowledge homophily pattern, where topologically close entities form distinct high-knowledge (red) and low-knowledge (blue) communities. Note that graph layout is visualized by the ForceAtlas2 algorithm(Jacomy et¬†al.,2014)to preserve topological proximity.",
                "position": 109
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Knowledge Homophily Discovery",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23773/Figure/fig_2.png",
                "caption": "Figure 2.(a): Distribution of node knowledgeability homophily for each dataset;(b): Average knowledge homophily across datasets and LLMs with black line showing Citeseer benchmark (0.74) for high homophily in graph analysis.",
                "position": 246
            },
            {
                "img": "https://arxiv.org/html/2509.23773/Figure/fig_3.png",
                "caption": "Figure 3.(a) Neighboring nodes possess similar knowledgeability scores than randomly sampled nodes, indicating that homophily is a intrinsic structural property of LLMs; (b) Entities with their distinct knowledgeability levelsùí¶‚Äã(v)\\mathcal{K}(v)indicated by node color (Red = High, Blue = Low).",
                "position": 268
            }
        ]
    },
    {
        "header": "4.Knowledge Homophily Application",
        "images": []
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "6.Ethical Considerations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23773/Figure/knowledgeapplication.png",
                "caption": "Figure 4.Homophily-guided Knowledge Injection and Retrieval: The process begins by training a GNN on a subset of entities with ground-truth knowledgeability scores (Blue Nodes) obtained by querying the base LLM. The trained GNN then infers the knowledgeability scores for all remaining entities (Green Nodes). Based on these predictions, triplets associated with entities estimated to have the lowest knowledge values are selected until the budget is met. Finally, the base LLM is fine-tuned on these less-known triplets to efficiently inject new knowledge, and its improved performance is measured on a held-out test set (Orange Nodes) in Figure4(a). The estimated knowledgeability scores also guide retrieval, as illustrated in Figure4(b).",
                "position": 575
            }
        ]
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.23773/Figure/sensitivity.png",
                "caption": "Figure 5.The knowledge injection performance of the fine-tuned Mistral models on the CoDEx-S dataset across varying test set sizes. The GNN-guided approach maintains a significant performance advantage over other methods.",
                "position": 1284
            },
            {
                "img": "https://arxiv.org/html/2509.23773/Figure/sensitivity_retrieval.png",
                "caption": "Figure 6.The knowledge-aware retrieval performance across the varying training budgets of the underlying knowledgeability estimator. For both 2-hop (left) and 3-hop (right) QA on the CoDEx-S dataset, the GNN-based search (G-BS) consistently outperforms the homophily-agnostic MLP-based search (M-BS) and the semantic baseline.",
                "position": 1287
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]