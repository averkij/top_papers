[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/x1.png",
                "caption": "Figure 1:WoWis a world model that integrates perception, prediction, Judgement, reflection, and action. It learns from real-world interaction data and generates high-quality, physically consistent robot videos in seen and out-of-distribution scenarios, enabling real-world robotic execution.",
                "position": 192
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Rethinking World Model: Towards A World-Omniscient Intelligent",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/x2.png",
                "caption": "Figure 2:Developmental trajectory of world models, from modality-specific models (e.g., VGM, LLM) to unified models after a critical emergence point.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/physical_consistency.png",
                "caption": "Figure 3:The technological development of world models in pursuit of intrinsic physical consistency has primarily followed two approaches. One possible approach is to construct a world model with inherent physical modeling capabilities for robots. The mainstream approaches to realizing such a world model include two methods. One is based on generative AI combined with a differentiable physics engine. The other, grounded in video generation models, constructs a neural network-driven physics engine that possesses both intrinsic physical consistency and external high visual fidelity.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/Brain_and_Mind_Model.png",
                "caption": "Figure 4:The architecture of an embodied agent with a world model. An intelligent agent perceives the environment through various sensory inputs (e.g., visual, sound, heat, force). These perceptions are processed by a World Model, which builds an internal, predictive representation of the environment. The model’s predictions and past experiences, stored in short-term and long-term memory, inform Reasoning and Judgement. Based on this internal simulation, the Actor generates Actions that manipulate the real world. This closed-loop system allows the agent to learn the dynamics of its environment, plan for the future, and achieve complex goals. (Figure inspired by(Brain-inspired Cognitive Intelligence Lab,2025))",
                "position": 499
            }
        ]
    },
    {
        "header": "3WoW World Model",
        "images": []
    },
    {
        "header": "4Self-Optimizing Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/x3.png",
                "caption": "Figure 5:Comparison of Diffusion, JEPA(Assran et al.,2025), and SOPHIA.ThePredictorgenerates aFuturefrom the inputContext. This outcome is then evaluated to produce areward, which directs theRefiner. Finally, theRefinerleverages this reward and externallanguage/embeddingguidance to issue a corrective signal, iteratively improving the next prediction cycle.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x4.png",
                "caption": "Figure 6:Overview of the Video Diffusion World Model.(a) Inference: a latent diffusion transformer predicts future frames from image observations and text-based action descriptions.\n(b) Training: DINO features supervise intermediate DiT representations via a token relation distillation loss to improve spatial-temporal modeling.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x5.png",
                "caption": "Figure 7:Overview of Solver-Critic Video Generation Agents.The left panel illustrates the Dynamic Critic Model Team, trained on annotated real and synthetic videos to evaluate physical plausibility.\nThe right panel depicts the Refiner Agent, which iteratively rewrites prompts based on critic feedback and regenerates videos, forming a closed-loop optimization process.",
                "position": 662
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/IDM.png",
                "caption": "Figure 8:Work flow of inverse dynamics model. Giving two frame predictions, our FM-IDM can estimate the delta End-Effect action of the robot.",
                "position": 720
            }
        ]
    },
    {
        "header": "5WoWBench: A Multi-faceted Benchmark for Embodied World Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/x6.png",
                "caption": "Figure 9:The Overall Design of WoWBench.Our benchmark is structured around five core components.(Top-left)A multi-facetedMetricssuite evaluates generated videos from four key perspectives: video quality, planning reasoning, physical rules, and instruction understanding.(Top-center)These metrics are designed to assess four fundamentalAbilitiesof an embodied world model: Perception, Planning, Prediction, and Generalization.(Top-right)The benchmark is built upon a large-scaleData Constructionpipeline that leverages diverse data sources (in-house, public, and AI-generated) and a human-in-the-loop annotation process (GPT-based selection followed by human labeling) to create video-prompt pairs. Three pie charts demonstrate the statistics of data distribution in different dimensions.(Center)The benchmark uses two different evaluation methods. Strong expert models for motion and consistency, and GPT or Fine-tuned VLM for instruction understanding and planning.(Bottom)We also asked 12 domain experts to perform a human evaluation on the generated videos.",
                "position": 777
            }
        ]
    },
    {
        "header": "6Experiment: Evaluating Generative World Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/figs/diff_model_v3.png",
                "caption": "Table 1:Comparative analysis of foundational video generation models.We benchmark ourWoW-DiTagainst SOTA models using direct text-to-video generation. All metrics: higher is better. Best results areboldwith\\cellcolormylightgreen highlight.",
                "position": 1036
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/diff_model_v3.png",
                "caption": "Figure 10:Performance Comparison Across Different Models in WoWBench.The results indicate that all models subjected to post-training demonstrate superior performance compared to their respective baselines.",
                "position": 1329
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/diff_model_v3.png",
                "caption": "Figure 10:Performance Comparison Across Different Models in WoWBench.The results indicate that all models subjected to post-training demonstrate superior performance compared to their respective baselines.",
                "position": 1332
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x7.png",
                "caption": "Figure 11:Scaling Curves for Training Data.We divide the benchmark into three levels of difficulty: Easy, Medium, and Hard. The left figure shows that as training data increases from 30k to 2M, performance on the Easy tasks begins to saturate, while the Hard tasks continue to benefit from more data.",
                "position": 1337
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/scaling_model_size.png",
                "caption": "Figure 12:Visual Quality Comparison Among scaling Model Size.An analysis of inference speed and performance for models of varying sizes, specifically 2B, 7B, and 14B parameters. Performance is evaluated using the low-level metric, PSNR.",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x8.png",
                "caption": "Figure 13:Performance Comparison Across Different Models in Detail Metrics in WoWBench.Different color blocks stand for different dimensions in WoWBench. In every block, we demonstrate intuitive charts to present detailed scores in varied metrics in our WoWBench.",
                "position": 1348
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x9.png",
                "caption": "Figure 14:Cross-Embodied Generalization AbilityCase Study in generalization ability of different robot types.",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x10.png",
                "caption": "Figure 15:Robot-Action Generalization AbilityGeneralization ability in action typies.",
                "position": 1377
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/moreood.png",
                "caption": "Figure 16:More Generalization AbilityCase Study in generalization ability of five other aspects.",
                "position": 1380
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/IDM_case_study.png",
                "caption": "Figure 17:Difficult Level Separate of IDM.",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x11.png",
                "caption": "Figure 18:WoW’s Efficacy in Real-World Robotics.(Left)Qualitative examples of successful trajectories generated by WoW foreasyandmediumdifficulty tasks executed on a physical robot.(Right)Quantitative results demonstrating the real-world accuracy comparison of three different world model backbones. Across all base models, fine-tuning provides a dramatic boost in real-world performance, with WoW-cosmos2 achieving the highest score.",
                "position": 1770
            }
        ]
    },
    {
        "header": "7Case Study: Advanced Reasoning and Generalization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/x12.png",
                "caption": "Figure 19:OOD Counterfactual Physical Reasoning via World Model Generation.The figure shows our model translating textual counterfactuals (e.g., a \"stone\" jacket) into physically coherent video simulations. By first performing an explicit linguistic reasoning step, the model correctly predicts and visualizes the consequences of these hypothetical rules, such as failing to lift an impossibly heavy object. This demonstrates a core capability: grounding abstract language into dynamic physical simulations, moving beyond pattern replication.",
                "position": 1782
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x13.png",
                "caption": "Figure 20:Case study illustrating tool-use generalization via iterative prompt refinement in a rope-cutting task.",
                "position": 1807
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x14.png",
                "caption": "Figure 21:Compositional Reasoning.The figure illustrates two examples, showing WoW’s reasoning ability. SpecificallyLogical Negation(top) andConditional Logic(bottom), which grounds symbolic reasoning in imagined physical interactions.",
                "position": 1817
            }
        ]
    },
    {
        "header": "8Foundation Model For Application Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22642/x15.png",
                "caption": "Figure 22:Advantages of the 4D Generative World Model over standard cross-view World Models.Our method removes the need for first-frame guidance, enables consistent novel-view generation,\nleverages 3D geometric priors for more reliable conditioning, and enhances VLA model training\nby providing richer viewpoint data.",
                "position": 1850
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x16.png",
                "caption": "Figure 23:Overview of our4D World Modelpipeline.\nGiven a small set of external anchor views (top), we reconstruct geometry and lift it into point clouds.\nA dedicated wrist-view head predicts the egocentric camera pose, enabling wrist-view projection of the 3D evidence.\nFrom these coarse condition maps, our diffusion-based generator synthesizes high-fidelity, temporally coherent wrist-view videos (ours), which align closely with ground-truth wrist observations.\nThe pipeline effectively bridges third-person anchor views and egocentric perspectives, ensuring geometric consistency and perceptual realism.",
                "position": 1858
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x17.png",
                "caption": "Figure 24:Demonstration of WoW when lift to 3D occupancy environment.We first plan and optimize a plausible path in 3D occupancy environment, and conducts a trajectory-guided video generation process afterwards to produce corresponding high-quality video following(Li et al.,2025b).",
                "position": 1876
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x18.png",
                "caption": "Figure 25:Inference procedure of Action-to-VideoAction-to-Video trains a latent video diffusion model in the latent space provided by pre-trained VAE. An autoregressive spatial-temporal transformer is used to predict future tokens conditioned on the corresponding action at each step.",
                "position": 1891
            },
            {
                "img": "https://arxiv.org/html/2509.22642/figs/Visual_Enhancement_case_1.png",
                "caption": "Figure 26:Case study illustrating Visual Style Transfer Enhancement",
                "position": 1924
            },
            {
                "img": "https://arxiv.org/html/2509.22642/x19.png",
                "caption": "Figure 27:Self-correction of VLM planning via world model simulation.(a) Our iterative loop: a VLM planner proposes an action, a world model simulates the future frame, and a VLM critic provides feedback, enabling the planner to refine its next step.\n(b) Terminal frames from the simulation, illustrating a successful plan (top) versus a detected failure (bottom) that triggers re-planning.",
                "position": 1965
            }
        ]
    },
    {
        "header": "9Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]