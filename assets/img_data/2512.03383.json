[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03383/figs/logo.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03383/x1.png",
                "caption": "Figure 1:(Proposed framework overview.)UniQL supports Transformers, SSMs, and hybrid models, enabling one-shot compression using a single server-class GPU. The on-device pruning of the quantized model is feasible and configurable based on the current device workload. We present actual latency on Nano 8G in relation to accuracy for different pruning rates across three distinct models on the right.\nCircle sizes correspond to model sizes.\nLatency is measured using 512 prefilling tokens and 512 generated tokens on Nano.",
                "position": 221
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03383/x2.png",
                "caption": "Figure 2:(The UniQL pipeline.)We devise pseudo-inverse-free, quantization-aware, and state-aware matrix decomposition methods for the grouped weights to obtain sorted weights (a).\nDuring fine-tuning, we sample global pruning rates, and masked out the weight channels (b).\nThe refined patches are fused into the weights, followed by model quantization for deployment (c).\nBased on the system utilization, we perform on-device adaptive pruning of the quantized model (d).",
                "position": 264
            }
        ]
    },
    {
        "header": "3Proposed Framework: UniQL",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03383/x3.png",
                "caption": "Figure 3:(Joint weight decomposition.)We visualize the group of sortedweightsin MLP (a), MHSA (b), and Mamba (c) blocks.\nThe group of weights for joint decomposition is shown in the same background color,e.g.,ùêñq\\mathbf{W}_{q}andùêñk\\mathbf{W}_{k}in the pink background, and other groups are distinguished by different colors.\nWe devise different types of joint compression algorithms that are efficient and quantization-aware to support on-device pruning.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2512.03383/x4.png",
                "caption": "Figure 4:(The fused kernel and SVD decomposition.)In the left illustration, gathering and slicing rotary positional embeddings by the index vector forQQandKKare fused in one kernel to reduce memory access. The embeddings for the pruned head dimensionDhd‚Ä≤D^{\\prime}_{\\mathrm{hd}}are gathered from the index arrayùêís‚Äãy‚Äãm\\mathbf{S}_{sym}in the fused kernel. On the right, we combine the diagonal matrixùö∫\\mathbf{\\Sigma}withùêî\\mathbf{U}as the group shares a quantization scaling factor to reduce the quantization errors.",
                "position": 527
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": []
    },
    {
        "header": "5Ablation Study",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix ADetailed Structured Sorting Algorithms",
        "images": []
    },
    {
        "header": "Appendix BBroader Evaluation Results",
        "images": []
    },
    {
        "header": "Appendix CAdditional Ablation Studies",
        "images": []
    },
    {
        "header": "Appendix DPareto-front Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03383/x5.png",
                "caption": "Figure 5:(Pareto-front analysis on A6000.)We evaluate the trade-off between average accuracy (%) and time-to-last-token (sec.) for various LLMs under different quantization and pruning configurations. Circle, square, and star markers denote GPTQ (W4A16), FP16, and our proposed UniQL (W4A16), respectively. Marker size indicates memory footprint.",
                "position": 2965
            },
            {
                "img": "https://arxiv.org/html/2512.03383/x6.png",
                "caption": "Figure 6:(Pareto-front analysis on Nano 8G.)We evaluate the trade-off between average accuracy (%) and time-to-last-token (sec.) for diverse LLMs with different quantization and pruning settings. Circle, square, and star markers represent TAO-HQQ (W4A16), FP16, and our UniQL (W4A16), respectively. Marker size reflects memory usage.",
                "position": 2969
            },
            {
                "img": "https://arxiv.org/html/2512.03383/x7.png",
                "caption": "Figure 7:(Energy efficiency analysis on A6000.)Nemotron-H incorporates SSM blocks to decrease KV cache memory requirements. UniQL continually offers superior energy efficiency for both Transformer and SSM architectures.",
                "position": 2973
            }
        ]
    },
    {
        "header": "Appendix EEnergy Profiling",
        "images": []
    },
    {
        "header": "Appendix FLayer-wise Pruning Rates",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03383/x8.png",
                "caption": "Figure 8:(Layer-wise pruning rates.)Hybrid model self-attention layers exhibit low pruning rates, as evidenced by the high BI scores in the figure.",
                "position": 3085
            }
        ]
    },
    {
        "header": "Appendix GImplementation Details",
        "images": []
    }
]