[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "Introduction",
        "images": []
    },
    {
        "header": "Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03616/scaling_law.png",
                "caption": "Figure 1:Plot of transformer parameter count vs value of the top activation to median ratio per model, in each respective final model checkpoint.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/start-end-ma-3-models.png",
                "caption": "Figure 2:Top activation magnitudes per layer in models Pythia-14M, Pythia-1.4B and Pythia-12B at revision step 0 and 143000, which correspond to the start and end of training. Pythia-14M reaches a top 1 to median ratio of  83, Pythia-1.4B reaches  2350, and Pythia-12B reaches  3200.",
                "position": 239
            }
        ]
    },
    {
        "header": "Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03616/figures/1B-ratio-per-layer.png",
                "caption": "Figure 3:Evolution of the ratio of top activations to median (Equation9) during training for Pythia 1B. It is a linear interpolation of 37 data points corresponding to different training checkpoints. Apart from the highest activation which is the focus of our study, we also plot ratios corresponding to the top 2 and 3 for comparison. The plots show the training steps on the x-axis, and the ratio of the top magnitudes to median activations on the y-axis.",
                "position": 254
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/heatmap_log_max_y.png",
                "caption": "(a)Middle-depth layers display significantly higher MAs than shallow and deep layers.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/heatmap_log_max_y.png",
                "caption": "(a)Middle-depth layers display significantly higher MAs than shallow and deep layers.",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/heatmap_max_x.png",
                "caption": "(b)Note the very stark change from shallow and deep layers to middle ones, particularly in the bigger (>410M) models.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/fits_6.9_1b.png",
                "caption": "Figure 5:Example fits for two model sizes: 1B and 6.9B and an example shallow, middle and deep layer from each. The plot shows the best fit for Equation10, and data points corresponding to Equation9. The last training step for the Pythia family is 143k.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/r2_plot_vertical.png",
                "caption": "(a)ExampleR2R^{2}values for various models.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/r2_plot_vertical.png",
                "caption": "(a)ExampleR2R^{2}values for various models.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/heatmap_r2_green.png",
                "caption": "(b)Full map of all models and all layers and the strength or ease of fit. A greener color means that massive activation trajectories in that coordinate can be modeled with high confidence with Equation10. Even the lower scoring locations still show evidence of reasonable fits.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/lambert-function.png",
                "caption": "Figure 7:Plot illustrating the change in the location of a peak relative to changes in (log) parameters of Equation14. Note that Only real valued solutions to the Lambert W function are pictured.t0t_{0}set to a fixed, typical value found across the Pythia model family. Small changes inγ\\gammacan have large effects in location of a peak.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03b3_2d_pdp_layer-index-norm_vs_attention-density.png",
                "caption": "(a)2D PDP plot relating parameterγ\\gamma, attention density, and layer position. Values in PDP shown in original input feature scale.",
                "position": 744
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03b3_2d_pdp_layer-index-norm_vs_attention-density.png",
                "caption": "(a)2D PDP plot relating parameterγ\\gamma, attention density, and layer position. Values in PDP shown in original input feature scale.",
                "position": 747
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03b3_2d_pdp_model-scale-log_vs_heads-per-layer.png",
                "caption": "(b)2D PDP plot relating parameterγ\\gamma, hidden layer size relative to attention head count. Values in PDP shown in original input feature scale.",
                "position": 752
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03b3_shap_summary.png",
                "caption": "(a)SHAP Summary Plot showing directional relationships between model architecture features and predicted values ofγ\\gamma",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03b3_shap_summary.png",
                "caption": "(a)SHAP Summary Plot showing directional relationships between model architecture features and predicted values ofγ\\gamma",
                "position": 774
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03b3_shap_waterfall_high.png",
                "caption": "(b)High Parameterγ\\gammaExample - Waterfall",
                "position": 779
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_K_shap_summary.png",
                "caption": "(a)SHAP Summary Plot for Parameter K revealing directional relationships between architectural features and steady-state predictions. Colors indicate feature values (red = high, blue = low).",
                "position": 796
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_K_shap_summary.png",
                "caption": "(a)SHAP Summary Plot for Parameter K revealing directional relationships between architectural features and steady-state predictions. Colors indicate feature values (red = high, blue = low).",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_K_shap_waterfall_high.png",
                "caption": "(b)SHAP Waterfall Plot for highest Parameter K prediction showing individual feature contributions. Predicted value f(x) = 1.811 in transformed space corresponds to approximately 5.1 in original scale.",
                "position": 804
            },
            {
                "img": "https://arxiv.org/html/2508.03616/figures/param_u03bb_shap_summary.png",
                "caption": "Figure 11:SHAP summary plot for parameterλ\\lambda, indicating directional relationships between architectural choices and values ofλ\\lambdapredicted by the top performing machine learning model.",
                "position": 827
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "Methodology",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]