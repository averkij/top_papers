[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18076/x1.png",
                "caption": "Figure 1:SUPEutilizes unlabeled trajectory datatwice, both forofflineunsupervised skill pretraining and foronlinehigh-level policy learning using RL.Left:in theofflinepretraining phase(Stage 1), we unsupervisedly learn both atrajectory segment encoder (a)and alow-level latent conditioned skill policy (b)via a behavior cloning objective where the policy is optimized to reconstruct the action in the trajectory segment.Right:in theonlineexploration phase(Stage 2), the pretrainedtrajectory segment encoder (a)and anoptimistic reward module (d)are used to pseudo-label the prior data and transform it into high-level trajectories(f)that can be readily consumed by a high-level off-policy RL agent. Leveraging these offline trajectories and the online replay buffer(e), we learn ahigh-level policy (c)that picks the pretrained low-level skills online to explore in the environment. Finally, the observed transitions and reward values are used to update the optimistic reward module and the online replay buffer.",
                "position": 61
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Skills fromUnlabeledPrior data forExploration (SUPE)",
        "images": []
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18076/x2.png",
                "caption": "(a)a)AntMaze: three maze layouts (medium,largeandultra), and four goals for each layout.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x2.png",
                "caption": "(a)a)AntMaze: three maze layouts (medium,largeandultra), and four goals for each layout.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2410.18076/extracted/5946876/figures/kitchen.png",
                "caption": "(b)b)Kitchen",
                "position": 270
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x3.png",
                "caption": "(c)c)Visual AntMaze: maze layout and image observation examples",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2410.18076/extracted/5946876/figures/viz_image_2.png",
                "caption": "",
                "position": 290
            },
            {
                "img": "https://arxiv.org/html/2410.18076/extracted/5946876/figures/viz_image_3.png",
                "caption": "",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2410.18076/extracted/5946876/figures/viz_image_4.png",
                "caption": "",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x4.png",
                "caption": "Figure 3:Aggregated normalized return across three different domains.Oursachieves the best performance through training on all three domains.ExPLOReachieves strong later stage performance on AntMaze, but struggles in high-dimensional Visual AntMaze and Kitchen tasks.Online w/ HILP SkillsandHILP w/ Offline Dataachieve decent initial return on Kitchen, but struggle to learn in all three domains.Online w/ Trajectory Skillsconsistently underperformsOursacross all three environments.Diffusion BC + JSRLlearns reasonably well in Kitchen, but performs much worse in AntMaze and Visual AntMaze.Onlinedoes not perform competitively at any stage of exploration. Section5.2contains details on the baselines we compare with. Each curve is an average over 8 seeds. For AntMaze, we aggregate over 3 maze layouts and 4 goals. For Kitchen, we aggregate over 3 tasks. For Visual AntMaze, we aggregate over 4 goals on one maze layout.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x5.png",
                "caption": "Figure 4:Normalized return on individual AntMaze and Kitchen tasks.Oursachieves the strongest performance on all tasks.Online w/ Trajectory Skillslearns much slower on all AntMaze tasks, and is asymptotically worse on two of the three more challenging Kitchen tasks.ExPLORestruggles to learn on Kitchen, and performs worse as maze size increases. None of the other baselines are competitive on any tasks.\nEach curve is an average over four goals with 8 seeds for AntMaze, and 16 seeds for Kitchen.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x6.png",
                "caption": "Figure 5:Coverage on three different AntMaze mazes, averaged over runs on four goals.Ourshas the best coverage performance on the challengingantmaze-ultra, and is only passed byHILP w/ Offline Dataonantmaze-large.Online w/ Traj. SkillsandOnline with HILP Skillsstruggle to explore after initial learning, andOnlineandDiffusion BC + JSRLgenerally perform poorly at all time steps.",
                "position": 589
            }
        ]
    },
    {
        "header": "6Discussion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACompute Resources",
        "images": []
    },
    {
        "header": "Appendix BVAE Architecture and hyperparameters",
        "images": []
    },
    {
        "header": "Appendix CImplementation details for baselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18076/x7.png",
                "caption": "Figure 6:Success rate on Visual AntMaze environment with and without ICVF.Oursworks well without ICVF, almost matching the original performance. However, the other baselinesOnline w/ Trajectory SkillsandExPLOReachieve far worse performance without ICVF, which shows that using offline data both for extracting skills and online learning leads to better utilization of noisy exploration bonuses. Initializing ExPLORe critic with ICVF helps, but does not substantially change performance.",
                "position": 1798
            }
        ]
    },
    {
        "header": "Appendix DICVF Implementation Details and Ablation Experiments for Visual Antmaze",
        "images": []
    },
    {
        "header": "Appendix EKL Penalty Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18076/x8.png",
                "caption": "Figure 7:Normalized return on three AntMaze mazes, comparing Ours with a KL regularized alternative (Ours (KL)).We thatOursconsistently outperformsOurs (KL)on all three mazes, with initial learning that is at least as fast and significantly improved asymptotic performance. OnlyOursis able to meet or surpass the asymptotic performance ofExPLOReon all mazes.",
                "position": 1814
            }
        ]
    },
    {
        "header": "Appendix FAntMaze Results by Different Maze Layouts and Goal Locations",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18076/x9.png",
                "caption": "Figure 8:Success rate by goal location.The addition of online RND inExPLOReleads to better performance on goals with less offline data coverage, and slightly worse performance on goals well-represented in the dataset.Oursconsistently matches are outperforms all other methods on all goals throughout training.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x10.png",
                "caption": "Figure 9:Coverage for every goal location on three antmaze environments.There is significant variation between goals, andOursconsistently has the best initial coverage performance on 11 of 12 goals. Flattening coverage compared to other methods can be at least partially attributed to having already found the goal, and sucessfully optimizing reaching that goal, rather than continuing to explore after already finding the goal.",
                "position": 1835
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x11.png",
                "caption": "Figure 10:Data corruption ablation on state-basedantmaze-large.Top: The success rate of different methods on these data corruption settings.Bottom: Visualization of the data distribution for each corruption setting. We experiment with two data corruption settings. Our method performs worse than the full data setting but still consistently outperforms all baselines.",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x12.png",
                "caption": "",
                "position": 1866
            },
            {
                "img": "https://arxiv.org/html/2410.18076/x13.png",
                "caption": "",
                "position": 1867
            }
        ]
    },
    {
        "header": "Appendix GRobustness Against Offline Data Corruptions",
        "images": []
    }
]