[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction & Related Work",
        "images": []
    },
    {
        "header": "2Problem Definition",
        "images": []
    },
    {
        "header": "3System Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08118/x1.png",
                "caption": "Figure 1:MirrorBenchArchitecture:Six-layer stack from low-level execution backends & persistence up through the core engine, plugin components, CLI & reporting, and task drivers. Top layers are user-facing; bottom layers are low-level infrastructure abstractions.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x2.png",
                "caption": "Figure 2:MirrorBenchexecution flow. The framework decomposes an evaluation job intounits{U1,…,Up}\\{U_{1},\\ldots,U_{p}\\}; each unitUiU_{i}iterates overepisodes{e1,…,en}\\{e_{1},\\ldots,e_{n}\\}produced by the dataset adapter and executed via task drivers. Metrics are computed per episode and aggregated within each unit with confidence intervals.",
                "position": 292
            }
        ]
    },
    {
        "header": "4Metrics & Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08118/x3.png",
                "caption": "Figure 3:Human-likeness of five user-proxy LLMs across four datasets.Higher is better for judge-based metrics (GTEval, PI, RNR). Lexical-diversity metrics arezz-scored to human baselines (0 is best). We fix the judge to Claude-4-Sonnet and the assistant to GPT-4o.",
                "position": 599
            }
        ]
    },
    {
        "header": "5Datasets & Tasks",
        "images": []
    },
    {
        "header": "6Experiments & Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08118/x4.png",
                "caption": "Figure 4:Judge sensitivity of judge-realism metrics on ChatbotArena with assistant & user-proxy both set to GPT-4o; bars vary thejudgemodel. Error bars are 95% CIs.",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x5.png",
                "caption": "Figure 5:Judge–human correlation on ChatbotArena:Correlation of Claude-4-Sonnet judge scores & human scores for GTEval and PI, evaluated onGemini-2.5-Prouser-proxy outputs (N=100 per metric). Solid line: linear fit; dashed: identity. Point size encodes local sample density. All correlations p<<0.001.",
                "position": 691
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x6.png",
                "caption": "Figure 6:Avg per-episode telemetry for GPT-4o as user-proxy & assistant, and judged by Claude-4-Sonnet across 4 datasets on 6 metrics (Fig.3); only non-cached episodes considered.Left:Token usage by role (darker: input, lighter: output).Right:Cumulative latency per episode.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x7.png",
                "caption": "Figure 7:Throughput vs. concurrency forGTEvalon ChatbotArena (async backend, cache off); only thejudgevaries. User-proxy and assistant fixed to GPT-4o.",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x8.png",
                "caption": "Figure 8:Cost-quality trade-off forPI:cost per evaluation (USD) vs. PIΔ​w\\Delta w(↑\\uparrowbetter). Judge = Claude-4-Sonnet; assistant = GPT-4o; temperature = 0; cache off. Markers denote user-proxies; labels denote datasets. Dashed line: Pareto frontier. See Table4for model pricing.",
                "position": 724
            }
        ]
    },
    {
        "header": "7Conclusion & Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALayer-by-Layer Architecture Details",
        "images": []
    },
    {
        "header": "Appendix BDetails on Benchmarking datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08118/x9.png",
                "caption": "Figure 9:Human-likeness of user-proxy LLMs across four datasets:higher is better for judge metrics; diversity scores are z-scored to humans (0 is best). Judge & assistant fixed to GPT-4o.",
                "position": 1787
            }
        ]
    },
    {
        "header": "Appendix CAuxiliary User-Proxy Evaluations",
        "images": []
    },
    {
        "header": "Appendix DImplementation Details of Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08118/x10.png",
                "caption": "Figure 15:Job configuration (abridged).YAML that fully specifies aMirrorBenchrun: backend and concurrency, proxy/assistant clients, dataset binding, judge metrics (with calibration controls), and the mirror-conversation driver.",
                "position": 2293
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x11.png",
                "caption": "Figure 16:Plan manifest (JSON, abridged).Fully resolved, replayable spec listing the (proxy, dataset, metric, seed) units, driver params, versions, andconfig_hash, enabling deterministic re-runs.",
                "position": 2296
            },
            {
                "img": "https://arxiv.org/html/2601.08118/x12.png",
                "caption": "Figure 17:Final run report (JSON, abridged).Aggregate metric scores with 95% CIs, calibration controls (HH/PP), unit grid, and run metadata for the completed evaluation.",
                "position": 2299
            }
        ]
    },
    {
        "header": "Appendix EPrompts, Config, Manifest & Report",
        "images": []
    }
]