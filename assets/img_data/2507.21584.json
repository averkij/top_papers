[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21584/fig/tars.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x1.png",
                "caption": "",
                "position": 104
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x2.png",
                "caption": "",
                "position": 105
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21584/x3.png",
                "caption": "Figure 2:Motivation illustration for TARS.\n(a) and (b) illustrate standard DPO and our token-adaptive strategy.\n(c) shows a VQA example where DPO hallucinates, while TARS avoids ungrounded output.\n(d) and (e) visualize token-to-query attention maps during decoding. DPO over-attends to spurious tokens, while TARS attends to causally grounded visual-semantic cues.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21584/x4.png",
                "caption": "Figure 3:Overview ofTARS. TARS reformulates preference optimization as a Min–Max problem:\n(1) The maximization branch perturbs visual-agnostic tokens to simulate semantically shifted contexts (red dashed box);\n(2) The minimization branch fine-tunes the model to align with human preferences via the DPO objective (purple dashed box).\nTARS encourages the model to attend to causally grounded visual signals rather than spurious textual correlations, thereby reducing hallucinations.",
                "position": 260
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21584/x5.png",
                "caption": "Table 2:Ablation results of token-level perturbation (TP), cross-modal alignment score (CAS), and spectral preference alignment (SPA) on the AMBER and OBJHal using 7B scale.",
                "position": 754
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x5.png",
                "caption": "Figure 4:AMBER hallucination rate vs. preference data scale across 7B and 13B models.",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x6.png",
                "caption": "Figure 5:Distribution of hidden representations across preference-aligned, non-hallucinated, and hallucinated responses of different MLLMs. Top and right margins show marginal distributions along key feature dimensions. We extract representations from 100 preference training instances and 200 AMBER inputs across text and vision modalities. Responses to AMBER inputs are categorized as non-hallucinated or hallucinated based on factual coherence. TARS aligns with preference data while avoiding overfitting to spurious correlations, demonstrating superior factual fidelity.",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x7.png",
                "caption": "",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x8.png",
                "caption": "",
                "position": 843
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Overview",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": []
    },
    {
        "header": "8Algorithm Flowchart",
        "images": []
    },
    {
        "header": "9Extended Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21584/x9.png",
                "caption": "Figure 6:Comparison of average scores across question categories on the MMHal benchmark.",
                "position": 2534
            }
        ]
    },
    {
        "header": "10Additional Experimental Results",
        "images": []
    },
    {
        "header": "11Discussions and Insights",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21584/x10.png",
                "caption": "Table 8:Qualitative comparisons between DPO and our proposed TARS across five diverse image-prompt pairs, denoted as (a)–(e).\nEach row presents the same visual input and accompanying question, with model responses shown for both methods.\nHallucinated content is highlighted inred, while accurate visual grounding is marked ingreen.\nTARS consistently produces more faithful and informative responses, demonstrating superior grounding and hallucination mitigation even under visually ambiguous or linguistically subtle conditions.",
                "position": 2804
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x11.png",
                "caption": "",
                "position": 2813
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x12.png",
                "caption": "",
                "position": 2819
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x13.png",
                "caption": "",
                "position": 2825
            },
            {
                "img": "https://arxiv.org/html/2507.21584/x14.png",
                "caption": "",
                "position": 2831
            }
        ]
    },
    {
        "header": "12Qualitative Examples",
        "images": []
    }
]