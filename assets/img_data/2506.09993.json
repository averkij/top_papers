[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09993/x1.png",
                "caption": "Figure 1:Text-Aware Image Restoration (TAIR).Given a low-quality (LQ) image containing degraded text, our method faithfully restores the original textual content with high legibility and fidelity, whereas previous diffusion-based models[5,94,51,9]often fail to recover the text regions.",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09993/x2.png",
                "caption": "Figure 2:SA-Text curation pipeline.First, a text-spotting model such as DG-Bridge Spotter[31]is applied to the entire image to detect text regions. Since detection at the full-image scale may fail to capture certain text instances, we further extract image patches corresponding to the detected regions and reapply the same model to each patch in order to detect potential false negatives. Next, two vision-language models (VLMs)[4,57]transcribe the text within each bounding box, and only patches with consistent predictions from both models are retained and annotated. Finally, a single VLM[4]classifies each patch into one of three categories based on image sharpness and blurriness.",
                "position": 194
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3SA-Text Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09993/x3.png",
                "caption": "Figure 3:Illustration of our dataset curation pipeline’s effectiveness.(a) Original high-resolution image with multiple text instances. (b) Ambiguous text instances are removed during the Vision–Language Model (VLM) filtering stage when the two VLMs produce differing recognition outputs. (c) Incorrect detections from the full image are corrected by re-running the detection model on smaller crops; here, the phrase \"Powered by Yorkshire Bank\" is successfully split into individual words. (d) False negatives (missed instances) from the initial detection on the entire image are effectively captured during the second detection pass on the smaller crops; the previously missed instance \"WORK\" is now correctly detected.",
                "position": 232
            },
            {
                "img": "https://arxiv.org/html/2506.09993/x4.png",
                "caption": "Figure 4:Example images from our SA-Text.Our dataset comprises high-quality, diverse images featuring text in varied sizes, styles, and layouts—including curved, rotated, and complex forms—providing a robust foundation for the proposed TAIR task.",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2506.09993/x5.png",
                "caption": "Figure 5:Overview of the TeReDiff architecture, training, and inference pipeline.TAIR integrates a text-spotting module into a diffusion image restoration framework, using text supervision during training and recognized text as a prompt at inference to enhance text-aware image restoration.",
                "position": 341
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ASA-Text Curation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09993/x6.png",
                "caption": "Figure 6:Examples of images classified by blurriness.Images are categorized into Levels 1–3: Level 1 (very blurry), Level 2 (slightly blurry), and Level 3 (clearly focused). Images classified as Level 1 and Level 2 are excluded from the final dataset to ensure that only clearly focused (Level 3) images are used for TAIR.",
                "position": 1460
            }
        ]
    },
    {
        "header": "Appendix BDiffusion Features for Text Spotting",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09993/x7.png",
                "caption": "Figure 7:Example samples for user study.",
                "position": 2268
            }
        ]
    },
    {
        "header": "Appendix EAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09993/x8.png",
                "caption": "Figure 8:Qualitative results on SA-Text test set Level 1.",
                "position": 2281
            },
            {
                "img": "https://arxiv.org/html/2506.09993/x9.png",
                "caption": "Figure 9:Qualitative results on SA-Text test set Level 2.",
                "position": 2286
            },
            {
                "img": "https://arxiv.org/html/2506.09993/x10.png",
                "caption": "Figure 10:Qualitative results on SA-Text test set Level 3.",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2506.09993/x11.png",
                "caption": "Figure 11:Qualitative results on Real-Text.",
                "position": 2296
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]