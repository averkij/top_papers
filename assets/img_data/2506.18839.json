[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18839/x1.png",
                "caption": "Figure 1:Our method enables the creation of 4D scenes from a text prompt by combining a diffusion model that directly generates synchronized multi-view videos with a feedforward reconstruction model that efficiently produces Gaussian-based representations.",
                "position": 69
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18839/x2.png",
                "caption": "Figure 2:Our 4D video model supports input types including: (a) a fixed-view video, (b) a freeze-time video showing multiple angles of a scene at a single timestep, and (c) a combination of both. Each input can be generated from a text prompt using standard video models.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2506.18839/x3.png",
                "caption": "Figure 3:We analyze three architectures for 4D video generation: (a) sequential cross-view and cross-time attention, (b) parallel cross-view and cross-time attention with an extra synchronization layer, and (c) our proposed architecture using fused view-time attention with masked self-attention.",
                "position": 330
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18839/x4.png",
                "caption": "Figure 4:Overview of our feedforward reconstruction model. Built on top ofVGGTwang2025vggt, it incorporatestemporal attentionlayers,camera token replacementto ensure consistent cameras over time, and aGaussian headthat predicts Gaussian parameters.",
                "position": 396
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18839/x5.png",
                "caption": "Figure 5:Visual comparison of 4D video generation methods. Each image includes a temporal slice (right) along the red line to reveal temporal flickering. ReCamMaster-V2 shows strong flickering; ReCamMaster-V1 has inconsistent backgrounds across views. TrajectoryCrafter exhibits artifacts from noisy point clouds. 4Real-Video misses thin structures, and SynCamMaster produces inconsistent synthetic-style results. Our method achieves the best visual quality and consistency.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2506.18839/x6.png",
                "caption": "Figure 6:Color and depth renderings of Gaussians produced by inputting our generated 4D video grids into the reconstruction model.",
                "position": 670
            },
            {
                "img": "https://arxiv.org/html/2506.18839/x7.png",
                "caption": "Figure 7:Qualitative comparison of our feedforward reconstruction model with the baselines on novel view renderings of dynamic scenes from the Neural3DVideoli2022neural3dvideosynthesisdataset.",
                "position": 931
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AVisual results for comparing architectures for 4D video generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18839/x8.png",
                "caption": "Figure S1:Qualitative comparison of the outputs of our proposed architecture (fused view-time attention) with our implementation of sequential and parallel architectures and SV4Dsv4d.",
                "position": 983
            }
        ]
    },
    {
        "header": "Appendix BAdditional details on 4D video diffusion model",
        "images": []
    },
    {
        "header": "Appendix CAdditional details on the feedforward reconstruction model",
        "images": []
    },
    {
        "header": "Appendix DVisual results for static scene novel view synthesis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18839/x9.png",
                "caption": "Figure S2:Qualitative comparison of our renderings with the baselines GSLRMgslrm2024and BTimerliang2024btimeron the task of novel view synthesis for static scenes. Each method includes two variations, using 4 and 16 input views. Note that all variations of GSLRM and BTimer requireinput camera parametersandmanual scene scale tuning.",
                "position": 1176
            },
            {
                "img": "https://arxiv.org/html/2506.18839/x10.png",
                "caption": "Figure S3:Qualitative comparison of our renderings with the baselines PixelSplatcharatan23pixelsplatand MVSPlatchen2024mvsplaton the task of novel view synthesis for static scenes. Note that the baselines requireinput camera parameters, whereas our method infers the camera parameters from the input images.",
                "position": 1179
            },
            {
                "img": "https://arxiv.org/html/2506.18839/x11.png",
                "caption": "Figure S4:Qualitative comparison of our results with the baselines for novel view synthesis of static scenes, where the target camera deviates significantly from the input trajectory.",
                "position": 1185
            }
        ]
    },
    {
        "header": "Appendix EDataset details",
        "images": []
    },
    {
        "header": "Appendix FBroader impact",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]