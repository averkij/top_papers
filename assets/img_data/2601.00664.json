[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00664/x1.png",
                "caption": "",
                "position": 163
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Background",
        "images": []
    },
    {
        "header": "4Avatar Forcing",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00664/x2.png",
                "caption": "Figure 2:Overall architecture of Avatar Forcing. We encode the use motion and audio, as well as avatar audio into a unified condition by Dual Motion Encoder. Causal Motion Generator infer the motion latent block of the avatar, which are then decoded into an avatar video.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x3.png",
                "caption": "Figure 3:Architecture ofvŒ∏v_{\\theta}. The look-ahead causal attention mask enables a smooth transition across the blocks.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x4.png",
                "caption": "Figure 4:Architectural comparison between bidirectional and causal structure. (a) Bidirectional DiT used in INFP[infp]requires access to the entire temporal window for motion generation. (b) Our blockwise causal DFoT predicts the next block without using future context and supports KV caching.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x5.png",
                "caption": "Figure 5:Variance visualizationof the L2-norm of 3DMM expressions[spectre]for the speaker and listener on ViCo[rlhg]dataset. Higher variance indicates higher expressiveness.",
                "position": 496
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x6.png",
                "caption": "Table 1:Quantitative comparison resultson RealTalk[realtalk]. Best results highlighted inbold.‚àódenotes the reproduced version that is publicly unavailable. We also report the results from a non-interactive talking head model[float], shown in gray, for reference.",
                "position": 499
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x6.png",
                "caption": "Figure 6:Human preference studyon interactive avatar generation models, comparing Avatar Forcing and INFP‚àó.",
                "position": 603
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00664/x7.png",
                "caption": "Figure 7:Qualitative comparison of interactive head avatar generation modelson the RealTalk[realtalk]dataset. Our model generates more reactive (red arrow) and expressive (red box) avatar motion compared to INFP*.\nWe provide the videos in supplementary materials.",
                "position": 792
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x8.png",
                "caption": "Figure 8:Ablation study on the user motion. Withoutùê¶u\\mathbf{m}_{u}, the avatar remains static even when the user smiles. Withùê¶u\\mathbf{m}_{u}, our model reacts by smiling after the user (red arrow) and shifting to a focused expression when the user begins speaking (green box).",
                "position": 934
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x9.png",
                "caption": "Figure 9:Ablation study on preference optimization. Model fine-tuned with DPO produces more expressive motion (red box) and reactive (red arrow) compared to model without DPO.",
                "position": 937
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix ABackground",
        "images": []
    },
    {
        "header": "Appendix BDetails on Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00664/x10.png",
                "caption": "Figure 10:Overview of Motion Latent Encoder. It encodes an image into a latent vector that has explicit identity-motion decomposition.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x11.png",
                "caption": "Figure 11:Detailed architecture for Motion Generator invŒ∏v_{\\theta}.",
                "position": 1020
            }
        ]
    },
    {
        "header": "Appendix CDetails on Preference Optimization",
        "images": []
    },
    {
        "header": "Appendix DExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.00664/x12.png",
                "caption": "Figure 12:Attention Mask Comparison.(Left) framewise causal mask; (Middle) blockwise causal mask; (Right) blockwise causal look-ahead mask (Ours).",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x13.png",
                "caption": "Figure 13:Human evaluation interface. (Left) Instructions for human evaluation; (Middle) A reference sheet for consistent evaluation; (Right) Test and answer sheet.",
                "position": 1355
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x14.png",
                "caption": "Figure 14:Qualitative comparison on talking head avatar generation.",
                "position": 1400
            },
            {
                "img": "https://arxiv.org/html/2601.00664/x15.png",
                "caption": "Figure 15:Qualitative comparison on listening head avatar generation.",
                "position": 1404
            }
        ]
    },
    {
        "header": "Appendix EDiscussion",
        "images": []
    }
]