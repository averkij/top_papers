[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x1.png",
                "caption": "Figure 1:Frame In-N-Outpresents a new task in the image-to-video generation that extends the first frame into anunboundedcanvas region, where the model could be conditioned on identity reference with motion trajectory control to achieve Frame In and Frame Out cinematic technique.",
                "position": 77
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Problem Definition",
        "images": []
    },
    {
        "header": "4Dataset Curation and In-N-Out Pattern Recognition",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x2.png",
                "caption": "Figure 2:Data Curation Pipeline.Our curation pipeline will provide high-quality filtered videos, text prompts, tracking trajectories with semantic labels, and bounding boxes that can be ideal partitions between the first frame and canvas region.",
                "position": 281
            }
        ]
    },
    {
        "header": "5Frame In-N-Out Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x3.png",
                "caption": "Figure 3:Main Architecture.Our video Diffusion Transformer embraces the first frame with canvas expansion, motion, identity reference, and text prompt as the conditions for video generation.",
                "position": 348
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x4.png",
                "caption": "Figure 4:Qualitative comparison on our benchmark dataset.In (a), we compare our model on Frame Out cases against DragAnything[70]and ToRA[87]. Both baselines fail to fully move the person outside the image boundaries, while our model successfully handles a complete exit. In (b), we evaluate Frame In scenarios against Phantom[36]and SkyReals-A2[13]. Only our model can reach Frame In effect with the designated identity.",
                "position": 811
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AOverview",
        "images": []
    },
    {
        "header": "Appendix BDataset Curation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x5.png",
                "caption": "Figure 5:Inference Pipeline.",
                "position": 2300
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix DAblation Study",
        "images": []
    },
    {
        "header": "Appendix EAdditional Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x6.png",
                "caption": "Figure 6:Detailed Conditioning for Generated Videos on Teaser Image.",
                "position": 2460
            },
            {
                "img": "https://arxiv.org/html/2505.21491/x7.png",
                "caption": "Figure 7:More Generated Examples.",
                "position": 2464
            },
            {
                "img": "https://arxiv.org/html/2505.21491/x8.png",
                "caption": "Figure 8:Extra Frame In Comparisons.",
                "position": 2484
            },
            {
                "img": "https://arxiv.org/html/2505.21491/x9.png",
                "caption": "Figure 9:Extra Frame Out Comparisons.",
                "position": 2487
            }
        ]
    },
    {
        "header": "Appendix FFull Canvas Generation Extension",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.21491/x10.png",
                "caption": "Figure 10:Full Canvas Generation Extension.We show two examples from the teaser image of how our unbounded canvas generation outside the first frame region. The green bounding box is the final generated video that will be cropped in the inference.",
                "position": 2494
            },
            {
                "img": "https://arxiv.org/html/2505.21491/x11.png",
                "caption": "Figure 11:Limitations.Given the same input and conditions, our model may produce different outputs under different random seeds. We consider (a) to be an ideal case and illustrate several limitations: camera motion ambiguity in (b), ID reference pose ambiguity in (c), overly large reference objects in (d), and overly small reference objects in (e).",
                "position": 2514
            }
        ]
    },
    {
        "header": "Appendix GLimitation",
        "images": []
    }
]