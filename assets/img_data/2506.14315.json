[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14315/x1.png",
                "caption": "Figure 1.ImmerseGen creates panoramic 3D worlds from input prompts by generating compact alpha-textured proxies through agent-guided asset design and arrangement, alleviating the reliance on rich and complex assets while ensuring diversity and realism, which is tailored for immersive VR experience.",
                "position": 124
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14315/x2.png",
                "caption": "Figure 2.Asset comparison from different sources.We compare assets created by learning-based generative methods (blue captions) and or artists (green captions).\nOur generative RGBD-textured proxy assets achieves better visual details than existing models(Zhang et al.,2024c; Xiang et al.,2024)with fewer triangles, delivering realism comparable to artist-created high-poly or baked assets.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x3.png",
                "caption": "Figure 3.Overview.Given a user’s textual input, our method first retrieve a base terrain and apply terrain-conditioned texturing to synthesize RGBA terrain texture and skybox aligned with base mesh, forming the base world.\nNext, We enrich the environment by introducing lightweight assets, where VLM-based asset agents\nare used to select appropriate templates, design detailed asset prompts and determine asset arrangement within the scene.\nEach placed asset is then instantiated as alpha-textured assets through context-aware RGBA texture synthesis.\nFinally, we enhance multi-modal immersion by incorporating dynamic visual effects and synthesized ambient sound based on the generated scene.",
                "position": 230
            }
        ]
    },
    {
        "header": "2.Related works",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14315/x4.png",
                "caption": "Figure 4.Workflow of base world generation.Panoramic textures for terrain mesh and sky are generated for the base world. To tame the diffusion model for terrain texturing, we propose geometric adaption (b) for depth control and user-centric texture mapping (c).",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x5.png",
                "caption": "Figure 5.The proposedcontext-aware texture synthesis(a) produces diverse RGBA textures directly on lightweight proxies with coherent context for both foreground and midground scenery (b).",
                "position": 375
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x6.png",
                "caption": "Figure 6.The proposedsemantic grid-based analysisoverlays a labeled grid with masked unsuitable regions as visual prompts, enabling the VLM agent to progressively select grid cells in a coarse-to-fine manner, enhancing the accuracy and semantic coherence of asset arrangement.",
                "position": 409
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x7.png",
                "caption": "Figure 7.We compare our method with Infinigen(Raistrick et al.,2023a), DreamScene360(Zhou et al.,2025), WonderWorld(Yu et al.,2025)and LayerPano3D(Yang et al.,2024d)based on the generated 3D scenes using identical text prompts, visualizing both panoramic and perspective views of the generated scenes.",
                "position": 430
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14315/x8.png",
                "caption": "Figure 8.We present more examples of generated environments in panoramic and perspective views.",
                "position": 604
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x9.png",
                "caption": "Figure 9.We analyze the geometric adaptation and fine-tuning of the conditional network for terrain-conditioned texture generation.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x10.png",
                "caption": "Figure 10.We inspect the efficacy of semantic grid-based analysis of our asset arranger by comparing it with random layout, LLM-based layout and naïve VLM-based layout.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x11.png",
                "caption": "Figure 11.We analyze the contribution of different scenery by ablating proxy scenery of different types.",
                "position": 707
            }
        ]
    },
    {
        "header": "5.Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "S1.Implementation details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14315/x12.png",
                "caption": "Figure S1.We compare the original matting and tile-based matting.",
                "position": 2073
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x13.png",
                "caption": "Figure S2.We compare our method with other SOTAs on terrain texturing.",
                "position": 2123
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x14.png",
                "caption": "Figure S3.We compare the RGBA texture synthesis with other methods.",
                "position": 2180
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x15.png",
                "caption": "Figure S4.We show the enhancement of bottom view before and after repainting and displacement.",
                "position": 2194
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x16.png",
                "caption": "Figure S5.We compare our method with DreamScene360(Zhou et al.,2025)using panoramic images from our resulting world.",
                "position": 2252
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x17.png",
                "caption": "Figure S6.Examples of Generated Scenes in Extended Styles",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x18.png",
                "caption": "Figure S7.Prompt Examples for Agent Selector and Designer.",
                "position": 2311
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x19.png",
                "caption": "Figure S8.Examples of Prompts for Agent Arranger (including coarse arranger and fine arranger).",
                "position": 2316
            },
            {
                "img": "https://arxiv.org/html/2506.14315/x20.png",
                "caption": "Figure S9.Prompt Examples of effect agent and sound agent for immersion enhancement.",
                "position": 2321
            }
        ]
    },
    {
        "header": "S2.More Experiments",
        "images": []
    }
]