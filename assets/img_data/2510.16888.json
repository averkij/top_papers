[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16888/figure/440551759288100_.pic_hd.jpg",
                "caption": "",
                "position": 65
            },
            {
                "img": "https://arxiv.org/html/2510.16888/figure/hf-logo.png",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x1.png",
                "caption": "Figure 1:On ImgEdit(Ye et al.,2025b)and GEdit-Bench(Liu et al.,2025b)leaderboards, our method achievesstate-of-the-artperformance.",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x2.png",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16888/x3.png",
                "caption": "Figure 2:Overview of the Edit-R1 pipeline.\nOur framework consists of three parts:\n1) We employ the DPM-Solver(Lu et al.,2022)to perform a rapid rollout, generating a group of candidate images from the policy.\n2) We use implicit feedback from MLLM to score the image editing effect and provide rewards. The scoring instructions include both a basic instruction for general editing requirements and a task instruction designed for fine-grained scoring based on the specific editing task type.\n3) We fine-tune the velocity predictor using DiffusionNFT(Zheng et al.,2025), enhanced bygroup filteringmethod that removes low-variance groups.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x4.png",
                "caption": "Figure 3:An example of “change the car to red” where samples within a group are typically successful, causing noise amplification after normalization.",
                "position": 301
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16888/x5.png",
                "caption": "Figure 4:Qualitative comparison of basic editing capabilities before and after policy optimization.. Basic editing refers to single-step modifications applied to an image.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x6.png",
                "caption": "Figure 5:Data Composition Overview. Our dataset comprises 9 tasks: Replace, Adjust, Remove, Background, Hybrid, Action, Text Edit, Redbox Control, Reference, and Extract.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x7.png",
                "caption": "Figure 6:Quantitative comparison on GEdit-Bench(Liu et al.,2025b).Boldindicates the best performance.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x7.png",
                "caption": "Figure 7:User study for Kontext-based (up) and Qwen-based (bottom) model.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x8.png",
                "caption": "",
                "position": 832
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x9.png",
                "caption": "Figure 8:Results of different policy optimization methods on FLUX.1-Kontext [Dev].",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x9.png",
                "caption": "Figure 8:Results of different policy optimization methods on FLUX.1-Kontext [Dev].",
                "position": 847
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x10.png",
                "caption": "Figure 9:Reward hacking phenomenon observed in the 3B reward model (Left) and Training reward dynamics across varying reward model scales (Right).",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x11.png",
                "caption": "",
                "position": 899
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x12.png",
                "caption": "Figure 10:Pairwise accuracy of different reward methods against human preferences",
                "position": 905
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x12.png",
                "caption": "Figure 11:Performance comparison across different editing tasks.",
                "position": 953
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining Settings",
        "images": []
    },
    {
        "header": "Appendix BDetailed Human Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.16888/x13.png",
                "caption": "Figure 12:Performance comparison across different editing tasks.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x13.png",
                "caption": "Figure 12:Performance comparison across different editing tasks.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x14.png",
                "caption": "Figure 13:Performance comparison across different models.",
                "position": 2006
            },
            {
                "img": "https://arxiv.org/html/2510.16888/x15.png",
                "caption": "Figure 14:Score distribution across different reward methods",
                "position": 2023
            }
        ]
    },
    {
        "header": "Appendix CPrompt Template",
        "images": []
    }
]