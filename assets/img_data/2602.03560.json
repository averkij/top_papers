[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background & Motivation",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03560/x1.png",
                "caption": "Figure 1:HySparse Architecture Diagram. Each full attention layers is interleaved with multiple sparse attention layers. Sparse attention directly reuses the KV cache and important token indices from the preceding full attention layer.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03560/x2.png",
                "caption": "Figure 2:HySparse Accuracy vs. Training Iterations.",
                "position": 1038
            }
        ]
    },
    {
        "header": "5Discussion & Future Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]