[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08521/x1.png",
                "caption": "Figure 1:(a) Comparison of the performance of RL algorithms with and without HA-DW on Qwen3-4B-Base across five mathematical reasoning benchmarks. (b) Significant biased advantage estimation on the MATH dataset under 8 and 128 rollouts. (c) Performance gain by GRPO+HA-DW on MATH500 stratified by difficulty levels.",
                "position": 160
            }
        ]
    },
    {
        "header": "2Why Your Advantage Estimation is Biased?",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08521/x2.png",
                "caption": "Figure 2:Illustration of advantage estimation bias as a function ofptp_{t}and\ngroup sizeGG.",
                "position": 612
            }
        ]
    },
    {
        "header": "3Proposed Solution",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08521/x3.png",
                "caption": "Figure 3:HA-DW consists of two collaborative phases. In the first phase, an evolving difficulty anchor incorporates cross-batch historical information by propagating the model’s prior through a history buffer, capturing long-term reward trends. In the second phase, prompt weights are adaptively adjusted based on their estimated difficulty under the model’s evolving state, compensating for biased advantage estimates.",
                "position": 621
            }
        ]
    },
    {
        "header": "4Theoretical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08521/latex/main-experiment-0102.jpg",
                "caption": "Figure 4:Comparison of training dynamics under different training strategies. Average accuracy across five benchmarks, training reward and response length of Qwen3-4B-Base and Qwen3-8B-Base on different training methods.",
                "position": 1109
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Related Work",
        "images": []
    },
    {
        "header": "Appendix BDetailed Instantiations for GRPO and Related Algorithms",
        "images": []
    },
    {
        "header": "Appendix CSetup Details",
        "images": []
    },
    {
        "header": "Appendix DTheoretical Proof",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08521/x4.png",
                "caption": "Figure 5:Illustration of advantage bias under truncated Gaussian rewards for different group sizes.",
                "position": 3849
            }
        ]
    },
    {
        "header": "Appendix ESupplementary Experiments",
        "images": []
    },
    {
        "header": "Appendix FHard Evolving Difficulty Anchor",
        "images": []
    },
    {
        "header": "Appendix GPrompt",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.08521/x5.png",
                "caption": "Figure 6:The distribution of prompts by the number of correct and incorrect responses on the MATH dataset and DAPO-Math-17k under 8 and 128 rollouts of Qwen3-4B-Base.",
                "position": 4006
            }
        ]
    },
    {
        "header": "Appendix HCase Study",
        "images": []
    }
]