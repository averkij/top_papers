[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05965/figs/logo.png",
                "caption": "",
                "position": 80
            },
            {
                "img": "https://arxiv.org/html/2512.05965/x1.png",
                "caption": "",
                "position": 118
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05965/x2.png",
                "caption": "Figure 2:The Pipeline ofThink-while-Edit.EditThinker is a multi-round instruction iterative refinement framework. In the first round, the original imageIs​r​cI_{src}and instructionTsT_{s}are fed into an editor to produce an initial edited imageIe​d​i​ttI_{edit}^{t}. This edited image, along with the original image and instruction, is then fed into EditThinker, which generates the edit scoreStS_{t}, refined promptTtT_{t}, and corresponding reasoning processRtR_{t}. If the score falls below a threshold, the framework proceeds to the next iteration with the refined prompt until a satisfactory result is achieved.",
                "position": 200
            }
        ]
    },
    {
        "header": "3Think-while-Edit",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05965/x3.png",
                "caption": "Figure 3:Data construction pipeline of ourThinkEdit.We construct our dataset through four sequential steps: (1)Trajectory Generation:We use several image edit models and expert evaluator GPT-4.1 to iteratively edit image, evaluate it and generates refined instructions until issuing a⟨stop⟩\\langle\\text{stop}\\rangletoken. (2)Trajectory Filter:An edit scorer assigns scoresStS_{t}to each step, retaining only trajectories wheremax⁡(St>1)≥S1\\max(S_{t>1})\\geq S_{1}and truncating them at the highest-scoring stepkk. (3)Step-wise Filter:We unroll trajectories into individual training samples pairing inputs (Is​r​cI_{src},Ie​d​i​tt−1I_{edit}^{t-1},TsT_{s},Tt−1T_{t-1}) with outputs (Rt,TtR_{t},T_{t}), then balance the dataset across task types and score distributions. (4)Data Partition:The filtered data is split for SFT and RL training.",
                "position": 292
            }
        ]
    },
    {
        "header": "4ThinkEditDataset",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AKris-Bench Result",
        "images": []
    },
    {
        "header": "Appendix BMore Ablation Analysis",
        "images": []
    },
    {
        "header": "Appendix CAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DDetails of ThinkEdit-140K Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.05965/x4.png",
                "caption": "Figure 4:Qualitative visualizations of EditThinker paired with different editors.\nSubfigures (a) and (b) show results with FLUX.1 Kontext [Dev], (c) and (d) use OmniGen2, and (e) and (f) use Qwen-Image-Edit.",
                "position": 1812
            },
            {
                "img": "https://arxiv.org/html/2512.05965/x5.png",
                "caption": "Figure 5:Visualization of EditThinker’s reasoning traces and intermediate editing results when paired with FLUX.1 Kontext [Dev]. The figure illustrates how the Thinker evaluates the current output, identifies issues, and iteratively refines the instruction over multiple rounds.",
                "position": 1818
            }
        ]
    },
    {
        "header": "Appendix EVisualization",
        "images": []
    }
]