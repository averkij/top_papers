[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06745/x1.png",
                "caption": "Figure 1:The ONEBench Framework.Left: ONEBench comprises a set of models, a pool of data samples spanning multiple test sets, metadata describing models and data samples, and a collection of heterogeneous, sample-level measurements.Right: the user formulates a query to capture the desired model capability, using a mix of structured metadata filters and semantic search. Selected models are then ranked on a subset of data samples that meet the specified criteria.",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x2.png",
                "caption": "Figure 2:Top-10 model ranking changes across different aggregation methods.Our Plackett-Luce-based rank aggregation method (Ours) shows the most similarity to the Ground Truth model rankings (GT). However, there is a progressive degradation in ranking accuracy for LMArena (LMArena) and Elo (ELO). Comparisons are shown for ONEBench-LLM (top) and ONEBench-LMM (bottom). Our method best preserves the ranking of the top-10 models.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x3.png",
                "caption": "Figure 3:Sample-efficient convergence (top) and robustness to sparsity.KendallœÑùúè\\tauitalic_œÑbetween ground-truth ranking and different ranking methods as random individual data samples are dropped (top) and model measurements are randomly removed (bottom). Methods typically remain robust to missing data, with Plackett-Luce consistently achieving higher correlation, even with 95% measurements missing.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x3.png",
                "caption": "",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x4.png",
                "caption": "",
                "position": 461
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x5.png",
                "caption": "Figure 4:Constituent datasets of ONEBench-LLM (left) and OneBench-LMM (right). We provide task type, metric, and license about each dataset inTab.2andTab.3.",
                "position": 501
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x6.png",
                "caption": "Figure 5:Capability Probing (Qualitative):we provide six sample retrieval results for a set of queries covering a diverse set of topics and report the top-5 models for each query.",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2412.06745/x7.png",
                "caption": "Figure 6:Additional qualitative analysis for ONEBench‚Äôs capability probing for selected queries.",
                "position": 5318
            }
        ]
    },
    {
        "header": "Part IAppendix",
        "images": []
    }
]