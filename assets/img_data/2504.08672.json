[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08672/extracted/6354665/Figures/logo.png",
                "caption": "",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08672/x1.png",
                "caption": "Figure 1:Typical reinforce-like approaches to boost LLM reasoning. (a) and (b) abstract two types of reinforce-like methods, which require final answer and verification respectively. (c) depicts the ultimate goal of our research.xùë•xitalic_xdenotes the input query,aùëéaitalic_adenotes the LLM-generated responses that contain multiple steps, andyùë¶yitalic_yis the final answer (if exists).",
                "position": 171
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08672/x2.png",
                "caption": "Figure 2:The overall framework ofGenius. It only receives the unsupervised NL queries as inputs. To complete goal of self-improving, the policy LLM goes throughKùêæKitalic_Ksteps of sampling and rewarding for each query (Step 1-4), collects the high-quality response sequence as the training data (Step 5), and trains itself with the advantage calibrated optimization loss (Step 6).",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2504.08672/x3.png",
                "caption": "Figure 3:Visualization of the calibration function. The x-axis denotes the the differences betweenAlsubscriptùê¥ùëôA_{l}italic_A start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPTandAwsubscriptùê¥ùë§A_{w}italic_A start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, while the y-axis is the value of the calibration term. By adjustingŒ±ùõº\\alphaitalic_Œ±, we can control the decay rate of the curve.",
                "position": 427
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08672/x4.png",
                "caption": "(a)Base LLM: Qwen2.5-3B-Instruct",
                "position": 836
            },
            {
                "img": "https://arxiv.org/html/2504.08672/x4.png",
                "caption": "(a)Base LLM: Qwen2.5-3B-Instruct",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2504.08672/x5.png",
                "caption": "(b)Base LLM: Qwen2.5-7B-Instruct",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2504.08672/x6.png",
                "caption": "Figure 5:Results on AIME 2024.",
                "position": 921
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08672/x7.png",
                "caption": "Figure 6:Post-training scaling law with LLaMA3.1-8B-Instruct as the base LLM.",
                "position": 1145
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGradient Analysis of ACO",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CAnalysis of The Training Corpus",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08672/x8.png",
                "caption": "Figure 7:Visualization of data distribution. The training corpus is marked in red color.",
                "position": 2351
            }
        ]
    },
    {
        "header": "Appendix DMore Experiments on Coding Tasks",
        "images": []
    }
]