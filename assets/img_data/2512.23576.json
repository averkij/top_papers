[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23576/x1.png",
                "caption": "Figure 1:Overview of the LiveTalk system.Given a user audio/text query, Qwen3-Omnixu2025qwen3processes the query and generates streaming audio responses in real-time. Our few-step multimodal diffusion model takes the streaming audio along with the reference image avatar and text conditions to generate synchronized video responses through block-wise AR generation.\nEach block (3 latent frames) undergoes 4-step diffusion, achieving 20Ã— acceleration over the baseline (See Tab.1, Ours vs. OmniAvatar-1.3Bgan2025omniavatar). To support long-horizon streaming with sub-second latency, we perform clean KV prefilling across blocks using asink+rollingtoken cache: persistent sink tokens retain global context, rolling tokens carry recent history.",
                "position": 87
            },
            {
                "img": "https://arxiv.org/html/2512.23576/figures/huggingface-icon.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Improved Distillation for Real-Time Multimodal Interactive Video Diffusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23576/x2.png",
                "caption": "Figure 2:Degraded training performance with Self Forcing DMD.Left and middle columns show failure cases from Self Forcing DMD training, exhibiting quality degradation. Right column shows stable results from our method.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Building Real-Time Multimodal Interactive Systems",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.23576/x3.png",
                "caption": "Figure 3:Examples of multimodal-conditioned avatar video generation by our model.Our model generates temporally coherent video with natural facial expressions, accurate lip-sync to the audio conditions, and consistent visual identity across frames.",
                "position": 653
            },
            {
                "img": "https://arxiv.org/html/2512.23576/x4.png",
                "caption": "Figure 4:Ablation study visualization. Generated video shows progressive improvements for each ablated component: (1) curated multimodal conditions, (2) ODE initialization for full convergence (20k steps), and (3) aggressive hyperparameter settings (doubled learning rates, CFG=6).",
                "position": 829
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AMulti-Round Evaluation Details",
        "images": []
    },
    {
        "header": "Appendix BTraining Configuration Details",
        "images": []
    }
]