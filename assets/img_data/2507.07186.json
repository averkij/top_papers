[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07186/x1.png",
                "caption": "Figure 1:Potential causal relationships between pretraining, instruction finetuning, and bias emergence in LMs.Blue representspossible causes(finetuning, pretraining, and training randomness). Purple represents theeffect(observed bias).\nThe Grey element denotes a possbilelatent cause(latent bias). Dashed arrows (→→\\rightarrow→) indicate possible causal effects and non-dashed indicate known casual effect.\nOur two-step causal analysis examines the effect of training randomness inStep 1and the influence of instruction data versus pretraining inStep 2. Our results indicate thatpretraining is the leading cause of biases in LMs, shaping the latent bias that later affects observed biases.\nTraining randomness and instruction data modify observed biases based on the latent bias already embedded in the pretrained model.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x2.png",
                "caption": "(a)The cross-tuning setup. Two pretrained models (rows) are finetuned on two instruction datasets (columns), resulting in four types of finetuned models.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x2.png",
                "caption": "(a)The cross-tuning setup. Two pretrained models (rows) are finetuned on two instruction datasets (columns), resulting in four types of finetuned models.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x3.png",
                "caption": "(b)Potential bias causes and their respective hypotheses about bias score trends.\n(1) If biases stem from instruction-tuning, similar trends should arise from the same instruction data;\n(2) If there is a complex interplay between bias sources, we expect mixed results;\n(3) If biases originate from pretraining, bias trends should remain consistent across different datasets.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3What Causes Bias in LLMs?",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07186/x4.png",
                "caption": "Figure 3:Variability of biases and MMLU scores across random seeds: Mean of standard deviations computed across three random seeds, averaged over all32323232biases and17171717MMLU subcategories scores for each model. Bias scores consistently show slightly more variation across seeds than MMLU scores, suggesting a modest sensitivity to training randomness.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x5.png",
                "caption": "(a)",
                "position": 525
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x5.png",
                "caption": "(a)",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x6.png",
                "caption": "(b)",
                "position": 534
            }
        ]
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07186/x7.png",
                "caption": "(a)",
                "position": 721
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x7.png",
                "caption": "(a)",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x8.png",
                "caption": "(b)",
                "position": 730
            }
        ]
    },
    {
        "header": "6Discussion and Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABias Data and Bias Scores",
        "images": []
    },
    {
        "header": "Appendix BVerifying the Performance of Finetuned Models",
        "images": []
    },
    {
        "header": "Appendix CFinetuning Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07186/x9.png",
                "caption": "(a)",
                "position": 3286
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x9.png",
                "caption": "(a)",
                "position": 3287
            },
            {
                "img": "https://arxiv.org/html/2507.07186/x10.png",
                "caption": "Figure 7:Mean bias score per cluster grouped by pretraining model.",
                "position": 3292
            }
        ]
    },
    {
        "header": "Appendix DAnalysis Details",
        "images": []
    }
]