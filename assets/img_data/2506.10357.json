[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/extracted/6535108/figures/logo/optimus3.png",
                "caption": "",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x1.png",
                "caption": "Figure 1:Demonstration of Optimus-3’s capabilities as a generalist agent in Minecraft. It can perform long-horizon task planning, captioning, embodied QA, grounding, low-level action generation, and reflection in an interactive manner. All of these capabilities are seamlessly integrated into a unified end-to-end architecture, enabling robust and coherent performance across diverse task scenarios.",
                "position": 153
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/x2.png",
                "caption": "Figure 2:A: The architecture of Optimus-3, which includes a task router that selects a specific task expert for each query, a ViT[11]for visual encoding, and a MoE LLM for generating responses and low-level actions. Given a long-horizon task, it can generate a feasible plan and then execute the sub-goals sequentially.B: The proposed Multimodal Reasoning-Augmented Reinforcement Learning effectively enhances the agent’s performance.C: Performance comparison of Optimus-3 against current task-specific SOTA agents, GPT-4o[1], and the original backbone Qwen2.5-VL[2].",
                "position": 177
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/x3.png",
                "caption": "Figure 3:Different agent framework in Minecraft.(a)Goal-conditioned policy which takes observation and instruction as input.(b)Function calling, which employs (M)LLM as the planner, then generates executable functions.(c)(M)LLM as the planner, which then employs a goal-conditioned policy to generate low-level actions.(d)MLLM generates latent tokens, which are used as control conditions for the policy.(e)End-to-end architecture (ours) which is capable of generating both low-level actions and textual responses.",
                "position": 210
            }
        ]
    },
    {
        "header": "3Optimus-3",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/x4.png",
                "caption": "Figure 4:Data Generation Pipeline. Given a task pool, we utilize a knowledge graph[26]to generate task plans, forming the planning dataset. These plans are then used as instructions for STEVE-1[28], which interacts with the environment to produce the action dataset. During this process, we randomly sample images and employ expert models[36,33]with environmental feedback to generate the captioning, embodied QA, and grounding datasets.",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x5.png",
                "caption": "Table 1:Main Result of Optimus-3 onLong-Horizontasks. We reportSRon each task group, the results of each task can be found in theAppendix. H. Planner†denotes MLLM generates the task plan, followed by STEVE-1[28]generates actions, as adopted in previous work[27].",
                "position": 334
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x6.png",
                "caption": "",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x7.png",
                "caption": "",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x8.png",
                "caption": "",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x9.png",
                "caption": "",
                "position": 349
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x10.png",
                "caption": "",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x11.png",
                "caption": "",
                "position": 353
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/x12.png",
                "caption": "Figure 5:Ablation Study on Training Data.originalrefers to the original Qwen2.5-VL,tuned_w/o_kindicates the model fine-tuned on data without knowledge,tuned_w/_krepresents the model tuned on data generated by knowledge-enhanced pipeline.",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x13.png",
                "caption": "Figure 6:Visual comparison of Optimus-3 (ours), Qwen2.5-VL (tuned on our data), and GPT-4o. Red highlights indicate errors, while blue highlights denote correct outputs.",
                "position": 718
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x14.png",
                "caption": "Figure 7:Performance comparison between token-level routing and task-level routing.",
                "position": 721
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix BBroader Impact",
        "images": []
    },
    {
        "header": "Appendix CMinecraft",
        "images": []
    },
    {
        "header": "Appendix DDataset Generation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/x15.png",
                "caption": "Figure 8:Planning and Action samples generated from our pipeline.",
                "position": 1598
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x16.png",
                "caption": "Figure 9:Captioning and Embodied QA samples generated from our pipeline.",
                "position": 1601
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x17.png",
                "caption": "Figure 10:Grounding and Reflection samples generated from our pipeline.",
                "position": 1604
            }
        ]
    },
    {
        "header": "Appendix EMoE Architecture with Task-level Routing",
        "images": []
    },
    {
        "header": "Appendix FMultimodal Reasoning-Augmented Reinforcement Learning",
        "images": []
    },
    {
        "header": "Appendix GTraining Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.10357/x18.png",
                "caption": "Table 7:Samples from the Captioning and Embodied QA Evaluation Set.",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x19.png",
                "caption": "",
                "position": 2256
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x20.png",
                "caption": "",
                "position": 2269
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x21.png",
                "caption": "",
                "position": 2282
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x22.png",
                "caption": "",
                "position": 2299
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x23.png",
                "caption": "",
                "position": 2312
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x24.png",
                "caption": "",
                "position": 2325
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x25.png",
                "caption": "",
                "position": 2338
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x26.png",
                "caption": "Table 8:Samples from the Grounding and Reflection Evaluation Set.",
                "position": 2350
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x27.png",
                "caption": "",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x28.png",
                "caption": "",
                "position": 2422
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x29.png",
                "caption": "",
                "position": 2440
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x30.png",
                "caption": "",
                "position": 2462
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x31.png",
                "caption": "",
                "position": 2480
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x32.png",
                "caption": "",
                "position": 2498
            },
            {
                "img": "https://arxiv.org/html/2506.10357/x33.png",
                "caption": "",
                "position": 2516
            }
        ]
    },
    {
        "header": "Appendix HQuantitative Analysis",
        "images": []
    }
]