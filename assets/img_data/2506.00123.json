[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00123/x1.png",
                "caption": "Figure 1:Overview of VeBrain and VeBrain-600k.Compared to existing MLLMs, VeBrain achieves the best trade-off performance on benchmarks of multimodal understanding, visual-spatial reasoning, and robot control into one MLLM. To support the unified training of VeBrain, VeBrain-600kis built with a semi-automated data engine covering a variety of data sources and tasks.",
                "position": 132
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00123/x2.png",
                "caption": "Figure 2:Illustration of VeBrain architecture and robotic adapter.In VeBrain, the MLLM is capable of perception, thinking, and decision-making in common MLLM tasks. For robot control, an additional adapter is combined with the MLLM to achieve closed-loop control of the real robot.",
                "position": 207
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.00123/x3.png",
                "caption": "Figure 3:Visualization of VeBrain on robotic arm and legged robot.VeBrain demonstrates compositional capabilities in handling complex robotic tasks. Due to space limitations, most of the text regarding the thinking process is omitted in the figure.",
                "position": 1300
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BMore Results",
        "images": []
    }
]