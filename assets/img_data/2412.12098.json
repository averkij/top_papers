[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12098/x1.png",
                "caption": "(a)Normalized average performance ofMaxInfoRLacross several deep RL benchmarks on state-based tasks.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x1.png",
                "caption": "(a)Normalized average performance ofMaxInfoRLacross several deep RL benchmarks on state-based tasks.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x2.png",
                "caption": "(b)Normalized average performance ofMaxInfoDrQv2on the humanoid visual control tasks (stand, walk, and run).",
                "position": 140
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3MaxInfoRL",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12098/x3.png",
                "caption": "Figure 2:Learning curves of all methods on several environments from the OpenAI gym and DMC suite benchmarks.",
                "position": 664
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x4.png",
                "caption": "Figure 3:Performance ofMaxInfoSACand SAC on the HumanoidBench benchmark.",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x5.png",
                "caption": "Figure 4:Phase plots during learning ofMaxInfoRLand SAC on the Pendulum environment.MaxInfoSACcovers the state space much faster and effectively solves the swing-up task within10101010K environment interactions.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x6.png",
                "caption": "Figure 5:Learning curves for state-based tasks for different values of the action cost parameterKùêæKitalic_K.",
                "position": 680
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x7.png",
                "caption": "Figure 6:Learning curves from visual control tasks of the DMC suite.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x8.png",
                "caption": "Figure 7:Learning curves from the visual control humanoid tasks of the DMC suite.",
                "position": 700
            }
        ]
    },
    {
        "header": "5Related Works",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAnalyzingœµitalic-œµ\\epsilonitalic_œµ‚ÄìGreedy for Multi-armed Bandits",
        "images": []
    },
    {
        "header": "Appendix BProof ofTheorem3.1",
        "images": []
    },
    {
        "header": "Appendix CMaxInfoRLfrom the perspective of KL-minimization",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.12098/x9.png",
                "caption": "Figure 8:Evolution of the intrinsic reward coefficientŒªùúÜ\\lambdaitalic_ŒªofSACEipoandMaxInfoSACover environment interaction.",
                "position": 2731
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x10.png",
                "caption": "Figure 9:We combineMaxInfoRLwith REDQ (MaxInfoREDQ) and report the learning curves of SAC, REDQ,MaxInfoSAC, andMaxInfoREDQ.",
                "position": 2737
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x11.png",
                "caption": "Figure 10:Learning curves ofMaxInfoDrQv2with different noise levelsœÉ‚àà{0.0,0.2}ùúé0.00.2\\sigma\\in\\{0.0,0.2\\}italic_œÉ ‚àà { 0.0 , 0.2 }compared to DrQv2 andMaxInfoDrQ.",
                "position": 2744
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x12.png",
                "caption": "Figure 11:MaxInfoDrQv2evaluated on the humanoid walk task with no action noise.",
                "position": 2750
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x13.png",
                "caption": "Figure 12:Learning curves ofMaxInfoSACwith RND as the intrinsic reward, instead of the information gain, compared to SAC and standardMaxInfoSAC.",
                "position": 2759
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x14.png",
                "caption": "Figure 13:Learning curves ofœµitalic-œµ\\epsilonitalic_œµ‚ÄìMaxInfoRLwith disagreement as the intrinsic reward, SAC andMaxInfoSAC.",
                "position": 2767
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x15.png",
                "caption": "Figure 14:Learning curves ofœµitalic-œµ\\epsilonitalic_œµ‚ÄìMaxInfoRLwith disagreement as the intrinsic reward, SAC andMaxInfoSACfor varying levels of action costsKùêæKitalic_K.",
                "position": 2770
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x16.png",
                "caption": "Figure 15:Learning curves ofœµitalic-œµ\\epsilonitalic_œµ‚ÄìMaxInfoRLwith disagreement and curiosity as the intrinsic reward compared with SAC.",
                "position": 2777
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x17.png",
                "caption": "Figure 16:Learning curves ofœµitalic-œµ\\epsilonitalic_œµ‚ÄìMaxInfoRLwith disagreement. We compare a version ofœµitalic-œµ\\epsilonitalic_œµ‚ÄìMaxInfoRLwhich switches between extrinsic and intrinsic policy every step with one which switches every 32 steps.",
                "position": 2788
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x18.png",
                "caption": "Figure 17:Learning curves of OAC compared withMaxInfoSACand aMaxInfoRLversion of OAC (MaxInfoOAC).",
                "position": 2795
            },
            {
                "img": "https://arxiv.org/html/2412.12098/x19.png",
                "caption": "Figure 18:Learning curves of DrM compared with our version of DrM (MaxInfoDrM).",
                "position": 2803
            }
        ]
    },
    {
        "header": "Appendix EExperiment Details",
        "images": []
    }
]