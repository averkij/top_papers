[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09995/x1.png",
                "caption": "Figure 1:Simulated videos of our PlayerOne. Given an egocentric image as the scene to be explored, we can simulate egocentric immersive videos that are accurately aligned with the user’s motion sequence captured by an exocentric camera. All the users have been anonymized and action videos are shot with the front camera.",
                "position": 68
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09995/x2.png",
                "caption": "Figure 2:Overall framework of our PlayerOne. It begins by converting the egocentric first frame into visual tokens. The human motion sequence is split into groups and fed into the motion encoders respectively to generate part-wise motion latents, with the head parameters converted into a rotation-only camera sequence. This camera sequence is then encoded via a camera encoder, and its output is injected into noised video latents to improve view-change alignment. Next, we render a 4D scene point map sequence with the ground truth video, which is then processed by a point map encoder with an adapter to produce scene latents. Then we input the concatenation of these latents into the DiT Model and perform noising and denoising on both the video and scene latents to ensure world-consistent generation. Finally, the denoised latents are decoded by VAE decoders to produce the final results. Note that only the first frame and the human motion sequence are needed for inference.",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2506.09995/x3.png",
                "caption": "Figure 3:The overall pipeline of the dataset construction. By seamlessly integrating detection and human pose estimation models, we can extract motion-video pairs from existing egocentric-exocentric video datasets while retaining high-quality data through our automatic filtering scheme.",
                "position": 202
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09995/x4.png",
                "caption": "Figure 4:Investigation on coarse-to-fine training. “Joint-Train” and “No Pretrain” denote training with both motion-video pairs and large-scale egocentric videos in a one-stage manner and training with only motion-video pairs respectively. The Wanx2.1 1.3B is adopted as the baseline.",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2506.09995/x5.png",
                "caption": "Figure 5:Investigation on part-disentangled motion injection. “ControlNet” denotes injecting motion latents with a ControlNetzhang2023adding. “Entangled” and “No Cam” denote inputting the whole motion sequence into a motion encoder without dividing into groups and removing the camera encoder respectively.",
                "position": 296
            },
            {
                "img": "https://arxiv.org/html/2506.09995/x6.png",
                "caption": "Figure 6:Investigation on scene-frame reconstruction. “No Recon”/“No Adapter” denote training without reconstruction/the adapter. “DUStR” is replacing CUT3R with DUStR for point map rendering.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2506.09995/x7.png",
                "caption": "Figure 7:Qualitative evaluation on the motion alignment. We generate simulated videos based on the same first frame but different motion sequences. Results show that we can achieve accurate motion alignment.",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2506.09995/x8.png",
                "caption": "Figure 8:Qualitative comparisons between our method and other competitors. Our PlayerOne can achieve the best performance on both the motion alignment, video quality.",
                "position": 444
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]