[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17269/logos/tum_logo.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x1.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2510.17269/logos/core.png",
                "caption": "",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2510.17269/logos/st_logo.png",
                "caption": "",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x2.png",
                "caption": "",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x3.png",
                "caption": "",
                "position": 116
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x4.png",
                "caption": "",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x5.png",
                "caption": "",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x6.png",
                "caption": "",
                "position": 121
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x7.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x8.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x9.png",
                "caption": "Figure 1:Pipeline overview.Left to right: (1) ingestion of raw sources; (2) canonicalization and image & text cleaning; (3)\nde-duplication and test-set decontamination using SSCD embeddings(Pizzi et al.,2022); (4) per-turn quality assessment with LLM/VLM-as-a-judge(Zheng et al.,2023; Wang et al.,2023c). Each stage includes human checkpoints (mapping review, script sign-off, and post-conversion audits) to ensure faithful annotation\nconsumption, consistent quality, and safety.",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2510.17269/logos/db.png",
                "caption": "",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x10.png",
                "caption": "",
                "position": 155
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2FineVision Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17269/x11.png",
                "caption": "Figure 2:Decontamination report.Per-benchmark contamination heatmap for FineVision and comparable open-source alternatives (rows: benchmarks, columns: datasets subsets). FineVision’s contamination is sparse and concentrated in a few subsets and benchmarks, and consistently lower than the baselines.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x12.png",
                "caption": "",
                "position": 359
            }
        ]
    },
    {
        "header": "3Exploring FineVision",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17269/x13.png",
                "caption": "Figure 3:Category distribution.Share of samples across the nine categories.\nFineVision provides a good baseline mixture, which could be further tuned via up- and downsampling and in correlation with the quality ratings.",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x14.png",
                "caption": "(a)Formatting",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x14.png",
                "caption": "(a)Formatting",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x15.png",
                "caption": "(b)Relevance",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x16.png",
                "caption": "(c)Visual Dependency",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x17.png",
                "caption": "(d)Image Correspondence",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x18.png",
                "caption": "Figure 5:Dataset Characterization Along Four AxesWe apply per-dataset PCA over the different characteristic scores. From the analysis, it appearsFormattingandRelevanceare highly correlated, whileVisual DependencyandImage–Question Correspondenceare strongly inversely correlated.Groundingattains the highestFormatting/Relevance, whileChart & Tableattains highImage–Question Correspondence.CaptioningandGeneral VQAshow highVisual Dependencycombined with strongFormatting/Relevance. In contrast,Naive OCRexhibits highVisual Dependencybut lower scores onFormatting/Relevance. Arrows indicate variable loadings; points are dataset centroids with covariance ellipses per category.",
                "position": 454
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x19.png",
                "caption": "Figure 6:Visual diversity analysis.While FineVision and Cambrian show similarly high conceptual breadth (effective rank), FineVision exhibits superior conceptual balance (participation ratio).\nHigher values are better for both axes; both axes are linear and dimensionless.\nMarker size corresponds to the number of images in each dataset. For intuition, a dataset with high effective rank but low participation ratio might cover many animal species but be numerically dominated by a few (e.g., cats/dogs).",
                "position": 472
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17269/x20.png",
                "caption": "Figure 7:Training dynamics on original and decontaminated datasets.Left: mean normalized performance (%) across 11 evaluation benchmarks (higher is better), with the training step shown in thousands (×103\\times 10^{3}). Each benchmark score is min–max normalized to [0,100] and averaged per evaluation step; the model trained on FineVision (blue) leads throughout the second half of training and attains the best final score.Right: comparison of the final performance between the original data and after decontamination. FineVision exhibits the smallest drop at the end of training with 1.6 pp, whereas baselines degrade by roughly 2.7–3.7 pp, indicating that FineVision’s gains are not explained by contamination.",
                "position": 601
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.17269/x21.png",
                "caption": "Figure 8:Duplicate detection visualization withτ=0.95\\tau=0.95.Each panel shows the query image and retrieved matches with similarity scores. These three different scenarios, show the difficulty in picking a single threshold: (A, left) true photographic duplicates under mild crops/brightness; (B, middle) false positives (e.g., templated charts with different numbers) just aboveτ\\tau; (C, right) false negatives (hand drawings) just belowτ\\tau. After qualitative experiments we settled onτ=0.95\\tau=0.95since it provided a good trade off between Precision and Recall.",
                "position": 3607
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x21.png",
                "caption": "",
                "position": 3611
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x22.png",
                "caption": "",
                "position": 3617
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x23.png",
                "caption": "",
                "position": 3621
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x24.png",
                "caption": "(a)Formatting",
                "position": 3935
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x24.png",
                "caption": "(a)Formatting",
                "position": 3938
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x25.png",
                "caption": "(b)Image Correspondence",
                "position": 3944
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x26.png",
                "caption": "(c)Relevance",
                "position": 3950
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x27.png",
                "caption": "(d)Visual Dependency",
                "position": 3956
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x28.png",
                "caption": "Figure 10:Model performance under combined prompt-based quality filtering.We we combine all filters into a single criterion, meaning we only select datapoints that have all four ratings above a certain threshold, training on the full dataset also results in the best performance.",
                "position": 3963
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x29.png",
                "caption": "Figure 11:Token length and image resolution by category.Left:split-violin token-length distributions by category; questions (blue) vs. answers (orange), median is dotted line, y-axis capped at 1000 for visibility. These shapes expose task archetypes and the information gap between prompt and response.Right:image resolution distributions by category; width (blue) and height (orange), median is dotted line.",
                "position": 4027
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x30.png",
                "caption": "",
                "position": 4030
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x31.png",
                "caption": "Figure 12:Turns per sample by category.Categories such asChart & TableandGrounding & Countingsupport more multi-turn interactions per image.",
                "position": 4034
            },
            {
                "img": "https://arxiv.org/html/2510.17269/x32.png",
                "caption": "Figure 13:Total tokens by category stacked by question and answer.Left:total tokens,Right:mean tokens per turn",
                "position": 4037
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]