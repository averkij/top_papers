[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06071/x1.png",
                "caption": "Figure 1:The architecture of our proposed KaSA encompasses two stages: (Left) knowledge-based SVD truncation toremove the noisy knowledge from the base model;(Right) knowledge-aware singular-value adaptation to adjust singular values that dynamically activate knowledge acrossŒî‚Å¢ùêñŒîùêñ\\Delta\\mathbf{W}roman_Œî bold_Wmodel parameters based on its relevance to downstream tasks.",
                "position": 229
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06071/x2.png",
                "caption": "Figure 2:Components ablation study about knowledge-based SVD truncation, knowledge-aware singular value adaptation, singular value regularization‚Ñí2subscript‚Ñí2\\mathcal{L}_{2}caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and orthogonal regularization‚Ñí3subscript‚Ñí3\\mathcal{L}_{3}caligraphic_L start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTon MRPC, CoLA, and RTE datasets.",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x3.png",
                "caption": "Figure 3:Budget parameter scalability of fine-tuning RoBERTa-base with LoRA, PiSSA, MiLoRA, and KaSA on MRPC, CoLA, and RTE datasets.",
                "position": 1519
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x4.png",
                "caption": "Figure 4:The final distribution of knowledge-aware singular values forùêñqsubscriptùêñùëû\\mathbf{W}_{q}bold_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPTandùêñvsubscriptùêñùë£\\mathbf{W}_{v}bold_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPTupon fine-tuning the RoBERTa-base model on the MNLI and QQP benchmarks.\nIn this context, thexùë•xitalic_x-axis corresponds to the layer index, and theyùë¶yitalic_y-axis denotes the position index. Each value signifies the relevance of the associated knowledge.",
                "position": 1549
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APseudocode for KaSA",
        "images": []
    },
    {
        "header": "Appendix BBaselines",
        "images": []
    },
    {
        "header": "Appendix CDetails of Benchmark Datasets",
        "images": []
    },
    {
        "header": "Appendix DPrompt Templates",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06071/x5.png",
                "caption": "Figure 5:Prompt template of data synthesis for summarization tasks by GPT4o.",
                "position": 3207
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x6.png",
                "caption": "Figure 6:Prompt template of data synthesis for classification, coding, and closed QA tasks by GPT4o.",
                "position": 3210
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x7.png",
                "caption": "Figure 7:Prompt template to evaluate the fine-tuned model‚Äôs response by GPT4o.",
                "position": 3213
            }
        ]
    },
    {
        "header": "Appendix ETraining Details",
        "images": []
    },
    {
        "header": "Appendix FAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06071/x8.png",
                "caption": "Figure 8:Components ablation study about knowledge-based SVD truncation, knowledge-aware singular value adaptation, singular value regularization‚Ñí2subscript‚Ñí2\\mathcal{L}_{2}caligraphic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and orthogonal regularization‚Ñí3subscript‚Ñí3\\mathcal{L}_{3}caligraphic_L start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPTon SST-2, QNLI, and STS-B datasets.",
                "position": 3864
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x9.png",
                "caption": "Figure 9:The impact of varying the rank of SVD truncation on the model‚Äôs performance across three datasets.",
                "position": 3882
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x10.png",
                "caption": "Figure 10:The impact of parameter initialization on the task-specific knowledge update, denoted asŒî‚Å¢ùêñ=Œî‚Å¢(ùêîùêíùêï‚ä§)ŒîùêñŒîsuperscriptùêîùêíùêïtop\\Delta\\mathbf{W}=\\Delta(\\mathbf{U}\\mathbf{S}\\mathbf{V^{\\top}})roman_Œî bold_W = roman_Œî ( bold_USV start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT )across three datasets.",
                "position": 4179
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x11.png",
                "caption": "Figure 11:The singular-value and orthogonal regularization curve at the last layer of RoBERTabasebase{}_{\\text{base}}start_FLOATSUBSCRIPT base end_FLOATSUBSCRIPT(Upper) and overall training loss curve (Lower) on CoLA dataset.",
                "position": 4212
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x12.png",
                "caption": "",
                "position": 4216
            }
        ]
    },
    {
        "header": "Appendix GInitialization and Singular-value Adaptation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.06071/x13.png",
                "caption": "Figure 12:Responses on a math problem from MT-Bench. Each response is generated by Gemma 7B models fine-tuned on 51K Alpaca dataset with KaSA, LoRA, and PiSSA methods respectively.",
                "position": 4823
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x14.png",
                "caption": "Figure 13:Responses on a STEM problem from MT-Bench. Each response is generated by LLaMA3 8B models fine-tuned on 51K Alpaca dataset with KaSA, LoRA, and PiSSA methods respectively.",
                "position": 4826
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x15.png",
                "caption": "Figure 14:Responses on a reasoning problem from MT-Bench. Each response is generated by Mistral 7B models fine-tuned on 51K Alpaca dataset with KaSA and MiLoRA methods respectively.",
                "position": 4829
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x16.png",
                "caption": "Figure 15:Responses on a classification problem from the test split of ‚ÄúNo Robots‚Äù dataset. Each response is generated by LLaMA3 8B models fine-tuned on 128K synthetic dataset with KaSA, PiSSA, and LoRA methods respectively.",
                "position": 4832
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x17.png",
                "caption": "Figure 16:Responses on a summarization problem from the test split of ‚ÄúNo Robots‚Äù dataset. Each response is generated by Gemma 7B models fine-tuned on 128K synthetic dataset with KaSA, LoRA, and PiSSA methods respectively.",
                "position": 4836
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x18.png",
                "caption": "Figure 17:Responses on a ClosedQA problem from the test split of ‚ÄúNo Robots‚Äù dataset. Each response is generated by Mistral 7B models fine-tuned on 128K synthetic dataset with KaSA, MiLoRA, and PiSSA methods respectively.",
                "position": 4840
            },
            {
                "img": "https://arxiv.org/html/2412.06071/x19.png",
                "caption": "Figure 18:Responses on a Coding problem from the test split of ‚ÄúNo Robots‚Äù dataset. Each response is generated by Gemma 7B models fine-tuned on 128K synthetic dataset with KaSA and PiSSA methods respectively.",
                "position": 4844
            }
        ]
    },
    {
        "header": "Appendix HCase Study",
        "images": []
    }
]