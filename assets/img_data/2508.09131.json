[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09131/x1.png",
                "caption": "Figure 1.Text-conditioned color editing.Our method, ColorCtrl with FLUX.1-dev, edits colors across multiple materials while preserving light-matter interactions. For example, in the fourth case, the ball’s color, its water reflection, specular highlights, and even small droplets on the glass have all been changed. It also enables fine-grained control over the intensity of specific descriptive terms.",
                "position": 148
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09131/x2.png",
                "caption": "Figure 2.Pipeline of ColorCtrl.(a) Visualizes the attention mechanism in MM-DiT blocks. (b) Enables effective color editing while maintaining structural consistency. (c) Preserves colors in non-editing regions. (d) Applies attribute re-weighting to specific word tokens. Symbols in thesource branchhave no superscript. Symbols with a superscript∗indicate thetarget branch, and hats (e.g.,V^\\hat{V},M^\\hat{M}) denote edited outputs.",
                "position": 313
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x3.png",
                "caption": "Figure 3.Top row: SD3 results; bottom row: FLUX.1-dev results.(a)The edit prompt is “white fox”→\\to“orange fox”. Left to right: source image, our full method, without color preservation, with swapped text-to-text part in structure preservation, and with swappedVtextV^{\\text{text}}in color preservation.(b)The generation prompt is “a white fox in a forest”, and the token for mask extraction is “fox”. From left to right: the mask extracted from vision-to-text parts, and from text-to-vision parts.",
                "position": 519
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09131/x4.png",
                "caption": "Figure 4.Qualitative image results compared with training-free methods and commercial models on PIE-Bench.The top three rows are generated using FLUX.1-dev, while the bottom two are generated using SD3.Best viewed with zoom-in.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x5.png",
                "caption": "Figure 5.Qualitative video results compared with training-free methods on PIE-Bench.Prompt is “green tea”→\\to“red tea”. Each shows three frames.",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x6.png",
                "caption": "Figure 6.Examples of attribute re-weighting.The top two rows are generated using FLUX.1-dev, while the bottom one are generated using SD3.",
                "position": 874
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x7.png",
                "caption": "Figure 7.Examples of results generated with Step1X-Edit (left) and FLUX.1 Kontext dev (right).Green arrows: first edit using the editing model.Blue arrows: second edit using the editing model.Orange arrows: second edit using the editing model with ColorCtrl.\nTop left: a red diamond is added to the neck, then changed to blue. Bottom left: an orange cap is added, then changed to purple. Top right: blue butterflies are added, then changed to yellow. Bottom right: a flashlight with blue light is added, then changed to green.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x8.png",
                "caption": "Figure 8.Examples of real image editing.Left: results generated with SD3. Right: results generated with FLUX.1-dev.",
                "position": 1018
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.09131/x9.png",
                "caption": "Figure 9.Examples of FireFlow and RF-Solver with difference end timesteps on SD3.The selected end timestep refers to the setting used in the benchmark evaluation, while the higher end timestep denotes a larger value chosen for comparison purposes.",
                "position": 1852
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x10.png",
                "caption": "Figure 10.Examples of image editing results.",
                "position": 2027
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x11.png",
                "caption": "Figure 11.Additional qualitative video results compared with training-free models on ColorCtrl-Bench.Left: “gray sofa”→\\to“brown sofa”. Right: “brown suitcase”→\\to“black suitcase”. Each shows three frames.",
                "position": 2046
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x12.png",
                "caption": "Figure 12.Examples of failure cases.Left: results generated with SD3. Right: results generated with FLUX.1-dev.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x13.png",
                "caption": "Figure 13.Qualitative image results compared with training-free methods and commercial models on ColorCtrl-Bench.The left four columns are generated using FLUX.1-dev, while the right three are generated using SD3.Best viewed with zoom-in.",
                "position": 2062
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x14.png",
                "caption": "Figure 14.Qualitative video results compared with training-free models on ColorCtrl-Bench.Left: “yellow bus”→\\to“green yellow”. Right: “green backpack”→\\to“yellow backpack”. Each shows three frames.",
                "position": 2065
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x15.png",
                "caption": "Figure 15.Additional qualitative image results compared with training-free methods and commercial models on PIE-Bench.The left three columns are generated using FLUX.1-dev, while the right two are generated using SD3.Best viewed with zoom-in.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x16.png",
                "caption": "Figure 16.Additional qualitative video results compared with training-free models on PIE-Bench.Left: “white shirt”→\\to“blue shirt”. Right: “white lamb”→\\to“blue lamb”. Each shows three frames.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2508.09131/x17.png",
                "caption": "Figure 17.The prompt used to generate ColorCtrl-Bench with GPT.",
                "position": 2074
            }
        ]
    },
    {
        "header": "Appendix BMore Results and Analysis",
        "images": []
    }
]