[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18282/figures/peek_emoji.png",
                "caption": "",
                "position": 54
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18282/x1.png",
                "caption": "Figure 1:PEEK enables policy generalization by modulating minimal representations ofwhereto focus andwhatto do for robust policy learning.",
                "position": 83
            }
        ]
    },
    {
        "header": "IIRelated Works",
        "images": []
    },
    {
        "header": "IIIPEEK: Guiding and Minimal Image Representations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18282/x2.png",
                "caption": "Figure 2:Policy Training and Inference Pipeline.The VLM is called everyHHsteps to generate a path and task-relevant points. An (arbitrary) RGB-input policy is conditioned on the path and masked image to either predict actions forinferenceor fortraining. The same path and mask is applied onto incoming observations forHHsteps, after which the VLM is re-queried.",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2509.18282/x3.png",
                "caption": "Figure 3:Franka Sim-to-Real Tasks.Zero-shot evaluation environments along with associated path-drawn and masked images produced by PEEK.Simulationdenotes the generated simulation data that the policies were trained on.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2509.18282/x4.png",
                "caption": "Figure 4:WidowX Tasks.Evaluation environments along with associated path-drawn and masked images produced by PEEK.",
                "position": 246
            }
        ]
    },
    {
        "header": "IVExperimental Setup",
        "images": []
    },
    {
        "header": "VExperimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18282/x5.png",
                "caption": "Figure 5:Real-World Zero-Shot Generalization Results.Task completion rates (including partial credit for grasping or reaching objects correctly) and task success rates across 3 task variants:Basic,Clutter, andSemanticin our Franka Sim-to-Real experiments (top) and WidowX BRIDGE experiments (bottom). Results are averaged over all trials and tasks within each variant.PEEKresults are bolded for visibility. Full tables in AppendixSectionA-E.",
                "position": 342
            }
        ]
    },
    {
        "header": "VIConclusion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18282/x6.png",
                "caption": "Figure 6:Data Labeling Pipeline.A detailed overview of the data labeling pipeline as described inSectionA-A: We (1) use CoTracker3[40]to detect moving points across each trajectory, points are discarded if they do not move significantly, and the rest become task-relevant pointsPt​a​s​kP^{task}; (2) mask areas withoutPt​a​s​kP^{task}to black and apply a pre-trained gripper detector to construct 2D gripper path pointsPg​r​i​pP^{grip}; (3) segment each trajectory into subtrajectories; and (4) construct gripper pathsppand task-relevant masking pointsmmfor each subtrajectory. Notice in (4) that target object placement areas become visible beforehand through including points from the last timestepPTt​a​s​kP^{task}_{T}.",
                "position": 922
            }
        ]
    },
    {
        "header": "Appendix A",
        "images": []
    }
]