[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19894/x1.png",
                "caption": "Figure 1:Overview of PromptCoT 2.0.\nWe begin with open-source problems and annotate their associated concepts and rationales, forming concept–rationale–problem triples that provide cold-start data for the rationale generation model and the prompt generation model.\nDuring EM optimization, theE-stepupdates the rationale generation model by maximizing the reward defined in Eq.5, while theM-stepupdates the prompt generation model to better align with the generated rationales.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3PromptCoT 2.0",
        "images": []
    },
    {
        "header": "4LLM Post-Training with Synthesized Prompts",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.19894/x2.png",
                "caption": "(a)Self-play on code benchmarks.",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x2.png",
                "caption": "(a)Self-play on code benchmarks.",
                "position": 927
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x3.png",
                "caption": "(b)Self-play on math benchmarks.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x4.png",
                "caption": "(c)SFT on code benchmarks.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x5.png",
                "caption": "(d)SFT on math benchmarks.",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x6.png",
                "caption": "(a)MDS visualization for code datasets.",
                "position": 977
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x6.png",
                "caption": "(a)MDS visualization for code datasets.",
                "position": 980
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x7.png",
                "caption": "(b)MDS visualization for math datasets.",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2509.19894/x8.png",
                "caption": "Figure 4:NLL trajectories during EM optimization.\nCurves compare trainingwithandwithoutthe E-step, alongside initialization references with and without rationales.\nConfidence bands (shaded) are reported for the optimized variants.",
                "position": 1099
            }
        ]
    },
    {
        "header": "6Related Works",
        "images": []
    },
    {
        "header": "7Conclusions and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProofs of Variational Results",
        "images": []
    },
    {
        "header": "Appendix BAdditional Results on Ring-Lite",
        "images": []
    },
    {
        "header": "Appendix CInstruction for Concept Extraction",
        "images": []
    },
    {
        "header": "Appendix DInstruction for Rationale Generation",
        "images": []
    },
    {
        "header": "Appendix ECase Study",
        "images": []
    }
]