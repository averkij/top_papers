[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07171/x1.png",
                "caption": "Figure 1:Motivation of IterComp.We select three types of compositional generation methods. The results show that different models exhibit distinct\nstrengths across various aspects of compositional generation.fig.3further demonstrated these distinct strengths quantitatively.",
                "position": 147
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07171/x2.png",
                "caption": "Figure 2:Overview of IterComp. We collect composition-aware model preferences from multiple models and employ an iterative feedback learning approach to enable the progressive self-refinement of both the base diffusion model and reward models.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x3.png",
                "caption": "Table 1:Statistics on the composition-aware model preference dataset. The dataset consists of 3,500 text prompts, 27,500 images, and 52,500 image-rank pairs.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x4.png",
                "caption": "Figure 4:Qualitative comparison between our IterComp and three types of compositional generation methods: text-controlled, LLM-controlled, and layout-controlled approaches. IterComp is the first reward-controlled method for compositional generation, utilizing an iterative feedback learning framework to enhance the compositionality of generated images. Colored text denotes the advantages of IterComp in generated images.",
                "position": 522
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07171/x5.png",
                "caption": "Figure 5:Results of user study.",
                "position": 709
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x6.png",
                "caption": "(a)Impact on CLIP Score.",
                "position": 837
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x6.png",
                "caption": "(a)Impact on CLIP Score.",
                "position": 840
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x7.png",
                "caption": "(b)Impact on Aesthetic Score.",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x8.png",
                "caption": "(c)Impact on ImageReward.",
                "position": 850
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x9.png",
                "caption": "Figure 7:Ablation study on the iterations of feedback learning.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2410.07171/x10.png",
                "caption": "Figure 8:The generation performance of integrating IterComp into RPG and Omost.",
                "position": 872
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.07171/x11.png",
                "caption": "Figure 9:More visualization results for IterComp and its base diffusion model, SDXL.",
                "position": 1977
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]