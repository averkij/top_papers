[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.13926/x1.png",
                "caption": "Figure 1.Overview ofSpaceBlender, a pipeline that extends state-of-the-art generative AI models to blend users’ physical surroundings into unified virtual environments for VR telepresence.",
                "position": 218
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.SpaceBlenderSystem",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.13926/extracted/5869119/Figures/CameraReady/fig2-early_exploration_blend.png",
                "caption": "Figure 2.A birds-eye view of two meshes that failed to blend due to the lack of geometric guidance and context throughout the iterative mesh completion process.",
                "position": 331
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x2.png",
                "caption": "Figure 3.Overview ofStage 1components as described in Sec.3.3.",
                "position": 395
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x3.png",
                "caption": "Figure 4.Comparison between unaligned submeshes and submeshes aligned with our semantic floor alignment technique. The unaligned spaces have floors at different levels and inclines that can be jarring to navigate.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x4.png",
                "caption": "Figure 5.Overview ofStage 2components as described in Sec.3.4.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x5.png",
                "caption": "Figure 6.Comparison of output generated with varying weights of ControlNet depth and layout models, impacting the prior’s impact on the output (generated with fixed seed). Top: input images including the input image, depth prior image, and layout prior image rendered from geometric prior. Bottom: results with varying weights are indicated in parentheses.",
                "position": 515
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x6.png",
                "caption": "Figure 7.Overview of the environments used for theGeneric3DandText2Roomconditions.",
                "position": 556
            }
        ]
    },
    {
        "header": "4.Preliminary User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.13926/extracted/5869119/Figures/CameraReady/fig8-study_participant_screenshots.jpg",
                "caption": "Figure 8.Captures of participants manipulating sticky notes while represented by Ubiq avatars in the individual (left) and collaborative (right) task phases.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x7.png",
                "caption": "Figure 9.Overview ofSpaceBlendermeshes generated based on input images uploaded (withgreenoutline) or selected by participants (no outline).",
                "position": 643
            }
        ]
    },
    {
        "header": "5.Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.13926/x8.png",
                "caption": "Figure 10.Plot of score distributions of Possible Actions, Self-Location, Copresence, and task impact factors. Levels of statistical significance:†forp<0.05𝑝0.05p<0.05italic_p < 0.05(before Bonferroni correction),∗forp<0.05𝑝0.05p<0.05italic_p < 0.05,∗∗forp<0.01𝑝0.01p<0.01italic_p < 0.01, and∗∗∗forp<0.001𝑝0.001p<0.001italic_p < 0.001(after Bonferroni correction).",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x9.png",
                "caption": "Figure 11.Plot of participant preferences per condition.",
                "position": 657
            }
        ]
    },
    {
        "header": "6.Discussion and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.13926/x10.png",
                "caption": "Figure 12.Examples ofSpaceBlenderpipeline components errors: (A) protrusion of outdoor structure, (B) distorted table, (C) depth estimation error, (D) depth alignment error, (C) depth estimation error, (F) floor expansion error.",
                "position": 814
            }
        ]
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.13926/x11.png",
                "caption": "Figure A.1.Additional results of the ControlNet-Layout model. Each of these output images was generated by combining Control-Layout and ControlNet-Depth with weights 0.6 and 0.3, respectively.",
                "position": 2056
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x12.png",
                "caption": "Figure A.2.Comparative image generation output of the recent LooseControl model and our ControlNet-Layout model, including prior images and output images each. The LooseControl prior and output images in this figure were reproduced from the paper’s web page (https://shariqfarooq123.github.io/loose-control) with permission from the authors. The ControlNet-Layout prior images were manually created to match the room structure depicted in the LooseControl prior images.",
                "position": 2060
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x13.png",
                "caption": "Figure A.3.Visual explanation of the impact of submesh count and shape on the shape of the geometric prior mesh and final blended space.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x14.png",
                "caption": "Figure A.4.Examples of spaces generated with various numbers of submeshes. Left: aSpaceBlendermesh based on four input images, all featuring corners. Right: aSpaceBlendermesh based on five input images featuring a mixture of room shapes.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2409.13926/x15.png",
                "caption": "Figure A.5.Overview of environments used in the walkthrough segment of the semi-structured interview, including acollaborative study sessionscenario (left) and acooking class with friendsscenario (right).",
                "position": 2220
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]