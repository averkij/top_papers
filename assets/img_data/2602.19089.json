[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19089/x1.png",
                "caption": "Figure 1:Given a reference human image and a target SMPL mesh sequence, our method synthesizes photorealistic 3D human animation. Unlike the previous state-of-the-art (SOTA) methods (e.g., LHM[58](top-right))that are limited to rigid motion, ourAni3DHuman(bottom)can further generate high-fidelity nonrigid dynamics, capturing the natural flow of the dress.",
                "position": 193
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19089/x2.png",
                "caption": "Figure 2:Pipeline overview.OurAni3DHumananimates a 3D Gaussianùí¢\\mathcal{G}(reconstructed with LHM[58]from the reference image) with a mesh sequence.\nOur layered motion combines a mesh-rigged motion with a residual field for non-rigid dynamics.\nA coarse renderingùíö{\\bm{y}}from the rigid motion is restored to a high-quality videoùíô‚àó{\\bm{x}}^{*}using our self-guided stochastic sampling.\nThis restored videoùíô‚àó{\\bm{x}}^{*}then provides supervision to progressively optimize the residual motion field.",
                "position": 242
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary: Flow Matching",
        "images": []
    },
    {
        "header": "4Proposed Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19089/x3.png",
                "caption": "Figure 3:Distribution mismatch in deterministic flow matching.Our degraded inputùíö{\\bm{y}}(out-of-distribution, OOD) creates a noisy latentùíôt{\\bm{x}}_{t}that is off the marginal distributionpt‚Äã(ùíô)p_{t}({\\bm{x}}).\nA deterministic Flow-ODE (orange path) follows an incorrect trajectory as its velocity predictions are inaccurate for OOD samples, resulting in a low-quality sample.\nThis motivates our use of an SDE sampler, which can actively correct the path by driving the sample back toward the marginal distribution.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x4.png",
                "caption": "Figure 4:Diagonal view-time sampling.\n(a) Illustration ofdiagonal samplingin a view-time matrix (Ntraj=3N_{\\text{traj}}=3). This method simultaneously evolves the camera view and time, distinct from fixed-time (bullet-time) or fixed-camera (independent-view) sampling. (b) An example trajectory shows the camera orbiting 360¬∞ as time progresses.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x5.png",
                "caption": "Figure 5:Comparison with state-of-the-art methods.Our method (Ours) is the only one to simultaneously achieve high quality, identity preservation, and realistic non-rigid motion. Existing methods fail in key areas: Disco4D[53]and SV4D 2.0[87]suffers from low quality (due to SDS and multi-view video diffusion); PERSONA loses identity (due to direct reconstruction from pose-driven video diffusion); and LHM[58]captures identity but fails to model clothing dynamics. (* self-implementation)",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x6.png",
                "caption": "Figure 6:Comparison on other video re-rendering methods.(a) original renderingùíô{\\bm{x}}; (b-f) competitive sampling methods; (g) our resultsùíô‚àó{\\bm{x}}^{*}.\nOnly our self-guided stochastic sampling can generate sharp details while preserving the original identity well.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x7.png",
                "caption": "Figure 7:Ablative experiment on self-guided stochastic sampling.We compare full sampling method (e) and the generation results of our model with a set of ablations. (a) Original rendering with mesh-rigged animation; (b) replacing personalized diffusion prior with general diffusion prior introduces slight performance degradation and artifacts; (c) We observe that our method produces significant quality drop when removing stochastic sampling; (d) removing self-guided sampling greatly reduces the identity preservation.",
                "position": 588
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19089/x8.png",
                "caption": "Figure 8:Ablation study on motion field.Our layered motion (right) captures intricate hand details, while the single-layer baseline (left) fails.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x9.png",
                "caption": "Figure 9:Ablation study on sampling method.Baseline methods (left) suffer from significant floaters and spikes, while our diagonal sampling (right) reconstructs sharp details.",
                "position": 838
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Video",
        "images": []
    },
    {
        "header": "Appendix BProof",
        "images": []
    },
    {
        "header": "Appendix CBackground",
        "images": []
    },
    {
        "header": "Appendix DMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix EAblation Studies (Extended)",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.19089/x10.png",
                "caption": "Figure 10:Sensitivity analysis of the initial noise strengtht0t_{0}.We visualize restoration results across varying noise strengths.\nLow noise levels (t0=0.2,0.4t_{0}=0.2,0.4) fail to deviate sufficiently from the source, leaving artifacts from the coarse mesh rendering intact.\nHigher noise levels (t0=0.6,0.8t_{0}=0.6,0.8) effectively hallucinate plausible details and correct non-rigid dynamics.\nNotably, thanks to our self-guidance mechanism, the identity is preserved even at high noise strengths (t0=0.8t_{0}=0.8), overcoming the traditional quality-fidelity trade-off.",
                "position": 2537
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x11.png",
                "caption": "(a)Effect of Densification.",
                "position": 2561
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x11.png",
                "caption": "(a)Effect of Densification.",
                "position": 2564
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x12.png",
                "caption": "(b)Effect of Mask Loss.",
                "position": 2570
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x13.png",
                "caption": "(c)Effect of Dataset Update.",
                "position": 2576
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x14.png",
                "caption": "Figure 12:Comparison with image-based animation.",
                "position": 2586
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x15.png",
                "caption": "Figure 13:Visual comparison of sampling strategies.Cases I: Two girls are walking (Row2/4) and running (Row1/3). The Mesh-Rigged Animation (Input) exhibits unrealistic artifacts, such as unnatural cloth dynamics and blurry edges. Direct Generation suffers from severe identity shift, introducing hallucinations like a bag (Row 2) or a watch (Row 3). ODE Sampling (represented by MCS[76]) fails to recover high-frequency details, leaving garment edges blurry due to the OOD nature of the input. In contrast, Ours successfully restores high-fidelity details and realistic motion while maintaining strict identity consistency.",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x16.png",
                "caption": "Figure 14:Visual comparison of sampling strategies. Case II: Two girls are walking (Row1/3), running (Row2), dancing (Row4).",
                "position": 2592
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x17.png",
                "caption": "Figure 15:Qualitative evaluation on the ActorsHQ[23]dataset (I).We show single person with difference motions.\nThe asterisk (*) denotes renderings at a specific viewpoint (elevation10‚àò10^{\\circ}, azimuth0‚àò0^{\\circ}).\nNote that slight spatial misalignments between the generation and ground truth are due to inherent errors in the SMPL estimation derived from the source video.\nDespite relying on a single-view input, our method faithfully preserves human identity and captures complex non-rigid deformations (e.g., dress dynamics), even during extreme poses such as high leg raises.",
                "position": 2595
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x18.png",
                "caption": "Figure 16:Human reconstruction results in ActorsHQ[23]dataset (II).We show different person with diverse motions.",
                "position": 2603
            },
            {
                "img": "https://arxiv.org/html/2602.19089/x19.png",
                "caption": "Figure 17:Additional human animation results.We visualize diverse subjects performing various motions, rendered with dynamic 360-degree camera trajectories.",
                "position": 2607
            }
        ]
    },
    {
        "header": "Appendix FResults (Extended)",
        "images": []
    }
]