[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16288/x1.png",
                "caption": "Figure 1:Global Matching Random Walks.We present a self-supervised method for tracking all physical points over the course of a video, i.e., the Tracking Any Point problem[7]. Our model uses a global matching transformer[49]to track points cycle consistently over time, using the contrastive random walk[19]. Our approach outperforms self-supervised tracking methods, such as self-supervised DIFT[40]and supervised optical flow methods, like RAFT[42], on the TAP-Vid benchmark[7].",
                "position": 83
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method: Global Matching Random Walk",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16288/x2.png",
                "caption": "Figure 2:Model Architecture.Our model takes a pair of imagesItsubscriptğ¼ğ‘¡I_{t}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandIt+1subscriptğ¼ğ‘¡1I_{t+1}italic_I start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTas input over which it computes correspondences. We extract visual features from a CNN, add positional encodings, and pass them as tokens to our global matching transformer. The transformer consisting of 6 stacked layers of self-attention, cross-attention and feed-forward networks, processes these features and produces correlated featuresFtsubscriptğ¹ğ‘¡F_{t}italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandFt+1subscriptğ¹ğ‘¡1F_{t+1}italic_F start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT. We compute self-attention overFtsubscriptğ¹ğ‘¡F_{t}italic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTandFt+1subscriptğ¹ğ‘¡1F_{t+1}italic_F start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPTand use the attention as the transition matrix for performing contrastive random walks. To compute tracks during evaluation, we can take an expectation over the affinity matrix to get coordinates(x,y)ğ‘¥ğ‘¦(x,y)( italic_x , italic_y ).",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2409.16288/x3.png",
                "caption": "Figure 3:Label Warping.We propose label warping as a remedy to avoid shortcut solutions that arise when we use transformer-based models for contrastive random walks. Instead of warping the last feature to match the first feature, we propose to warp the label used for cycle consistency. For an image pairIt,It+1subscriptğ¼ğ‘¡subscriptğ¼ğ‘¡1I_{t},I_{t+1}italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT, we apply different affine transformationsTfsuperscriptğ‘‡ğ‘“T^{f}italic_T start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT,Tbsuperscriptğ‘‡ğ‘T^{b}italic_T start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPTto the forward and backward cycle. We then computeAtt+1superscriptsubscriptğ´ğ‘¡ğ‘¡1A_{t}^{t+1}italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT,At+1tsuperscriptsubscriptğ´ğ‘¡1ğ‘¡A_{t+1}^{t}italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPTand chain them together to get the affinity matrix for cycle consistency. We then supervise it with the warped identity matrixTfbâ¢(I)superscriptsubscriptğ‘‡ğ‘“ğ‘ğ¼T_{f}^{b}(I)italic_T start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT ( italic_I )whereTfbsuperscriptsubscriptğ‘‡ğ‘“ğ‘T_{f}^{b}italic_T start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPTrepresents the transformation to go fromTfsuperscriptğ‘‡ğ‘“T^{f}italic_T start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPTtoTbsuperscriptğ‘‡ğ‘T^{b}italic_T start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT.",
                "position": 207
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.16288/x4.png",
                "caption": "Figure 4:Qualitative results.We show qualitative results for TapVid-DAVIS videos and compare them with DIFT and RAFT. DIFT relies on semantic correspondences and often loses the point of interest when motion occurs in the video. RAFT produces accurate movements for several tracks but suffers from drifting of points when the predictions are chained over a long period. In the first video, our method can track points accurately over the long timesteps. RAFT, on the other hand, loses current locations for 2 query points and latches on points on the ground and starts tracking them. In the other 2 videos as well, our method works better than RAFT and DIFT. DIFT produces inaccurate tracks that do not capture motion well. RAFT being accurate most of the time, loses track of points close to the boundary.",
                "position": 909
            },
            {
                "img": "https://arxiv.org/html/2409.16288/x5.png",
                "caption": "Figure 5:Optical flow visualization.Although our method is not trained for the optical flow prediction task, it is able to produce reasonable flow outputs over multiple timesteps. RAFT produces high quality flows as it is an optical flow method trained for this objective. DIFT predicts inaccurate flow which are spotty in nature, suggesting that it relies on finding semantic correspondence for certain points in the image, instead of relying local motion cues.",
                "position": 915
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]