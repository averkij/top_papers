[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works and Preliminaries",
        "images": []
    },
    {
        "header": "3Task Space for Generalizable RL",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23698/x1.png",
                "caption": "Figure 1:The Post-Training Pipeline.We synthesize large-scale, mixed-difficulty cross-view interaction tasks in an open-world environment by randomly sampling terrain, distances, target objects, and camera views. The foundational policy is fine-tuned using our distributed RL framework and then deployed in unseen 3D worlds via a simple action space mapping.",
                "position": 293
            }
        ]
    },
    {
        "header": "4Pipeline Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23698/x2.png",
                "caption": "Figure 2:Trajectory Storage Comparison.",
                "position": 474
            },
            {
                "img": "https://arxiv.org/html/2507.23698/x3.png",
                "caption": "Figure 3:RL Post-Training Boosts Generalizable Spatial Reasoning and Open-World Interaction Capabilities.(a)RL training curves for five skills in the Minecraft environment. This panel shows simultaneous performance gains across all skills. It also highlights the policy’s performance collapse in later training stages without a KL divergence constraint.(b)Sample target viewpoints for each skill during training, encompassing various camera view ranges (e.g., eye-level and top-down). “Archery” involves long-range interaction with mobs, while “Melee Hunt” requires close-quarters combat.(c)Comparison of curriculum-based training (mixed difficulties) with non-curriculum training (hard tasks only). The “Discounted Reward” plot on the left shows curriculum learning leads to higher training efficiency and faster reward accumulation, while the “Value Function Explained Variance” plot on the right demonstrates it also accelerates value function learning.(d)Results table for current SOTA goal-conditioned agents in Minecraft. Success rate is reported. Our agent is the first to achieve successful multi-task RL in challenging Minecraft environment. Several representative single-task RL agents are also listed for reference.(e)Point Prediction and Visibility Prediction loss comparison before and after RL training. Losses for these heads on the pre-training dataset remain largely unchanged despite not being optimized during RL, indicating that RL preserved the policy’s original representations.(f)This panel shows significant improvements in DMLab30 fruit collection, robot car approach, and Unreal rescue reward after RL training, demonstrating the model’s effective generalization to unseen 3D worlds.(g)Case studies of domain transfer. We analyze some successful and failure cases here. More details can be found in supplementary details. We performed 32 runs for each experiment.",
                "position": 479
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACross-View Task Synthesis Details",
        "images": []
    },
    {
        "header": "Appendix BReinforcement Learning Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23698/figures/storage_pipeline.png",
                "caption": "Figure 4:Our fragment-based storage strategy.Our rollout workers only save the initial latent states (K-V caches) at the beginning of each contiguous fragment. Latent states within the fragments are computed on the fly during tBPTT.",
                "position": 1336
            }
        ]
    },
    {
        "header": "Appendix CEvaluation Protocols",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23698/figures/car_exp.png",
                "caption": "Figure 5:The zero shot setting for real world environments.The goal would be blocked by the paper box if the car naively rotates towards the direction.",
                "position": 1493
            },
            {
                "img": "https://arxiv.org/html/2507.23698/figures/distance.png",
                "caption": "Figure 6:The easy and hard variant of cross-view approach setting.",
                "position": 1498
            },
            {
                "img": "https://arxiv.org/html/2507.23698/figures/other_settings.png",
                "caption": "Figure 7:A long distance approach task.The agent fails in the marble hallway due to Out Of Distribution challenges and perform better in the indoor case.",
                "position": 1504
            },
            {
                "img": "https://arxiv.org/html/2507.23698/figures/goal_comparison.png",
                "caption": "Figure 8:Different goal captures.Goals from phone cameras does not deteriorate the performance of our method.",
                "position": 1510
            }
        ]
    },
    {
        "header": "Appendix DModel Architecture",
        "images": []
    },
    {
        "header": "Appendix EAnalyzing Failure Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.23698/figures/goal_gallary.png",
                "caption": "(a)Gallery of task synthesis results, illustrating both successful cases and challenging corner cases.",
                "position": 1638
            },
            {
                "img": "https://arxiv.org/html/2507.23698/figures/goal_gallary.png",
                "caption": "(a)Gallery of task synthesis results, illustrating both successful cases and challenging corner cases.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2507.23698/figures/minecraft_demos.png",
                "caption": "(b)Minecraft Demonstrations.We present demonstrations on theApproach,Break,Interact,Melee Hunt, andArcherytasks. Additionally, we compare performance onHardversusEasytasks, noting thatHardtasks typically require exploration guided by visual cues.",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2507.23698/figures/zero_shot_envs.png",
                "caption": "Figure 10:Zero-Shot Environment Showcases.We evaluated both the pre-trained ROCKET-2 and our agent in Unreal(Zhong et al.2024), DMLab30(Beattie et al.2016), and real-world environments. Experimental results demonstrate that this reinforcement learning approach can significantly improve performance even in unseen settings.",
                "position": 1654
            }
        ]
    },
    {
        "header": "Appendix FDemonstration Showcases",
        "images": []
    }
]