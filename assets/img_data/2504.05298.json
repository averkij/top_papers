[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05298/x1.png",
                "caption": "",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05298/x2.png",
                "caption": "Figure 2:All RNN layers can be expressed as a hidden state that transitions according to an update rule.\nThe key idea in[43]is to make the hidden state itself a modelfùëìfitalic_fwith weightsWùëäWitalic_W, and the update rule a gradient step on the self-supervised loss‚Ñì‚Ñì\\ellroman_‚Ñì.\nTherefore, updating the hidden state on a test sequence is equivalent to training the modelfùëìfitalic_fat test time.\nThis process, known as Test-Time Training (TTT), is programmed into TTT layers.\nFigure and caption taken from[43].",
                "position": 102
            }
        ]
    },
    {
        "header": "2Test-Time Training Layers",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05298/x3.png",
                "caption": "Figure 3:Overview of our approach.Left: Our modified architecture adds a TTT layer with a learnable gate after each attention layer. See Subsection3.1.Right: Our overall pipeline creates input sequences composed of 3-second segments.\nThis structure enables us to apply self-attention layers locally over segments and TTT layers globally over the entire sequence.\nSee Subsection3.2.",
                "position": 270
            }
        ]
    },
    {
        "header": "3Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05298/x4.png",
                "caption": "Figure 4:On-chip Tensor Parallel, discussed in Subsection3.5.Left:To reduce the memory required on each SM for TTT-MLP, we shard the hidden stateW(1)superscriptùëä1W^{(1)}italic_W start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPTandW(2)superscriptùëä2W^{(2)}italic_W start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPTacross SMs, transferring them between HBM and SMEM only during initial loading and final output.Right:We update the hidden state entirely on-chip and use the DSMEM feature on the NVIDIA Hopper GPU architecture toAllReduceintermediate activations among SMs.",
                "position": 455
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05298/x5.png",
                "caption": "Figure 5:Video frames comparing TTT-MLP against Gated DeltaNet and sliding-window attention, the leading baselines in our human evaluation.\nTTT-MLP demonstrates better scene consistency by preserving details across transitions and better motion naturalness by accurately depicting complex actions.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2504.05298/x6.png",
                "caption": "",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2504.05298/x7.png",
                "caption": "Figure 7:Artifacts in videos generated by TTT-MLP.Temporal consistency: Objects sometimes morph at the boundaries of 3-second segments, potentially because the diffusion model samples from different modes across the segments.Motion naturalness: Objects sometimes float unnaturally because gravitational effects are not properly modeled.Aesthetics: Lighting changes do not consistently align with actions unless explicitly prompted.\nComplex camera movements, such as parallax, are sometimes depicted inaccurately.",
                "position": 738
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.05298/x8.png",
                "caption": "Figure 8:Illustration of the three prompt formats discussed in Subsection3.2: (1) a short summary of the plot, (2) sentence-level descriptions of the segments, and (3) a detailed storyboard.",
                "position": 1802
            }
        ]
    },
    {
        "header": "Appendix BOn-Chip Tensor Parallel Details",
        "images": []
    }
]