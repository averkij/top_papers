[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08294/x1.png",
                "caption": "",
                "position": 104
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3OpenSubject",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08294/x2.png",
                "caption": "Figure 2:Overview of theOpenSubjectpipeline. (a)Video curation: collect videos from OpenHumanVid, OpenVid, and OpenS2V, and apply resolution and aesthetic filters. (b)Cross-frame subject mining and pairing: verify objects with a visionâ€“language model (category consensus, visual clarity, occlusion, facial visibility), localize with Grounding-DINO, and select diverse frame pairs. (c)Identity-preserving reference synthesis: use mask-guided outpainting for generation and box-guided inpainting for manipulation. (d)Automated verification and captioning: perform VLM-based artifact checks and regenerate failures, then produce short and long captions for training.",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2512.08294/x3.png",
                "caption": "Figure 3:Dataset statistics of OpenSubject.(a) Spatial resolution distributions.\n(b) Distribution of the number of references per sample.\n(c) Word cloud for subjects.\n(d) Task composition across four sub-tasks.\n(e) Subject category frequency.",
                "position": 237
            }
        ]
    },
    {
        "header": "4Benchmark",
        "images": []
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08294/x4.png",
                "caption": "Figure 4:Qualitative comparison.Colored dashed boxes mark regions of interest for comparison. Boxes of the same color denote corresponding regions across methods and refer to the related area in the input image.",
                "position": 1115
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Implementation Details about OpenSubject Data Construction Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08294/x5.png",
                "caption": "Figure 8:Visualization of samples that fail and pass role-based bounding box filtering.",
                "position": 1895
            },
            {
                "img": "https://arxiv.org/html/2512.08294/x6.png",
                "caption": "Figure 9:Visualization of samples that fail and pass vlm-based bounding box filtering.",
                "position": 1898
            },
            {
                "img": "https://arxiv.org/html/2512.08294/x7.png",
                "caption": "Figure 10:Visualization of samples with and without erosion.",
                "position": 1901
            },
            {
                "img": "https://arxiv.org/html/2512.08294/x8.png",
                "caption": "Figure 11:Visualization of samples synthesized with FLUX.1-Fill [Dev].",
                "position": 1904
            }
        ]
    },
    {
        "header": "8Implementation Details about OpenSubject Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/multi_character_object_combined_04.png",
                "caption": "Figure 21:Visualization example.",
                "position": 2398
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/multi_character_object_combined_05.png",
                "caption": "Figure 22:Visualization example.",
                "position": 2401
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/multi_object_combined_01.png",
                "caption": "Figure 23:Visualization example.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/scene_character_combined_02.png",
                "caption": "Figure 24:Visualization example.",
                "position": 2407
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/scene_object_combined_03.png",
                "caption": "Figure 25:Visualization example.",
                "position": 2410
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/scene_object_combined_04.png",
                "caption": "Figure 26:Visualization example.",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/single_character_combined_01.png",
                "caption": "Figure 27:Visualization example.",
                "position": 2416
            },
            {
                "img": "https://arxiv.org/html/2512.08294/suppl_image/single_object_combined_02.png",
                "caption": "Figure 28:Visualization example.",
                "position": 2419
            }
        ]
    },
    {
        "header": "9Visualization",
        "images": []
    }
]