[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05827/x1.png",
                "caption": "",
                "position": 97
            }
        ]
    },
    {
        "header": "IIntroduction",
        "images": []
    },
    {
        "header": "IIRelated Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05827/x2.png",
                "caption": "Figure 2:Architecture and four-stage training pipeline of SparseVideoNav.(Top)denotes our whole training architecture. Current observation, historical observations, and the language instruction are fed into the video generation model (VGM) backbone to generate future sparse video latents.\nDiT-based action head then predicts continuous actions conditioned on generated sparse future and the language instruction.(Bottom)denotes our four-stage training pipeline,\nwithStage 1 (Sec.III-C)adapting T2V to I2V,Stage 2 (Sec.III-D)injecting history into I2V backbone;Stage 3 (Sec.III-E)distilling the backbone to reduce denoising steps;Stage 4 (Sec.III-F)learning actions based on generated sparse future.\nComponents not utilized in a specific stage are indicated bygray blocks.",
                "position": 164
            }
        ]
    },
    {
        "header": "IIIMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05827/x3.png",
                "caption": "Figure 3:Qualitative comparison of different sparse intervals.With the sparse interval of 3, the model successfully imagines a path towards beyond-the-view target, while maintaining visual fidelity.",
                "position": 258
            }
        ]
    },
    {
        "header": "IVEvaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05827/x4.png",
                "caption": "Figure 4:Qualitative results of zero-shot beyond-the-view navigation in challenging, unstructured environments.SparseVideoNav successfully navigates through challenging scenarios, including dead ends, narrow accessible ramp, and hillside with high inclination angles.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x5.png",
                "caption": "Figure 5:Analysis of video generation results of SparseVideoNavduring zero-shot deployment in beyond-the-view navigation.",
                "position": 669
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x6.png",
                "caption": "Figure 6:Ablation studyon a) data scalibility and computational overhead comparison over b) sparse design, c) distillation, and d) history compression.",
                "position": 672
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x7.png",
                "caption": "Figure 7:Ablation study on\nthe effectiveness of diffusion distillation (Stage 3).Our distillation strategy enables the model to achieve visual fidelity with only 4 denoising steps (top row) comparable to the original model using 50 steps (bottom row).",
                "position": 676
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x8.png",
                "caption": "Figure 8:Discussion\nof SparseVideoNav under dynamic pedestrian interference.SparseVideoNav successfully avoids the oncoming pedestrian and reaches the door.",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x9.png",
                "caption": "Figure 9:Discussion\nof SparseVideoNav with the camera fixed at 50cm.Images in green boxes demonstrate the predictions generated by SparseVideoNav. SparseVideoNav exhibits strong robustness against camera height variations.",
                "position": 774
            }
        ]
    },
    {
        "header": "VConclusion and Limitation",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05827/x10.png",
                "caption": "Figure A-1:Potential mode collapse of SparseVideoNav in challenging scenarios.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x11.png",
                "caption": "Figure A-2:Data Curation Pipeline.The workflow consists of three stages: (1) temporal sampling from raw video; (2) pose estimation via Depth Anything 3; and (3) extrinsic-based action extraction.",
                "position": 1509
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x12.png",
                "caption": "Figure A-3:Task specifications.We present the language instructions, the initial positions of robot dog and the task settings for each scene. When the precise locations can not be clearly seen, we utilize a green arrow icon to denote the initial positions and a location pin to mark the destination for visualization clarity.",
                "position": 1670
            },
            {
                "img": "https://arxiv.org/html/2602.05827/x13.png",
                "caption": "Figure A-4:Task specifications.We present the language instructions, the initial positions of robot dog and the task settings for each scene.",
                "position": 1673
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]