[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22134/x1.png",
                "caption": "",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22134/x2.png",
                "caption": "Figure 2:VLMs possess strong reasoning ability but lack action skills. Specialist VLAs achieve strong action capability but lose general reasoning. Reasoning VLAs partially recover reasoning through additional supervision, yet their action performance drops, illustrating the action degeneration problem. Our goal is to build a model that excels at both reasoning and action simultaneously.",
                "position": 227
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22134/x3.png",
                "caption": "Figure 3:Overview ofVLA Scoreevaluation pipeline.Given the policy trajectory, task description, and optional reasoning as input,VLA Scorefirst performs dual retrieval to fetch task-relevant textual examples and visually similar trajectories from a curated knowledge base. The retrieved samples serve as few-shot context for the VLM judge, which evaluates the trajectory along four dimensions: Reasoning, Action, Intention, and Alignment. These scores are then combined with the simulation outcome to produce the finalVLA Score.",
                "position": 311
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22134/x4.png",
                "caption": "Figure 4:Visualizationof the two real-world task progress.",
                "position": 705
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x5.png",
                "caption": "Figure 5:Ablation for distillation.",
                "position": 1118
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x5.png",
                "caption": "Figure 5:Ablation for distillation.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x6.png",
                "caption": "Figure 6:Ablation for base models.",
                "position": 1127
            }
        ]
    },
    {
        "header": "5Conclusion, Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVLA Score",
        "images": []
    },
    {
        "header": "Appendix BAdditional Qualitative Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22134/x7.png",
                "caption": "Figure 7:Embedding visualization of embodied reasoning and multimodal data.Robotic reasoning samples form dense, low-entropy clusters, whereas multimodal data remain more dispersed, indicating higher semantic diversity.",
                "position": 2373
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x8.png",
                "caption": "Figure 8:Example of redundant embodied reasoning across consecutive robot frames.Despite scene and motion changes, multiple frames share identical reasoning (e.g., “Move Near”), revealing redundancy that motivates pruning.",
                "position": 2379
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x9.png",
                "caption": "Figure 9:Visual examples of SimplerEnv Google robot tasks driven byDualVLA.",
                "position": 2413
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x10.png",
                "caption": "Figure 10:Visual examples of SimplerEnv WidowX robot tasks driven byDualVLA.",
                "position": 2417
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x11.png",
                "caption": "Figure 11:The successful cases ofDualVLAin real-world tasks.",
                "position": 2421
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x12.png",
                "caption": "Figure 12:The successful cases ofDualVLAin real-world tasks.",
                "position": 2425
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x13.png",
                "caption": "Figure 13:The failure cases ofDualVLAin SimplerEnv.",
                "position": 2429
            },
            {
                "img": "https://arxiv.org/html/2511.22134/x14.png",
                "caption": "Figure 14:The failure cases ofDualVLAin real-world tasks.",
                "position": 2433
            }
        ]
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]