[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11089/x1.png",
                "caption": "Figure 1:Comparison of performance and efficiency between Full Attention model and our NSA. Left: Despite being sparse, NSA surpasses Full Attention baseline on average across general benchmarks, long-context tasks, and reasoning evaluation. Right: For 64k-length sequence processing, NSA achieves substantial computational speedup compared to Full Attention in all stages: decoding, forward propagation, and backward propagation.",
                "position": 440
            }
        ]
    },
    {
        "header": "2Rethinking Sparse Attention Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11089/x2.png",
                "caption": "Figure 2:Overview of NSAâ€™s architecture. Left: The framework processes input sequences through three parallel attention branches: For a given query, preceding keys and values are processed into compressed attention for coarse-grained patterns, selected attention for important token blocks, and sliding attention for local context. Right: Visualization of different attention patterns produced by each branch. Green areas indicate regions where attention scores need to be computed, while white areas represent regions that can be skipped.",
                "position": 481
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11089/x3.png",
                "caption": "Figure 3:Kernel design for NSA. The kernel loads queries by GQA groups (Grid Loop), fetches corresponding sparse KV blocks (Inner Loop), and performs attention computation on SRAM. Green blocks indicate data on SRAM, while blue indicates data on HBM.",
                "position": 810
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11089/x4.png",
                "caption": "Figure 4:Pretraining loss comparison between Full Attention and our NSA on 27B-parameter model. Both models exhibit stable convergence, with NSA achieving lower loss values.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2502.11089/x5.png",
                "caption": "Figure 5:Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length. NSA achieves perfect accuracy through its hierarchical sparse attention design.",
                "position": 1031
            }
        ]
    },
    {
        "header": "5Efficiency Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11089/x6.png",
                "caption": "Figure 6:Comparison of Triton-based NSA kernel with Triton-based FlashAttention-2 kernel. Our implementation significantly reduces latency across all context lengths, with the improvement becoming more pronounced as input length increases.",
                "position": 1086
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.11089/x7.png",
                "caption": "Figure 7:Compare training loss on a 3B-parameter model with Full Attention and different token selection strategies and. Our NSA achieves better performance.",
                "position": 1160
            },
            {
                "img": "https://arxiv.org/html/2502.11089/x7.png",
                "caption": "Figure 7:Compare training loss on a 3B-parameter model with Full Attention and different token selection strategies and. Our NSA achieves better performance.",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2502.11089/x8.png",
                "caption": "Figure 8:Visualization of Attention Map on a Full Attention transformer. Light-colored regions indicate higher attention values. As shown in the figure, attention scores exhibit blockwise clustering distribution.",
                "position": 1168
            }
        ]
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExample of AIME Results",
        "images": []
    }
]