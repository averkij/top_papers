[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21046/x1.png",
                "caption": "Figure 1:A conceptual trajectory illustrating the progression from large language models (LLMs) to foundation agents, advancing to self-evolving agents—our focus, and ultimately toward hypothetical Artificial Super Intelligence (ASI). Along this path, intelligence and adaptivity increase, marking a shift toward more autonomous and agentic AI systems.",
                "position": 273
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21046/x2.png",
                "caption": "Figure 3:A comprehensive overview of self-evolving agents across key dimensions:What to evolve: covering four major categories—Model, Context, Tool, and Architecture;When to evolve: differentiating intra-test-time and inter-test-time self-evolution, via in-context learning (ICL), supervised fine-tuning (SFT), or reinforcement learning (RL);How to evolve: centered on three main paradigms—reward-based, imitation and demonstration, and population-based methods. These are complemented by cross-cutting dimensions.Where to evolve: ranging from general-purpose domains to specific domains;Evaluation: focusing on goals (e.g., adaptivity, safety, generalization) and evaluation paradigms (static, short-horizon, or long-horizon).",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2507.21046/x3.png",
                "caption": "Figure 4:An evolutionary landscape of several representative self-evolving agent frameworks from 2022 to 2025. The figure chronologically organizes major research milestones in the development of self-evolving agents with capabilities such as autonomous planning, tool use, and continual self-improvement.",
                "position": 739
            }
        ]
    },
    {
        "header": "2Definitions and Foundations",
        "images": []
    },
    {
        "header": "3What to Evolve?",
        "images": []
    },
    {
        "header": "4When to Evolve",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21046/x4.png",
                "caption": "Figure 5:An overview of when to evolve. The top pathway illustrates intra-test-time self-evolution, where adaptation (e.g., variant generation, verification, and policy update) occurs within task execution. The bottom pathway depicts inter-test-time self-evolution, where learning happens retrospectively through rollout, trajectory analysis, and policy updates.",
                "position": 1481
            }
        ]
    },
    {
        "header": "5How to Evolve",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21046/x5.png",
                "caption": "Figure 6:Overview of reward-based self-evolution strategies, categorized into textual, implicit, internal, and external rewards, each associated with distinct feedback sources and mechanisms.",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2507.21046/x6.png",
                "caption": "Figure 7:Illustration of cross-cutting evolutionary dimensions in agent self-evolution, structured along three key axes: learning paradigm (offline/online), policy consistency (on/off-policy), and reward granularity (process-based, outcome-based, and hybrid). These dimensions jointly characterize how autonomous agents generate data, interact with environments, adapt policies, and receive feedback, providing a structured lens for analyzing reward-based, imitation-based, and population-based evolution strategies.",
                "position": 2270
            }
        ]
    },
    {
        "header": "6Where to Evolve?",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21046/x7.png",
                "caption": "Figure 8:Categorization of where to evolve into two major types: General Domain Evolution, which focuses on broad capability enhancement across diverse tasks (e.g., memory mechanisms, co-evolution, curriculum training), and Specific Domain Evolution, which targets domain-specific expertise in areas such as coding, GUI, finance, medical, education, and others.",
                "position": 2577
            }
        ]
    },
    {
        "header": "7Evaluation of Self-evolving Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.21046/x8.png",
                "caption": "Figure 9:Overview of evaluation angles for self-evolving agents, encompassing core Evaluation Goals and Metrics—such as adaptivity, retention, generalization, safety, and efficiency—and a continuum of Evaluation Paradigms spanning from static assessment to short-term adaptability and long-horizon lifelong learning evaluation.",
                "position": 2670
            }
        ]
    },
    {
        "header": "8Future Direction",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]