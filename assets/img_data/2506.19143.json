[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x1.png",
                "caption": "Figure 1:Summary of our three methods for principled attribution to important sentences in reasoning traces.A.An example reasoning trace with sentences labeled per our taxonomy.B.Our proposed methods are: black-box resampling, receiver heads, and attention suppression.C.A directed acyclic graph among sentences prepared by one of our techniques, made available open source.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Setup",
        "images": []
    },
    {
        "header": "3Measuring counterfactual influence",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x2.png",
                "caption": "Figure 2:Accuracy over 100 rollouts at each sentence for (A) one correct and (B) one incorrect base solution. Red dots mark significant spikes or dips. Local minima and maxima sentences are annotated with category initials (e.g., PG =plan generation). The analyses below focus on the counterfactual KL-divergence between sentences, but resampling accuracy is visualized here as it is more intuitive.",
                "position": 271
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x3.png",
                "caption": "Figure 3:The mean of each sentence category for (A) forced-answer importance and (B) counterfactual importance, per the resampling method, plotted against the sentence category’s mean position in the reasoning trace. Only the 5 most common sentence types are shown (seeAppendixF).",
                "position": 280
            }
        ]
    },
    {
        "header": "4Attention aggregation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x4.png",
                "caption": "Figure 4:A. Lines show the vertical attention scores for each sentence by the 40 different heads in layer 36. Head 6 has been highlighted as a receiver head, and its corresponding attention weight matrix is shown for reference. Its prominent spikes cause the distribution to have a high kurtosis.B. Histogram of these kurtosis values across all attention heads, averaged across all reasoning traces.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x5.png",
                "caption": "Figure 5:The boxplot shows the average top-16 receiver-head score for each sentence type. The boxes correspond to the interquartile range across different reasoning traces.",
                "position": 343
            }
        ]
    },
    {
        "header": "5Attention suppression",
        "images": []
    },
    {
        "header": "6Case study",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x6.png",
                "caption": "Figure 6:Case study: problem #4682 (correct). Red matrix shows the effect of suppressing one sentence (x-axis) on a future sentence (y-axis). Darker colors indicate higher values. Bottom-left line plot shows the average attention toward each sentence by all subsequent sentences via the top-32 receiver heads (32 attention heads with the highest kurtosis score). Flowchart summarizes the model’s CoT with chunks defined around key sentences receiving high attention via receiver heads. Sentence 13 is emphasized as it has high counterfactual importance per the resampling method (see Figure2A).",
                "position": 375
            }
        ]
    },
    {
        "header": "7Related work",
        "images": []
    },
    {
        "header": "8Discussion and Limitations",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASentence taxonomy",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x7.png",
                "caption": "Figure 7:Counts and frequencies of taxonomic sentence categories in our dataset.",
                "position": 1119
            }
        ]
    },
    {
        "header": "Appendix BPrompt information",
        "images": []
    },
    {
        "header": "Appendix CSentence category probing",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x8.png",
                "caption": "Figure 8:Confusion matrix showing the sentence category classification performance of a logistic regression probe trained on activations from layer 47 of the R1-Distill-Qwen-14B model. Values represent the proportion of examples from each true category (rows) classified as each predicted category (columns). Diagonal elements indicate correct classifications.",
                "position": 1219
            }
        ]
    },
    {
        "header": "Appendix DOther reasoning model",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x9.png",
                "caption": "Figure 9:Accuracy over 100 rollouts at each sentence for (A) one correct and (B) one incorrect base solution for R1-Distill-Llama-8B. Red dots mark significant spikes or dips. Local minima and maxima sentences are annotated with category initials. Our analyses focus on the counterfactual KL-divergence between sentences, but resampling accuracy is visualized here as it is more intuitive.",
                "position": 1241
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x10.png",
                "caption": "Figure 10:The mean of each sentence category for (A) forced-answer importance and (B) counterfactual importance for R1-Distill-Llama-8B, per the resampling method, plotted against the sentence category’s mean position in the reasoning trace. Only the 5 most common sentence types are shown.",
                "position": 1244
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x11.png",
                "caption": "Figure 11:The plots here show the vertical-attention score patterns associated with the R1-Distill-Llama-8B data.A.This histogram shows the kurtosis values across all attention heads, averaged across all reasoning traces; parallels Figure4based on the R1-Qwen-14B data.B.This scatterplot shows the kurtosis of each head’s vertical-attention score, organized by layer. Figure17is the R1-Distill-Qwen-14B version of this figure, which showed an upward trend into later layers that is not evident here.",
                "position": 1260
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x12.png",
                "caption": "Figure 12:Based on the R1-Distill-Llama-8B data, the boxplot shows the average top-64 receiver-head score for each sentence type. The boxes correspond to the interquartile range across different reasoning traces. Figure5is the R1-Distill-Qwen-14B version of this figure; note that for the R1-Distill-Qwen-14B figure, the top-16 heads were used. We found that for Llama 8B, examining the top-64 heads yielded more pronounced differences, although the sentence types with the highest scores remain the same.",
                "position": 1263
            }
        ]
    },
    {
        "header": "Appendix EEmbeddings model",
        "images": []
    },
    {
        "header": "Appendix FAdditional resampling results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x13.png",
                "caption": "Figure 13:The mean of each sentence category for (A) forced-answer importance and (B) counterfactual importance for R1-Distill-Qwen-14B, per the resampling method, plotted against the sentence category’s mean position in the reasoning trace. All sentence types are shown.",
                "position": 1281
            }
        ]
    },
    {
        "header": "Appendix GAdditional resampling details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x14.png",
                "caption": "Figure 14:(A) Fraction of semantically different resampled sentences by category, showing thatuncertainty managementandplan generationsentences produce more divergent alternatives when resampled. (B) Transition probabilities between original and resampled sentence categories.",
                "position": 1300
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x15.png",
                "caption": "Figure 15:Comparison between counterfactual and resampling importance metrics across sentence categories. Each point represents a single sentence and the dashed gray line is they=x𝑦𝑥y=xitalic_y = italic_xline.",
                "position": 1306
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x16.png",
                "caption": "Figure 16:Sentence-to-sentence importance matrix for the 32 most important sentences in problem #2236 (incorrect), selected based on total outgoing and incoming importance. Each cell(i,j)𝑖𝑗(i,j)( italic_i , italic_j )shows the causal importance of sentencei𝑖iitalic_ion sentencej𝑗jitalic_j, calculated as the difference in the probability sentencej𝑗jitalic_jsemantically occurs (>0.8absent0.8>0.8> 0.8cosine similarity) when sentencei𝑖iitalic_iis present versus resampled.",
                "position": 1372
            }
        ]
    },
    {
        "header": "Appendix HAdditional receiver head information",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x17.png",
                "caption": "Figure 17:This scatterplot shows the kurtosis of each head’s vertical-attention score, organized by layer. There is an upward trend across layers and a strong uptick among some late-layer heads.",
                "position": 1414
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x18.png",
                "caption": "Figure 18:The attention weight matrices for the receiver head with the highest kurtosis score are shown here for all twenty responses. The coloring was defined such that the darkest navy corresponds to values surpassing 99.5th percentile value of each matrix. White is zero.",
                "position": 1417
            },
            {
                "img": "https://arxiv.org/html/2506.19143/x19.png",
                "caption": "Figure 19:The attention weight matrices for response #1591 (incorrect) are shown here for the 20 attention heads yielding the highest kurtosis score across all responses. No effort was taken to “cherry-pick” responses showing prominent receiver head patterns; we are showing #1591 (incorrect) because it corresponded to the alphabetically earliest problem number among the ten problems analyzed (correct/incorrect chosen randomly). The coloring was defined such that the darkest navy corresponds to values surpassing 99.5th percentile value of each matrix. White is zero.",
                "position": 1420
            }
        ]
    },
    {
        "header": "Appendix IReasoning versus base model differences in receiver heads",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.19143/x20.png",
                "caption": "Figure 20:The navy and red lines on the left show the receiver-head scores assigned to sentences, averaged across the 16 heads with the highest kurtoses. The green lines on the right represent the ratio of the navy and blue lines for a given sentence rank. Sentences with high receiver head scores receive more attention in the reasoning model compared to the base model.",
                "position": 1430
            }
        ]
    },
    {
        "header": "Appendix JEffects of ablating receiver heads",
        "images": []
    },
    {
        "header": "Appendix KReceiver head correlations with sentence-sentence resampling importance",
        "images": []
    },
    {
        "header": "Appendix LCase study transcript",
        "images": []
    },
    {
        "header": "Appendix MSentence position effects on receiver-head scores",
        "images": []
    }
]