[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06307/x1.png",
                "caption": "Figure 1:Left:From observations of the sound emitted by a speaker, our model constructs an impulse response field that can synthesize observations at novel listener positions.Right:Visualization of spatial variation of impulse responses on MeshRIR[20]. The synthesized impulse responses at different locations are transformed into the frequency domain, where we visualize phase and amplitude distributions at a specific wavelength (1m).",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2411.06307/x2.png",
                "caption": "Figure 2:AcoustiXfor improved acoustic simulation.Time-of-flight indicates how long it takes for an emitted sound to reach a listener. With sound traveling at a constant speed, the time-of-arrival should be proportional to the emitter-listener distance. While SoundSpace¬†2.0 simulations show significant time-of-flight errors, particularly at short emitter-listener distances,AcoustiXproduces more accurate arrival times. All simulations are performed in the Gibson Montreal room[56]with direct line-of-sight between emitter and listener.",
                "position": 123
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06307/x3.png",
                "caption": "Figure 3:Acoustic Rendering pipeline.We sample points along the ray that is shot from the microphone and query the network to obtain signalss‚Å¢(t)ùë†ùë°s(t)italic_s ( italic_t )and densityœÉùúé\\sigmaitalic_œÉ.\nTime delay (dvùëëùë£\\frac{d}{v}divide start_ARG italic_d end_ARG start_ARG italic_v end_ARG) is applied to account for the wave propagation. After that, we combine signals and densities to perform acoustic volume rendering for each ray to get the directional signal (hd‚Å¢i‚Å¢r‚Å¢(t)subscript‚Ñéùëëùëñùëüùë°h_{dir}(t)italic_h start_POSTSUBSCRIPT italic_d italic_i italic_r end_POSTSUBSCRIPT ( italic_t )).\nWe integrate along the sphere to combine signals from all possible directions with gain patternG‚Å¢(œâ)ùê∫ùúîG(\\omega)italic_G ( italic_œâ )to obtain the final rendered impulse responseh‚Å¢(t)‚Ñéùë°h(t)italic_h ( italic_t ).",
                "position": 283
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06307/x4.png",
                "caption": "Figure 4:Visualization of spatial signal distributions.We compare the spatial signal distributions between ground truth and various methods on the MeshRIR dataset and two simulated environments. While NAF and INRAS fail to capture the signal distributions, our model can estimate amplitude and phase distributions accurately.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2411.06307/x5.png",
                "caption": "Figure 5:Top-down view of loudness map on MeshRIR.AVRpredicts an accurate loudness map, while NAF and INRAS have inaccurate patterns.",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2411.06307/x6.png",
                "caption": "Figure 6:Examples of synthesized impulse response with different methods.We visualize the synthesized impulse responses as time signals (x-axis) on both real and simulated datasets.Orangelines represent model predictions andBluelines represent ground truth. All plots share a common y-axis for easier comparison.",
                "position": 665
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASampling Rays and Points",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Metric",
        "images": []
    },
    {
        "header": "Appendix CMore Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06307/extracted/5989538/fig/inference_speed.png",
                "caption": "Figure 7:Impact of ray and point counts on inference speed.",
                "position": 2022
            },
            {
                "img": "https://arxiv.org/html/2411.06307/extracted/5989538/fig/raf_furnished_loudness.png",
                "caption": "Figure 8:Loudness map.We visualize the loudness map of various methods using the RAF-Furnished dataset, which features the most complex structure among all the datasets we utilized.Greendots and arrows represent the speaker positions and orientations from a top view.Graydots represent the room structures, outlining the geometry of walls, objects, and other elements.",
                "position": 2088
            }
        ]
    },
    {
        "header": "Appendix DAcoustic Simulation Platform",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.06307/extracted/5989538/fig/ir_plot.png",
                "caption": "Figure 9:Example of a simulated impulse response",
                "position": 2126
            }
        ]
    },
    {
        "header": "Appendix ESocial Impact",
        "images": []
    }
]