[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09984/x1.png",
                "caption": "Figure 1:Video frames generated from audio and multi-concept reference images (human heads/full bodies, objects, scenes) display rich, audio-matched expressions. Our method enables compositional generation including outfit changes, human‚Äìobject interactions, anime styles, dialogues even without a start frame. Red and green wave icons denote speaking and listening, respectively.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09984/extracted/6530212/figs/framework.jpg",
                "caption": "Figure 2:Illustration of our framework, which adaptively predicts masks as the spatial guidance of audio condition injection. In training, we train the mask predictor (cross-attn w/ MLP) with mask loss; in inference, we collect mask predictions to cache and leverage masks predicted from the last denoising step (t‚àí1ùë°1t-1italic_t - 1) to guide the audio cross-attn in the current denoising step (tùë°titalic_t).",
                "position": 160
            },
            {
                "img": "https://arxiv.org/html/2506.09984/x2.png",
                "caption": "Figure 3:Qualitative comparison with previous methods on multi-concept audio injection.",
                "position": 199
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.09984/x3.png",
                "caption": "Figure 4:Qualitative comparison with previous methods on subject consistency and text following.",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2506.09984/x4.png",
                "caption": "Figure 5:Qualitative ablation on audio injection strategies.",
                "position": 577
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAlgorithm of Our Model Implementation",
        "images": []
    },
    {
        "header": "Appendix BExperiment Details of Our Model",
        "images": []
    },
    {
        "header": "Appendix CDetails of Audio-driven Base Model‚Äôs Architecture and Training",
        "images": []
    },
    {
        "header": "Appendix DAdditional Details for Audio-driven Base Model Dataset",
        "images": []
    },
    {
        "header": "Appendix EUser Study Details",
        "images": []
    }
]