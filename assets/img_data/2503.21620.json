[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/Fig0_v2.png",
                "caption": "Figure 1:Left: Overall performance of UI-R1-3B on both in-domain (i.e.,AndroidControl) and out-of-domain (i.e., ScreenSpot-Pro, ScreenSpot desktop and web subsets) tasks;Right: Employing reinforcement fine-tuning (RFT), UI-R1-3B achieves performance comparable to SFT models with significantly fewer data and GPU hours. The circle radius indicates the model size.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/Fig_1_v5.png",
                "caption": "Figure 2:Overview of UI-R1 training framework. Given a GUI screenshot and a text instruction from the user, the policy model (i.e., Qwen2.5-VL-3B) generates multiple action planning responses with reasoning. Our proposed rule-based action reward function is then applied, and the policy model is updated using a policy gradient optimization algorithm.",
                "position": 172
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/study.png",
                "caption": "Figure 3:Left: Impact of data selection methods and data size;Right: Study of relation between answering accuracy and reasoning length.",
                "position": 1377
            },
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/ablation.png",
                "caption": "Figure 4:Left: Ablation on reward function;Right: Ablation on data selection method.",
                "position": 1380
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ATraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/loss_reward.png",
                "caption": "Figure 5:UI-R1 training process.",
                "position": 1892
            }
        ]
    },
    {
        "header": "Appendix BOther Evaluation",
        "images": []
    },
    {
        "header": "Appendix COther Ablation",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/iteration.png",
                "caption": "Figure 6:Accuracy change over rounds.",
                "position": 2033
            },
            {
                "img": "https://arxiv.org/html/2503.21620/extracted/6315182/images/user_case.png",
                "caption": "Figure 7:An example of use case.",
                "position": 2149
            }
        ]
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    }
]