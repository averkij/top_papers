[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11514/assets/teaser.jpg",
                "caption": "",
                "position": 78
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11514/assets/motivation.jpg",
                "caption": "Figure 2:(Top) Objects captured in casual settings pose challenges like clutter, poor viewpoints, low resolution, noise, motion blur, and occlusions that are difficult to segment, even interactively. (Bottom) State-of-the-art 3D models often fail in these scenarios, while ShapeR remains robust and effective.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11514/assets/method.jpg",
                "caption": "Figure 3:The ShapeR denoising transformer, built on the FLUX DiT architecture, denoises latent VecSets by conditioning on multiple modalities: posed images, SLAM points, captions, and the 2D projections of SLAM points observed in those input images. SLAM points are encoded with a sparse 3D ResNet, images using a frozen DINOv2 backbone, poses using Plücker encodings, and projection masks via a 2D convolutional network. The denoised latent is decoded into a SDF, from which the final object shape is extracted using marching cubes.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/ablation_points.jpg",
                "caption": "Figure 4:Incorporating SLAM points significantly enhances robustness. These points provide a complementary geometric signal to posed images, encoding aggregated shape information across the entire sequence.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/augmentations.jpg",
                "caption": "Figure 5:(Left) We pretrain on 600K object meshes with extensive, compositional augmentations across all modalities, simulating realistic backgrounds via image compositing, and introducing diverse occlusions and noise in both images and SLAM points. (Right) We then fine-tune on object-centric crops from Aria Synthetic Environment scenes, which feature realistic image occlusions, SLAM point cloud noise, and inter-object interactions.",
                "position": 201
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11514/assets/comparison_recon.jpg",
                "caption": "Figure 6:Qualitative comparison on the ShapeR evaluation dataset against posed multiview-to-3D methods. For scene-centric fusion approaches (EVL, Foundation Stereo), ground-truth meshes are used to segment individual object shapes. For methods relying on image segmentation masks (DP-Recon, LIRM), we employ SAM2, prompted with bounding boxes, to generate input image masks.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/comparison_image3d.jpg",
                "caption": "Figure 7:Qualitative comparison against foundation image-to-3D models. For these baselines, we manually select a view with clear object visibility and use interactive SAM2-based segmentations to provide optimal input. In contrast, ShapeR operates fully automatically on multiple posed views and preprocessed inputs, requiring no manual intervention.",
                "position": 391
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/comparison_scene.jpg",
                "caption": "Figure 8:Comparison with image-to-scene methods. MIDI uses a single image and SceneGen uses four views, both with manual object segmentations. These approaches struggle with object scale and arrangement, while ShapeR reconstructs each object metrically and independently, maintaining consistent scale and layout across the scene and without interactive segmentation.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/ablation_augmentation.jpg",
                "caption": "Figure 9:Ablations of components. (a) Without point augmentations, the model overfits to point inputs, missing geometry in regions without points. Image augmentations address occlusions and incomplete objects crops. Omitting background composition requires pre-segmentation, which can introduce noisy masks and prediction errors. (b) Fine-tuning on scene-centric crops improves robustness in challenging scenarios over object-centric training alone. (c) Prompting DINO features with 2D point projections clarifies which object to reconstruct in cluttered scenes, reducing confusion from nearby objects and improving reconstruction accuracy.",
                "position": 403
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.11514/assets/dataset_motivation.jpg",
                "caption": "Figure 10:Comparison of 3D reconstruction datasets. DTC[23]and StanfordORB[40]offer controlled studio captures of isolated objects, while ScanNet++[87]and Replica[72]provide realistic scenes but lack complete ground-truth shapes. The ShapeR evaluation dataset features casually captured sequences with complete meshes for geometric evaluation (seeFigs.11and12).",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/dataset_samples.jpg",
                "caption": "Figure 11:(Top) Examples from the ShapeR evaluation dataset. Each sub-image shows the annotated ground-truth mesh, a representative frame containing the object, the mesh placed within the sequence, and the projection of the mesh onto the image. (Bottom) Distribution of object shapes categories in the ShapeR evaluation set, covering 178 objects across 7 sequences",
                "position": 1703
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/dataset_annotation_process.jpg",
                "caption": "Figure 12:To obtain pseudo-ground truth geometry for an object in the sequence (left), we first place the object in isolation to avoid clutter and occlusion, and capture a high-quality, uncluttered image. We then apply segmentation and image-to-3D modeling to generate the object’s geometry (mid). This geometry is manually aligned and inserted back into the original casual sequence using a web annotation interface, verified by matching 2D projections to image silhouettes and by checking alignment with the sequence’s point cloud (right).",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/comparison_data_trend.jpg",
                "caption": "Figure 13:DTC Active, DTC Passive, and ShapeR Evaluation datasets represent a progression from highly controlled capture setups (DTC Active), to slightly less controlled environments (DTC Passive), and finally to casual, real-world scenes (ShapeR Evaluation). As the datasets become more challenging, baseline method metrics deteriorate, while ShapeR remains comparatively stable. Notably, the increase in scene casualness is not linear; ShapeR Evaluation is significantly more challenging than DTC Passive.",
                "position": 1709
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/single_image_mapanything.jpg",
                "caption": "Figure 14:Single image to metric 3D with ShapeR. While ShapeR is trained to leverage posed multi-view signals, it can be configured for single-image 3D reconstruction without retraining by using a metric point cloud and camera estimator such as MapAnything[38]. This enables ShapeR to generate metrically accurate 3D shapes from a monocular image.",
                "position": 1712
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/limitations.jpg",
                "caption": "Figure 15:ShapeR limitations. (a) Low image fidelity or limited views lead to incomplete or low-detail reconstructions. (b) Closely stacked or attached objects can cause meshes to include parts of adjacent structures, even when the point associated with these structures are not in the input (c) ShapeR relies on upstream 3D detection; missed or inaccurate detections result in unrecoverable objects.",
                "position": 1715
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/comparison_sam3dobject.jpg",
                "caption": "Figure 16:Comparison with SAM 3D Objects[14]. SAM 3D Objects takes a single image and interactive object segments to produce non-metric 3D shapes, which are generally accurate but may exhibit minor hallucinations (e.g., predicting five lamps instead of four) and poor object placement. In contrast, ShapeR leverages posed images from a sequence to generate metrically accurate geometry and consistently well-placed objects.",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/comparison_dtc_lirm.jpg",
                "caption": "Figure 17:Comparison against LIRM[44]on DTC[23]Active and Passive sequences. Both setups feature tabletop objects without clutter or occlusions; however, Passive sequences allow more free user movement, while Active sequences involve the user circling the object. ShapeR performs competitively on Active sequences and surpasses LIRM on the slightly more casual Passive sequences.",
                "position": 1721
            },
            {
                "img": "https://arxiv.org/html/2601.11514/assets/third_party_comparison.jpg",
                "caption": "Figure 18:Reconstruction results on ScanNet++[87]and Replica[72]scenes, compared to DPRecon[56]. ShapeR produces complete reconstructions, often surpassing the ground-truth scans in completeness, as the latter lack geometry in occluded regions.",
                "position": 1724
            }
        ]
    },
    {
        "header": "Appendix AShapeR Evaluation Dataset",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    }
]