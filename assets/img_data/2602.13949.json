[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13949/x1.png",
                "caption": "Figure 1:InExperiential Reinforcement Learning(ERL), instead of learning from feedback or outcome directly, an agent learns to (1) verbally reflect on its experience and observed outcome, and (2) internalize the reflections to induce behavioral changes in future iterations.",
                "position": 89
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13949/x2.png",
                "caption": "Figure 2:Conceptual comparison of learning dynamics in RLVR and Experiential Reinforcement Learning (ERL). RLVR relies on repeated trial-and-error driven by scalar rewards, leading to back-and-forth exploration without durable correction. ERL augments this process with an experience–reflection–consolidation loop that generates a revised attempt and internalizes successful corrections, enabling persistent behavioral improvement.",
                "position": 97
            },
            {
                "img": "https://arxiv.org/html/2602.13949/x3.png",
                "caption": "Figure 3:Overview of Experiential Reinforcement Learning (ERL).\nGiven an input taskxx, the language model first produces an initial attempt and receives environment feedback.\nThe same model then generates a self-reflection conditioned on this attempt, which is used to guide a second attempt.\nBoth attempts and reflections are optimized with reinforcement learning, while successful second attempts are internalized via self-distillation, so the model learns to reproduce improved behavior directly from the original input without self-reflection.",
                "position": 144
            }
        ]
    },
    {
        "header": "2Experiential Reinforcement Learning (ERL)",
        "images": []
    },
    {
        "header": "3Experiment",
        "images": []
    },
    {
        "header": "4Result and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13949/x4.png",
                "caption": "Figure 4:Validation reward trajectories versus training wall-clock time on FrozenLake, HotpotQA, and Sokoban for Qwen3-4B-Instruct-2507 and Olmo-3-7B-Instruct. ERL consistently achieves higher reward and faster improvement than RLVR across tasks and models.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2602.13949/x5.png",
                "caption": "Figure 5:Final evaluation reward on FrozenLake, HotpotQA, and Sokoban. ERL consistently outperforms RLVR for both Qwen3-4B-Instruct-2507 and Olmo-3-7B-Instruct.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2602.13949/x6.png",
                "caption": "Figure 6:Training reward trajectories for Qwen3-4B-Instruct-2507 and Olmo-3-7B-Instruct comparing RLVR with ERL before and after reflection. Post-reflection trajectories consistently achieve higher reward than both RLVR and pre-reflection trajectories.",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2602.13949/x7.png",
                "caption": "Figure 7:Ablation study on Qwen3-4B-Instruct-2507 in FrozenLake. We compare full ERL with two variants: (1) no memory, which disables cross-episode reflection reuse, and (2) no reflection, which replaces structured self-reflection with raw first-attempt context and a generic retry instruction.",
                "position": 463
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AFull Algorithm and Gated Reflection",
        "images": []
    },
    {
        "header": "Appendix BEnvrionment Configuration Details",
        "images": []
    },
    {
        "header": "Appendix CTraining Configuration Details",
        "images": []
    }
]