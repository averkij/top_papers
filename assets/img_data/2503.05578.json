[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05578/x1.png",
                "caption": "Figure 1:Comparison of manual reference view-based novel object 6D pose estimation methods.(a)Dense reference views-based methods typically rely on \\scriptsize{1}⃝: 3D object reconstruction or \\scriptsize{2}⃝: template matching, which is time- and storage-consuming.(b)The proposed SinRef-6D estimates novel object pose using only a single reference view, providing enhanced efficiency and scalability.",
                "position": 72
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x2.png",
                "caption": "Figure 2:Our proposed SinRef-6D framework. Given a normal RGB-D reference view of a novel object, we aim to predict its 6D pose from any query view. SinRef-6D comprises four modules: (A) RGB-D images from the reference and query views are segmented, and the segmented depth maps are back-projected into point clouds. (B) The corresponding point clouds from the reference and query views are focalized from the object coordinate system to the camera coordinate system. (C) Leveraging the proposed Points and RGB SSMs (details are shown in Fig.3and Fig.4), features are extracted from the focalized point clouds and RGB images, forming point-wise reference and query features. (D) These features are then used to establish point-wise alignment to solve the object pose. Finally, the computed pose is fed back into module (B) to iteratively improve the accuracy of the point-wise alignment, yielding a more precise object pose.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05578/x3.png",
                "caption": "Figure 3:Detailed architecture of the proposed Points SSM.",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x4.png",
                "caption": "Figure 4:Detailed architecture of the proposed RGB SSM.",
                "position": 175
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05578/x5.png",
                "caption": "Figure 5:The qualitative comparison results on the LineMod dataset[28]are presented, visualizing the outputs of Gen6D[47], our SinRef-6D, and ground truth from top to bottom.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x6.png",
                "caption": "Figure 6:The qualitative comparison results on the LM-O[1], TUD-L[31], IC-BIN[14], and YCB-V[77]datasets. We visualize the results of MegaPose[37], ZeroPose[6], our SinRef-6D, and ground truth from top to bottom.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x7.png",
                "caption": "Figure 7:Some failure cases. The top row shows a single reference view (randomly selected RGB-D image as described in Sec.3.1), and the bottom row displays the estimated object pose in a query view. As observed, the accuracy of SinRef-6D decreases when the query view is a top-down view or the object is a reflective metal.",
                "position": 1026
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05578/x8.png",
                "caption": "",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x9.png",
                "caption": "",
                "position": 2602
            }
        ]
    },
    {
        "header": "Appendix AReal-world Deployment",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.05578/x10.png",
                "caption": "Figure 10:Qualitative comparison results on the LM-O[1]and TUD-L[31]datasets.",
                "position": 2624
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x11.png",
                "caption": "Figure 11:Qualitative comparison results on the IC-BIN[14]and YCB-V[77]datasets.",
                "position": 2627
            },
            {
                "img": "https://arxiv.org/html/2503.05578/x12.png",
                "caption": "Figure 12:Qualitative comparison results on the LineMod dataset[28].",
                "position": 2630
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Comparison",
        "images": []
    }
]