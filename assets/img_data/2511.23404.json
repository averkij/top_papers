[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23404/images/logo_hf.png",
                "caption": "",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2511.23404/x1.png",
                "caption": "Fig.1:The LFM2 Portfolio. (A) We present a family of Liquid Foundation Models (LFMs) across a suite of scales, modalities, and edge capabilities. (B) The taxonomy of the LFM2 portfolio steps from co-designed architectures and pre-trained base models, to multi-modality, and specialized downstream tasks.",
                "position": 311
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23404/x2.png",
                "caption": "Fig.2:LFM2 architecture.The LFM2 architecture supports both dense and mixture-of-experts (MoE) variants. Note the MoE experts are also SwiGLU blocks.",
                "position": 364
            }
        ]
    },
    {
        "header": "3Pre-Training",
        "images": []
    },
    {
        "header": "4Post-Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23404/x3.png",
                "caption": "Fig.4:Pareto frontier of Average Evaluation Score vs prefill (left) and decode (right) throughput.LFM2 models dominate the Pareto frontier. Each point corresponds to a single model configuration profiled on the Samsung S25 device (Section2.4). We chart prefill throughput (tok/s) when processing prompts of length 4k tokens, and decode throughput (tok/s) with a 4k-token prefix. Average Eval Score is computed as the unweighted mean of all evaluation scores reported in Table6and Table7.",
                "position": 1832
            }
        ]
    },
    {
        "header": "5Vision-Language LFM2",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23404/x4.png",
                "caption": "Fig.5:LFM2-VL model architecture. A SigLIP2 image encoder processes images either at native resolution for small inputs or with dynamic tiling for high-resolution inputs. A lightweight connector (PixelUnshuffle + MLP) reduces the number of vision tokens and projects visual embeddings into the LFM2 language token space, enabling unified multimodal processing.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2511.23404/x5.png",
                "caption": "Fig.6:Accuracy of LFM2-VL on selected benchmarks under varying vision token budgets.We vary the maximum number of vision tokens per image at inference by adjusting the preprocessing pipeline (downsizing single-frame inputs or limiting the number of tiles for high-resolution images), using the token budget as a proxy for on-device latency. Across multimodal reasoning, general vision understanding, and high-resolution perception benchmarks, LFM2-VL retains strong performance under compression, with performance degrading gracefully, especially on high-resolution inputs.",
                "position": 2281
            }
        ]
    },
    {
        "header": "6LFM2-Audio",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.23404/x6.png",
                "caption": "Fig.7:LFM2-Audio model architecture. (A) The LFM2 backbone operates on a single sequence of embeddings that can interleave text and audio tokens, producing next-token predictions.\n(B) The audio generation pipeline. Continuous LFM2 embeddings are turned into discrete code sequences, which are subsequently turned into a waveform by a Fourier based GAN generator.",
                "position": 2306
            }
        ]
    },
    {
        "header": "7LFM2-ColBERT",
        "images": []
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "10Authors",
        "images": []
    },
    {
        "header": "Appendix ADecoupled Top-K Knowledge Distillation",
        "images": []
    },
    {
        "header": "Appendix BModel Merging Techniques",
        "images": []
    },
    {
        "header": "Appendix CEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix DMultilingual Vision Evaluations",
        "images": []
    }
]