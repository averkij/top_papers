[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01753/pics/compare_results.png",
                "caption": "Figure 1:ObjEmbed achieves balanced and superior performance across a wide span of benchmarks.",
                "position": 89
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01753/x1.png",
                "caption": "Figure 2:The architecture of ObjEmbed. ObjEmbed is a single-tower model built upon a large multimodal language model, enhanced with an object projector and five special tokens (⟨\\langleobject⟩\\rangle,⟨\\langleiou⟩\\rangle,⟨\\langleglobal⟩\\rangle,⟨\\langlelocal_text⟩\\rangle, and⟨\\langleglobal_text⟩\\rangle) whose hidden states from the last layer are used as embeddings. ObjEmbed encodes all object embeddings, IoU embeddings, and the global image embeddings in a single forward pass. The visual embeddings and the text embeddings share the same LLM encoder. For detected objects, object embeddings are initialized from RoI features. And the final matching score is computed as the product of the predicted IoU score (predicted from IoU embeddings) and the classification score (predicted from object embeddings and local text embeddings).",
                "position": 110
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Task Instructions",
        "images": []
    },
    {
        "header": "Appendix BDetails of Dataset Annotation",
        "images": []
    },
    {
        "header": "Appendix CDetails of Local Image Retrieval Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DLimitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01753/x2.png",
                "caption": "Figure 3:Visualizations of referring expression comprehension results with text queries and image queries.",
                "position": 2682
            },
            {
                "img": "https://arxiv.org/html/2602.01753/x3.png",
                "caption": "Figure 4:Visualizations of retrieval results on SORCE-1K. Our ObjEmbed successfully ranks the target image as the top result and accurately localizes the target objects (highlighted with red bounding boxes). In contrast, global image embedding models, like Qwen3-VL-Embedding 8B, tend to overlook small objects.",
                "position": 2685
            },
            {
                "img": "https://arxiv.org/html/2602.01753/x4.png",
                "caption": "Figure 5:Visualizations of self-annotated data. Each image is annotated with high-quality image-level and object-level captions. Images come from SA-1B(Kirillovet al.,2023).",
                "position": 2691
            }
        ]
    },
    {
        "header": "Appendix EVisualization",
        "images": []
    }
]