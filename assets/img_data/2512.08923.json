[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08923/x1.png",
                "caption": "Figure 1:Summary of our work.Left:OurRESTbenchmark measures whether MLLMs can consistently reason over identical information across modalities. We first verify text recognition (OCR) capability, then evaluate the same question in three modalities (text, image, mixed). Cross-modal inconsistency occurs when models produce different answers depending on the input format.Center:RER consistency score measures the degree to which a model outputs the same answer in all modalities. We evaluate 15 MLLMs (on OCR correct, sorted onREST) and find that the degree of cross-modal inconsistency varies substantially across models even when controlling for OCR.Right:Matching samples (i.e., different modalities containing the same information) show higher cosine similarity than non-matching ones, and the extent of this difference correlates with the consistency score on our benchmark.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Our Benchmarks: REST and REST+",
        "images": []
    },
    {
        "header": "3Benchmarking Methodology",
        "images": []
    },
    {
        "header": "4Benchmarking Results: REST",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08923/x2.png",
                "caption": "Figure 2:Cross-modal inconsistency leaves model potential untapped.This figure shows the cumulative distribution of correctly solved questions across sets of modalities (OCR-correct subset).\nFrom left to right, the bars represent: the percentage of questions that can be solved in all three modalities, followed by including questions that can only be solved in fewer modalities, ending with the Max Modal Coverage (green), which shows the percentage of questions that can be solved in at least one modality.\nResults are shown for GSM8k-Symbolic, which contains open-ended questions that cannot be solved by guessing.",
                "position": 514
            }
        ]
    },
    {
        "header": "5Benchmarking Results: REST+",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08923/figures/rest_plus_vision_tokens_consistency.png",
                "caption": "Figure 3:Models generally achieve higher text accuracy despite using fewer text tokens.Current MLLMs need more vision tokens than text tokens to achieve the same accuracy, except for Qwen2.5-VL-32B, where fewer vision tokens obtain higher accuracy (OCR-correct subset).",
                "position": 1343
            },
            {
                "img": "https://arxiv.org/html/2512.08923/figures/rest_plus_color_performance_dot.png",
                "caption": "Figure 4:Colored text makes models perform better.Relative improvements from either red or yellow text compared to black (OCR-correct subset).",
                "position": 1359
            }
        ]
    },
    {
        "header": "6RQ4: Multimodal Representation Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08923/figures/interpretability/0_image.png",
                "caption": "(a)Image",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2512.08923/figures/interpretability/0_image.png",
                "caption": "(a)Image",
                "position": 1386
            },
            {
                "img": "https://arxiv.org/html/2512.08923/figures/interpretability/0_text.png",
                "caption": "(b)Written-down",
                "position": 1392
            },
            {
                "img": "https://arxiv.org/html/2512.08923/figures/interpretability/sim_scores.png",
                "caption": "Figure 6:Benchmark performance is correlated to the similarity of modalities.The similarity between image vs. word (a) and written-down vs. word (b) representations correlates with RER (as determined using ourRESTbenchmark).R2R^{2}denotes the variance explained by the fitted line, and the grey area shows the bootstrapped 95% confidence interval.",
                "position": 1406
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Discussion and Conclusion",
        "images": []
    },
    {
        "header": "9Benchmark Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/ARC/32_image_partial.jpg",
                "caption": "(a)ARC - Mixed modality",
                "position": 1502
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/ARC/32_image_partial.jpg",
                "caption": "(a)ARC - Mixed modality",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/ARC/32_image_full.jpg",
                "caption": "(b)ARC — Image modality",
                "position": 1511
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/GSM8K_Sym/151_image_partial.jpg",
                "caption": "(c)GSM8K — Mixed modality",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/GSM8K_Sym/151_image_full.jpg",
                "caption": "(d)GSM8K — Image modality",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/MMLU/191_image_partial.jpg",
                "caption": "(e)MMLU — Mixed modality",
                "position": 1531
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/MMLU/191_image_full.jpg",
                "caption": "(f)MMLU — Image modality",
                "position": 1537
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/50_dejavu_sans_black.jpg",
                "caption": "DejaVu Sans @ 50 DPI",
                "position": 1647
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/50_dejavu_sans_black.jpg",
                "caption": "DejaVu Sans @ 50 DPI",
                "position": 1650
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/100_dejavu_sans_black.jpg",
                "caption": "DejaVu Sans @ 100 DPI",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/200_dejavu_sans_magenta.jpg",
                "caption": "DejaVu Sans @ 200 DPI (Magenta & Black)",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/50_courier_new_black.jpg",
                "caption": "Courier New @ 50 DPI",
                "position": 1669
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/100_courier_new_black.jpg",
                "caption": "Courier New @ 100 DPI",
                "position": 1675
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/200_courier_new_black.jpg",
                "caption": "Courier New @ 200 DPI",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/50_cursive_black.jpg",
                "caption": "Cursive @ 50 DPI",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/100_cursive_black.jpg",
                "caption": "Cursive @ 100 DPI",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/REST_plus/200_cursive_black.jpg",
                "caption": "Cursive @ 200 DPI (Magenta)",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_3_001_image_partial.png",
                "caption": "(a)3 Variables - Mixed modality",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_3_001_image_partial.png",
                "caption": "(a)3 Variables - Mixed modality",
                "position": 1728
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_4_009_image_partial.png",
                "caption": "(b)4 Variables - Mixed modality",
                "position": 1734
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_5_012_image_partial.png",
                "caption": "(c)5 Variables - Mixed modality",
                "position": 1740
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_3_001_image_full.png",
                "caption": "(d)3 Variables - Image modality",
                "position": 1747
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_4_009_image_full.png",
                "caption": "(e)4 Variables - Image modality",
                "position": 1753
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/SoEBench/puzzle_5_012_image_full.png",
                "caption": "(f)5 Variables - Image modality",
                "position": 1759
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/stair_case_1.png",
                "caption": "Figure 12:Cumulative distribution of correctly solved questions across modality combinations for models 1-8. Each step represents questions solvable in progressively fewer modalities, with the leftmost portion showing questions solved consistently across all three modalities.",
                "position": 3769
            },
            {
                "img": "https://arxiv.org/html/2512.08923/x3.png",
                "caption": "Figure 13:Cumulative distribution of correctly solved questions across modality combinations for models 9-15. Models with higher cross-modal consistency show larger proportions of questions solved in all modalities (leftmost region).",
                "position": 3772
            },
            {
                "img": "https://arxiv.org/html/2512.08923/app_sections/figures/mmmu_correlation/mmmu_inset.png",
                "caption": "Figure 14:Models that perform well on MMMU also score well on REST and REST+. The zoom inset shows that models with high MMMU scores do not generally obtain high REST+ scores.",
                "position": 5446
            }
        ]
    },
    {
        "header": "10Extended Results",
        "images": []
    }
]