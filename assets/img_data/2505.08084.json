[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08084/x1.png",
                "caption": "",
                "position": 73
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08084/x2.png",
                "caption": "Figure 2:Overview of VISATR.VISTAR uses an LLM to generate faithful SoTs via in-context learning. Given the input including the query, ground truth, scene graph in an image and sub-task operation sequence inside the dataset (top), LLaMA-3.1-70B-Instruct would output a SoT to answer the query. The generated SoT is then used to fine-tune an MLLM, enabling it to produce both visual (object-level bounding boxes) and textual explanations during inference (bottom).",
                "position": 162
            }
        ]
    },
    {
        "header": "4GQA-SoT",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08084/x3.png",
                "caption": "Figure 3:Quantitative results of visual explanations.(a) Comparison of answer correctness and object localization accuracy (measured by IoU when the answer is correct). (b-d) Precision & recall at different IoU thresholds (0.5, 0.75, 0.95). VISTAR consistently outperforms NVILA-8B in both accuracy and object-level visual grounding, demonstrating improved interpretability and localization quality.",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2505.08084/x4.png",
                "caption": "Figure 4:Examples of successful and failed cases on human evaluation.We present some successful and failed cases for ourSoTprediction based on VISTAR.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2505.08084/x5.png",
                "caption": "Figure 5:Consistency evaluation between answers and reasoning steps.(a) Ablation results without intermediate answer supervision, evaluated byGPT-4-turboover operation and arguments with answers (b) Results from the full training procedure, evaluated byGPT-4-turboover operation and arguments with answers (c) Human evaluation results, assessing the consistency of predicted sub-tasks with their arguments and intermediate results relative to the final answers. Notably, ‘T’ in the figure means true, ’F’ means false and ‘TT’ indicates both the sub-task and intermediate answer are correct while ‘TF’, ‘FT’, and ‘FF’ represent cases where either the sub-task, intermediate answer, or both are incorrect.",
                "position": 534
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "AExamples Outputs of Subtask-of-Thought",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08084/x6.png",
                "caption": "Figure 6:Examples of LLM-generated Subtask-of-Thought",
                "position": 1124
            }
        ]
    },
    {
        "header": "BPrompt For Subtask-of-Thought Generation",
        "images": []
    },
    {
        "header": "CSub-task Operation Definition",
        "images": []
    },
    {
        "header": "DPrompt for the output of bounding boxes",
        "images": []
    },
    {
        "header": "EExample of converting sub-task operations into subtask-of-thought",
        "images": []
    }
]