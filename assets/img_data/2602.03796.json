[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03796/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03796/x2.png",
                "caption": "Figure 2:Overview of 3DiMo.Our framework consists of end-to-end trained motion encoders‚Äî‚Ñ∞b\\mathcal{E}_{b}for the body and‚Ñ∞h\\mathcal{E}_{h}for hands‚Äîand an DiT-based video generator.\nGiven a reference frameùë∞R\\bm{I}_{R}and a driving videoùëΩD\\bm{V}_{D}, driving frames are first augmented with random perspective transformations before being encoded by the motion encoder to extract view-agnostic motion representations.\nThese resulting features are then injected into the generator through cross-attention, enabling the model to synthesize a target sequenceùëΩtgt\\bm{V}_{\\mathrm{tgt}}that reenacts the same underlying 3D motion while preserving flexible text-driven camera control.\nTo facilitate 3D-aware learning, we introduce early-stage auxiliary geometric supervision by regressing the encoded motion to external parametric reconstruction resultsŒ∏b\\theta_{b}andŒ∏h\\theta_{h}.\nDuring training, view-rich data is used to jointly supervise same-view reconstruction and cross-view motion reproduction, driving the emergence of expressive and 3D-aware motion representations.\nAt inference, motion tokens extracted directly from 2D driving frames provide rich 3D spatial cues that can animate any reference character, supporting high-fidelity and view-adaptive motion-controlled video generation.",
                "position": 173
            }
        ]
    },
    {
        "header": "3Our Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03796/x3.png",
                "caption": "Figure 3:Our collected view-rich datasetcombines internet videos, UE renderings, and in-house captures, covering camera categories including single-view, multi-view, and camera-motion sequences.\nHigh-quality large-scale single-view footage exposes the model to diverse human motions, while complementary multi-view data provides consistent cross-view observations that are crucial for learning genuine 3D-aware motion representations.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03796/x4.png",
                "caption": "Figure 4:Visualization comparisons with baselines.Red and yellow bounding boxes highlight depth ambiguities and inaccurate poses, respectively.\n‚ÄúA.A.‚Äù denotes AnimateAnyone.\nOur method produces accurate and 3D-plausible motion reenactment videos.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2602.03796/x5.png",
                "caption": "Figure 5:Visualizations of ablation results.Using SMPL poses as motion representation introduces typical depth ambiguity errors.\nRemoving any view-rich data supervision impairs camera control.\nRemoving auxiliary geometric supervision or using channel concatenation causes training instability and quality degradation.\nWithout the hand encoder, fine-grained hand motions are lost.",
                "position": 484
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.03796/x6.png",
                "caption": "Figure S1:Broader applications of 3DiMo.We demonstrate the versatility of our framework on three downstream tasks: (a) single-image novel view synthesis by enforcing static motion; (b) video stabilization by suppressing camera jitter from the driving video; and (c) automatic motion-appearance alignment without explicit calibration.",
                "position": 1103
            }
        ]
    },
    {
        "header": "AEthical Considerations",
        "images": []
    },
    {
        "header": "BBroader Applications",
        "images": []
    },
    {
        "header": "CIn-House Data Acquisition Setup",
        "images": []
    },
    {
        "header": "DLimitations and Future Work",
        "images": []
    },
    {
        "header": "EImplementation Details",
        "images": []
    }
]