[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13515/x1.png",
                "caption": "Figure 1:Qualitative examples of text-to-video generation.\nWe compare the original full-attention model with SpargeAttention2 under high attention sparsity.\nSpargeAttention2 preserves visual quality, temporal coherence, and textâ€“video alignment comparable to full attention,\nwhile substantially reducing attention computation.\nThe prompts used for generation is in AppendixB",
                "position": 115
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13515/x2.png",
                "caption": "(a)A uniformPP. We keep the largest probabilities whose sum reaches60%60\\%in each row.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2602.13515/x2.png",
                "caption": "(a)A uniformPP. We keep the largest probabilities whose sum reaches60%60\\%in each row.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2602.13515/x3.png",
                "caption": "(b)A skewedPP, where we keep the largest probabilities whose sum reaches60%60\\%in each row.",
                "position": 372
            },
            {
                "img": "https://arxiv.org/html/2602.13515/x4.png",
                "caption": "(a)APPbefore fine-tuning using sparse attention. Each row keeps the largest probabilities whose sum reaches60%60\\%.",
                "position": 435
            },
            {
                "img": "https://arxiv.org/html/2602.13515/x4.png",
                "caption": "(a)APPbefore fine-tuning using sparse attention. Each row keeps the largest probabilities whose sum reaches60%60\\%.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2602.13515/x5.png",
                "caption": "(b)A sparserPPafter fine-tuning using sparse attention. Each row keeps the probabilities whose sum reaches60%60\\%.",
                "position": 444
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.13515/x6.png",
                "caption": "Figure 4:A representative example of text-to-video generation under high attention sparsity,\nevaluated on Wan2.1-14B at 720p.\nSpargeAttention2 produces a semantically correct video.\nIn contrast, SLA and VSA produce videos in which the male character walks backward,\nwhile VMoBA fails to generate the female character specified in the prompt.\nThe prompt used for generation is in AppendixB",
                "position": 969
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyper-parameters",
        "images": []
    },
    {
        "header": "Appendix BPrompts for Qualitative Visualizations",
        "images": []
    }
]