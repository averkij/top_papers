[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14990/x1.png",
                "caption": "Figure 1:A user asks the same question, \"What is the tradition of one family giving another family money during marriage called?\", in English, Hindi, and Arabic. Because each language is associated with a different cultural context, the model returns distinct concepts: Dahej in Hindi, where the bride‚Äôs family gives wealth to the groom‚Äôs family, and Mahr in Arabic, where the groom or his family gives wealth to the bride. This showcases examples of LSK, where culturally grounded responses emerge based on the language of the query.",
                "position": 86
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LSKExtractorMethodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14990/x2.png",
                "caption": "Figure 2:Overview ofLSKExtractor. Our method consists of two main steps. In Step 1, we embed training queries into a shared semantic space and cluster them based on topical similarity. For each cluster, we determine the expert language‚Äîi.e., the language that yields the most accurate or contextually appropriate reasoning‚Äîby comparing model responses across languages. In Step 2, during test-time inference, we embed the test query into the same space, identify its nearest cluster, and select the corresponding expert language (e.g., Spanish) to guide the model toward producing a more informed and culturally grounded response.",
                "position": 140
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14990/extracted/6460575/figures/culture_atlas.png",
                "caption": "Figure 3:Classification accuracy on theCultureAtlasdataset. On average,LSKExtractorimproves classification accuracy on the best-performing baseline by 18.20%.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2505.14990/extracted/6460575/figures/blend.png",
                "caption": "Figure 4:Classification accuracy on theBLEnDdataset. On average,LSKExtractorimproves classification accuracy on the best-performing baseline by 8.13%.",
                "position": 259
            },
            {
                "img": "https://arxiv.org/html/2505.14990/extracted/6460575/figures/socialiqa.png",
                "caption": "Figure 5:Classification accuracy on theSocial IQadataset. On average,LSKExtractorimproves classification accuracy on the best-performing baseline by 4.91%.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2505.14990/extracted/6460575/figures/culture_language_per_cluster.png",
                "caption": "Figure 6:The distribution of expert languages per cluster for the CultureAtlas dataset. Each bar represents a model (name on the x-axis). The y-axis represents the number of clusters with the corresponding expert language. Similar figures for BLEnD and Social IQa are in AppendixB.",
                "position": 291
            }
        ]
    },
    {
        "header": "5Ablation Experiments: forced multilingual CoT",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14990/x3.png",
                "caption": "Figure 7:LSK in(a)Gemma-3-1B,(b)Phi-3-small, and(c)Aya-23-8B, respectively, on theCultureAtlasdataset, broken down by countries. The x-axis contains the languages in which CoT reasoning was performed. The y-axis outlines the countries where the cultural norms come from. Each square represents the performance of a language model using CoT reasoning with a specific language on set of cultural norms from a specific country.Note: Darker red colors indicate improvement over the \"No Reasoning\" baseline. Two cells in the same row are comparable and inform which language is better for reasoning for a given country.",
                "position": 352
            },
            {
                "img": "https://arxiv.org/html/2505.14990/x4.png",
                "caption": "Figure 8:LSK in(a)Gemma-3-1B,(b)Phi-3-small, and(c)Aya-23-8B, respectively, on theBLEnDdataset, broken down by countries. The x- and y-axis definitions follow those in Figure7.",
                "position": 355
            },
            {
                "img": "https://arxiv.org/html/2505.14990/x5.png",
                "caption": "Figure 9:LSK in Gemma-3-1B, Phi-3-small, and Aya-23-8B, respectively, on theCultureAtlasdataset, broken down bykùëòkitalic_k-means clusters. The x-axis contains the languages in which CoT reasoning was performed.",
                "position": 393
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Specifications",
        "images": []
    },
    {
        "header": "Appendix BRemaining Language-Cluster Figures",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.14990/extracted/6460575/figures/blend_language_per_cluster.png",
                "caption": "Figure 10:The distribution of expert languages per cluster for the BLEnD dataset.",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2505.14990/extracted/6460575/figures/social_language_per_cluster.png",
                "caption": "Figure 11:The distribution of expert languages per cluster for the Social IQa dataset.",
                "position": 817
            }
        ]
    },
    {
        "header": "Appendix CModel Prompts",
        "images": []
    },
    {
        "header": "Appendix DLicenses",
        "images": []
    }
]