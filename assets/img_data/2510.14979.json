[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14979/figures/Gallery_Icon.png",
                "caption": "",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14979/x1.png",
                "caption": "Figure 1:Overview of our native vision-language frameworks, which project arbitrary-resolution images into a continuous latent space, integrating the virtues of modular VLM architectures and enabling efficient vision-language encoding, alignment, and interaction in an early-fusion manner.",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2510.14979/x2.png",
                "caption": "Figure 2:Overview of our proposed NEO architecture. We begin with lightweight patch and word embedding layers that encode images and text into token sequences, which are subsequently processed by a monolithic decoder-only architecture. The pre-Buffer and post-LLM components, each stacked with multiple native primitives, facilitate efficient and precise pixel–word alignment and reasoning.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14979/x3.png",
                "caption": "Figure 3:Overview of our native primitive, which integrates native attention with bi-directional dependencies within images and word / frame-wise causal interactions, together with native rotary position embeddings parameterized by modality-specific frequency, channel, and index allocation. It is inherently unified and intrinsically multimodal, substantially enhancing pixel–word correspondence.",
                "position": 169
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14979/x4.png",
                "caption": "Figure 4:Overview of the entire training recipe. During pre-training, NEO learns visual perception from massive web-scale and synthetic image-caption pairs with frozen LLM weights to preserve linguistic knowledge. In mid-training and supervised fine-tuning, the full model is progressively optimized end-to-end using caption, conversation, OCR, detection, and high-quality instruction data.",
                "position": 252
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14979/x5.png",
                "caption": "Figure 5:Configurations of pre-Buffer.",
                "position": 1024
            },
            {
                "img": "https://arxiv.org/html/2510.14979/x6.png",
                "caption": "Figure 6:Comparison with pre-Buffer and vision encoders. All models are initialized post-LLM using Qwen3-1.7B.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2510.14979/x6.png",
                "caption": "Figure 6:Comparison with pre-Buffer and vision encoders. All models are initialized post-LLM using Qwen3-1.7B.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2510.14979/x7.png",
                "caption": "Figure 7:Evaluation results across three progressive training procedures.",
                "position": 1208
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]