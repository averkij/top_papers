[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13685/x1.png",
                "caption": "Figure 1:Framework of MoM.Each input token selectively activates and updatesKùêæKitalic_Kmemory states, leaving non-activated memory states unchanged to avoid interference from current input. Additionally, we introduce a continuously activated shared memory. This figure presents the basic memory update mechanism; other mechanisms involving gating or more complex updates follow a similar approach.",
                "position": 249
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13685/x2.png",
                "caption": "Table 2:Results on Recall-Intensive Tasks.All inputs are truncated to a maximum length of 2K tokens. MoM significantly outperforms all other linear models across both model sizes. In the 1.3B model, MoM even achieves performance very close to that of Transformer models.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2502.13685/x2.png",
                "caption": "Table 3:Results on Common-Sense Reasoning Tasks.The performance of linear models and Transformer models is comparable; however, MoM consistently achieves the best average performance across all model sizes.",
                "position": 681
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13685/x2.png",
                "caption": "Figure 2:Efficiency of MoM.We demonstrate the inference time and GPU memory consumption required to generate 1K tokens at specific sequence lengths.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2502.13685/extracted/6216874/figs/loss.png",
                "caption": "Figure 3:Training Loss.Loss curves for training 340M models on 15B tokens with a fixed random seed of 42.",
                "position": 1179
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AComparison between MoM and MoE",
        "images": []
    },
    {
        "header": "Appendix BDataset and Benchmark",
        "images": []
    },
    {
        "header": "Appendix CExperiments Details",
        "images": []
    }
]