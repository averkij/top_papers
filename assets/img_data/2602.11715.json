[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11715/DICE/a_figures/dice.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11715/x1.png",
                "caption": "Figure 1:Overview ofDICE. The framework enhances CUDA kernel generation robustness in dLLMs by leveraging TraceRL. This hierarchical approach integrates: (1)Bi-phase Curated Reinforcement Learning framework, a progressive RL training strategy that consists of kernel infilling and end-to-end kernel generation stages to ensure functional correctness and high performance of generated CUDA kernels, and (2)Data Scheduling, transitioning training data from basic single operations to complex whole-model structures during the two RL stages. A valid reward will only be returned when the generated CUDA kernel can be compiled and functions correctly.",
                "position": 119
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11715/x2.png",
                "caption": "Figure 2:The inference paradigm of diffusion large language models.Left Part: The sequence is divided into several blocks, where the block length equals four in this figure. The block diffusion mechanism enables models to generate autoregressively between blocks, while parallel discrete decoding within blocks. All the KV cache from previous blocks will be reused.Right Part: An actual step-by-step generation trajectory for an example CUDA kernel. While the overall trend remains autoregressive, we can clearly observe lots of non-autoregressive behavior during the generation process.",
                "position": 169
            }
        ]
    },
    {
        "header": "3CuKe Dataset Construction",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11715/x3.png",
                "caption": "Figure 3:Our defined CUDA kernel components: the prefix, the suffix, and the core implementation, which is a C++ snippet.",
                "position": 230
            }
        ]
    },
    {
        "header": "5Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.11715/x4.png",
                "caption": "Figure 4:RL training trajectory comparison of BiC-RL framework and baseline RL on 8B model.",
                "position": 1445
            },
            {
                "img": "https://arxiv.org/html/2602.11715/x5.png",
                "caption": "Figure 5:Comparison of correctness trends for BiC-RL and baseline RL of 8B model on KernelBench Level 2.",
                "position": 1452
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AOne-shot Prompt Templates",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CResults Comparison",
        "images": []
    },
    {
        "header": "Appendix DCase Study of Deceptive Behaviour",
        "images": []
    }
]