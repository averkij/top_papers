[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.18058/x1.png",
                "caption": "Figure 1:Frontier LLMs can sacrifice honesty instead of refusing and this distorts evaluation.When given jailbreak prompts, some models choose to not refuse and instead produce harmful-sounding but deliberately flawed instructions. Current output-based LLM scorers misclassify these responses as successful jailbreaks, inflating success rates. In contrast, white-box methods (e.g., linear probes) can identify this strategic deception. The example above shows that QwQ-32B lists chemicals and describes how they can be synthesized, although neither Neuroxin-9 nor Cyanide-12 exist.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x2.png",
                "caption": "Figure 2:An MCQ-based design to study how models balance values.When facing a harmful query, a model cannot satisfy all three HHH values. We prompt it to resolve the conflict by sacrificing one value. In-context examples demonstrate each strategy. The structured response format allows us evaluate adherence to the chosen strategy.",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x3.png",
                "caption": "Figure 3:Many frontier LLMs sacrifice honesty.For each model we report the share of MCQ examples in which it sacrifices (a) honesty, (b) helpfulness, or (c) harmlessness. Many frontier LLMs show a strong propensity forstrategic dishonestyby producing harmful-looking jailbreak responses instead of refusing. Harmful queries sourced from ClearHarm; all results are averaged over five seeds.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x4.png",
                "caption": "Figure 4:Quality of fake harmful outputs scales with capability and tricks LLM judges.Left:We score fake harmful outputs from >80 models with popular LLM judges and plot normalized scores (scaled to[0,1][0,1]) against general capability (LLM-Arena score). For readability, points show averages within bins of 50 arena scores.Right:We illustrate \"harmful\" scores that some models achieve as an example, with Gemini 2.5 Pro tricking all evaluated judges.",
                "position": 338
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x5.png",
                "caption": "(a)",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x5.png",
                "caption": "(a)",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x6.png",
                "caption": "(b)",
                "position": 371
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x7.png",
                "caption": "Figure 6:Steering with the deception probe direction.Left: EvilMath.Right: MCQ",
                "position": 528
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x8.png",
                "caption": "Figure 7:Strategic Deception Obfuscates Jailbreaking Objectives.We run a nanoGCG attack against Qwen3-32B-no-thinking, with either a faking (honeypot) system prompt or a safe system prompt.Left:The attack successfully elicits the target prefix from the honeypot model.Center:Unlike the safe model, the honeypot model does not provide an explicit refusal signal to the attacker.Right:However, 96% of honeypotted outputs are classified as deceptive, i.e., pseudo-harmful, whereas compliant outputs from the safe model are classified as honest, i.e., genuinely harmful.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x9.png",
                "caption": "",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x10.png",
                "caption": "Figure 8:Some LLMs Sacrifice Honesty While Others Do Not.For each model we report the proportion of MCQ examples in which it sacrifices (a) honesty, (b) helpfulness, or (c) harmlessness.\nWe additionally report the share of errors arising from format failures, MCQ refusals, and API errors (d).\nAll draft responses are also evaluated with a refusal judge; cases scored as refusals but with an MCQ selection of A or C are regraded into B.\nHarmful queries are sourced from ClearHarm, and all results are averaged over five seeds.",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x11.png",
                "caption": "Figure 9:Comparison of the Direct Evaluation to the Propensity to Fake.For each model, we report the share of MCQ examples in which it sacrifices honesty, and the share of non-refused outputs in the direct evaluation. We treat the latter as an upper bound on faking propensity.",
                "position": 2667
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x12.png",
                "caption": "Figure 10:For a model trained to deceive on harmful math, probes successfully separate cases where the model chooses to deceive from those where it decides to be honest. Probes work without an explicit instruction to deceive.",
                "position": 3383
            },
            {
                "img": "https://arxiv.org/html/2509.18058/x13.png",
                "caption": "Figure 11:For a model with an unfaithful chain-of-thought, the probe fails to separate cases where deception occurred from those where it did not. In Correct answers to EvilMath an unfaithful-CoT-model still plans to deceive, and this activates the probes.",
                "position": 3454
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]