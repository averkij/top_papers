[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04965/x1.png",
                "caption": "Figure 1:The overview ofBitNet a4.8with both weight and activation quantization. All the parameters are ternery (i.e., 1.58-bit as in BitNet b1.58[12]). We use a hybrid quantization\nand sparsification strategy to deal with outlier activations in certain Transformer sub-layers.",
                "position": 80
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2BitNet a4.8",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04965/x2.png",
                "caption": "Figure 2:The distribution of the inputs to each projection. The visualization is conducted with a 7B BitNet b1.58 model on a subset of the valid set of C4. For the layers that exhibit Gaussian-like distributions, we employ 4-bit activation quantization. For the layers which distributions are sharp, we adopt Q-Sparse[18]to perform sparsification on the activations.",
                "position": 132
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x3.png",
                "caption": "Figure 3:The distribution of the inputs to the output projection of attention with different quantization and sparsification. The visualization is conducted with a 7B BitNet b1.58 model on a subset of the valid set of C4.",
                "position": 222
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.04965/x4.png",
                "caption": "Figure 4:Ablation study on the hybrid quantization and sparsification.",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x4.png",
                "caption": "",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x5.png",
                "caption": "",
                "position": 702
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x6.png",
                "caption": "Figure 5:Ablation study on different quantization or activation function for the inputs to down projection of FFN.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x6.png",
                "caption": "",
                "position": 711
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x7.png",
                "caption": "",
                "position": 715
            },
            {
                "img": "https://arxiv.org/html/2411.04965/x8.png",
                "caption": "Figure 6:Ablations on 4-bit quantizers for the inputs to attention and FFN.",
                "position": 772
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "5Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHyper-parameters",
        "images": []
    }
]