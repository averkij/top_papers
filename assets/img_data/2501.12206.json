[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12206/extracted/6145987/llava.png",
                "caption": "Figure 1:Architecture of LLaVA-1.5",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2501.12206/x1.png",
                "caption": "Figure 2:An example of in-context hallucination in LLaVA-1.5. The responses that are not grounded in the image are highlighted in red.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2501.12206/extracted/6145987/modality-gap.png",
                "caption": "Figure 3:Visualization of the embeddings of visual tokens and text tokens in the semantic space, along with the full token vocabulary of Vicuna-7B. The figure clearly shows that the visual tokens, projected by the MLP into the text embedding space, are significantly distant from the text token embeddings, indicating a modality gap.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2501.12206/extracted/6145987/visualize-attn.png",
                "caption": "Figure 4:Attention maps during the decoding process of a model response for LLaVA-1.5-7B. The visual tokens are highlighted inred. In the shallow layers (e.g., layer 1), attention is relatively evenly distributed across both visual and text tokens. However, in the deeper layers (e.g., layer 16 and 32), attention becomes concentrated on system prompt tokens (text that is prepended before the visual tokens as part of the instruction), prompt tokens, and output tokens, while paying very little attention to the visual tokens.",
                "position": 405
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]