[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05005/x1.png",
                "caption": "Figure 1:A single, unified diffusion-based model for both generative and discriminative learning. If the model receives an RGB image as input, its function is to predict an accurate visual attribute map. Simultaneously, the model is equipped to produce photo-realistic and coherent multi-modal data sampled from Gaussian noise. We use depth as an example here for illustration, and the framework is also applicable to other visual attributes such as segmentation, surface normal,etc.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Unified Diffusion Model: Diff-2-in-1",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05005/x2.png",
                "caption": "Figure 2:Our self-improving learning paradigm with two sets of interplayed parameters during training.The data creation parameterŒ∏CsubscriptùúÉC\\theta_{\\text{C}}italic_Œ∏ start_POSTSUBSCRIPT C end_POSTSUBSCRIPTgenerates samples serving as additional training data for the data exploitation parameterŒ∏EsubscriptùúÉE\\theta_{\\text{E}}italic_Œ∏ start_POSTSUBSCRIPT E end_POSTSUBSCRIPT, whileŒ∏EsubscriptùúÉE\\theta_{\\text{E}}italic_Œ∏ start_POSTSUBSCRIPT E end_POSTSUBSCRIPTperforms discriminative learning and provides guidance to updateŒ∏CsubscriptùúÉC\\theta_{\\text{C}}italic_Œ∏ start_POSTSUBSCRIPT C end_POSTSUBSCRIPTthrough exponential moving average. Finally,Œ∏CsubscriptùúÉC\\theta_{\\text{C}}italic_Œ∏ start_POSTSUBSCRIPT C end_POSTSUBSCRIPTperforms both discriminative and generative tasks during inference.",
                "position": 206
            }
        ]
    },
    {
        "header": "4Learning Mechanism of Diff-2-in-1",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05005/x3.png",
                "caption": "Figure 3:Real data samples from NYUv2 and synthesized samples generated from Gaussian noise. The distribution of the generated data varies from the real data distribution.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x3.png",
                "caption": "Figure 3:Real data samples from NYUv2 and synthesized samples generated from Gaussian noise. The distribution of the generated data varies from the real data distribution.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x4.png",
                "caption": "Figure 4:In-distribution data generation using partial noise. We generate in-distribution data by denoising from a noisy image at timestepTùëáTitalic_Twith0<T<Tmax0ùëásubscriptùëámax0<T<T_{\\mathrm{max}}0 < italic_T < italic_T start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT. A largerTùëáTitalic_Tleads to greater diversity, whereas a smallerTùëáTitalic_Tenhances the resemblance to the original distribution.",
                "position": 261
            }
        ]
    },
    {
        "header": "5Experimental Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05005/x5.png",
                "caption": "Figure 5:Ablation study on different data settings with our Diff-2-in-1.Green line: Performance of the baseline VPD.Yellow line: Performance with our Diff-2-in-1.Gray bars: Improvement in each data setting. Our Diff-2-in-1 could consistently bring performance gain for all different data settings with more benefits in mid-range data settings.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x5.png",
                "caption": "Figure 5:Ablation study on different data settings with our Diff-2-in-1.Green line: Performance of the baseline VPD.Yellow line: Performance with our Diff-2-in-1.Gray bars: Improvement in each data setting. Our Diff-2-in-1 could consistently bring performance gain for all different data settings with more benefits in mid-range data settings.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x6.png",
                "caption": "Figure 6:Multi-modal samples generated by Diff-2-in-1 on NYUD-MT(Silberman et¬†al.,2012). Our method can generate high-quality RGB images and precise multi-modal annotations, further facilitating discriminative learning via our self-improvement.",
                "position": 954
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05005/x7.png",
                "caption": "Figure A:Captions generated by ClipCap(Mokady et¬†al.,2021)and BLIP-2(Li et¬†al.,2023b)on the NYUv2(Silberman et¬†al.,2012)dataset. The generated captions using these two off-the-shelf image captioning models not only have similar semantic meanings, but also share similar text formats.",
                "position": 2400
            }
        ]
    },
    {
        "header": "Appendix BAdditional Ablation Study",
        "images": []
    },
    {
        "header": "Appendix CMore Visualizations",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.05005/x8.png",
                "caption": "Figure B:Qualitative results on the surface normal prediction task of NYUv2(Silberman et¬†al.,2012; Ladicky et¬†al.,2014). Our proposed Diff-2-in-1 outperforms the baseline with more accurate surface normal estimations, indicating that our unified diffusion-based models excel at handling discriminative tasks. The black regions in the ground truth visualizations are invalid regions.",
                "position": 2707
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x9.png",
                "caption": "Figure C:Qualitative results on the surface normal task of NYUv2(Silberman et¬†al.,2012; Ladicky et¬†al.,2014). Our proposed Diff-2-in-1 outperforms the baseline with more accurate surface normal estimations, indicating that our unified diffusion-based models excel at handling discriminative tasks. The black regions in the ground truth visualizations are invalid regions.",
                "position": 2710
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x10.png",
                "caption": "Figure D:Qualitative results on the multi-task datasets NYUD-MT(Silberman et¬†al.,2012)and PASCAL-Context(Mottaghi et¬†al.,2014). Diff-2-in-1 has superior performance compared to the baselines, demonstrating the effectiveness of our unified diffusion-based model design. Zoom in for the regions with bounding boxes to better see the comparison.",
                "position": 2713
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x11.png",
                "caption": "Figure E:Synthetic samples from our method after the Diff-2-in-1 framework is trained on the surface normal task of NYUv2(Silberman et¬†al.,2012; Ladicky et¬†al.,2014). The odd rows are the generated RGB images while the even rows are the generated surface normal maps. The model is capable of generating diverse and high-fidelity images with the corresponding surface normal maps matching the generated RGB images.",
                "position": 2716
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x12.png",
                "caption": "Figure F:Synthetic samples from our method after the Diff-2-in-1 framework is trained on the surface normal task of NYUv2(Silberman et¬†al.,2012; Ladicky et¬†al.,2014). The odd rows are the generated RGB images while the even rows are the generated surface normal maps. The model is capable of generating diverse and high-fidelity images with the corresponding surface normal maps matching the generated RGB images.",
                "position": 2719
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x13.png",
                "caption": "Figure G:Synthetic samples from our method after the Diff-2-in-1 framework is trained on the multi-task setting of NYUD-MT(Silberman et¬†al.,2012). Each batch of samples contains four rows: RGB, depth map, surface normal map, and semantic labels(from top to bottom). The generated samples are of high quality with their multi-task annotations.",
                "position": 2722
            },
            {
                "img": "https://arxiv.org/html/2411.05005/x14.png",
                "caption": "Figure H:Synthetic samples from our method after the Diff-2-in-1 framework is trained on the multi-task setting of PASCAL-Context(Mottaghi et¬†al.,2014). Each batch of samples contains five rows: RGB, semantic labels, human parsing labels, saliency map, and surface normal map(from top to bottom). If the human parsing labels are all black, it means that there is no human in the generated image. The generated samples are of high quality with their multi-task annotations.",
                "position": 2725
            }
        ]
    },
    {
        "header": "Appendix DDiscussions and Future Work",
        "images": []
    }
]