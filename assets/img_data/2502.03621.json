[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03621/x1.png",
                "caption": "Figure 1.DynVFX augments real-world videos with new dynamic content described via simple user-provided text instruction.",
                "position": 92
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03621/x2.png",
                "caption": "Figure 2.Controlling fidelity to the original scene using different extended attention mechanisms. (a-b) SDEdit suffers from the original scene preservation/edit fidelity trade-off. (c-e) Three Extended Attention variants during sampling demonstrate different control levels: Full Extended Attention closely reconstructs the input scene, Masked Extended Attention proves too constrained in overlapping regions despite allowing new content emergence, and our Anchor Extended Attn. achieves optimal results by applying dropout â€“ extending attention only at sparse points within selected regions.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x3.png",
                "caption": "Figure 3.DynVFX pipeline. Top row: Given an input videoğ’±origsubscriptğ’±orig\\mathcal{V}_{\\text{orig}}caligraphic_V start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT, we apply DDIM inversion (see Sec. 3) and extract spatiotemporal keys and values[ğŠorig,ğ•orig]subscriptğŠorigsubscriptğ•orig[\\mathbf{K}_{\\text{orig}},\\mathbf{V}_{\\text{orig}}][ bold_K start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT ]from the original noisy latents. Given the user instructionğ’«VFXsubscriptğ’«VFX\\mathcal{P}_{\\text{VFX}}caligraphic_P start_POSTSUBSCRIPT VFX end_POSTSUBSCRIPTwe instruct the VLM to envision the augmented scene and output the text edit promptğ’«compsubscriptğ’«comp\\mathcal{P}_{\\text{comp}}caligraphic_P start_POSTSUBSCRIPT comp end_POSTSUBSCRIPT, prominent object descriptionsğ’ªorigsubscriptğ’ªorig\\mathcal{O}_{\\text{orig}}caligraphic_O start_POSTSUBSCRIPT orig end_POSTSUBSCRIPTthat are used to mask out the extracted keys and values and target object descriptionsğ’ªeditsubscriptğ’ªedit\\mathcal{O}_{\\text{edit}}caligraphic_O start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT. Bottom row: We estimate a residualğ’™ressubscriptğ’™res\\bm{x}_{\\text{res}}bold_italic_x start_POSTSUBSCRIPT res end_POSTSUBSCRIPTto the original video latent (ğ’™origsubscriptğ’™orig\\bm{x}_{\\text{orig}}bold_italic_x start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT). This is done by iteratively applying SDEdit with our Anchor Extended Attention, segmenting the target objects (ğ’ªeditsubscriptğ’ªedit\\mathcal{O}_{\\text{edit}}caligraphic_O start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT) from the clean result, and updatingğ’™ressubscriptğ’™res\\bm{x}_{\\text{res}}bold_italic_x start_POSTSUBSCRIPT res end_POSTSUBSCRIPTaccordingly.",
                "position": 185
            }
        ]
    },
    {
        "header": "3.Preliminaries",
        "images": []
    },
    {
        "header": "4.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03621/x4.png",
                "caption": "Figure 4.Ablations. (b) Excluding both AnchorExtAttn and the Iterative refinement process results in significant misalignment with the original scene and poor harmonization (e.g., the size of the puppy relative to the scene and boundary artifacts). (c) Omitting AnchorExtAttn leads to incorrect positioning of the new content. (d) Removing iterative refinement results in poor harmonization. Our full method (e) exhibits good localization and harmonization of the edit",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x5.png",
                "caption": "Figure 5.Sample results of our method. See SM for full vide results.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x6.png",
                "caption": "Figure 6.Qualitative comparison. Sample results of our method, SDEdit(Meng etÂ al.,2022), DDIM inversion(Song etÂ al.,2020), Lora fine-tuning(Hu etÂ al.,2021), and Gen-3(R Team, Runway,[n.â€‰d.]). See SM for videos.",
                "position": 471
            }
        ]
    },
    {
        "header": "5.Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03621/x7.png",
                "caption": "Figure 7.Metrics. We measure CLIP Directional score (higher is better) and masked SSIM (higher is better). Our method demonstrates a better balance between these two metrics.",
                "position": 678
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x8.png",
                "caption": "Figure 8.Limitations. In some cases, the T2V diffusion model struggles to precisely follow the edit prompt",
                "position": 699
            }
        ]
    },
    {
        "header": "6.Discussion and Conclusions",
        "images": []
    },
    {
        "header": "7.Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03621/x9.png",
                "caption": "Figure 9.Additional examples for Ablations.",
                "position": 1621
            }
        ]
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BBaselines comparison details",
        "images": []
    },
    {
        "header": "Appendix CAdditional comparisons",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03621/x10.png",
                "caption": "Figure 10.Comparison to MagicVFX. The result of MagicVFX the output differs significantly from the original video.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x11.png",
                "caption": "Figure 11.Output example for protocol",
                "position": 1679
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x12.png",
                "caption": "Figure 12.VLM instructions used for generating the textual descriptions.",
                "position": 1682
            },
            {
                "img": "https://arxiv.org/html/2502.03621/x13.png",
                "caption": "Figure 13.VLM evaluation protocol",
                "position": 1685
            }
        ]
    },
    {
        "header": "Appendix DVLM Prompting",
        "images": []
    }
]