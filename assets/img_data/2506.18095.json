[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/fig_0.png",
                "caption": "Figure 1:Overview of the ShareGPT-4o-Image. The dataset comprises 91K synthetic samples fromGPT-4o-Image, capturing its advanced capabilities for bothtext-to-imageandtext-and-image-to-imagegeneration tasks. Displayed prompts are simplified.",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/fig_1.png",
                "caption": "Figure 2:Image Generation Gains from ShareGPT-4o-Image. Fine-tuning Janus-Pro with ShareGPT-4o-Image yieldsJanus-4o, which shows notable improvements in image generation. Janus-4o also supportstext-and-image-to-imagegeneration, outperforming other baselines with just 91K training samples.",
                "position": 154
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2ShareGPT-4o-Image",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18095/x1.png",
                "caption": "Figure 3:The flow diagram for the dataset construction process. The prompt in the diagram is the simplified version.",
                "position": 204
            }
        ]
    },
    {
        "header": "3Janus-4o: Fine-Tuning with ShareGPT-4o-Image",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/fig2.png",
                "caption": "Figure 4:Overview of Janus-4o model. Built upon Janus-Pro, it is constructed via fine-tuning on ShareGPT-4o-Image. It incorporates enhancements to supporttext-and-image-to-imagegeneration. Bothtext-to-imageandtext-and-image-to-imagetasks are jointly trained.",
                "position": 246
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/logo.png",
                "caption": "Table 1:Evaluation oftext-to-imagegeneration ability on GenEval benchmark.‚Ä†indicates results rigorously reproduced by us, while others are taken from the original papers.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/logo.png",
                "caption": "Table 2:Evaluation oftext-to-imagegeneration ability on DPG-Bench benchmark.‚Ä†indicates results rigorously reproduced by us, while others are reported in the respective papers.",
                "position": 534
            },
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/logo.png",
                "caption": "Table 3:Evaluation oftext-and-image-to-imagegeneration ability on ImgEdit-Bench.#Datadenotes training data size (K = thousand). Column abbreviations: Add. = Addition, Rmv. = Removement, Repl. = Replacement, Mot. = Motion Change, Style = Style Transfer, Bkg. = Background Change, Obj. = Object Extraction, Hyb. = Hybrid Edit, Avg. = Average across all edits.",
                "position": 688
            },
            {
                "img": "https://arxiv.org/html/2506.18095/x2.png",
                "caption": "Figure 5:Human evaluation of Janus-4o‚Äôs performance ontext-to-imageandtext-and-image-to-imagegeneration tasks.Winsindicate that the evaluator found Janus-4o superior in both instruction-following and image quality.Tiesmean both models performed equally well, whileLossesindicate that Janus-4o performed worse in those cases. Results were reported as preference ratios.",
                "position": 817
            }
        ]
    },
    {
        "header": "5conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BImage Generation Categories",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/t2i_stat.png",
                "caption": "Figure 6:Text-to-Image Categories distributions.",
                "position": 1823
            },
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/category_distribution_pie_chart.png",
                "caption": "Figure 7:Distribution ofText-and-Image-to-Imagecategories.",
                "position": 1920
            },
            {
                "img": "https://arxiv.org/html/2506.18095/extracted/6561742/image/exp_decay_distribution.png",
                "caption": "Figure 8:Illustration of the exponential decay distribution used for samplingkùëòkitalic_k.",
                "position": 1960
            }
        ]
    },
    {
        "header": "Appendix CPrompts for Generation",
        "images": []
    },
    {
        "header": "Appendix DDocument Pipeline",
        "images": []
    },
    {
        "header": "Appendix EEthical Considerations and Societal Impact",
        "images": []
    }
]