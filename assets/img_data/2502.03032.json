[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x1.png",
                "caption": "Figure 1:Schematic illustration of inner-layer matching. We select a feature with indexi𝑖iitalic_ion the SAE trained at the layer output. Its embedding𝐟𝐟\\mathbf{f}bold_f, which is thei𝑖iitalic_ith column of this SAE’s decoder weight, is compared to every column of other SAEs on the same layer (after the MLP and attention blocks, as well as with the SAE on the residual stream before some layer). These comparisons indicate the feature’s source. See Section3.3for more details.",
                "position": 203
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x2.png",
                "caption": "Figure 2:An illustration of the resulting flow graph, which we also use in the deactivation experiment (section5.2). As a starting point, we select the feature on the 24th-layer residual with index 14548. For a detailed explanation of this graph, see AppendixE.",
                "position": 388
            }
        ]
    },
    {
        "header": "4Experimental setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x3.png",
                "caption": "Figure 3:Example of cosine similarity vs. simultaneous activation with a predecessor (350 features were sampled per layer). “From MLP” and “From RES” groups are notably different: highs(M)superscript𝑠𝑀s^{(M)}italic_s start_POSTSUPERSCRIPT ( italic_M ) end_POSTSUPERSCRIPTand lows(R)superscript𝑠𝑅s^{(R)}italic_s start_POSTSUPERSCRIPT ( italic_R ) end_POSTSUPERSCRIPTsuggest simultaneous activation with an MLP-module match. Cosine similarity serves as a good proxy for shared semantic and mechanistic properties.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x4.png",
                "caption": "Figure 4:Percentage of statistically significant differences between groups for each module’s similarity scores. AO means moduleP𝑃Pitalic_Pis active in only one group, AB means active in both, and IB means inactive in both. For MLP, two groups differ ins(R)superscript𝑠𝑅s^{(R)}italic_s start_POSTSUPERSCRIPT ( italic_R ) end_POSTSUPERSCRIPTonly 87% of the time when MLP is active in both groups.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x5.png",
                "caption": "Figure 5:Percentages of each group at each layer of Gemma 2 2B, illustrating how feature formation proceeds in the model.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x6.png",
                "caption": "Figure 6:Deactivation methods compared. Group labels show which active predecessors were deactivated. The random approach underperforms, suggesting that choosing thetop1subscripttop1\\operatorname{top}_{1}roman_top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTfeature is already meaningful for causal analysis.",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x7.png",
                "caption": "Figure 7:Mean activation changes when deactivating one predecessor at a time. Deactivation of some predecessor causes less impact if this predecessor is not activated alone, which leads to the conclusion that combined groups exhibit circuit-like behavior.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x8.png",
                "caption": "Figure 8:Impact of differentr𝑟ritalic_rvalues on deactivation success, with rescaling of all available predecessors. Whenr<1𝑟1r<1italic_r < 1, the activation change grows nonlinearly, indicating alternative causal pathways still convey information. Relative loss change measured as(Lnew−Lold)/Loldsubscript𝐿newsubscript𝐿oldsubscript𝐿old(L_{\\text{new}}-L_{\\text{old}})/L_{\\text{old}}( italic_L start_POSTSUBSCRIPT new end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT old end_POSTSUBSCRIPT ) / italic_L start_POSTSUBSCRIPT old end_POSTSUBSCRIPTis a proxy for forward pass impact.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x9.png",
                "caption": "Figure 9:Deactivating the “Scientific concepts and entities” theme. The dashed black line shows the default generation score. Red points mark the best layer for eachr𝑟ritalic_rin the single-layer method. Largerr𝑟ritalic_rboosts performance but shifts the optimal layer earlier.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x10.png",
                "caption": "Figure 10:Comparison of best deactivation scores. The green line indicates deactivation using only the initial feature set. Multi-layer interventions (orange, blue) perform better across differentr𝑟ritalic_rvalues, suggesting additional discovered features reduce hyperparameter sensitivity.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x11.png",
                "caption": "Figure 11:Activation of specific topics. We compare single-layer steering and cumulative approaches with three rescaling strategies (AppendixB). Activating multiple similar features amplifies a topic’s presence but may degrade overall text coherence.",
                "position": 591
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Impact statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix BSteering details",
        "images": []
    },
    {
        "header": "Appendix CAdditional results for experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x12.png",
                "caption": "Figure 12:(a) Percentage of feature groups obtained for each dataset. (b) Distribution of scores for layers 8 and 18. We observe a clear distinction between groups, which additionally indicates the validity of the proposed method.",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x12.png",
                "caption": "",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x13.png",
                "caption": "",
                "position": 1472
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x14.png",
                "caption": "Figure 13:Probability of group A (row) to appear in group B (column), aggregated over all layers. For example, if we take the “From ATT” group, then with a probability of 0.45, features from this group would appear in the “From RES & ATT” group. High scores for the “From nowhere” group represent its stochasticity.",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x15.png",
                "caption": "Figure 14:Percentage of statistically significant differences between groups with respect to a certain score.",
                "position": 1481
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x16.png",
                "caption": "Figure 15:(a) Percentage of features per each method. There was a total of 13106 activated features, and for every feature, four matching strategies were applied. We see thattop5subscripttop5\\operatorname{top}_{5}roman_top start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPTmethod detects many more combined groups than other methods, especially “From RES & MLP”. (b) Probability for a feature from some groupA𝐴Aitalic_A(labeled as the subplot title) to become from groupB𝐵Bitalic_B(shown in legend) after deactivation of some predecessor. Each bar shows the percentage of times the feature falls into a new category.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x16.png",
                "caption": "",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x17.png",
                "caption": "",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x18.png",
                "caption": "Figure 16:From each flow graph, we select features on a particular layerl𝑙litalic_land perform steering with the four different strategies. Bars represent the best result for each layer among all scoress𝑠sitalic_s. In some cases, steering on a layer other than 12 may improve results.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x19.png",
                "caption": "Figure 17:(a) Amount of features selected for activation of “Research methodology and experimentation” theme. Vertical lines represent the placement of the initially selected features. (b) Results for steering of selected features. Score is a total metric measured asBehavioral×CumulativeBehavioralCumulative\\text{Behavioral}\\times\\text{Cumulative}Behavioral × Cumulative. We can see that despite none of the initial features being placed on the 5th layer, it gives us the best result.",
                "position": 1589
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x19.png",
                "caption": "",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x20.png",
                "caption": "",
                "position": 1596
            }
        ]
    },
    {
        "header": "Appendix DExperiments with Llama Scope",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x21.png",
                "caption": "Figure 18:(a) Distribution of groups for Llama Scope. We observe a clear distinction from Gemma Scope results (Figure12) due to a much smoother distribution. This may be a consequence of various factors: the architecture of the models or SAEs, the training procedure, differences in data distribution, etc. (b) Distribution of groups across multiple layers. We observe approximately the same pattern as for Gemma Scope (Figure5), indicating shared properties between the models. (c) Distribution of scores for different groups. We see that the groups are slightly less distinct from each other compared to the case of Gemma Scope (Figure12), but they are still present. This is also reflected in (d) the separability of different groups based on their cosine similarity relations.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x21.png",
                "caption": "",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x22.png",
                "caption": "",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x23.png",
                "caption": "",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x24.png",
                "caption": "",
                "position": 1638
            }
        ]
    },
    {
        "header": "Appendix EExamples of flow graphs",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x25.png",
                "caption": "Figure 19:Flow graph for the 12/res/14455 feature. As reported inChalnev et al. (2024), steering of that feature might produce themes related to fashion, and we clearly observe that our flow graph captures this semantics in the earlier layers.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x26.png",
                "caption": "Figure 20:Flow graph for the 12/res/4230 feature. In this case, we observe that the second half of the model is closely related to wedding and marriage ceremonies. We believe that the “official” aspect in the interpretation of features in earlier layers is closely related to the fact that wedding ceremonies and marriage are themselves official procedures—the registration of a specific type of interpersonal relationship.",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2502.03032/extracted/6182995/schemes/sae_to_transcoders.png",
                "caption": "Figure 21:Two SAEs with a learned transition matrixT𝑇Titalic_Tcan be seen as a transcoder from layert𝑡titalic_tto layert+1𝑡1t+1italic_t + 1.",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x27.png",
                "caption": "Figure 22:Explained variance of the various permutation variants. Cosine similarity between decoders’ vectors (𝐈x>0⁢top1⁢𝑾dec(14)⊤⁢𝑾dec(15)subscript𝐈𝑥0subscripttop1superscriptsubscript𝑾declimit-from14topsuperscriptsubscript𝑾dec15\\mathbf{I}_{x>0}\\text{ top }_{1}\\boldsymbol{W}_{\\text{dec}}^{(14)\\top}%\n\\boldsymbol{W}_{\\text{dec }}^{(15)}bold_I start_POSTSUBSCRIPT italic_x > 0 end_POSTSUBSCRIPT top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 14 ) ⊤ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 15 ) end_POSTSUPERSCRIPT) performs best. See AppendixFfor more details.",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x28.png",
                "caption": "Figure 23:Comparison of variousk𝑘kitalic_kintopksubscripttop𝑘\\operatorname{top}_{k}roman_top start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPToperator and different weights of the SAE. Cosine similarity (𝐈x>0⁢top1⁢𝑾dec(14)⊤⁢𝑾dec(15)subscript𝐈𝑥0subscripttop1superscriptsubscript𝑾declimit-from14topsuperscriptsubscript𝑾dec15\\mathbf{I}_{x>0}\\text{ top }_{1}\\boldsymbol{W}_{\\text{dec}}^{(14)\\top}%\n\\boldsymbol{W}_{\\text{dec}}^{(15)}bold_I start_POSTSUBSCRIPT italic_x > 0 end_POSTSUBSCRIPT top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 14 ) ⊤ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 15 ) end_POSTSUPERSCRIPT) performs best. See AppendixFfor more details.",
                "position": 1898
            }
        ]
    },
    {
        "header": "Appendix FSimilarity between Matching and Transcoders",
        "images": []
    }
]