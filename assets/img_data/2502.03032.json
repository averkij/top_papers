[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x1.png",
                "caption": "Figure 1:Schematic illustration of inner-layer matching. We select a feature with indexiùëñiitalic_ion the SAE trained at the layer output. Its embeddingùêüùêü\\mathbf{f}bold_f, which is theiùëñiitalic_ith column of this SAE‚Äôs decoder weight, is compared to every column of other SAEs on the same layer (after the MLP and attention blocks, as well as with the SAE on the residual stream before some layer). These comparisons indicate the feature‚Äôs source. See Section3.3for more details.",
                "position": 203
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x2.png",
                "caption": "Figure 2:An illustration of the resulting flow graph, which we also use in the deactivation experiment (section5.2). As a starting point, we select the feature on the 24th-layer residual with index 14548. For a detailed explanation of this graph, see AppendixE.",
                "position": 388
            }
        ]
    },
    {
        "header": "4Experimental setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x3.png",
                "caption": "Figure 3:Example of cosine similarity vs. simultaneous activation with a predecessor (350 features were sampled per layer). ‚ÄúFrom MLP‚Äù and ‚ÄúFrom RES‚Äù groups are notably different: highs(M)superscriptùë†ùëÄs^{(M)}italic_s start_POSTSUPERSCRIPT ( italic_M ) end_POSTSUPERSCRIPTand lows(R)superscriptùë†ùëÖs^{(R)}italic_s start_POSTSUPERSCRIPT ( italic_R ) end_POSTSUPERSCRIPTsuggest simultaneous activation with an MLP-module match. Cosine similarity serves as a good proxy for shared semantic and mechanistic properties.",
                "position": 520
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x4.png",
                "caption": "Figure 4:Percentage of statistically significant differences between groups for each module‚Äôs similarity scores. AO means modulePùëÉPitalic_Pis active in only one group, AB means active in both, and IB means inactive in both. For MLP, two groups differ ins(R)superscriptùë†ùëÖs^{(R)}italic_s start_POSTSUPERSCRIPT ( italic_R ) end_POSTSUPERSCRIPTonly 87% of the time when MLP is active in both groups.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x5.png",
                "caption": "Figure 5:Percentages of each group at each layer of Gemma¬†2‚Äâ2B, illustrating how feature formation proceeds in the model.",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x6.png",
                "caption": "Figure 6:Deactivation methods compared. Group labels show which active predecessors were deactivated. The random approach underperforms, suggesting that choosing thetop1subscripttop1\\operatorname{top}_{1}roman_top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTfeature is already meaningful for causal analysis.",
                "position": 541
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x7.png",
                "caption": "Figure 7:Mean activation changes when deactivating one predecessor at a time. Deactivation of some predecessor causes less impact if this predecessor is not activated alone, which leads to the conclusion that combined groups exhibit circuit-like behavior.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x8.png",
                "caption": "Figure 8:Impact of differentrùëüritalic_rvalues on deactivation success, with rescaling of all available predecessors. Whenr<1ùëü1r<1italic_r < 1, the activation change grows nonlinearly, indicating alternative causal pathways still convey information. Relative loss change measured as(Lnew‚àíLold)/Loldsubscriptùêønewsubscriptùêøoldsubscriptùêøold(L_{\\text{new}}-L_{\\text{old}})/L_{\\text{old}}( italic_L start_POSTSUBSCRIPT new end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT old end_POSTSUBSCRIPT ) / italic_L start_POSTSUBSCRIPT old end_POSTSUBSCRIPTis a proxy for forward pass impact.",
                "position": 563
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x9.png",
                "caption": "Figure 9:Deactivating the ‚ÄúScientific concepts and entities‚Äù theme. The dashed black line shows the default generation score. Red points mark the best layer for eachrùëüritalic_rin the single-layer method. Largerrùëüritalic_rboosts performance but shifts the optimal layer earlier.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x10.png",
                "caption": "Figure 10:Comparison of best deactivation scores. The green line indicates deactivation using only the initial feature set. Multi-layer interventions (orange, blue) perform better across differentrùëüritalic_rvalues, suggesting additional discovered features reduce hyperparameter sensitivity.",
                "position": 585
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x11.png",
                "caption": "Figure 11:Activation of specific topics. We compare single-layer steering and cumulative approaches with three rescaling strategies (AppendixB). Activating multiple similar features amplifies a topic‚Äôs presence but may degrade overall text coherence.",
                "position": 591
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Impact statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Experimental Setup",
        "images": []
    },
    {
        "header": "Appendix BSteering details",
        "images": []
    },
    {
        "header": "Appendix CAdditional results for experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x12.png",
                "caption": "Figure 12:(a) Percentage of feature groups obtained for each dataset. (b) Distribution of scores for layers 8 and 18. We observe a clear distinction between groups, which additionally indicates the validity of the proposed method.",
                "position": 1465
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x12.png",
                "caption": "",
                "position": 1468
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x13.png",
                "caption": "",
                "position": 1472
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x14.png",
                "caption": "Figure 13:Probability of group A (row) to appear in group B (column), aggregated over all layers. For example, if we take the ‚ÄúFrom ATT‚Äù group, then with a probability of 0.45, features from this group would appear in the ‚ÄúFrom RES & ATT‚Äù group. High scores for the ‚ÄúFrom nowhere‚Äù group represent its stochasticity.",
                "position": 1478
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x15.png",
                "caption": "Figure 14:Percentage of statistically significant differences between groups with respect to a certain score.",
                "position": 1481
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x16.png",
                "caption": "Figure 15:(a) Percentage of features per each method. There was a total of 13106 activated features, and for every feature, four matching strategies were applied. We see thattop5subscripttop5\\operatorname{top}_{5}roman_top start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPTmethod detects many more combined groups than other methods, especially ‚ÄúFrom RES & MLP‚Äù. (b) Probability for a feature from some groupAùê¥Aitalic_A(labeled as the subplot title) to become from groupBùêµBitalic_B(shown in legend) after deactivation of some predecessor. Each bar shows the percentage of times the feature falls into a new category.",
                "position": 1497
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x16.png",
                "caption": "",
                "position": 1500
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x17.png",
                "caption": "",
                "position": 1505
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x18.png",
                "caption": "Figure 16:From each flow graph, we select features on a particular layerlùëôlitalic_land perform steering with the four different strategies. Bars represent the best result for each layer among all scoressùë†sitalic_s. In some cases, steering on a layer other than 12 may improve results.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x19.png",
                "caption": "Figure 17:(a) Amount of features selected for activation of ‚ÄúResearch methodology and experimentation‚Äù theme. Vertical lines represent the placement of the initially selected features. (b) Results for steering of selected features. Score is a total metric measured asBehavioral√óCumulativeBehavioralCumulative\\text{Behavioral}\\times\\text{Cumulative}Behavioral √ó Cumulative. We can see that despite none of the initial features being placed on the 5th layer, it gives us the best result.",
                "position": 1589
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x19.png",
                "caption": "",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x20.png",
                "caption": "",
                "position": 1596
            }
        ]
    },
    {
        "header": "Appendix DExperiments with Llama Scope",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x21.png",
                "caption": "Figure 18:(a) Distribution of groups for Llama Scope. We observe a clear distinction from Gemma Scope results (Figure12) due to a much smoother distribution. This may be a consequence of various factors: the architecture of the models or SAEs, the training procedure, differences in data distribution, etc. (b) Distribution of groups across multiple layers. We observe approximately the same pattern as for Gemma Scope (Figure5), indicating shared properties between the models. (c) Distribution of scores for different groups. We see that the groups are slightly less distinct from each other compared to the case of Gemma Scope (Figure12), but they are still present. This is also reflected in (d) the separability of different groups based on their cosine similarity relations.",
                "position": 1622
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x21.png",
                "caption": "",
                "position": 1625
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x22.png",
                "caption": "",
                "position": 1629
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x23.png",
                "caption": "",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x24.png",
                "caption": "",
                "position": 1638
            }
        ]
    },
    {
        "header": "Appendix EExamples of flow graphs",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.03032/x25.png",
                "caption": "Figure 19:Flow graph for the 12/res/14455 feature. As reported inChalnev et¬†al. (2024), steering of that feature might produce themes related to fashion, and we clearly observe that our flow graph captures this semantics in the earlier layers.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x26.png",
                "caption": "Figure 20:Flow graph for the 12/res/4230 feature. In this case, we observe that the second half of the model is closely related to wedding and marriage ceremonies. We believe that the ‚Äúofficial‚Äù aspect in the interpretation of features in earlier layers is closely related to the fact that wedding ceremonies and marriage are themselves official procedures‚Äîthe registration of a specific type of interpersonal relationship.",
                "position": 1863
            },
            {
                "img": "https://arxiv.org/html/2502.03032/extracted/6182995/schemes/sae_to_transcoders.png",
                "caption": "Figure 21:Two SAEs with a learned transition matrixTùëáTitalic_Tcan be seen as a transcoder from layertùë°titalic_tto layert+1ùë°1t+1italic_t + 1.",
                "position": 1874
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x27.png",
                "caption": "Figure 22:Explained variance of the various permutation variants. Cosine similarity between decoders‚Äô vectors (ùêàx>0‚Å¢top1‚Å¢ùëædec(14)‚ä§‚Å¢ùëædec(15)subscriptùêàùë•0subscripttop1superscriptsubscriptùëædeclimit-from14topsuperscriptsubscriptùëædec15\\mathbf{I}_{x>0}\\text{ top }_{1}\\boldsymbol{W}_{\\text{dec}}^{(14)\\top}%\n\\boldsymbol{W}_{\\text{dec }}^{(15)}bold_I start_POSTSUBSCRIPT italic_x > 0 end_POSTSUBSCRIPT top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 14 ) ‚ä§ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 15 ) end_POSTSUPERSCRIPT) performs best. See AppendixFfor more details.",
                "position": 1886
            },
            {
                "img": "https://arxiv.org/html/2502.03032/x28.png",
                "caption": "Figure 23:Comparison of variouskùëòkitalic_kintopksubscripttopùëò\\operatorname{top}_{k}roman_top start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPToperator and different weights of the SAE. Cosine similarity (ùêàx>0‚Å¢top1‚Å¢ùëædec(14)‚ä§‚Å¢ùëædec(15)subscriptùêàùë•0subscripttop1superscriptsubscriptùëædeclimit-from14topsuperscriptsubscriptùëædec15\\mathbf{I}_{x>0}\\text{ top }_{1}\\boldsymbol{W}_{\\text{dec}}^{(14)\\top}%\n\\boldsymbol{W}_{\\text{dec}}^{(15)}bold_I start_POSTSUBSCRIPT italic_x > 0 end_POSTSUBSCRIPT top start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 14 ) ‚ä§ end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT dec end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 15 ) end_POSTSUPERSCRIPT) performs best. See AppendixFfor more details.",
                "position": 1898
            }
        ]
    },
    {
        "header": "Appendix FSimilarity between Matching and Transcoders",
        "images": []
    }
]