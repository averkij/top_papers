[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x1.png",
                "caption": "Figure 1:XVerse enables single/multi-subject personalization and the additional control of semantic attributes such as pose, style, and lighting. Input conditions are highlighted with red dots.",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x2.png",
                "caption": "Figure 2:Overview of the XVerse framework. The reference images are processed by a T-Mod Resampler and subsequently injected into the per-token modulation adapter. Additionally, to supplement image details, the VAE-encoded features of the reference image are also utilized as input to the single block of DiTs.",
                "position": 143
            },
            {
                "img": "https://arxiv.org/html/2506.21416/x3.png",
                "caption": "Figure 3:Training Data Construction Pipeline.",
                "position": 258
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x4.png",
                "caption": "Figure 4:Data distribution and samples for XVerseBench. XVerseBench includes evaluations of single-subject, dual-subject, and triple-subject controlled image generation. The figure also illustrates the number of test samples allocated to each category.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2506.21416/x5.png",
                "caption": "Figure 5:Qualitative comparison with different methods on XVerseBench.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2506.21416/x6.png",
                "caption": "Figure 6:Effect of text-stream modulation resampler and VAE-encoded image features.",
                "position": 464
            },
            {
                "img": "https://arxiv.org/html/2506.21416/x7.png",
                "caption": "Figure 7:The control of sementic attributes, such as cloth, pose, lighting, and style.",
                "position": 467
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASamples for Training Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x8.png",
                "caption": "Figure 8:Examples of training data for multi-subject controlled generation.",
                "position": 990
            }
        ]
    },
    {
        "header": "Appendix BImpact of Prompt Variation on the Generated Image",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x9.png",
                "caption": "Figure 9:Impact of prompt variation on subject-controlled Image generation. The reference image and the initial output of our text-to-image generation model are shown on the left side. The right side illustrates the influence of different prompts on the generated output, with the prompt variances highlighted in red.",
                "position": 1000
            }
        ]
    },
    {
        "header": "Appendix CComparison of the CLIP-T and DPG scores",
        "images": []
    },
    {
        "header": "Appendix DIllustration of Region Preservation Loss",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x10.png",
                "caption": "Figure 10:Illustration of the region preservation loss.",
                "position": 1017
            }
        ]
    },
    {
        "header": "Appendix EAblation Study for Text-Image Attention Loss",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21416/x11.png",
                "caption": "Figure 11:The qualitative comparsion of Text-Image Attention Loss. This image shows the generated results and attention maps for \"woman\", \"coffee cup\", and \"pink suit\" for each method.",
                "position": 1027
            }
        ]
    },
    {
        "header": "Appendix FBroader Impacts",
        "images": []
    }
]