[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/running_critique_grpo_ours.png",
                "caption": "Figure 1:(a) Critique-GRPO enhances online reinforcement learning by enabling the model to learn from both its initial responses and subsequent refinements through critiques (natural language feedback), highlighted in green, rather than relying solely on scalar rewards (numerical feedback). (b) Critique-GRPO improves average pass@1 scores by approximately 6.5% and 5.1% on five challenging mathematical reasoning (in-distribution) tasks and three STEM & general reasoning (out-of-distribution) tasks for Qwen2.5-7B-Base and Qwen3-8B-Base, respectively.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Limitations of RL with Numerical Feedback and the Promise of Natural Language Guidance",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/rl_finetuning_dynamics.png",
                "caption": "(a)RL fine-tuning dynamics of Qwen2.5-7B.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/rl_finetuning_dynamics.png",
                "caption": "(a)RL fine-tuning dynamics of Qwen2.5-7B.",
                "position": 199
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/rl_finetuning_dynamics_qwen3.png",
                "caption": "(b)RL fine-tuning dynamics of Qwen3-8B.",
                "position": 204
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/compare_self_reflection_revised.png",
                "caption": "(a)Reasoning behavior contributions to successful problem-solving in RL-tuned Qwen2.5-7B.",
                "position": 220
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/compare_self_reflection_revised.png",
                "caption": "(a)Reasoning behavior contributions to successful problem-solving in RL-tuned Qwen2.5-7B.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/compare_self_reflection_qwen3_revised.png",
                "caption": "(b)Reasoning behavior contributions to successful problem-solving in RL-tuned Qwen3-8B.",
                "position": 228
            }
        ]
    },
    {
        "header": "4Critique-GRPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/rl_analysis.drawio.png",
                "caption": "Figure 4:Overview of Critique-GRPO. Given a question, Critique-GRPO samples initial responses and then refines these responses using critiques generated by a reasoning-based reward model. These refinements are combined with the initial responses to optimize the policy within an online reinforcement learning framework. A weighted advantage function, combined with policy shaping, emphasizes correct refinements while strongly penalizing incorrect ones.",
                "position": 527
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/entropy_qwen25.png",
                "caption": "(a)Entropy dynamics during RL finetuning on Qwen2.5-7B.",
                "position": 1265
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/entropy_qwen25.png",
                "caption": "(a)Entropy dynamics during RL finetuning on Qwen2.5-7B.",
                "position": 1268
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/response_length_qwen25.png",
                "caption": "(b)Response length changes during RL finetuning on Qwen2.5-7B.",
                "position": 1273
            },
            {
                "img": "https://arxiv.org/html/2506.03106/extracted/6510944/floats/raw_figures/qwen3_length.png",
                "caption": "Figure 6:Response length changes during RL finetuning on Qwen3-8B.",
                "position": 1289
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAnalysis of Cognitive Behaviors",
        "images": []
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    },
    {
        "header": "Appendix CLeveraging Textual Critiques for Refining LLM Responses",
        "images": []
    },
    {
        "header": "Appendix DThe Critique-GRPO Algorithm",
        "images": []
    },
    {
        "header": "Appendix EAn Example of Successful Refinement using a CoT Critique",
        "images": []
    },
    {
        "header": "Appendix FResponses Utilized in Qualitative Analysis",
        "images": []
    }
]