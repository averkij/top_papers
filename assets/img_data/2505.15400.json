[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15400/x1.png",
                "caption": "Figure 1:“Internal Self-Recovery Mechanism”: accurate answer achieved via Continue-Thinking behavior, but not when No-Thinking process is suppressed.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15400/x2.png",
                "caption": "Figure 2:Pass@256 on four benchmarks.",
                "position": 191
            },
            {
                "img": "https://arxiv.org/html/2505.15400/x3.png",
                "caption": "Figure 3:Overview of reasoning mode effects in LRMs. (a) Pass@1 and pass@256 under different reasoning modes: the model’s pass@1 drops sharply across the three modes. (b) Insufficient reasoning leads to failure on a difficult problem. (c) Overthinking causes the model to change a correct answer to an incorrect one.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2505.15400/x4.png",
                "caption": "Figure 4:Overview of the ASRR framework. Under No-Thinking mode, special prefixes suppress unnecessary reasoning while allowing implicit self-recovery on harder problems. RL training incorporates a dynamic length penalty based on group-level accuracy to encourage adaptive reasoning. After training, LRMs can adaptively perceive problem difficulty and switch reasoning modes during inference.",
                "position": 212
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15400/x5.png",
                "caption": "Figure 5:Illustration of the trade-off between inference-time thinking length and pass@1 accuracy, across various length-controlled LRMs.",
                "position": 595
            },
            {
                "img": "https://arxiv.org/html/2505.15400/x6.png",
                "caption": "Figure 6:Continue-Thinking Ratio (primary y-axis) and pass@1 accuracy (secondary y-axis) of our method on six subplots, including AIME, Olympiad Bench, AMC, MATH500, GSM8K, and the average across all five benchmarks. The x-axis in each subplot represents model size (DeepSeek-R1-Distill-Qwen-1.5B and 7B).\nOur method enables adaptive thinking strategies under the “Internal Self-Recovery Mechanism”: Achieves 80.6% (1.5B) and 81.5% (7B) Continue-Thinking ratios on high-difficulty AIME tasks, significantly higher than the 2.6% (1.5B) and 0.3% (7B) ratios observed on low-difficulty GSM8K.",
                "position": 601
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BCase Study",
        "images": []
    },
    {
        "header": "Appendix CDetailed Description of Benchmarks",
        "images": []
    },
    {
        "header": "Appendix DDetailed Results Compared with Length-Controlled LRMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.15400/x7.png",
                "caption": "Figure 7:The evaluation of safety prompt template.",
                "position": 2252
            }
        ]
    },
    {
        "header": "Appendix EEvaluation on Safety Alignment",
        "images": []
    }
]