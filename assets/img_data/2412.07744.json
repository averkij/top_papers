[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x1.png",
                "caption": "",
                "position": 86
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x2.png",
                "caption": "Figure 2:Existing image and video stylization methods either fail in keeping local texture or suffer from content leakage.Note: * means StyleCrafter does not support transfer, we use text and reference style image to generate results.",
                "position": 96
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x3.png",
                "caption": "Figure 3:Comparison between Style30K with our dataset generated by model illusion. Style30K cannot ensure consistency within a style group (highlighted by the same color), while ours owns absolute consistency.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x4.png",
                "caption": "Figure 4:The pipeline of our proposed StyleMaster.We first obtain patch features and image embedding of the style image from CLIP, then we select the patches sharing less similarity with text prompt as texture guidance, and use a global projection module to transform it into global style descriptions. The global projection module is trained with a contrastive dataset constructed bymodel illusionthrough contrastive learning. The style information is then injected into the model through the decoupled cross-attention. The motion adapter and gray tile ControlNet are used to enhance dynamic quality and enable content control respectively.",
                "position": 168
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x5.png",
                "caption": "Figure 5:Similarity between the extracted global style representations among image patches. Without our global projection, the CLIP image embedding only attends to specific regions; while after the projection, the attention shows an even distribution.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x6.png",
                "caption": "Figure 6:The selection of texture feature using similarity with prompt features. Top: the kept patches under different drop rates, showing that the dropped tokens are mainly on human-related regions (especially when the drop rate is0.70.70.70.7). Bottom: the attention map of the cross-attention between texture feature and latent when the drop rate is00and0.950.950.950.95, and their generated results.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x7.png",
                "caption": "Figure 7:Uncurated image style transfer results. We compare with the recent state-of-the-art methods InstantStyle[44], StyleID[10]and CSGO[51]. Best viewed in Color.",
                "position": 283
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x8.png",
                "caption": "Figure 8:Qualitative comparison of single-reference and multi-reference style-guided T2V generation. We compare with StyleCrafter[28]and VideoComposer[46]. Best viewed in color.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x9.png",
                "caption": "Figure 9:Video style transfer results compared with DomoAI. Their results disrupt semantics, shown in red bounding box.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x10.png",
                "caption": "Figure 10:Ablation of different conditions of ControlNet in our method. The gray tile achieves the best performance.",
                "position": 658
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Overview",
        "images": []
    },
    {
        "header": "7Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x11.png",
                "caption": "Figure 11:The structure of our base model.",
                "position": 1431
            },
            {
                "img": "https://arxiv.org/html/2412.07744/x12.png",
                "caption": "Figure 12:The model illusion process during T2I generation.",
                "position": 1447
            }
        ]
    },
    {
        "header": "8Image Style Transfer",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x13.png",
                "caption": "Figure 13:The image style transfer results generated by four different methods.",
                "position": 1603
            }
        ]
    },
    {
        "header": "9Stylized Video Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07744/x14.png",
                "caption": "Figure 14:More stylized video generation results.We compare our method with VideoComposer[46]and StyleCrafter[28].",
                "position": 1730
            }
        ]
    },
    {
        "header": "10Video Style Transfer",
        "images": []
    }
]