[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15661/figures/title.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15661/x1.png",
                "caption": "Figure 1:Illustration of the average accuracy improvement (averaged over seven datasets) through successive evolutions (Evo 1 to Evo 5) on Qwen2.5-VL-3B-Instruct, compared to a baseline trained on Vision-47K with GRPO, demonstrating the effectiveness of ourVisPlay.",
                "position": 109
            },
            {
                "img": "https://arxiv.org/html/2511.15661/figures/Visplay.png",
                "caption": "Figure 2:An illustration of ourVisPlayframework, depicting the co-evolution of the Image-Conditioned Questioner and Multimodal Reasoner. Top: During the Questioner training stage, the Image-Conditioned Questioner is optimized via GRPO to produce challenge questions. The reward stems from the uncertainty of the frozen Multimodal Reasoner, computed by the consistency of its multiple generated answers. Bottom: In the Reasoner training stage, the Multimodal Reasoner is trained via GRPO on a curated set of challenging questions from the now-frozen Image-Conditioned Questioner, leveraging pseudo-labels from its own majority voting.",
                "position": 148
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15661/figures/snowflake.png",
                "caption": "Table 1:Comprehensive results on visual reasoning benchmarks. Each base model is evaluated against two settings: aVisPlay(challenger) baseline, in which the Reasoner is trained on questions produced by an untrained Challenger, and our iterativeVisPlayframework. The highest performance reached during training for each model is emphasized in bold. We take accuracy as the metric.",
                "position": 493
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.15661/figures/dffacc3b.png",
                "caption": "Figure 3:Changes in question difficulty (orange, left axis) and problem-solving accuracy (blue, right axis)\nduring Image-Conditioned Questioner and Multimodal Reasoner training across three VLMs.",
                "position": 753
            },
            {
                "img": "https://arxiv.org/html/2511.15661/figures/dffacc3b.png",
                "caption": "",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2511.15661/figures/dffaccq7b.png",
                "caption": "",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2511.15661/figures/dffaccm7b.png",
                "caption": "",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2511.15661/figures/f1.png",
                "caption": "Table 4:Examples of challenging questions generated by the self-evolving Vision-Language model across three training iterations. The questions progressively increase in complexity, illustrating the growth in difficulty of the Questionerâ€™s outputs over iterations. Images on the left and right correspond to the visual context for each question. The questions are raised by Qwen2.5-VL-3B-Instruct.",
                "position": 930
            },
            {
                "img": "https://arxiv.org/html/2511.15661/figures/f2.png",
                "caption": "",
                "position": 943
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Limitation",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "A.1 Detailed Training Dataset and Benchmarks",
        "images": []
    },
    {
        "header": "A.2 Training Configuration",
        "images": []
    },
    {
        "header": "A.3 Prompt Templates",
        "images": []
    }
]