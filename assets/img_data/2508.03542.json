[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03542/x1.png",
                "caption": "Figure 1:S2L methods schematic illustration. (a) Post-correction approach. (b) Multi-modal end-to-end approach (SALMONN). In (a), audio is transcribed by an ASR model, and the result is passed to an LLM for LaTeX conversion. In (b), raw audio is processed by 2 audio encoders and an adapter, and the resulting audio and textual prompt tokens are fed into a LLaMA-based LLM to generate the LaTeX.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Dataset Collection",
        "images": []
    },
    {
        "header": "4Methodology and Experimental Setups",
        "images": []
    },
    {
        "header": "5Results and Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03542/x2.png",
                "caption": "Figure 2:S2L-equationscollection and annotation pipeline overview.",
                "position": 1787
            }
        ]
    },
    {
        "header": "Appendix BTraining Hyperparameteres",
        "images": []
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]