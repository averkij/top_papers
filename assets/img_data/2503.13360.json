[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Take-along Visual Conditioning: Sustaining Visual Evidence for Multi-modal Long CoT Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13360/x1.png",
                "caption": "Figure 1:The visual forgetting phenomenon by removing the image at different reasoning stages.It shows that by the midpoint of the reasoning process, the model becomes less dependent on the image, causing text-over-relied outputs.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2503.13360/x2.png",
                "caption": "Figure 2:Illustration of layer-level and token-level attention weights.(a) The layer-level attention weights of image tokens across different response token positions. (b) The token-level attention weights at the middle layer. It shows that the modelâ€™s attention to the image gradually decreases during the reasoning process.",
                "position": 142
            },
            {
                "img": "https://arxiv.org/html/2503.13360/x3.png",
                "caption": "Figure 3:Overview of TVC System Design.We enable the model to have take-along visual conditioning capabilities through two stages: training and inference.",
                "position": 210
            }
        ]
    },
    {
        "header": "3Data-Centric Implementation of Multimodal Reasoning System",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13360/x4.png",
                "caption": "Figure 4:Data Generation Pipeline of TVC.We use iterative distillation to collect long-chain reasoning data, followed by a comprehensive response filtering process to ensure high-quality reasoning.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13360/x5.png",
                "caption": "Figure 5:Ablations on the amount of training data.TVCbenefits from data scaling, continually improving the reasoning capabilities.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2503.13360/x6.png",
                "caption": "Figure 6:Case Study of TVC.TVCeffectively re-examines the image during the reflection process to correct mistakes, guiding the model to the correct answer.",
                "position": 586
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BMore Details of Reasoning Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.13360/x7.png",
                "caption": "Figure 7:The token and reflection word distribution of the long-chain reasoning dataset.",
                "position": 1239
            },
            {
                "img": "https://arxiv.org/html/2503.13360/x8.png",
                "caption": "Figure 8:Qualitative Results ofTVC.",
                "position": 1341
            }
        ]
    },
    {
        "header": "Appendix CDiscussion",
        "images": []
    }
]