[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x1.png",
                "caption": "Figure 1:Global Attention Sinks (GAS) in DiT.We highlight tokens (here, tokens #1 and #16) that act as GAS in late layers. These tokens allocate disproportionately high and nearly uniform attention across all text and image tokens simultaneously. GAS are absent in early layers, emerge consistently in deeper blocks, and serve as indicators of semantic structure. While uninformative themselves, they can suppress useful signals when they occur on meaningful tokens.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x2.png",
                "caption": "Figure 2:Pipeline overview.We first extract cross-attention maps for the referring expression with attention magnets. Next, we filter out stop words and attention magnets, aggregate the remaining maps, identify the argmax location, and apply SAM to generate the final segmentation mask.",
                "position": 109
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Semantic Features from Diffusion Transformers",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x3.png",
                "caption": "Figure 3:Emergence of semantic information in DiT.Top: text-to-text attention across layers. Early layers (0–19) are diffuse and uniform, while middle and late layers (20–47) develop block-diagonal structure, indicating meaningful linguistic grouping. Bottom: text-to-image attention for the “_patches” token. Early layers spread attention broadly over the scene, whereas middle layers begin to localize, and late layers sharpen around the target object. These dynamics illustrate how semantic alignment emerges progressively with depth.",
                "position": 137
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x4.png",
                "caption": "Figure 4:Entropy across transformer blocks.Blocks 0-25 contain no specific information.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x4.png",
                "caption": "Figure 4:Entropy across transformer blocks.Blocks 0-25 contain no specific information.",
                "position": 207
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x5.png",
                "caption": "Figure 5:Influence of attention magnets on RVOS.Examples demonstrating attention magnets filtering impact.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x6.png",
                "caption": "Figure 6:Qualitative examples.Referring image object segmentation results.\nEach triplet shows the input image with the referring expression, the cross-attention heatmap with the detected argmax point (star), and the final segmentation mask produced by SAM.",
                "position": 1086
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x7.png",
                "caption": "Figure 7:Visualization of different representaiton spaces.RefAMfeatures with cross attention representations or output representations of attention. For referral tasks,RefAMuse cross attention.",
                "position": 2262
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x7.png",
                "caption": "",
                "position": 2265
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x8.png",
                "caption": "",
                "position": 2270
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x9.png",
                "caption": "",
                "position": 2275
            }
        ]
    },
    {
        "header": "Appendix BStop Word Filtering & Additional Stop Words",
        "images": []
    },
    {
        "header": "Appendix CDetails of Noun Phrase and Spatial Bias Extraction",
        "images": []
    },
    {
        "header": "Appendix DLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x10.png",
                "caption": "Figure 8:Visualization of SAM2 failure under-segmentations.",
                "position": 2654
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x10.png",
                "caption": "",
                "position": 2657
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x11.png",
                "caption": "",
                "position": 2662
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x12.png",
                "caption": "",
                "position": 2667
            }
        ]
    },
    {
        "header": "Appendix ESocietal Impact",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22650/x13.png",
                "caption": "Figure 9:Additional qualitative examples for referring image object segmentation.Each panel shows the input image with its corresponding referring expression and the predicted segmentation mask. These examples complement the results in the main paper and illustrate the diversity of object categories and spatial references handled by our method.",
                "position": 2689
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x14.png",
                "caption": "Figure 10:Qualitative examples.VROS task, evaluated on Ref-DAVIS17. From left to right: first frame of the video with the corresponding ref.expression on the top, avg. attention map from ourRefAM+ inversion features, segmentation outputs with SAM2.",
                "position": 2692
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x15.png",
                "caption": "Figure 11:Qualitative examples.Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected.",
                "position": 2695
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x16.png",
                "caption": "Figure 12:Qualitative examples.Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected.",
                "position": 2698
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x17.png",
                "caption": "Figure 13:Qualitative examples.Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected.",
                "position": 2701
            },
            {
                "img": "https://arxiv.org/html/2509.22650/x18.png",
                "caption": "Figure 14:Qualitative examples.Qualitative comparison of attention maps obtained with and without additional stop words. The top row shows the first frame of the video along with the corresponding referring expression. The first row includes the average attention map, where the star indicates the argmax point with indication if it was correctly detected.",
                "position": 2704
            }
        ]
    },
    {
        "header": "Appendix FQualitative Examples",
        "images": []
    }
]