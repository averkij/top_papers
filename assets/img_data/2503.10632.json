[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/x1.png",
                "caption": "Figure 1:Model parameters vs.Â Top-1 Test accuracyin ImegeNet-1K training of vanilla ViTs(Dosovitskiy etÂ al.,2020), Vision KAN (DeiT+KAN) by(Chen etÂ al.,2024), ViT+KAN and Kolmogorov-Arnold Transformer (KAT) by(Yang & Wang,2025).",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/x2.png",
                "caption": "Figure 2:(a) The standardsoftmaxself-attention forithsuperscriptğ‘–thi^{\\rm th}italic_i start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPThead in thejthsuperscriptğ‘—thj^{\\rm th}italic_j start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPTencoder block. (b) The Kolmogorov-Arnold Attention (KArAt) replaces thesoftmaxwith a learnable functionÎ¦i,jsuperscriptÎ¦ğ‘–ğ‘—\\Phi^{i,j}roman_Î¦ start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT. (c) The ideal KArAt is with an operator matrix,Î¦i,jsuperscriptÎ¦ğ‘–ğ‘—\\Phi^{i,j}roman_Î¦ start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTwithNÃ—Nğ‘ğ‘N\\times Nitalic_N Ã— italic_Nlearnable units that act on each row ofğ’œi,jsuperscriptğ’œğ‘–ğ‘—{\\cal A}^{i,j}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT. (d) WhileÎ¦i,jâˆˆâ„NÃ—NsuperscriptÎ¦ğ‘–ğ‘—superscriptâ„ğ‘ğ‘\\Phi^{i,j}\\in\\mathbb{R}^{N\\times N}roman_Î¦ start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N Ã— italic_N end_POSTSUPERSCRIPTis impossible to implement due to computational constraints, our architecture uses an operatorÎ¦^i,jsuperscript^Î¦ğ‘–ğ‘—\\widehat{\\Phi}^{i,j}over^ start_ARG roman_Î¦ end_ARG start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTwithNÃ—rğ‘ğ‘ŸN\\times ritalic_N Ã— italic_rlearnable unitsrâ‰ªNmuch-less-thanğ‘Ÿğ‘r\\ll Nitalic_r â‰ª italic_N, followed by a linear projector with learnable weightsWâˆˆâ„rÃ—Nğ‘Šsuperscriptâ„ğ‘Ÿğ‘W\\in\\mathbb{R}^{r\\times N}italic_W âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_r Ã— italic_N end_POSTSUPERSCRIPT.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3How Can We Design Learnable Attention?",
        "images": []
    },
    {
        "header": "4Our Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/x3.png",
                "caption": "(a)Attention matrixğ’œi,jsuperscriptğ’œğ‘–ğ‘—{\\mathcal{A}^{i,j}}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTbeforesoftmaxactivation.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x3.png",
                "caption": "(a)Attention matrixğ’œi,jsuperscriptğ’œğ‘–ğ‘—{\\mathcal{A}^{i,j}}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTbeforesoftmaxactivation.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x4.png",
                "caption": "(b)Attention matrixÏƒâ¢(ğ’œi,j)ğœsuperscriptğ’œğ‘–ğ‘—\\sigma(\\mathcal{A}^{i,j})italic_Ïƒ ( caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT )aftersoftmaxactivation.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x5.png",
                "caption": "Figure 4:Different configurations to updateÎ¦^^Î¦\\widehat{\\Phi}over^ start_ARG roman_Î¦ end_ARG:(a) Blockwise configuration, whereÎ¦i,1â‰ Î¦i,2â‰ â‹¯â‰ Î¦i,LsuperscriptÎ¦ğ‘–1superscriptÎ¦ğ‘–2â‹¯superscriptÎ¦ğ‘–ğ¿\\Phi^{i,1}\\neq\\Phi^{i,2}\\neq\\cdots\\neq\\Phi^{i,L}roman_Î¦ start_POSTSUPERSCRIPT italic_i , 1 end_POSTSUPERSCRIPT â‰  roman_Î¦ start_POSTSUPERSCRIPT italic_i , 2 end_POSTSUPERSCRIPT â‰  â‹¯ â‰  roman_Î¦ start_POSTSUPERSCRIPT italic_i , italic_L end_POSTSUPERSCRIPTfor alli=1,2,â€¦,hğ‘–12â€¦â„i=1,2,...,hitalic_i = 1 , 2 , â€¦ , italic_h(b) universal configuration, whereÎ¦i,1=Î¦i,2=â‹¯=Î¦i,L=Î¦isuperscriptÎ¦ğ‘–1superscriptÎ¦ğ‘–2â‹¯superscriptÎ¦ğ‘–ğ¿superscriptÎ¦ğ‘–\\Phi^{i,1}=\\Phi^{i,2}=\\cdots=\\Phi^{i,L}=\\Phi^{i}roman_Î¦ start_POSTSUPERSCRIPT italic_i , 1 end_POSTSUPERSCRIPT = roman_Î¦ start_POSTSUPERSCRIPT italic_i , 2 end_POSTSUPERSCRIPT = â‹¯ = roman_Î¦ start_POSTSUPERSCRIPT italic_i , italic_L end_POSTSUPERSCRIPT = roman_Î¦ start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTfor alli=1,2,â€¦,h.ğ‘–12â€¦â„i=1,2,...,h.italic_i = 1 , 2 , â€¦ , italic_h .",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/TrainingPlots.png",
                "caption": "Figure 5:Training loss and test accuracyof vanilla ViTs and their Fourier KArAt versions on CIFAR-10, CIFAR-100 and ImageNet-1K.",
                "position": 359
            }
        ]
    },
    {
        "header": "5Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/WeightDist.png",
                "caption": "Figure 6:Weight distributionof (top to bottom) ViT-Tiny, ViT-Tiny+Fourier KArAt, ViT-Base, ViT-Base+Fourier KArAt. The columns (left to right) represent weights at initialization, at epoch 50, at epoch 100, and their superposition.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x6.png",
                "caption": "Figure 7:Spectral decomposition of the attention matrixfor ViT-Tiny on CIFAR-10 dataset with vanillasoftmaxattention and our learnable Fourier attention.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/LossLandscapeAllV2.png",
                "caption": "Figure 8:Loss landscape for ViT-Tiny and ViT-Base(the smallest and the largest model) along the two largest principal component directions of the successive change of model parameters. The top row provides a 3D-visulaization of the loss surface including the local optima and saddle points. The bottom row shows the loss contours along with the trajectory of the optimizers.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/DinoAttnVisV2.png",
                "caption": "Figure 9:Visualizing attention maps for Vit-Tiny.The original images used for inference are on the left, and on the right, we show the attention score map and the image regions of the dominant head (Top row: Fourier KArAt, bottom row: Vanilla MHSA).",
                "position": 576
            }
        ]
    },
    {
        "header": "6Conclusion and Future Direction",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASolution to Problem (3)",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c10.png",
                "caption": "(a)Training time on CIFAR-10 dataset.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c10.png",
                "caption": "(a)Training time on CIFAR-10 dataset.",
                "position": 1814
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c100.png",
                "caption": "(b)Training time on CIFAR-100 dataset.",
                "position": 1820
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_in1k.png",
                "caption": "(c)Training time on Imagenet-1K dataset.",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/throughput.png",
                "caption": "(d)Throughput comparison during inference.",
                "position": 1832
            }
        ]
    },
    {
        "header": "Appendix BAddendum to Empirical Analysis",
        "images": []
    }
]