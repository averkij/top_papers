[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/x1.png",
                "caption": "Figure 1:Model parameters vs. Top-1 Test accuracyin ImegeNet-1K training of vanilla ViTs(Dosovitskiy et al.,2020), Vision KAN (DeiT+KAN) by(Chen et al.,2024), ViT+KAN and Kolmogorov-Arnold Transformer (KAT) by(Yang & Wang,2025).",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/x2.png",
                "caption": "Figure 2:(a) The standardsoftmaxself-attention forithsuperscript𝑖thi^{\\rm th}italic_i start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPThead in thejthsuperscript𝑗thj^{\\rm th}italic_j start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPTencoder block. (b) The Kolmogorov-Arnold Attention (KArAt) replaces thesoftmaxwith a learnable functionΦi,jsuperscriptΦ𝑖𝑗\\Phi^{i,j}roman_Φ start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT. (c) The ideal KArAt is with an operator matrix,Φi,jsuperscriptΦ𝑖𝑗\\Phi^{i,j}roman_Φ start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTwithN×N𝑁𝑁N\\times Nitalic_N × italic_Nlearnable units that act on each row of𝒜i,jsuperscript𝒜𝑖𝑗{\\cal A}^{i,j}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT. (d) WhileΦi,j∈ℝN×NsuperscriptΦ𝑖𝑗superscriptℝ𝑁𝑁\\Phi^{i,j}\\in\\mathbb{R}^{N\\times N}roman_Φ start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_N end_POSTSUPERSCRIPTis impossible to implement due to computational constraints, our architecture uses an operatorΦ^i,jsuperscript^Φ𝑖𝑗\\widehat{\\Phi}^{i,j}over^ start_ARG roman_Φ end_ARG start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTwithN×r𝑁𝑟N\\times ritalic_N × italic_rlearnable unitsr≪Nmuch-less-than𝑟𝑁r\\ll Nitalic_r ≪ italic_N, followed by a linear projector with learnable weightsW∈ℝr×N𝑊superscriptℝ𝑟𝑁W\\in\\mathbb{R}^{r\\times N}italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_N end_POSTSUPERSCRIPT.",
                "position": 104
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3How Can We Design Learnable Attention?",
        "images": []
    },
    {
        "header": "4Our Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/x3.png",
                "caption": "(a)Attention matrix𝒜i,jsuperscript𝒜𝑖𝑗{\\mathcal{A}^{i,j}}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTbeforesoftmaxactivation.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x3.png",
                "caption": "(a)Attention matrix𝒜i,jsuperscript𝒜𝑖𝑗{\\mathcal{A}^{i,j}}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPTbeforesoftmaxactivation.",
                "position": 292
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x4.png",
                "caption": "(b)Attention matrixσ⁢(𝒜i,j)𝜎superscript𝒜𝑖𝑗\\sigma(\\mathcal{A}^{i,j})italic_σ ( caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT )aftersoftmaxactivation.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x5.png",
                "caption": "Figure 4:Different configurations to updateΦ^^Φ\\widehat{\\Phi}over^ start_ARG roman_Φ end_ARG:(a) Blockwise configuration, whereΦi,1≠Φi,2≠⋯≠Φi,LsuperscriptΦ𝑖1superscriptΦ𝑖2⋯superscriptΦ𝑖𝐿\\Phi^{i,1}\\neq\\Phi^{i,2}\\neq\\cdots\\neq\\Phi^{i,L}roman_Φ start_POSTSUPERSCRIPT italic_i , 1 end_POSTSUPERSCRIPT ≠ roman_Φ start_POSTSUPERSCRIPT italic_i , 2 end_POSTSUPERSCRIPT ≠ ⋯ ≠ roman_Φ start_POSTSUPERSCRIPT italic_i , italic_L end_POSTSUPERSCRIPTfor alli=1,2,…,h𝑖12…ℎi=1,2,...,hitalic_i = 1 , 2 , … , italic_h(b) universal configuration, whereΦi,1=Φi,2=⋯=Φi,L=ΦisuperscriptΦ𝑖1superscriptΦ𝑖2⋯superscriptΦ𝑖𝐿superscriptΦ𝑖\\Phi^{i,1}=\\Phi^{i,2}=\\cdots=\\Phi^{i,L}=\\Phi^{i}roman_Φ start_POSTSUPERSCRIPT italic_i , 1 end_POSTSUPERSCRIPT = roman_Φ start_POSTSUPERSCRIPT italic_i , 2 end_POSTSUPERSCRIPT = ⋯ = roman_Φ start_POSTSUPERSCRIPT italic_i , italic_L end_POSTSUPERSCRIPT = roman_Φ start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPTfor alli=1,2,…,h.𝑖12…ℎi=1,2,...,h.italic_i = 1 , 2 , … , italic_h .",
                "position": 322
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/TrainingPlots.png",
                "caption": "Figure 5:Training loss and test accuracyof vanilla ViTs and their Fourier KArAt versions on CIFAR-10, CIFAR-100 and ImageNet-1K.",
                "position": 359
            }
        ]
    },
    {
        "header": "5Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/WeightDist.png",
                "caption": "Figure 6:Weight distributionof (top to bottom) ViT-Tiny, ViT-Tiny+Fourier KArAt, ViT-Base, ViT-Base+Fourier KArAt. The columns (left to right) represent weights at initialization, at epoch 50, at epoch 100, and their superposition.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2503.10632/x6.png",
                "caption": "Figure 7:Spectral decomposition of the attention matrixfor ViT-Tiny on CIFAR-10 dataset with vanillasoftmaxattention and our learnable Fourier attention.",
                "position": 556
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/LossLandscapeAllV2.png",
                "caption": "Figure 8:Loss landscape for ViT-Tiny and ViT-Base(the smallest and the largest model) along the two largest principal component directions of the successive change of model parameters. The top row provides a 3D-visulaization of the loss surface including the local optima and saddle points. The bottom row shows the loss contours along with the trajectory of the optimizers.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/DinoAttnVisV2.png",
                "caption": "Figure 9:Visualizing attention maps for Vit-Tiny.The original images used for inference are on the left, and on the right, we show the attention score map and the image regions of the dominant head (Top row: Fourier KArAt, bottom row: Vanilla MHSA).",
                "position": 576
            }
        ]
    },
    {
        "header": "6Conclusion and Future Direction",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASolution to Problem (3)",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c10.png",
                "caption": "(a)Training time on CIFAR-10 dataset.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c10.png",
                "caption": "(a)Training time on CIFAR-10 dataset.",
                "position": 1814
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c100.png",
                "caption": "(b)Training time on CIFAR-100 dataset.",
                "position": 1820
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_in1k.png",
                "caption": "(c)Training time on Imagenet-1K dataset.",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/throughput.png",
                "caption": "(d)Throughput comparison during inference.",
                "position": 1832
            }
        ]
    },
    {
        "header": "Appendix BAddendum to Empirical Analysis",
        "images": []
    }
]