[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12203/x1.png",
                "caption": "Figure 1.Pipeline of LazyDrag.(a) An input image is inverted to a latent codeğ’›T\\bm{z}_{T}. Our correspondence map generation then yields an updated latentğ’›^T\\hat{\\bm{z}}_{T}, point matching map, and weightsÎ±\\alpha. Tokens cached during inversion are used to guide the sampling process for identity and background preservation. (b) In attention input control, a dual strategy is employed. For background regions (graycolor),ğ\\mathbf{Q},ğŠ\\mathbf{K}, andğ•\\mathbf{V}tokens are replaced with their cached originals. For destination (redandbluecolors) and transition regions (yellowcolor), theğŠ\\mathbf{K}andğ•\\mathbf{V}tokens are concatenated with re-encoded (ğŠ\\mathbf{K}only) source tokens retrieved via the map (c) Attention output refinement performs value blending of attention output.âŠ—\\otimesandâŠ•\\oplusdenotes element-wise product and addition.",
                "position": 178
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12203/x2.png",
                "caption": "Figure 2.Effect of inversion strength.Examples of LazyDrag under different inversion strengths. The additional prompt is â€œaredapple in the mouthâ€.",
                "position": 222
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12203/x3.png",
                "caption": "Figure 3.Pipeline of LazyDrag.(a) An input image is inverted to a latent codeğ’›T\\bm{z}_{T}. Our correspondence map generation then yields an updated latentğ’›^T\\hat{\\bm{z}}_{T}, point matching map, and weightsÎ±\\alpha. Tokens cached during inversion are used to guide the sampling process for identity and background preservation. (b) In attention input control, a dual strategy is employed. For background regions (graycolor),ğ\\mathbf{Q},ğŠ\\mathbf{K}, andğ•\\mathbf{V}tokens are replaced with their cached originals. For destination (redandbluecolors) and transition regions (yellowcolor), theğŠ\\mathbf{K}andğ•\\mathbf{V}tokens are concatenated with re-encoded (ğŠ\\mathbf{K}only) source tokens retrieved via the map (c) Attention output refinement performs value blending of attention output.âŠ—\\otimesandâŠ•\\oplusdenotes element-wise product and addition.",
                "position": 252
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12203/x4.png",
                "caption": "Figure 4.Qualitative results compared with baselines on Drag-Bench.Best viewed with zoom-in.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x5.png",
                "caption": "Figure 5.Comparison between drag and move mode on Drag-Bench.",
                "position": 725
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x6.png",
                "caption": "Figure 6.Qualitative cumulative ablation on Drag-Bench.Rows remove one component relative to the row above.\nWhen WTA and Latent Init are removed we use latent init in FastDrag. When ID Pres. and Attn Refine are removed we switch to CharaConsist attention-similarity control.",
                "position": 732
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x7.png",
                "caption": "Figure 7.Qualitative ablation of activation timesteps on Drag-Bench.From left to right, the activation timestep is increased.",
                "position": 760
            }
        ]
    },
    {
        "header": "5.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12203/x8.png",
                "caption": "Figure 8.Examples of Drag-Bench cases with various additional text prompts.",
                "position": 1732
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x9.png",
                "caption": "Figure 9.Effect of activation timestep sensitivity on Drag-Bench.From left to right, the activation timestep is progressively increased.",
                "position": 1735
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x10.png",
                "caption": "Figure 10.Failure cases on Drag-Bench.",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x11.png",
                "caption": "Figure 11.Qualitative ablation of WTA and Latent Init with U-Nets on Drag-Bench.",
                "position": 1797
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x12.png",
                "caption": "Figure 12.Additional qualitative results compared with baselines on Drag-Bench.",
                "position": 1806
            },
            {
                "img": "https://arxiv.org/html/2509.12203/imgs/user_study.png",
                "caption": "Figure 13.User interface for user study.",
                "position": 1809
            },
            {
                "img": "https://arxiv.org/html/2509.12203/x13.png",
                "caption": "Figure 14.Instruction ofSCevaluation.",
                "position": 1812
            }
        ]
    },
    {
        "header": "Appendix BMore Results and Analysis",
        "images": []
    }
]