[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20690/x1.png",
                "caption": "",
                "position": 58
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20690/x2.png",
                "caption": "Figure 2:“Data Efficiency” increases inversely with the amount of training data, while CLIP score reflects editing performance. Our method achieves high editing precision with fewer training data.",
                "position": 80
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20690/x3.png",
                "caption": "Figure 3:Attention Map Visualization of Edit Instructions (§3.1).We computed the attention values for the selected text by aggregating sums and averages across different steps and layers.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x4.png",
                "caption": "Figure 4:Exploration of Two Training-Free In-Context Edit Structures (§3.1).Example images for each framework are edited outputs from them. Despite some artifacts, they demonstrate potential for instruction-based editing tasks.",
                "position": 146
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x5.png",
                "caption": "Figure 5:We augment the inpainting framework’s editing capabilities through LoRA-MoE hybrid tuning, integrating parameter-efficient adaptation with dynamic expert routing for specialized feature processing and dynamic computation (§3.2).",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x6.png",
                "caption": "Figure 6:Illustration of Inference-Time Scaling Strategy (§3.3).The upper rows demonstrate that edit success can be assessed within a few initial steps. These early results are used to filter the optimal initial noise with VLM judges.",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x7.png",
                "caption": "Figure 7:Comparison with baseline models on the Emu Edit test set (§4.1).Our method demonstrates superior performance in both edit-instruction accuracy and preservation of non-edited regions compared to the baseline models.\\faSearchZoom infor detailed view.",
                "position": 221
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.20690/x8.png",
                "caption": "Figure 8:We employ the VIE-score to evaluate human preference alignment and quantify improvements from our inference-time scaling strategy (w/ Inf. Sca.) (§4.1and §4.2).",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x9.png",
                "caption": "Figure 9:In human-centric image editing, SeedEdit prioritizes aesthetics at the expense of identity consistency, whereas our approach ensures more precise edits aligned with the intended goals.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x10.png",
                "caption": "Figure 10:Ablation on Inference-Time Scaling (§4.2).Our strategy significantly enhances edit quality. For example, with the instruction “get rid of the helmet,” default fixed seed incorrectly removes the character’s head—a flawed outcome prevented by VLM filtering.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x11.png",
                "caption": "Figure 11:Our method achievesmore harmonious editing resultsby automatically incorporating shadow effects and style alignment, leading to significantly improved outcomes (§4.3).",
                "position": 428
            },
            {
                "img": "https://arxiv.org/html/2504.20690/x12.png",
                "caption": "Figure 12:Applications (§4.3). Without additional tuning, our method demonstrates robust generalization across diverse tasks.",
                "position": 515
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]