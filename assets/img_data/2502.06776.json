[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06776/x1.png",
                "caption": "Figure 1:Overview of the proposed agent pipeline.We develop a pipeline for training web navigation agents at internet scale using tasks proposed, attempted, and evaluated by pretrained large language models. We generate 150k diverse tasks across 1M internet sites. Code for our data generation pipeline, and traces for agent rollouts will be available on our website:data-for-agents.github.io.",
                "position": 126
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Language Model Agents",
        "images": []
    },
    {
        "header": "4Internet-Scale Task Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06776/x2.png",
                "caption": "Figure 2:Task proposal and filtering for 150k live websites.Starting from 1,000,000 websites, we employ a pretrained language model that marks sites as safe/unsafe for annotation, and assigns a realistic task that a hypothetical user might want to accomplish on each site. The task proposer aggressively filters out 85% of websites from the pipeline, resulting in 150k safe websites annotated with realistic tasks.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x3.png",
                "caption": "Figure 5:Feasibility rates vs PageRank values.We visualize PageRank values, a proxy for the popularity of websites, versus the expert feasibility rates of proposed web tasks. Human-written tasks perform on par with LLMs for popular sites, but as target sites become less popular and annotators are less familiar with them, LLMs begin to outperform human annotators at creating feasible tasks for agents.",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x4.png",
                "caption": "Figure 6:Distribution of 150k tasks.We compare the distribution of tasks generated by our pipeline (bluepoints) to the Mind2Web(Deng et al.,2023)dataset (orangepoints) via textual features extracted by a sentence embedding model, and projected in 2D with UMAP(McInnes et al.,2020). Our distribution is denser than human-written tasks, and has broad coverage of diverse real-world sites and tasks.",
                "position": 360
            }
        ]
    },
    {
        "header": "5Internet-Scale Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06776/x5.png",
                "caption": "Figure 7:Automatic evaluation for agents with language model judges.Building on the large and diverse set of tasks generated by the pipeline, we employ pretrained language models to attempt and evaluate web navigation tasks. We dispatch language model agents to perform tasks by making calls to the Playwright API. We then employ language model judges to evaluate the trajectories.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x6.png",
                "caption": "Figure 8:Language models are robust evaluators.We measure the accuracy of language models for detecting successful trajectories, and find that accuracy remains stable relative to PageRank values (left plot). As models become more confident, their accuracy improves (right plot), suggesting confidence is a useful proxy for the reliability of their predictions.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x7.png",
                "caption": "Figure 9:Scaling LLM agents to 150k live sites.We run agents based on Llama 3.1 70B to complete tasks generated by our pipeline. We estimate success probabilities using a language model evaluator (left plot), and estimate probabilities agents are on the right track (right plot). 16.7% of tasks are estimated to be successful withconf= 1, and the spread of probabilities suggests data spans many difficulties.",
                "position": 407
            }
        ]
    },
    {
        "header": "6Training Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06776/x8.png",
                "caption": "Figure 10:Data from InSTA improves efficiency.Language model agents trained on mixtures of our data and human demonstrations scale faster than agents trained on human data. In a setting with 32 human actions, adding our data improvesStep Accuracyby +89.5% relative to human data for Mind2Web, and +122.1% relative to human data for WebLINX.",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x9.png",
                "caption": "Figure 11:Our data improves generalization.We train agents with all human data from the WebLINX and Mind2Web training sets, and resulting agents struggle to generalize to more diverse test data. Adding our data improves generalization by +149.0% for WebLINX, and +156.3% for Mind2Web.",
                "position": 442
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "8Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALimitations & Safeguards",
        "images": []
    },
    {
        "header": "Appendix BEthical Considerations",
        "images": []
    },
    {
        "header": "Appendix CBroader Impacts",
        "images": []
    },
    {
        "header": "Appendix DAgents.txt& Standards For Internet Agents",
        "images": []
    },
    {
        "header": "Appendix EMore Details On Task Generation",
        "images": []
    },
    {
        "header": "Appendix FUnderstanding Agent Capabilities & Limitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.06776/x10.png",
                "caption": "Figure 14:Largest categories for internet-scale task generation. We assign categories to 150k web navigation tasks generated by our pipeline in Section4, and visualize the number of tasks for each of the largest 70 categories. Top categories includearticle search,news search,recipe search,product lookup, and more. The top 12 task categories have more than 1600 tasks assigned to each of them, the mean number of tasks per category is 16.9, and 89% of categories (7741 in total) have fewer than the mean number of tasks.",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x11.png",
                "caption": "Figure 15:Most successful categories for internet-scale task generation. We explore the rates of task completion for the\ntop categories of tasks generated by our pipeline. We restrict our focus to categories where at least 100 tasks are assigned, and plots the success rates for the top 70 of such categories. Results show that 22 categories are solved with more than a 50% rate with agents based onLlama 3.1 70B.",
                "position": 2096
            },
            {
                "img": "https://arxiv.org/html/2502.06776/x12.png",
                "caption": "Figure 16:Least successful categories for internet-scale task generation. Similar to the previous figure, we explore the rates of task completion for the bottom 70 categories that have at least 100 tasks assigned to them. While the majority of the least successful categories have success rates greater than 20%, performance drops as low as 5%. Many of the categories shown in the plot above involve actions that are not feasible given the current limitations of the Playwright API, and may be possible in future work that extends agents to a fully-operable virtual computer environment. In addition, better LLM backbones are likely to improve performance.",
                "position": 2106
            }
        ]
    },
    {
        "header": "Appendix GAgent & Judge System Prompts",
        "images": []
    },
    {
        "header": "Appendix HDetails For Training Agents",
        "images": []
    },
    {
        "header": "Appendix IAdditional Related Works",
        "images": []
    },
    {
        "header": "Appendix JHyperparameters",
        "images": []
    },
    {
        "header": "Appendix KCost Analysis For Llama 3.1 70B",
        "images": []
    }
]