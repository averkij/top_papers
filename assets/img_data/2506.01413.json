[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/github.png",
                "caption": "",
                "position": 206
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/huggingface-color.png",
                "caption": "",
                "position": 206
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/intro_copy.png",
                "caption": "Figure 1:Complex instructions with various atomic constraints and compositions pose great challenges to instruction-following capabilities of LLMs (best viewed magnified).\nThe CoT prompting of existing LLMs often elicits shallow reasoning that blindly, mechanically responds to the request without formulation of structured analyses.\nIn contrast to R1 and QwQ,\nmost fask-thinking models cannot benefit from the vanilla CoT at all due to such superficial nature (see Sec.A.6).\nOur proposed method boosts deep reasoning of both fast- and slow-thinking LLMs under complex instructions.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/framework_copy.png",
                "caption": "Figure 2:Illustration of the proposed method for advanced instruction-following via reasoning.",
                "position": 332
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/step2.png",
                "caption": "Figure 3:The averaged number ofreasoning tokensandscoresover steps (best viewed magnified).",
                "position": 1405
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/reasoning_increasement.png",
                "caption": "Figure 4:The averaged frequency change of keyword tokens of DeepSeek-Qwen1.5B, DeepScaleR-1.5B, Qwen2.5-1.5B-Instruct, and Qwen2.5-7B-Instruct before/after RL (best viewed magnified).",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/keepratio-reward-vs-steps2.png",
                "caption": "Figure 5:Theratioof samples kept by superior CoT and thetotal rewardover steps of Qwen2.5-7B-Instruct (best viewed magnified).",
                "position": 1659
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/response_length.png",
                "caption": "Figure 35:Training dynamics across model families: Qwen2.5-7B, LLaMA3.1-8B, and Ministral-8B (best viewed magnified).",
                "position": 10152
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/total_length.png",
                "caption": "",
                "position": 10162
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/pretrain_loss.png",
                "caption": "",
                "position": 10169
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/kl.png",
                "caption": "",
                "position": 10175
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/reward_format.png",
                "caption": "",
                "position": 10182
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/reward_accuracy.png",
                "caption": "",
                "position": 10188
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/policy.png",
                "caption": "",
                "position": 10195
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics/return.png",
                "caption": "",
                "position": 10201
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/response_length.png",
                "caption": "Figure 36:Training dynamics across model families: DeepSeek-Qwen1.5B, DeepscaleR-1.5B, and DeepSeek-Qwen7B (best viewed magnified).",
                "position": 10208
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/total_length.png",
                "caption": "",
                "position": 10218
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/pretrain_loss.png",
                "caption": "",
                "position": 10225
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/KL.png",
                "caption": "",
                "position": 10231
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/reward_format.png",
                "caption": "",
                "position": 10238
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/reward_accuracy.png",
                "caption": "",
                "position": 10244
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/policy_loss.png",
                "caption": "",
                "position": 10251
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-ds/return.png",
                "caption": "",
                "position": 10257
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/response_length.png",
                "caption": "Figure 37:Training dynamics on Qwen-2.5 models (1.5B/7B-Instruct) and ablation studies (best viewed magnified).",
                "position": 10264
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/total_length.png",
                "caption": "",
                "position": 10274
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/ptx.png",
                "caption": "",
                "position": 10281
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/kl.png",
                "caption": "",
                "position": 10287
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/reward_format.png",
                "caption": "",
                "position": 10294
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/reward_accuracy.png",
                "caption": "",
                "position": 10300
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/policy_loss.png",
                "caption": "",
                "position": 10307
            },
            {
                "img": "https://arxiv.org/html/2506.01413/extracted/6501056/training_dynamics-all/return.png",
                "caption": "",
                "position": 10314
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]