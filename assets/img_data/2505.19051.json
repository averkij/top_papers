[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x1.png",
                "caption": "Figure 1:Average improvement over uniform sampling across six tasks vs. runtime. The model used is Llama2-7B[Touvron et¬†al.,2023], and the training dataset is Tulu V2[Ivison et¬†al.,2023]. The annotation ‚ÄúM/N‚Äù indicates that the method selected M samples from a pool of size N. Further details are provided in Section5.",
                "position": 179
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x2.png",
                "caption": "Figure 2:(Left) Distribution of unconstrained weights, (Middle) Distribution of robust weights forŒª=0.02ùúÜ0.02\\lambda\\mathbin{=}0.02italic_Œª = 0.02, and (Right) validation loss during training with different variants in the running experiment setting. Robust weights are found by minimizing Objective7using the SLSQP algorithm[Kraft,1988]implemented in the SciPy library[Virtanen et¬†al.,2020].",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x2.png",
                "caption": "",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x3.png",
                "caption": "",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x4.png",
                "caption": "",
                "position": 283
            }
        ]
    },
    {
        "header": "4Efficient Influence Distillation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x5.png",
                "caption": "Figure 3:(Left) Effect of the number of landmarks on the performance of Influence Distillation across six tasks using Llama2-7B. (Right) MMLU accuracy of Influence Distillation on Llama2-7B across different pool sizes and number of selected samples.",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x5.png",
                "caption": "",
                "position": 1000
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x6.png",
                "caption": "",
                "position": 1004
            }
        ]
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGradient Analysis During Training",
        "images": []
    },
    {
        "header": "Appendix BLinear Model Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x7.png",
                "caption": "Figure 4:Average gradient cosine similarity on unseen samples from Tulu V2 (top) and BBH (bottom) across checkpoints.",
                "position": 2012
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x8.png",
                "caption": "",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x9.png",
                "caption": "Figure 5:Correlation between gradient norm and number of label tokens, across checkpoints on four datasets.",
                "position": 2374
            }
        ]
    },
    {
        "header": "Appendix CAdam Optimizer",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x10.png",
                "caption": "Figure 6:(Left) Distribution of theoretical robust weights for the linear case withœµ=10‚àí4italic-œµsuperscript104\\epsilon=10^{-4}italic_œµ = 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT, and (Right) validation loss during training with different variants in the running experiment setting.",
                "position": 2521
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x10.png",
                "caption": "",
                "position": 2524
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x11.png",
                "caption": "",
                "position": 2528
            }
        ]
    },
    {
        "header": "Appendix DProof of Theorem4.1",
        "images": []
    },
    {
        "header": "Appendix EDataset and Model Details",
        "images": []
    },
    {
        "header": "Appendix FEmbeddings Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x12.png",
                "caption": "Figure 7:(Left) Gradient direction recovery vs number of landmarks, when different proxy embdeding functions are used, and (Right) gradient direction recovery when the actual gradients are used as an ideal embedding.",
                "position": 3022
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x12.png",
                "caption": "",
                "position": 3025
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x13.png",
                "caption": "",
                "position": 3029
            }
        ]
    },
    {
        "header": "Appendix GAn Active-Set Solution",
        "images": []
    },
    {
        "header": "Appendix HFirst- vs Second-Order Influence Distillation",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x14.png",
                "caption": "Figure 8:Ratio of second- to first-order terms for Qwen-2.5 1.5B across learning rates on two target datasets.",
                "position": 3427
            }
        ]
    },
    {
        "header": "Appendix IProjection Details",
        "images": []
    },
    {
        "header": "Appendix JWeighted Training Loss",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19051/x15.png",
                "caption": "Figure 9:Average gradient cosine similarity on unseen samples from GSM8k (top) and SQuAD (bottom) across checkpoints.",
                "position": 3512
            },
            {
                "img": "https://arxiv.org/html/2505.19051/x16.png",
                "caption": "",
                "position": 3516
            }
        ]
    },
    {
        "header": "Appendix KDiffered Figures",
        "images": []
    }
]