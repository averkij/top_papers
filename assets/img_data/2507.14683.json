[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/github.png",
                "caption": "",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/hf.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/website.png",
                "caption": "",
                "position": 125
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Enhancing Math Reasoning Capabilities of Language Models via Supervised Fine-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14683/x1.png",
                "caption": "Figure 1:Length distribution for of datasets.",
                "position": 436
            }
        ]
    },
    {
        "header": "4Boosting Reasoning Performance and Efficiency with Reinforcement Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/Fig1.png",
                "caption": "Figure 2:Initial composition distribution. Big-Math comprises HARP and reformulated machine outputs(Albalak et al.,2025); Skywork-OR1-RL-Data(He et al.,2025a)contains only maths.",
                "position": 693
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/Fig1.png",
                "caption": "Figure 2:Initial composition distribution. Big-Math comprises HARP and reformulated machine outputs(Albalak et al.,2025); Skywork-OR1-RL-Data(He et al.,2025a)contains only maths.",
                "position": 696
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/Fig2.png",
                "caption": "Figure 3:Final composition distribution. Data are filtered using the inclusion-exclusion criteria described in Section4.1and the illustration shown in Figure4.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/Fig3.png",
                "caption": "Figure 4:Inclusion-Exclusion Criteria. Overview of the filtering strategy used to construct the final training dataset, consisting of 62,118 problems selected from an initial pool of 1M candidates drawn from four different data sources. The filtering process, which applies criteria such as de-duplication, difficulty-based pruning, and verifiability constraints, results in the exclusion of approximately 94% of the original data.",
                "position": 724
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/mean_response_len.png",
                "caption": "Figure 5:Average token length of model responses computed across all rollout attempts, evaluated over 64 independent runs on the corresponding test sets. This analysis includes both correct and incorrect answers, highlighting overall response efficiency regardless of correctness.",
                "position": 1034
            },
            {
                "img": "https://arxiv.org/html/2507.14683/x2.png",
                "caption": "Figure 6:The training process is more stable with repetition penalty.",
                "position": 1053
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/correct_response_len.png",
                "caption": "Figure 7:Average token count of model responses conditioned on correct answers, computed over all rollouts and averaged across 64 runs on the respective test sets. Unlike Figure5, this analysis considers only responses that received correct reward signals, highlighting that correct, rewarded outputs appear more efficient.",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/sections/figures/chapter4/correct_response_len.png",
                "caption": "Figure 7:Average token count of model responses conditioned on correct answers, computed over all rollouts and averaged across 64 runs on the respective test sets. Unlike Figure5, this analysis considers only responses that received correct reward signals, highlighting that correct, rewarded outputs appear more efficient.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2507.14683/x3.png",
                "caption": "Figure 8:RL benefits from a more accurate verifier, highlighting the critical role of verifier quality in enabling robust training. High-quality verification ensures that reward signals align more closely with ground truth correctness, reducing noise and inconsistencies during optimization.",
                "position": 1083
            },
            {
                "img": "https://arxiv.org/html/2507.14683/x4.png",
                "caption": "(a)AIME24",
                "position": 1226
            },
            {
                "img": "https://arxiv.org/html/2507.14683/x4.png",
                "caption": "(a)AIME24",
                "position": 1229
            },
            {
                "img": "https://arxiv.org/html/2507.14683/x5.png",
                "caption": "(b)AIME25",
                "position": 1234
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/7b_performance_training.png",
                "caption": "Figure 10:The model’s performance steadily improves throughout the training process.",
                "position": 1285
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/7b_response_length.png",
                "caption": "Figure 11:Response length trend during two-stage training. Under the 16K generation cap, the model generates relatively shorter responses. Once the limit is increased to 32K, the model begins producing significantly longer outputs.",
                "position": 1299
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/7b_one_stage.png",
                "caption": "Figure 12:Performance trend of the model trained using a single-stage 32K max context length schema.",
                "position": 1305
            },
            {
                "img": "https://arxiv.org/html/2507.14683/extracted/6634854/figures/robust_evaluation.png",
                "caption": "Figure 13:Evaluation stability assessment: 64 repeated evaluations ofMiroMind-M1-RL-7Bmodel performance on AIME24.",
                "position": 1311
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]