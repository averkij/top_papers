[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08377/x1.png",
                "caption": "Figure 1:UniVideois a unified system that canunderstandmulti-modal instructions andgeneratevideo content.\nMore videos are available onproject website.",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08377/x2.png",
                "caption": "Figure 2:Model architecture.UniVideois a dual-stream model consisting of an MLLM for understanding and an MMDiT module for generation. While prior work such as Qwen-Image and OmniGen2, explores a similar idea in the image domain, our model generalizes this design to video.",
                "position": 132
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08377/x3.png",
                "caption": "Figure 3:UniVideoleverages the MLLM stream to understand and interpret user intent from complex multimodal prompts that cannot be handled by the DiT alone. For example, users can provide diagrams or visual annotations to guide video generation without writing dense textual prompts.",
                "position": 154
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08377/x4.png",
                "caption": "Figure 4:Qualitative comparisonofUniVideowith SoTA Task Specific Experts onIn Context GenerationandIn Context Editingtasks.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x5.png",
                "caption": "Figure 5:Zero-Shot Generalization. We demonstrate two type of generalization. (i)UniVideowas not trained on General Free-form Video Editing data. It transfers this ability from diverse image editing data to the video domain through joint training with in-context video generation and editing data (limited to ID deletion, swapping, addition, and stylization), enabling it to handle previously unseen video editing instructions. (ii)UniVideocan also generalize to novel task compositions, even though it was not explicitly trained on such compositions.",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x6.png",
                "caption": "Figure 6:Qualitative results ofUniVideowith visual prompt inputs.We illustrate two types of visual prompts: in the first three examples, annotations are drawn on a canvas, while in the last example, the annotation is drawn directly on an input image.",
                "position": 1057
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CLimitation and Future Work",
        "images": []
    },
    {
        "header": "Appendix DTraining Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08377/x7.png",
                "caption": "Figure 7:Construction pipeline of in-context video generation test set.",
                "position": 2495
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x8.png",
                "caption": "Figure 8:Example of single-ID test case in in-context video generation test set.",
                "position": 2501
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x9.png",
                "caption": "Figure 9:Example of multi-ID test case in in-context video generation test set.",
                "position": 2507
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x10.png",
                "caption": "Figure 10:Example of ID insertion test case.",
                "position": 2520
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x11.png",
                "caption": "Figure 11:Example of ID swap test case.",
                "position": 2526
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x12.png",
                "caption": "Figure 12:Example of ID deletion test case.",
                "position": 2532
            },
            {
                "img": "https://arxiv.org/html/2510.08377/x13.png",
                "caption": "Figure 13:Example of stylization test case.",
                "position": 2538
            }
        ]
    },
    {
        "header": "Appendix EEvaluation Benchmark",
        "images": []
    }
]