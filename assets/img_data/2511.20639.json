[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20639/plots/github_logo.png",
                "caption": "",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2511.20639/x1.png",
                "caption": "Figure 1:Evaluation ofLatentMASacross (i) accuracy performance (%), (ii) inference speed (times(s)/run), and (ii) token usage (per token) over 9 benchmarks and 3 LLM model scales under the Hierarchical MAS setting. LatentMAS consistently improves system-level reasoning accuracy while substantially reducing computational overhead compared with single model and text-based MAS.",
                "position": 203
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20639/plots/idea-bulb.png",
                "caption": "",
                "position": 222
            }
        ]
    },
    {
        "header": "2Preliminary and Notations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20639/x2.png",
                "caption": "Figure 2:Illustration of sequential and hierarchical MAS.",
                "position": 290
            }
        ]
    },
    {
        "header": "3Building a Latent Collaborative Multi-Agent System",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20639/x3.png",
                "caption": "Figure 3:Overview of LatentMAS.Each LLM agent in the system first generates latent thoughts through last-layer hidden states, then transfers information layer-wise via shared latent working memory stored in KV-caches, enabling completely system-wide latent collaboration.",
                "position": 308
            }
        ]
    },
    {
        "header": "4Empirical Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20639/x4.png",
                "caption": "Figure 4:Efficiency gains of LatentMAS over single model and TextMAS under the sequential MAS setting.Left:LatentMAS achieves substantially faster end-to-end inference, even though all baselines are accelerated with vLLM backend.Right:LatentMAS requires far fewer system-wise token usage.",
                "position": 1662
            },
            {
                "img": "https://arxiv.org/html/2511.20639/x5.png",
                "caption": "Figure 5:Illustration of the semantic meaning encoded by latent thoughts in LatentMAS.Newly generated latent thought embeddings in LatentMAS largely cover the embedding space of text-based generated tokens, indicating semantic consistency and greater expressive capacity than discrete text.",
                "position": 1686
            },
            {
                "img": "https://arxiv.org/html/2511.20639/x6.png",
                "caption": "Figure 6:Effectiveness of the input-output alignmentWaW_{a}on MedQA.Unaligned output embeddings (hth_{t}) drift away from the original input embeddings (ete_{t}), while the aligned vectors (et+1e_{t+1}) realign withete_{t}, demonstrating thatWaW_{a}preserves embedding-space structure and prevents representation drift.",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2511.20639/x7.png",
                "caption": "Figure 7:Downstream performance before/after applying the input-output alignmentWaW_{a}.",
                "position": 1707
            },
            {
                "img": "https://arxiv.org/html/2511.20639/x7.png",
                "caption": "Figure 7:Downstream performance before/after applying the input-output alignmentWaW_{a}.",
                "position": 1710
            },
            {
                "img": "https://arxiv.org/html/2511.20639/x8.png",
                "caption": "Figure 8:Effectiveness of different latent step depths of LatentMAS on downstream performance.",
                "position": 1715
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AInput-Output Alignment in LatentMAS",
        "images": []
    },
    {
        "header": "Appendix BTheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix CExperiment Setups",
        "images": []
    },
    {
        "header": "Appendix DCase Study",
        "images": []
    },
    {
        "header": "Appendix EPrompt Template for LatentMAS",
        "images": []
    }
]