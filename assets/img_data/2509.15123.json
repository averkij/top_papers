[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15123/figures/H.png",
                "caption": "Figure 1:(a) Overview of our RGB-only supervised camera parameter optimization. (b) Front view of the3D Gaussian fieldreconstructed by our camera estimates at timett. (c)2D renderings(RGB and depth) at timettwith quantitative metrics. Our optimization is not only significantly more efficient and accurate, but also avoids overfitting the reconstruction to specific viewpoints.Record3Disa mobile appthat factory-calibrates the intrinsic and uses LiDAR sensors to collect metric depth for camera pose estimates, thus does not have valid runtime.",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15123/figures/filters2.png",
                "caption": "Figure 2:Patch-wise tracking filters.(1) PartitioningF0F_{0}into patches of sizew×ww\\times w, the patch-wise texture filter computes the texture map𝐓0\\mathbf{T}_{0}and marks the high-texture patches ingray; (2) Within each high-texture patch, the patch-wise gradient filter selects one potential tracking point with the highest gradient norm. (3) The visibility filter removes the entire trajectory of a point if it becomes invisible at any time (∙\\bullet,■\\blacksquare,▲\\blacktriangle→\\rightarrowkept trajectories;∘\\circ,□\\square,△\\triangle→\\rightarrowremoved trajectories); (4) The patch-wise distribution filter only keeps the one with the largest gradient norm when multiple trajectories fall into the same patch.𝐏\\mathbf{P}and𝐈\\mathbf{I}are the location and index of trajectory, and↔\\leftrightarrowis the trajectory range.",
                "position": 255
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/cam.png",
                "caption": "Figure 3:Outlier-aware Joint Optimization.∙\\bulletrepresents𝐏t\\mathbf{P}_{t}and𝐏t′\\mathbf{P}_{t^{\\prime}}on each frame. The static samplesp′⁣c​a​l​ip^{\\prime cali}andp′′⁣c​a​l​ip^{\\prime\\prime cali}can establish concrete triangulation relations with their corresponding𝐏t\\mathbf{P}_{t},𝐏t′\\mathbf{P}_{t^{\\prime}}, and cameras, resulting in lowerγ′\\gamma^{\\prime}andγ′′\\gamma^{\\prime\\prime}. In contrast, the dynamic samplep′′′⁣c​a​l​ip^{\\prime\\prime\\prime cali}exhibits the opposite behavior.",
                "position": 336
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/cam.png",
                "caption": "Figure 3:Outlier-aware Joint Optimization.∙\\bulletrepresents𝐏t\\mathbf{P}_{t}and𝐏t′\\mathbf{P}_{t^{\\prime}}on each frame. The static samplesp′⁣c​a​l​ip^{\\prime cali}andp′′⁣c​a​l​ip^{\\prime\\prime cali}can establish concrete triangulation relations with their corresponding𝐏t\\mathbf{P}_{t},𝐏t′\\mathbf{P}_{t^{\\prime}}, and cameras, resulting in lowerγ′\\gamma^{\\prime}andγ′′\\gamma^{\\prime\\prime}. In contrast, the dynamic samplep′′′⁣c​a​l​ip^{\\prime\\prime\\prime cali}exhibits the opposite behavior.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/timetrend.png",
                "caption": "Figure 4:Runtime Trends.As the frame count increases, our runtime grows almost linearly, whereasCOLw/o​m​a​s​k\\texttt{COL}^{w/omask}scales exponentially. The runtime of casualSAM is too large to fit in this figure. The complete runtime is inLABEL:tab:average_time, andsection˜E.1.1(table˜9,table˜10,table˜11).",
                "position": 344
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15123/figures/dvis.png",
                "caption": "Figure 5:Qualitative NVS Results on DAVISdavis.Our performance is the best because of our accurate camera estimates. More are insection˜E.2.2(fig.˜14,fig.˜15,fig.˜16, andfig.˜17).",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/dvis.png",
                "caption": "Figure 5:Qualitative NVS Results on DAVISdavis.Our performance is the best because of our accurate camera estimates. More are insection˜E.2.2(fig.˜14,fig.˜15,fig.˜16, andfig.˜17).",
                "position": 759
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/mpivis.png",
                "caption": "Figure 6:Qualitative Results of Camera Pose on MPI-Sintelmpi-sintel.–represents our camera estimates;–represents the GT. Our estimated camera trajectories almost perfectly align with the GT.",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/ivis.png",
                "caption": "Figure 7:Qualitative NVS Results on iPhoneiphone.Our method outperforms other SOTA RGB-only supervised approaches and even surpasses LiDAR-basedRecord3Dwhen the movement in scenes with large motion (top row). More are insection˜E.2.2(fig.˜11,fig.˜12, andfig.˜13).",
                "position": 904
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/nerfdsvis.png",
                "caption": "Figure 8:Qualitative NVS results on NeRF-DSnerfds.Our renderings are the most plausible. More are insection˜E.2.2(fig.˜18).",
                "position": 913
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/nerfdsvis.png",
                "caption": "Figure 8:Qualitative NVS results on NeRF-DSnerfds.Our renderings are the most plausible. More are insection˜E.2.2(fig.˜18).",
                "position": 915
            }
        ]
    },
    {
        "header": "5Conclusion and Limitation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15123/figures/failure.png",
                "caption": "Figure 9:Failure cases of ours and casualSAM on MPI-Sintelmpi-sintel.",
                "position": 1092
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADynamic Scene Optimization",
        "images": []
    },
    {
        "header": "Appendix BDerivation of Cauchy Negative-log-likelihood",
        "images": []
    },
    {
        "header": "Appendix CDatasets",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.15123/figures/track.png",
                "caption": "Figure 10:Trajectory Comparisons on the NeRF-DSnerfdsDataset.In each scenario, top row→\\rightarrowF0F_{0}; bottom row→\\rightarrowF247F_{247};w/ow/opatch-wise filters→\\rightarrowraw CoTrackercotracker;w/w/patch-wise filters→\\rightarrowOurs. It is easy to see our proposed method avoids the inaccurate trajectories in the low-texture regions, whereas the trajectories of the points in the low-texture regions tracked by raw CoTrackercotrackerare extremely unreliable.",
                "position": 2546
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/imore1.png",
                "caption": "Figure 11:More Qualitative NVS Results on iPhoneiphone- Part 1.Our renderings exhibit higher fidelity and more accurate geometry compared to other RGB-only supervised methods. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-basedRecord3Dapp.",
                "position": 2557
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/imore2.png",
                "caption": "Figure 12:More Qualitative NVS Results on iPhoneiphone- Part 2.Our renderings exhibit higher fidelity and more accurate geometry compared to other RGB-only supervised methods. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-basedRecord3Dapp.",
                "position": 2560
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/imore3.png",
                "caption": "Figure 13:More Qualitative NVS Results on iPhoneiphone- Part 3.Our renderings exhibit higher fidelity and more accurate geometry compared to other RGB-only supervised methods. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-basedRecord3Dapp.",
                "position": 2563
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/dmore1.png",
                "caption": "Figure 14:More Qualitative NVS Results on DAVISdavis- Part 1.Our renderings exhibit higher fidelity compared to other RGB-only supervised methods.",
                "position": 2566
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/dmore2.png",
                "caption": "Figure 15:More Qualitative NVS Results on DAVISdavis- Part 2.Our renderings exhibit higher fidelity compared to other RGB-only supervised methods.",
                "position": 2569
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/dmore3.png",
                "caption": "Figure 16:More Qualitative NVS Results on DAVISdavis- Part 3.Our renderings exhibit higher fidelity compared to other RGB-only supervised methods.",
                "position": 2572
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/dmore4.png",
                "caption": "Figure 17:More Qualitative NVS Results on DAVISdavis- Part 4.Our renderings exhibit higher fidelity compared to other RGB-only supervised methods.",
                "position": 2575
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/nerfdsmore1.png",
                "caption": "Figure 18:More Qualitative NVS Results on NeRF-DSnerfds.Our renderings exhibit higher fidelity compared to other RGB-only supervised methods.",
                "position": 2578
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/field3.png",
                "caption": "Figure 19:Optimized 3D Gaussian Fields on iPhoneiphone- Part 1.Our reconstructed 3D Gaussian Fields are more geometrically accurate compared to the ones of other RGB-only supervised methods, which demonstrates our camera estimates are more accurate. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-basedRecord3Dapp.",
                "position": 2588
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/field1.png",
                "caption": "Figure 20:Optimized 3D Gaussian Fields on iPhoneiphone- Part 2.Our reconstructed 3D Gaussian Fields are more geometrically accurate compared to the ones of other RGB-only supervised methods, which demonstrates our camera estimates are more accurate. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-basedRecord3Dapp.",
                "position": 2591
            },
            {
                "img": "https://arxiv.org/html/2509.15123/figures/field2.png",
                "caption": "Figure 21:Optimized 3D Gaussian Fields on iPhoneiphone- Part 3.Our reconstructed 3D Gaussian Fields are more geometrically accurate compared to the ones of other RGB-only supervised methods, which demonstrates our camera estimates are more accurate. Besides, our performance is comparable with, or even better than, the ones of the LiDAR-basedRecord3Dapp.",
                "position": 2594
            }
        ]
    },
    {
        "header": "Appendix EMore Results",
        "images": []
    }
]