[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17383/x1.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/reference.png",
                "caption": "(a)Reference Input",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/reference.png",
                "caption": "(a)Reference Input",
                "position": 144
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/animateanyone.png",
                "caption": "(b)AnimateAnyone",
                "position": 149
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/mimicmotion.png",
                "caption": "(c)MimicMotion",
                "position": 154
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Intro/ours.png",
                "caption": "(d)Ours",
                "position": 160
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3System Setting",
        "images": []
    },
    {
        "header": "4Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17383/x2.png",
                "caption": "Figure 3:Training pipeline for AnchorCrafter: Based on a video diffusion model, AnchorCrafter injects human and multi-view object references into the video via HOI-appearance perception. The motion is controlled through HOI-motion injection, with the training objective reweighted in the HOI region.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2411.17383/x3.png",
                "caption": "Figure 4:HOI-appearance perception: The feature of the target objectfOsubscriptùëìùëÇf_{O}italic_f start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPTis extracted through multi-view object feature fusion and combined with the human reference featurefHsubscriptùëìùêªf_{H}italic_f start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPTwithin a human-object dual adapter to achieve improved disentanglement results.",
                "position": 307
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17383/x4.png",
                "caption": "Figure 5:Qualitative comparisons with other methods. Different colored squares highlight the different types of generation artifacts. In the results of MimicMotion and AnimateAnyone, the objects fail to maintain their appearance and cannot move in sync with the hands, while AnyV2V generates apparent artifacts in the edited videos. The combined results of AnyDoor and MimicMotion demonstrate a lack of preservation in object details.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/noadapter_3.png",
                "caption": "(a)w/o Dual Adapter",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/noadapter_3.png",
                "caption": "(a)w/o Dual Adapter",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/noadapter_3.png",
                "caption": "(a)w/o Dual Adapter",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/noobj.png",
                "caption": "(b)w/o Multi-View Obj.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2411.17383/x5.png",
                "caption": "(c)Ours",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/mouse_nohand.png",
                "caption": "(d)w/o 3D Hand Mesh",
                "position": 627
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/mouse_nohand.png",
                "caption": "(d)w/o 3D Hand Mesh",
                "position": 630
            },
            {
                "img": "https://arxiv.org/html/2411.17383/extracted/6025749/image/Ablation/mouse_noloss.png",
                "caption": "(e)w/o Re. Loss",
                "position": 635
            },
            {
                "img": "https://arxiv.org/html/2411.17383/x6.png",
                "caption": "(f)Ours",
                "position": 640
            },
            {
                "img": "https://arxiv.org/html/2411.17383/x7.png",
                "caption": "Figure 7:Limitions.",
                "position": 746
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]