[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06373/x1.png",
                "caption": "",
                "position": 64
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06373/x2.png",
                "caption": "Figure 2:The overall framework ofVG-Refiner. In our reward design, we consider the quality of tool feedbackIoUt\\text{IoU}_{t}. For different circumstances, we adopt different levels of reward to encourage the model to refine the toolâ€™s incorrect results or accept the reliable results. We use GRPO to optimize the policy model, which produces variousGGrollouts during training. After the think process, the model queries a referring visual toolkit for additional reference outputs. In GRPO, KL divergence constrains strategy deviation from the frozen reference model to ensure stable optimization.",
                "position": 112
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06373/x3.png",
                "caption": "Figure 3:User prompt for the PiTER evaluation process. This prompt is shared across all model types, requiring the model to produce grounding results in a JSON format through a single-stage conversation, without any CoT reasoning or tool interaction. The placeholder{Question}\\{\\text{Question}\\}is replaced with the referring expression, while{tool results}\\{\\text{tool results}\\}is substituted with the feedback from either a strong or weak tool corresponding to the given question.",
                "position": 140
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06373/x4.png",
                "caption": "Figure 4:Visualization of VG-Refiner handling three representative types of tool-induced errors in TrRGR. The first two grounding error categories often occur in a good tool, EVF-SAM[evf-sam], whereas the third occurs in the not fine-tuned Grounding DINO T[grounding-dino].",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2512.06373/x5.png",
                "caption": "Figure 5:Visualization of the overall reasoning paradigm, first performing self-thinking and then re-thinking based on the tool outputs.",
                "position": 973
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6The Choice of Tools",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06373/x6.png",
                "caption": "Figure 6:(a)The common three errors in the referring expression comprehension task. (b) The output mask of the RES model with expression: A lady is walking away from the surfer.",
                "position": 1126
            }
        ]
    },
    {
        "header": "7More Benchmark Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.06373/x7.png",
                "caption": "Figure 7:The reward changes with the training steps. We show the mean value across the batch.",
                "position": 1375
            },
            {
                "img": "https://arxiv.org/html/2512.06373/x8.png",
                "caption": "",
                "position": 1379
            },
            {
                "img": "https://arxiv.org/html/2512.06373/x9.png",
                "caption": "Figure 8:More visualization results.",
                "position": 1383
            }
        ]
    },
    {
        "header": "8Training Details",
        "images": []
    },
    {
        "header": "9PiTER protocol Analysis",
        "images": []
    },
    {
        "header": "10More Case Study",
        "images": []
    }
]