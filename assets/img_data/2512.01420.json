[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01420/figures/preliminary.png",
                "caption": "Figure 1:Illustration of Model Drifting Problem.(a) Organizations may replace the source model with a more capable, cost-efficient, or open-source alternative.\nHowever, directly transferring the source-engineered prompt (e.g.GPT-4o) to a stronger target model (e.g.o3) yields only 92.27%\naccuracy on the HumanEval dataset, far below the target model‚Äôs achievable 98.37%, revealing that prompts engineered or optimized for one model do not reliably generalize to another.\n(b) Cross-model evaluation on HumanEval demonstrates model drifting: prompts designed for a source model frequently degrade when applied to different target models.",
                "position": 239
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Problem Formulation",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01420/figures/method.png",
                "caption": "Figure 2:Overview of the proposed PromptBridge framework.The framework operates in two stages: 1) Calibration, where Model-Adaptive Reflective Prompt Evolution (MAP-RPE) optimizes task-specific prompts for each model using quantitative feedback; and 2) Cross-Model Transfer, where pairs of calibrated source‚Äìtarget prompts are used to learn a transferable prompt mapping functionùíØ\\mathcal{T}. This learned mapping acts as a bridge that enables the system to synthesize target-compatible prompts for unseen tasks, supporting zero-shot prompt transfer without additional training.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/example.png",
                "caption": "Figure 3:A demonstration of PromptBridge.1) shows the prompt template used by the Mapping Extractor for learning transferable mappings between source and target prompts.\n2) presents an example of the learned transferable knowledge, which may contain multiple sections analyzing cross-model prompt differences.\n3) illustrates the prompt template used by the Adapter Model, which applies the learned mapping to adapt prompts for unseen tasks.",
                "position": 433
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01420/figures/swe.png",
                "caption": "Figure 4:Results on SWE-Bench Verified.We useo4-minias the source model and evaluate transferability ono3,Llama-3.1-70B-Instruct, andGPT-4oas target models. On the source, the resolved rate with the default prompt is 38.60%, which increases to 42.20% after calibration with the optimized prompt.\nDirect Transfer refers to directly applying the optimized prompt fromo4-minito the other models without further adaptation.",
                "position": 708
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/terminal_gpt4o.png",
                "caption": "Figure 5:Results on Terminal-Bench.We useGPT-4oas the source model and evaluate transferability ono3ando4-minias target models. On the source, the accuracy with the default prompt is 15%, which increases to 18.75% after calibration with the optimized prompt.\nOriginal denotes the baseline performance using the default prompt template from Terminus. Direct Transfer refers to directly applying the optimized prompt fromGPT-4oto the other models without further adaptation.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/humaneval_calibration.png",
                "caption": "Figure 6:Ablation study on the number of calibration questions.Pass@1 accuracy on HumanEval using different calibration methods to derive the optimal prompt for each model. Default Prompt denotes the original designed prompt for solving coding tasks. Gen-Select refers to generating multiple prompt candidates and selecting the best-performing one.",
                "position": 1177
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Appendix Contents",
        "images": []
    },
    {
        "header": "Appendix APreliminary Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01420/figures/qwen-llama.png",
                "caption": "Figure 7:Cross-model frozen prompt transfer on xCodeEval (left) and HumanEval (right).Each row shows the source model and each column the target model. Darker diagonal cells indicate the accuracy obtained when applying a model‚Äôs own optimized prompt, while lighter off-diagonal cells show performance when transferring that optimized prompt to a different model without further adaptation. The consistent diagonal‚Äìoff-diagonal gap reveals substantial model drifting: prompts carefully optimized for one model fail to generalize to others, even within the same family.",
                "position": 1240
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/delta_cosine_heatmap.png",
                "caption": "(a)GPT4o‚ÜíLlama-3.1-70B-Instruct.",
                "position": 1267
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/delta_cosine_heatmap.png",
                "caption": "(a)GPT4o‚ÜíLlama-3.1-70B-Instruct.",
                "position": 1270
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/delta_cosine_heatmap_o3.png",
                "caption": "(b)GPT4o‚Üío3.",
                "position": 1275
            }
        ]
    },
    {
        "header": "Appendix BExperimental Settings",
        "images": []
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.01420/figures/terminal.png",
                "caption": "Figure 9:Results on Terminal-Bench.",
                "position": 1826
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/consistency_summary_GPT-4o_o3.png",
                "caption": "Figure 10:Transfer Mapping Consistency Across Runs.Each heatmap shows the pairwise similarity of transfer mappings obtained from five independent runs under identical configurations.\nThe top panel reports consistency when transferring fromGPT-4otoo3, while the bottom panel reports transfer fromGPT-4otoLlama-3.1-70B-Instruct. Higher off-diagonal values indicate stronger stability of the transfer effects across repeated runs.",
                "position": 1948
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/consistency_summary_GPT-4o_Llama3.1-70B-Instruct.png",
                "caption": "",
                "position": 1952
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/consistency_prompt_GPT-4o_o3_coding.png",
                "caption": "Figure 11:Optimized Prompt Consistency Across Runs.Each heatmap visualizes the pairwise similarity among optimized prompts obtained from five independent runs under identical configurations.\nThe top panel corresponds to transfer fromGPT-4otoo3, and the bottom panel corresponds to transfer fromGPT-4otoLlama-3.1-70B-Instruct.\nDiagonal entries denote perfect self-similarity (1.0), while higher off-diagonal values reflect greater stability of the optimized prompt across runs.",
                "position": 1959
            },
            {
                "img": "https://arxiv.org/html/2512.01420/figures/consistency_prompt_GPT-4o_Llama3.1-70B-Instruct_coding.png",
                "caption": "",
                "position": 1963
            }
        ]
    },
    {
        "header": "Appendix DDiscussions",
        "images": []
    },
    {
        "header": "Appendix EPrompt Template",
        "images": []
    }
]