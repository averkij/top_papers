[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17450/x1.png",
                "caption": "(a)One-shot Planning: Errors accumulate due to lack of correction.",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2511.17450/x1.png",
                "caption": "(a)One-shot Planning: Errors accumulate due to lack of correction.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2511.17450/x2.png",
                "caption": "(b)Iterative Generation: Inefficient repeated generations.",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2511.17450/x3.png",
                "caption": "(c)SketchVerify: A multimodal verifier ranks control plans using warped video sketches before synthesis.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17450/x4.png",
                "caption": "Figure 2:Overview of our framework.Given a prompt and initial frame, we (1) decompose instructions and segment movable objects, (2) sample and verify candidate trajectories using lightweight video sketches scored by a multimodal verifier and (3) synthesize the final video using a trajectory-conditioned diffusion model. We provide more detail about the MLLM verifier inFig.3.",
                "position": 183
            }
        ]
    },
    {
        "header": "3Method – SketchVerify",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17450/x5.png",
                "caption": "Figure 3:Illustration of MLLM verifier.\nGiven a video sketch and sub-instruction, the MLLM outputs semantic and physics scores used to rank candidate trajectories.",
                "position": 331
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17450/x6.png",
                "caption": "Table 1:Comparison onWorldModelBench[Li2025WorldModelBench].\nWe report scores for instruction following, physical law coherence (grouped under “Physics”), and commonsense consistency (“Frame” and “Temporal”), along with an overall sum score aggregating all metrics.\nBest results in each column are highlighted in bold.",
                "position": 357
            },
            {
                "img": "https://arxiv.org/html/2511.17450/x6.png",
                "caption": "Figure 4:Qualitative comparison on four representative domains fromWorldModelBench: Human, Natural, Video Game, and Robotics.\nEach group shows sampled frames from competing models given the same text prompt.\nFrames are uniformly sampled from each generated 81-frame video.",
                "position": 574
            }
        ]
    },
    {
        "header": "5Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17450/x7.png",
                "caption": "Figure 5:Ablation study on verifier modality.\nIntroducing visual input to the verifier significantly improves both instruction following and physical plausibility,\nhighlighting the importance of multimodal grounding for reliable trajectory evaluation.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2511.17450/x8.png",
                "caption": "Figure 6:Ablation on the number of sampled trajectoriesKKduring planning. LargerKKvalues enable stronger verifier-guided selection.K=0K=0denotes the setting without a verifier, which is identical to the VideoMSG baseline.",
                "position": 851
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17450/x9.png",
                "caption": "Figure 7:Qualitative comparison on WorldModelBench.",
                "position": 1121
            },
            {
                "img": "https://arxiv.org/html/2511.17450/x10.png",
                "caption": "Figure 8:Qualitative comparison on PhyWoldBench.",
                "position": 1126
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]