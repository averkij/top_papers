[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/x1.png",
                "caption": "",
                "position": 115
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/x2.png",
                "caption": "Figure 2:Construction pipeline for our benchmark, consisting of four stages: (1) collecting high-quality real and synthetic images, (2) filtering out inappropriate or low-quality samples, (3) performing hierarchical category classification, and (4) generating and validating editing instructions by Gemini and humans.",
                "position": 305
            }
        ]
    },
    {
        "header": "3MultiBanana",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/x3.png",
                "caption": "Figure 3:(Left) Comparison between the statistics of real data only and those after adding synthetic data.\nThe original dataset was biased toward background images, with few person- and object-related samples.\nTo correct this imbalance, we generated additional synthetic images using Nanobanana and ChatGPT-Image-1, focusing on clear subjects such as people, animals, and objects.\nThis significantly increased person- and object-related categories, resulting in a more balanced and comprehensive benchmark.\n(Right) Examples of synthesized images in each category.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x4.png",
                "caption": "",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x5.png",
                "caption": "Figure 4:(Left) Breakdown of the multi-reference tasks.\nThe editing sets were selected to ensure that the number of sets within each task is balanced across different reference counts.\n(Middle) For every X–references task, the dataset contains at least 390 editing sets. Further, each colored task category also includes at least 70 sets, which is larger than the prior work[xia2025dreamomni2].\n(Right) Word cloud generated from all prompts. It primarily consists of terms that describe a wide range of object categories as well as words indicating spatial directions.",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x6.png",
                "caption": "",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x7.png",
                "caption": "",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x8.png",
                "caption": "Figure 5:Changes in scores for each evaluation criterion when varying the number of reference images.\nBoth open-source and closed-source models exhibit a general trend of decreasing all scores as the number of references increases.",
                "position": 650
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x9.png",
                "caption": "Figure 6:Results for diffucult reference combinations.\nFor cross-domain and different scale and view tasks, every model shows lower scores than tasks without such conditions.",
                "position": 664
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/x10.png",
                "caption": "Figure 7:(Left) Failure cases of cross-domain, different scale, rare concept, multilingual text. (Right) Failure cases of multi-references in open-source models. They tend to ignore multiple subjects under high-reference conditions.",
                "position": 695
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "ADetails on Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/figures/example_duplicated_series.png",
                "caption": "Figure 8:Examples of a duplicated synthetic image.",
                "position": 820
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x11.png",
                "caption": "Figure 9:(Left) Number of tasks by reference count. The two-reference tasks contain more samples than the other reference tasks because they include multiple task types.\n(Right) Breakdown of the two-reference tasks. It contains eleven tasks.",
                "position": 823
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x12.png",
                "caption": "",
                "position": 832
            }
        ]
    },
    {
        "header": "BFurther Statistics for MultiBanana",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/x13.png",
                "caption": "Figure 10:Qualitative example of “Hard” category of ImgEdit. These tasks are close to being fully solvable by advanced models such as Nano Banana.",
                "position": 890
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x14.png",
                "caption": "Figure 11:Qualitative example of DreamOmni2 benchmark. These tasks are close to being fully solvable by advanced models such as Nano Banana.",
                "position": 893
            }
        ]
    },
    {
        "header": "CFurther Comparison with Prior Benchmarks",
        "images": []
    },
    {
        "header": "DFurther Discussions for AI and Human Evaluation",
        "images": []
    },
    {
        "header": "EDetailed Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22989/x15.png",
                "caption": "Figure 12:Scores of each model across evaluation metrics for each task by GPT. The horizontal axis denotes the scores for the five evaluation criteria, and the vertical axis denotes the number of reference images.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x16.png",
                "caption": "Figure 13:Scores of each model across evaluation metrics for each task by Gemini. The horizontal axis denotes the scores for the five evaluation criteria, and the vertical axis denotes the number of reference images.",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x17.png",
                "caption": "Figure 14:Average scores of each model across evaluation metrics for each task by GPT and Gemini. The horizontal axis denotes the scores for the five evaluation criteria, and the vertical axis denotes the number of reference images.",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x18.png",
                "caption": "Figure 15:Overall scores of each model for single and two-reference tasks by GPT, Gemini, and their average.",
                "position": 1251
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x19.png",
                "caption": "Figure 16:Overall scores for each difficult reference combination across models. Darker colors represent the average score of tasks that include the corresponding combination, while lighter colors indicate the average score of tasks that do not include it.",
                "position": 3600
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x20.png",
                "caption": "Figure 17:Qualitative example for 4-Object Tasks.",
                "position": 3603
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x21.png",
                "caption": "Figure 18:Qualitative example for 3-Object + Background Tasks.",
                "position": 3606
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x22.png",
                "caption": "Figure 19:Qualitative example for 3-Object + Local Tasks.",
                "position": 3609
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x23.png",
                "caption": "Figure 20:Qualitative example for 3-Object + Global Tasks.",
                "position": 3612
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x24.png",
                "caption": "Figure 21:Qualitative example for Tasks with 8 references.",
                "position": 3615
            },
            {
                "img": "https://arxiv.org/html/2511.22989/figures/agent_matsutani.png",
                "caption": "Figure 22:Example of changes in the Iterative Prompt Refinement (IPR) framework. Comparison and evaluation results between Nano Banana and GPT on the task of generating images from 8 object references according to the instruction prompt.",
                "position": 3668
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x25.png",
                "caption": "Figure 23:Detailed results of multi-reference image generation using the IPR framework with Gemini.",
                "position": 3741
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x26.png",
                "caption": "Figure 24:Detailed results of multi-reference image generation using the CAFG framework with Gemini.",
                "position": 3751
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x27.png",
                "caption": "Figure 25:Detailed results of multi-reference image generation using the SRA framework with Gemini.",
                "position": 3761
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x28.png",
                "caption": "Figure 26:Detailed results of multi-reference image generation using the IPR framework with GPT.",
                "position": 3771
            },
            {
                "img": "https://arxiv.org/html/2511.22989/x29.png",
                "caption": "Figure 27:Detailed results of single and two-reference image generation.",
                "position": 3781
            }
        ]
    },
    {
        "header": "FExtended Related Works",
        "images": []
    },
    {
        "header": "GPrompts",
        "images": []
    }
]