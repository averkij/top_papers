[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16782/x1.png",
                "caption": "Figure 1:Comparative analysis of different synthetic images and their impact on zero-shot quantization-aware training with object detection network Mask R-CNN. It is observed that task-specific calibration set matters and results in better performance on the MS-COCO dataset. Inspired by this, we integrate novel task-specific manners into data synthesis and quantized detection network finetuning processes.",
                "position": 170
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Motivation",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16782/x2.png",
                "caption": "Figure 2:Overall architecture of our method. Our framework comprises: 1) constructing a task-specific condensed calibration set and 2) conducting quantization-aware training with task-specific distillation. See Section3for details.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2507.16782/x3.png",
                "caption": "Figure 3:(a) Images generated byAdaptive Label Samplingon a YOLOv5 detector pre-trained on MS-COCO. (b)Adaptive Label Samplingcan generate a category distribution frequency similar to MS-COCO in a zero-shot setting.",
                "position": 350
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16782/x4.png",
                "caption": "Figure 4:Visualization of images generated by YOLO11 and Transformer-backbone Mask R-CNN. More examples can be found in AppendixE.",
                "position": 698
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Works",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16782/x5.png",
                "caption": "Figure 5:An overview ofAdaptive Label Samplingprocess. We randomly initialize the labely춾춾洧녽\\overline{y}over춾 start_ARG italic_y end_ARGand initialize the input imagex춾춾洧논\\overline{x}over춾 start_ARG italic_x end_ARGusing Gaussian noise. For Every fixed interval, we use a pre-trained object detection model to re-detect objects inx춾춾洧논\\overline{x}over춾 start_ARG italic_x end_ARGand update the targety춾춾洧녽\\overline{y}over춾 start_ARG italic_y end_ARG. In the subsequent iterations,x춾춾洧논\\overline{x}over춾 start_ARG italic_x end_ARGis optimized toward the updated targety춾춾洧녽\\overline{y}over춾 start_ARG italic_y end_ARG. We observe that, over iterations,x춾춾洧논\\overline{x}over춾 start_ARG italic_x end_ARGandy춾춾洧녽\\overline{y}over춾 start_ARG italic_y end_ARGbecome increasingly aligned with each other.",
                "position": 2013
            }
        ]
    },
    {
        "header": "Appendix CAblation Study",
        "images": []
    },
    {
        "header": "Appendix DSample Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16782/x6.png",
                "caption": "Figure 6:(a) Our synthetic condensed calibration set is 1/60 the size of the MS-COCO training set. (b) The training convergence speed can be improved by up to 16칑\\timesbold_칑compared to LSQ.",
                "position": 2579
            },
            {
                "img": "https://arxiv.org/html/2507.16782/x7.png",
                "caption": "Figure 7:Visualization of images composed by different architecture-based object recognition networks.",
                "position": 2589
            },
            {
                "img": "https://arxiv.org/html/2507.16782/x8.png",
                "caption": "Figure 8:A comparison of the image quality generated by various sampling methods.",
                "position": 2602
            },
            {
                "img": "https://arxiv.org/html/2507.16782/x9.png",
                "caption": "Figure 9:Qualitative analysis of object detection performance across different neural networks",
                "position": 2615
            }
        ]
    },
    {
        "header": "Appendix EAdditional Qualitative Results",
        "images": []
    }
]