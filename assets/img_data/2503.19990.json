[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LEGO-Puzzles",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19990/x1.png",
                "caption": "Figure 1:Problem Statistics in LEGO-Puzzles.",
                "position": 197
            },
            {
                "img": "https://arxiv.org/html/2503.19990/x2.png",
                "caption": "Figure 2:Task examples of LEGO-Puzzles. From left to right, the columns represent tasks in Spatial Understanding, Single-Step Sequential Reasoning, and Multi-Step Sequential Reasoning. Note: The questions above are slightly simplified for clarity and brevity.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2503.19990/x3.png",
                "caption": "Figure 3:Data curation pipeline.\nOur pipeline first collects a diverse set of LEGO building instructions to render and extract LEGO images in a unified format. Next, we generate question-answer pairs by using a combination of human annotation and predefined question templates. Finally, we implement three quality control strategies to ensure the accuracy, consistency, and reliability of the data.",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2503.19990/x4.png",
                "caption": "Figure 4:Task-specific template. Our question-answer template includes instructions, questions, and answers. Here, we provide an example from thePositiontask for reference.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Evaluation on LEGO-Puzzles",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19990/x5.png",
                "caption": "Figure 5:Task Similarity Heatmap.The heatmap illustrates the pairwise correlation between tasks in our benchmark, measured using SRCC, PLCC, and R² scores.",
                "position": 1079
            }
        ]
    },
    {
        "header": "5Error Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19990/x6.png",
                "caption": "Figure 6:Visualization of sample failure cases inHeightandOrdering. The Ground Truth answer is marked inblue, while the MLLM’s answer is marked inred. Note: The questions above are slightly simplified for clarity and brevity.",
                "position": 1105
            },
            {
                "img": "https://arxiv.org/html/2503.19990/x7.png",
                "caption": "Figure 7:Qualitative image generation results forRotation*andMultiview*tasks.Note: The questions above are slightly simplified for clarity and brevity.",
                "position": 1126
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]