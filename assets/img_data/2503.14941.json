[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14941/x1.png",
                "caption": "Figure 1:Existing methods for evaluating MLLMs face various challenges. Our proposed UPME framework addresses these limitations by leveraging a peer review mechanism, reducing annotation costs, and aligning closely with human judgment.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14941/x2.png",
                "caption": "Figure 2:The UPME framework consists of three main components:(i)ùëñ(i)( italic_i )Peer Review Mechanism, where two candidate models and one review model are randomly selected from the MLLM pool. The review model generates questions based on a selected image, and candidate models provide responses.(i‚Å¢i)ùëñùëñ(ii)( italic_i italic_i )Vision-Language Judgment Scoring System, which evaluates answers based on textual correctness, visual understanding and reasoning, and image-text correlation.(i‚Å¢i‚Å¢i)ùëñùëñùëñ(iii)( italic_i italic_i italic_i )Dynamic Weight Optimization, ensuring consistency between confidence weights and estimated scores through iterative optimization cycles.",
                "position": 184
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14941/x3.png",
                "caption": "Figure 3:Convergence experiments.",
                "position": 666
            },
            {
                "img": "https://arxiv.org/html/2503.14941/x4.png",
                "caption": "Figure 4:The performance of UPME in different sample size.",
                "position": 796
            }
        ]
    },
    {
        "header": "5Human Preference Alignment Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14941/x5.png",
                "caption": "Figure 5:Model accuracy comparison in peer review framework w/ and w/o UPME, wherePeer Review_Cor.represents the correctness of the original peer review, andUPME_Cor.andUPME_Vis.correspond to the two judgment dimensions of response correctness and visual understanding, introduced insubsection¬†3.2.",
                "position": 857
            },
            {
                "img": "https://arxiv.org/html/2503.14941/x6.png",
                "caption": "(a) Verbosity Bias",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2503.14941/x6.png",
                "caption": "(a) Verbosity Bias",
                "position": 870
            },
            {
                "img": "https://arxiv.org/html/2503.14941/x7.png",
                "caption": "(b) Self Preference",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2503.14941/x8.png",
                "caption": "Figure 7:Case study illustration of UPME. We provide the original human-designed questions and the UPME-generated questions, along with the answer analysis. The upper case presents theDisability of review model, where the review model can not answer the original question itself. The middle case demonstrates cases exhibitingverbosity bias. The bottom case showsself-preference bias.",
                "position": 935
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Model Selection",
        "images": []
    },
    {
        "header": "8Extended Experiment",
        "images": []
    },
    {
        "header": "9More Information about UPME",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.14941/extracted/6270370/imgs/screenshot.png",
                "caption": "Figure 8:Screenshot of human annotation.",
                "position": 2334
            }
        ]
    },
    {
        "header": "10Prompt Template",
        "images": []
    }
]