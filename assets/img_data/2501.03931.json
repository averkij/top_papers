[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x1.png",
                "caption": "Figure 2:Magic Mirror generates dynamic facial motion.ID-Animator[11]and Video Ocean[19]exhibit limited motion range due to a strong identity-preservation constraint. Magic Mirror achieves more dynamic facial expressions while maintaining reference identity fidelity.",
                "position": 246
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x2.png",
                "caption": "Figure 3:Overview of Magic Mirror.The framework employs a dual-branch feature extraction system with ID and face perceivers, followed by a cross-modal adapter¬†(illustrated inFig.4) for DiT-based video generation. By optimizing trainable modules marked by the flame, our method efficiently integrates facial features for controlled video synthesis while maintaining model efficiency.",
                "position": 286
            }
        ]
    },
    {
        "header": "3Magic Mirror",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x3.png",
                "caption": "Figure 4:Cross-modal adapterin DiT blocks, featuring Conditioned Adaptive Normalization (CAN) for modal-specific feature modulation and decoupled attention integration.",
                "position": 386
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x4.png",
                "caption": "Figure 5:Overview of our training datasets.The pipeline includes image pre-training data (A-D) and video post-training data (D). We utilize both self-reference data (A, B) and filtered synthesized pairs with the same identity¬†(C, D). Numbers of (images + synthesized images) are reported.",
                "position": 439
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x5.png",
                "caption": "Figure 6:Qualitative comparisons.Captions and reference identity images are presented in the top-left corner for each case.",
                "position": 755
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x6.png",
                "caption": "Figure 7:Examples for ablation studies.Left: Ablation on modules.Right: Ablation on and training strategies.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x7.png",
                "caption": "Figure 8:CAN speeds up the convergence.Without the Conditioned Adaptive Normalization, the model cannot fit the simplest appearance features like hairstyle in the image pre-train stage.",
                "position": 978
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x8.png",
                "caption": "Figure 9:Different modalities‚Äô scale distribution using t-SNE.Each point represents the scale with a unique timestep-layer index. We also illustrate a shift variant on text and video‚Äôs adaptive scale using different colors.",
                "position": 1078
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x9.png",
                "caption": "Figure 10:Detailed training data processing pipeline.Building uponFig.5, we illustrate comprehensive filtering criteria, prompt examples, and processing specifications. The data flow is indicated byblue arrows, while filtering rules leading to data exclusion are marked withred arrows.",
                "position": 1118
            }
        ]
    },
    {
        "header": "Appendix AExperiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x10.png",
                "caption": "Figure 11:Impact of prompt length on image-to-video generation.We demonstrate how image-to-video models perform differently with concise versus enhanced prompts. Frames with large artifacts aremarked in red. First frame images are generated from enhanced prompts.",
                "position": 1145
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x11.png",
                "caption": "Figure 12:Face Motion (FM) calculation.FMinterinter{}_{\\text{inter}}start_FLOATSUBSCRIPT inter end_FLOATSUBSCRIPTfollows a similar computation across consecutive video frames.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x12.png",
                "caption": "Figure 13:Detailed implementation of Conditioned Adaptive Normalization.We present the expanded architecture ofœÜcondsubscriptùúëcond\\varphi_{\\text{cond}}italic_œÜ start_POSTSUBSCRIPT cond end_POSTSUBSCRIPT(illustrated in the unmasked region above) with comprehensive annotations of input-output tensor dimensions at each transformation.",
                "position": 1212
            }
        ]
    },
    {
        "header": "Appendix BAdditional Discussions",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x13.png",
                "caption": "Figure 14:Modulation layers reflect data distribution.Fine-tuning solely the modulation layer weights demonstrates adaptation to distinct data distributions, affecting both spatial fidelity and temporal dynamics.",
                "position": 1230
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x14.png",
                "caption": "Figure 15:Limitations of Magic Mirror.(a) Fine-grained feature preservation failure in facial details and accessories. (b) Motion artifacts in generated videos showing temporal inconsistencies.",
                "position": 1243
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results & Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x15.png",
                "caption": "Figure 16:Additional applications of Magic Mirror.We can generate identity-preserved videos across artistic styles and can generate multi-shot videos with consistent characters.More results are presented in the project page.",
                "position": 1257
            }
        ]
    },
    {
        "header": "Appendix DAcknowledgments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.03931/x16.png",
                "caption": "Figure 17:Image generation using Magic Mirror.Model in the image pre-train stage captures ID embeddings of the reference ID (Ref-ID), yet over-fits on some low-level distributions such as image quality, style, and background.",
                "position": 1293
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x17.png",
                "caption": "Figure 18:Advantages over I2V generation.Magic Mirror successfully handles challenging scenarios including partially occluded initial frames and maintains identity consistency through complex facial dynamics, addressing limitations of traditional I2V approaches.",
                "position": 1296
            },
            {
                "img": "https://arxiv.org/html/2501.03931/x18.png",
                "caption": "Figure 19:Video generation results.We demonstrate Magic Mirror‚Äôs capability across varying facial scales and compositions.Additional examples and comparative analyses are available in the project page.",
                "position": 1299
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]