[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20102/figures/pe_ppl_bar.png",
                "caption": "(a)",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/pe_ppl_bar.png",
                "caption": "(a)",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/pe_benchmark_bar.png",
                "caption": "(b)",
                "position": 83
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/pe_entropy_layers.png",
                "caption": "(c)",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/pe_sparsity_layers.png",
                "caption": "(d)",
                "position": 93
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20102/figures/ssa_method_v1.png",
                "caption": "Figure 2:Illustration of the SSA training framework. At each iteration, the model has an equal probability of following either the Sparse Attention (SA) stream or the Full Attention (FA) stream. In the SA stream, the model learns sparse attention while aligning its output with a full-attention counterpart computed on the fly. Conversely, in the FA stream, the model learns full attention constrained by alignment with the corresponding sparse-attention output. For clarity, skip connections, normalization, and dropout layers are omitted from the figure.",
                "position": 173
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Experimental Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20102/figures/extrapolation_1.png",
                "caption": "(a)",
                "position": 590
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/extrapolation_1.png",
                "caption": "(a)",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/extrapolation_2.png",
                "caption": "(b)",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/extrapolation_3.png",
                "caption": "(c)",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/extrapolation_4.png",
                "caption": "(d)",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/length_extrapolation_32k_ppl.png",
                "caption": "(a)",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/length_extrapolation_32k_ppl.png",
                "caption": "(a)",
                "position": 789
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/ssa_32kppl.png",
                "caption": "(b)",
                "position": 794
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/ssa_attn_sink.png",
                "caption": "(c)",
                "position": 799
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/ssa_length_sparsity.png",
                "caption": "(d)",
                "position": 804
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contact Information",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BGated Attention",
        "images": []
    },
    {
        "header": "Appendix CKL Divergence",
        "images": []
    },
    {
        "header": "Appendix DFull Longbench Results",
        "images": []
    },
    {
        "header": "Appendix EDifferent Configuration for Receptive Field of 1024",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20102/figures/length_extrapolation_nsa.png",
                "caption": "Figure 5:Perplexity across different context lengths for NSA architectural ablations, whereCMPdenotes the compression module,SELthe selection module, andSWAthe sliding-window module. NSA relies on the sliding-window component (SWA) to maintain PPL stability at long context lengths, whereas SSA achieves even better PPL stability without requiring a sliding-window mechanism.",
                "position": 1672
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/sink_1b_8k.png",
                "caption": "Figure 6:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 8k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1685
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/sink_1b_16k.png",
                "caption": "Figure 7:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 16k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/sink_1b_32k.png",
                "caption": "Figure 8:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 32k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1691
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/sink_mha_32k.png",
                "caption": "Figure 9:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 32k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1694
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/sink_moba_32k.png",
                "caption": "Figure 10:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 32k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/sink_ssa_32k.png",
                "caption": "Figure 11:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 32k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1700
            },
            {
                "img": "https://arxiv.org/html/2511.20102/figures/app_sink_mha_confirm_fixed_position.png",
                "caption": "Figure 12:Attention score distributions for FullAttn, MoBA, and SSA at a context length of 32k. All sparse-attention models are evaluated using full-attention inference.",
                "position": 1703
            }
        ]
    },
    {
        "header": "Appendix FLength Extrapolation",
        "images": []
    }
]