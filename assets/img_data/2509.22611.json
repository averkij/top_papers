[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22611/x1.png",
                "caption": "Figure 1:Entropy–performance dynamics on Qwen3-8B-Base. Left: DAPO withClip-Higherprevents early collapse but triggers an early entropy spike (steps 10–80) and a later performance plateau. Right: our quantile baseline (QAE) stabilizes policy entropy and sustains pass@1 gains by steering training into a balanced exploration regime.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3The Entropy Dilemma in RL Scaling: From Collapse to Explosion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22611/x2.png",
                "caption": "Figure 2:DAPO training dynamics on Qwen3–8B.Left: withoutClip-Higher;Right: withClip-Higher.\nIn both settings we observe two phases—an earlycorrelated growthbetween anthropomorphic token frequency and pass@1, followed by adecoupling then plateau.\nWhileClip-Higheraverts collapse, it does not prevent the later performance stall.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x3.png",
                "caption": "Figure 3:Evolution of high-entropy token usage under DAPO (steps 20/80/200).Early training exhibits diverse anthropomorphic tokens (e.g.,wait,perhaps); by steps 80–200 the distribution homogenizes around rigid reasoning templates (e.g.,so,let), indicating reduced exploratory diversity consistent with entropy explosion.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x4.png",
                "caption": "Figure 4:Quantile baseline reshapes weighting and entropy dynamics.Left: policy entropy over training split by advantage sign—negative-advantage samples drive the surge.Middle/Right: query-level weights vs. success ratepp; GRPO & DAPO use symmetricp​(1−p)\\sqrt{p(1-p)}weighting, whereas our method applies a thresholded scheme (K=0.4K\\!=\\!0.4).",
                "position": 387
            }
        ]
    },
    {
        "header": "4Method: Quantile-Based Advantage Estimation for Entropy Regulation",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22611/x5.png",
                "caption": "Figure 5:Training dynamics and sparsity.(a)AIME’24 (Qwen3–8B):\nQAE boosts pass@1 while keeping pass@16 comparable—showing higher sample efficiency.(b)Entropy by sign: DAPO’s explosion stems from negative-advantage samples; QAE suppresses it.(c)Response sparsity:  80% responses have zero advantage, focusing updates on informative subsets.",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x6.png",
                "caption": "Figure 6:Performance and ablations.(a)QAE improves DAPO on the 14B model for both AIME’25 and AIME’24 (pass@1).(b)With weaker high-end clipping (ϵhigh=0.28\\epsilon_{\\text{high}}{=}0.28), controlling\nnegative-advantage updates (Neg-Mask) is most critical, closely tracking full QAE.(c)With stronger clipping (ϵhigh=0.20\\epsilon_{\\text{high}}{=}0.20), positive-advantage\ncontrol (Pos-Mask) dominates.",
                "position": 794
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AProof",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.22611/x7.png",
                "caption": "Figure 7:High-entropy token diagnostics under QAE.Green bars: counts of anthropomorphic high-entropy tokens; orange line: overall pass@1.\nEarly coupled growth transitions to later decoupling—token counts plateau while accuracy improves—indicating entropy-safe, selective exploration.",
                "position": 1659
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x8.png",
                "caption": "Figure 8:Token-level diagnostics.Probability mass over top high-entropy tokens at different training steps.\nUnder QAE, exploratory tokens increase in a controlled manner, aligning with the stable-entropy regime in Fig.5b.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x9.png",
                "caption": "Figure 9:Training curves under differentKKon Qwen3-8B-Base. Left: entropy; middle: accuracy (AIME24@32); right: response length.",
                "position": 1689
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x10.png",
                "caption": "Figure 10:Training curves under DAPO and DAPO + QAE on Qwen3-8B-Base. Left: entropy; middle: accuracy (AIME24@32); right: response length.",
                "position": 1724
            },
            {
                "img": "https://arxiv.org/html/2509.22611/x11.png",
                "caption": "Figure 11:Training curves under DAPO and DAPO + QAE on Qwen3-14B-Base. Left: entropy; middle: accuracy (AIME24@32); right: response length.",
                "position": 1727
            }
        ]
    },
    {
        "header": "Appendix BExperiments",
        "images": []
    }
]