[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07015/x1.png",
                "caption": "Figure 1.Illustration of two key challenges in cross-modal knowledge distillation: distillation path selection and knowledge drift. Top: Performance comparison of unimodal students under different teachers (multimodal & cross-modal) on VGGSound-50k (visual & audio), with red lines as baselines. Bottom: Grad-CAM comparison between multimodal and cross-modal teachers on CrisisMMD-V2 (visual & text).",
                "position": 193
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07015/x2.png",
                "caption": "Figure 2.Overview of the MST-Distill framework in a two-modality setting, consisting of three stages: Collaborative Initialization (CI), Specialized Teacher Adaptation (STA), and Dynamic Knowledge Distillation (DKD).",
                "position": 277
            }
        ]
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07015/x3.png",
                "caption": "Figure 3.Overall architecture of MaskNet. A soft mask is generated to reconstruct the intermediate feature maps of the teacher model through a standard multi-head self-attention mechanism.",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x4.png",
                "caption": "Table 1.Performance comparison on multimodal classification datasets. The evaluation metric is the average overall accuracy over five independent runs. Dashed lines separate baselines trained independently on each modality. CM and MM denote cross-modal and multimodal teachers, respectively. Distillation results with performance gains are highlighted inBlue. The top two performing results are shown inRed, with the best result furtherunderlinedfor emphasis.",
                "position": 566
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07015/x4.png",
                "caption": "Table 2.Performance comparison on NYU-Depth-V2 using Overall Accuracy (OA), Average Accuracy (AA), and mean IoU (mIoU) for both RGB and Depth modalities.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x4.png",
                "caption": "Table 3.Ablation study of the three stages in MST-Distill. Each value indicates the mean accuracy averaged over all modality-specific students and five runs across four multimodal classification datasets.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x4.png",
                "caption": "Figure 4.Average routing probabilities of specialized teachers from a single run of MST-Distill for the visual student on the RAVDESS dataset. Solid and dashed lines indicate multimodal and cross-modal teachers, respectively.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x5.png",
                "caption": "Figure 5.Box plots of OA improvements under different teacher configurations in MST. Results are based on five independent runs conducted on two representative multimodal classification datasets. Different box colors represent different teacher configurations.",
                "position": 1169
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x6.png",
                "caption": "Figure 6.Performance trends under different top-kùëòkitalic_kvalues in dynamic knowledge distillation. Student performance across modalities is distinguished by color and marker.",
                "position": 1177
            }
        ]
    },
    {
        "header": "5.Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupplementary Analysis Supporting Method Motivation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07015/x7.png",
                "caption": "Figure 7.Performance comparison of multimodal and unimodal models trained from scratch across five multimodal datasets. Different colors are used to distinguish the performance of the multimodal model and various unimodal models.",
                "position": 2155
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x8.png",
                "caption": "Figure 8.Performance comparison between multimodal and cross-modal teachers under response-based knowledge distillation. Subfigures (a) and (b) present the global accuracies of target-modality student models under different teacher selection strategies. The performance of unimodal models trained from scratch is indicated in green for reference.",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x9.png",
                "caption": "Figure 9.Grad-CAM visualizations comparing the visual attention of a multimodal teacher and a unimodal visual student on the CrisisMMD-V2 dataset. Text inputs are shown above each image. The attention map of the better-performing model is highlighted in bold; both are bolded if their performance is similar.",
                "position": 2183
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Implementation and Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.07015/x10.png",
                "caption": "Figure 10.t-SNE visualizations of teacher features before and after MaskNet processing on AV-MNIST dataset. (a) Early phase and (b) Late phase of the STA stage. Left: features colored by processing status; Right: features colored by class labels.",
                "position": 2489
            },
            {
                "img": "https://arxiv.org/html/2507.07015/x11.png",
                "caption": "Figure 11.Grad-CAM visualizations of different models on four CrisisMMD-V2 samples. Each row shows the visualizations produced by a specific model, with the ground-truth label shown atop each column. Green and red borders indicate correct and incorrect predictions, respectively. MM-T and UM-S denote the multimodal teacher and unimodal student, respectively. The prefixORGdenotes models trained from scratch, whileS-indicates the Specialized Teachers within the MST-Distill framework.",
                "position": 2495
            }
        ]
    },
    {
        "header": "Appendix CSupplementary Materials for MST-Distill",
        "images": []
    }
]