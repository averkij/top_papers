[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10046/x1.png",
                "caption": "Figure 1:Illustration of the deep fusion approach and baselines.We conduct controlled comparisons with baseline methods that incorporate text representations from a single text encoder layer into each DiT layer using late fusion within the attention mechanism, a strategy we term as the “shallow fusion” approach.",
                "position": 103
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Deep Fusion of LLMs and DiTs",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10046/x2.png",
                "caption": "Figure 2:Illustration of the attention mask.Each dotted square indicates whether the row can attend to the column.",
                "position": 149
            }
        ]
    },
    {
        "header": "4Experiment Setup",
        "images": []
    },
    {
        "header": "5Comparing Deep and Shallow Fusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10046/x3.png",
                "caption": "Figure 3:Illustration of cross-modal attention in the shallow fusion baselines.The key and query states of the condition are directly projected from text representations.",
                "position": 241
            }
        ]
    },
    {
        "header": "6Examining Key Design Choices",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10046/x4.png",
                "caption": "Figure 4:Illustration of timestep conditioning strategies.Removing timesetp conditioning leads to the fewest parameters and the best overall performance.",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2505.10046/x5.png",
                "caption": "Figure 5:Illustration of RoPE.The indices denote position IDs.",
                "position": 576
            }
        ]
    },
    {
        "header": "7Training at Scale",
        "images": []
    },
    {
        "header": "8Further Exploration",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.10046/x6.png",
                "caption": "Figure 6:Illustration of architecture alignment.Dashed boxes indicate parameters that have been reduced by decreasing either the hidden size or the number of layers.",
                "position": 1113
            },
            {
                "img": "https://arxiv.org/html/2505.10046/x7.png",
                "caption": "Figure 7:Samples generated by FuseDiT.",
                "position": 1311
            }
        ]
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]