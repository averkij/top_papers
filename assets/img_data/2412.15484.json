[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15484/x1.png",
                "caption": "Figure 1:The process of generating a data sample for evaluating hallucination detection methods in detailed image captioning tasks. Human annotators identify and labelobject hallucinationswithin the caption generated by LLaVA-NeXT(Liu et¬†al.,2024a)for an image.",
                "position": 149
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15484/x2.png",
                "caption": "(a)Confidence-Token Index",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x2.png",
                "caption": "(a)Confidence-Token Index",
                "position": 173
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x3.png",
                "caption": "(b)Consistency-Token Index",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x4.png",
                "caption": "Figure 3:Overview of CapMAS. The decomposer LLM breaks an initial caption into atomic units. These units are converted into True/False questions and fed into the MLLM along with the image, where each unit is assigned ahallucinationscore according to Equation (1). Units are classified as True or False based on the thresholdœÄùúã\\piitalic_œÄ, and the corrector LLM then revises the initial caption accordingly.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x5.png",
                "caption": "Figure 4:An example of our coverage evaluation data sample. The dataset consists of multiple-choice questions with four or fewer options. As demonstrated, the dataset includes questions with varying levels of granularity, ranging from broad to highly detailed. We have an LLM solve these problems using only the provided captions.",
                "position": 389
            }
        ]
    },
    {
        "header": "4Experimental Results and Discussion",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHuman Evaluation Dataset Construction",
        "images": []
    },
    {
        "header": "Appendix BUndesirable Bias in FACTSCORE",
        "images": []
    },
    {
        "header": "Appendix CAblation Study",
        "images": []
    },
    {
        "header": "Appendix DThe complete version of Table2",
        "images": []
    },
    {
        "header": "Appendix ECase",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.15484/x6.png",
                "caption": "Figure 5:An example of a caption generated by CapMAS, with LLaVA-NeXT-7B as both the captioning and fact-checking model and LLaMA-3-8B as both the decomposer and corrector LLM.",
                "position": 1938
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x7.png",
                "caption": "Figure 6:The five prompt inputs used to generate captions in our experiments.",
                "position": 1946
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x8.png",
                "caption": "Figure 7:The prompt input for LLaMA-3-8B serving as the decomposer.",
                "position": 1950
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x9.png",
                "caption": "Figure 8:The prompt input for LLaMA-3-8B serving as the corrector.",
                "position": 1954
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x10.png",
                "caption": "Figure 9:The prompt input for LLaMA-3-8B serving as the summerizer. We use the prompt employed in the work of(Ge et¬†al.,2024).",
                "position": 1958
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x11.png",
                "caption": "Figure 10:The prompt input for GPT-4o used to create the meta-evaluation dataset of Table2.",
                "position": 1962
            },
            {
                "img": "https://arxiv.org/html/2412.15484/x12.png",
                "caption": "Figure 11:The prompt input for GPT-4 used to create the dataset of Figure1. We use the prompt employed in the work of(Ge et¬†al.,2024).",
                "position": 1966
            }
        ]
    },
    {
        "header": "Appendix FPrompt Templates",
        "images": []
    }
]