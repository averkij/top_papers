[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x1.png",
                "caption": "Figure 1:AudioStory decomposes multimodal instructions into a sequence of coherent audio segments, capturing scene transitions, emotional tone, and segment timestamps. Unlike prior T5-based diffusion models, which struggle with complex queries, AudioStory empowers LLMs with high-level planning ability for instruction-followed and consistent long audio generation.",
                "position": 153
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Narrative Audio Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x2.png",
                "caption": "Figure 2:Overview of AudioStory, with three core components: (a) A unified framewrok: The reasoning-capable LLM processes the instruction input, decomposes the long audio into structured generation sub-tasks, and sequentially generates a caption, semantic tokens, and residual tokens for each audio clip. (b) Audio Generator: After fusing semantic and residual tokens, they are combined with the duration information as conditioning inputs to the DiT, which then generates each audio clip. (c) Training strategy: Training is conducted in three stages to progressively enhance generation fidelity, semantic understanding, and global coherence.",
                "position": 245
            }
        ]
    },
    {
        "header": "4AudioStory",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x3.png",
                "caption": "Figure 3:Qualitative case of long-form audio generation.",
                "position": 746
            }
        ]
    },
    {
        "header": "6Extended Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x4.png",
                "caption": "Figure 4:Case of naive video dubbing: First, we extract captions from the video, then write the extracted captions as instructions and send them to AudioStory for audio generation.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x5.png",
                "caption": "Figure 5:Case of video dubbing: We input both the video and the instruction into the model, which parses the narrative into segments, extracts story details with corresponding audio elements, and sequentially generates aligned audio clips..",
                "position": 1245
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x6.png",
                "caption": "Figure 6:Qualitative cases of audio continuation.",
                "position": 1248
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BQualitative Cases",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x7.png",
                "caption": "Figure 7:Long-form audio generation case #2.",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x8.png",
                "caption": "Figure 8:Long-form audio generation case #3.",
                "position": 1973
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x9.png",
                "caption": "Figure 9:Audio continuation case #2.",
                "position": 1976
            }
        ]
    },
    {
        "header": "Appendix CMore Explorations of Residual Tokens",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x10.png",
                "caption": "Figure 10:Ablations of token merging.",
                "position": 2054
            }
        ]
    },
    {
        "header": "Appendix DWhat do Residual Tokens Learn?",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.20088/x11.png",
                "caption": "Figure 11:Visualizations of residual tokens.",
                "position": 2071
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x12.png",
                "caption": "Figure 12:AudioStory-10k annotation prompts.",
                "position": 2084
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x13.png",
                "caption": "Figure 13:The datasets construction prompt.",
                "position": 2099
            },
            {
                "img": "https://arxiv.org/html/2508.20088/x14.png",
                "caption": "Figure 14:AudioStory-10k cases.",
                "position": 2102
            }
        ]
    },
    {
        "header": "Appendix EAudioStory-10k Benchmark",
        "images": []
    }
]