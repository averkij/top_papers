[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05397/x1.png",
                "caption": "Figure 1:Illustration of LoongX for hands-free image editing via multimodal neural signals.",
                "position": 138
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05397/x2.png",
                "caption": "Figure 2:The L-Mind dataset comprises 23,928 multimodal editing samples, each including an original image, a ground truth text editing instruction, a ground truth edited image, as well as measured EEG, fNIRS, PPG, motion and speech signals. (a) Multimodal data collection pipeline; (b) Illustration and statistics of 35 types of image editing tasks.",
                "position": 250
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05397/x3.png",
                "caption": "Figure 3:Overview of our proposed LoongX method for hands-free image editing. Receiving an input image, LoongX outputs an edited image using neural signals (and optional speech) as conditions.",
                "position": 301
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05397/x4.png",
                "caption": "Figure 4:Evaluation results for different signal combinations using DGF.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x4.png",
                "caption": "Figure 4:Evaluation results for different signal combinations using DGF.",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x5.png",
                "caption": "Figure 5:Evaluation results on different brain region signals where LoongX is trained and tested on each respective EEG channel.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x6.png",
                "caption": "Figure 6:Breakdown results of text and neural-driven image editing metrics. BG: background.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x7.png",
                "caption": "Figure 7:Qualitative evaluation comparing the text prompt-based method and our neural-driven methods for four editing categories: (a) background, (b) object, (c) global, and (d) text editing.",
                "position": 730
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.05397/x8.png",
                "caption": "Figure 8:(a) Signal acquisition mechanism. (b) Cognitive function of different brain regions.",
                "position": 1616
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x9.png",
                "caption": "Figure 9:Multi-label classification result under different settings: (a) different single input modalities; (b) different modality fusion combinations; (c) different EEG input sequence lengths.",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x10.jpg",
                "caption": "Figure 10:Qualitative comparison of our neural-driven and speech-neural fusion methods and text-prompt baseline for Global Editing category.",
                "position": 2079
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x11.jpg",
                "caption": "Figure 11:Qualitative comparison of our neural-driven and speech-neural fusion methods and text-prompt baseline for Background Editing category.",
                "position": 2082
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x12.jpg",
                "caption": "Figure 12:Qualitative comparison of our neural-driven and speech-neural fusion methods and text-prompt baseline for Object Editing category.",
                "position": 2085
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x13.jpg",
                "caption": "Figure 13:Qualitative comparison of our neural-driven and speech-neural fusion methods and text-prompt baseline for Text Editing category.",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2507.05397/x14.jpg",
                "caption": "Figure 14:Qualitative analysis on three failure cases: (a) Overly exaggerated descriptions, e.g., \"long-legged space creature\"; (b) Vague instructions lacking detail, such as omitting whether to retain the background; (c) Uncommon image dimensions, e.g., panoramic input images.",
                "position": 2091
            }
        ]
    },
    {
        "header": "Appendix ATechnical Appendix and Supplementary Information",
        "images": []
    }
]