[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/title.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/pipeline.png",
                "caption": "Figure 2:3D-RE-GEN Framework Overview.Our framework converts a single image into a complete 3D scene. First, we segment the image: these masks provide the 2D silhouette loss, while our novel Application-Querying (A-Q) model generates clean, inpainted object images for 3D meshing. In parallel, the input and a generated ”empty room” image are used to extract the camera and scene point cloud, which is masked to create the 3D geometric loss. Finally, our Scene Positioning model assembles the 3D assets and background by minimizing both losses, using a novel 4-DOF constrained workflow to ensure all ground based objects are physically aligned to the floor.",
                "position": 116
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/appl_querying.png",
                "caption": "(a)Input and output of the Application-Querying (A-Q) method.",
                "position": 195
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/appl_querying.png",
                "caption": "(a)Input and output of the Application-Querying (A-Q) method.",
                "position": 198
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/appl_querying_fail.png",
                "caption": "(b)Failure cases. The object image completion won’t work correctly without a proper UI image prompt, e.g., wrong materials and shapes.",
                "position": 204
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/comparison_proxy.png",
                "caption": "Figure 4:Qualitative comparison across different methods\nfor different input scenes. Starting with 4 scenes based on synthetic datasets and two real images. In the bottom line, we even tested an outside image.",
                "position": 263
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/survey/survey_cake.png",
                "caption": "(a)Overall reception",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/survey/survey_cake.png",
                "caption": "(a)Overall reception",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/survey/survey_bar.png",
                "caption": "(b)Reason for choice",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/room_004/GT_Image.png",
                "caption": "(a)Input image",
                "position": 387
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/room_004/GT_Image.png",
                "caption": "(a)Input image",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/room_004/ours_rendered.png",
                "caption": "(b)3D-RE-GEN output",
                "position": 396
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/ablation/render_cam1_noBan.png",
                "caption": "(c)Without A-Q",
                "position": 402
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/ablation/render_cam1.png",
                "caption": "(d)Without 4-DoF",
                "position": 408
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgements",
        "images": []
    },
    {
        "header": "Overview",
        "images": []
    },
    {
        "header": "Appendix AAdditional Videos",
        "images": []
    },
    {
        "header": "Appendix BDiscussion",
        "images": []
    },
    {
        "header": "Appendix CBackground Extraction and Texturing",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/supplemental/Suppl_Background_extraction.png",
                "caption": "Figure 7:Background extraction workflow.Extraction of pointclouds trough geometry transformer; pointclouds getting meshed with surfacing algorithm and then texture applied through projection.",
                "position": 627
            }
        ]
    },
    {
        "header": "Appendix DPrompts",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/supplemental/suppl_inline_fail_full.png",
                "caption": "Figure 8:Using just an inline approach can lead to wrong scene understanding.",
                "position": 668
            },
            {
                "img": "https://arxiv.org/html/2512.17459/sec/images/supplemental/supp_gradio.png",
                "caption": "Figure 9:Gradio App to finetune masks from GroundedSAM",
                "position": 697
            }
        ]
    },
    {
        "header": "Appendix EUser-friendly mask generation",
        "images": []
    }
]