[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/main_figure_v2.png",
                "caption": "Figure 1:Overview of Compression Benchmarks and Methods for Large Language Models (LLMs). (a) Benchmark Comparison: Illustrates the transition from single-turn quantized LLMs to multi-turn compressed LLMs in agentic scenarios. (b) Compression Methods: Summarizes the techniques used for quantization (e.g., GPTQ, AWQ) and sparsification (e.g., SparseGPT, Wanda). (c) Overview of Agent Compression Benchmark: Provides a comprehensive view of the capabilities and components involved in agentic LLM compression, including action execution, workflow build, real-world applications, and long-context processing.",
                "position": 387
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/quantization_llama-2-7b-hf.png",
                "caption": "Figure 2:ERank analysis difference analysis for quantized LLaMA-2-7B (left) and Mistral-7B (right) models",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/quantization_mistral-7b.png",
                "caption": "",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/json_str_diff_quant.png",
                "caption": "Figure 3:Comparison of format performance differences between (left) quantized and (right) sparse model architectures",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/json_str_diff_sparse.png",
                "caption": "",
                "position": 553
            }
        ]
    },
    {
        "header": "3Statistical Analysis of Compression Effects",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/rank_consistency_analyze.png",
                "caption": "Figure 4:Top-k Ranking Consistency Analysis for quantized Phi-3.5.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/spearman_correlation.png",
                "caption": "Figure 5:Spearman correlation analysis between perplexity and Top-k ranking correlation metrics across model sizes and quantization levels.",
                "position": 573
            }
        ]
    },
    {
        "header": "4Evaluation on Action Execution",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/fixed_figure5_part1.png",
                "caption": "Figure 6:Performance Evaluation of InternLM-2.5-7B and Qwen-2.5-7B Models Across Different Sparsification (left) and Quantization (right) Techniques for Tool Use",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/fixed_figure5_part2.png",
                "caption": "",
                "position": 646
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/v2_fig6.png",
                "caption": "Figure 7:Comparison of average F1 scores across different model sizes and configurations. The models are evaluated based on their performance with approximately 1.5B, 3B, 7B, and 14B parameters. The configurations include AWQ(INT4), FP16, GPTQ(INT4), GPTQ(INT8), and SmoothQ(W8A8). Each bar represents the average F1 score achieved by the respective model configuration at different parameter sizes.",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/radar_distill_deepseek.png",
                "caption": "Figure 8:Performance Comparison between DeepSeek-R1-Qwen2.5-7B and Qwen2.5-7B across four evaluation benchmarks. The distilled version performs worse in most cases.",
                "position": 655
            }
        ]
    },
    {
        "header": "5Evaluation on Workflow Generation",
        "images": []
    },
    {
        "header": "6Evaluation on Long-Context Understanding",
        "images": []
    },
    {
        "header": "7Evaluation on Real-World Applications",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Acknowledgments",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AMore Related Works",
        "images": []
    },
    {
        "header": "Appendix BDetailed Experiment Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/tool_use_bar_sparse.png",
                "caption": "Figure 9:Performance comparison of sparse compression methods on tool use tasks",
                "position": 3224
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/tool_use_bar_quant.png",
                "caption": "Figure 10:Performance comparison of quantization methods on tool use tasks",
                "position": 3227
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_Instruct_full_64_40k.png",
                "caption": "InternLM-2.5-7B Base Model (Uncompressed)",
                "position": 6384
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_Instruct_full_64_40k.png",
                "caption": "InternLM-2.5-7B Base Model (Uncompressed)",
                "position": 6387
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_GPTQ_full_64_40k.png",
                "caption": "InternLM-2.5-7B with GPTQ",
                "position": 6392
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_AWQ_full_64_40k.png",
                "caption": "InternLM-2.5-7B with AWQ",
                "position": 6398
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_wanda_2_4_full_64_40k.png",
                "caption": "InternLM-2.5-7B with Wanda (2:4)",
                "position": 6403
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_mag_2_4_full_64_40k.png",
                "caption": "InternLM-2.5-7B with Magnitude (2:4)",
                "position": 6409
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/InternLM2.5_7b_sparsegpt_2_4_full_64_40k.png",
                "caption": "InternLM-2.5-7B with SparseGPT (2:4)",
                "position": 6414
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_full_64_40k.png",
                "caption": "Qwen-2.5-7B Base Model (Uncompressed)",
                "position": 6421
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_full_64_40k.png",
                "caption": "Qwen-2.5-7B Base Model (Uncompressed)",
                "position": 6424
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_7b_GPTQ_full_64_40k.png",
                "caption": "Qwen-2.5-7B with GPTQ",
                "position": 6429
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_7b_AWQ_full_64_40k.png",
                "caption": "Qwen-2.5-7B with AWQ",
                "position": 6435
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_7b_RTN_full_64_40k.png",
                "caption": "Qwen-2.5-7B with RTN",
                "position": 6440
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_7b_mag_2_4_full_64_40k.png",
                "caption": "Qwen-2.5-7B with Magnitude (2:4)",
                "position": 6446
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/Qwen2.5_7b_sparsegpt_2_4_full_64_40k.png",
                "caption": "Qwen-2.5-7B with SparseGPT (2:4)",
                "position": 6451
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/deepseek-qwen-1.5b_full_64_40k_slm.png",
                "caption": "DeepSeek-Qwen-1.5B",
                "position": 6458
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/deepseek-qwen-1.5b_full_64_40k_slm.png",
                "caption": "DeepSeek-Qwen-1.5B",
                "position": 6461
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/deepseek-qwen-7b_full_64_40k_slm.png",
                "caption": "DeepSeek-Qwen-7B",
                "position": 6466
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/qwen-1.5b-gptq-int4_full_64_40k_slm.png",
                "caption": "Qwen-1.5B with GPTQ (INT4)",
                "position": 6472
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/qwen-1.5b-gptq-int8_full_64_40k_slm.png",
                "caption": "Qwen-1.5B with GPTQ (INT8)",
                "position": 6477
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/qwen-3b-gptq-int4_full_64_40k_slm.png",
                "caption": "Qwen-3B with GPTQ (INT4)",
                "position": 6483
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/needle/megrez-3b_full_64_40k_slm.png",
                "caption": "Megrez-3B",
                "position": 6488
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_02.png",
                "caption": "Logits Distribution at token 2",
                "position": 6502
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_02.png",
                "caption": "Logits Distribution at token 2",
                "position": 6505
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_82.png",
                "caption": "Logits Distribution at token 82",
                "position": 6510
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_134.png",
                "caption": "Logits Distribution at token 134",
                "position": 6515
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_184.png",
                "caption": "Logits Distribution at token 184",
                "position": 6521
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_225.png",
                "caption": "Logits Distribution at token 225",
                "position": 6526
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/logits_vis/combined_position_246.png",
                "caption": "Logits Distribution at token 246",
                "position": 6531
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_01.png",
                "caption": "Energy Distribution at token 1",
                "position": 6545
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_01.png",
                "caption": "Energy Distribution at token 1",
                "position": 6548
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_64.png",
                "caption": "Energy Distribution at token 64",
                "position": 6553
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_117.png",
                "caption": "Energy Distribution at token 117",
                "position": 6558
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_175.png",
                "caption": "Energy Distribution at token 175",
                "position": 6564
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_244.png",
                "caption": "Energy Distribution at token 244",
                "position": 6569
            },
            {
                "img": "https://arxiv.org/html/2505.19433/extracted/6477732/figs/energy_vis/combined_position_252.png",
                "caption": "Energy Distribution at token 252",
                "position": 6574
            }
        ]
    },
    {
        "header": "Appendix CMore Experiment Results",
        "images": []
    }
]