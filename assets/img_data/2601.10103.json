[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10103/figures/teaser.png",
                "caption": "Figure 1:We present FlowAct-R1, a novel framework that enables lifelike, responsive, and high-fidelity humanoid video generation for seamless real-time interaction.",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10103/figures/overview.png",
                "caption": "Figure 2:Overview of the FlowAct-R1 framework. It consists of training and inference stages: training integrates converting base full-attention DiT to streaming AR model via autoregressive adaptation, joint audio-motion finetuning for better lip-sync and body motion, multi-stage diffusion distillation; inference adopts a structured memory bank (Reference/Long/Short-term Memory, Denoising Stream) with chunkwise autoregressive generation and memory refinement. Complemented by system-level optimizations, it achieves 25fps real-time 480p video generation (TTFF  1.5s) with vivid behavioral transitions.",
                "position": 236
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.10103/figures/experiments.jpg",
                "caption": "Figure 3:Comparisons with KlingAvatar 2.0[31], LiveAvatar[16], and Omnihuman-1.5[18]via a user study using the GSB (good-same-bad) metric. Theorange segmentsindicate the percentage of user votes favoring FlowAct-R1 over other methods. Video demos are shown in our project page.",
                "position": 350
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]