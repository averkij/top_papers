[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17909/x1.png",
                "caption": "Figure 1:Reconstruction and generation performance across different generation spaces.Compared to vanillaVAE,RAEimproves generation coverage speed but quickly saturates due to its unconstrained semantic space and weak reconstruction.\nTo address this, we project RAE features into a compact 96-channel latent space with a semantic reconstruction objective, formingS-VAE, which mitigates off-manifold issues and improves generation performance.\nFinally,PS-VAEfurther augments the semantic latent space with pixel-level reconstruction, enriching structural and texture details and achieving superior performance in both reconstruction and generation.",
                "position": 101
            }
        ]
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17909/x2.png",
                "caption": "Figure 2:Visualization comparison between RAE and VAE.(a) RAE shows a noticeable gap in reconstruction performance compared to VAE.\nBenefiting from its rich semantic representation, RAE demonstrates stronger prompt-following ability in image editing tasks that require understanding the input image (b). However, its poor reconstruction quality limits practical usability, as it fails to preserve fine-grained and consistent details from the input image. Counterintuitively, in text-to-image generation, RAE exhibits severe structural and texture artifacts and substantially lags behind VAE (c), with a performance gap far larger than that observed in reconstruction.",
                "position": 179
            },
            {
                "img": "https://arxiv.org/html/2512.17909/x3.png",
                "caption": "Figure 3:Off-manifold behavior varies significantly with feature dimensionality.We construct a 2D ‘PS’-shaped distribution and embed it into an 8D ambient space, yielding two learning settings with intrinsic dimension 2 and ambient dimension 8.(a)The 8D setting produces substantially more off-manifold samples than the intrinsic 2D space.(b)We measure the mean nearest-neighbor distance of the top 5% tail samples and observe that samples generated in 8D deviate much farther from the data manifold, indicating stronger off-manifold drift.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2512.17909/x4.png",
                "caption": "Figure 4:Visual comparison of generated examples across progressively improved latent spaces (RAE→\\rightarrowS-VAE→\\rightarrowPS-VAE).Artifacts are gradually reduced, with step-by-step improvements in texture and structure.",
                "position": 244
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/pipeline.png",
                "caption": "Figure 5:Compact latent space construction for preserving semantic structure and fine-grained detailsWe first regularize the unconstrained representation-encoder feature space by freezing the encoder and training a semantic VAE using only theℒs\\mathcal{L}_{s}andℒkl\\mathcal{L}_{\\mathrm{kl}}; during this stage, the pixel decoder is trained on the detached semantic latent with pixel reconstruction lossℒP\\mathcal{L}_{\\mathrm{P}}. After semantic reconstruction converges, we unfreeze all components and allow the pixel decoder to backpropagate the gradient into the encoder, ensuring that the representation encoder captures fine-grained details of the input image.",
                "position": 264
            },
            {
                "img": "https://arxiv.org/html/2512.17909/x5.png",
                "caption": "Figure 6:Block comparison of three deep-fusion architectures.",
                "position": 272
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17909/figures/all_vae_geneval.png",
                "caption": "Figure 7:Coverage curves for generation and editing tasks across different feature spaces.By jointly providing rich semantics and state-of-the-art reconstruction fidelity,PS-VAEconsistently outperforms semantic-only RAE and pixel-only VAEs across both generation and editing benchmarks.\nThe strong semantic structure and well-regularized latent space ofPS-VAEenable significantly faster convergence during text-to-image training, while also facilitating better image understanding and, consequently, stronger instruction-following in image editing.\nMeanwhile, higher reconstruction fidelity leads to more realistic structures and textures in text-to-image generation, improved detail consistency between the edited and input images during editing, and thus better overall performance in both tasks.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/all_vae_dpg.png",
                "caption": "",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/all_vae_editing_reward.png",
                "caption": "",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2512.17909/x6.png",
                "caption": "Figure 8:Editing visual examples of models trained on different feature spaces.As shown in (a), bothPS-VAEand RAE exhibit reasonable visual grounding, correctly identifying the elderly man and the background wall in (a). However, RAE’s performance is strongly limited by its weak reconstruction ability, resulting in inconsistent details with the input image (a,b,c). In contrast, benefiting from strong semantic alignment and high-fidelity reconstruction,PS-VAEachieves accurate instruction following while preserving consistent visual details between the input and edited images, such as the human face in (a,b,c).",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2512.17909/x7.png",
                "caption": "Figure 9:Text-to-image generation examples usingPS-VAE96​c\\texttt{PS-VAE}_{96c}.Despite being trained only at256×256256\\times 256resolution, the semantically structured and detail-preserving latent space enables the generator to accurately follow complex text prompts, yielding images with correct structures, fine-grained textures, precise text rendering, realistic portraits, and flexible compositions of abstract concepts.\nPrompts are simplified for visualization; full prompts can be found in the Supplementary Material.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/scaling_geneval.png",
                "caption": "Figure 10:Scaling behavior of 653M (dashed) and 1708M (solid) models under differentPS-VAEchannel sizes (32c/96c) on (a) GenEval, (b) DPG-Bench, and (c) Editing Reward.",
                "position": 505
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/scaling_dpg.png",
                "caption": "",
                "position": 514
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/scaling_editing_reward.png",
                "caption": "",
                "position": 519
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/geneval_channels_all.png",
                "caption": "Figure 11:Channel ablation ofPS-VAE.Left:reconstruction metrics versus channel dimensionality.Middle/Right:convergence curves on GenEval and DPG-Bench.\nAs channel dimensionality increases, convergence becomes slightly slower; GenEval and DPG-Bench remain stable from 32c to 96c, while DPG-Bench drops beyond 96c, suggesting an intrinsic latent dimensionality of approximately∼\\sim96 channels under our training setup. Further increasing the channel dimension primarily captures high-frequency details, which may consume model capacity and interfere with semantic learning.",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2512.17909/figures/dpg_channels_all.png",
                "caption": "",
                "position": 770
            },
            {
                "img": "https://arxiv.org/html/2512.17909/x8.png",
                "caption": "Figure 12:Directly enriching details in a high-dimensional space leads to severe generation artifacts (a).\nWe further verify that this behavior arises from reconstruction shortcuts in high-dimensional feature spaces (b).",
                "position": 902
            }
        ]
    },
    {
        "header": "5Ablation Study",
        "images": []
    }
]