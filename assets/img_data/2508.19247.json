[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19247/x1.png",
                "caption": "",
                "position": 68
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19247/x2.png",
                "caption": "Figure 2:Pipeline.Given an input 3D model, a user-specified editing region, and a text prompt, the off-the-shelf models[40,3]are used to inpaint the rendered view from the 3D model. Subsequently, ourVoxHammer, a training-free framework based on structured 3D diffusion models[90], performs native 3D editing conditioned on the input 3D and the edited image.",
                "position": 138
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19247/x3.png",
                "caption": "Figure 3:Architecture of VoxHammer.Our framework adopts TRELLIS[90]as the base model, which predicts sparse structures at the first structure (ST) stage and denoise fine-grained structured latents at the second sparse-latent (SLAT) stage.VoxHammerperforms inversion prediction in both the ST and SLAT stages, which map the textured 3D asset to its terminal noise, with latents and key/value tensors cached at each timestep. Subsequently,VoxHammerdenoises from the inverted noise, and replace the features of the preserved regions with the corresponding cached latents and key-value tokens, thereby achieving precise and coherent editing in native 3D space.",
                "position": 171
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19247/x4.png",
                "caption": "Figure 4:Qualitative comparisons on Edit3D-Bench.Our method achieves best performance on precision of editing and overall quality.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2508.19247/x5.png",
                "caption": "Figure 5:Ablation studies.Results demonstrate the effectiveness of key-value replacement in attention mechanism and latent replacement.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2508.19247/x6.png",
                "caption": "Figure 6:The impact of inversion stages on reconstruction.ST stage inversion lacks detailed consistency, while inversion on both stages achieves fine-grained geometry and texture reconstruction.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2508.19247/x7.png",
                "caption": "Figure 7:More applications.VoxHammereasily generalizes to part-aware 3D object, scene, and NeRF[58]or 3DGS[38]editing. We show the input models in the top row and the edited results in the bottom two rows.",
                "position": 599
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Text-Condition 3D Editing",
        "images": []
    },
    {
        "header": "7Explanation of Evaluation Metrics",
        "images": []
    },
    {
        "header": "8More Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19247/x8.png",
                "caption": "Figure 8:Visualization results of text-condition 3D editing.",
                "position": 2009
            },
            {
                "img": "https://arxiv.org/html/2508.19247/x9.png",
                "caption": "Figure 9:Pipeline of text-condition (left) and image-condition (right) 3D editing.",
                "position": 2072
            },
            {
                "img": "https://arxiv.org/html/2508.19247/x10.png",
                "caption": "Figure 10:More visualization results of image-condition 3D editing.",
                "position": 2075
            }
        ]
    },
    {
        "header": "9Limitation",
        "images": []
    }
]