[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04434/x1.png",
                "caption": "Figure 1:Overview of Durian.Given a portrait and a reference image specifying target attributes (e.g., hairstyle, eyeglasses), our Durian generates an animatable 2D portrait video with composable visual attributes in a single-stage pipeline.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04434/x2.png",
                "caption": "Figure 2:Overview of Training Pipeline.Given an attribute-masked portrait image𝐈~port\\tilde{\\mathbf{I}}_{\\mathrm{port}}and an attribute-only image𝐈~attr\\tilde{\\mathbf{I}}_{\\mathrm{attr}}, our Durian synthesizes a portrait animation with the transferred attribute. These inputs are constructed by randomly sampling two frames from a training video and applying the estimated masks. A sequence of facial keypoints𝒌ττ=1F{\\bm{k}_{\\tau}}_{\\tau=1}^{F}is extracted from the video to guide the motion of the generated animation. During generation, spatial features from PRNet and ARNet are fused via spatial attention into the DNet, enabling identity-preserving and attribute-consistent video synthesis.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x3.png",
                "caption": "Figure 3:Aligned Attribute Mask Estimation.To enhance spatial alignment between the attribute and the portrait input, we generate an aligned attribute mask by reenacting a 3D avatar reconstructed from the attribute image.",
                "position": 431
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04434/x4.png",
                "caption": "Figure 4:Qualitative Comparison.We compare our method and the baselines that combine X-PortraitXie et al. (2024)with face editing methodsChung et al. (2025); Yang et al. (2023); Zhang et al. (2025); Bilecen et al. (2024).",
                "position": 723
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x5.png",
                "caption": "Figure 5:Ablation Study.Removing each component and varying the training strategy results in visible degradation of synthesis quality.",
                "position": 763
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x6.png",
                "caption": "Figure 6:Multi-Attribute Transfer.Our model supports composition of multiple attributes (e.g., hair, eyeglasses, beard, hat) in a single forward pass.",
                "position": 782
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04434/x7.png",
                "caption": "Figure 7:Attribute Interpolation.Our model enables smooth and consistent transitions between attributes. We present interpolation examples for hair, beard, and eyeglasses by varying the interpolation parameterα\\alpha.",
                "position": 807
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AGuidance Generation",
        "images": []
    },
    {
        "header": "Appendix BAdditional Application",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.04434/x8.png",
                "caption": "Figure 8:Text-to-Image-to-Attribute Transfer for Portrait Animation.We demonstrate portrait animation with attribute transfer from a textual description. We employ FLUXLabs (2024)to generate a high-quality portrait image with the desired hair attribute from a text prompt.",
                "position": 1553
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x9.png",
                "caption": "Figure 9:Qualitative Comparison.We compare our method and the baselines that combine portrait animation methodGuo et al. (2024); Xie et al. (2024); Yang et al. (2025)with face editing methodsChung et al. (2025); Yang et al. (2023); Zhang et al. (2025); Bilecen et al. (2024).",
                "position": 1558
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x10.png",
                "caption": "Figure 10:More Qualitative Results.We present additional results on hair, hat, eyeglasses, and beard attribute transfer for portrait animation. Our method preserves the fine details of the original portrait while achieving natural and seamless attribute transfer.",
                "position": 1562
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x11.png",
                "caption": "Figure 11:More Multi-Attribute Transfer Results.We demonstrate the results of simultaneously transferring two attributes for portrait animation.",
                "position": 1566
            },
            {
                "img": "https://arxiv.org/html/2509.04434/x12.png",
                "caption": "Figure 12:More Multi-Attribute Transfer Results.We present results of simultaneously transferring three attributes. In each example, the image at the top-left corner indicates the portrait that receives the attribute transfer.",
                "position": 1571
            }
        ]
    },
    {
        "header": "Appendix CAdditional Qualitative Comparison",
        "images": []
    }
]