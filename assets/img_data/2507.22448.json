[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22448/figures/tii_logo.png",
                "caption": "",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/hf_logo.png",
                "caption": "",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/github_logo.png",
                "caption": "",
                "position": 200
            }
        ]
    },
    {
        "header": "1.  Introduction",
        "images": []
    },
    {
        "header": "2.  Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22448/x1.png",
                "caption": "Figure 1:Falcon-H1 architecture. Attention and SSM run in parallel within each block; their outputs are concatenated before the block’s output projection. The number of SSM/Attention heads can be flexibly tuned.",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x2.png",
                "caption": "Figure 2:(Left): The loss of fully parallelSAMhybrid block configuration for all possible(αS,αA,αM)(\\alpha_{S},\\alpha_{A},\\alpha_{M})( italic_α start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_α start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT )channel allocations according to (1),(2).(Right)The loss of all 3 considered block configurationsSAM(3),SA_M(4), andS_A_M(5) for fixed optimal attention allocationαA=18\\alpha_{A}=\\frac{1}{8}italic_α start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 8 end_ARGand varied SSM/MLP channel allocation.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x3.png",
                "caption": "(a)",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x3.png",
                "caption": "(a)",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x4.png",
                "caption": "(b)",
                "position": 625
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x5.png",
                "caption": "(a)",
                "position": 652
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x5.png",
                "caption": "(a)",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x6.png",
                "caption": "(b)",
                "position": 660
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x7.png",
                "caption": "(a)",
                "position": 764
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x7.png",
                "caption": "(a)",
                "position": 767
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x8.png",
                "caption": "(b)",
                "position": 772
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/humaneval_score_plot.png",
                "caption": "Figure 6:Model performance regarding different splitting strategies.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/gsm8k_performance_comparison.png",
                "caption": "Figure 8:Performance on mathematical benchmarks for two 1B models during the training decay stage. The model trained with a tokenizer augmented with specializedLaTeXtokens (blue) consistently outperforms the baseline model (orange).",
                "position": 994
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/gsm8k_performance_comparison.png",
                "caption": "",
                "position": 997
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/math_hard_leaderboard_performance_comparison.png",
                "caption": "",
                "position": 1001
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/mathqa_performance_comparison.png",
                "caption": "",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/minerva_math_performance_comparison.png",
                "caption": "",
                "position": 1010
            }
        ]
    },
    {
        "header": "3.  Pretraining",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22448/x9.png",
                "caption": "Figure 9:Model’s memorization window and loss trajectives",
                "position": 1342
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x10.png",
                "caption": "Figure 10:Behavior of weight norms of embeddingWembW_{\\mathrm{emb}}italic_W start_POSTSUBSCRIPT roman_emb end_POSTSUBSCRIPT, unembeddingWunembW_{\\mathrm{unemb}}italic_W start_POSTSUBSCRIPT roman_unemb end_POSTSUBSCRIPT, and input/output projectionsWin,WoutW_{\\mathrm{in}},W_{\\mathrm{out}}italic_W start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPTlayers of pure Mamba2 model.(Left)Evolution of weight norms during training of 1B model. In addition to no WD run, we show WD sweep (solid) and LR sweep (dashed), both of which display similar impact on weight norms. Learning rate is measured in the units of10−510^{-5}10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT; and the jump of norms at 10GT corresponds to batch size doubling within rampup.(Right)Average weight norm across all model layers at two-dimensional(η,λ)(\\eta,\\lambda)( italic_η , italic_λ )sweep on 300M model. The combinationsλη\\sqrt{\\frac{\\lambda}{\\eta}}square-root start_ARG divide start_ARG italic_λ end_ARG start_ARG italic_η end_ARG end_ARGandη​λ\\sqrt{\\eta\\lambda}square-root start_ARG italic_η italic_λ end_ARGare used as coordinate axes to demonstrate that weight norms are scaled as in (10). LR and WD are measured in the units ofη0=256×10−5\\eta_{0}=256\\times 10^{-5}italic_η start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 256 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPTandλ0=0.1\\lambda_{0}=0.1italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0.1. Empty cells correspond to diverged runs.",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x11.png",
                "caption": "Figure 11:Effect of LRη\\etaitalic_ηand WDλ\\lambdaitalic_λof the training loss curve of a 300M pure Mamba2 model.(Left)LR and WD sweeps on left and right subplots show that increasing (or decreasing) either LR or WD has a similar effect on the loss curve.(Right)Noise level measure as the loss gap before and after LR decayLnoise=Lbefore LRD−Lafter LRDL_{\\text{noise}}=L_{\\text{before LRD}}-L_{\\text{after LRD}}italic_L start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT = italic_L start_POSTSUBSCRIPT before LRD end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT after LRD end_POSTSUBSCRIPTat two-dimensional(η,λ)(\\eta,\\lambda)( italic_η , italic_λ )sweep. Again, the use of ELR and EWD for coordinate axis shows that the noise is mostly determined byηeff=η​λ\\eta_{\\mathrm{eff}}=\\sqrt{\\eta\\lambda}italic_η start_POSTSUBSCRIPT roman_eff end_POSTSUBSCRIPT = square-root start_ARG italic_η italic_λ end_ARG.",
                "position": 1584
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x12.png",
                "caption": "Figure 12:Sensitivity of the loss with respect to all 35μ\\muitalic_μP multipliers we have tuned. The multipliers are organized into 4 groups as in table8, with matrix layers ELR and EWD multipliers are shown in the middle plot side-by-side for comparison. All 4 multiplier groups have different magnitudes of the effect on the loss, and the y-axis limits were adjusted accordingly. ELR multipliers have the strongest impact, followed by foraward multipliers, then EWD multipliers, and lastly LR multipliers of vector-like layers. Although the sensitivities w.r.t. to the last group are low, note that the final multiplier values are quite far from those of the matrix layers (see table8), and, therefore, separating LRs of vector-like and matrix-like layers (as was done in our tuning) still yields significant performance improvement.",
                "position": 2137
            },
            {
                "img": "https://arxiv.org/html/2507.22448/x13.png",
                "caption": "Figure 13:Rampup and warmup related observations on a 1.5B pure Mamba2 model.(Top Left)Train loss trajectories during rampup with and without batch scaling. No batch scaling run exhibits loss jumps at the moments of batch size increase. These jumps could be associated with a decrease in the noise level.(Top Right)Loss after learning rate decay at 70GT on a LR sweep for different rampup strategies.(Bottom Left)Evolution of the loss for runs with and without BS scaling and 3 different learning rates. To take into account the noise level, we measure the loss after LR decay, which is performed at several positions of the training trajectory. To be able to display losses at very distant moments of training, we subtract the loss of the reference run. We see that, for all considered learning rates, the runs with batch scaling have a better trend with longer training duration.(Bottom Right)Evolution of the loss gap between runs with different warmup durations. Unlike the previous plot, the loss is measured directly (without LR decay) because warmup duration does not seem to affect noise level, as can be seen from the last predecay measurement at 60GT and after decay measurement at 70GT being very close. We see that at early stages (≲16\\lesssim 16≲ 16GT) loss vs warmup duration curve shifts with measurement time, reflecting that short warmup runs simply had more high LR steps at the beginning of the training. However, at later stages (≳16\\gtrsim 16≳ 16GT) the curve stabilizes, suggesting a well-defined optimal warmup duration with long-lasting effect on the training.",
                "position": 2146
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/mp_throughput/mp_colored_v3.png",
                "caption": "Figure 14:Diagram illustrating the Mixer Parallelism (MP) strategies. Each row represents a full decoder layer. Output projection and all reduce operations are applied at the end of each layer.(Top)Without MP, all GPUs compute both mixer types sequentially.(Middle)Naive MP statically assigns GPUs to one mixer type for all layers.(Bottom)Interleaved MP alternates assignments per layer, achieving better load balancing.",
                "position": 2346
            },
            {
                "img": "https://arxiv.org/html/2507.22448/figures/mp_throughput/mp_3b_7b.png",
                "caption": "Figure 15:Throughput comparison for Mixer Parallelism across different model sizes (3B and 7B).",
                "position": 2388
            }
        ]
    },
    {
        "header": "4.  Post-trainining",
        "images": []
    },
    {
        "header": "5.  Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.22448/figures/throughput.png",
                "caption": "Figure 16:Model efficiency comparison between Falcon-H1-34B and Qwen2.5-32B.",
                "position": 6283
            }
        ]
    },
    {
        "header": "6.  Model Integrations",
        "images": []
    },
    {
        "header": "7.  Conclusion",
        "images": []
    },
    {
        "header": "8.  Authors",
        "images": []
    },
    {
        "header": "9.  Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ALanguages used for training Falcon-H1 tokenizers",
        "images": []
    },
    {
        "header": "Appendix BScalar stochastic dynamics with weight decay",
        "images": []
    },
    {
        "header": "Appendix CTuningμ​P\\mu Pitalic_μ italic_Pmultipliers",
        "images": []
    },
    {
        "header": "Appendix DDetailed evaluation results",
        "images": []
    },
    {
        "header": "Appendix ETraining Data",
        "images": []
    }
]