[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22143/x1.png",
                "caption": "Figure 1.Video dubbing via joint audio–visual generation.Top: an input video with spoken dialogue in the source language.Bottom: the same video dubbed into a target language, generated by our trained model built on top of an audio–visual foundation backbone. Translated speech and lip motion are produced jointly, while the visual context (such as scene dynamics, face expressions, and body movements), speaker identity, and non-speech events (e.g., pauses, background sounds) are preserved.",
                "position": 166
            }
        ]
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22143/x2.png",
                "caption": "Figure 2.Pipeline for Generating Paired Audio-Visual Dubbing Data.The pipeline consists of two stages. First, the audio–visual generation model produces a contiguous sequence containing a context clip (e.g., spoken English) followed by a target clip (e.g., spoken French).\nSecond, the audio and lip-region video of the target clip are masked, and the same unified model is used in an inpainting setting to regenerate the masked content, conditioned on the context clip and a new text prompt (e.g., re-dubbing the target into English).",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x3.png",
                "caption": "Figure 3.The Identity–Pronunciation Trade-off.Naïve audio inpainting reveals a fundamental conflict between preserving speaker identity and achieving linguistically correct pronunciation. When denoising from scratch (Left), the model exhibitsvoice drift, failing to preserve the speaker’s vocal identity. When conditioning on the source audio to maintain identity (Middle), phonetic and prosodic patterns leak across languages, resulting inprosody leakage. Our approach (Right) resolves this trade-off by conditioning generation on a reference clip that preserves speaker identity while exhibiting the target-language phonetic style.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x4.png",
                "caption": "Figure 4.Model Training.Our framework follows an in-context generation paradigm where clean context audio-visual pairs are concatenated with noised target pairs. We fine-tune only LoRA adapters while keeping a pre-trained Audio-Visual (AV) Diffusion Transformer frozen. Conditioned on a text prompt (e.g. “The person is speaking in French”), the model learns to propagate edits from the context while maintaining temporal synchronization between audio and video. We introduce a modality-specific masking strategy in AV cross-attention, ensuring that noisy audio attends only to noisy video and vice versa, since conditioning a noisy signal on clean context from the other modality leads to signal leakage and conflicting guidance, which this masking prevents.",
                "position": 285
            }
        ]
    },
    {
        "header": "4.Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22143/x5.png",
                "caption": "Figure 5.User Study Results.We compare our method against LatentSync and HeyGen through a user study, evaluating Lip Synchronization, Prompt Adherence, and Overall Quality. Results indicate that participants prefer our method over baselines across all evaluated metrics.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x6.png",
                "caption": "(a)Duration Alignment.Our method precisely matches the original video’s duration and synchronizes translated audio, effectively avoiding baseline artifacts such as reversed motion (“Rolling back”) or mismatched durations.",
                "position": 542
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x6.png",
                "caption": "(a)Duration Alignment.Our method precisely matches the original video’s duration and synchronizes translated audio, effectively avoiding baseline artifacts such as reversed motion (“Rolling back”) or mismatched durations.",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x7.png",
                "caption": "(b)Preservation of Non-Dialogue Events.Our method naturally preserves crucial non-dialogue audio events (e.g., pauses, laughter), leveraging visual cues, while baseline methods ignore these segments entirely.",
                "position": 551
            }
        ]
    },
    {
        "header": "5.Conclusions",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22143/x8.png",
                "caption": "Figure 7.Preservation of Non-Dialogue Events and Scene Grounding.Our joint generative framework enables holistic scene modeling where visual dynamics and acoustic events co-evolve.Top: In the ”dog barking” scenario, our method synchronizes the timing of environmental sounds with physical gestures, whereas baselines like HeyGen often omit or misalign these cues.Bottom: In the ”eating while talking” example, our unified model elegantly manages cross-lingual duration mismatches. When translating the source English dialogue (9 syllables) into a longer French equivalent (11 syllables), the framework adaptively inserts speech frames during the speaker’s chewing phase rather than when the mouth is obstructed by food. As demonstrated in the supplemental materials, the resulting output faithfully reproduces the acoustic texture of ”talking through chewing,” showcasing a bidirectional interaction between the scene’s physical actions and the generated audio.",
                "position": 706
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x9.png",
                "caption": "Figure 8.Qualitative Comparisons.Top: Profile views and occlusions.Bottom: Non-human scenarios.\nBaseline methods exhibit noticeable artifacts and often fail under these challenging conditions, while our method robustly preserves identity and visual coherence while synchronizing the lips with our generated translation audio.",
                "position": 731
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x10.png",
                "caption": "",
                "position": 735
            }
        ]
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.22143/x11.png",
                "caption": "Figure 9.Example of sampled benchmark videos.HDFT and TalkVid consist mainly frontal faces, limited pose variation, and clean\nacoustic conditions. Our curated benchmark exhibits profile views, significant pose shifts, occlusions, and stylized appearances.",
                "position": 1538
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x12.png",
                "caption": "Figure 10.We overlay a green mask on the lip region of the input video and encode the corrupted video using the Video VAE. Left: Corrupted input. Middle: Output without Latent-Aware Fine Masking, showing pronounced green light artifacts around the mouth and lower face due to latent information leakage from the masked region. Right: Output with Latent-Aware Fine Masking, where leakage-induced artifacts are eliminated and coherent, audio-aligned facial motion is regenerated.",
                "position": 1542
            },
            {
                "img": "https://arxiv.org/html/2601.22143/x13.png",
                "caption": "Figure 11.Effect of Lip Augmentation on Data Generation Diversity. We present visualizations on original video frames (top row), inpainted results without lip augmentation (middle row), and inpainted results with lip augmentation (bottom row).",
                "position": 1552
            },
            {
                "img": "https://arxiv.org/html/2601.22143/figures/supp/Async_overfit.png",
                "caption": "Figure 12.ASync metrics overfit to frontal videos.",
                "position": 1555
            }
        ]
    },
    {
        "header": "Appendix BAdditional Visualizations",
        "images": []
    }
]