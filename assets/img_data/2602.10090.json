[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10090/figures/logo8.png",
                "caption": "",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10090/x1.png",
                "caption": "Figure 1:Agent World Model (AWM) is a synthetic environment generation pipeline that synthesizes 1,000 diverse code-driven agentic environments with databases for training tool-use agents.",
                "position": 133
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x2.png",
                "caption": "Figure 2:Overview ofAWM. Starting from scenario synthesis, we progressively generate tasks, database, interface and verification to obtain fully executable environments. Then, we perform multi-turn RL training for tool-use agents in our synthesized environments.",
                "position": 155
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Agent World Model",
        "images": []
    },
    {
        "header": "4Agentic Reinforcement Learning",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10090/x3.png",
                "caption": "Figure 3:Diversity analysis of 1,000 synthesized environments. (a) Embedding diversity is calculated by encoding the scenario description, database schema and toolset schema. (b) Category coverage counts the number of unique topics of scenarios.",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x4.png",
                "caption": "Figure 4:Scaling ofAWMover environment sizes with 4B model.",
                "position": 1326
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Use of AI Assistants",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.10090/figures/step_reward_cmp.png",
                "caption": "Figure 5:Format error ratio comparison ofAWM. “w/o Format” means disabling the step-level format correctness reward.",
                "position": 2564
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x5.png",
                "caption": "Figure 6:Distribution of synthesized scenarios ofAWM.",
                "position": 2590
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x5.png",
                "caption": "Figure 6:Distribution of synthesized scenarios ofAWM.",
                "position": 2593
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x6.png",
                "caption": "Figure 7:Wordcloud of the scenario descriptions ofAWM.",
                "position": 2598
            },
            {
                "img": "https://arxiv.org/html/2602.10090/figures/database_view.png",
                "caption": "Figure 8:Visualization of part of the SQLite database schema of “Spotify” environment.",
                "position": 2604
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x7.png",
                "caption": "Table 10:Example of interface schema snippet with rich annotations and explicit database constraints.",
                "position": 3921
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x7.png",
                "caption": "Figure 28:Case study for verification: code-based verifier and LLM judge fully align on a clean, database-grounded success signal.",
                "position": 4967
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x8.png",
                "caption": "Figure 29:Case study for verification: Tool/Infrastructure error produces a false negative for code-only verification, while the judge uses trajectory context to recover the correct judgment.",
                "position": 4973
            },
            {
                "img": "https://arxiv.org/html/2602.10090/x9.png",
                "caption": "Figure 30:Case study for verification: Tool calling ambiguity causes a “wrong-entity” action that looks successful locally; verification grounded in the true database state prevents a false positive.",
                "position": 4979
            }
        ]
    },
    {
        "header": "Appendix BAnalysis",
        "images": []
    }
]