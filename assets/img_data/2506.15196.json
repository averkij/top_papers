[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/framework.png",
                "caption": "Figure 1:Overview of the HeurAgenix framework for automatic heuristic design and adaptive selection. In the heuristic evolution phase, an LLM autonomously discovers evolution strategies by analyzing contrastive solution tuples, while in the problem solving phase, an adaptive heuristic selection mechanism integrates Test-time Scaling (TTS)wei2022chain;wang2022self.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Preliminary and Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/single_evolution_example.png",
                "caption": "Figure 2:Illustration of one heuristic-evolution step on a four-node TSP evolution instance. The cumulative effect of this and subsequent refinements can be seen in Figure3.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/evolution_example.png",
                "caption": "Figure 3:Example of heuristic evolution for TSP. The left panel illustrates successive strategy refinements and their impact on TSPLIBreinelt1991tsplibperformance. The right panel details a specific evolution step, where an alternative cost function is induced by the LLM based on counterfactual analysis. For a step-by-step extraction of the refinement highlighted in Round 2, see Figure2and for further details and code, see AppendixC.",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/reward.png",
                "caption": "Figure 4:Detailed reward design, showing the operational mechanisms of the novel POR and CPR as well as auxiliary Format Rewardgrpoand Language Rewardsdeepseek2025.",
                "position": 727
            },
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/exp_reward.png",
                "caption": "Figure 5:Effect of noisy rollout data on heuristic selection (rd100in TSPLIBreinelt1991tsplib).\nY-axis: expected optimality gap (lower is better) after completing the tour by random sampling.\nX-axis: decision rounds.\nBlue: always selecting the best heuristic (oracle).\nGreen: uniformly selecting from the top 30% heuristics (positive set).\nRed: random selection.\nSelecting from the positive set almost matches the oracle and clearly outperforms random choice.",
                "position": 739
            },
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/CPR.png",
                "caption": "Figure 6:Impact of the CPR mechanism in rectifying qualitative judgment errors during LLM inference. The illustration shows how CPR guides the model toward accurate contextual assessments, mitigating errors that might otherwise remain uncorrected.",
                "position": 862
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/evolution_result.png",
                "caption": "Figure 7:Average optimality gaps (%; lower is better) before and after heuristic evolution on five representative CO problems. Complete results for all instances appear in AppendixF.1.",
                "position": 1032
            },
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/problem_solving_with_llm.png",
                "caption": "Figure 8:Average optimality gap (%; lower is better) of HeurAgenix and baselines on five CO benchmarks. Dark-blue bars correspond to HeurAgenix (Ours), draw-orange bars to EoH related\nmethod, light-orange bars to ReEvo related methods, and gray bars to traditional methods.\nA dagger (â€ ) after a method name indicates that the result is copied from the original publication.\nComplete results for all instances appear in AppendixF.2.",
                "position": 1051
            },
            {
                "img": "https://arxiv.org/html/2506.15196/extracted/6567559/figure/tts_influence.png",
                "caption": "Figure 9:Impact of rollout budget on pr152 from TSPLIBreinelt1991tsplib.\nX-axis: Monte Carlo samples per candidate heuristic (rollout budget; 0 = direct use of the LLM-proposed heuristic with no search). Base model is Qwen-7B.\nY-axis: optimality gap (%; Lower is better).",
                "position": 1313
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AHeuristic Format",
        "images": []
    },
    {
        "header": "Appendix BInitial Heuristic Generation",
        "images": []
    },
    {
        "header": "Appendix CHeuristic Evolution Example",
        "images": []
    },
    {
        "header": "Appendix DMonte Carlo Evaluation Strategy",
        "images": []
    },
    {
        "header": "Appendix EDetailed Parameter Settings",
        "images": []
    },
    {
        "header": "Appendix FDetailed Experiment Results",
        "images": []
    }
]