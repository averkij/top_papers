[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x1.png",
                "caption": "",
                "position": 70
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x2.png",
                "caption": "Figure 2:Steer3D data engine: we begin with one rendered view for each Objaverese object and query an LLM for diverse editing instructions. Then, a 2D editing model is used to generate an edited view, which is subsequently reconstructed into a 3D asset. Editing pairs pass through a two-stage filter, in which approximately 30% of all generated pairs are kept.",
                "position": 131
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x3.png",
                "caption": "Figure 3:Steer3D architecture: we design a ControlNet-based architecture to leverage the shape and geometry prior of pretrained image-to-3D generative models. We add a trainable ControlNet block corresponding to each transformer block in the base model.",
                "position": 166
            }
        ]
    },
    {
        "header": "4Edit3D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x4.png",
                "caption": "Figure 4:We presentEdit3D-Bench, which contains diverse edits spanning texture, addition, and removal. It comprises 250 objects and 250 distinct edits, which is41.6×41.6\\timesthe size of the existing commonly-evaluated objects from InstructNerf2Nerf.",
                "position": 271
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x5.png",
                "caption": "Figure 5:(a): Qualitative comparison of our method with baselines. Steer3D shows strong advantage both in editing correctness and in consistency with the pre-edit 3D asset. Steer3D demonstrates strong localization capability – e.g., editing only the chair’s hollow backrest, while baselines like Edit-TRELLIS and LL3M fail by modifying the whole chair. (b): More qualitative examples of Steer3D. (c): Steer3D is able to perform edits that are not visible in the input view, whereas Edit-TRELLIS, which relies on front-view 2D editing, fails.",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x6.png",
                "caption": "Figure 6:Ablations and scaling analysis of Steer3D. (a): DPO effectively alleviates the failure mode of “no edit” predictions. (b): Ablations on the ControlNet architecture and data filtering. “No ControlNet” corresponds to simply adding text as another conditioning signal and finetuning the base model. Both the ControlNet design and data filtering improve all metrics. (c): We see a steady improvement on both LPIPS (lower better) and Chamfer Distance as the training data size increases. Reported metrics are evaluated on the unseen addition set.",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x7.png",
                "caption": "Figure 7:Inference time comparison. Steer3D is the fastest –2.4×2.4\\timesto28.5×28.5\\timesfaster than competing methods.",
                "position": 647
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x8.png",
                "caption": "Figure 8:Limitations of Steer3D. For complex edits, Steer3D may yield inconsistency (“add potted plant”, “remove orange petals”, edit leaking (“change the chair red”), or partial edits (“remove gold chain”).",
                "position": 655
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "Appendix AAdditional Qualitative Examples of Steer3D",
        "images": []
    },
    {
        "header": "Appendix BAdditional Examples ofEdit3D-Bench",
        "images": []
    },
    {
        "header": "Appendix CHuman Evaluation",
        "images": []
    },
    {
        "header": "Appendix DAdditional Training Details and Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x9.png",
                "caption": "Figure 9:Scaling analysis on texture edits, evaluated on unseen assets. As the training size grows, LPIPS decreases steadily, showing the importance of scaling up. The x axis is plotted on log scale.",
                "position": 786
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x10.png",
                "caption": "Figure 10:Additional qualitative examples onEdit3D-Bench. Pre-Edit (3D) shows the base image-to-3D model generation that we want to steer. Steer3D output is shown in 4 views.",
                "position": 907
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x11.png",
                "caption": "Figure 11:Additional qualitative examples onEdit3D-Bench. Pre-Edit (3D) shows the base image-to-3D model generation that we want to steer. Steer3D output is shown in 4 views.",
                "position": 911
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x12.png",
                "caption": "Figure 12:Additional qualitative examples on challenging in-the-wild objects, captured by iPhone photos, online photo, or AI-generated image. Image-to-3D shows the original base model output based on the input image. Steer3D output is shown in 4 views. We show best-of-3 output for in-the-wild evaluation.",
                "position": 915
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x13.png",
                "caption": "Figure 13:We provide additional examples fromEdit3D-Bench, which shows thatEdit3D-Benchis diverse.",
                "position": 919
            },
            {
                "img": "https://arxiv.org/html/2512.13678/figs/humaneval_ui.png",
                "caption": "Figure 14:User interface for human evaluation. The top row shows the pre-edit 3D asset rendered in 4 views. The middle and bottom rows are two blinded methods in randomized order per example. The user presses “A” for method A, “B” for method B, or “T for a tie.",
                "position": 924
            }
        ]
    },
    {
        "header": "Appendix EAdditional Data Engine Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.13678/x14.png",
                "caption": "Figure 15:LLM-based filter to filter out image editing errors. One LLM, without knowing the editing instruction, is asked to list the differences between two renderings of the object from the same camera angle. Another LLM compares whether the difference matches exactly with the edit instruction. Objects that fail this check are rejected.",
                "position": 943
            },
            {
                "img": "https://arxiv.org/html/2512.13678/x15.png",
                "caption": "Figure 16:2D perceptual similarity (DreamSim) score to filter out reconstruction inconsistency. Consistent objects yield a low distance, and inconsistent objects yield a high distance.",
                "position": 951
            }
        ]
    },
    {
        "header": "Appendix FAuthor Contributions",
        "images": []
    }
]