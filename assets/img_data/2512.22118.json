[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22118/x1.png",
                "caption": "Figure 2:Framework comparison between (a) previous methods and (b) our method. To address the issue of excessive source image information injection, we introduce the Shift module for inverted noise and the Mix module for the attention injection, alleviating the editing failures caused by these issues.",
                "position": 99
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22118/x2.png",
                "caption": "Figure 3:Excessive source image information injection phenomenon in RF-Solver. We validate it by visualizing the attention from source and target text tokens to the visual tokens during initial and sampling stage. In RF-Solver, the attention from the source text token to the visual tokens remains higher than that from the target text token. However, after removing attention injection, the attention from “black” and “orange” to visual tokens returns to similar levels, but some subject attributes (e.g., pose) change accordingly.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2512.22118/x3.png",
                "caption": "Figure 4:Pipeline of our ProEdit.The mask extraction module identifies the edited region based on source and target prompts during the first inversion step. After obtaining the inverted noise, we apply Latents-Shift to perturb the initial distribution in the edited region, reducing source image information. In selected sampling steps, we fuse source and target attention features in the edited region while directly injecting source features in non-edited regions to achieve accurate attribute editing and background preservation simultaneously.",
                "position": 251
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22118/x4.png",
                "caption": "Figure 5:Qualitative comparison on image editing. With our method, various flow-based inversion methods achieve more appropriate editing while preserving the consistency of background and non-editing content.",
                "position": 575
            },
            {
                "img": "https://arxiv.org/html/2512.22118/x5.png",
                "caption": "Figure 6:Visualization of attention map after performing ProEdit. The initial distribution is shifted to target prompt and during the sampling, the model can accurately edit the image while maintaining non-editing attribute and background concsistent.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2512.22118/x6.png",
                "caption": "Figure 7:Qualitative comparison on video editing. The video comprises 48 frames with a resolution of540×960540\\times 960.",
                "position": 591
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AExtracting Mask From Attention Map",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22118/x7.png",
                "caption": "Figure 8:A visual comparison of the editing region mask extracted from the last Double block and all blocks.Using ”orange” as the editing target, the editing region masks extracted from both the last Double block and all blocks effectively segment the editing region.",
                "position": 786
            }
        ]
    },
    {
        "header": "Appendix CQuantitative Results of Attention Feature Combination Effect",
        "images": []
    },
    {
        "header": "Appendix DMore Qualitative Results for Image Editing",
        "images": []
    },
    {
        "header": "Appendix EMore Qualitative Results for Video Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.22118/x8.png",
                "caption": "Figure 9:More qualitative comparison of image editing on PIE-Bench[ju2023direct].",
                "position": 863
            },
            {
                "img": "https://arxiv.org/html/2512.22118/x9.png",
                "caption": "Figure 10:More video editing results.",
                "position": 866
            },
            {
                "img": "https://arxiv.org/html/2512.22118/x10.png",
                "caption": "Figure 11:Qualitative results of image editing based on editing instruction.The actual input editing instruction are shown above each source image and its corresponding edited image.",
                "position": 869
            }
        ]
    },
    {
        "header": "Appendix FEditing by Instruction",
        "images": []
    }
]