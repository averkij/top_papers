[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/x1.png",
                "caption": "Figure 1:Our proposed synthetic data generation pipeline can automatically produce high-quality and highly diverse video editing data, encompassing both global and local editing tasks. We highly recommend the readers to see the supplementary video samples.",
                "position": 82
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/x2.png",
                "caption": "Figure 2:Our scalable data synthesis pipeline. (1) Pre-processing: A diverse video pool is curated via automated deduplication and motion filtering. (2) The core engine synthesizes video triplets, conditioning an in-context generator on automated instructions, appearance context from edited key-frames, and structural context from depth maps. (3) Post-processing: Final visual quality is guaranteed by a VLM-based filter and a denoising enhancer.",
                "position": 157
            }
        ]
    },
    {
        "header": "3Ditto-1M",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/x3.png",
                "caption": "Figure 3:Source categories.",
                "position": 229
            },
            {
                "img": "https://arxiv.org/html/2510.15742/x3.png",
                "caption": "Figure 3:Source categories.",
                "position": 231
            }
        ]
    },
    {
        "header": "4Model Training via Modality Curriculum Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/x4.png",
                "caption": "Figure 4:Model training pipeline. We train the context blocks based on the in-context video generator with curriculum learning by graduallyannealingand eventuallydroppingthe reference frame.",
                "position": 385
            },
            {
                "img": "https://arxiv.org/html/2510.15742/x5.png",
                "caption": "Figure 5:Qualitative comparisons with prior arts TokenFlow(Geyer et al.,2024), InsV2V(Cheng et al.,2024), InsViE(Wu et al.,2025b)and Gen4-Aleph(Runway,2025).",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2510.15742/x6.png",
                "caption": "Figure 6:Our data and learned model enable the translation from synthetic videos to the real domain.",
                "position": 478
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/x7.png",
                "caption": "Figure 7:Unlike the original data generator, which fails to handle newly emerging informationbeyond key frames, our model - trained with filtering and scaling techniques - outperforms it.",
                "position": 493
            },
            {
                "img": "https://arxiv.org/html/2510.15742/x8.png",
                "caption": "Figure 8:Ablation studies on training data scale and modality curriculum learning (MCL).",
                "position": 496
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix Overview",
        "images": []
    },
    {
        "header": "Appendix BJustifying the Design of Data Generation Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/x9.png",
                "caption": "Figure 9:Results of various settings for data generation.",
                "position": 1069
            }
        ]
    },
    {
        "header": "Appendix CDemonstration of User Study Interface",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15742/figures_supp/user_study_interface.png",
                "caption": "Figure 10:The interface of the user study.",
                "position": 1081
            },
            {
                "img": "https://arxiv.org/html/2510.15742/x10.png",
                "caption": "Figure 11:Additional visualization of data from the proposed dataset and model outputs. Please view the site for additional video samples.",
                "position": 1087
            },
            {
                "img": "https://arxiv.org/html/2510.15742/figures_supp/wordcloud.png",
                "caption": "Figure 12:The word cloud of editing instructions.",
                "position": 1094
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results of Dataset and Model",
        "images": []
    }
]