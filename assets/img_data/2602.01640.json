[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01640/x1.png",
                "caption": "Figure 1:(a): Capacity distributions across existing expert-defined and manually annotated embodied VLM benchmarks reveal redundant yet sparse coverage, leading to ranking distortion and excessive evaluation costs.(b): A2Eval replaces this expert-defined, annotation-heavy paradigm with an automated suite that maintains capability coverage while compressing benchmarks, achieving 4.6×\\timesspeedup and improved human alignment. (Details in Tables3and5.)",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x2.png",
                "caption": "",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x3.png",
                "caption": "Figure 2:Overview of A2Eval. Thelefthalf shows theData Agentwith three roles, Proposer, Reviewer, and Assigner, which constructs compact, balanced benchmarks. Therighthalf shows theEval Agentwith two roles, Evaluator and Scorer, which produces model predictions and scores. Each role is represented by a distinct color.",
                "position": 245
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01640/x4.png",
                "caption": "Figure 3:UMAP visualization of sample embeddings, which demonstrates that our unified subset maintains comprehensive semantic coverage despite high sparsity.",
                "position": 1153
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x4.png",
                "caption": "",
                "position": 1155
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x5.png",
                "caption": "",
                "position": 1159
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ABenchmark Description and Redundancy Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01640/plot.png",
                "caption": "Figure 4:Heatmap of inter-benchmark similarity. The results reveal severe redundancy in the current evaluation ecosystem, with benchmarks exhibiting up to91%overlap (e.g., RefSpatial vs. Where2Place).",
                "position": 2010
            },
            {
                "img": "https://arxiv.org/html/2602.01640/balance.png",
                "caption": "Figure 5:Capability distribution across dimensions before and after diversity-aware sampling. The retained set exhibits a substantially more balanced coverage compared to the highly skewed source distribution.",
                "position": 2126
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x6.png",
                "caption": "Figure 6:UMAP visualization of sample embeddings across all dimensions.",
                "position": 2129
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x7.png",
                "caption": "",
                "position": 2138
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x8.png",
                "caption": "",
                "position": 2143
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x9.png",
                "caption": "",
                "position": 2148
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x10.png",
                "caption": "",
                "position": 2154
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x11.png",
                "caption": "",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x12.png",
                "caption": "",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2602.01640/x13.png",
                "caption": "",
                "position": 2169
            }
        ]
    },
    {
        "header": "Appendix BDetailed Evaluation Setting",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details of the Evaluation Agents",
        "images": []
    },
    {
        "header": "Appendix DHuman–Agent Agreement Protocol",
        "images": []
    },
    {
        "header": "Appendix EHuman-Centric Evaluation Protocol",
        "images": []
    },
    {
        "header": "Appendix FDetailed Fidelity Analysis",
        "images": []
    }
]