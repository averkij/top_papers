[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/x1.png",
                "caption": "",
                "position": 149
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Human Preference Dataset v3",
        "images": []
    },
    {
        "header": "4Human Preference Score v3",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/x2.png",
                "caption": "Figure 2:An overview of HPDv3 and HPSv3.HPDv3 integrates both real-world collected and generated images. HPSv3 employs a VLM backbone to extract rich semantic representations from images and captions, then utilizes uncertainty-aware ranking to effectively learn human preferences from paired comparison data.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x3.png",
                "caption": "Figure 3:The overview of Chain-of-Thought rounded image generation withHPSv3(CoHP).CoHP incorporates both model-wise and sample-wise preferences, selected byHPSv3, to build a thinking-and-choosing image generation process.",
                "position": 621
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/figure/model_metrics_comparison_correct_ordered.png",
                "caption": "Figure 4:Ranking of generative models across different metrics.HPSv3 shows the highest correlation with human annotation, indicating its superior performance in reflecting human preferences.",
                "position": 1070
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x4.png",
                "caption": "Figure 5:Image generation with Chain-of-Human-Preference (CoHP).It illustrates that the quality of the generated images improves progressively through CoHP. The first row displays all the candicate images generated by participant models (Flux-dev, Kolors, Playground v2.5, and SD3).",
                "position": 1248
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x5.png",
                "caption": "Figure 6:Comparison of CoHP results with different preference models.CoHP-HPSv3 shows the best coherence and fidelity, while others suffer from oversaturation or instability.",
                "position": 1324
            },
            {
                "img": "https://arxiv.org/html/2508.03789/figure/human_evaluation.png",
                "caption": "Figure 7:Human evaluation on CoHP with different human preference models.HPSv3 outperforms all other human preference models by a large margin.",
                "position": 1341
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "1Image Sources of HPDv3",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/x6.png",
                "caption": "Figure S1:Image numbers per prompt of each dataset.",
                "position": 2220
            }
        ]
    },
    {
        "header": "2Category distribution of HPDv3",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/figure/supp_category_distribution.png",
                "caption": "Figure S2:Distribution of real images across1212categories collected from the Internet.",
                "position": 2233
            }
        ]
    },
    {
        "header": "3HPDv3Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/figure/realimage_category.png",
                "caption": "Figure S3:Category distribution of high-quality real images in HPDv3 dataset.",
                "position": 2270
            },
            {
                "img": "https://arxiv.org/html/2508.03789/figure/gender_age.png",
                "caption": "Figure S4:Demographic Profile of Annotators: Gender Distribution and Age Stratification.",
                "position": 2308
            },
            {
                "img": "https://arxiv.org/html/2508.03789/figure/annotation_inferface.png",
                "caption": "Figure S5:Annotation interface of pairwise image comparison.",
                "position": 2311
            }
        ]
    },
    {
        "header": "4Annotation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/figure/convergency_by_category.png",
                "caption": "Figure S6:Average convergency score by categories.",
                "position": 2499
            }
        ]
    },
    {
        "header": "5HPSv3Training Details",
        "images": []
    },
    {
        "header": "6HPDv3Dataset Visualization",
        "images": []
    },
    {
        "header": "7More Result of CoHP",
        "images": []
    },
    {
        "header": "8HPSv3 as Reward Model",
        "images": []
    },
    {
        "header": "9Term of Use of HPDv3",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.03789/x7.png",
                "caption": "Figure S10:HPDv3Dataset Visualization.Our dataset contains a diverse range of images spanning multiple categories including animals, architecture, characters, and other subjects. Each row displays different samples demonstrating the variety and quality of the dataset.",
                "position": 3088
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x7.png",
                "caption": "",
                "position": 3091
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x8.png",
                "caption": "",
                "position": 3096
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x9.png",
                "caption": "",
                "position": 3101
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x10.png",
                "caption": "Figure S11:More Result of CoHP",
                "position": 3107
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x11.png",
                "caption": "Figure S12:More Result of CoHP",
                "position": 3110
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x12.png",
                "caption": "Figure S13:More Result of CoHP with different human preference models",
                "position": 3113
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x13.png",
                "caption": "Figure S14:More Result of CoHP with different human preference models",
                "position": 3116
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x14.png",
                "caption": "Figure S15:Results of DanceGRPO Using Different human preference models",
                "position": 3119
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x15.png",
                "caption": "Figure S16:Results of DanceGRPO Using Different human preference models",
                "position": 3122
            },
            {
                "img": "https://arxiv.org/html/2508.03789/x16.png",
                "caption": "Figure S17:More Results of DanceGRPO Using HPSv3 as Reward Model Based on SD1.4",
                "position": 3125
            }
        ]
    },
    {
        "header": "10Limitation",
        "images": []
    }
]