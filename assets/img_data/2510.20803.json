[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20803/x1.png",
                "caption": "Figure 1:ARGenSeg is a unified framework for visual understanding, segmentation, and generation. It supports semantic, instance, interactive, and zero-shot reasoning segmentation, as well as anomaly detection, by leveraging strong visual understanding capabilities.",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20803/x2.png",
                "caption": "Figure 2:The architecture of ARGenSeg and its training and inference procedures.Left:ARGenSeg integrates image segmentation into the MLLM via an autoregressive image generation paradigm. A unified classification prediction head is used to generate both text and visual tokens.Right:Visual tokens are generated in parallel using the next-scale prediction strategy. During training, a VAE encoder is used to construct supervision for cross-entropy loss. During inference, the VAE decoder reconstructs the image from the predicted visual tokens.[S]/[E]denotes<gen_start>/<gen_end>.",
                "position": 322
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20803/x3.png",
                "caption": "Figure 3:Multi-scale generation process of the segmentation mask.\nThe model first localizes the target object and then progressively refines its boundaries.",
                "position": 853
            },
            {
                "img": "https://arxiv.org/html/2510.20803/x4.png",
                "caption": "Figure 4:Top:Visualization of interactive segmentation. Points and scribbles are provided as visual prompts, while bounding boxes are input via text.Bottom:Visualization results of instruction-based image generation. The model is trained on image generation data for only50â€‹k50kiterations.",
                "position": 964
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix of ARGenSeg",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Qualitative Results",
        "images": []
    },
    {
        "header": "Appendix CAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20803/x5.png",
                "caption": "Figure 5:Visualization of using different segmentation instructions in the same image.",
                "position": 2318
            },
            {
                "img": "https://arxiv.org/html/2510.20803/x6.png",
                "caption": "Figure 6:Comparison between multi-scale and single-scale generative segmentation approach. The examples highlight scenarios where the multi-scale approach excels.",
                "position": 2323
            }
        ]
    },
    {
        "header": "Appendix DAdditional Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.20803/x7.png",
                "caption": "Figure 7:Comparison between direct visual token generation and DiT-based generation. The DiT-based approach, which uses semantic embeddings from the MLLM, struggles with pixel-level accuracy, leading to artifacts like spatial shifts and imprecise boundaries.",
                "position": 2507
            }
        ]
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "Appendix FBroader Impacts",
        "images": []
    }
]