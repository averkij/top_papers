[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07491/extracted/6349196/figures/logo.png",
                "caption": "",
                "position": 123
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x1.png",
                "caption": "Figure 1:Comparison betweenKimi-VL-Thinkingand frontier open-source VLMs, including short-thinking VLMs (e.g.Gemma-3 series, Qwen2.5-VL series) and long-thinking VLMs (QVQ-72B-Preview), on MathVision benchmark. Our model achieves strong multimodal reasoning with just 2.8B LLM activated parameters.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x2.png",
                "caption": "Figure 2:Highlights ofKimi-VLperformance for a wide range of benchmarks like, general benchmarks (MMMU, MMBench), OCR (InfoVQA), multi-image (BLINK), long video (LongVideoBench, Video-MME), long document (MMLongBench-Doc), and agent (ScreenSpot-Pro and OSWorld). Detailed results are presented in Table3.",
                "position": 144
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07491/x3.png",
                "caption": "Figure 3:The model architecture of Kimi-VL and Kimi-VL-Thinking, consisting of a MoonViT that allows native-resolution images, an MLP projector, and a Mixture-of-Experts (MoE) language decoder.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x4.png",
                "caption": "Figure 4:The pre-training stages of Kimi-VL consume a total of 4.4T tokens after text-only pre-training of its language model. To preserve text abilities, all stages that update the language model are joint training stages.",
                "position": 228
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x5.png",
                "caption": "Figure 5:The post-training stages of Kimi-VL and Kimi-VL-Thinking, including two stages of joint SFT in 32K and 128K context, and further long-CoT SFT and RL stages to activate and enhance long thinking abilities.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x6.png",
                "caption": "Figure 6:Manuscript reasoning visualization. Kimi-VL-Thinking demonstrates the ability to perform historical and scientific inference by analyzing handwritten manuscripts step by step. In this example, our model identifies the author as Albert Einstein based on handwriting style, content analysis, and language cues. It reasons that the manuscripts relate to gravitational field equations, consistent with Einstein’s contributions to general relativity.",
                "position": 456
            }
        ]
    },
    {
        "header": "3Data Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07491/x7.png",
                "caption": "Figure 7:Kimi-VL exhibits strong visual reasoning capabilities by grounding visual content in spatial, contextual, and cultural knowledge. It accurately identifies matching urban locations based on structural and layout features, interprets scenes from video games like Cyberpunk 2077 using stylistic cues, and recognizes real-world landmarks such as the Rogers Centre in Toronto.",
                "position": 903
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07491/x8.png",
                "caption": "Figure 8:Kimi-VL demonstrates its capability to perform symbolic reasoning and geometric inference by solving a circle geometry problem step by step. The model analyzes given conditions, applies geometric theorems such as the inscribed angle theorem and properties of triangle angles, and accurately derives the target angle.",
                "position": 932
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x9.png",
                "caption": "Figure 9:Diverse OCR visualization. Kimi-VL demonstrates strong OCR capabilities across varied content types, including structured financial tables, complex mathematical formulas, and handwritten Chinese text. The model accurately parses tabular data into markdown, converts formulas to LaTeX, and transcribes handwritten paragraphs with contextual understanding, showcasing its versatility in multimodal text extraction and interpretation.",
                "position": 935
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x10.png",
                "caption": "Figure 10:Kimi-VL is capable of following multi-step reasoning processes to complete complex GUI tasks. In this example, it successfully enables the “Do Not Track” feature in the Chrome browser to enhance online privacy. The agent interprets each screen, identifies relevant UI elements, and performs the appropriate actions sequentially with clear thoughts, actions, and API calls.",
                "position": 938
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x11.png",
                "caption": "Figure 11:Video scene splitting.\nKimi-VL processes a long-form video by segmenting it into coherent scenes and providing detailed start/end timestamps along with fine-grained natural language descriptions for each scene.‡‡footnotemark:‡",
                "position": 941
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x12.png",
                "caption": "Figure 12:Catching and understanding key details from an hour-long video course.\nKimi-VL demonstrates its ability to comprehend and interpret instructional video content by analyzing frame sequences and extracting conceptual progression over time. In this case, the model identifies a deepening of the traditional saying “Teach a man to fish, and you feed him for a lifetime” into a more nuanced idea: “Teach him the taste of fish and make him hungry.”¶¶footnotemark:¶",
                "position": 945
            },
            {
                "img": "https://arxiv.org/html/2504.07491/x13.png",
                "caption": "Figure 13:Test-time accuracy when scaling the max thinking token length of ourKimi-VL-Thinkingmodel.",
                "position": 1086
            }
        ]
    },
    {
        "header": "5Conclusion, Limitation, and Future Work",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AContributions",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    }
]