[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12599/extracted/6143419/figures/logo.png",
                "caption": "",
                "position": 156
            },
            {
                "img": "https://arxiv.org/html/2501.12599/x1.png",
                "caption": "Figure 1:Kimi k1.5 long-CoT results.",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2501.12599/x2.png",
                "caption": "Figure 2:Kimi k1.5 short-CoT results.",
                "position": 172
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.12599/x3.png",
                "caption": "(a)System overview",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2501.12599/x3.png",
                "caption": "(a)System overview",
                "position": 623
            },
            {
                "img": "https://arxiv.org/html/2501.12599/x4.png",
                "caption": "(b)Partial Rollout",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2501.12599/x5.png",
                "caption": "Figure 4:Hybrid Deployment Framework",
                "position": 671
            }
        ]
    },
    {
        "header": "2Approach: Reinforcement Learning with LLMs",
        "images": []
    }
]