[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03193/figures/LOGO.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03193/x1.png",
                "caption": "Figure 1:Motivation of UniCorn. UMMs often exhibit an understanding-generation gap: they can accurately understand and critique errors in an image, yet fail to generate the same scene correctly. This conduction aphasia motivates our framework to leverage the model’s superior internal understanding to strengthen and refine its generative capabilities through self-contained feedback.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x2.png",
                "caption": "Figure 2:Visualization results ofUniCorn.",
                "position": 197
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03193/x3.png",
                "caption": "Figure 3:Results of BAGELbageland GPT-4ogpt4oon four understanding benchmarks.For Omini-RewardBenchjin2025omniand MMRB2hu2025multimodal, we evaluate the T2I task. Performances are normalized with GPT-4achiam2023gptresults for better visualization.",
                "position": 257
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x4.png",
                "caption": "Figure 4:Overview of the UniCorn Framework. (a) Illustrates the self-multi-agent collaboration for high-quality data sampling. (b) Details the Cognitive Pattern Reconstruction process, which reorganizes data to facilitate robust and efficient learning. (c) Presents the UniCycle benchmark evaluation, verifying whether the model can accurately reconstruct key textual information from its own generated content.",
                "position": 268
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03193/x5.png",
                "caption": "Figure 5:Qualitative comparison betweenUniCorn, BAGEL andUniCorn’s adifferent data settings. Our method jointly balances visual aesthetics, prompt fidelity, and realism in generation.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x6.png",
                "caption": "Figure 6:Visualization results of UniCorn at 1024×1024 resolution.",
                "position": 654
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x7.png",
                "caption": "Figure 7:Data scaling result on TIIF. The score consistently improves when the dataset size scales up. Notably,UniCornsurpasses many powerful models only using 5k training data.",
                "position": 895
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Statement",
        "images": []
    },
    {
        "header": "Content",
        "images": []
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    },
    {
        "header": "Appendix BMore Related Work",
        "images": []
    },
    {
        "header": "Appendix CTheoretical Analysis",
        "images": []
    },
    {
        "header": "Appendix DBenchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.03193/x8.png",
                "caption": "Table 7:Detailed data type range, description and judgement rubrics.",
                "position": 1488
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x9.png",
                "caption": "",
                "position": 1550
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x10.png",
                "caption": "",
                "position": 1571
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x11.png",
                "caption": "",
                "position": 1592
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x12.png",
                "caption": "",
                "position": 1613
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x13.png",
                "caption": "",
                "position": 1634
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x14.png",
                "caption": "",
                "position": 1655
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x15.png",
                "caption": "",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x16.png",
                "caption": "",
                "position": 1697
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x17.png",
                "caption": "",
                "position": 1718
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x18.png",
                "caption": "Table 8:Examples of Generation, Caption, Judgement, Reflection training data. We choose the same image and prompt for better illustration.",
                "position": 1725
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x19.png",
                "caption": "",
                "position": 1760
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x20.png",
                "caption": "",
                "position": 1775
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x21.png",
                "caption": "",
                "position": 1792
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x22.png",
                "caption": "",
                "position": 1793
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x23.png",
                "caption": "Figure 10:Cases ofUniCycle.",
                "position": 3421
            },
            {
                "img": "https://arxiv.org/html/2601.03193/x24.png",
                "caption": "Figure 13:Failure cases ofUniCornin chanllenging tasks of Negation and Counting.",
                "position": 3571
            }
        ]
    },
    {
        "header": "Appendix EAdditional Results",
        "images": []
    }
]