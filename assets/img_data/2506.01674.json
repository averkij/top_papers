[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x1.png",
                "caption": "Figure 1:Motivation and approach overview.(a) Temporal dynamics inherent in motion distinguish videos from static images. (b) Existing MLLMs show limitations in fine-grained motion detection. (c) Our approach shows superior performance on MotionBench and FAVOR-Bench compared to SOTA.",
                "position": 134
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x2.png",
                "caption": "Figure 2:Overview of the interaction process.Left:Our𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝\\mathtt{MotionSight}typewriter_MotionSightpipeline captions high-quality data, transforming it into data assets.Right:This data undergoes rigorous filtering to align with human preferences, resulting in our high-quality dataset𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍−𝚀𝙰𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍𝚀𝙰\\mathtt{MotionVid-QA}typewriter_MotionVid - typewriter_QA.",
                "position": 159
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x3.png",
                "caption": "Figure 3:Comparison of our method with other existing methods.Directly applying image visual prompts like background blur can instead lead to misinterpretation due to the lack of motion context information in the background. By employing decoupled object-guided motion focusing and inter-frame information enhancement, our method addresses the challenge faced by previous methods.",
                "position": 166
            }
        ]
    },
    {
        "header": "3MotionSight",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x4.png",
                "caption": "Figure 4:The detailed pipeline of𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝\\mathtt{MotionSight}typewriter_MotionSight.Our method includes query-based motion decoupling, gating based on object motion and camera motion. Subsequently, it passes through three modules: Object Referring, Action Focusing, and Motion Blur. Then, we carefully designed a template prompt for MLLMs to understand our enhanced input and make final decisions.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2506.01674/x5.png",
                "caption": "Figure 5:𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍−𝚀𝙰𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍𝚀𝙰\\mathtt{MotionVid-QA}typewriter_MotionVid - typewriter_QA: High-quality filtering.Construction of a high-quality video dataset via filtration.Extensive data distribution.Diverse sources yield varied scenes, subjects, and camera perspectives.Human preference comparison.Our preference data annotation significantly surpasses baseline, particularly for camera motion.",
                "position": 406
            },
            {
                "img": "https://arxiv.org/html/2506.01674/x6.png",
                "caption": "Figure 6:A comparative visualization of𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍−𝚀𝙰𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍𝚀𝙰\\mathtt{MotionVid-QA}typewriter_MotionVid - typewriter_QAagainst existing data.Our proposed dataset incorporates high-fidelity QAs augmented with human signals. In contrast to antecedent methodologies reliant upon class labels or captions, our approach facilitates the provision of substantially richer and more diverse informational content. Notably, even in scenarios characterized by the absence of salient principal objects, our methodology consistently yields high-quality annotations pertaining to object and camera dynamics.",
                "position": 411
            }
        ]
    },
    {
        "header": "4MotionVid",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x7.png",
                "caption": "Figure 7:Qualitative examples of𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝\\mathtt{MotionSight}typewriter_MotionSight.Our method,𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝𝙼𝚘𝚝𝚒𝚘𝚗𝚂𝚒𝚐𝚑𝚝\\mathtt{MotionSight}typewriter_MotionSight, possesses fine-grained inter-frame difference perception, enabling precise sensing of subtle motions.",
                "position": 1123
            }
        ]
    },
    {
        "header": "6Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Supplementary Contents",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x8.png",
                "caption": "Figure S1:Our carefully designed prompt.",
                "position": 1875
            }
        ]
    },
    {
        "header": "Appendix BDataset Filter and Curation for𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍−𝚀𝙰𝙼𝚘𝚝𝚒𝚘𝚗𝚅𝚒𝚍𝚀𝙰\\mathtt{MotionVid-QA}typewriter_MotionVid - typewriter_QA",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x9.png",
                "caption": "Figure S2:The results corresponding to our three different thresholds are presented separately.Top:High consistency between text and video. The camera movement and changes in viewpoint are strictly described in chronological order, resulting in extremely high quality.Middle:Fairly good consistency between text and video. The actions of the main characters are described with reasonable accuracy, but some imprecise areas exist.Bottom:Relatively poor consistency between text and video, providing limited or erroneous information.",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2506.01674/x10.png",
                "caption": "Figure S3:Interactive annotation interface for DPO focused on fine-grained video motion understanding. This Python-based front-end allows annotators to choose between two textual descriptions (\"Option A\" and \"Option B\") for the same video clip, selecting the one that more accurately captures the nuanced motion in the video. The interface supports loading data in JSONL format and records annotator preferences, thereby providing data for the model’s preference learning.",
                "position": 1975
            }
        ]
    },
    {
        "header": "Appendix CGuidelines for Human Preference Annotation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.01674/x11.png",
                "caption": "Figure S4:Quantitative results between baseline and our fine-tuned models trained using both SFT and DPO. For each case, the upper one is the Qwen2.5VL-7B baseline, and the lower one is our model after fine-tuning. Our fine-tuned model, trained on our dataset, demonstrates superior fine-grained motion perception capabilities and outperforms the baseline.",
                "position": 2385
            }
        ]
    },
    {
        "header": "Appendix DFine-tuning MotionChat on the Dataset",
        "images": []
    }
]