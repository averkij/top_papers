[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/x1.png",
                "caption": "Figure 1:Can we align video generative models using real data as preference data without a reward model?(a) Comparison between using the reward model to score synthetic data for preference learning and our RealDPO method, which uses high-quality real data as win samples. Our method avoids the limitations of the reward model and the associated hacking issues. (b) Comparison between the video generated by the pretrained model and the real video for the same scene. The three scores on the right represent the scores given by the reward model from VisionReward(Xu et al.,2024a), the human action metric from VBench(Huang et al.,2024a;b), and human preference, respectively. It can be observed that while the existing reward model and VBench can evaluate semantic correctness, they are limited in assessing human motion quality. (c) Three model-generated examples from the same prompt, each with different initial noise, exhibit poor limb interaction, making it challenging for human annotators to identify which sample should be chosen as the win sample for reward model training.",
                "position": 85
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/x2.png",
                "caption": "Figure 2:RealDPO vs SFT.A qualitative comparison between RealDPO and supervised fine-tuning (SFT). RealDPO demonstrates more natural motion generation. For more details regarding the comparison, please refer to the supplementary material.",
                "position": 91
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/x3.png",
                "caption": "Figure 3:Overview of the RealAction-5K Dataset.(a) Samples of RealAction-5K Dataset (b) Data Collection and Processing Pipeline (c) Video Caption Word Distribution (d) Action Content Distribution (e) Prompt Length Distribution",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2510.14955/x4.png",
                "caption": "Figure 4:The RealDPO Framework. We use real data as the win samples in DPO, and illustrate the data pipeline on the left hand side. We present the RealDPO loss, and reference model update strategy on the right hand side.",
                "position": 209
            }
        ]
    },
    {
        "header": "4The RealDPO Paradigm",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/x5.png",
                "caption": "Figure 5:Qualitative Results.We visualize the effect of before and after applying RealDPO. See the supplementary for videos.",
                "position": 376
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/x6.png",
                "caption": "Figure 6:Qualitative Comparison.We recommend readers refer to our appendix files to view more visualizations.",
                "position": 564
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/x7.png",
                "caption": "Figure 7:Qualitative Results.Comparison with pre-trained model.",
                "position": 1038
            },
            {
                "img": "https://arxiv.org/html/2510.14955/x8.png",
                "caption": "Figure 8:Qualitative Results.Comparison with supervised fine-tuning.",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2510.14955/x9.png",
                "caption": "Figure 9:Qualitative Results.comparison with other Alignment Method.",
                "position": 1044
            }
        ]
    },
    {
        "header": "Appendix BDetails of the Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.14955/img/user_study.png",
                "caption": "Figure 10:User Study Scoring Interface for users to give socre.",
                "position": 1083
            }
        ]
    },
    {
        "header": "Appendix CPseudo-code of RealDPO",
        "images": []
    },
    {
        "header": "Appendix DThe Use of Large Language Models (LLMs)",
        "images": []
    }
]