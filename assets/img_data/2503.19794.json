[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19794/x1.png",
                "caption": "Figure 1:Top: Evaluating a Video LLM on audio-visual QA and 3D QA tasks.Bottom: Adapting Video LLMs by adding a small “patch” of additional operations and parameters, without changing its existing architecture or vast pre-trained weights.",
                "position": 84
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19794/x2.png",
                "caption": "Figure 2:(a)Overview of PAVE. PAVE presents a simple, parameter-efficient adapter to integrate videos and side-channel signals. This is done by fusing side-channel tokens𝐳ssuperscript𝐳𝑠\\mathbf{z}^{s}bold_z start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPTand video tokens𝐳vsuperscript𝐳𝑣\\mathbf{z}^{v}bold_z start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT, and further adding the results to the original video tokens𝐳vsuperscript𝐳𝑣\\mathbf{z}^{v}bold_z start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT. (b)Details of PAVE’s fusion function. The fusion functiong⁢(⋅)𝑔⋅g(\\cdot)italic_g ( ⋅ )consists of a few blocks of temporal-aligned cross-attention layer, MLP, and layer normalization. (c)Temporal-aligned Cross-Attention. Visual tokens𝐳vsuperscript𝐳𝑣\\mathbf{z}^{v}bold_z start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPTand side-channel tokens𝐳ssuperscript𝐳𝑠\\mathbf{z}^{s}bold_z start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPTare aligned along the temporal axis. A video token𝐳v⁢(k)superscript𝐳𝑣𝑘\\mathbf{z}^{v}(k)bold_z start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT ( italic_k )is treated as query, and only attends to keys and values (defined by side-channel tokens) in its temporal neighborhood.",
                "position": 153
            }
        ]
    },
    {
        "header": "3Patching and Adapting Video LLMs",
        "images": []
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19794/x3.png",
                "caption": "Figure 3:Visualization of sample results. We visualize the compare the results from our base model LLaVA-OneVision (under zero-shot inference) and PAVE across 3D QA and audio-visual QA tasks. Both succeful and failure cases are shown.",
                "position": 918
            },
            {
                "img": "https://arxiv.org/html/2503.19794/x4.png",
                "caption": "Figure 4:Visualization of cross-attention scores in PAVEwhen injecting high frame rate videos as the side-channel. Cross-attention scores are calculated between the selected video tokens from the original low frame rate video (red cells on the left) and side-channel tokens from the high frame rate video. Scores are displayed as heatmaps over densely sampled video frames (on the right).",
                "position": 922
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experiment Results",
        "images": []
    },
    {
        "header": "Appendix BImplementation and Experiment Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.19794/x5.png",
                "caption": "Figure 5:Visualization of the QA results on enhanced video QA task. By making use of the video feature of the densely sampled video frames, PAVE captures more details in the video and thus improves the performance of video understanding.",
                "position": 2615
            }
        ]
    },
    {
        "header": "Appendix CAdditional Visualization",
        "images": []
    }
]