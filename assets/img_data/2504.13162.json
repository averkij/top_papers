[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13162/x1.png",
                "caption": "",
                "position": 76
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13162/x2.png",
                "caption": "Figure 2:Overview of Fine-tuning.We fine-tune a text-to-image auto-regressive model using 3-5 input images, each paired with a text prompt that includes a unique identifier and the subject class name (e.g., “A photo of [V] dog”). The process involves two stages: first, we fine-tune the text embedding for the identifier [V], and second, we additionally fine-tune the transformer layers to enhance the model’s performance.",
                "position": 218
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13162/x3.png",
                "caption": "Figure 3:Qualitative results.We generate images of personalized objects to showcase the generative capabilities of re-contextualization and property modification.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2504.13162/x4.png",
                "caption": "Figure 4:Qualitative results.We generate images of personalized animals to showcase the generative capabilities of re-contextualization and accessorization.",
                "position": 394
            },
            {
                "img": "https://arxiv.org/html/2504.13162/x5.png",
                "caption": "Figure 5:Preservation of class semantic priors.Fine-tuning auto-regressive models with a set of reference images does not result in language drift or reduced output diversity. The first column displays the training images, the next three columns show images generated using free-form prompts that include the specific subject class name.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2504.13162/x6.png",
                "caption": "Figure 6:Qualitative comparison of fine-tuning strategies: text embeddings only vs. text embeddings and transformer layers.The first column shows the input images. The second and third columns display images generated by models fine-tuned only on text embeddings, while the fourth column shows results from models fine-tuned on both text embeddings and transformer layers.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2504.13162/x7.png",
                "caption": "Figure 7:Failure cases of various applications.This figure presents applications of novel view synthesis, artistic renditions, and property modifications. The first column displays the input images.\nSymbols in the bottom-left corner indicate whether the generated images accurately reflect the prompts.\nFor the failure cases, we include comparison images generated with the same prompt but without the “[v]” identifier, allowing us to assess the model’s inherent capabilities alongside the effects of fine-tuning.",
                "position": 597
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]