[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15869/x1.png",
                "caption": "Figure 1:Our method synthesizes high-quality, immersive 3D urban scenes solely from multi-view satellite imagery, enabling realistic drone-view navigation without relying on additional 3D or street-level training data.Given multiple satellite images from diverse viewpoints and dates (left), our method leverages 3D Gaussian Splatting combined with pre-trained text-to-image diffusion models in an iterative refinement framework to generate realistic 3D block-scale city from limited satellite-view input (right). Our method significantly enhances visual fidelity, geometric sharpness, and semantic consistency, enabling real-time immersive exploration.",
                "position": 67
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15869/x2.png",
                "caption": "Figure 2:Limitations of existing novel-view synthesis methods from satellite imagery.(a) Sat-NeRF(Marí et al.,2022)and naive 3DGS(Kerbl et al.,2023)yield blurred or distorted building facades due to insufficient geometric detail and limited parallax from satellite viewpoints. (b) City generation methods(Xie et al.,2024;2025b)produce oversimplified building geometries and unrealistic appearances, primarily due to strong assumptions about the input data, and overfitting to small-scale, domain-specific datasets. In comparison, our method synthesizes more realistic appearances and geometries from aerial views.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15869/x3.png",
                "caption": "Figure 3:Overview of the proposed Skyfall-GS pipeline.Our method synthesizes immersive and free-flight navigable city-block scale 3D scenes solely from multi-view satellite imagery in two stages. (a) In the Reconstruction Stage, we first reconstruct the initial 3D scene using 3DGS, enhanced by pseudo-camera depth supervision to address limited parallax in satellite images. We use an appearance modeling component to handle varying illumination conditions across multi-date satellite images. (b) In the Synthesis Stage, we introduce a curriculum-based Iterative Dataset Update (IDU) refinement technique leveraging (c) a pre-trained T2I diffusion model(Labs,2024b)with prompt-to-prompt editing(Kulikov et al.,2024). By iteratively updating training datasets with progressively refined renders, our approach significantly reduces visual artifacts, improving geometric accuracy and texture realism, particularly in previously occluded areas such as building facades.",
                "position": 155
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x4.png",
                "caption": "Figure 4:The motivation of curriculum strategy.Renderings of the initial 3D reconstruction from varied elevation angles reveal progressive degradation as the viewing angle decreases.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x4.png",
                "caption": "Figure 4:The motivation of curriculum strategy.Renderings of the initial 3D reconstruction from varied elevation angles reveal progressive degradation as the viewing angle decreases.",
                "position": 251
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x5.png",
                "caption": "Figure 5:Render refinement.(a) Original 3DGS render with artifacts and blurry textures; (b) Refined result showing enhanced geometry and texture quality.",
                "position": 258
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15869/x6.png",
                "caption": "Figure 6:Qualitative comparison on (a) DFC2019 and (b) GoogleEarth datasets.The leftmost column shows one representative example of the input satellite images. Our method outperforms all baselines in geometric accuracy and texture quality in low-altitude novel views, demonstrating enhanced building geometry, detailed facades, and reduced floating artifacts. Notably, our approach correctly preserves distinctive features such as the red pavement in scene 010 that competing methods miss. Unlike CityDreamer(Xie et al.,2024)and GaussianCity(Xie et al.,2025b), our method operates directly on satellite imagery without requiring pixel-aligned semantic maps or height-fields, enabling synthesis of complex geometric structures that more closely match GES references.",
                "position": 526
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x7.png",
                "caption": "(a)Compare on DFC2019 dataset.",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x7.png",
                "caption": "(a)Compare on DFC2019 dataset.",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x8.png",
                "caption": "(b)Compare on GoogleEarth dataset.",
                "position": 552
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x9.png",
                "caption": "Figure 8:Satellite-view training and IDU refinement ablation.The opacity regularization reduces floating artifacts and yields denser reconstructions. The pseudo-camera depth supervision improves geometry in texture-less areas like rooftops and roads. The multiple diffusion samples per view enhance texture consistency and geometry. The curriculum learning progressively introduces challenging views, significantly improving geometric coherence in previously occluded regions.",
                "position": 708
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.15869/x10.png",
                "caption": "Figure 9:The sampling strategy of pseudo camera.In this example, we sample 240 points using the strategy.",
                "position": 2390
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/refine_1.png",
                "caption": "Figure 10:Pseudo-cam Depth Supervision.We use MoGe(Li et al.,2024c)to estimate the scale-invariant depthDestD_{\\text{est}}from the rendered RGB imageIRGBI_{\\text{RGB}}. The rightmost figures show the rasterized depthDGSD_{\\text{GS}}from 3DGS.",
                "position": 2393
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/moge_1.png",
                "caption": "",
                "position": 2399
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/depth_1.png",
                "caption": "",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/refine_2.png",
                "caption": "",
                "position": 2403
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/moge_2.png",
                "caption": "",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/depth_2.png",
                "caption": "",
                "position": 2405
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x11.png",
                "caption": "Figure 11:Additional qualitative comparison on the DFC2019 dataset with Sat-NeRF(Marí et al.,2022), Mip-Splatting(Yu et al.,2024), CoR-GS(Zhang et al.,2024b), and EOGS(Savant Aira et al.,2025).Our method significantly outperforms baseline approaches in both geometric accuracy and texture quality when rendering low-altitude novel views. Note the superior building geometry, facade details, and reduced floating artifacts in our final result.",
                "position": 2538
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x12.png",
                "caption": "Figure 12:Additional qualitative comparison on the GoogleEarth dataset with CityDreamer(Xie et al.,2024), GaussianCity(Xie et al.,2025b), and CoR-GS(Zhang et al.,2024b).Our method is able to synthesize texture and geometry that is closer to the reference GES render.",
                "position": 2542
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/004_sat_Medium.jpeg",
                "caption": "Figure 13:Qualitative results across primary scenes.Visualization of satellite image inputs and corresponding rendered frames for our four main AOIs.",
                "position": 2918
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/004_1_Medium.jpeg",
                "caption": "",
                "position": 2928
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/004_2_Medium.jpeg",
                "caption": "",
                "position": 2929
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/004_3_Medium.jpeg",
                "caption": "",
                "position": 2930
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/068_sat_Medium.jpeg",
                "caption": "",
                "position": 2938
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/068_1_Medium.jpeg",
                "caption": "",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/068_2_Medium.jpeg",
                "caption": "",
                "position": 2940
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/068_3_Medium.jpeg",
                "caption": "",
                "position": 2941
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/214_sat_Medium.jpeg",
                "caption": "",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/214_1_Medium.jpeg",
                "caption": "",
                "position": 2950
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/214_2_Medium.jpeg",
                "caption": "",
                "position": 2951
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/214_3_Medium.jpeg",
                "caption": "",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/260_sat_Medium.jpeg",
                "caption": "",
                "position": 2960
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/260_1_Medium.jpeg",
                "caption": "",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/260_2_Medium.jpeg",
                "caption": "",
                "position": 2962
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/260_3_Medium.jpeg",
                "caption": "",
                "position": 2963
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/164_sat_Medium.jpeg",
                "caption": "Figure 14:Qualitative results across additional scenes.Visualization of satellite image inputs and corresponding rendered frames for four additional AOIs with distinctive characteristics: JAX_164 features a city hall building, JAX_175 contains an American football stadium, while JAX_168 and JAX_264 present other notable urban structures.",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/164_1_Medium.jpeg",
                "caption": "",
                "position": 2985
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/164_2_Medium.jpeg",
                "caption": "",
                "position": 2986
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/164_3_Medium.jpeg",
                "caption": "",
                "position": 2987
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/168_sat_Medium.jpeg",
                "caption": "",
                "position": 2995
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/168_1_Medium.jpeg",
                "caption": "",
                "position": 2996
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/168_2_Medium.jpeg",
                "caption": "",
                "position": 2997
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/168_3_Medium.jpeg",
                "caption": "",
                "position": 2998
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/175_sat_Medium.jpeg",
                "caption": "",
                "position": 3006
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/175_1_Medium.jpeg",
                "caption": "",
                "position": 3007
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/175_2_Medium.jpeg",
                "caption": "",
                "position": 3008
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/175_3_Medium.jpeg",
                "caption": "",
                "position": 3009
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/264_sat_Medium.jpeg",
                "caption": "",
                "position": 3017
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/264_1_Medium.jpeg",
                "caption": "",
                "position": 3018
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/264_2_Medium.jpeg",
                "caption": "",
                "position": 3019
            },
            {
                "img": "https://arxiv.org/html/2510.15869/figs/results/264_3_Medium.jpeg",
                "caption": "",
                "position": 3020
            },
            {
                "img": "https://arxiv.org/html/2510.15869/x13.png",
                "caption": "Figure 15:Visualization of multi-data satellite imagery of the DFC2019 dataset.Note the substantial shifts in appearance, including changes in illumination, cloud cover, and surface characteristics, which introduce challenges for consistent 3D reconstruction.",
                "position": 3032
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]