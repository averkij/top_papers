[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18975/x1.png",
                "caption": "Figure 1:An example ofUnbounded. We follow the life ofArchibus, the user’s custom wizard character. The user can interact with the generative game using natural language, and Archibus’ hunger, energy and fun meters update accordingly. A spontaneous and unconstrained story unfolds while the user plays, and the character can explore new environments with a myriad of possible actions and unexpected interactions. The game runs in interactive speeds, refreshing every second.",
                "position": 107
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18975/x2.png",
                "caption": "Figure 2:Example ofUnbounded. Based on an initial user input,Unboundedsets up game simulation environments, and generates character actions in the environments. Users can interact with the character with natural language instructions, exploring the game with unlimited options.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18975/x3.png",
                "caption": "Figure 3:Generative game examples ofUnbounded. The user can insert a custom character into the game, engage with the character through natural language instructions, bring the character to different environments, and interact with it to maintain a healthy state under the games’ mechanics.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18975/x4.png",
                "caption": "Figure 4:(a) Our overall image generation method. We achieve real-time image generation with LCM LoRA, maintain character consistency with DreamBooth LoRAs, and introduce a regional IP-Adapter (shown in (c)) for improved environment and character consistency. (b) Our proposed dynamic mask genreation separating the environment and character conditioning, preventing interference between the two.",
                "position": 193
            },
            {
                "img": "https://arxiv.org/html/2410.18975/x5.png",
                "caption": "Figure 5:Attention map between character embedding and hidden states in cross-attention layers in different blocks. The character embedding we use is “A [V] witch”.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2410.18975/x6.png",
                "caption": "Figure 6:Overview of our user-simulation data collection process for LLM distillation. (a) We begin by collecting diverse topic and character data, filtered using ROUGE-L for diversity. (b) The World LLM and User LLM interact to generate user-simulation data through multi-round exchanges.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18975/x7.png",
                "caption": "Figure 7:Comparison with other approaches for generating environment and character consistent images based on text prompts. We observe that our method strongly outperfoms related work.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2410.18975/x8.png",
                "caption": "Figure 8:Comparison between our regional IP-Adapter approach and baseline approaches.",
                "position": 555
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APrompt for synthetic user-simulator interaction data collection",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Prompt",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.18975/x9.png",
                "caption": "Figure 9:Prompt template used to collect diverse topic and character data.",
                "position": 1811
            },
            {
                "img": "https://arxiv.org/html/2410.18975/x10.png",
                "caption": "Figure 10:Prompt template used to query user LLM.",
                "position": 1815
            },
            {
                "img": "https://arxiv.org/html/2410.18975/x11.png",
                "caption": "Figure 11:Prompt template used to query world simulation LLM.",
                "position": 1819
            },
            {
                "img": "https://arxiv.org/html/2410.18975/x12.png",
                "caption": "Figure 12:Prompt template used to query GPT4 to compare the performance between two LLM outputs. The prompt is adapted from Vicuna(Chiang et al.,2023).",
                "position": 1823
            }
        ]
    },
    {
        "header": "Appendix CReproducibility Statement",
        "images": []
    }
]