[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02835/x1.png",
                "caption": "Figure 1:R1-Reward performance on multimodal reward benchmarks.Performance improves significantly when using a majority voting strategy (Voting@5/155155/155 / 15) over multiple inference samples.",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02835/x2.png",
                "caption": "(a)Policy Loss Convergence",
                "position": 81
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x2.png",
                "caption": "(a)Policy Loss Convergence",
                "position": 84
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x3.png",
                "caption": "(b)Response Length During Training",
                "position": 92
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary and Limitations",
        "images": []
    },
    {
        "header": "4R1-Reward",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02835/x4.png",
                "caption": "Figure 3:An example of the R1-Reward output.R1-Reward shows “human-like” self-reflective\nthought process, which is also called “Aha moment”[9].",
                "position": 499
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02835/x5.png",
                "caption": "(a)MM-RLHF Reward Bench",
                "position": 814
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x5.png",
                "caption": "(a)MM-RLHF Reward Bench",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x6.png",
                "caption": "(b)VL Reward Bench",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x7.png",
                "caption": "(c)Multimodal Reward Bench",
                "position": 833
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.02835/x8.png",
                "caption": "Figure 5:Ablation studies of the StableReinforce algorithm, evaluating the impact of different components on policy loss and model response length. The subfigures compare the performance of the algorithm with and without specific components: (a) and (b) show results when the advantage filter is removed; (c) and (d) when Pre Clip is removed. Each method is evaluated in terms of policy loss and response length over training steps.",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x8.png",
                "caption": "",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x9.png",
                "caption": "",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x10.png",
                "caption": "",
                "position": 1389
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x11.png",
                "caption": "",
                "position": 1396
            },
            {
                "img": "https://arxiv.org/html/2505.02835/x12.png",
                "caption": "Figure 6:An example of the R1-Reward output.",
                "position": 1406
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]