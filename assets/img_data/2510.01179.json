[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01179/x1.png",
                "caption": "",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x2.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Toucan: Scaling Tool-Agentic Data with Real World MCPs",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01179/x3.png",
                "caption": "Figure 1:MCP servers filtering process",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x4.png",
                "caption": "Figure 2:TheToucanconstruction pipeline: A systematic five-stage process from MCP server onboarding through trajectory filtering, with three extensions for enhancing data diversity and realism.",
                "position": 258
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x5.png",
                "caption": "Figure 3:MCP servers distribution by domain, covering a wide range of categories. Values in parentheses indicate the number of servers belonging to each category.",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x6.png",
                "caption": "Figure 4:The figures above illustrate theToucandataset analysis. Subfigure (a) and (b) provide statistics on the number of servers and required tools per instance, highlightingToucan’s comprehensive coverage of multi-server and multi-tool tasks. Subfigures (c) and (d) reveal that most tasks include more tools in the context than the targeted tools, underscoring the non-trivial tool selection challenges. Subfigure (e) displays the length of user messages in tokens. Subfigures (f) and (h) demonstrate the multi-turn nature of the tasks, characterized by extended and diverse interactions among users, agents, and tools. Subfigure (g) demonstrates thatToucanencompasses both single and parallel tool calls, which enhance the dataset’s versatility in capturing diverse agent-tool interaction patterns.",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x7.png",
                "caption": "Figure 5:ToucanSubset Statistics",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x7.png",
                "caption": "Figure 5:ToucanSubset Statistics",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x8.png",
                "caption": "Figure 6:ToucanQuality Statistics",
                "position": 378
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01179/x9.png",
                "caption": "Figure 7:This figure compares the performance ofToucan-tuned models with other open-source models on MCP-Universe(Luo et al.,2025). Model sizes increase from left to right. Bars with darker colors represent task success rate (full task completion), while lighter colors represent average evaluation scores considering partial task completion.Toucan-tuned models are shown with black borders.Toucan-tuned models outperform other models of similar sizes across most tasks.",
                "position": 663
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x10.png",
                "caption": "Figure 8:Model Performance vs Size on MCP-Universe Benchmark. We report overall task success rate (OSR). Our models push the Pareto frontier forward, achieving higher OSR at smaller model sizes.",
                "position": 666
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "6Use of Large Language Models (LLMs)",
        "images": []
    },
    {
        "header": "7Ethics Statement",
        "images": []
    },
    {
        "header": "8Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Schema and Examples",
        "images": []
    },
    {
        "header": "Appendix BMore on Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01179/x11.png",
                "caption": "Figure 10:Distribution of the most frequently occurring MCP servers in theToucandataset.",
                "position": 1774
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x12.png",
                "caption": "Figure 11:Tools Number distribution across MCP servers",
                "position": 1777
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x13.png",
                "caption": "Figure 12:This figure is the visualization of 50K random-sampledToucaninstances via Embedding Atlas(Ren et al.,2025).",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2510.01179/x14.png",
                "caption": "Figure 13:Pearson correlation between human annotators and LLM-as-a-Judge evaluations across different models.",
                "position": 1791
            }
        ]
    },
    {
        "header": "Appendix CMore on Experiments",
        "images": []
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    }
]