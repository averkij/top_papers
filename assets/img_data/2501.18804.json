[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/DTU/0/all.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/RealEstate10k/0/all.png",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/ScanNetVideo/0/all.png",
                "caption": "",
                "position": 88
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/diagram_new.png",
                "caption": "Figure 2:Diagram of our proposed Multi-View Geometric Diffusion (MVGD) framework, at inference time.NùëÅNitalic_Ninput imagesùêàcnsuperscriptsubscriptùêàùëêùëõ\\mathbf{I}_{c}^{n}bold_I start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTwith camerasùíûcnsuperscriptsubscriptùíûùëêùëõ\\mathcal{C}_{c}^{n}caligraphic_C start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPTare used for scene conditioning, and a different cameraùíûtsubscriptùíûùë°\\mathcal{C}_{t}caligraphic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTis selected for novel view and depth synthesis.",
                "position": 406
            }
        ]
    },
    {
        "header": "3Multi-View Geometric Diffusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9__00194.png",
                "caption": "Figure 3:MVGD novel view and depth synthesis resultsrandomly sampled from different evaluation benchmarks and in-the-wild datasets. Top images are conditioning views (colored cameras), and bottom images are the target view (black camera), showing from left-to-right: ground-truth image, predicted image, and predicted depth map. These predictions are used to produce a colored 3D pointcloud observed from the target view. For more examples and additional visualizations, please refer to the supplementary material.",
                "position": 608
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/DL3DV-cam0-DL3DV_int_10/DL3DV-cam0-DL3DV_int_10__00124.png",
                "caption": "",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9__00187.png",
                "caption": "",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/RealEstate10k-pixelsplat_2view-cam0-RealEstate10k_pixelsplat/RealEstate10k-pixelsplat_2view-cam0-RealEstate10k_pixelsplat__00022.png",
                "caption": "",
                "position": 613
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/DDAD-lidar-camCAMERA_01-DDAD_int_10/DDAD-lidar-camCAMERA_01-DDAD_int_10__00011.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single/LLFF-test_skip8_view3-cam0-LLFF_view3__00004.png",
                "caption": "",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single/DL3DV-cam0-DL3DV_int_10__00001.png",
                "caption": "",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/WildRGBD-cam0-WildRGBD_int_15/WildRGBD-cam0-WildRGBD_int_15__00044.png",
                "caption": "",
                "position": 618
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADatasets Preparation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9__00100.png",
                "caption": "Figure 4:Zero-Shot MVGD novel view and depth synthesis resultsrandomly sampled from different evaluation benchmarks and in-the-wild datasets. Top left images are conditioning views (colored cameras), and bottom images are the target view (black camera), showing from left-to-right: ground-truth image, predicted image, and predicted depth map. These predictions are used to produce a colored 3D pointcloud observed from the target viewpoint.",
                "position": 1964
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/MarkTJ-cam0-MarkTJ_int_9/MarkTJ-cam0-MarkTJ_int_9__00068.png",
                "caption": "",
                "position": 1967
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/ScanNetVideo-cam0-ScanNetVideo_ext_15/ScanNetVideo-cam0-ScanNetVideo_ext_15__00131.png",
                "caption": "",
                "position": 1968
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/WildRGBD-cam0-WildRGBD_int_15/WildRGBD-cam0-WildRGBD_int_15__00107.png",
                "caption": "",
                "position": 1970
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9__00376.png",
                "caption": "",
                "position": 1971
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/DTU-dtu_2view-cam0-DTU_view2/DTU-dtu_2view-cam0-DTU_view2__00058.png",
                "caption": "",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/LLFF-test_skip8_view9-cam0-LLFF_view9/LLFF-test_skip8_view9-cam0-LLFF_view9__00010.png",
                "caption": "",
                "position": 1974
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/MIPNeRF360-train_test_split_9-cam0-MIPNeRF360_reconfusion9/MIPNeRF360-train_test_split_9-cam0-MIPNeRF360_reconfusion9__00139.png",
                "caption": "",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/DL3DV-cam0-DL3DV_int_10/DL3DV-cam0-DL3DV_int_10__00097.png",
                "caption": "",
                "position": 1976
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/MVImgNet-cam0-MVImgNet_int_9/MVImgNet-cam0-MVImgNet_int_9__00188.png",
                "caption": "",
                "position": 1978
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9__00163.png",
                "caption": "",
                "position": 1979
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/single_new/single/DL3DV-cam0-DL3DV_int_10/DL3DV-cam0-DL3DV_int_10__00226.png",
                "caption": "",
                "position": 1980
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9_268/all.png",
                "caption": "Figure 5:Accumulated MVGD pointclouds, obtained by generating novel images and depth maps from various viewpoints (black cameras), using the same conditioning views (colored cameras), and stacking them together without any post-processing. Our zero-shot architecture is capable of directly generating multi-view consistent predictions that match the scale from conditioning cameras.",
                "position": 1986
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/DTU-dtu_2view-cam0-DTU_view2_49/all.png",
                "caption": "",
                "position": 1989
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9_338/all.png",
                "caption": "",
                "position": 1991
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9_367/all.png",
                "caption": "",
                "position": 1992
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/MIPNeRF360-train_test_split_9-cam0-MIPNeRF360_reconfusion9_141/all.png",
                "caption": "",
                "position": 1994
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/RealEstate10k-train_test_split_9-cam0-RealEstate10k_reconfusion9_10/all.png",
                "caption": "",
                "position": 1995
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all2/all/RealEstate10k-train_test_split_9-cam0-RealEstate10k_reconfusion9_95/all.png",
                "caption": "",
                "position": 1997
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/all_new/CO3Dv2-train_test_split_9-cam0-CO3Dv2_reconfusion9/mixed1.png",
                "caption": "",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad/frame_00005.png",
                "caption": "(a)",
                "position": 2008
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad/frame_00012.png",
                "caption": "",
                "position": 2016
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad/frame_00022.png",
                "caption": "",
                "position": 2025
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad/frame_00034.png",
                "caption": "",
                "position": 2026
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/all_mod22.png",
                "caption": "(a)",
                "position": 2034
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad2/frame_00003.png",
                "caption": "",
                "position": 2044
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad2/frame_00012.png",
                "caption": "",
                "position": 2045
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad2/frame_00020.png",
                "caption": "",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/ddad2/frame_00028.png",
                "caption": "",
                "position": 2055
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/motion/all_mod2.png",
                "caption": "(b)",
                "position": 2063
            },
            {
                "img": "https://arxiv.org/html/2501.18804/x1.png",
                "caption": "Figure 7:MVGD per-frame novel view and depth synthesis resultsusing different numbers of fixed conditioning views, on ScanNet (scene0086‚Å¢_‚Å¢020086_020086\\_020086 _ 02, with1267126712671267images). The same model from all experiments reported in the main paper was used. Legend numbers indicate the stride (i.e., how many target images are positioned between each conditioning view). As expected, results consistently improve as more input information is available, eventually plateauing at around100100100100conditioning views.",
                "position": 2070
            },
            {
                "img": "https://arxiv.org/html/2501.18804/x2.png",
                "caption": "",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2501.18804/x3.png",
                "caption": "Figure 8:MVGD per-frame novel view and depth synthesis resultswith and without our proposed incremental conditioning strategy, on ScanNet (scene0086‚Å¢_‚Å¢020086_020086\\_020086 _ 02, with1267126712671267images). The same model from all experiments reported in the main paper was used. The blue line indicates a fixed number (25) of conditioning views, evenly spaced with a stride of50505050. The red line indicates our proposed incremental conditioning strategy, in which each new generation uses previously generated images as additional conditioning. This strategy leads to consistently better and more stable results in novel view and depth synthesis, especially in regions further away from conditioning views.",
                "position": 2080
            },
            {
                "img": "https://arxiv.org/html/2501.18804/x4.png",
                "caption": "",
                "position": 2084
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/RealEstate10k/5/all.png",
                "caption": "(a)",
                "position": 2090
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/RealEstate10k/5/all.png",
                "caption": "(a)",
                "position": 2093
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/CO3Dv2/2/all.png",
                "caption": "(b)",
                "position": 2098
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/teaser5.png",
                "caption": "(c)",
                "position": 2104
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/teaser3.png",
                "caption": "(d)",
                "position": 2109
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/incremental2/ScanNetVideo/6/all.png",
                "caption": "(e)",
                "position": 2115
            },
            {
                "img": "https://arxiv.org/html/2501.18804/extracted/6136653/figures/images/teaser2.png",
                "caption": "(f)",
                "position": 2120
            }
        ]
    },
    {
        "header": "Appendix BAdditional Qualitative Examples",
        "images": []
    },
    {
        "header": "Appendix CImplicit Dynamics Modeling",
        "images": []
    },
    {
        "header": "Appendix DIncremental Conditioning",
        "images": []
    },
    {
        "header": "Appendix ELimitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]