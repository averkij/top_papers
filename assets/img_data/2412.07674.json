[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x1.png",
                "caption": "Figure 1:Overview.We propose the FiVA dataset and adapter to learn fine-grained visual attributes for better controllable image generation.",
                "position": 108
            }
        ]
    },
    {
        "header": "2Related Words",
        "images": []
    },
    {
        "header": "3Dataset Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x2.png",
                "caption": "Figure 2:Examples of visual consistency application range.Some visual attributes, such as ‘color’ and ‘stroke,’ are easily transferable across different subjects (left). However, other attributes, like ‘lighting’ and ‘dynamics,’ are range-sensitive, meaning they produce varying visual effects depending on the domain (right), resulting in more fine-grained, subject-specific definitions of sub-attributes.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x3.png",
                "caption": "Figure 3:FiVA-Adapter architecture and training pipeline.FiVA-Adapter has two key designs: 1) Attribute-specific Visual Prompt Extractor, 2) Multi-image Dual Cross-Attention Module.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x4.png",
                "caption": "Figure 4:Qualitative comparisons on single attribute transferring.",
                "position": 370
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x5.png",
                "caption": "Figure 5:The combination of multiple visual attributesenables the integration of specific characteristics from different reference images into the target subject.",
                "position": 399
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x6.png",
                "caption": "Figure 6:Attribute decomposition.One reference image can be decomposed into different attributes via different tags.",
                "position": 498
            },
            {
                "img": "https://arxiv.org/html/2412.07674/extracted/6048517/figures/filter_abl.jpg",
                "caption": "Figure 7:Ablation on range-sensitive data filter.It helps improve the attribute accuracy and protect the original generation capacity.",
                "position": 503
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x7.png",
                "caption": "Figure S1:More image examples with different visual attributes.",
                "position": 1019
            }
        ]
    },
    {
        "header": "Appendix BAdditional Details on Dataset Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x8.png",
                "caption": "Figure S2:Statistics and Analysis.We visualize the rough distribution of visual attributes and subjects on the left. On the right, we show an example pair of images that shares similar lighting condition. We also visualize the attribute alignment accuracy via human validation here.",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x9.png",
                "caption": "(a)Subject Tree oflighting: moonlight.",
                "position": 1038
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x9.png",
                "caption": "(a)Subject Tree oflighting: moonlight.",
                "position": 1041
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x10.png",
                "caption": "(b)GPT4V based Range-sensitive Data Filtering.",
                "position": 1048
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.07674/x11.png",
                "caption": "Figure S4:Ablation on attribute input augmentation.Models trained with tag augmentation handle slight deviations in input text during inference, while those without augmentation would fail in these cases.",
                "position": 1093
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x12.png",
                "caption": "Figure S5:Examples with real-world images.We demonstrate that our adapter can be effectively extended to real-world images, which have a different distribution from generated images.",
                "position": 1099
            },
            {
                "img": "https://arxiv.org/html/2412.07674/x13.png",
                "caption": "Figure S6:An example of the input, instruction, and output of the GPT study.GPT-4V shows sufficient ability in understanding the problem and providing comprehensive analysis and judgement to these questions that can hardly be evaluated by other pre-trained models.",
                "position": 1194
            }
        ]
    },
    {
        "header": "Appendix DDatasheet for Datasets",
        "images": []
    },
    {
        "header": "Checklist",
        "images": []
    }
]