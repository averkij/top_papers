[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19897/x1.png",
                "caption": "Figure 1:Supervised Fine-Tuning (SFT) is commonly used to learn from expert demonstration datasets, but its off-policy nature leads to catastrophic forgetting of general capabilities.\nWe introduce Self-Distillation Fine-Tuning (SDFT), which turns expert demonstrations into on-policy learning signals by using a demonstration-conditioned version of the model as its own teacher. In this way, SDFT enables true continual learning with the model improving on new tasks as they arise without regressing existing capabilities.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x2.png",
                "caption": "Figure 2:(Left)SDFT leverages a model’s in-context learning ability to generate on-policy training signals.\nFor each queryxx, the model acts in two roles. A student that is conditioned only on the queryP=π(⋅|x)P=\\pi(\\cdot|x)and the teacher, which is the same model conditioned on an expert demonstrationcc, producing a demonstration-aware distributionQ=π(⋅|x,c)Q=\\pi(\\cdot|x,c). Training minimizes the reverse KL divergence between the student and teacher, yielding on-policy updates.(Right)Conditioning the model on the expert demonstrations creates a teacher with an output distribution that is substantially closer to the base model, while maintaining the same new-task accuracy.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Self-Distillation Fine-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19897/x3.png",
                "caption": "(a)SDFT",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x3.png",
                "caption": "(a)SDFT",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x4.png",
                "caption": "(b)SFT",
                "position": 328
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19897/x5.png",
                "caption": "Figure 4:Performance trade-offs between new task accuracy and retention of prior capabilities. Each point represents a trained model, with the top-right indicating ideal performance (high accuracy on both new and previous tasks). SDFT consistently achieves superior Pareto efficiency compared to baselines across all three skill learning tasks.",
                "position": 453
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x6.png",
                "caption": "Figure 5:(Left)SDFT benefits from model scale. Performance gap between SDFT and SFT on the Science Q&A task increases with model size, as larger models have stronger in-context learning capabilities.(Right)SDFT improves pass@k across various k, indicating genuine skill acquisition rather than entropy collapse.",
                "position": 573
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x6.png",
                "caption": "",
                "position": 576
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x7.png",
                "caption": "",
                "position": 580
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x8.png",
                "caption": "Figure 6:On-policy learning is essential for performance gains. While offline distillation from the improves over standard SFT, it consistently underperforms on-policy SDFT, demonstrating that the benefits cannot be attributed solely to teacher quality.",
                "position": 648
            }
        ]
    },
    {
        "header": "5Discussion and Limitations",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.19897/x9.png",
                "caption": "Figure 7:Conditioning the teacher on both article text and answer (89% strict accuracy) substantially outperforms text-only conditioning (75%), showing that the full demonstration context is critical for effective knowledge transfer.",
                "position": 1534
            },
            {
                "img": "https://arxiv.org/html/2601.19897/x10.png",
                "caption": "Figure 8:EMA teacher provides stable and effective training. Using the frozen base model, which fails to track learning progress lead to inferior results. Using the current student directly as the teacher leads to training instabilities.",
                "position": 1558
            }
        ]
    },
    {
        "header": "Appendix BTraining and Evaluation details",
        "images": []
    }
]