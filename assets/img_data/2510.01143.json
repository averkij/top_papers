[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01143/figures/states.png",
                "caption": "Figure 1:LLM hidden states are 3-D tensors, where attention and feedforward blocks explicitly transfer information between tokens and features, respectively. By instead treating parallel scaling generations as a single tensor rather than independent slices, our method,Bridge, operates along the batch axis, so that tokens from all sequences that share the same prompt can share information throughout generation.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Background & Related Works",
        "images": []
    },
    {
        "header": "3Bridge: Connecting Generation Paths",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01143/figures/algorithm.png",
                "caption": "Figure 2:Our method design.(Left)ABridgeblock and input normalization layer are added after each feedforward block.(Right)A timestep’s tokens stemming from the same input prompt attend to each other inBridgeblocks, denoted by the arrows. Dotted arrows illustrate all the locations of information transfer to different sequences in a Markovian fashion (token features only at the current timestep are shared to predict the next timestep’s tokens). Attention is masked for tokens from different prompts and from completed generations. White squares are masked cells.",
                "position": 234
            },
            {
                "img": "https://arxiv.org/html/2510.01143/figures/warmup.png",
                "caption": "Figure 3:Warm up procedure. The original LLM generates candidate traces which are filtered by correctness and compiled into a dataset. SFT on this generated dataset only updates new parameters. The P-Match baseline substitutesBridgeblocks with MLPs matched in parameter count.",
                "position": 372
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.01143/x1.png",
                "caption": "Table 1:Accuracy comparison across math benchmarks. In each section, the 4 rows from top to bottom are the performance of the original model, RLVR applied on the original model, P-Match (extra MLPs) with SFT warm up and RLVR, andBridgewith SFT warm up and RLVR. The 2 rightmost columns show the average across all benchmarks and the average improvement over the original model. MATH-500, AMC23, BRUMO25, CMIMC25, and HMMT_ FEB25 are abbreviated to MATH, AMC, BRU, CMI, and HMMT, respectively.",
                "position": 459
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x1.png",
                "caption": "Figure 4:G-Pass@8τ8_{\\tau}averaged across AIME24, AIME25, AMC23, BRUMO25, CMIMC25, and HMMT_FEB25. Each chart measures the minimum number of correct answers (τ⋅k\\tau\\cdot k) out ofk=8k=8simultaneous responses.Bridgehas the greatest coverage (τ⋅k=1\\tau\\cdot k=1) and answers correctly most consistently (τ⋅k>1\\tau\\cdot k>1) in the vast majority of cases. Higher is better.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x2.png",
                "caption": "",
                "position": 486
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x3.png",
                "caption": "",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x4.png",
                "caption": "",
                "position": 488
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x5.png",
                "caption": "Table 2:Accuracy across 16 samples of varyingBridgegeneration widths,ww, with DS-Qwen-7B which was trained at width 4 with RLVR. ABridgewidth of 1 is equivalent to independent generation. Tasks are abbreviated as described in Table4.2.1.",
                "position": 517
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x5.png",
                "caption": "Figure 5:G-Pass@8τ8_{\\tau}improvement upon the original DS-Qwen-7B model averaged across AIME24, AIME25, AMC23, BRUMO25, CMIMC25, and HMMT_FEB25 with relation to the evaluation generation widthwwofBridge. The x-axis (τ⋅k\\tau\\cdot k) indicates the number of responses out ofk=8k=8that must be correct.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x6.png",
                "caption": "Figure 6:From left to right, DS-Qwen-1.5B MATH-500 accuracy, coverage, G-Pass@40.54_{0.5}, and G-Pass@414_{1}as generation length increases. We generate 4 responses per input.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x7.png",
                "caption": "",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x8.png",
                "caption": "",
                "position": 544
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x9.png",
                "caption": "",
                "position": 545
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x10.png",
                "caption": "Figure 7:Ratio between feature norms of the block output and residual of every DS-Qwen-7B layer.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2510.01143/x11.png",
                "caption": "",
                "position": 608
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Training Hyperparameters",
        "images": []
    },
    {
        "header": "7Parameter Count Breakdown",
        "images": []
    },
    {
        "header": "8Further GRPO Considerations",
        "images": []
    },
    {
        "header": "9BridgePlacement",
        "images": []
    },
    {
        "header": "10BridgePseudocode",
        "images": []
    }
]