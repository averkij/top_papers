[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13840/x1.png",
                "caption": "Figure 1:Overview of the EfficientLLM framework.",
                "position": 342
            }
        ]
    },
    {
        "header": "2Observations and Insights",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13840/x2.png",
                "caption": "Figure 2:Ranking of LLM training and inference efficiency and performance across various techniques. The chart compares attention mechanisms, MoE designs, and architecture types (top block), parameter-efficient fine-tuning methods (middle block), and quantization strategies (bottom block) across eight dimensions: performance, utilization (AMU, PCU), latency (AL, TT), throughput (ST, IT, TT), energy consumption (AEC), and compression (MCR). For parameter-efficient tuning, “Freeze” refers to the method, which freezes the frist 8 layers of the model. Methods marked with an asterisk (∗), such as “Full∗”, utilize DeepSpeed ZeRO-3.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2505.13840/x3.png",
                "caption": "Figure 3:Efficiency LLM Results. This figure illustrates the performance and efficiency trade-offs of various architectural improvements for LLMs.(a)Radar charts comparing different Efficient Attention Mechanisms (MQA, GQA, MLA, and NSA) across 0.5B, 1.5B, and 3B model parameters, evaluated on Perplexity (PPL), Average Memory Utilization (AMU), Average Latency (AL), Tokens Throughput (TT), and Average Energy Consumption (AEC).(b)Bar chart assessing Efficient Positional Encoding methods (RoPE, Absolute, Learnable Absolute, Relate, and None) for a 1.5B parameter model on the same five key metrics.(c)Bubble chart comparing Dense Models with Mixture-of-Experts (MoE) Models of varying parameter sizes, highlighting differences in PPL, AMU, AL, TT, and AEC.\nThese visualizations correspond to the detailed results presented in Tables 4, 5, and 6.Note:All metrics presented in this figure are normalized.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2505.13840/x4.png",
                "caption": "Figure 4:Assessment of training and fine-tuning efficiency across multiple LLMs.(a) Comparison of different fine-tuning methods (LoRA, LoRA-plus, RSLoRA, DoRA, PISSA, Freeze, and full fine-tuning using DeepSpeed) across seven model architectures (Llama-3.2-1B/3B, Llama-3.1-8B, Qwen-2.5-7B/14B, Mistral-Small-24B, and Mistral-7B) using the O1-SFT dataset. Each bar shows the correspondingEfficiency Score(higher is better) andLoss(lower is better). The Efficiency Score is computed as a weighted harmonic combination of normalized resource metrics. Methods marked with * denote full fine-tuning using DeepSpeed.",
                "position": 445
            },
            {
                "img": "https://arxiv.org/html/2505.13840/x5.png",
                "caption": "Figure 5:Assessment of quantization-based inference efficiency across model precisions.Radar plots compare normalized efficiency metrics across three quantization formats:bfloat16,float16, andint4. Each plot evaluates models from DeepSeek, Qwen, Phi, and Yi families using six normalized metrics (all↑↑\\uparrow↑higher is better): average task performance, inference throughput (IT), average memory utilization (AMU), sum latency (Sum AL), average energy consumption (AEC), and model compression ratio (MCR). All values are normalized as deilted in SectionNormalization Methodology for Efficiency Metrics. While bfloat16 typically yields higher performance scores, int4 excels in throughput, memory, and compression, indicating its efficiency in deployment-constrained environments.",
                "position": 457
            }
        ]
    },
    {
        "header": "3Background",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13840/extracted/6454168/figure/elm_hardware_2x.png",
                "caption": "Figure 6:The development trends of computational efficiency and memory capacity across NVIDIA GPU series. Note that different colored dots represent different architectures, and the red line indicates the fitted trend of computational efficiency over time.",
                "position": 496
            }
        ]
    },
    {
        "header": "4Techniques for Improving LLM Efficiency",
        "images": []
    },
    {
        "header": "5Assessment",
        "images": []
    },
    {
        "header": "6Scalability of EfficientLLM Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.13840/x6.png",
                "caption": "Figure 7:Scalability analysis of EfficientLLM for LVM and VLM optimization.(a) Normalized efficiency scores across five metrics (FID↑↑\\uparrow↑, AMU↑↑\\uparrow↑, AL↑↑\\uparrow↑, TT↑↑\\uparrow↑, AEC↑↑\\uparrow↑) for attention variants (MHA, MQA, GQA, MLA, NSA) in three DiT-based LVM architectures (DiT-XL/2, L/8, B/4). All metrics are min-max normalized to [0,1] and higher values indicate better efficiency.\n(b) MoE vs. dense models across identical DiT backbones. MoE-based architectures consistently outperform dense counterparts in throughput and FID while incurring moderate AMU and AEC overhead.\n(c) Comparison of Parameter-Efficient Fine-Tuning (PEFT) methods (e.g., LoRA, RSLoRA, PISSA, DoRA) on various VLMs. Bars indicate normalizedEfficiency score(top, higher is better) andLoss(bottom, lower is better). Methods marked with * indicate full fine-tuning using DeepSpeed.",
                "position": 3274
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Discussion",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    }
]