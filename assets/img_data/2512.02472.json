[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02472/x1.png",
                "caption": "Figure 1:R-Fewdelays the performance plateau seen in R-Zero and achieves higher performance. After training, it outperforms baselines multiple benchmarks, showing more stable self-evolution.",
                "position": 137
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries: Self-Play For Data-Free Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02472/x2.png",
                "caption": "Figure 2:The math examples shown in the figure are not real data, but are included for demonstration to aid understanding. The figure provides an overview of ourR-Fewframework. The Challenger is incentivized to generate moderately (“medium”) uncertain questions that lie at the edge of the Solver’s current abilities; the Solver is rewarded for solving increasingly challenging tasks – sourced from both humans and the Challenger – via curriculum-based selection.",
                "position": 279
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02472/figures/domain.png",
                "caption": "Figure 3:Impact of domain-sampled human data on performance across MMLU-Pro categories.",
                "position": 891
            },
            {
                "img": "https://arxiv.org/html/2512.02472/x3.png",
                "caption": "Figure 4:Training curves of synthetic question diversity (measured by 2-gram lexical diversity), length (measured by word count), and difficulty (evaluated by Qwen3-8B-Base, with ground-truth labeled by Gemini-2.5-Pro) over training.\nR-Zero collapses in diversity and exhibits length inflation via verbosity, whereasR-Fewmaintains stable length and diversity during self-evolution.",
                "position": 907
            },
            {
                "img": "https://arxiv.org/html/2512.02472/x4.png",
                "caption": "",
                "position": 910
            },
            {
                "img": "https://arxiv.org/html/2512.02472/x5.png",
                "caption": "",
                "position": 911
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion and Future Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.02472/x6.png",
                "caption": "Figure 5:Training curve of the solver (Qwen3-8B-Base), trained for 100 steps while alternating with the challenger.",
                "position": 978
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]