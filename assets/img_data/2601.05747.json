[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_applications_1.jpg",
                "caption": "Figure 1:Applications like urban traffic monitoring and sling-load cargo deliveries with unmanned aerial vehicles pose a major challenge for human pose estimation due to varying person sizes, occlusion and low-resolution overhead imagery[4,9,50].",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_applications_1.jpg",
                "caption": "",
                "position": 79
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_applications_2.png",
                "caption": "",
                "position": 84
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_applications_3.jpg",
                "caption": "",
                "position": 89
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_applications_4.png",
                "caption": "",
                "position": 94
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_flypose_1.png",
                "caption": "Figure 2:Two examples from our FlyPose-104 dataset with manually annotated bounding boxes and poses, featuring frequent self-occlusions of lower body and facial joints (marked in red)[50,4].",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_flypose_1.png",
                "caption": "",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/intro_flypose_2.png",
                "caption": "",
                "position": 113
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/method_system.png",
                "caption": "Figure 3:System overview: the bottom illustrates the FlyPose pipeline, where the detector and pose estimation model are trained separately, the top is an example for how the aggregated information can be used for downstream tasks within the drone system for various applications.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/method_modelcomp.png",
                "caption": "Figure 4:Pose Estimation performance of pretrained models on the UAV-Human dataset, plotted against their latency on an RTX A6000 GPU. Each circleâ€™s radius is proportional to the model parameter count.",
                "position": 221
            }
        ]
    },
    {
        "header": "4Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/result_multimodal_det.png",
                "caption": "Figure 5:Qualitative detections (red) on the VisDrone2019-DET (top), FlyPose-104 (bottom left) and HIT-UAV (bottom right).",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/result_poses_uavhuman.jpg",
                "caption": "Figure 6:Qualitative FlyPose result on the UAV-Human test-set.",
                "position": 465
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/result_realflight.png",
                "caption": "Figure 7:We fly our setup in real flight to demonstrate the feasibility of FlyPose. The synchronized snapshot shows the onboard camera view (left) and the UAV in flight (right). The person detection and predicted poses using FlyPose are overlayed.",
                "position": 481
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/discussion_qualitative.png",
                "caption": "Figure 8:Qualitative FlyPose Results on various aerial datasets. The baseline is the COCO-pretrained ViTPose-S and Ours is the finetuned version.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/discussion_visdrone1.jpg",
                "caption": "Figure 9:Qualitative FlyPose results of person detection and pose estimation on the VisDrone dataset.",
                "position": 503
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/discussion_visdrone1.jpg",
                "caption": "",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2601.05747/images/final/discussion_visdrone2.png",
                "caption": "",
                "position": 510
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]