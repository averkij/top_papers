[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14590/x1.png",
                "caption": "Figure 1:Illustration of the quantization with mixed-precision between output features and kernel execution.",
                "position": 133
            }
        ]
    },
    {
        "header": "2Background, Related Work, and Discussion",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14590/x2.png",
                "caption": "Figure 2:The percentage of high-salient out features within each linear layer of Llama 3.1 8B model according to each featureâ€™s contribution to the final loss after quantizing to 4-bit, with 10% high-salient features globally.\nEach decoder layer containsq_proj,k_proj,v_proj,o_proj,gate_proj,up_proj, anddown_projin order.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2412.14590/x3.png",
                "caption": "Figure 3:The float and integer value of binary(010010110xx...x), each within a consecutive range.",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2412.14590/x4.png",
                "caption": "Figure 4:The GPU kernel software pipeline of group-wise W4A8/W8A8 quantized MatMul.\nIt assumes perfect overlapping.\nG2S: load global to shared memory;\nS2R: load shared memory to register;\nMMA: matrix multiply-accumulation;\nI2F: integer to float conversion;\ndeq: dequantize;\nacc: accumulate.",
                "position": 491
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.14590/x5.png",
                "caption": "Figure 5:The speedup of two types of single linear layers over torch float16 baseline on the A100 GPU.",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2412.14590/x6.png",
                "caption": "Figure 6:The perplexity (wikitext2) of Llama 3.1 8B model with different configurations.",
                "position": 1732
            }
        ]
    },
    {
        "header": "5Summary",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]