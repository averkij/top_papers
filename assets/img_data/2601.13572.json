[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13572/x1.png",
                "caption": "Figure 1:Performance comparison of RAM/RAM+ and baselines on 12 tasks across 3 agent domains. Our method achieves the best average performance and secures SOTA results on 9 out of 12 tasks, surpassing even the original specialized agents (Coding, Memory, Tool).",
                "position": 240
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Reinforced Task Vector Behaviors",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13572/x2.png",
                "caption": "Figure 2:Left: Density (1-Sparsity) of task vectors varies between agent models.Right: Non-zero elements distributions of task vector varies on the number of overlaps with other task vectors.",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x3.png",
                "caption": "Figure 3:The performance gain (%) of merging unique regions of reinforced task vectors across domains.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x4.png",
                "caption": "Figure 4:Method Overview.(a)A base model is trained via RL to different agents, we track the distributions of obtained reinforced task vectors.(b)Probing the distribution of task vectors to shared, unique, unchanged sets.(c)Selective merging task vectors by averaging shared regions and rescaling unique regions to the base model.",
                "position": 347
            }
        ]
    },
    {
        "header": "4Merging Reinforced Agentic Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13572/x5.png",
                "caption": "Figure 5:The performances of merging two agents across domains.",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x5.png",
                "caption": "",
                "position": 729
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x6.png",
                "caption": "",
                "position": 734
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x7.png",
                "caption": "",
                "position": 739
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13572/x8.png",
                "caption": "Figure 6:Merging results for RL agents trained from Llama3.2-3B-Instruction base model.",
                "position": 940
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13572/figs/hf-logo.png",
                "caption": "",
                "position": 1822
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.13572/x9.png",
                "caption": "Figure 7:Additional distribution analysis for sparse reinforced task vectors trained from Qwen2.5-7B-Instruction.Left: Density (1-Sparsity) of task vectors varies between agent models.Right: The number of overlaps with other task vectors.",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x10.png",
                "caption": "Figure 8:The task vector analysis for agents trained from Llama3.2-3B-Instruction via RL.Left: Density (1-Sparsity) of task vectors varies between tool-using agent, web search agent, and the math reasoning agent models.Right: The number of overlaps with other task vectors.",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2601.13572/x11.png",
                "caption": "Figure 9:RAM/RAM+ demonstrates a superior trade-off between merging time and average score compared to the baseline method.",
                "position": 2249
            }
        ]
    },
    {
        "header": "Appendix DBaseline Details",
        "images": []
    },
    {
        "header": "Appendix ERescaling Variant",
        "images": []
    }
]