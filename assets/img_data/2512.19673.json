[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19673/x1.png",
                "caption": "",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2512.19673/x2.png",
                "caption": "Figure 1:(a):The residual stream within Transformer which moves from previous layer hidden states into self-attention and feed-forward network (FFN) sequentially.(b):Any hidden states with unembedding matrixð„u\\mathbf{E}_{\\text{u}}can be transformed into probability distributionð\\mathbf{P}over the vocabulary space, which can be considered as the samplable policy.(c):We surprisingly find that Qwen-series contains a progressive reasoning pattern in FFN, where start from exploration expansion to integrate middle layer knowledge into final prediction convergence, specially in Qwen3.",
                "position": 189
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminary",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19673/x3.png",
                "caption": "(a)Continuous entropy dynamics of internal policy for different models. The residual stream flows fromð‡lâˆ’1\\mathbf{H}^{l-1}intoð€l\\mathbf{A}^{l},ð…l\\mathbf{F}^{l}, and finally to the next layerð‡l\\mathbf{H}^{l}.",
                "position": 308
            }
        ]
    },
    {
        "header": "3Language Model Policy Secretly Contains Internal Policies",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19673/x3.png",
                "caption": "Figure 7:Entropy Change Dynamics of Internal Policy. The entropy changeÎ”â€‹Hl\\Delta H^{l}across layers represents the uncertainty of current policyâ€™s hidden exploration space. A positiveÎ”â€‹Hl>0\\Delta H^{l}>0indicates increasing exploration,Î”â€‹Hlâ‰ˆ0\\Delta H^{l}\\approx 0signifies exploitation of existing knowledge, andÎ”â€‹Hl<0\\Delta H^{l}<0suggests a tendency toward convergence within the reasoning process.",
                "position": 431
            },
            {
                "img": "https://arxiv.org/html/2512.19673/x4.png",
                "caption": "(a)Residual cosine similarity across different Qwen models",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2512.19673/x4.png",
                "caption": "Figure 11:Training dynamics of internal policy. Effects of varying the optimized policy on (a) reward, (b) entropy of language model policyÏ€Î¸\\pi_{\\theta}, (c) response length. The backbone model isQwen3-4B.",
                "position": 467
            },
            {
                "img": "https://arxiv.org/html/2512.19673/x5.png",
                "caption": "Figure 12:Analysis of internal policy optimization. (a) Similarity between the hidden states of optimized layer 6 and the higher layers. (b) Entropy ChangeÎ”â€‹HLayer6\\Delta H^{6}_{\\text{Layer}}of the optimizedÏ€Layer6\\pi_{\\text{Layer}}^{6}. (c) The PPL trend of the language model policyÏ€Î¸\\pi_{\\theta}. The backbone model isQwen3-4B.",
                "position": 473
            }
        ]
    },
    {
        "header": "4Internal Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19673/x6.png",
                "caption": "Figure 13:Average Pass@KKresults on MATH500, AMC23, AIME24 and AIME25. To reduce evaluation variance, we setn=300n=300.",
                "position": 795
            }
        ]
    },
    {
        "header": "5Bottom-up Policy Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19673/x7.png",
                "caption": "(a)Entropy dynamics during training with GRPO and BuPO with different internal policy.",
                "position": 828
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Appendix ADetailed Experiment Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.19673/x7.png",
                "caption": "(a)Continuous Entropy Dynamics of Internal Policy for additional models. The information flows fromð‡lâˆ’1\\mathbf{H}^{l-1}intoð€l\\mathbf{A}^{l},ð…l\\mathbf{F}^{l}, and finally to the next layerð‡l\\mathbf{H}^{l}.",
                "position": 1480
            },
            {
                "img": "https://arxiv.org/html/2512.19673/x7.png",
                "caption": "(b)Entropy Change Dynamics of Internal Policy with more models.",
                "position": 1485
            },
            {
                "img": "https://arxiv.org/html/2512.19673/x7.png",
                "caption": "Figure 23:Pass@K results on MATH500, AMC23, AIME24 and AIME25. To reduce evaluation variance, we setn=300n=300.",
                "position": 1497
            }
        ]
    },
    {
        "header": "Appendix BExtended Experiment Results",
        "images": []
    }
]