[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22706/x1.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2InsScene-15K Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22706/x2.png",
                "caption": "Figure 2:Data Curation Pipeline.Our data is collected from various sources and then annotated by a novel data engine driven by SAM2(Ravi et al.,2024). (a) For video captured scenes (i.e., RE10k(Zhou et al.,2018)), we annotate them through a customized SAM2 video dense prediction pipeline. (b) For RGBD-scan scenes (e.g., ScanNet++(Yeshwanth et al.,2023)), we regenerate dense mask annotations for each image and align them with the projected coarse GT masks.",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x3.png",
                "caption": "Figure 3:Visualization of mask annotations from three different sources. For the RGBD-scan scene, we additionally compare the vanilla ground-truth masks from ScanNet++(Yeshwanth et al.,2023)with our refined annotations, along with their corresponding matched IDs and mIoU scores.",
                "position": 133
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22706/x4.png",
                "caption": "Figure 4:Overview ofIGGT. Given input images, our method encodes them into a series of Unified Token Representations, which are then processed by the Geometry Head and the Instance Head to produce high-quality geometric reconstructions and instance-grounded clusterings simultaneously. In the end, we introduce Instance-Grounded Scene Understanding to perform multiple applications.",
                "position": 179
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22706/x5.png",
                "caption": "Figure 5:Qualitative results on Instance Spatial Tracking. We present two example scenes from ScanNet(Dai et al.,2017)and ScanNet++(Yeshwanth et al.,2023), and compare our method with SAM2* and SpaTracker+SAM. All instances are visualized with distinct IDs and colors for clarity.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x6.png",
                "caption": "Figure 6:We visualize our 3D-consistent PCA results with corresponding clustered masks derived from instance-grounded features. Similar colors in PCA indicate higher feature similarity between instances. For clustered masks, the same object instance shares the same color across multi-views.",
                "position": 601
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x7.png",
                "caption": "Figure 7:Qualitative Results of 2D Open-Vocabulary Segmentation on Scannet and Scannet++.",
                "position": 612
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x8.png",
                "caption": "Figure 8:Visualization of 3D Open-Vocab. Segmentation.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x9.png",
                "caption": "Figure 9:Applications of QA Scene Understanding compared with vanilla Gemini 2.5 ProComanici et al. (2025)model.",
                "position": 631
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x9.png",
                "caption": "Figure 9:Applications of QA Scene Understanding compared with vanilla Gemini 2.5 ProComanici et al. (2025)model.",
                "position": 634
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x10.png",
                "caption": "Figure 10:Visualization of our method using different VLMs.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x11.png",
                "caption": "Figure 11:Ablation on Cross-Modal Fusion.",
                "position": 649
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x11.png",
                "caption": "Figure 11:Ablation on Cross-Modal Fusion.",
                "position": 651
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Ethics Statement",
        "images": []
    },
    {
        "header": "7Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22706/x12.png",
                "caption": "Figure 12:Visualization of our manually annotated tracking GT and our tracking results.",
                "position": 1365
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x13.png",
                "caption": "Figure 13:Visualization of the pipeline from RGB 3D points to semantic labeling, voxelization, and voxel comparison for 3D mIoU.",
                "position": 1388
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x14.png",
                "caption": "Figure 14:We visualize the RGB and semantic 3D points of the ground truth, IGGT(Ours), LSM(Multi-Views), and Feature-3DGS.",
                "position": 1391
            },
            {
                "img": "https://arxiv.org/html/2510.22706/x15.png",
                "caption": "Figure 15:Comparison between vanilla Scannet++ GT masks and our refined results.",
                "position": 1418
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]