[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/figures/dodo.png",
                "caption": "",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/figures/heatmap_full.png",
                "caption": "Figure 1:DODO: High-throughput parallel generation.Unlike autoregressive models constrained to a strict left-to-right sequence, DODO generates text across the entire canvas simultaneously (with same color) based on visual confidence.\nIn this example, it resolves148148tokens in just1515forward passes (≈10\\approx 10tokens/step on average).\nNotably, large, distinct regions appear early, while ambiguous high-frequency tokens (e.g., punctuation) are deferred to later steps.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/x1.png",
                "caption": "Figure 2:Semantically flexiblevs.semantically rigid vision–language tasks.Left:Image captioning admits multiple, semantically equivalent descriptions of the same image. Different decoding trajectories can converge to distinct but equally valid captions, and lexical or structural variations are naturally absorbed.Right:OCR requires a single, exact transcription determined by the image. Even minimal local deviations, such as an incorrect token choice or boundary, render the output incorrect.\nAs a result, conditioned on the image, OCR exhibits extremely low output variability, which makes it a natural candidate for parallel decoding, but also a demanding setting in which errors cannot be compensated by alternative phrasings or later corrections.",
                "position": 221
            }
        ]
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/x2.png",
                "caption": "Figure 3:Conditional independence assumption.Parallel decoding assumes masked that masked tokens can be predicted independently given the context.(Top)In open-ended tasks, ambiguity between valid options (e.g., “Eiffel Tower” vs. “Great Wall”) risks sampling incoherent mixtures like “Eiffel Wall.”(Bottom)In deterministic regimes like OCR, the strong visual signal resolves this ambiguity, enabling conflict-free parallel decoding.",
                "position": 326
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/x3.png",
                "caption": "Figure 4:Fullvs.block diffusion. In standard full diffusion (left), MDM sampling is applied globally to the entire sequence. In contrast, block diffusion (right) restricts parallel sampling to discrete windows, processing blocks sequentially from left to right.",
                "position": 420
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/x4.png",
                "caption": "Figure 5:Inference throughput comparison.While standard DODO matches the speed of the autoregressive Qwen 2.5 VL baseline (≈21\\approx 21tokens/sec), the DODOfastleverages block-causal attention and KV-caching to triple the throughput to≈63\\approx 63tokens/sec, establishing a new efficiency standard for diffusion-based VLMs.",
                "position": 660
            }
        ]
    },
    {
        "header": "6Ablation and Empirical Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/x5.png",
                "caption": "Figure 6:Decoding Efficiency.Distribution of inference steps normalized by output length.\nThe autoregressive baseline is structurally limited to generating a single token per step.\nIn contrast, DODO leverages parallel decoding to generate multiple tokens simultaneously, effectively compressing the inference process by an order of magnitude (typically<0.1<0.1steps per token).",
                "position": 922
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/x6.png",
                "caption": "Figure A1:Visualization of the attention structure.Full bidirectional attention allows prior blocks to attend to the current block (green hatched), meaning their internal representations dynamically adapt during the forward pass.\nBlock-causal masking prevents prior blocks from attending to the current block.\nThis ensures the history representations remain invariant, enabling exact KV-caching for faster inference.",
                "position": 1702
            }
        ]
    },
    {
        "header": "Appendix BAdditional Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_00039896.1983.10545823.pdf_1.png",
                "caption": "Figure C3:Qualitative Results.Each row displays a different document from OmniDocBench.Left:Input document image.Center:DODO’s generated transcript rendered to PDF.Right:Visualization of the decoding process (heatmap of token commitment order).\nDODO successfully recovers complex layouts, including multi-column text, tables, and mathematical formulas, while maintaining high parallel efficiency.",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_00039896.1983.10545823.pdf_1.png",
                "caption": "",
                "position": 1790
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x7.png",
                "caption": "",
                "position": 1794
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x8.png",
                "caption": "",
                "position": 1799
            },
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_llm-raw-scihub-o.O-dneu.20833.pdf_13.png",
                "caption": "",
                "position": 1804
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x9.png",
                "caption": "",
                "position": 1808
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x10.png",
                "caption": "",
                "position": 1813
            },
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_llm-raw-scihub-o.O-hup.777.pdf_7.png",
                "caption": "",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x11.png",
                "caption": "",
                "position": 1822
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x12.png",
                "caption": "",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_llm-raw-scihub-o.O-j.apcata.2006.05.010.pdf_2.png",
                "caption": "Figure C4:Additional Qualitative Results.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_llm-raw-scihub-o.O-j.apcata.2006.05.010.pdf_2.png",
                "caption": "",
                "position": 1844
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x13.png",
                "caption": "",
                "position": 1848
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x14.png",
                "caption": "",
                "position": 1853
            },
            {
                "img": "https://arxiv.org/html/2602.16872/examples/docstructbench_llm-raw-scihub-o.O-j.jcrimjus.2010.04.003.pdf_8.png",
                "caption": "",
                "position": 1858
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x15.png",
                "caption": "",
                "position": 1862
            },
            {
                "img": "https://arxiv.org/html/2602.16872/x16.png",
                "caption": "",
                "position": 1867
            }
        ]
    },
    {
        "header": "Appendix CDocument Parsing Examples",
        "images": []
    }
]