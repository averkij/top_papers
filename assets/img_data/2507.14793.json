[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/visual_proof_1.png",
                "caption": "Figure 1:G-RNNs are not generally flow equivariant.We show this by simple counterexample with:ht+1=ht+ftsubscriptℎ𝑡1subscriptℎ𝑡subscript𝑓𝑡h_{t+1}=h_{t}+f_{t}italic_h start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. See §A.4for the full proof.",
                "position": 384
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/train_loss_2digit_small.png",
                "caption": "Figure 2:Increased flow equivariance increases training speed on data with flow symmetry. Validation loss vs. train steps.",
                "position": 616
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/train_loss_2digit_small.png",
                "caption": "Figure 2:Increased flow equivariance increases training speed on data with flow symmetry. Validation loss vs. train steps.",
                "position": 619
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x1.png",
                "caption": "Figure 3:FERNNs exhibit next-step prediction length-generalization capabilities far surpassing G-RNNs on simple flows. We plot samples for the forward prediction trajectories of a G-RNN and FERNN-V2Tsubscriptsuperscript𝑉𝑇2V^{T}_{2}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTtrained on Translating MNISTV2Tsuperscriptsubscript𝑉2𝑇V_{2}^{T}italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTto predict 10-steps into the future (down-sampled by a factor of 4 in time for visualization). We see the G-RNN performs well on this training regime but diverges rapidly with lengths longer than training. The FERNN generalizes nearly perfectly. On the right, we plot this forward prediction error vs. the forward prediction timestep for both models.",
                "position": 689
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x2.png",
                "caption": "Figure 4:FERNNs exhibit next-step prediction generalization to previously unseen rotation flow velocities where G-RNNs fail. Samples of forward predictions for FERNN-V4Rsubscriptsuperscript𝑉𝑅4V^{R}_{4}italic_V start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTtrained on Rotating MNIST withV1Rsubscriptsuperscript𝑉𝑅1V^{R}_{1}italic_V start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand tested onV5Rsubscriptsuperscript𝑉𝑅5V^{R}_{5}italic_V start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT. We see the FERNN-V4Rsubscriptsuperscript𝑉𝑅4V^{R}_{4}italic_V start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPTachieves near perfect forward-prediction performance in this range. On the right we plot the full distribution of error acrossVt⁢e⁢s⁢tsubscript𝑉𝑡𝑒𝑠𝑡V_{test}italic_V start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT.",
                "position": 698
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x3.png",
                "caption": "Figure 5:FERNNs exhibit next-step prediction generalization to previously unseen translation flow velocities where G-RNNs fail. Samples of forward prediction trajectories for FERNN-V2Tsubscriptsuperscript𝑉𝑇2V^{T}_{2}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTtrained on Translating MNIST withV1Tsubscriptsuperscript𝑉𝑇1V^{T}_{1}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPTand tested onV2Tsubscriptsuperscript𝑉𝑇2V^{T}_{2}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. On the right we plot the full distribution of error across the 2D velocity coordinates inVt⁢e⁢s⁢tsubscript𝑉𝑡𝑒𝑠𝑡V_{test}italic_V start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x4.png",
                "caption": "Figure 6:FERNNs outperform non-equivariant models on action recognition undergoing out-of-distribution flows (orange). Test accuracy of models trained on KTH Action Recognition with no motionV0Tsubscriptsuperscript𝑉𝑇0V^{T}_{0}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT(top row) and trained on data augmented by flow generatorsV1Tsubscriptsuperscript𝑉𝑇1V^{T}_{1}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT(bottom row). The models are tested on data augmented by flow generatorsV0Tsubscriptsuperscript𝑉𝑇0V^{T}_{0}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT,V1Tsubscriptsuperscript𝑉𝑇1V^{T}_{1}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT,V2Tsubscriptsuperscript𝑉𝑇2V^{T}_{2}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT(columns).",
                "position": 768
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x5.png",
                "caption": "Figure 7:A visualization of the same counter example of Figure1from the main text, but for the Flow Equivariant RNN. We see that the model is built with a set of extraV𝑉Vitalic_Vchannels, depicted as the three separate rows of hidden states, where each row flows independently per timestep according to itsν𝜈\\nuitalic_νparameter (e.g. the bottom row flows with velocity−11-1- 1, to the left, while the top row flows with velocity1111to the right). When the model is then processing an input with a corresponding flow symmetry (for example a motion of velocity1111to the right), the corresponding flowing channel of the hidden state processes this input in the same reference frame, as if it were stationary (the top row in this example). The remaining rows then process the input with a difference corresponding to the difference in velocity between the hidden state channel and the input velocity. We see that this results in theν−ν^𝜈^𝜈\\nu-\\hat{\\nu}italic_ν - over^ start_ARG italic_ν end_ARGchannel permutation in the latent space (the vertical shift in theV𝑉Vitalic_Vdimension by 1 row).",
                "position": 2645
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/samples/KTH_V0.png",
                "caption": "(a)Original KTH datasetV0Tsuperscriptsubscript𝑉0𝑇V_{0}^{T}italic_V start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT",
                "position": 2993
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/samples/KTH_V0.png",
                "caption": "(a)Original KTH datasetV0Tsuperscriptsubscript𝑉0𝑇V_{0}^{T}italic_V start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT",
                "position": 2996
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/samples/KTH_V1.png",
                "caption": "(b)KTH withV=V1T𝑉superscriptsubscript𝑉1𝑇V=V_{1}^{T}italic_V = italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT",
                "position": 3001
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/samples/KTH_V2.png",
                "caption": "(c)KTH withV=V2T𝑉superscriptsubscript𝑉2𝑇V=V_{2}^{T}italic_V = italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT",
                "position": 3006
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x6.png",
                "caption": "Figure 9:In-distribution sequence predictions for the models from Table1& Figure2, trained on Rotating MNISTV4Rsubscriptsuperscript𝑉𝑅4V^{R}_{4}italic_V start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT(left) and Translating MNISTV2Tsuperscriptsubscript𝑉2𝑇V_{2}^{T}italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT(right), evaluated on the same flows.",
                "position": 3190
            },
            {
                "img": "https://arxiv.org/html/2507.14793/extracted/6636682/figs/rot_mnist_lengen.png",
                "caption": "Figure 10:MSE vs. Forward prediction horizon for models on Rotating MNIST. Analogous plot to Figure3(right) but for Rotating MNIST.",
                "position": 3200
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x7.png",
                "caption": "Figure 11:Samples from models trained on Translating MNISTV2Tsuperscriptsubscript𝑉2𝑇V_{2}^{T}italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTwith training sequence lengths of 20, tested on sequence lengths of 80. We plot the 70-forward prediction steps here, subsampled by half in time (giving 35 elements).",
                "position": 3211
            },
            {
                "img": "https://arxiv.org/html/2507.14793/x8.png",
                "caption": "Figure 12:Samples from models trained on Rotating MNISTV1Rsuperscriptsubscript𝑉1𝑅V_{1}^{R}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPT(left) and Translating MNISTV1Tsuperscriptsubscript𝑉1𝑇V_{1}^{T}italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT(right), and tested on sequences with significantly higher velocity flows (V4Rsuperscriptsubscript𝑉4𝑅V_{4}^{R}italic_V start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_R end_POSTSUPERSCRIPTandV2Tsuperscriptsubscript𝑉2𝑇V_{2}^{T}italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPTrespectively). We see that the FERNN (bottom rows) has no problem generalizing to these new velocities despite never having seen them in training, while the G-RNN (top) fails. The specific flows plotted in this example are±40⁢d⁢e⁢gs⁢t⁢e⁢pplus-or-minus40𝑑𝑒𝑔𝑠𝑡𝑒𝑝\\pm 40\\frac{deg}{step}± 40 divide start_ARG italic_d italic_e italic_g end_ARG start_ARG italic_s italic_t italic_e italic_p end_ARGfor Rotating MNIST (left), andν=(±2,±2)𝜈plus-or-minus2plus-or-minus2\\nu=(\\pm 2,\\pm 2)italic_ν = ( ± 2 , ± 2 )for Translating MNIST.",
                "position": 3222
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]