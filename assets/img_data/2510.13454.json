[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13454/x1.png",
                "caption": "Figure 1:Text-to-3D generation withVIST3A.Video models excel at generating latent visual content from text prompts, whereas 3D foundation models shine when it comes to decoding such a latent representation into consistent scene geometry. By stitching a video generator and a 3D reconstruction network together and aligning their latents, we obtain an end-to-end model that produces high-quality Gaussian splats (a) or point maps (b) from text prompts.",
                "position": 129
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13454/x2.png",
                "caption": "Figure 2:Comparison with existing, LDM-based 3D generators.Instead of training a custom decoder from multi-view 2D latents to 3D outputs, we stitch and align an existing, pretrained 3D reconstruction model.",
                "position": 142
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13454/x3.png",
                "caption": "Figure 3:VIST3A constructs a 3D VAE through model stitching (top), then aligns it with a generative model via direct reward finetuning (bottom).Stitching repurposes a part of a pretrained 3D vision model as decoder to obtain a 3D VAE. Direct reward finetuning simulates full-trajectory denoising, forcing the generative model to produce 3D-consistent, decodable latents.",
                "position": 213
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13454/x4.png",
                "caption": "Figure 4:Qualitative results for 3DGS generation.We show samples from T3Bench (top), SceneBench (middle), and DPG-bench (bottom). VIST3A generates realistic and crisp 3D scenes and adheres to intricate details in the prompt.",
                "position": 677
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x5.png",
                "caption": "(a)log-MSE value in Eq.2",
                "position": 864
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x5.png",
                "caption": "(a)log-MSE value in Eq.2",
                "position": 867
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x6.png",
                "caption": "(b)Acc.↓\\downarrow",
                "position": 872
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x7.png",
                "caption": "(c)Comp.↓\\downarrow",
                "position": 877
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x8.png",
                "caption": "(d)NC.↑\\uparrow",
                "position": 882
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended related works",
        "images": []
    },
    {
        "header": "Appendix BMethodology Details and Its Implementation",
        "images": []
    },
    {
        "header": "Appendix CDetails on Experimental Setups",
        "images": []
    },
    {
        "header": "Appendix DFurther Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13454/x9.png",
                "caption": "(a)Acc.↓\\downarrow",
                "position": 3165
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x9.png",
                "caption": "(a)Acc.↓\\downarrow",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x10.png",
                "caption": "(b)Comp.↓\\downarrow",
                "position": 3173
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x11.png",
                "caption": "(c)NC.↑\\uparrow",
                "position": 3178
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x12.png",
                "caption": "(d)Reconstructed images through VAE according toα\\alpha",
                "position": 3184
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.13454/x13.png",
                "caption": "(a)Wan",
                "position": 3205
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x13.png",
                "caption": "(a)Wan",
                "position": 3208
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x14.png",
                "caption": "(b)Hunyuan",
                "position": 3213
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x15.png",
                "caption": "(c)SVD",
                "position": 3219
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x16.png",
                "caption": "(d)CogvideoX",
                "position": 3224
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x17.png",
                "caption": "Figure 8:Qualitative comparison of 3DGS generation.The top two rows show samples from DPG-Bench, and the bottom two rows present samples from T3Bench. VIST3A generates realistic scenes with fine-grained details that faithfully reflect the input prompt, outperforming baselines.",
                "position": 3252
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x18.png",
                "caption": "Figure 9:Qualitative comparison of 3DGS generation on SceneBench.VIST3A outperforms baselines by generating higher-fidelity scenes with accurate geometry and appearance.",
                "position": 3256
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x19.png",
                "caption": "Figure 10:Generated 3D scenes from VIST3A: Wan + AnySplat.These are 3DGS viewed directly in the interactive viewer. VIST3A preserves high visual quality even under noticeably altered camera trajectories, demonstrating robustness and stability across novel viewpoints.",
                "position": 3259
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x20.png",
                "caption": "Figure 11:Qualitative results on text-to-poinmap generation.By integrating VGGT, VIST3A generates structurally consistent pointmaps and fine-grained details across diverse prompts.",
                "position": 3262
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x21.png",
                "caption": "Figure 12:Qualitative comparison of 3DGS generation on SceneBench - VISTA: Wan+MVDUSt3R.",
                "position": 3267
            },
            {
                "img": "https://arxiv.org/html/2510.13454/x22.png",
                "caption": "Figure 13:Generated 3D scenes from VIST3A: Wan + AnySplat bt extending the number of frames.These are 3DGS viewed directly in the interactive viewer. VIST3A preserves high visual quality even under noticeably altered camera trajectories, demonstrating robustness and stability across novel viewpoints.",
                "position": 3270
            }
        ]
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    },
    {
        "header": "Appendix GUse of Large Language Models",
        "images": []
    }
]