[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/vlm2-bench-icon_final.png",
                "caption": "",
                "position": 226
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/teaser.png",
                "caption": "Figure 1:Previous benchmarksfail to assess the ability to link matching visual cues, whereas ourVLM2-Benchexplicitly tests this ability, as shown in the example where the model need to identify the reappearance of the same person by linking visual cues, like facial features or clothing, across non-adjacent frames.",
                "position": 270
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/x1.png",
                "caption": "Figure 2:Overview ofVLM2-Bench. The benchmark is categorized into three subsets based on visual cues: GC (General Cue), OC (Object-centric Cue), and PC (Person-centric Cue), each comprising multiple subtasks. To comprehensively evaluate VLMs’ ability to visually link matching cues, the benchmark includes diverse question formats—T/F, multiple-choice, numerical, and open-ended—ensuring a comprehensive evaluation.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/tf-icon.png",
                "caption": "",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/mcq-icon.png",
                "caption": "",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/nq-icon.png",
                "caption": "",
                "position": 278
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/oeq.png",
                "caption": "",
                "position": 278
            }
        ]
    },
    {
        "header": "2VLM2-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/general-pipe.png",
                "caption": "Figure 3:Construction ofGC: (i) We start by manually verifying the edited image data based on three key criteria. (ii) A VLM is then prompted to generate captions for each image, followed by salient score-based filtering to retain the challenging cases. (iii) Finally, visual cues are extracted from two sources and incorporated into a QA prompt, guiding an LLM to generate both positive and negative answer pairs.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/bench-stastics.png",
                "caption": "Figure 4:Statistical overview ofVLM2-Bench. The pie chart shows the distribution of 9 subtasks across the 3 main categories of visual cues. The bar plot illustrates the percentage breakdown by question format.",
                "position": 406
            }
        ]
    },
    {
        "header": "3Evaluation",
        "images": []
    },
    {
        "header": "4How Prompting Methods affect VLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/gc-analysis.png",
                "caption": "(a)Results of CoT-normal, CoT-special, and VP-grid on GC.",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/gc-analysis.png",
                "caption": "(a)Results of CoT-normal, CoT-special, and VP-grid on GC.",
                "position": 862
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/oc-analysis.png",
                "caption": "(b)Results of CoT and VP-zoom-o on OC.",
                "position": 868
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/pc-analysis.png",
                "caption": "(c)Results of CoT and VP-zoom-p on PC.",
                "position": 874
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Takeaways",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix Outline",
        "images": []
    },
    {
        "header": "Appendix BLicencing and Intended Use",
        "images": []
    },
    {
        "header": "Appendix CVLM2-Bench Statistics",
        "images": []
    },
    {
        "header": "Appendix DBaselines",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/GUI_human.png",
                "caption": "Figure 8:The GUI used for human-level testing.",
                "position": 2098
            }
        ]
    },
    {
        "header": "Appendix EMore details on Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/gui.jpg",
                "caption": "Figure 9:The GUI used for manually screening image editing data and refining edited prompts in General Cue (GC).",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2502.12084/x2.png",
                "caption": "Figure 11:The overview of the structured design of the Object-centric Cue (OC) images.Central Layer (Main Meta-Objects): The innermost circle represents the predefined8 object categories, which serve as the foundation for our dataset. These categories includePet, Plush, Bag, Book, Cup, Shirt, Shoes, and Toy. Each category consists of 4 main meta-objects.Middle Layer (Example Meta-Objects within Each Category): Each segment surrounding the center showcases a representativemain meta-objectwithin its category. These meta-objects serve as core instances for data collection. For example, thePetcategory includesCatandDog, while theBagcategory includesBackpack,SchoolbagandFashion Bag.Outer Layer (Distractor Meta-Objects & Visual Cue Distraction Principles): The outermost ring presents 1 out of 4distractor meta-objectsspecifically selected to create challenging image sequences. Each distractor meta-object shares one or moredistractive visual cueswith its corresponding main meta-object.",
                "position": 2327
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/duration_distribution.png",
                "caption": "Figure 12:Distribution of video duration in the subtask of Video Identity Description (VID) in Person-centric Cue (PC).",
                "position": 2857
            }
        ]
    },
    {
        "header": "Appendix FMore details on Prompting Approaches",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/grid_prompt.png",
                "caption": "Figure 13:An illustration of how VP-grid works for GC.",
                "position": 3100
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/pc-o_prompt.png",
                "caption": "Figure 14:An illustration of how VP-zoom-o works for OC.",
                "position": 3103
            },
            {
                "img": "https://arxiv.org/html/2502.12084/extracted/6211287/img/pc-p_prompt.png",
                "caption": "Figure 15:An illustration of how VP-zoom-p works for PC.",
                "position": 3106
            },
            {
                "img": "https://arxiv.org/html/2502.12084/x3.png",
                "caption": "Figure 16:Case study illustrating how CoT-special improves performance of the subtask of Tracking (Trk) in General Cue (GC). The model, InternVL2.5-8B, demonstrates a step-by-step reasoning process: In Step 2, it identifies key details such as “Vase with flowers on the table\" and \"Chandelier above\" in Image 1, while noting the absence of the vase in Image 2. In Step 3, it compares the images, recognizing that while many elements remain unchanged (e.g., chandelier, kitchen area, fruit bowl, window), the vase’s removal is the primary difference. In Step 4, the model concludes that the statement \"The vase on top of the table was removed\" accurately reflects the visual change, leading to the correct answer.",
                "position": 3131
            },
            {
                "img": "https://arxiv.org/html/2502.12084/x4.png",
                "caption": "Figure 17:Case study illustrating why VP-grid leads to performance degradation in Qwen2.5-VL-7B. The model correctly identifies visual elements (e.g., a pedestrian with a high-visibility vest at coordinates(2,5,3)253(2,5,3)( 2 , 5 , 3 )) but fails to interpret the image sequence correctly. It mistakenly associates the coordinates with the first image instead of the second, despite the explicit definition in the textual prompt. This misinterpretation results in incorrect visual cue linking and faulty reasoning, highlighting the model’s difficulty in handling structured spatial instructions and visual prompts.",
                "position": 3204
            },
            {
                "img": "https://arxiv.org/html/2502.12084/x5.png",
                "caption": "Figure 18:Case study demonstrating why VP-grid leads to performance improvement for GPT-4o. Unlike models that often misinterpret or overlook spatial cues, GPT-4o effectively uses VP-grid to align visual and textual information. In the example shown in Figure18, the model correctly identifies the cat’s nose at coordinates(1,2,4)124(1,2,4)( 1 , 2 , 4 )in the first image and(2,2,4)224(2,2,4)( 2 , 2 , 4 )in the second, accurately capturing the visual change in the attribute (from a lighter pink to a darker black). This success highlights GPT-4o’s ability to handle structured spatial prompts and improve performance through visual prompting.",
                "position": 3207
            },
            {
                "img": "https://arxiv.org/html/2502.12084/x6.png",
                "caption": "Figure 19:Case study illustrating why CoT leads to performance degradation. In the example shown in Figure19, InternVL2.5-26B’s response correctly identifies that no grouping occurs for the same meta-object in the sequence, with the correct answer being ‘D) None’. However, in the CoT response, the model incorrectly selects option C) 2 and 3. While it correctly states that “the second and third images both have dinosaurs wearing sunglasses,\" the lack of detailed analysis leads to an inaccurate conclusion. A closer examination reveals a key difference between the images—the dinosaur in image 3 is holding a keyboard instead of a skateboard, which should have prevented the grouping of the two images. This highlights the importance of providing more detailed and unambiguous cues in CoT reasoning.",
                "position": 3242
            }
        ]
    },
    {
        "header": "Appendix GCase Study",
        "images": []
    }
]