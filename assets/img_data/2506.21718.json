[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x1.png",
                "caption": "Figure 1:Overview: Using(x,y)ğ‘¥ğ‘¦(x,y)( italic_x , italic_y )metric data collected from a variety of system logs, we train a encoder-decoder via standard next-token cross-entropy for performance prediction.",
                "position": 149
            }
        ]
    },
    {
        "header": "2System Performance Prediction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x2.png",
                "caption": "Figure 2:Example anonymized string representations of some features used to constructxğ‘¥xitalic_x. More detailed representation can be found in AppendixB.1.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x2.png",
                "caption": "Table 1:Average character counts for each YAML feature.",
                "position": 339
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x3.png",
                "caption": "Figure 4:Left:Diagonal fit (â•±â•±\\diagupâ•±) is better. RLMâ€™s pointwise prediction against ground truth target.Right:Higher is better (â†‘â†‘\\uparrowâ†‘). Spearman-rank correlation of the test evaluations, after removing top outliers by MSE error.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x4.png",
                "caption": "Figure 5:Better density capture of target points is better. Kernel Density Estimate (KDE) plot of samples frompÎ¸â¢(y|x)subscriptğ‘ğœƒconditionalğ‘¦ğ‘¥p_{\\theta}(y|x)italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y | italic_x )along with actual target points, over varying timestamps. Note that samples are generated fromxğ‘¥xitalic_xâ€™s with distinct timestamps, while some target points may share timestamps.",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x5.png",
                "caption": "Figure 6:Left-skewness(â†)â†(\\leftarrow)( â† )is better. Note both axes are log-scaled.Left:Distribution of residuals as per-sample squared error, along with mean squared error as a vertical line.Right:Analogous results, but for an out-of-distribution task.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x6.png",
                "caption": "Figure 7:Higher is better (â†‘â†‘\\uparrowâ†‘). Evaluation task rank-correlation, when varying the number of finetuning examples starting from a pretrained checkpoint trained over varying numbers of tasks. Runs repeated over 10 seeds each and averaged.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x7.png",
                "caption": "Figure 8:Correlation between prediction uncertainty (density variance) and residual squared error.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x7.png",
                "caption": "Figure 8:Correlation between prediction uncertainty (density variance) and residual squared error.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x8.png",
                "caption": "Figure 9:RLM densitypÎ¸â¢(y|x)subscriptğ‘ğœƒconditionalğ‘¦ğ‘¥p_{\\theta}(y|x)italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y | italic_x )for an example inputxğ‘¥xitalic_x, along with trueyğ‘¦yitalic_y-values.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x9.png",
                "caption": "Figure 10:Diagonal fit (â•±â•±\\diagupâ•±) is better. Scatter plot of predictions and ground truth targets over multiple tasks.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x10.png",
                "caption": "Figure 11:Better density capture of target points is better. KDE plot over multiple tasks.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x11.png",
                "caption": "Figure 12:Higher is better(â†‘)â†‘(\\uparrow)( â†‘ ).Left:Explained variance per task.Right:McFaddenâ€™s Pseudo-R2superscriptğ‘…2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTper task. Tasks are sorted by decreasing resultant rank correlations.",
                "position": 563
            }
        ]
    },
    {
        "header": "5Experiments: Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x12.png",
                "caption": "Figure 13:Lower is better(â†“)â†“(\\downarrow)( â†“ ). MSE across checkpoint-steps.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x12.png",
                "caption": "Figure 13:Lower is better(â†“)â†“(\\downarrow)( â†“ ). MSE across checkpoint-steps.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x13.png",
                "caption": "Figure 14:Higher is better(â†‘)â†‘(\\uparrow)( â†‘ ). SpearmanÏğœŒ\\rhoitalic_Ïacross checkpoint-steps.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x14.png",
                "caption": "Figure 15:Lower is better(â†“)â†“(\\downarrow)( â†“ ). Validation losses when varying architectures. Note: â€œ0E4Dâ€ is a decoder-only model.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x14.png",
                "caption": "Figure 15:Lower is better(â†“)â†“(\\downarrow)( â†“ ). Validation losses when varying architectures. Note: â€œ0E4Dâ€ is a decoder-only model.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x15.png",
                "caption": "Figure 16:Lower is better(â†“)â†“(\\downarrow)( â†“ ). Lowest observed validation losses when varying model sizes.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x16.png",
                "caption": "Figure 17:Lower is better(â†“)â†“(\\downarrow)( â†“ ). Lowest observed validation losses when training over varying maximum sequence lengths.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x16.png",
                "caption": "Figure 17:Lower is better(â†“)â†“(\\downarrow)( â†“ ). Lowest observed validation losses when training over varying maximum sequence lengths.",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x17.png",
                "caption": "Figure 18:Lower is better(â†“)â†“(\\downarrow)( â†“ ). Validation losses when showing certain features (â€œCâ€ = Cell, â€œWâ€ = Window). â€œRâ€ = using rest of the features.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x18.png",
                "caption": "Figure 19:Lower is better(â†“)â†“(\\downarrow)( â†“ ). MSE after OOD fine-tuning from a fixed checkpoint, while varying the learning rate. Early checkpoint (Step 10K) is used for adaptation.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x18.png",
                "caption": "Figure 19:Lower is better(â†“)â†“(\\downarrow)( â†“ ). MSE after OOD fine-tuning from a fixed checkpoint, while varying the learning rate. Early checkpoint (Step 10K) is used for adaptation.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x19.png",
                "caption": "Figure 20:Lower is better(â†“)â†“(\\downarrow)( â†“ ). MSE after OOD fine-tuning from a specific checkpoint-step during pretraining. A fixed learning rate of5Ã—10âˆ’55superscript1055\\times 10^{-5}5 Ã— 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPTwas used.",
                "position": 649
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Theory",
        "images": []
    },
    {
        "header": "Appendix BAdditional Data Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x20.png",
                "caption": "Figure 22:Spread of cells included in our study, separated by month.",
                "position": 1131
            }
        ]
    },
    {
        "header": "Appendix CExperimental Settings",
        "images": []
    }
]