[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x1.png",
                "caption": "Figure 1:Overview: Using(x,y)𝑥𝑦(x,y)( italic_x , italic_y )metric data collected from a variety of system logs, we train a encoder-decoder via standard next-token cross-entropy for performance prediction.",
                "position": 149
            }
        ]
    },
    {
        "header": "2System Performance Prediction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x2.png",
                "caption": "Figure 2:Example anonymized string representations of some features used to constructx𝑥xitalic_x. More detailed representation can be found in AppendixB.1.",
                "position": 308
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x2.png",
                "caption": "Table 1:Average character counts for each YAML feature.",
                "position": 339
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x3.png",
                "caption": "Figure 4:Left:Diagonal fit (╱╱\\diagup╱) is better. RLM’s pointwise prediction against ground truth target.Right:Higher is better (↑↑\\uparrow↑). Spearman-rank correlation of the test evaluations, after removing top outliers by MSE error.",
                "position": 509
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x4.png",
                "caption": "Figure 5:Better density capture of target points is better. Kernel Density Estimate (KDE) plot of samples frompθ⁢(y|x)subscript𝑝𝜃conditional𝑦𝑥p_{\\theta}(y|x)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y | italic_x )along with actual target points, over varying timestamps. Note that samples are generated fromx𝑥xitalic_x’s with distinct timestamps, while some target points may share timestamps.",
                "position": 512
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x5.png",
                "caption": "Figure 6:Left-skewness(←)←(\\leftarrow)( ← )is better. Note both axes are log-scaled.Left:Distribution of residuals as per-sample squared error, along with mean squared error as a vertical line.Right:Analogous results, but for an out-of-distribution task.",
                "position": 521
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x6.png",
                "caption": "Figure 7:Higher is better (↑↑\\uparrow↑). Evaluation task rank-correlation, when varying the number of finetuning examples starting from a pretrained checkpoint trained over varying numbers of tasks. Runs repeated over 10 seeds each and averaged.",
                "position": 527
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x7.png",
                "caption": "Figure 8:Correlation between prediction uncertainty (density variance) and residual squared error.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x7.png",
                "caption": "Figure 8:Correlation between prediction uncertainty (density variance) and residual squared error.",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x8.png",
                "caption": "Figure 9:RLM densitypθ⁢(y|x)subscript𝑝𝜃conditional𝑦𝑥p_{\\theta}(y|x)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y | italic_x )for an example inputx𝑥xitalic_x, along with truey𝑦yitalic_y-values.",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x9.png",
                "caption": "Figure 10:Diagonal fit (╱╱\\diagup╱) is better. Scatter plot of predictions and ground truth targets over multiple tasks.",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x10.png",
                "caption": "Figure 11:Better density capture of target points is better. KDE plot over multiple tasks.",
                "position": 557
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x11.png",
                "caption": "Figure 12:Higher is better(↑)↑(\\uparrow)( ↑ ).Left:Explained variance per task.Right:McFadden’s Pseudo-R2superscript𝑅2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPTper task. Tasks are sorted by decreasing resultant rank correlations.",
                "position": 563
            }
        ]
    },
    {
        "header": "5Experiments: Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x12.png",
                "caption": "Figure 13:Lower is better(↓)↓(\\downarrow)( ↓ ). MSE across checkpoint-steps.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x12.png",
                "caption": "Figure 13:Lower is better(↓)↓(\\downarrow)( ↓ ). MSE across checkpoint-steps.",
                "position": 579
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x13.png",
                "caption": "Figure 14:Higher is better(↑)↑(\\uparrow)( ↑ ). Spearmanρ𝜌\\rhoitalic_ρacross checkpoint-steps.",
                "position": 583
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x14.png",
                "caption": "Figure 15:Lower is better(↓)↓(\\downarrow)( ↓ ). Validation losses when varying architectures. Note: “0E4D” is a decoder-only model.",
                "position": 596
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x14.png",
                "caption": "Figure 15:Lower is better(↓)↓(\\downarrow)( ↓ ). Validation losses when varying architectures. Note: “0E4D” is a decoder-only model.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x15.png",
                "caption": "Figure 16:Lower is better(↓)↓(\\downarrow)( ↓ ). Lowest observed validation losses when varying model sizes.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x16.png",
                "caption": "Figure 17:Lower is better(↓)↓(\\downarrow)( ↓ ). Lowest observed validation losses when training over varying maximum sequence lengths.",
                "position": 618
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x16.png",
                "caption": "Figure 17:Lower is better(↓)↓(\\downarrow)( ↓ ). Lowest observed validation losses when training over varying maximum sequence lengths.",
                "position": 620
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x17.png",
                "caption": "Figure 18:Lower is better(↓)↓(\\downarrow)( ↓ ). Validation losses when showing certain features (“C” = Cell, “W” = Window). “R” = using rest of the features.",
                "position": 624
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x18.png",
                "caption": "Figure 19:Lower is better(↓)↓(\\downarrow)( ↓ ). MSE after OOD fine-tuning from a fixed checkpoint, while varying the learning rate. Early checkpoint (Step 10K) is used for adaptation.",
                "position": 643
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x18.png",
                "caption": "Figure 19:Lower is better(↓)↓(\\downarrow)( ↓ ). MSE after OOD fine-tuning from a fixed checkpoint, while varying the learning rate. Early checkpoint (Step 10K) is used for adaptation.",
                "position": 645
            },
            {
                "img": "https://arxiv.org/html/2506.21718/x19.png",
                "caption": "Figure 20:Lower is better(↓)↓(\\downarrow)( ↓ ). MSE after OOD fine-tuning from a specific checkpoint-step during pretraining. A fixed learning rate of5×10−55superscript1055\\times 10^{-5}5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPTwas used.",
                "position": 649
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Theory",
        "images": []
    },
    {
        "header": "Appendix BAdditional Data Information",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.21718/x20.png",
                "caption": "Figure 22:Spread of cells included in our study, separated by month.",
                "position": 1131
            }
        ]
    },
    {
        "header": "Appendix CExperimental Settings",
        "images": []
    }
]