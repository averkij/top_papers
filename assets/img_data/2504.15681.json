[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x1.png",
                "caption": "",
                "position": 56
            }
        ]
    },
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x2.png",
                "caption": "Figure 1:Temporal retrieval accuracy of different models on the proposed VUE-TR benchmark.",
                "position": 71
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x3.png",
                "caption": "Figure 2:Examples of temporal retrieval queries and their corresponding time ranges from the proposed VUE-TR benchmark. The duration of the example video is3,87138713,8713 , 871seconds (i.e., 01:04:31). Each query is presented in one of three formats (i.e., keyword, phrase, sentence) with visual or audio information. Facial regions in the video frames are blurred to preserve privacy.",
                "position": 86
            }
        ]
    },
    {
        "header": "2General Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x4.png",
                "caption": "Figure 3:An overview of the Vidi architecture. Raw visual and audio inputs are first process by pretrained modality-specific encoders to extract token sequences. A multimodal adapter is applied to both the visual and audio branches to compress the token sequences and project them into the input space of the pretrained LLM[11,24]. Notably, the LLM operates using the decomposed attention[13]to enable efficient and scalable modeling over long, densely sampled multimodal sequences.",
                "position": 110
            }
        ]
    },
    {
        "header": "3Model Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x5.png",
                "caption": "Figure 4:Decomposed Attention[13]equivalently decomposes causal self-attention in an LLM into three components: visual-to-visual (V2V), textual-to-textual (T2T), and textual-to-visual (T2V) attentions.",
                "position": 148
            }
        ]
    },
    {
        "header": "4Multimodal Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x6.png",
                "caption": "Figure 5:An overview of the synthetic training data generation pipeline. Visual and audio segments can be duplicated at multiple timestamps to simulate real videos.",
                "position": 233
            },
            {
                "img": "https://arxiv.org/html/2504.15681/x7.png",
                "caption": "Figure 6:An overview of the proposed real video training data generation pipeline. Long videos are first segmented into short clips using scene boundary detection and subtitle punctuation. Then, existing LMMs are used to generate dense, timestamp-aligned captions for each clip, resulting in high-quality supervision for real-world temporal grounding tasks.",
                "position": 257
            }
        ]
    },
    {
        "header": "5Application Post-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x8.png",
                "caption": "Figure 7:An illustration of post-training data generation pipeline for temporal retrieval. Queries of varying formats (keywords, phrases, and sentences) are constructed and paired with timestamp annotations from real videos.",
                "position": 345
            }
        ]
    },
    {
        "header": "6Evaluation Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x9.png",
                "caption": "Figure 8:The distribution of query modality and format in the VUE-TR benchmark. This diverse distribution reflects real-world retrieval scenarios and enables fine-grained analysis of model performance across input types.",
                "position": 449
            },
            {
                "img": "https://arxiv.org/html/2504.15681/x10.png",
                "caption": "",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2504.15681/x11.png",
                "caption": "Figure 9:Definition of intersection and union for temporal retrieval. Both prediction and ground-truth annotations may contain multiple timestamp ranges. The IoU for each sample is computed as‚àë‚Ñê‚Å¢(TP,TG)/‚àëùí∞‚Å¢(TP,TG)‚ÑêsubscriptùëáùëÉsubscriptùëáùê∫ùí∞subscriptùëáùëÉsubscriptùëáùê∫\\sum\\mathcal{I}(T_{P},T_{G})/\\sum\\mathcal{U}(T_{P},T_{G})‚àë caligraphic_I ( italic_T start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) / ‚àë caligraphic_U ( italic_T start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ).",
                "position": 493
            }
        ]
    },
    {
        "header": "7Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.15681/x12.png",
                "caption": "Figure 10:Overall performance curves for temporal retrieval on the VUE-TR benchmark. We report accuracy across varying thresholds for different models.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2504.15681/x12.png",
                "caption": "",
                "position": 985
            },
            {
                "img": "https://arxiv.org/html/2504.15681/x13.png",
                "caption": "",
                "position": 989
            },
            {
                "img": "https://arxiv.org/html/2504.15681/x14.png",
                "caption": "",
                "position": 993
            }
        ]
    },
    {
        "header": "8Related Work",
        "images": []
    },
    {
        "header": "9Conclusion",
        "images": []
    },
    {
        "header": "10Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]