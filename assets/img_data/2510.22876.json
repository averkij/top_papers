[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22876/x1.png",
                "caption": "Figure 1:Batch speculative decoding on Vicuna-7B/68M: Existing methods achieve high throughput butviolate the fundamental requirement of output equivalenceby producing corrupted outputs. Our approach maintains perfect correctness while still achieving competitive performance.",
                "position": 102
            }
        ]
    },
    {
        "header": "2Design Space Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22876/x2.png",
                "caption": "Figure 2:The ragged tensor problem in batch speculative decoding.\nDiffering numbers of accepted draft tokens across sequences in the same batch lead to ragged-shaped input IDs tensors and KV Cache that disrupt subsequent batch operations.",
                "position": 215
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22876/x3.png",
                "caption": "Figure 3:EqSpecsynchronizes via unpad–append–repad.",
                "position": 554
            },
            {
                "img": "https://arxiv.org/html/2510.22876/x4.png",
                "caption": "Figure 4:EXSpecpools ragged sequences by length, avoiding realignment; only unmatched sequences need syncing, turning fixed overhead into optional cost.",
                "position": 568
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22876/x5.png",
                "caption": "Figure 5:Decomposing batch speculative decoding performance. (a) Batch scaling efficiency normalized to BS=1, isolating GPU parallelism from per-sequence speculation;EXSpecinitially exceeds the No-Ragged-Scaling upper bound before degrading due to alignment overhead. (b) Alignment overhead grows super-linearly with batch size, consuming up to 38% of inference time, validating thatcoverhead​(B)c_{\\text{overhead}}(B)dominates at scale. (c) Cross-batch grouping rates on Multi30k for random vs. uniform-length sequences, showing that length homogeneity transforms grouping effectiveness.",
                "position": 961
            },
            {
                "img": "https://arxiv.org/html/2510.22876/x6.png",
                "caption": "Figure 6:Speculative decoding lags non-speculative baselines, with larger batches further degrading throughput.",
                "position": 992
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Reproducibility Statement",
        "images": []
    },
    {
        "header": "Disclose on LLM usage",
        "images": []
    },
    {
        "header": "Ethics Statement",
        "images": []
    }
]