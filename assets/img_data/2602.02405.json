[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02405/x1.png",
                "caption": "Figure 1:Overview of DAIL. Starting from a small set of expert solutions, we generate in-distribution transformed solutions for training viamixed policy decoding: the student model uses the expert solution as a reference to produce a detailed reasoning trace. This process mitigatesdidactic shortcuts, e.g., the expert solution skips the proof of why∠​A​O​M\\angle AOMis acute (inred), while the transformed solution explicitly details this reasoning (ingreen). Using this dataset of transformed solutions, DAIL applies contrastive learning on paired full vs. partial solutions to discourage imitatingrationalization shortcutsin transformed solutions, e.g., the model ignoring the irrational part of a derived result to force the generation of the correct answer (inorange).",
                "position": 162
            }
        ]
    },
    {
        "header": "2Distribution Aligned Imitation Learning",
        "images": []
    },
    {
        "header": "3Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02405/x2.png",
                "caption": "Figure 2:Pass@kkperformance comparison of DAIL on Qwen2.5-7B-Instruct withe1-verifiablecompared to RLVR methods on IMO-Answer, Beyond AIME, and AIME 2024 / 2025 benchmarks. DAIL exhibits consistent performance improvements over the base instruction model, while applying RLVR methods results in pass@k reductions due to the difficulty of training dataset problems.",
                "position": 272
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x3.png",
                "caption": "Figure 3:Baselines.Comparing pass@k performance of DAIL to temperature, STaR rationalization, and direct SFT on expert solutions baselines. To better visualize the relative performance between baselines, the results are plotted fork≥8k\\geq 8. See Figure15for results at lowerkkvalues.",
                "position": 275
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x4.png",
                "caption": "Figure 4:Test-Time Efficiency.Performance on mathematics benchmarks under various token reasoning limits. To ensure gains are not simply due to non-response, models are given 2048 tokens beyond these limits to produce their final answer. Compared to Qwen3-8B (think), the model trained with DAIL one1-proofyields improved performance across token budgets.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x5.png",
                "caption": "Figure 5:Contrastive loss consistently outperforms NLL.Comparison of the performance of Qwen2.5-7B-Instruct trained with DAIL’s contrastive objective and standard NLL. Contrastive loss outperforms NLL across generation settings and metrics.",
                "position": 477
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x6.png",
                "caption": "Figure 6:Baseline methods learn to imitate training data shortcuts.Evaluation one1-verifiable. While NLL baselines learn to mimic shortcuts and RLVR baselines overfit to stochastic successes, both achieve high pass@k on seen data but degrade on unseen benchmarks (see Figures2and15). DAIL’s lower training performance yet superior test generalization confirms that our contrastive loss effectively penalizes non-robust reasoning patterns.",
                "position": 494
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02405/x7.png",
                "caption": "Figure 7:Effect of mixed policy rollouts.Comparison of pass@128 improvement for Qwen2.5-7B-Instruct and Qwen3-8B (thinking). For the non-reasoning model (left), mixed policy rollouts slightly underperform compared to direct sampling. Conversely, reasoning models (right) benefit from mixed policy rollouts, suggesting that more in-distribution samples better support more deliberate reasoning processes.",
                "position": 516
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02405/x8.png",
                "caption": "Figure 11:Calibratingτ\\mathbf{\\tau}.e1-verifiabletraining dataset performance acrossτ\\tauvalues. We selectτ=0.8\\tau=0.8across experiments.",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x9.png",
                "caption": "Figure 12:Distribution of lengths of expert human solutions and in-distribution traces generated with mixed policy rollouts using Qwen2.5-7B-Instruct one1-verifiable.",
                "position": 1261
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x10.png",
                "caption": "Figure 13:Loss curve for our contrastive loss. We find training is stable, with spikes in loss only occurring at epoch boundaries.",
                "position": 1357
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x10.png",
                "caption": "Figure 13:Loss curve for our contrastive loss. We find training is stable, with spikes in loss only occurring at epoch boundaries.",
                "position": 1360
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x11.png",
                "caption": "Figure 14:IMO-AnswerBench results with problems with the same source de-duplicated. There is little difference between these results and those in Figure2.",
                "position": 1365
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x12.png",
                "caption": "Figure 15:A version of Figure3for allkkvalues. These results include the aggregated performance of Qwen2.5-7B-Instruct when training one1-verifiable.",
                "position": 1371
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x13.png",
                "caption": "Figure 16:Test-time efficiency on smaller, less capable models. We find that for these less capable models, improvements are more mixed. This finding indicates we need a baseline level of reasoning performance to apply DAIL.",
                "position": 1401
            },
            {
                "img": "https://arxiv.org/html/2602.02405/x14.png",
                "caption": "",
                "position": 1405
            }
        ]
    },
    {
        "header": "Appendix BGeneration, Training, and Evaluation Details",
        "images": []
    }
]