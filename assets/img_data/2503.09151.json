[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/teaser-after.jpeg",
                "caption": "",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/training-pipeline.png",
                "caption": "Figure 3:Multi-view motion learning pipelines for(a) Static view transportand(b) Dynamic camera control.For both tasks, we distill view-robust motion of the underlying scene to a pre-trained MM-DiT video model[85], using all visible pixels within the sampled videos.\nThis few-shot, self-supervised training optimizes only the LoRA layers[35,62], enabling lightweight training.",
                "position": 257
            }
        ]
    },
    {
        "header": "3Reangle-A-Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/inpainting-method.jpeg",
                "caption": "Figure 4:Multi-view consistent image inpainting using stochastic control guidance.In experiments, we setS=25ùëÜ25S=25italic_S = 25.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/inpainting-compare.jpeg",
                "caption": "Figure 5:Qualitative inpainting comparisons.We compare naive inpainting to inpainting with stochastic control guidance.",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/comparisons.jpg",
                "caption": "Figure 6:Qualitative comparisons.Top half shows(a) Static view transportand bottom half presents(b) Dynamic camera controlresults.\nThe first row in each half displays the input videos, and for each input video, two generated videos corresponding to target cameras (1 and 2) are shown for each method.\nAcross baseline, same camera parameters were used for each 1,2.\nVisitour pagefor full-video results.",
                "position": 487
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/main-user-study.png",
                "caption": "Figure 7:User study results.Top: Static view transport results.\nBottom: Dynamic camera control results.",
                "position": 694
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/without-warped.jpeg",
                "caption": "Figure 8:Novel view video generation with and without using warped video for training (target viewpoint: dolly zoom in).",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/robustness.jpeg",
                "caption": "Figure 9:Top: Unseen view video generation.\nBottom: Novel view video generation using an appearance-edited first frame.",
                "position": 756
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Experimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/camera-vis.png",
                "caption": "Figure 10:Visualizations of the used camera types.",
                "position": 2081
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/mask-downsampling.png",
                "caption": "Figure 11:Temporal downsampling of visibility masks.Except for the first mask frame, pixel-wise (element-wise) logical AND operation is done for every four masks.",
                "position": 2111
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/masked-diffusion-loss.png",
                "caption": "Figure 12:Impact of masked diffusion loss on video quality.Masking the diffusion loss effectively prevents artifacts.",
                "position": 2400
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/appendix_limit_1.jpeg",
                "caption": "Figure 13:Geometric misalignment in the warped frame.In this example, the target camera view is shifted 10 degrees to the (horizontal orbit) right of the input frame.",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/appendix_limit_2.jpeg",
                "caption": "Figure 14:Pixel-scale artifacts in the warped frame.In this example, the target camera view is shifted 10 degrees to the (horizontal orbit) left of the input frame.",
                "position": 2421
            },
            {
                "img": "https://arxiv.org/html/2503.09151/extracted/6271571/figures/failure-case.png",
                "caption": "Figure 15:Failure cases.",
                "position": 2427
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Failure Cases",
        "images": []
    }
]