[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02095/x1.png",
                "caption": "Figure 1:The above is outcome supervision in long-form generation tasks. Below is LongDPO uses process supervision with a global memory to maintain factual consistency, and external critiques to refine low-reward chosen candidates.",
                "position": 118
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02095/x2.png",
                "caption": "Figure 2:The pipeline of LongDPO. LongDPO incorporates process supervision and MCTS to collect stepwise preference data, where the preference data share the same prefix and only one pair is collected at each layer. During the selection phase, LongDPO uses the global memory pool to filter out candidates that may result in inconsistency, then selects the highest-scoring one as the chosen candidate, with another randomly selected as the rejected candidate. During tree expansion, some chosen candidates may have low rewards, LongDPO uses external knowledge to provide critiques for refinement. Then the collected preference pairs are used for step-level DPO training.",
                "position": 183
            }
        ]
    },
    {
        "header": "3LongDPO",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02095/x3.png",
                "caption": "Figure 3:Reward analysis of the selected candidates, we focus solely on the chosen candidate in each preference pair. On the x-axis, ’0-3.0’ represents the proportion of candidates with an average reward<3.0absent3.0<3.0< 3.0, while ’3.0-3.5’ represents the proportion of candidates with an average reward≥3.0absent3.0\\geq 3.0≥ 3.0but<3.5absent3.5<3.5< 3.5. Detailed reward distribution can be found in Appendix6.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2502.02095/x4.png",
                "caption": "Figure 4:Main body of generated critiques which have detailed in AppedixA.2",
                "position": 337
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02095/x5.png",
                "caption": "Figure 5:A case is randomly sampled from LongGenBench. The instruction primarily requires visiting the farmers’ market starting from week 10 and then every 5 weeks thereafter. On the left, LongWriter-Llama fulfills the requirement in week 10 but fails in week 15. On the right, after applying LongDPO, LongWriter-Llama is able to consistently meet the demands.",
                "position": 862
            }
        ]
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.02095/x6.png",
                "caption": "Figure 6:Detailed reward analysis of the chosen candidates.",
                "position": 2259
            },
            {
                "img": "https://arxiv.org/html/2502.02095/x7.png",
                "caption": "Figure 7:The part highlighted in red is the correct answer to the question. LongWriter-Llama fails to provide the correct answer, but after applying LongDPO, it is able to answer correctly.",
                "position": 2264
            },
            {
                "img": "https://arxiv.org/html/2502.02095/x8.png",
                "caption": "Figure 8:The part highlighted in red is the correct answer to the question. LongWriter-Llama fails to provide the correct answer, but after applying LongDPO, it is able to answer correctly.",
                "position": 2269
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]