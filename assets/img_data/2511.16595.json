[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16595/x1.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16595/x2.png",
                "caption": "Figure 2:Illustration of TimeViper, our proposed hybrid MLLM for long video understanding. The model consists of a ViT visual encoder, a projector with token merging, and a hybrid Mamba-Transformer LLM equipped with TransV. The token merging[bolya2022tokenmerge]compresses each frame into 16 vision tokens. Inside the LLM, TransV transfers information from redundant vision tokens to instruction tokens to reduce the number of vision tokens. Specifically, TransV uniformly drops vision tokens in shallow layers and removes low-attention vision tokens in deeper layers. The compression module is implemented through a Gated Cross-Attention mechanism[alayrac2022flamingo]with adaptive learnable weights. Note that TransV is illustrated before the attention layer for clarity, though it may be applied before any layer in practice.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16595/file/fig3-blockattn-qa-tvg-vdc-noblockboth.png",
                "caption": "Figure 3:Comparison of information blocking to illustrate the vision-to-text information aggregation phenomenon in hybrid MLLMs.\nFor instruction-centric tasks (e.g., multi-choice video QA), information is first aggregated from vision tokens to instruction tokens, which are then used for response generation.\nIn contrast, for vision-centric tasks (e.g., detailed video captioning), vision tokens directly contribute to response generation.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2511.16595/file/fig4-pdrop-qa-tvg-vdc.png",
                "caption": "Figure 4:Illustration of token redundancy. We compare performance under different vision-token dropping ratesppusing uniform dropping and attention-guided dropping strategies.\nIn the hybrid MLLM, vision token redundancy increases progressively with layer depth, allowing more aggressive token removal in deeper layers with minimal performance loss.",
                "position": 341
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16595/file/frame_mem.png",
                "caption": "Figure 5:Comparison of GPU memory usage during inference.\nWhile ToMe extends the context window to about 5K frames, TransV efficiently scales beyond 10K frames.",
                "position": 481
            },
            {
                "img": "https://arxiv.org/html/2511.16595/file/frame_time.png",
                "caption": "Figure 6:Comparison of prefilling time.\nTransV incurs no additional latency at low frame inputs (e.g., 64 frames) while significantly reducing prefilling time at high frame inputs.\nFor instance, at 4,096 frames, TransV reduces prefilling time by 15.7% compared to the ToMe baseline.",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2511.16595/file/benchmark_framenum.png",
                "caption": "Figure 7:Comparison of performance as the number of input frames increases on long-video understanding benchmarks.\nWe train our models with 256 frames as inputs, and sample 1 frame per second during evaluation.\nThe x-axis here denotes the maximum number of frames.\nIf a video exceeds this length, we take only the first max frames for inference.",
                "position": 921
            },
            {
                "img": "https://arxiv.org/html/2511.16595/file/fig5-matrix.png",
                "caption": "Figure 8:Illustration of attention score matrices in Nanov2[basant2025nanov2]and Qwen2.5[qwen2.5]at shallow and deep layers.\nWhite lines divide the input sequence into four distinct segments: system prompt, vision tokens, user instruction, and the generated response.",
                "position": 948
            },
            {
                "img": "https://arxiv.org/html/2511.16595/x3.png",
                "caption": "Figure 9:Qualitative results of TimeViper on three long video understanding tasks. (1) MCQ: The model demonstrates reasoning capability by correctly answering a multi-choice question about the videoâ€™s content. (2) TVG: It accurately localizes the temporal boundaries for a specific event, reaching an IoU of 0.75. (3) VDC: The model generates a detailed description that showcases its fine-grained comprehension. Green text highlights accurate detailed descriptions. Some output in the middle is omitted for brevity.",
                "position": 989
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ALimitations",
        "images": []
    },
    {
        "header": "Appendix BExperimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16595/file/fig6-attentions-per-layer-share2.png",
                "caption": "Figure 10:Comparison of average attention scores across all layers in Nanov2 and Qwen2.5.\nThe visualization shows both attention and Mamba layers for Nano, and attention layers for Qwen.\nFor Mamba layers, we normalize each row of the attention scores using theL1L_{1}norm so that all values fall within the range[0,1][0,1].",
                "position": 1092
            }
        ]
    },
    {
        "header": "Appendix CMain Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.16595/x4.png",
                "caption": "(a)Qualitative results on VDC.",
                "position": 1334
            },
            {
                "img": "https://arxiv.org/html/2511.16595/x4.png",
                "caption": "(a)Qualitative results on VDC.",
                "position": 1337
            },
            {
                "img": "https://arxiv.org/html/2511.16595/x5.png",
                "caption": "(b)Qualitative results on Charades.",
                "position": 1343
            },
            {
                "img": "https://arxiv.org/html/2511.16595/x6.png",
                "caption": "(c)Qualitative results on VideoMME.",
                "position": 1349
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": []
    }
]