[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Theory",
        "images": []
    },
    {
        "header": "3Approach",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.08727/extracted/6435953/loss_curve_layer_mbe.png",
                "caption": "Figure 1:Left: CE and MBE loss curves during pretraining with CE loss only, showing implicit momentum for representation compression. Right: final per-layer MBE values. Later layers show lower MBE, indicating representation compression.",
                "position": 549
            },
            {
                "img": "https://arxiv.org/html/2505.08727/extracted/6435953/cecegradcosim2.png",
                "caption": "Figure 2:Cosine similarity between CE gradients across batches. CE gradients become increasingly decorrelated over time, reflecting diminishing shared signal.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2505.08727/extracted/6435953/ce_mbe_grad_cosim4.png",
                "caption": "Figure 3:Cosine similarity between CE and MBE gradients over training. Alternating positive and negative phases indicate emergent memorizationâ€“compression cycles.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2505.08727/extracted/6435953/oscillation_heatmap.png",
                "caption": "Figure 4:Oscillation metrics between CE and MBE gradients across layers and parameter groups. Left: standard deviation; center: zero-crossing rate; right: periodic strength (peak-to-mean PSD ratio).",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2505.08727/extracted/6435953/layer_mbe_exp2.png",
                "caption": "Figure 5:Left: Layer-wise MBE for baseline vs. GAPT. Right: Per-layer MBE reduction with GAPT.",
                "position": 607
            },
            {
                "img": "https://arxiv.org/html/2505.08727/extracted/6435953/arithmetic_mbe_gapt.png",
                "caption": "Figure 6:Up: Comparison of baseline vs. GAPT on arithmetic generalization. Bottom: Arithmetic generalization performance summary. GAPT improves OOD generalization and yields more compact representations.",
                "position": 803
            }
        ]
    },
    {
        "header": "5Relevant Work",
        "images": []
    },
    {
        "header": "6Implication",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]