[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13797/figure/teaser_1.jpg",
                "caption": "Figure 1.Given an input video (yellow box in (a)) with significant viewpoint changes, Sketch3DVE generates realistic editing results (blue box) with the inputs of a text prompt, hand-drawn sketch, and a mask (in white). Our method can also edit a video generated given an input image and a camera trajectory (b). Input Video ©DL3DV-10K, Input image ©Pegah Sharifi.",
                "position": 90
            }
        ]
    },
    {
        "header": "1.Introduction",
        "images": []
    },
    {
        "header": "2.Related Work",
        "images": []
    },
    {
        "header": "3.Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13797/x1.png",
                "caption": "Figure 2.The pipeline of our sketch-based 3D-ware video editing method.\nGiven an input video, the edited result of the first frame,Ie​d​i​tI_{edit}is generated by the image editing model(Liu et al.,2024d)using the sketch, mask, and text prompt input.\nWe derive a 3D point cloud and camera parameters from the input video to propagate the editing. Then, we utilize a depth map to represent and align the newly edited content with the original scene to generate the edited point cloud.\nTo identify edited regions, we construct a 3D mask that is rendered for all the frames to generate a mask sequence.\nThe point cloud rendering results, the original video containing unedited regions, and the edited images serve as inputs for the conditional video diffusion model to synthesize the final edited video. Input Video ©DL3DV-10K.",
                "position": 261
            },
            {
                "img": "https://arxiv.org/html/2508.13797/figure/results2.jpg",
                "caption": "Figure 3.Sketch-based video editing results.\nFor each example, we show the input text prompt, sketch, and edited image on the left and the original and edited videos on the right. Our method generates realistic editing results for videos with significant viewpoint changes. Original Video ©DL3DV-10K, ©Taryn Elliott, ©Samir Smier.",
                "position": 430
            }
        ]
    },
    {
        "header": "4.Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13797/figure/compare_1.jpg",
                "caption": "Figure 4.Comparison with existing approaches. Given original video (a) and sketch-based editing on the first frame (Left), existing video editing methods, including AnyV2V(Ku et al.,2024)and I2VEdit(Ouyang et al.,2024a), generate fuzzy details.\n3D-aware video generation methods, including ViewExtrapolation(Liu et al.,2024c)and ViewCrafter(Yu et al.,2024)cannot handle significant viewpoint changes.\nDirectly fusing the generation results of Viewcrafter with the original video in pixel space improves the results but with obvious seams (f). In contrast,\nour method (g) generates the most realistic video editing results. Original Video ©DL3DV-10K.",
                "position": 492
            }
        ]
    },
    {
        "header": "5.Applications",
        "images": []
    },
    {
        "header": "6.Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.13797/figure/ablation_1.jpg",
                "caption": "Figure 5.The ablation study of point cloud alignment. (a) An input video. (b, d) Edited point cloud rendering results. (c, e) Edited videos.\nOur method generates a more reasonable edited point cloud than the baseline without depth-guided alignment, resulting in better video editing results with reasonable geometry and component interaction. Original Video ©DL3DV-10K.",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2508.13797/figure/ablation_2.jpg",
                "caption": "Figure 6.The ablation study of 3D-aware mask and point cloud rendering condition.\nGiven the original video (a) and editing operations (Left), our method produces the edited point cloud rendering and 3D-aware masks (b).\nWithout the masks (c), the unedited regions(e.g., the shape of stone)are not preserved.\nWithout the point cloud rendering (d), the shape of red leaves is changed in different frames.\nOur method (e) generates the best results. Original Video ©DL3DV-10K.",
                "position": 2021
            },
            {
                "img": "https://arxiv.org/html/2508.13797/figure/application_3D_gen_2_1.jpg",
                "caption": "Figure 7.The application of camera-controllable video generation.\nGiven a single input image, users can control the camera viewpoints and generate a video with novel view results. Local region editing can be further added to achieve detailed, controllable video generation. Input Image ©Cats Coming.",
                "position": 2038
            },
            {
                "img": "https://arxiv.org/html/2508.13797/figure/application_color_1.jpg",
                "caption": "Figure 8.The application of color stroke-based video editing.\nThe appearance of existing content (bed in the 1st example)\ncan be edited by using color strokes.\nUsers can also control the appearance of newly generated objects, with the editing sketch shown in the top right corner in the 2nd example. Original Video ©DL3DV-10K.",
                "position": 2052
            },
            {
                "img": "https://arxiv.org/html/2508.13797/figure/application_inpainting_1.jpg",
                "caption": "Figure 9.The application of image inpainting-based video editing.\nThese examples utilize the text-based(Alibaba,2024)and reference-based(Yang et al.,2023a)image inpainting to editing the first frame. The editing operations are well propagated into videos. Original Video ©DL3DV-10K, Reference image ©Pixabay.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2508.13797/figure/failure_case_1.jpg",
                "caption": "Figure 10.Failure cases. For the video that DUSt3R(Wang et al.,2024a)estimates wrong point clouds and/or camera parameters, the edited point cloud and mask are also mistakenly rendered. The quality of the generated video is severely affected. Original Image ©stein egil liland.",
                "position": 2084
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]