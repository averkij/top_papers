[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24869/extracted/6495138/figures/silvr.png",
                "caption": "",
                "position": 60
            },
            {
                "img": "https://arxiv.org/html/2505.24869/x1.png",
                "caption": "Figure 1:Strong Reasoning Capabilities of\\modelon Complex Video QA Tasks.\\modelleverages recent advances in reasoning LLMs for complex video QA tasks.\\modelachieves better performance than strong proprietary non-reasoning models (i.e., GPT-4o and Gemini-1.5) on benchmarks like Video-MME (Long), Video-MMMU (Comprehension), Video-MMLU, and EgoLife, which include temporal, causal, long-context, and external knowledge reasoning tasks.\nThe example reasoning trace shows\\model’s capability to perform self-correction, in which it successfully identifies that shells are decorative rather than functional.",
                "position": 103
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24869/x2.png",
                "caption": "Figure 2:Method Overview.\\modelis a simple two-stage language-based video reasoning framework.Top: The video is segmented into short clips and paired with speech. A clip captioner processes each segment to generate visual descriptions. The speech is transcribed using ASR.Bottom: A reasoning LLM takes the question, transcribed speech, and dense visual descriptions compressed by Adaptive Token Reduction to perform complex video reasoning.\nIn the shown example,\\modelinfers the correct order by integrating information across both visual and speech modalities. The model correctly identifies the sequence through reasoning and eliminating incorrect options.",
                "position": 146
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24869/x3.png",
                "caption": "Figure 3:Performance Breakdown Across Different Question Categories.Full question category names can be found in the supplementary materials (Table10). Compared to using a non-reasoning LLM (i.e., LLama 4), using DeepSeek-R1 achieves significantly larger improvements on Video-MME questions from the reasoning category (a gain of+11.1%) than the general perception questions (a gain of+4.9%).",
                "position": 542
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Ablation Study",
        "images": []
    },
    {
        "header": "Appendix BMore Experimental Results",
        "images": []
    },
    {
        "header": "Appendix CAdditional Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.24869/x4.png",
                "caption": "Figure 4:Example 1 of\\model’s Reasoning Trace.The question asks which ingredients are not used in the video. Initially,\\modelidentified all listed items as potential ingredients. However, through self-correction,\\modelcorrectly recognized that the shell is used as decoration rather than an ingredient.",
                "position": 1999
            },
            {
                "img": "https://arxiv.org/html/2505.24869/x5.png",
                "caption": "Figure 5:Example 2 of\\model’s Reasoning Trace.The video sequentially introduces six planets in detail: Mercury, Venus, Mars, Jupiter, Saturn, and Neptune.\\modelaccurately identifies the correct order of the planets and systematically inspects all answer choices, eliminating the incorrect ones through logical reasoning.",
                "position": 2003
            },
            {
                "img": "https://arxiv.org/html/2505.24869/x6.png",
                "caption": "Figure 6:Example 3 of\\model’s Reasoning Trace.The question asks about the size of the back touchscreen in the car shown in the video. The visual captioning module of\\modelfails to capture the details about the touchscreen, which appears briefly for only about one second. However, by identifying the vehicle type and leveraging external knowledge from the LLM,\\modelinfers the correct answer.",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2505.24869/x7.png",
                "caption": "Figure 7:Example 4 of\\model’s Reasoning Trace.Through step-by-step reasoning,\\modelis capable of solving complex chemistry questions. Notably,\\modeldoes not immediately terminate the reasoning process upon reaching a plausible answer. Instead, it continues to verify the correctness of the generated answer before finalizing its response.",
                "position": 2011
            }
        ]
    },
    {
        "header": "Appendix DQualitative Results",
        "images": []
    }
]