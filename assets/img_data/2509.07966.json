[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/QA_sample.png",
                "caption": "Figure 1:Sample question in our benchmark",
                "position": 98
            }
        ]
    },
    {
        "header": "2Visual-TableQA Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/tableqa.png",
                "caption": "Figure 2:Overview of the full pipeline architecture of Visual-TableQA. A subset of initial table images is first converted to LaTeX using a visual language model (VLM-0). The resulting LaTeX code, along with topic prompts, is then passed to a language model (LLM-1) to generate new, diverse tables. These newly generated tables are used as inputs for further iterations of table generation. All generated tables are then submitted to a second language model (LLM-2), which produces corresponding question-answer pairs. Finally, the QA pairs are evaluated by a jury of high-performing LLMs, and their quality is assessed using the ROSCOE score.",
                "position": 1139
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/trends_similarity.png",
                "caption": "Figure 3:Correlation of model rankings on Visual-TableQA with those on three established datasets—ChartQA (recognition-focused), ReachQA (balanced), and MATH-Vision (reasoning-focused)—using Spearman’sρ\\rhoand Kendall’sτ\\taumetrics. Higher values indicate stronger alignment in model performance trends. Visual-TableQA shows strong correlation with ReachQA, suggesting it effectively balances both visual recognition and reasoning, while its weaker correlation with ChartQA and MATH-Vision highlights its unique position as a comprehensive visual reasoning benchmark.",
                "position": 1207
            }
        ]
    },
    {
        "header": "4Discussion",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Works",
        "images": []
    },
    {
        "header": "Appendix BConsiderations and Limitations",
        "images": []
    },
    {
        "header": "Appendix CTableQA Layout and Topic Diversity",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/topics_coverage_curve.png",
                "caption": "Figure 4:Cumulative topic coverage as clusters are added by descending size. The uniform slope indicates an even distribution of topics across clusters.",
                "position": 3023
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/topics_umap_with_legend_grid.png",
                "caption": "Figure 5:2D projection of the 5,000 topics using UMAP and K-Means clustering. Each color denotes a semantic cluster. Representative topics are listed for the 12 largest clusters to illustrate diversity.",
                "position": 3026
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/gallery_seeds_sheet.png",
                "caption": "(a)Layout seeds used for the initial table generation.",
                "position": 3032
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/gallery_seeds_sheet.png",
                "caption": "(a)Layout seeds used for the initial table generation.",
                "position": 3035
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/gallery_layout_sheet.png",
                "caption": "(b)Sample of diverse layouts generated by our pipeline.",
                "position": 3040
            }
        ]
    },
    {
        "header": "Appendix DTable Generation Settings",
        "images": []
    },
    {
        "header": "Appendix ECommon Anomalies in Generated Tables",
        "images": []
    },
    {
        "header": "Appendix FLLM Jury Settings",
        "images": []
    },
    {
        "header": "Appendix GROSCOE Metrics Scores",
        "images": []
    },
    {
        "header": "Appendix HModel Finetuning Hyperparameters",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/Histogram_of_Errors_for_Qwen2.5-vl-7b_Pretrained.png",
                "caption": "Table 8:Hyperparameters Used for Fine-Tuning with LoRA. More details in ourGitHub repository. Abbreviations: lr=learning rate, r= LoRA rank,α\\alpha= LoRAα\\alpha, Targets=targets modules for LoRA.",
                "position": 3970
            }
        ]
    },
    {
        "header": "Appendix IErrors Taxonomy",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/Histogram_of_Errors_for_Qwen2.5-vl-7b_Pretrained.png",
                "caption": "(a)Histogram of Erros for Qwen2.5-vl-7b.",
                "position": 4172
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/Histogram_of_Errors_for_Qwen2.5-vl-7b_Pretrained.png",
                "caption": "(a)Histogram of Erros for Qwen2.5-vl-7b.",
                "position": 4175
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/Histogram_of_Errors_for_Qwen2.5-vl-7b_Finetuned.png",
                "caption": "(b)Histogram of Erros for Qwen2.5-vl-7b after Finetuning.",
                "position": 4180
            }
        ]
    },
    {
        "header": "Appendix JVisual-TableQA Sample",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/table-1.png",
                "caption": "Table 9:Sample of reasoning-intensive QA pairs. The first row’s question and answer are truncated for readability. These questions address multiple visual aspects and extend beyond simple information extraction to test interpretive reasoning, as illustrated in the second row with a “how” question.",
                "position": 4193
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/table-2.png",
                "caption": "",
                "position": 4242
            },
            {
                "img": "https://arxiv.org/html/2509.07966/imgs/table-3.png",
                "caption": "",
                "position": 4258
            }
        ]
    },
    {
        "header": "Appendix KImage-to-LaTeX Dataset",
        "images": []
    }
]