[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17146/x1.png",
                "caption": "Figure 1:TheMolmoarchitecture follows the simple and standard design of combining a language model with a vision encoder. Its strong performance is the result of a well-tuned training pipeline and our newPixModata.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17146/extracted/5879154/figures/elo_plot.png",
                "caption": "Figure 2:(Left) Average scores on the 11 academic benchmarks. See Table1for per-benchmark results. (Right) Elo ratings from our human preference evaluation.",
                "position": 113
            }
        ]
    },
    {
        "header": "3Data and Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17146/extracted/5879154/figures/openness.png",
                "caption": "Figure 3:VLM Openness Comparison.We characterize the openness of VLMs based on two attributes (open weights, open data and code) across three model components (the VLM and its two pre-trained components, the LLM backbone and the vision encoder). In addition to open vs. closed, we use the ”distilled” label to indicate that the data used to train the VLM includes images and text generated by a different, proprietary VLM, meaning that the model cannot be reproduced without a dependency on the proprietary VLM.",
                "position": 230
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2409.17146/x2.png",
                "caption": "Figure 4:Our Elo human preference evaluations used 15k image and text prompt pairs. We queried each VLM for responses, and presented the resulting image-text-response triplets for all VLM pairings to a set of∼similar-to\\scriptstyle\\sim∼870 human annotators who gave pairwise preference rankings, for a total of 325k pairwise comparisons across 27 models, making it the biggest human preference evaluation for multimodal models to date. As a reference, our ELO rankings are based on 3×{\\times}×more votes than Chatbot Arena (LMSYS) for vision models.",
                "position": 721
            }
        ]
    },
    {
        "header": "5Release Plan",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]