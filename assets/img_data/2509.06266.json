[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06266/x1.png",
                "caption": "",
                "position": 66
            },
            {
                "img": "https://arxiv.org/html/2509.06266/x2.png",
                "caption": "",
                "position": 66
            },
            {
                "img": "https://arxiv.org/html/2509.06266/x3.png",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Fig2_v4.png",
                "caption": "Figure 1:Ego3D-Benchis the first 3D spatial benchmark for VLMs using ego-centric multi-view images. It spans ego- and object-centric perspectives across 5 categories. A significant gap exists between human and VLM performance; our method,Ego3D-VLM, consistently narrows this gap.",
                "position": 79
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Ego3D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06266/fig3.png",
                "caption": "Figure 2:Overview of our benchmark creation pipeline. Human annotators played a key role throughout the process. On the right, we show the distribution of the samples in ourEgo3D-Bench.Ego.: Ego-centric,Obj.: Object-centric.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Fig5_v2.png",
                "caption": "Figure 3:Ego- and object-centric samples from each category inEgo3D-Bench.",
                "position": 200
            }
        ]
    },
    {
        "header": "4Post-Training 3D Spatial Understanding:Ego3D-VLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06266/Fig1_v2.png",
                "caption": "Figure 4:An overview ofEgo3D-VLM, our post-training 3D spatial understanding framework.",
                "position": 265
            }
        ]
    },
    {
        "header": "5Evaluation onEgo3D-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06266/Fig4_v2.png",
                "caption": "Figure 5:Example question with associated textual and visual cognitive maps.",
                "position": 1151
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Fig6.png",
                "caption": "Figure 6:Average performance of leading VLMs (GPT-4o, Gemini-1.5-Pro, InternVL3-78B, and Qwen2.5-VL-72B) with/withoutEgo3D-VLMvs. chance and human levels on each category ofEgo3D-Bench.",
                "position": 1165
            },
            {
                "img": "https://arxiv.org/html/2509.06266/All-Angle.png",
                "caption": "Figure 7:Settings of multi-view/video spatial understanding benchmarks for VLMs.",
                "position": 1812
            },
            {
                "img": "https://arxiv.org/html/2509.06266/All-Angle.png",
                "caption": "Figure 7:Settings of multi-view/video spatial understanding benchmarks for VLMs.",
                "position": 1815
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.06266/Templates.png",
                "caption": "Figure 8:Templates used to create questions ofEgo3D-Bench.",
                "position": 2657
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig3.png",
                "caption": "Figure 9:Example responses of the baseline andEgo3D-VLMon ego-centric absolute distance task.",
                "position": 3584
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig6.png",
                "caption": "Figure 10:Example responses of the baseline andEgo3D-VLMon object-centric absolute distance task.",
                "position": 3588
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig10.png",
                "caption": "Figure 11:Example responses of the baseline andEgo3D-VLMon multi-choice ego-centric absolute distance task.",
                "position": 3592
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig11.png",
                "caption": "Figure 12:Example responses of the baseline andEgo3D-VLMon multi-choice object-centric absolute distance task.",
                "position": 3596
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_Fig1.png",
                "caption": "Figure 13:Example responses of the baseline andEgo3D-VLMon object-centric motion reasoning task.",
                "position": 3599
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_Fig2.png",
                "caption": "Figure 14:Example responses of the baseline andEgo3D-VLMon ego-centric motion reasoning task.",
                "position": 3602
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig4.png",
                "caption": "Figure 15:Example responses of the baseline andEgo3D-VLMon travel time (ego-centric) task.",
                "position": 3606
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig5.png",
                "caption": "Figure 16:Example responses of the baseline andEgo3D-VLMon travel time (object-centric) task.",
                "position": 3610
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig7.png",
                "caption": "Figure 17:Example responses of the baseline andEgo3D-VLMon object-centric relative distance task.",
                "position": 3614
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig8.png",
                "caption": "Figure 18:Example responses of the baseline andEgo3D-VLMon ego-centric relative distance task.",
                "position": 3618
            },
            {
                "img": "https://arxiv.org/html/2509.06266/Appendix_fig9.png",
                "caption": "Figure 19:Example responses of the baseline andEgo3D-VLMon localization task.",
                "position": 3622
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]