[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21037/figures/file_main_fig.png",
                "caption": "Figure 1:Video generation models as visual reasoners, empowered by (1) enriched visual context for improved geometric control and (2) visual test-time scaling that allocates a larger inference-frame budget and enables stronger performance on long-horizon, complex sequential planning tasks, together demonstrating robust generalization across diverse scenarios.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Video Generation for Visual Planning",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21037/x1.png",
                "caption": "Figure 2:Generated solution ofTangramPuzzleby different system variants. For Qwen-3-VL, we visualize the layout based on the predicted coordinates and rotations. We crop the main area for the predictions from image editing model and video generation model. For video generation model, we only select the last frame as illustration here. For full details, please refer to Figure13.",
                "position": 625
            }
        ]
    },
    {
        "header": "5Discussion and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21037/x2.png",
                "caption": "Figure 3:Visual Test-Time Scaling forMazeNavigationusing unseen icon with more inference budget. Row 1 shows the performance curve when increasing the total number of frames per video; Row 2 shows the performance curve when changing the scaling factorκ\\kappato allocate a different number of frames per discrete step in the maze solution. Detailed results for both settings are shown in Figure6and7.",
                "position": 944
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperimental Setups",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21037/x3.png",
                "caption": "Figure 4:Agent Icons forMazeNavigationduring training and visual OOD evaluation.",
                "position": 1677
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x4.png",
                "caption": "Figure 5:Illustration of different variants forTangramPuzzle.",
                "position": 1781
            }
        ]
    },
    {
        "header": "Appendix BResults",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.21037/x5.png",
                "caption": "Figure 6:Fine-Grained Visual Test-Time Scaling forMazeNavigationusing unseen icon with more inference frame budget.",
                "position": 2282
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x6.png",
                "caption": "Figure 7:Fine-Grained Visual Test-Time Scaling forMazeNavigationusing unseen icon with more inference budget determined by the control variable scaling factorκ\\kappa.",
                "position": 2285
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x7.png",
                "caption": "Figure 8:Example of trajectory similar to “self-correction” inMazeNavigationwhen provided with more inference frame budget.",
                "position": 2288
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x8.png",
                "caption": "Figure 9:Visualizations of zero-shot inference on irregular maze with Wan 2.2 TI2V 5B trained on regular mazes. We observe that although not included in the training data, trained video generation model can adapt a certain level of planning capabilities to such irregular mazes with different background and meanwhile keeping the constraint (no change in the background, follow the path, do not cross the wall), and surprisingly can move diagonally.",
                "position": 2291
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x9.png",
                "caption": "Figure 10:We don’t observe similar Visual Test-Time Scaling trend inTangramPuzzle, possibly due to the constraint of geometric consistency throughout the generated video that hinders the performance. However, we still observe that by providing more inference frames (81-121 frames) compared to training setting (61-81 frames) would not severely harm the performance.",
                "position": 2294
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x10.png",
                "caption": "Figure 11:Correct examples ofMazeNavigationgenerated by fine-tuned Wan 2.2 TI2V 5B.",
                "position": 2300
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x11.png",
                "caption": "Figure 12:Wrong examples ofMazeNavigationgenerated by fine-tuned Wan 2.2 TI2V 5B.",
                "position": 2303
            },
            {
                "img": "https://arxiv.org/html/2601.21037/x12.png",
                "caption": "Figure 13:Examples ofTangramPuzzlegenerated by different systems. Red bounding boxes indicates wrong predictions, and green bounding box indicates distortion throughout the process.",
                "position": 2306
            }
        ]
    },
    {
        "header": "Appendix CPrompt",
        "images": []
    }
]