[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/teaser.png",
                "caption": "Figure 1:Pre-trained MLLMs (e.g., Qwen3-Omni) often exhibit suboptimal performance in audio-visual reasoning tasks due to inherent modality bias. To address this limitation, we reinforce the audio-visual reasoning ability by leveraging query intention and modality attention.",
                "position": 131
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/pipeline.png",
                "caption": "Figure 2:The schematic illustration of ourOmniVideo-R1. Based on the dataset collected from data preparation, our training consists of two stages: (1) QI stage establishes query-intensive grounding behavior by aligning multiple timeâ€“caption pairs without process-level annotations. (2) MA stage further performs modality-attentive fusion by optimizing a contrastive modality reward.",
                "position": 198
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/visualization.png",
                "caption": "Figure 3:Visualization of the responses and underlying reasoning process generated byOmniVideo-R1and Qwen3-Omni-30B-A3B-Instruct, -Thinking to an audio-visual understanding question.",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/data_pipeline.png",
                "caption": "Figure 4:Pipeline for our data preparation consisting of 3 stages.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/show1.png",
                "caption": "Figure 5:Visualization of the results obtained from the training of QI, and QI+MA.Redhighlights the incorrect text, whilegreenhighlights the correct text.Yellowhighlights the model overemphasizes one modality while neglecting cues from the other modality.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/show2.png",
                "caption": "Figure 6:Visualization of the results obtained from the training of QI, and QI+MA.Redhighlights the incorrect text, whilegreenhighlights the correct text.",
                "position": 1017
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/data.png",
                "caption": "Figure 7:(a) Our training data covers 16 categories. (b) Number of questions in terms of each category.",
                "position": 1787
            }
        ]
    },
    {
        "header": "Appendix BInstruction Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/prompt4method.png",
                "caption": "Figure 8:System prompt and user prompt suffix forOmniVideo-R1reasoning.",
                "position": 1841
            },
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/prompt4category.png",
                "caption": "Figure 9:Instruction for data categorizing in data preparation.",
                "position": 1851
            },
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/prompt4score.png",
                "caption": "Figure 10:Instruction for data quality assessment in data preparation.",
                "position": 1854
            },
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/prompt4consistency.png",
                "caption": "Figure 11:System prompt and user prompt for consistency judger.",
                "position": 1857
            },
            {
                "img": "https://arxiv.org/html/2602.05847/Figs/prompt4completeness.png",
                "caption": "Figure 12:System prompt and user prompt for completeness evaluator.",
                "position": 1860
            }
        ]
    },
    {
        "header": "Appendix CImplementation details",
        "images": []
    },
    {
        "header": "Appendix DLimitation & Future Work.",
        "images": []
    }
]