[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25413/figures/teaser_a.jpg",
                "caption": "(a)Outputs of advanced VLMs and DepthLM.",
                "position": 99
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/teaser_a.jpg",
                "caption": "(a)Outputs of advanced VLMs and DepthLM.",
                "position": 102
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/teaser_a.jpg",
                "caption": "(a)Outputs of advanced VLMs and DepthLM.",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/teaser_b.jpg",
                "caption": "(b)Point clouds generated by DepthLM.",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/teaser_c_80.jpg",
                "caption": "(c)Accuracyδ1​(↑)\\delta_{1}(\\uparrow)over 4 datasets (Nuscenes, ETH3D, sunRGBD, ibims1).",
                "position": 118
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3DepthLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25413/figures/analysis_pixel_reference_wider.jpg",
                "caption": "Figure 2:Pixel reference.",
                "position": 181
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/grpo_a_1.0_delta.jpg",
                "caption": "(a)Effect of GRPO rewards (8K training samples).",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/grpo_a_1.0_delta.jpg",
                "caption": "(a)Effect of GRPO rewards (8K training samples).",
                "position": 203
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/grpo_b_1.0.jpg",
                "caption": "(b)Accuracy vs training data size.",
                "position": 208
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/analysis_camera_ambiguity_v2_wider.jpg",
                "caption": "(a)Accuracy of different camera ambiguity handling strategies.",
                "position": 235
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/analysis_camera_ambiguity_v2_wider.jpg",
                "caption": "(a)Accuracy of different camera ambiguity handling strategies.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/focal_length_v2.jpg",
                "caption": "(b)Increasingfunif_{\\text{uni}}benefits the performance until 1000 pixels.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/analysis_scaling_v2.jpg",
                "caption": "(c)Accuracy with different number of training samples.",
                "position": 248
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/pipelinev5.jpg",
                "caption": "Figure 5:DepthLM.DepthLM first augment the input image to have a unified focal length. Then, it renders visual markers on the image for pixel reference and uses text to interact with VLMs directly.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2509.25413/figures/multiTask_v6.jpg",
                "caption": "Figure 6:Scaling DepthLM to more complex 3D tasks.",
                "position": 307
            }
        ]
    },
    {
        "header": "4Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25413/figures/visualization_v3.jpg",
                "caption": "Figure 7:Visualization.The scale of DepthLM is close to GT from small indoor scenes to large outdoor scenes. Pure vision models produce smooth points, which is advantageous on non-boundary regions. However, they also have over-smoothing effect on the (marked) boundary regions, leading to flying points between two distinct objects. Interestingly, DepthLM naturally avoids over-smoothing without enforcement during training or any post-processing.",
                "position": 670
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Examples of Rendered Markers and Text-based Pixel Reference",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25413/figures/marker_shape.jpg",
                "caption": "Figure 8:Actual markers rendered by our method.We show here 3 different types of markers used in the experiment of Fig.2.",
                "position": 1284
            }
        ]
    },
    {
        "header": "7Statistics of DepthLMBench",
        "images": []
    },
    {
        "header": "8Cross-Dataset Evaluation for SFT vs GRPO Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.25413/figures/grpo_c_1.0.jpg",
                "caption": "Figure 9:SFT vs GRPO with cross dataset evaluation.We show the result when we train on Argoverse2 and evaluate on NuScenes. The trend is similar as in Fig.3(b).",
                "position": 1333
            }
        ]
    },
    {
        "header": "9GRPO Hyper-parameters",
        "images": []
    },
    {
        "header": "10Prompts for Camera Ambiguity Analysis",
        "images": []
    },
    {
        "header": "11Hyper-parameters for DepthLM Training",
        "images": []
    },
    {
        "header": "12DepthLM Multi-task Performance on Each Dataset",
        "images": []
    }
]