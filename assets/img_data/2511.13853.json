[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13853/x1.png",
                "caption": "Figure 1:A comparison of reasoning approaches for a maze-solving task. Humans visualize the path via mental simulation. A Multimodal Large Language Model (MLLM) uses symbolic reasoning (CoT) to describe the path, e.g., via coordinates. In contrast, a Video Generation Model (VGM) uses generative visual reasoning (CoF) to physically simulate the process, generating frames of the square moving from start to finish.",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13853/x2.png",
                "caption": "Figure 2:Our Gen-ViRe evaluates six core cognitive dimensions: (1) Perceptual, (2) Analogical, (3) Abstract, (4) Planning, (5) Spatial & Temporal, and (6) Algorithmic & Logical, with each dimension comprising four different sub-categories.",
                "position": 94
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Gen-ViRe Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13853/x3.png",
                "caption": "Figure 3:Qualitative examples of Gen-ViRe tasks. It illustrates sample inputs and their expected Chain-of-Frames (CoF) visual reasoning outputs across the six cognitive dimensions, highlighting the benchmark’s breadth from foundational perception to high-order planning.",
                "position": 165
            }
        ]
    },
    {
        "header": "4Data Curation",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13853/x4.png",
                "caption": "Figure 4:The evaluation framework of Gen-ViRe.(a)Data Curation: Shows the benchmark development process, including defining the taxonomy, collecting data from multiple channels (web, existing datasets, AI generation), and designing & validating prompts through Peer Review.(b)Formulation of Evaluation Criteria: Demonstrates the process of formulating detailed, multi-dimensional evaluation criteria (as shown by C1-C5 in the figure) for each prompt of every subtask.(c)VLM-based Autorating Framework: Illustrates how the VLM (Autorater) conducts item-by-item analysis and automatic scoring of the generated videos based on the specific criteria defined in (b).",
                "position": 227
            }
        ]
    },
    {
        "header": "5Evaluation Methodology",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.13853/x5.png",
                "caption": "Figure 5:Left:The main chart compares the overall performance of the 7 state-of-the-art models across the six core cognitive dimensions (Abstract, Algorithmic, Analogy, Perceptual, Planning, and Spatial Reasoning).Right:The six sub-charts provide a detailed performance breakdown for the individual subtasks within each dimension. The legend (bottom) links each colored line to its respective model.",
                "position": 637
            },
            {
                "img": "https://arxiv.org/html/2511.13853/x6.png",
                "caption": "Figure 6:Showcase of Analogical Reasoning by Sora-2 and Veo-3.1.On the complex Rotation task (top row), both Sora-2 and Veo-3.1 failed to recognize and apply the abstract rotational rule. In sharp contrast, both models easily solved the simpler Color analogy (bottom row), which only required matching an attribute.",
                "position": 661
            },
            {
                "img": "https://arxiv.org/html/2511.13853/x7.png",
                "caption": "Figure 7:Showcase of Sora-2’s failures in Spatio-Temporal & Dynamic Reasoning.The top row shows input images; the bottom shows Sora-2’s mid-frames. These frames reveal fundamental failures in simulating basic physical laws: (Left) violating object permanence by showing a dog phasing through a closed glass door ; and (Middle) failing to simulate a continuous process, instead spawning paper towels into the scene ; (Right) depicting telekinesis, where an object is retrieved without contact.",
                "position": 673
            },
            {
                "img": "https://arxiv.org/html/2511.13853/x8.png",
                "caption": "Figure 8:Showcase of Sora-2’s in geometry task. Both Sora-2 and Veo-3.1 failed to identify the existing point D in the static image. This perceptual error caused them to ignore the instruction connect point C to point D’ and instead hallucinate a new point D, connecting C to this incorrect location.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2511.13853/x9.png",
                "caption": "Figure 9:Showcase of Sora-2’s Sudoku task.In the Sudoku task, Sora-2 exhibits an emergent, human-like thinking process. The model uses a question mark (?) as a placeholder for the unknown value in the third row. This suggests it can hold an internal state of the problem (“this cell is unsolved\"). Following the placeholder, the model generates frames that simulate the “moving\" of numbers (2) into their correct, logically-deduced positions.",
                "position": 691
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]