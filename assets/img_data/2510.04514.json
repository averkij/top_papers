[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/logo/logo-2.png",
                "caption": "",
                "position": 179
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x1.png",
                "caption": "Figure 1:Comparison of our work with the existing SoTA.(a)ChartAgentperforms visually grounded reasoning in the chart domain. For this unannotated chart, GPT-4o fails to produce the correct answer, whereasChartAgentsucceeds.(b)ChartAgentperformance on unannotated charts and numeric QA compared with the top-10 SoTA.",
                "position": 238
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x2.png",
                "caption": "Figure 2:ChartAgent.The(A)orchestrator extracts chart metadata and routes annotated charts with textual shortcuts and qualitative QA to the base MLLM, while unannotated charts and numeric queries trigger the ReAct-style loop. The system includes(B)a library of universal and chart-specific tools,(C)metadata for parameterizing tool usage and retrieving chart-type-specific ICL examples, and(D)few-shot ICL retrieval. Using these components as the(E)input,ChartAgentperforms(F)iterative visual reasoning, supported by(G)visual self-verification of intermediate outputs. When tool-based reasoning is unreliable,(H)the agent falls back to the base MLLM.",
                "position": 307
            }
        ]
    },
    {
        "header": "3ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Charts",
        "images": []
    },
    {
        "header": "4Experimental Protocol and Details",
        "images": []
    },
    {
        "header": "5Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x3.png",
                "caption": "Figure 3:(a) Left:ChartAgentvs. concurrent works:overall accuracy (↑\\uparrow) and average absolute error (↓\\downarrow).(b) Right: Effectiveness of visual self-verification:enabled 70% successful recoveries when invoked.",
                "position": 960
            },
            {
                "img": "https://arxiv.org/html/2510.04514/x4.png",
                "caption": "Figure 4:Analysis ofChartAgentPerformance.(a) Left:Stratified by visual complexity of charts and reasoning complexity of chart–QA pairs on unannotated charts, compared with top-10 SoTA.(b) Middle:ChartAgentperformance on unannotated+numeric chartQA when instantiated with different base MLLMs.(c) Right:Ablation study comparingChartAgentwith ReAct using no tools and ReAct with natural image–based generic tools.",
                "position": 1367
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations and Broader Perspective",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "Disclaimer",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Table of Contents",
        "images": []
    },
    {
        "header": "Appendix AAnnotated vs. Unannotated Charts",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x5.png",
                "caption": "Figure 5:Examples of annotated (top) vs. unannotated (bottom) charts.Anannotated chartcontains explicit textual annotations or shortcuts, whereas anunannotated chartlacks such explicit value indicators. For instance, in the first column (top), the bar chart includes printed bar values, while in the corresponding bottom chart, the values must be inferred through visual interpretation.",
                "position": 2657
            }
        ]
    },
    {
        "header": "Appendix BRelated Work",
        "images": []
    },
    {
        "header": "Appendix CDatasets",
        "images": []
    },
    {
        "header": "Appendix DChart Types Supported inChartAgent",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x6.png",
                "caption": "Figure 6:Chart types in the ChartX dataset: 18 types organized into three subcategories—general, fine-grained, and domain-specific chart types, with the percentage of data in each subcategory indicated. Over 60% of the data is unannotated, making ChartX a robust testbed for visual reasoning in charts.",
                "position": 2908
            },
            {
                "img": "https://arxiv.org/html/2510.04514/x7.png",
                "caption": "Figure 7:Chart types in the ChartBench dataset: 9 major types with 38 subtypes (excluding 4 subtypes with corrupted or incorrect ground-truth labels). Annotated subtypes are marked ingreen, and unannotated subtypes are marked inred. Over 75% of the data is unannotated, making ChartBench a robust testbed for visual reasoning in charts.",
                "position": 2911
            }
        ]
    },
    {
        "header": "Appendix EBaselines",
        "images": []
    },
    {
        "header": "Appendix FTaxonomy of Tools inChartAgent",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x8.png",
                "caption": "Figure 8:Illustrative examples of key intermediate and final output visualizations foruniversal toolsinChartAgent.These visualizations are critical to facilitating visual self-verification inChartAgent. Such tool observations enableChartAgentto perceptually assess the outputs and refine its tool usage in the next iteration—either by adjusting tool parameters or invoking a different tool if the intermediate results indicate incorrect or unexpected behavior. Note the diverse variations that our tools are capable of handling robustly.",
                "position": 3438
            },
            {
                "img": "https://arxiv.org/html/2510.04514/x9.png",
                "caption": "",
                "position": 3447
            },
            {
                "img": "https://arxiv.org/html/2510.04514/x10.png",
                "caption": "Figure 9:Illustrative examples of key intermediate and final output visualizations forchart-specific toolsinChartAgent.These visualizations enable visual self-verification inChartAgent, allowing it to refine tool usage through perceptual assessment and iterative correction. We intentionally present some easier examples here for illustration, to help readers quickly follow the process. However,ChartAgenttools are capable of handling a wide range of cases, including more difficult and complex ones, as demonstrated by the overall results.",
                "position": 3449
            }
        ]
    },
    {
        "header": "Appendix GImplementation Details",
        "images": []
    },
    {
        "header": "Appendix HExamples of Response Standardization for Accuracy Evaluation",
        "images": []
    },
    {
        "header": "Appendix IComplexity Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x11.png",
                "caption": "Figure 10:Complexity dimensions in chart–QA pairs.Representative examples are shown for (a)visual complexityof charts and (b)reasoning complexityof chart–QA pairs, each categorized intoEasy,Medium, andHardlevels.(a)Forvisual complexity: Easy charts (e.g., single bar or line plots) have few elements and clean layouts; Medium charts (e.g., multi-series line or stacked bar plots) add moderate overlap; Hard charts (e.g., radar charts, 3D plots, or heavily layered visuals) are highly cluttered.(b)Forreasoning complexity: Easy chart–QA pairs involve direct lookup; Medium pairs require comparisons or proportions; Hard pairs need complex multi-step reasoning.",
                "position": 3678
            }
        ]
    },
    {
        "header": "Appendix JExpanded Discussion on Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x12.png",
                "caption": "Figure 11:Tool-use statistics across benchmark datasets.Percentage of timesChartAgentemploys a given tool when solving queries for each chart type. As expected, universal tools are used broadly across all chart types, whereas chart-specific tools are invoked selectively depending on the chart type detected by theChartAgentorchestrator.",
                "position": 5442
            }
        ]
    },
    {
        "header": "Appendix KDetails on Failure Mode Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.04514/x13.png",
                "caption": "(a)Illustrations of common failure modes inChartAgent.",
                "position": 7057
            },
            {
                "img": "https://arxiv.org/html/2510.04514/x13.png",
                "caption": "(a)Illustrations of common failure modes inChartAgent.",
                "position": 7060
            },
            {
                "img": "https://arxiv.org/html/2510.04514/x14.png",
                "caption": "(b)Qualitative failure cases whereChartAgentproduces incorrect responses.",
                "position": 7066
            },
            {
                "img": "https://arxiv.org/html/2510.04514/figures/prompt/metadata-prompt.png",
                "caption": "",
                "position": 8186
            }
        ]
    },
    {
        "header": "Appendix LPrompts",
        "images": []
    }
]