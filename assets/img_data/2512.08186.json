[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08186/x1.png",
                "caption": "Figure 1:The proposed dual-system framework decouples high-level reasoning from low-level control. System 2 (slow, 2 Hz) uses a 7B pretrained VLM to generate pixel goal and latent goal, while System 1 (fast, 30 Hz) is a lightweight diffusion-based policy that converts the goals into smooth trajectories with high-frequency RGB inputs.\nThe asynchronous inference enables continuous and smooth navigation process. DualVLN sets a new state-of-the-art on VLN-CE and VLN-PE, and shows strong generalization in real-world deployments.",
                "position": 109
            }
        ]
    },
    {
        "header": "1introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08186/x2.png",
                "caption": "Figure 2:Overview of DualVLN. System 2 takes as input a sequence of egocentric images and the instruction to predict either view-adjustment actions or a 2D pixel coordinate within the image for the next navigation waypoint. System 1 then takes as input both the latent goal embeddings and high-frequency RGB inputs, then generates continuous trajectories for the robot to follow through a diffusion-based policy.",
                "position": 157
            }
        ]
    },
    {
        "header": "4Social Vision-and-Language Navigation Benchmark.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08186/x3.png",
                "caption": "Figure 3:Typical robot-humanoid interactions that pose key challenges to the robot’s human-aware obstacle avoidance capabilities, including not only situations with a single agent but also cases involving multiple humanoids simultaneously.",
                "position": 247
            }
        ]
    },
    {
        "header": "5Experiments.",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08186/x4.png",
                "caption": "Figure 4:Qualitative Results of Social-VLN Experiments.",
                "position": 862
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x5.png",
                "caption": "Figure 5:Evaluation Metrics of Real-World Experiments.",
                "position": 880
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x6.png",
                "caption": "Figure 6:Real-World Performance Analysis of VLM-based methods.",
                "position": 883
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x7.png",
                "caption": "Figure 7:Ablation Study on the role of different goal representations in conditioning System-1.w/o Sys.2 Trainmeans training System 1 & 2 jointlyin one-stagewithout explicit intermediate pixel goals.w/o Pixel Goalmeans removing the pixel-goal text before appending the latent queries.w/o Latent Goalmeans using the frozen VLM hidden states of the generated pixel goal.",
                "position": 901
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x8.png",
                "caption": "Figure 8:System 1 is robust to pixel-goal regression errors that still indicates the correct direction but may place the goal near or on an obstacle. But this robustness does not extend to large or semantically incorrect pixel goals especially when the agent is close to the obstacles.",
                "position": 982
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x9.png",
                "caption": "Figure 9:Data Scaling Results of Sys 1.",
                "position": 990
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x10.png",
                "caption": "Figure 10:Correlation between Predicted Pixel Goal and Trajectory",
                "position": 1001
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Contributions and Acknowledgments",
        "images": []
    },
    {
        "header": "Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AData Preparation and Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08186/x11.png",
                "caption": "Figure 11:Visualization of attention maps when predicting the pixel goal.",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x12.png",
                "caption": "",
                "position": 1197
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x13.png",
                "caption": "",
                "position": 1199
            },
            {
                "img": "https://arxiv.org/html/2512.08186/x14.png",
                "caption": "",
                "position": 1201
            }
        ]
    },
    {
        "header": "Appendix BAttention Map Analysis for Pixel-Goal Grounding",
        "images": []
    }
]