[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09616/x1.png",
                "caption": "Figure 1:CoT Reasoning, with dense prefilling and lengthy decoding, incurs substatial computation load at both training and inference. In contrast,Concise Reasoningcoupled with token compression is significantly more efficient, thanks to sparse prefilling and concise decoding.",
                "position": 76
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmarks and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09616/x2.png",
                "caption": "Figure 2:Overview of CoT models. After pre-training, they are typically post-trained via SFT stage using CoT annotations and RL stage. For both training and inference, the models suffer from heavy prefilling with dense visual tokens, and lengthy decoding due to human-like thinking generation.",
                "position": 138
            },
            {
                "img": "https://arxiv.org/html/2512.09616/x3.png",
                "caption": "Figure 3:Statistics of training and inference overhead.\n(a) Training overhead shows the training runtime of a CoT model (i.e., Video-R1[12]), which is measured via four A800-SXM4-80GB GPUs.\n(b) Inference overhead reports the inference statistics (i.e., decoding length and inference runtime) of different reason modes, which is measured through a single A800-SXM4-80GB GPU.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2512.09616/x4.png",
                "caption": "Figure 4:Framework of our method. (1) Typical CoT models are trained via three stages and perform long reasoning during inference. (2) In comparison, our method does not require the stage of supervised fine-tuning and the annotations at this stage, and generates concise reasoning during inference. (3) We further reduce computation overhead by trainable token compression.",
                "position": 293
            }
        ]
    },
    {
        "header": "4Methods",
        "images": []
    },
    {
        "header": "5Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.09616/x5.png",
                "caption": "Figure 5:Visualization of generated textfrom CoT reasoning (Video-R1) and concise reasoning (Ours). Partial text chunks are colored through human validation.Green: ground-truth or correct predicted answers.Blue: correct intermediate reasoning steps.Purple: unnecessary intermediate reasoning steps.Red: incorrect intermediate reasoning steps or final predictions.",
                "position": 903
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]