[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05573/x1.png",
                "caption": "Figure 1:Overview of Orient Anything V2. We upgrade the foundation orientation estimation model from bothDataandModelperspectives. It unifies the understanding of object orientation and rotation, achieving better estimation accuracy and gaining theNew Featuresto handle rotational symmetry and relative rotation. Zoom in for the best view.",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05573/x2.png",
                "caption": "Figure 2:Real assets from Objavese suffer from (a) low-quality texture and (b) limited realism.",
                "position": 199
            }
        ]
    },
    {
        "header": "3Revisiting Orient Anything V1",
        "images": []
    },
    {
        "header": "4Scalable Data Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05573/x3.png",
                "caption": "Figure 3:Overview of 3D Asset Synthesis Pipeline. We begin with class tags and use a series of advanced generative models to progressively generate high-quality 3D assets.",
                "position": 243
            },
            {
                "img": "https://arxiv.org/html/2601.05573/x4.png",
                "caption": "Figure 4:Overview of Robust Annotation Pipeline. \"Pseudo Label\" visualizes the azimuth direction of pseudo labels and objects in the horizontal plane. By fitting the pseudo labels to standard periodic distribution, we can robustly derive the orientation and symmetry label. Human calibration is only required for categories with symmetry inconsistencies.",
                "position": 282
            }
        ]
    },
    {
        "header": "5Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05573/x5.png",
                "caption": "Figure 5:Framework of Orient Anything V2. One or two input frames are tokenized by DINOv2 and then jointly encoded using transformer blocks. We finally employ MLP heads to predict the orientation or rotation distributions from the encoded learnable tokens of each frame.",
                "position": 316
            }
        ]
    },
    {
        "header": "6Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05573/x6.png",
                "caption": "Figure 6:Visualization of synthetic 3D assets and robust annotation.",
                "position": 649
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.05573/ref0.jpg",
                "caption": "Figure 7:Relative pose rotation estimation for images in the wild.",
                "position": 1458
            },
            {
                "img": "https://arxiv.org/html/2601.05573/no0.jpg",
                "caption": "Figure 8:Orientation estimation and Rotational symmetry recognition results on objects has no front direction.",
                "position": 1461
            },
            {
                "img": "https://arxiv.org/html/2601.05573/one0.jpg",
                "caption": "Figure 9:Orientation estimation and Rotational symmetry recognition results on objects has one front direction. Part 1.",
                "position": 1464
            },
            {
                "img": "https://arxiv.org/html/2601.05573/one1.jpg",
                "caption": "Figure 10:Orientation estimation and Rotational symmetry recognition results on objects has one front direction. Part 2.",
                "position": 1467
            },
            {
                "img": "https://arxiv.org/html/2601.05573/two0.jpg",
                "caption": "Figure 11:Orientation estimation and Rotational symmetry recognition results on objects has two front direction. Part 1.",
                "position": 1470
            },
            {
                "img": "https://arxiv.org/html/2601.05573/two1.jpg",
                "caption": "Figure 12:Orientation estimation and Rotational symmetry recognition results on objects has two front direction. Part 2.",
                "position": 1473
            },
            {
                "img": "https://arxiv.org/html/2601.05573/four0.jpg",
                "caption": "Figure 13:Orientation estimation and Rotational symmetry recognition results on objects has four front direction.",
                "position": 1476
            }
        ]
    },
    {
        "header": "Appendix AMore Visualizations of Images in The Wild",
        "images": []
    }
]