[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14988/x1.png",
                "caption": "Figure 1:Overview of the DMOSpeech 2 framework.(a) Left:The original DMOSpeech architecture, where the duration predictor (ùí´œïsubscriptùí´italic-œï\\mathcal{P}_{\\phi}caligraphic_P start_POSTSUBSCRIPT italic_œï end_POSTSUBSCRIPT) is trained self-supervisedly and separate from the TTS component, creating a disconnection that prevents end-to-end optimization.(b) Right:Our proposed DMOSpeech 2 framework, which employs Group Relative Policy Optimization (GRPO) to train the duration predictor with reinforcement learning (Algorithm1), using speaker similarity and word error rate as reward signals, enabling end-to-end optimization of the entire TTS pipeline.",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14988/x2.png",
                "caption": "Figure 2:Illustration of teacher-guided sampling (Algorithm2). The process begins with noise and uses the teacher modelGŒòsubscriptùê∫ŒòG_{\\Theta}italic_G start_POSTSUBSCRIPT roman_Œò end_POSTSUBSCRIPTfor early denoising steps (gray circles) to establish prosodic structure up to a transition pointtk‚àósubscriptùë°superscriptùëòt_{k^{*}}italic_t start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. Then, the student modelGŒ∏subscriptùê∫ùúÉG_{\\theta}italic_G start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT(blue circles) takes over for the remaining steps to refine acoustic details in much fewer steps.",
                "position": 152
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14988/x3.png",
                "caption": "Figure 4:Performance dynamics during RL training of the duration predictor with various group sizes. The left plot shows speaker similarity (SIM) while the right plot shows word error rate (WER). The vertical dashed line indicates the 1.5K steps we selected for our final model.",
                "position": 1656
            }
        ]
    },
    {
        "header": "Appendix BDMOSpeech Technical Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.14988/extracted/6638051/Image20250412043210.jpg",
                "caption": "Figure 5:Screenshot of the comparative subjective evaluation interface. The interface presents three audio samples: a reference recording at the top, and two synthesized speech samples for comparison below. Participants are asked to make direct comparisons between the two synthesized samples by selecting which one sounds more natural and which one is more similar to the reference voice.",
                "position": 2086
            }
        ]
    },
    {
        "header": "Appendix CSubjective Evaluation",
        "images": []
    }
]