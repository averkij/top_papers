[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18369/x1.png",
                "caption": "Figure 1:Visualizations of personalized image captioning results. In the first row, the zero-shot MLLM frequently fails to generate personalized captions. The used images are sourced from Yo’LLava[32]. The remaining rows illustrate multi-concept scenarios at inference time. Compared to other SFT-based methods, our approach consistently produces faithful and detailed captions while accurately recognizing all provided identities, even for 3 or 4 concepts. All images are sourced from MuDI[19].",
                "position": 120
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18369/x2.png",
                "caption": "Figure 2:Overview of our RePIC framework: (a) training phase and (b) inference phase. An abbreviated example of the prompt template is shown; complete templates are provided in the Appendix.",
                "position": 172
            }
        ]
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18369/x3.png",
                "caption": "Figure 3:Visualization of preference evaluation scores for single and 2-concept settings, corresponding to the first and second rows, respectively. In (a), our model outperforms all other baseline models, while in (b), it surpasses all ablation variants.",
                "position": 602
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x4.png",
                "caption": "Figure 4:Qualitative examples of 2-concept personalized image captioning.",
                "position": 605
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x5.png",
                "caption": "Figure 5:Visualization of training stability.",
                "position": 842
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Qualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18369/x6.png",
                "caption": "Figure A.1:Visualization of a more complex and diverse multi-concept image captioning result using our proposed RePIC. The generated query image is sourced from InstantFamily[20].",
                "position": 1557
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x7.png",
                "caption": "Figure A.2:Visualization of RePIC results on various personalized image captioning tasks.",
                "position": 1560
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x8.png",
                "caption": "Figure A.3:Examples of generated captions on single and 2-concept personalized image captioning tasks.",
                "position": 1563
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x9.png",
                "caption": "Figure A.4:Visualization of qualitative results for additional components used for our methods.",
                "position": 1566
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x10.png",
                "caption": "Figure A.5:Visualizations of output responses with and without the use of reasoning templates.",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x11.png",
                "caption": "Figure A.6:Examples illustrating additional limitations of RePIC in 2-concept scenario.",
                "position": 1607
            }
        ]
    },
    {
        "header": "Appendix BAdditional Experimental Configurations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18369/x12.png",
                "caption": "Figure A.7:Datasets used for training and evaluation. Note that the Subject200K+ dataset (a) was used for training, while all real datasets (b) to (f) were used only for evaluation.",
                "position": 1628
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x13.png",
                "caption": "Figure A.8:Visualization of the DreamBooth database constructed in this work.",
                "position": 1631
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x14.png",
                "caption": "Figure A.9:Visualization of data templates used for MLLM post-training, including examples of OCT, VLT, single-ICT, and multi-ICT.",
                "position": 1641
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x15.png",
                "caption": "Figure A.10:(a) Dataset composition, (b) instruction composition, and (c) the sensitivity to the proportion of identity grounding instructions within the overall training set.",
                "position": 1644
            }
        ]
    },
    {
        "header": "Appendix CAdditional Analyses",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.18369/x16.png",
                "caption": "Figure A.11:Distributions of mean verifiable rewards during training for each task: (a) OCT, (b–c) ICT, and (d) VLT.",
                "position": 1688
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x17.png",
                "caption": "Figure A.12:Ablation studies on output length distributions of image captioning across single and multi-concept evaluation datasets.",
                "position": 1704
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x18.png",
                "caption": "Figure A.13:Visualization of results: (a) measured output response length (e.g.between<answer>and</answer>tokens), (b) output length measured within the reasoning template (e.g.between<think>and</think>tokens), and (c) ablation studies.",
                "position": 1714
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x19.png",
                "caption": "Figure A.14:Visualization of image captioning results for general query prompts.",
                "position": 1717
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x20.png",
                "caption": "Figure A.15:Visualization of image captioning results for detail query prompts.",
                "position": 1720
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x21.png",
                "caption": "Figure A.16:Quantitative results of preference evaluations for the single-image captioning task without reference images, using (a) general query prompts and (b) detailed query prompts. Note that RePIC outperforms the zero-shot model in (a), and achieves comparable results in (b).",
                "position": 1736
            },
            {
                "img": "https://arxiv.org/html/2506.18369/x22.png",
                "caption": "Figure A.17:Visualization of KL-divergence and accuracy reward plots on the seen data.",
                "position": 1739
            }
        ]
    },
    {
        "header": "Appendix DUsed Templates",
        "images": []
    }
]