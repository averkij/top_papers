[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01106/x1.png",
                "caption": "Figure 1:Overview of the LoCAL pipeline. The multi-page document and query are encoded by a customized LMM (yellow). The most relevant page is retrieved through similarity-based matching, and a fine-tuned LMM (blue) generates the final answer from the evidence.",
                "position": 183
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LoCAL Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01106/x2.png",
                "caption": "Figure 2:Model overview of LoCAL. It contains two modules, which are finetuned using LoRA(Hu etÂ al.,2021), sharing thesamepretrained LLM backbone. The retrieval module selects evidence pages for the other QA module, which provides responses to user questions.",
                "position": 248
            }
        ]
    },
    {
        "header": "4LoCAL-Bench",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01106/x3.png",
                "caption": "Figure 3:Distribution of document types (left) and average document lengths in each types (right).",
                "position": 378
            },
            {
                "img": "https://arxiv.org/html/2411.01106/x4.png",
                "caption": "",
                "position": 387
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01106/x5.png",
                "caption": "Figure 4:Top-1 retrieval accuracy on MMLongBench-Doc using different hidden states across all layers of Phi-3-Vision.",
                "position": 1447
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExample of training pairs for retrieval module",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01106/x6.png",
                "caption": "Figure A.1:Example of training pairs within a batch (batch size: 4) for contrastive training, using samples from the SlideVQA dataset.",
                "position": 2531
            }
        ]
    },
    {
        "header": "Appendix BEvaluation Metrics",
        "images": []
    },
    {
        "header": "Appendix CExample of Inference Failure Scenario",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01106/x7.png",
                "caption": "Figure C.1:Inference example of a challenging case in the SlideVQA dataset. LoCAL-Paligemma retrieved the wrong evidence page due to limitations in its retrieval module, leading to an incorrect answer. LoCAL-Phi-3-V retrieved the correct page but provided a wrong answer due to limitations in its QA module. Meanwhile, LoCAL-InternVL2-4B also assigned the highest relevance score to an incorrect page. However, since it processes multiple pages (top 5), the correct evidence page was included in the input, allowing its fine-tuned QA module to deliver the correct answer.",
                "position": 2596
            },
            {
                "img": "https://arxiv.org/html/2411.01106/x8.png",
                "caption": "Figure C.2:Failure cases from the SlideVQA dataset, highlighting retrieval module errors. In the first two examples, some of the relevant information (highlighted in red boxes) on the true evidence pages is difficult even for human eyes to detect. In the third example, the retrieved page has a high similarity to the true evidence page, making it challenging to rank correctly. Additionally, answering the question accurately requires a deep understanding of the concept of chemical damage and related topics.",
                "position": 2603
            },
            {
                "img": "https://arxiv.org/html/2411.01106/x9.png",
                "caption": "Figure D.1:Question answering examples on the SlideVQA dataset using different QA modules. Models without fine-tuning, such as Phi-3-V and InternVL2-4B, tend to produce verbose and error-prone responses. However, in the second example, fine-tuning with the LoRA adapter significantly improves the accuracy of Phi-3-V and InternVL2-4B.",
                "position": 2612
            }
        ]
    },
    {
        "header": "Appendix DQaulitative Results in Question-Answering",
        "images": []
    }
]