[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13918/x1.png",
                "caption": "Figure 1:Overview of Our Video Alignment Paradigm.(a) Human Preference Annotation(Sec.3.1). We construct a dataset of 182k(prompt, video A, video B)triplets, collecting preference annotations on Visual Quality¬†(VQ), Motion Quality¬†(MQ), and Text Alignment¬†(TA) from human evaluators.(b) Reward Mode Training(Sec.3.2). We train a VLM-based reward model using the Bradley-Terry-Model-with-Ties formulation.(c) Video Alignment(Sec.4). We adapt alignment techniques ‚Äî DPO, RWR, and reward guidance ‚Äî to flow-based video generation models and provide a comprehensive comparison of their effectiveness.",
                "position": 232
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3VideoReward",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13918/x2.png",
                "caption": "Figure 2:Statistics of our training data.",
                "position": 470
            },
            {
                "img": "https://arxiv.org/html/2501.13918/x3.png",
                "caption": "Figure 3:Accuracy comparison between the BT and regression reward models across varying training data fractions (log scale).",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2501.13918/x4.png",
                "caption": "Figure 4:Visualization of theŒî‚Å¢rŒîùëü\\Delta rroman_Œî italic_rdistribution forthe BT reward model¬†(Left)andthe BTT reward model¬†(Right). The BTT model effectively distinguishes tie pairs from chosen/rejected pairs.",
                "position": 628
            }
        ]
    },
    {
        "header": "4Video Alignment",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13918/x5.png",
                "caption": "Figure 5:Visual comparison of videos generated by the original pretrained model and the Flow-DPO aligned model.",
                "position": 1265
            },
            {
                "img": "https://arxiv.org/html/2501.13918/x6.png",
                "caption": "Figure 6:Human evaluation of Flow-DPO aligned modelvs.pretrained model on VideoGen-Eval, which contains 400 prompts.",
                "position": 1283
            },
            {
                "img": "https://arxiv.org/html/2501.13918/x7.png",
                "caption": "Figure 7:Accuracy of time-dependentŒ≤tsubscriptùõΩùë°\\beta_{t}italic_Œ≤ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPTvs. constantŒ≤ùõΩ\\betaitalic_Œ≤for TA: Flow-DPO with a constantŒ≤ùõΩ\\betaitalic_Œ≤consistently outperforms the timestep-dependentŒ≤ùõΩ\\betaitalic_Œ≤across various settings.",
                "position": 1413
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of the Derivation",
        "images": []
    },
    {
        "header": "Appendix BInput Template for Reward Model",
        "images": []
    },
    {
        "header": "Appendix CDetails of Human Annotation",
        "images": []
    },
    {
        "header": "Appendix DDetails of Reward Model Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13918/x8.png",
                "caption": "Figure 8:Video Duration and Resolution in GenAI-Bench and VideoGen-Reward Bench",
                "position": 3241
            },
            {
                "img": "https://arxiv.org/html/2501.13918/x9.png",
                "caption": "Figure 9:The model coverage across the training sets of different baselines and the two evaluation benchmarks. VideoScore, VisionReward, and GenAI-Bench primarily focus on pre-SoRA-era models, while our training set and VideoGen-RewardBench concentrate on state-of-the-art T2V models.",
                "position": 3311
            }
        ]
    },
    {
        "header": "Appendix EPseudo-code of Flow-DPO and Flow-NRG",
        "images": []
    },
    {
        "header": "Appendix FHyperparameters",
        "images": []
    },
    {
        "header": "Appendix GExtended Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.13918/x10.png",
                "caption": "Figure 10:Human evaluation of Flow-DPO on TA-Hard prompt.",
                "position": 3709
            }
        ]
    },
    {
        "header": "Appendix HPrompt Subset of TA-Hard",
        "images": []
    }
]