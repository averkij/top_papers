[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x1.png",
                "caption": "Figure 1:Conventional data augmentations (DA) in semantic segmentation are incompatible with referring image segmentation. Random crop and horizontal flip could change the referred object (e.g.,“lady under the red umbrella on left”) to another one, and color distortion could make the described object disappear.",
                "position": 77
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x2.png",
                "caption": "(a)Performance with conventional augmentations",
                "position": 114
            },
            {
                "img": "https://arxiv.org/html/2411.19067/x2.png",
                "caption": "(a)Performance with conventional augmentations",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2411.19067/x3.png",
                "caption": "(b)Performance with individual augmentation on CARIS[33]",
                "position": 123
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x4.png",
                "caption": "Figure 3:The existing RIS method tends to be inaccurate when faced with occluded context. CARIS[33]represents the SoTA method in RIS. Words highlighted inredrepresent occluded objects in the image (left and center) and masked words in the text query (right).",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2411.19067/x5.png",
                "caption": "Figure 4:The overall framework of MaskRIS. Both image and text masking are employed to generate diverse image-text training pairs (Sec.3.2). To maximize the benefits of the masking strategy, Distortion-aware Contextual Learning (DCL) is introduced (Sec.3.3).",
                "position": 222
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x6.png",
                "caption": "Figure 5:Qualitative examples of segmentation results on the RefCOCO dataset.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2411.19067/x7.png",
                "caption": "Figure 6:Robustness on various image corruptions provided by ImageNet-C[12]and linguistically complex situations. The results (oIoU) are evaluated on the RefCOCO validation set.",
                "position": 564
            },
            {
                "img": "https://arxiv.org/html/2411.19067/x8.png",
                "caption": "Figure 7:Qualitative examples of masking strategies on the RefCOCO dataset. I-Mask, T-Mask, and Both denote the results of applying image masking, text masking, and both, respectively.",
                "position": 867
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contents",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x9.png",
                "caption": "Figure A:Various types of mask sampling strategies. We use patch-wise sampling[11]as our default. For block-wise sampling, we follow BEiT[2]to remove large random blocks. For Cutout[8], we follow the implementation of timm[51]. For all strategies except Cutout, we maintain a consistent masking ratio of 75%.",
                "position": 1788
            }
        ]
    },
    {
        "header": "Appendix AAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x10.png",
                "caption": "Figure B:Training loss analysis of MaskRIS on RefCOCO. In the CARIS w/ Masking setting, we employ both image and text masking as data augmentation strategies within the CARIS[33]framework. We visualize (a) the training loss for the original (i.e., unmasked) inputs, (b) the training loss for the masked inputs, and (c) the performance of the model on the RefCOCO validation set.",
                "position": 1831
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.19067/x11.png",
                "caption": "Figure C:Qualitative examples under various occluded contexts on the RefCOCO dataset. Although CARIS[33]tends to be inaccurate under occluded contexts, MaskRIS produces accurate predictions, demonstrating its robustness to occlusion and incomplete information. For text context masking, the word highlighted inredis masked.",
                "position": 1990
            },
            {
                "img": "https://arxiv.org/html/2411.19067/x12.png",
                "caption": "Figure D:More qualitative examples on the RefCOCO dataset. MaskRIS mitigates the limitations of CARIS[33]by (a) capturing target objects more precisely, (b) being robust to occlusions, and (c) effectively using various textual clues. (d) We also provide failure cases.",
                "position": 1993
            }
        ]
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    }
]