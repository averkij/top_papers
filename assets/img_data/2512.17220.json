[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17220/x1.png",
                "caption": "Figure 1:Average model ranks across five long-context benchmarks under 3/5/10-chunk settings.",
                "position": 220
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experimental Setting",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17220/x2.png",
                "caption": "Figure 2:Impact of retriever scale on average results across 5 benchmarks (DetectiveQA-ZH/EN,∞\\inftyBench, NoCha, NarrativeQA) with a Qwen2.5-72B generator.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2512.17220/x3.png",
                "caption": "Figure 3:Impact of generator model scale on average results over 5 benchmarks, with a MiA-Emb-8B retriever.",
                "position": 1245
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17220/x4.png",
                "caption": "Figure 4:Comparison of projection angles for MiA-Emb and Qwen3-Emb. Lower angles indicate better alignment of queries with the book’s semantic subspace.",
                "position": 1336
            },
            {
                "img": "https://arxiv.org/html/2512.17220/x5.png",
                "caption": "Figure 5:Layer-wise comparison of silver-chunk retrieval accuracy and attention allocation proportion.",
                "position": 1353
            },
            {
                "img": "https://arxiv.org/html/2512.17220/x6.png",
                "caption": "Figure 6:Attention pattern of MiA-Emb: the last token attends to preceding summary tokens, with red regions indicating tokens that receive high attention.",
                "position": 1383
            },
            {
                "img": "https://arxiv.org/html/2512.17220/x7.png",
                "caption": "Figure 7:Layer-wise Mindscape-Coherent Evidence Alignment (MCEA) scores for generator.",
                "position": 1398
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASupports of Mindscape-Aware Capabilities in Broader Research Fields",
        "images": []
    },
    {
        "header": "Appendix BMiA-Emb: Supervision and Training Objective",
        "images": []
    },
    {
        "header": "Appendix CImplementation Details",
        "images": []
    },
    {
        "header": "Appendix DAdditional Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.17220/x8.png",
                "caption": "Figure 8:Impact of retriever scale on retrieval performance (Recall@K) on DetectiveQA and NarrativeQA. DetectiveQA scores are averaged over its ZH and EN subsets.SFT-Embdenotes the baseline trained with the identical supervision as MiA-Emb but without access to mindscape summaries.",
                "position": 2562
            },
            {
                "img": "https://arxiv.org/html/2512.17220/x9.png",
                "caption": "Figure 9:Scaling results forMiA-Genversus the vanilla Qwen2.5-Instruct baseline.",
                "position": 2768
            }
        ]
    },
    {
        "header": "Appendix EDefinition of MCEA Metric",
        "images": []
    },
    {
        "header": "Appendix FPrompt Templates for MiA-RAG",
        "images": []
    }
]