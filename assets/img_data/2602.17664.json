[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17664/x1.png",
                "caption": "Figure 1:Illustration of attention sink behaviors in Diffusion Language Models. OursSink-Aware Pruningreduces sink variance by downscaling unstable sinks.",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2602.17664/x2.png",
                "caption": "Figure 2:Attention sink heatmap dynamics across generation steps for AR LLM (LLaMA-3-8B) and DLM (LLaDA). For each model,\nwe show 3 different generation stages (25, 50, and 75% of the total generation steps) and plot the attention mass received by each token position (y-axis) across all heads/layers (x-axis).\nIn LLaMA, the sink position (deep-blue vertical band) is stable across steps, while in LLaDA, the sink position shifts significantly across diffusion steps, indicating higher sink variance. The step in AR model refers to the generation process.",
                "position": 121
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17664/x3.png",
                "caption": "Figure 3:Overview of Sink-Aware Pruning. Given input activations, we compute per-token attention mass aggregated across all layers and heads (Step 1), identify sink tokens via a threshold-based criterion, and derive a soft down-weighting factorω=1−s\\omega=1-s. The original activationXXis then suppressed at sink positions to produce a new activationX~\\tilde{X}(Step 2), which is substituted into existing pruning criteria, Wanda or SparseGPT, to compute sink-aware importance scores (Step 3). Final pruning decisions are made based on the updated scores (Step 4).",
                "position": 183
            }
        ]
    },
    {
        "header": "3Sink-Aware Pruningfor DLMs",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17664/x4.png",
                "caption": "(a)Variance of the sink across generation/denoising steps.",
                "position": 298
            },
            {
                "img": "https://arxiv.org/html/2602.17664/x4.png",
                "caption": "(a)Variance of the sink across generation/denoising steps.",
                "position": 301
            },
            {
                "img": "https://arxiv.org/html/2602.17664/x5.png",
                "caption": "(b)Variance of total attention received by each token across all the generation/denoising steps.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2602.17664/x6.png",
                "caption": "Figure 5:Sink position across generation/denoising steps for diffusion and AR LMs. Shaded regions denote±\\pmstd across runs.",
                "position": 313
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.17664/x7.png",
                "caption": "Figure 6:Delta vs. baseline on average accuracy.Average accuracy change (Δ\\DeltaAvg, in percentage points) from applyingSink-Awarepruning on top of each baseline (Wanda/SparseGPT) at different sparsity levels. Bars reportΔ=Avg​(Sink-Aware)−Avg​(baseline)\\Delta=\\text{Avg}(\\texttt{Sink-Aware})-\\text{Avg}(\\text{baseline}); positive values indicate improved performance retention after pruning. Results are shown for LLaDA, Dream, and LLaDA1.5 over 8 benchmarks.",
                "position": 996
            },
            {
                "img": "https://arxiv.org/html/2602.17664/x8.png",
                "caption": "Figure 7:Sink strength and per-head mask disagreement (XOR) between baseline and sink-aware pruning.The left subfigure shows the sink strength of the original LLaDA-8B model, measured as the average attention each head allocates to sink tokens. The center and right show the fraction of weights per head whose pruning decision differs (XOR) between the baseline and our method, using Wanda (center) and SparseGPT (right), respectively.",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2602.17664/x9.png",
                "caption": "Figure 8:Per-head sparsity difference between baseline and sink-aware pruning.The left and right subfigures show the signed per-head sparsity difference for Wanda and SparseGPT, respectively. Red indicates that the sink-aware variant prunes more aggressively in a given head, while blue indicates it preserves more weights.",
                "position": 1027
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AResults on Additional Models",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Details",
        "images": []
    },
    {
        "header": "Appendix CModel Architecture Details",
        "images": []
    },
    {
        "header": "Appendix DBenchmark Descriptions",
        "images": []
    }
]