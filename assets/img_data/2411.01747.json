[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x1.png",
                "caption": "",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x2.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x3.png",
                "caption": "Figure 1:Illustration of theDynaSauragent framework. In the first step, the agent receives a list of human-designed actions𝒜usuperscript𝒜𝑢\\mathcal{A}^{u}caligraphic_A start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPTand a taskt𝑡titalic_tas input. It then proposes an actiona𝑎aitalic_a, implemented as a Python snippet. The function is executed by the environment, which internally contains an IPython kernel. Depending on the generated actiona𝑎aitalic_a, the kernel may interact with either the action retriever, to retrieve relevant generated actions in𝒜gsuperscript𝒜𝑔\\mathcal{A}^{g}caligraphic_A start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT; the internet, for information retrieval from the web; or the local operating system for any other tasks. We do not impose any constraints on which entities the agent can interact with, so the list shown in this figure is not exhaustive and is mainly for illustration purposes. After executing the actiona𝑎aitalic_a, the environment returns an observationo𝑜oitalic_oto the agent. The observation can either be the result of executinga𝑎aitalic_aor an error message if the kernel fails to executea𝑎aitalic_a.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Problem Formulation",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x4.png",
                "caption": "Figure 2:Impact of action accumulation on performance over time.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x5.png",
                "caption": "Figure 3:Distribution of error types in tasks where agent A (without action implementation) answers incorrectly, while agent B (with action implementation) answers correctly.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x6.png",
                "caption": "Figure 4:Mean coverage over the validation set as the number of actions increases. The red dashed line marks the point where human-designed actions are added to the action set. Subsequent data points reflect the accumulation of generated actions.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x7.png",
                "caption": "Figure 5:A case study demonstrates the difference in problem-solving flexibility between Agent A (a variant ofDynaSaurwithout action implementation) and Agent B (the proposed agent framework). Both agents begin with the same initial step, but only Agent B, equipped with the ability to implement its own actions, successfully completes the task. Due to space constraints, the first step taken by Agent B is not shown.",
                "position": 474
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x8.png",
                "caption": "Figure 6:Prompt for OpenAI’s o1 to perform qualitative evaluation.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x9.png",
                "caption": "Figure 7:The system prompt of ourDynaSauragent framework.",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x10.png",
                "caption": "Figure 8:A case study demonstrates the difference in problem-solving flexibility between Agent A (a variant ofDynaSaurwithout action implementation) and Agent B (the proposed agent framework).",
                "position": 1468
            }
        ]
    },
    {
        "header": "Appendix BAdditional Case Studies",
        "images": []
    }
]