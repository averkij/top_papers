[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x1.png",
                "caption": "",
                "position": 82
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x2.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x3.png",
                "caption": "Figure 1:Illustration of theDynaSauragent framework. In the first step, the agent receives a list of human-designed actionsğ’œusuperscriptğ’œğ‘¢\\mathcal{A}^{u}caligraphic_A start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPTand a tasktğ‘¡titalic_tas input. It then proposes an actionağ‘aitalic_a, implemented as a Python snippet. The function is executed by the environment, which internally contains an IPython kernel. Depending on the generated actionağ‘aitalic_a, the kernel may interact with either the action retriever, to retrieve relevant generated actions inğ’œgsuperscriptğ’œğ‘”\\mathcal{A}^{g}caligraphic_A start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT; the internet, for information retrieval from the web; or the local operating system for any other tasks. We do not impose any constraints on which entities the agent can interact with, so the list shown in this figure is not exhaustive and is mainly for illustration purposes. After executing the actionağ‘aitalic_a, the environment returns an observationoğ‘œoitalic_oto the agent. The observation can either be the result of executingağ‘aitalic_aor an error message if the kernel fails to executeağ‘aitalic_a.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Problem Formulation",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x4.png",
                "caption": "Figure 2:Impact of action accumulation on performance over time.",
                "position": 447
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x5.png",
                "caption": "Figure 3:Distribution of error types in tasks where agent A (without action implementation) answers incorrectly, while agent B (with action implementation) answers correctly.",
                "position": 457
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x6.png",
                "caption": "Figure 4:Mean coverage over the validation set as the number of actions increases. The red dashed line marks the point where human-designed actions are added to the action set. Subsequent data points reflect the accumulation of generated actions.",
                "position": 471
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x7.png",
                "caption": "Figure 5:A case study demonstrates the difference in problem-solving flexibility between Agent A (a variant ofDynaSaurwithout action implementation) and Agent B (the proposed agent framework). Both agents begin with the same initial step, but only Agent B, equipped with the ability to implement its own actions, successfully completes the task. Due to space constraints, the first step taken by Agent B is not shown.",
                "position": 474
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations",
        "images": []
    },
    {
        "header": "8Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.01747/x8.png",
                "caption": "Figure 6:Prompt for OpenAIâ€™s o1 to perform qualitative evaluation.",
                "position": 1447
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x9.png",
                "caption": "Figure 7:The system prompt of ourDynaSauragent framework.",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2411.01747/x10.png",
                "caption": "Figure 8:A case study demonstrates the difference in problem-solving flexibility between Agent A (a variant ofDynaSaurwithout action implementation) and Agent B (the proposed agent framework).",
                "position": 1468
            }
        ]
    },
    {
        "header": "Appendix BAdditional Case Studies",
        "images": []
    }
]