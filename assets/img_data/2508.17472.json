[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17472/x1.png",
                "caption": "Figure 1:Overview of T2I-ReasonBench.We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of text-to-image (T2I) models. It consists of four dimensions:Idiom Interpretation, Textual Image Design, Entity-Reasoning and Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the reasoning accuracy and image quality. We benchmark various T2I generation models, and provide comprehensive analysis on their performances.",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Benchmark Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17472/x2.png",
                "caption": "Figure 2:Left: Prompt collection process.Middle: Subcategories in the four evaluation dimensions.Right: Prompt Suite Statistics.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2508.17472/x3.png",
                "caption": "",
                "position": 187
            },
            {
                "img": "https://arxiv.org/html/2508.17472/x4.png",
                "caption": "",
                "position": 192
            },
            {
                "img": "https://arxiv.org/html/2508.17472/x5.png",
                "caption": "Figure 3:Word cloud to visualize the word distribution of each dimension in our prompt suite.",
                "position": 220
            }
        ]
    },
    {
        "header": "4Evaluation Metrics",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17472/x6.png",
                "caption": "Figure 4:Evaluation Framework of T2I-ReasonBench.We adopt a two-stage evaluation framework: prompt-specific evaluation question-criterion pairs generation by an LLM, then image analysis and scoring by an MLLM. This figure shows one evaluation example for each dimension.",
                "position": 317
            }
        ]
    },
    {
        "header": "5Evaluation Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17472/x7.png",
                "caption": "Figure 5:Qualitative examples.",
                "position": 1065
            }
        ]
    },
    {
        "header": "6Conclusion and Discussions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AMore details on prompt collection process",
        "images": []
    },
    {
        "header": "Appendix BEvaluation Framework",
        "images": []
    }
]