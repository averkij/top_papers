[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06362/x1.png",
                "caption": "Figure 1:Training and inference stages for Llama-MTSK. (Left) During training, we produce audio-visual tokens via pre-trained encoders, followed by specific-scale compression and projection modules. Then, we feed the concatenated audio-visual tokens at multiple scales to the pre-trained Llama-based LLM, which is adapted through one of the three proposed LoRAapproaches following the Matryoshka Representation Learning principle. (Right) At inference, Llama-MTSK allows us to change on-the-fly the audio-visual compression rates for each input data conditioned on our specific requirements using the same model architecture and weights, enabling high flexibility. Furthermore, only one projector and one LoRA module are activated at inference (in this figure, those associated with the audio and video compression rates equal to3333), guaranteeing model’s scalability in training and no extra cost in inference.andrepresent whether the parameters are trained or kept frozen, respectively.",
                "position": 131
            },
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/fire.png",
                "caption": "",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/ice.png",
                "caption": "",
                "position": 137
            }
        ]
    },
    {
        "header": "2Llama-MTSK",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06362/x2.png",
                "caption": "Figure 2:Our three proposed LoRA Matryoshka (LoRA) approaches.Multi-Scale(MS) LoRAuses a shared global LoRA module for all the audio-visual token scales (in this specific example there are three scales) to fine-tune the pre-trained matrices of the LLM. TheSpecific-Scale(SS) variant defines a LoRA module tailored to each scale, learning and specializing to a specific scale. The third approach,Multi-Specific-Scale(MSS), combinesMSandSSto support both global and specific-scale LoRAs. The global LoRA is responsible to capture relationships that can be shared among different-scale tokens, while specific-scale LoRAs learn tokens based on the specific scale.",
                "position": 173
            }
        ]
    },
    {
        "header": "3Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/nesting-dolls.png",
                "caption": "Table 1:Comparison between Llama-AVSR and our proposed LlamaMS,SS, andMSSapproaches on LRS2 and LRS3 benchmarks.†Llama-AVSR trains4444independent models tailored to each configuration of audio-video compression rates.",
                "position": 327
            },
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/nesting-dolls.png",
                "caption": "Table 2:Comparison between Llamaand multiple SOTA methods on the LRS2 and LRS3 benchmarks. The “Lab. Hrs.” column with values X/Y specifies how many labeled hours have been used in training for LRS2 (X) and LRS3 (Y).",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x3.png",
                "caption": "Figure 3:WER results for theaverage pooling(left) andstacking(right) compression methods for the ASR task. We use Whisper Small as audio encoder and Llama 3.2-1B as LLM.",
                "position": 536
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x3.png",
                "caption": "",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x4.png",
                "caption": "",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/nesting-dolls.png",
                "caption": "Table 3:Comparison between LlamaMSand a training-free Llama-AVSR-based approach that reduces the number of tokens via average pooling at inference time for the ASR task on the LRS3 dataset.",
                "position": 588
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x5.png",
                "caption": "Figure 4:WER results for theaverage pooling(left) andstacking(right) compression methods for the VSR task. We use AVHuBERT Large as video encoder and Llama 3.2-3B as LLM.",
                "position": 629
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x5.png",
                "caption": "",
                "position": 632
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x6.png",
                "caption": "",
                "position": 636
            },
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/nesting-dolls.png",
                "caption": "Table 4:Computational cost analysis of LlamaMSusing different compression rates and Llama 3.1-8B.",
                "position": 654
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "5Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06362/x7.png",
                "caption": "Figure 5:Additional WER results using stacking compression for the ASR task with {2222,4444,6666,8888,10101010} rates. We use the same configuration as in Figure3.",
                "position": 1690
            },
            {
                "img": "https://arxiv.org/html/2503.06362/x8.png",
                "caption": "Figure 6:Additional results for Llamausing stacking compression on the LRS3 dataset.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2503.06362/extracted/6261859/Figures/nesting-dolls.png",
                "caption": "Table 5:Comparison between Llama-AVSR and our proposed LlamaMS,SS, andMSSapproaches on LRS2 and LRS3 benchmarks. We employ Whisper medium and Llama 3.1-8B.†Llama-AVSR trains4444independent models tailored to each configuration of audio-video compression rates.",
                "position": 1726
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]