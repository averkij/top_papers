[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1.Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04816/x1.png",
                "caption": "Figure 1.Sustained TFLOPS across model scales on a singal GH200 (Qwen2.5 for 7B-32B) and H200 (Qwen2.5 72B and GPT-oss 120B). HorizonLM remains efficient while offloading baselines become GPU memory-bound.",
                "position": 106
            }
        ]
    },
    {
        "header": "2.Background and Related Work",
        "images": []
    },
    {
        "header": "3.Design Challenges",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04816/x2.png",
                "caption": "Figure 2.HorizonLM architecture: CPU acts as the parameter store while GPUs execute transient layer templates via asynchronous parameter streaming and gradient offloading.",
                "position": 421
            },
            {
                "img": "https://arxiv.org/html/2602.04816/x3.png",
                "caption": "Figure 3.End-to-end pipelined execution of Horizon-LM across compute, data movement, and CPU optimization.",
                "position": 515
            }
        ]
    },
    {
        "header": "4.System Design",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04816/x4.png",
                "caption": "Figure 4.Double-buffer streaming and slab-based gradient",
                "position": 726
            }
        ]
    },
    {
        "header": "5.Implementation",
        "images": []
    },
    {
        "header": "6.Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.04816/x5.png",
                "caption": "Figure 5.Host (CPU) memory footprint versus model scale across training systems.",
                "position": 1006
            },
            {
                "img": "https://arxiv.org/html/2602.04816/x6.png",
                "caption": "Figure 6.Depth scalability with fixed model width (hidden and FFN) size.",
                "position": 1009
            },
            {
                "img": "https://arxiv.org/html/2602.04816/x7.png",
                "caption": "Figure 7.Width scalability with fixed model layers",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2602.04816/x8.png",
                "caption": "Figure 8.Performance comparison on a single A100 PCIe system.",
                "position": 1231
            }
        ]
    },
    {
        "header": "7.Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]