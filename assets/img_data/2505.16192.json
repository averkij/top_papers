[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16192/x1.png",
                "caption": "Figure 1:This figure visualizes the contrast between traditional text-based CoT reasoning and our proposed VLM-R3approach, which integrates region grounding and refinement in an interleaved visual-textual reasoning chain. While conventional text-based reasoning fails when analyzing scenes that require dynamic, iterative, and fine-grained interaction with specific visual regions, our approach succeeds by precisely identifying and focusing on critical visual elements, such as the ’Hongdong Hotpot’ sign in this example, to derive accurate conclusions through targeted visual reasoning.",
                "position": 187
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/dataval.png",
                "caption": "Figure 2:Distribution of the VLIR dataset: (a) number of crops per image, (b) samples across different source datasets, and (c) categorization of crops based on relative size.",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/subplots_grounding_accuracy_impact.png",
                "caption": "Figure 3:Impact of region grounding accuracy on model performance across three benchmarks. Each subplot shows the performance trajectory from 40% to 90% grounding accuracy with confidence intervals (shaded regions).",
                "position": 716
            },
            {
                "img": "https://arxiv.org/html/2505.16192/x2.png",
                "caption": "Figure 4:Comparison of attention distribution patterns between the interleaved reasoning chain with visual region localization (top) and general textual reasoning chain (bottom).",
                "position": 725
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Settings",
        "images": []
    },
    {
        "header": "Appendix BPrompt Templates for VLIR Dataset Construction and Filtering",
        "images": []
    },
    {
        "header": "Appendix CDemonstrations for VLM-R³",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/case1.png",
                "caption": "Figure 5:This figure presents how VLM-R3captures details in a natural image and reasons about them.",
                "position": 1818
            },
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/case2.png",
                "caption": "Figure 6:This figure presents how VLM-R3gets the text details from the document image.",
                "position": 1821
            },
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/case4.png",
                "caption": "Figure 7:This figure presents how VLM-R3iteratively tracks multiple visual cues in an image.",
                "position": 1824
            },
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/case5.png",
                "caption": "Figure 8:This figure presents how VLM-R3performs complex interleaved text-image CoT reasoning.",
                "position": 1827
            },
            {
                "img": "https://arxiv.org/html/2505.16192/extracted/6465353/case3.png",
                "caption": "Figure 9:This figure presents how VLM-R3performs complex interleaved text-image CoT reasoning.",
                "position": 1830
            }
        ]
    },
    {
        "header": "Appendix DTechnical Appendices and Supplementary Material",
        "images": []
    }
]