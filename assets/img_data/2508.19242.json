[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Autoregressive Universal Video Segmentation Model (AUSM)",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19242/x1.png",
                "caption": "Figure 1:High-level overview of AUSM during training and inference.Left:At inference time, AUSM processes frames in a recurrent manner with constant decoding time per frame. Rather than maintaining an explicit memory buffer, temporal history is compressed via the Mamba layer within the History Compressor module and passed through time.Right:During training, a parallel formulation is used to jointly optimize over multiple frames, enabling efficient and scalable learning.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2508.19242/x2.png",
                "caption": "Figure 2:HistoryCompressormodule. Mamba encodes temporal dependencies, while self-attention captures spatial structure, enabling recurrent compression ofEEwith constant memory.\nSpecifically, the temporal mamba layer operates pixel-wise, mixing information of each pixel throughout the time dimensionTT.\nThe spatial self-attention layer, by contrast, is frame-independent that fuses information spanning overH‚ÄãWHWpixels.",
                "position": 286
            },
            {
                "img": "https://arxiv.org/html/2508.19242/x3.png",
                "caption": "Figure 3:Schematic ofPreprocess. The video contains three ground-truth instances:person1(the person riding the horse),person2(the person standing), andhorse. Each instance is matched with a vector fromùíú\\mathcal{A}, represented by colored circles: red forperson1, green forperson2, and blue forhorse. The highlighted contours indicate the randomly sampled timesteps (tsampleit_{\\text{sample}}^{i}) for each instance.",
                "position": 375
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19242/x4.png",
                "caption": "Figure 4:Comparison of training time (sec/iter) between iterative and parallel training approaches across different sequence lengths.",
                "position": 757
            },
            {
                "img": "https://arxiv.org/html/2508.19242/x4.png",
                "caption": "Figure 4:Comparison of training time (sec/iter) between iterative and parallel training approaches across different sequence lengths.",
                "position": 760
            },
            {
                "img": "https://arxiv.org/html/2508.19242/x5.png",
                "caption": "Figure 5:Performance comparison between Stage 2 (5-frame) and Stage 3 (16-frame) training across four benchmark datasets.",
                "position": 765
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BQualitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.19242/x6.png",
                "caption": "Figure 6:Qualitative comparison between unprompted and prompted video segmentation results using a single AUSM model.\nIn the unprompted mode, the model autonomously discovers, segments, and classifies objects in the scene without any external guidance.\nIn the prompted mode, it tracks only the object specified by an initial mask in the first frame.\nThese examples demonstrate the unified capability of AUSM to seamlessly support both modes within a single framework.",
                "position": 1064
            },
            {
                "img": "https://arxiv.org/html/2508.19242/x7.png",
                "caption": "Figure 7:Qualitative comparison between unprompted and prompted video segmentation results using a single AUSM model.\nIn the unprompted mode, the model autonomously discovers, segments, and classifies objects in the scene without any external guidance.\nIn the prompted mode, it tracks only the object specified by an initial mask in the first frame.\nThese examples demonstrate the unified capability of AUSM to seamlessly support both modes within a single framework.",
                "position": 1072
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]