[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16267/x1.png",
                "caption": "Figure 1:SOTA video VLM model comparison: (Left) Number of visual tokens vs. video-QA accuracy. (Right) Model size vs. video-QA accuracy.",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2410.16267/x1.png",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2410.16267/x2.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "2BLIP-3-Video",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16267/x3.png",
                "caption": "Figure 2:An illustration of the BLIP-3-Video model architecture. It has the explicit temporal encoder inserted to BLIP-3.",
                "position": 117
            },
            {
                "img": "https://arxiv.org/html/2410.16267/x4.png",
                "caption": "Figure 3:Visually comparing different types of temporal encoders we explored in our model architecture. (c) and (d) are particularly effective, as we discuss further in the experiments.",
                "position": 159
            }
        ]
    },
    {
        "header": "3Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16267/x5.png",
                "caption": "Figure 4:Example video captioning results on Mira dataset, formed in question-answering style.",
                "position": 695
            },
            {
                "img": "https://arxiv.org/html/2410.16267/x6.png",
                "caption": "Figure 5:Example video captioning results on MSVD and MSRVTT caption dataset.",
                "position": 698
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.16267/x7.png",
                "caption": "Figure 6:Example video captioning results on Mira dataset, formed in question-answering style. We compare the outputs of BLIP-3-Video, Tarsier, and LLaVA-OneVision. GT stands for the ground truth. Different colored texts are different parts of ground truth captions and their corresponding sentences in the model outputs. Underlined texts are hallucinations.",
                "position": 1494
            },
            {
                "img": "https://arxiv.org/html/2410.16267/x8.png",
                "caption": "Figure 7:Example video captioning results on Mira dataset, formed in question-answering style. We compare the outputs of BLIP-3-Video, Tarsier, and LLaVA-OneVision. GT stands for the ground truth. Different colored texts are different parts of ground truth captions and their corresponding sentences in the model outputs. Underlined texts are hallucinations.",
                "position": 1497
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]