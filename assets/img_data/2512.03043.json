[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03043/x1.png",
                "caption": "",
                "position": 105
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x2.png",
                "caption": "",
                "position": 107
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x3.png",
                "caption": "",
                "position": 120
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03043/x4.png",
                "caption": "Figure 2:Performance gains of our model over Qwen3-VL-Instruct-8B across diverse visual tasks after training.",
                "position": 129
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03043/x5.png",
                "caption": "Figure 3:Overview of our curated training dataset, including both image and video modalities for a diverse range of understanding tasks.",
                "position": 238
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x6.png",
                "caption": "Figure 4:Comparison of advantage formulations in three RL algorithms.",
                "position": 402
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03043/x7.png",
                "caption": "Figure 5:Performance on unseen visual tasks.",
                "position": 1624
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AReasoning Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.03043/x8.png",
                "caption": "Figure 6:Reasoning example of image question answering task.",
                "position": 1646
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x9.png",
                "caption": "Figure 7:Reasoning example of video question answering task.",
                "position": 1651
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x10.png",
                "caption": "Figure 8:Reasoning example of image caption task.",
                "position": 1656
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x11.png",
                "caption": "Figure 9:Reasoning example of video caption task.",
                "position": 1661
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x12.png",
                "caption": "Figure 10:Reasoning example of temporal grounding task.",
                "position": 1666
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x13.png",
                "caption": "Figure 11:Reasoning example of spatial grounding task.",
                "position": 1671
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x14.png",
                "caption": "Figure 12:Reasoning example of spatial-temporal grounding task.",
                "position": 1676
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x15.png",
                "caption": "Figure 13:Reasoning example of tracking task.",
                "position": 1681
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x16.png",
                "caption": "Figure 14:Reasoning example for an image segmentation task. The resulting answer will be forwarded to SAM2 to produce the mask.",
                "position": 1686
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x17.png",
                "caption": "Figure 15:Reasoning example for an video segmentation task. The resulting answer will be forwarded to SAM2 to produce the mask.",
                "position": 1691
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x18.png",
                "caption": "Figure 16:System prompt for all tasks.",
                "position": 1701
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x19.png",
                "caption": "Figure 17:Prompt for QA tasks.",
                "position": 1706
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x20.png",
                "caption": "Figure 18:Prompt for grounding and tracking tasks.",
                "position": 1711
            },
            {
                "img": "https://arxiv.org/html/2512.03043/x21.png",
                "caption": "Figure 19:Prompt for segmentation tasks.",
                "position": 1716
            }
        ]
    },
    {
        "header": "Appendix BPrompt Template",
        "images": []
    }
]