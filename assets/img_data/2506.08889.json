[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2SeerAttention-R",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08889/x1.png",
                "caption": "Figure 1:SeerAttention (Sparse Prefill) and SeerAttention-R (Sparse Decode). In SeerAttention-R, no sequence dimension compression/pooling operation is applied in Query (Q). Given that modern architectures predominantly use GQA, a linear layer projects the Q from its original number of heads down to the number of KV heads, enabling shared sparsity selection in a GQA group.",
                "position": 186
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x2.png",
                "caption": "Figure 2:Training Diagram and Training Kernel of SeerAttention-R. (a) Self-distillation training of AttnGate in SeerAttention-R. It uses 1D maxpooled attention scores from original model as ground truth to train AttnGate. Query head reduction is not plotted in the diagram for simplicity. (2) Pseudo code of attention forward kernel for training that directly generates ground truth and attention output.",
                "position": 265
            }
        ]
    },
    {
        "header": "3Inference of SeerAttention-R",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08889/x3.png",
                "caption": "Figure 3:Inference Diagram of SeerAttention-R. During inference, a K Compression Cache is used to cache the compressed key representation in AttnGate to speedup sparse block prediction. This K Compression Cache only updates once per block number of tokens is generated (block=4 in the plots for illustration). As a result, the last block of sequence is always selected to compensate when the compression cache has not been updated yet.gùëîgitalic_gis the group size of GQA.",
                "position": 288
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08889/x4.png",
                "caption": "Figure 4:Oracle Sparse Results of Qwen3-14B with block size 32, 64, 128.",
                "position": 350
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x5.png",
                "caption": "Figure 5:Accuracy Results of Full Attention, SeerAttention-R, and Quest. The Quest sparse configuration is set to be the same as SeerAttention-R for fair comparison, which uses a block size of 64 and sparse attention in all layers.",
                "position": 364
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x6.png",
                "caption": "Figure 6:Kernel Speedup of our Block Sparse Flash-Decoding Kernel on H100 GPU. Our TileLang implementation of the kernel achieves higher speedup ratio compared to Triton implementation. For longer sequence length or larger batch size cases, the speedups approach the theoretical upper bound compared to FA3 basline.",
                "position": 388
            }
        ]
    },
    {
        "header": "5Ablation Studies",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08889/x7.png",
                "caption": "Figure 7:AIME24 results using different block sizes with 4k token budget. SeerAttention-R achieves almost consistent performances on different block sizes. However, Quest gets lower accuracy when block size gets larger. Note that in this experiment, SeerAttention-R enables shared sparsity selection within each GQA group, whereas Quest does not.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x8.png",
                "caption": "Figure 8:AIME24 results of whether using dense attention in first two layers (Qwen3-4B).",
                "position": 423
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x9.png",
                "caption": "(a)",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x9.png",
                "caption": "(a)",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2506.08889/x10.png",
                "caption": "(b)",
                "position": 444
            }
        ]
    },
    {
        "header": "6Limitation and Future Work",
        "images": []
    },
    {
        "header": "7Related Works",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]