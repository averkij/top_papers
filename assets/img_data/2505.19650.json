[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19650/x1.png",
                "caption": "Figure 1:Performance comparison on instruction-based retrieval benchmarks (left: MMEBjiang2024vlm2vecand right: WebVid-CoVRventura2024covr). Our\\gradientRGBUnite72, 203, 19472, 116, 203achieves leading performance on various tasks, even surpassing models with larger parameter scales.",
                "position": 117
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Unite",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19650/x2.png",
                "caption": "Figure 2:Overview of\\gradientRGBUnite72, 203, 19472, 116, 203: (a) Model architecture utilizing LMM as the backbone, supporting multimodal inputs (text, images, videos, and their combinations). (b) Similarity matrix after applying MAMCL, which enables focused contrastive learning by restricting comparisons to samples sharing the same target modality, thus reducing inter-modal interference.",
                "position": 229
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Analysis",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19650/x3.png",
                "caption": "Figure 3:We develop a universal multimodal embedder\\gradientRGBUnite72, 203, 19472, 116, 203, allowing for a unified representation of arbitrary multimodal contents.",
                "position": 3518
            },
            {
                "img": "https://arxiv.org/html/2505.19650/x4.png",
                "caption": "Figure 4:Performance comparison on fine-grained video-text benchmark (CaReBench[96]) and image-text benchmarks (ShareGPT4V[12], Urban1K[105], DOCCI[64]). Our\\gradientRGBUnite72, 203, 19472, 116, 203achieves the overall optimal performance.",
                "position": 3521
            }
        ]
    },
    {
        "header": "Appendix AData",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.19650/x5.png",
                "caption": "Figure 5:Retrieval Adaptation 6.4M.Left: Data Distribution within Each Category. The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets. Right: The detailed quantities of datasets.",
                "position": 3536
            },
            {
                "img": "https://arxiv.org/html/2505.19650/x6.png",
                "caption": "Figure 6:Instruction Tuning 1.3M.Left: Data Distribution within Each Category. The outer circle shows the distribution of all data categories and the inner circle shows the distribution of data subsets. Right: The detailed quantities of datasets.",
                "position": 3617
            }
        ]
    },
    {
        "header": "Appendix BMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CMore Experimental Results",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    }
]