[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x1.png",
                "caption": "Figure 1:SynthesizeMedevises prompts for personalization of reward models. To address preference attribution in a low data setting,SynthesizeMetests hypotheses about users to reason over their preferences and induce personas. A real trace is shown from User 163 in PRISM.",
                "position": 255
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x2.png",
                "caption": "Table 1:Comparison ofSynthesizeMewith related personal reward model approaches. Unlike RS and P-Soups,SynthesizeMeis preference axis-free (meaning unconstrained preferences). This meansSynthesizeMehas to handlepreference attributionon top of thedata scarcityproblem all methods face.",
                "position": 299
            }
        ]
    },
    {
        "header": "3IntroducingSynthesizeMe",
        "images": []
    },
    {
        "header": "4Constructing PersonalRewardBench",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x2.png",
                "caption": "Table 2:Statistics of our filtered datasets comprising PersonalRewardBench. After filtering for personalization, PRISM is much larger than Chatbot Arena.",
                "position": 512
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Robustness ofSynthesizeMe",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x2.png",
                "caption": "Figure 2:SynthesizeMeprompts for LLM-as-a-Judge scale well with increasing amounts of preferences per user on chatbot arena. We test with Llama-3.3-70B and find an almost 0.8% improvement in accuracy for every additional context preference.\nJust five context preferences beat non-personalized LLM as a Judge.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2506.05598/x3.png",
                "caption": "Figure 3:Rate at which personas generated bySynthesizeMematch the stated user preferences in PRISM. We compare both the persona with the true user it came from and with a randomly selected user. The true rate is always higher than the random rate, and for Llama 70b this holds withp<0.05ð‘0.05p<0.05italic_p < 0.05.",
                "position": 873
            },
            {
                "img": "https://arxiv.org/html/2506.05598/x4.png",
                "caption": "Figure 4:Results of transferringSynthesizeMeprompts learned on one model and testing on another. GPT4o-mini works best and even personalizes on prompts learned by Llama 3.2 3B.",
                "position": 995
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "Ethical Considerations",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Related Works and Deep Dives",
        "images": []
    },
    {
        "header": "Appendix BHyperparameters",
        "images": []
    },
    {
        "header": "Appendix CPersonalRewardBench Filtering",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x5.png",
                "caption": "Table 7:Amount of data after each step of our data filtering pipeline and example queries from removed conversations.",
                "position": 2326
            }
        ]
    },
    {
        "header": "Appendix DAdditional Figures",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x5.png",
                "caption": "Figure 6:Scaling methods from Llama 3b to 70b on ChatbotArena. Methods shown in green improve across scale, gray fluctuate, and red decrease with scale.",
                "position": 2352
            },
            {
                "img": "https://arxiv.org/html/2506.05598/x6.png",
                "caption": "Figure 7:Scaling methods from Llama 3b to 70b on PRISM. Methods shown in green improve across scale, gray fluctuate, and red decrease with scale.",
                "position": 2355
            },
            {
                "img": "https://arxiv.org/html/2506.05598/x7.png",
                "caption": "Figure 8:Scaling with prism and arena users with more context. PRISM does not scale with more context the same way that Chatbot Arena does. We hypothesize that this is because PRISM users are constrained to 5-6 conversations, so having more interactions just means longer conversations about the same topic, rather than greater diversity of topics.",
                "position": 2358
            },
            {
                "img": "https://arxiv.org/html/2506.05598/x8.png",
                "caption": "Figure 9:Results of learning a personalized prompts usingSynthesizeMeon a teacher model then transferring to a student model for deployment. Larger models help more for both creating the prompts and executing the personalized reward model. In the case of chatbot arena the student model appears to have larger impact than the teacher though llama70b is very sensitive to the prompt source.",
                "position": 2361
            }
        ]
    },
    {
        "header": "Appendix EPersonas versus Demographics",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05598/x9.png",
                "caption": "Figure 10:Comparison of demographic categories to t-SNE ofSynthesizeMepersonas embedding with sBERT",
                "position": 2370
            }
        ]
    },
    {
        "header": "Appendix FLLM as a Judge and SynthesizeMe Prompts and Programs",
        "images": []
    },
    {
        "header": "Appendix GLearned Personas through SynthesizeMe",
        "images": []
    }
]