[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05288/x1.png",
                "caption": "",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05288/x2.png",
                "caption": "Figure 2:PlaceIt3D dataset creation. Given a scene and an asset as input(a)the goal is to create a prompt(f)and corresponding mask‚Ñ≥‚Ñ≥\\mathcal{M}caligraphic_Mof valid placements(e). We start by finding the set of points which are physically plausible placements, shown in red in(b).\nWe consider eight equally spaced rotation angles, which condition the valid placements. For this example, angle0‚àòsuperscript00^{\\circ}0 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPThas more valid placements than45‚àòsuperscript4545^{\\circ}45 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT. To generate the language constraints, we use the ground truth scene graph(c). Object anchors are selected from the scene graph and combined with relationship types to create a constraint and corresponding validity mask(d). The different placement constraints are combined in the final output by intersecting the validity masks(e)given a mask of validdenseplacements.\nBased on each selected set of anchors and constraint relationships, a natural language prompt is created using templates (please, see supplemental for more details).",
                "position": 156
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Language-Guided 3D Object Placement",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05288/x3.png",
                "caption": "Figure 3:Method overview.A point encoder extracts features from the 3D scene, which are then complemented with positional embeddings. Spatial pooling reduces feature dimensions, and a Q-Former merges the pooled features with trainable queriesQùëÑQitalic_Q(Section4.1). The asset is encoded into a single vector by using a pretrained asset encoder followed by max-pooling (Section4.2). This vector together with a size embedding is passed to a projection layer that aligns the features with the LLM space.\nThe LLM take as input (i) the output of the Q-Former, (ii) the text prompt, and (iii) the projected asset features and predicts three special tokens[ANC]delimited-[]ANC\\mathrm{[ANC]}[ roman_ANC ],[LOC]delimited-[]LOC\\mathrm{[LOC]}[ roman_LOC ]and[ROT]delimited-[]ROT\\mathrm{[ROT]}[ roman_ROT ]. A transformer based decoder takes as input the features associated with the three special tokens and the pooled scene features and performs a few self and cross attention operations (Section4.3). Three heads produce the final outputs:‚Ñ≥l‚Å¢o‚Å¢csubscript‚Ñ≥ùëôùëúùëê\\mathcal{M}_{loc}caligraphic_M start_POSTSUBSCRIPT italic_l italic_o italic_c end_POSTSUBSCRIPTthe valid placement mask;‚Ñ≥a‚Å¢n‚Å¢csubscript‚Ñ≥ùëéùëõùëê\\mathcal{M}_{anc}caligraphic_M start_POSTSUBSCRIPT italic_a italic_n italic_c end_POSTSUBSCRIPTan auxiliary mask that localizes the object anchors; and‚Ñ≥r‚Å¢o‚Å¢tsubscript‚Ñ≥ùëüùëúùë°\\mathcal{M}_{rot}caligraphic_M start_POSTSUBSCRIPT italic_r italic_o italic_t end_POSTSUBSCRIPTa mask indicating which rotation angles are valid at each location.",
                "position": 236
            }
        ]
    },
    {
        "header": "4PlaceWizard: Method Description",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05288/x4.png",
                "caption": "Figure 4:Qualitative benchmark results. Colored highlights indicate anchors referenced in the textual prompts (predictions are generated entirely from point clouds, with anchor information provided only as text). The asset position is marked with a yellow circle, and a yellow arrow denotes the frontal orientation. Our method successfully follows language instructions and meets the specified constraints. The top-right example illustrates a placement that satisfies constraints but slightly intersects with the scene mesh. The bottom-right example demonstrates a failure case where one constraint is not met (highlighted in red).",
                "position": 669
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Additional details on the training dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.05288/x5.png",
                "caption": "Figure 5:Comparison of our spatial pooling vs the superpoints used in[17]. Our regions are more local and more adequate to the task of object placement.",
                "position": 1381
            },
            {
                "img": "https://arxiv.org/html/2505.05288/x6.png",
                "caption": "Figure 6:Heatmap visualization of the predicted confidence scores by our model for spatial clusters in two examples from our dataset, across two scenes, given different assets and textual prompts. Warmer colors indicate higher confidence regions for asset placement, with white representing the highest confidence.",
                "position": 1752
            },
            {
                "img": "https://arxiv.org/html/2505.05288/x7.png",
                "caption": "Figure 7:Examples from our proposed dataset illustrating prompts with different constraints, along with the corresponding placement mask and a sample placed asset.",
                "position": 1760
            },
            {
                "img": "https://arxiv.org/html/2505.05288/x8.png",
                "caption": "Figure 8:Examples from our proposed dataset illustrating prompts with different constraints, along with the corresponding placement mask and a sample placed asset.",
                "position": 1763
            }
        ]
    },
    {
        "header": "9PlaceWizard Implementation Details",
        "images": []
    },
    {
        "header": "10Visualization of superpoints",
        "images": []
    },
    {
        "header": "11Further Qualitative Results",
        "images": []
    }
]