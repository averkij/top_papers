[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04370/x1.png",
                "caption": "Figure 1:Overview of our method.\nDreamDPO first constructs pairwise examples, then compares their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D presentation with a preference-driven loss function. The loss function pulls thewinexampleùê±twinsuperscriptsubscriptùê±ùë°win\\mathbf{x}_{t}^{\\text{win}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT win end_POSTSUPERSCRIPTcloser and pushes theloseexampleùê±tlosesuperscriptsubscriptùê±ùë°lose\\mathbf{x}_{t}^{\\text{lose}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT lose end_POSTSUPERSCRIPTaway.\nAs a piecewise objective, it selectively pushesùê±tlosesuperscriptsubscriptùê±ùë°lose\\mathbf{x}_{t}^{\\text{lose}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT lose end_POSTSUPERSCRIPTonly when the preference score gapsgapsubscriptùë†gaps_{\\text{gap}}italic_s start_POSTSUBSCRIPT gap end_POSTSUBSCRIPTexceeds a thresholdœÑùúè\\tauitalic_œÑ, preventing chaotic gradients from overly similarùê±tlosesuperscriptsubscriptùê±ùë°lose\\mathbf{x}_{t}^{\\text{lose}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT lose end_POSTSUPERSCRIPT.",
                "position": 135
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04370/x2.png",
                "caption": "Figure 2:Qualitative comparisons on the benchmark of GPTEval3D[25].\nExisting methods struggle with text matching, as marked in red.\nDreamDPO improves text matching, which provides better human preference results.\n(Zoom in to see the details.)",
                "position": 463
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04370/x3.png",
                "caption": "Figure 3:Qualitative comparisons with MVDream[7].\nDreamDPO performs well across short to long prompts, offering better human preference results, marked in red.¬†(Zoom in to see the details.)",
                "position": 479
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x4.png",
                "caption": "Figure 4:The analysis of backbone.\nWe present the results of DreamDPO using Stable Diffusion v2.1 (SD2.1)[17].\nDreamDPO demonstrates effective performance with SD2.1, highlighting its potential to leverage more advanced backbone diffusion models for further improvements.",
                "position": 530
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x4.png",
                "caption": "Figure 4:The analysis of backbone.\nWe present the results of DreamDPO using Stable Diffusion v2.1 (SD2.1)[17].\nDreamDPO demonstrates effective performance with SD2.1, highlighting its potential to leverage more advanced backbone diffusion models for further improvements.",
                "position": 533
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x5.png",
                "caption": "Figure 5:The analysis of reward models.\nWe present the results of DreamDPO using ImageReward[26].\nDreamDPO demonstrates effective performance with ImageReward, highlighting its potential to leverage stronger reward models to further enhance generation quality.",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x6.png",
                "caption": "Figure 6:The analysis of the score gap thresholdœÑùúè\\tauitalic_œÑ.\nWe conduct 2D toy experiments withœÑùúè\\tauitalic_œÑranging from0.010.010.010.01to00.\nThe results indicate that a small but non-zeroœÑùúè\\tauitalic_œÑeffectively filters out overly similarloseexamples, leading to more detailed outputs.",
                "position": 548
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x7.png",
                "caption": "Figure 7:Qualitative comparisons with DreamReward[12].\nDreamDPO improves both text matching (marked in red) and geometric/texture details.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x8.png",
                "caption": "Figure 8:The generation results of DreamDPO with large multi-modal models (LMMs).\nWe explore the potential of our method to leverage LMMs, such as QwenVL[41]for explicit guidance in correcting the number and attribute of 3D assets. The left corner shows the details of pairwise comparisons using the LMM, including the question and win/lose criteria.\nBy carefully designing the question, DreamDPO can leverage bothwinandloseexamples to guide optimization. (Zoom in to see the details.)",
                "position": 561
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x9.png",
                "caption": "Figure 9:The analysis of pairwise example construction.\nWe compare (1) different noises: adding different Gaussian noises with the same timesteps, and (2) difference timesteps: adding the same Gaussian noise with different timesteps.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x9.png",
                "caption": "Figure 9:The analysis of pairwise example construction.\nWe compare (1) different noises: adding different Gaussian noises with the same timesteps, and (2) difference timesteps: adding the same Gaussian noise with different timesteps.",
                "position": 571
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x10.png",
                "caption": "Figure 10:The further application of DreamDPO.\nWe conduct toy experiments on text-to-avatar generation by combining DreamDPO with Gaussian-based avatar generation framework[48]. More details can be checked in AppendixB.3.",
                "position": 577
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix CSupplementary Experimental Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04370/x11.png",
                "caption": "Figure 11:More qualitative results using DreamDPO.",
                "position": 2274
            },
            {
                "img": "https://arxiv.org/html/2502.04370/x12.png",
                "caption": "Figure 12:More qualitative results using DreamDPO.",
                "position": 2277
            }
        ]
    },
    {
        "header": "Appendix DSupplementary Experimental Results",
        "images": []
    }
]