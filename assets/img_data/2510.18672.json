[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3Experimental Settings",
        "images": []
    },
    {
        "header": "4Pilot Investigations: Serving LLM v.s. RLLM",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18672/x1.png",
                "caption": "Figure 1:Results of token budget variation across different datasets for 14B and 32B RLLM .",
                "position": 266
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x2.png",
                "caption": "",
                "position": 269
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x3.png",
                "caption": "Figure 2:The serving performance and behavior comparison of a batch requests between 7B RLLM and LLM. We can read from this figure that (1) RLLM exhibits significant KV Cache fluctuations than LLM; (2) long tail distribution of requests running time caused by straggler requests; (3) adaptive running time of RLLM; (4) domain preference on math. Please refer to Â§I.3for more results.",
                "position": 286
            }
        ]
    },
    {
        "header": "5Observations on RLLM Serving Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18672/x4.png",
                "caption": "Figure 3:Empirical results of current LLM quantization methods on 7B RLLM. current methods maintain or improve all serving-related metrics with less memory footprint while keep accuracy.",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x5.png",
                "caption": "",
                "position": 347
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x6.png",
                "caption": "Figure 4:Empirical results for KV cache quantization on 14B model across different datasets.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x7.png",
                "caption": "Figure 5:Empirical results of comparison for enable or disable prefix caching on 32B RLLM.",
                "position": 398
            }
        ]
    },
    {
        "header": "6Applying to Real World Workload",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18672/figures/GPU_KV_Cache_Usage_over_time_14B.jpg",
                "caption": "Figure 6:KV cache usage of 14B models under real-world workload across different datasets.",
                "position": 426
            },
            {
                "img": "https://arxiv.org/html/2510.18672/figures/Requests_over_time_14B.jpg",
                "caption": "Figure 7:Num of running requests in the inference engine for 14B models under real-world workload.",
                "position": 429
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix and Supplementary Material",
        "images": []
    },
    {
        "header": "Appendix AUse of LLMs Statement",
        "images": []
    },
    {
        "header": "Appendix BLimitation",
        "images": []
    },
    {
        "header": "Appendix CBoarder Impact",
        "images": []
    },
    {
        "header": "Appendix DExtended Related Work",
        "images": []
    },
    {
        "header": "Appendix EAn Introduction to LLM Serving",
        "images": []
    },
    {
        "header": "Appendix FImplementation and Reproduction Details",
        "images": []
    },
    {
        "header": "Appendix GExperiments Details",
        "images": []
    },
    {
        "header": "Appendix HExtend Observation",
        "images": []
    },
    {
        "header": "Appendix IDetailed Empirical Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18672/x8.png",
                "caption": "Figure 8:Results of token budget variation across different datasets for different scale RLLM .",
                "position": 2182
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x9.png",
                "caption": "",
                "position": 2185
            },
            {
                "img": "https://arxiv.org/html/2510.18672/",
                "caption": "",
                "position": 2187
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x11.png",
                "caption": "Figure 9:Results of RLLM vs LLM for 14B model size .",
                "position": 3991
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x12.png",
                "caption": "Figure 10:Results of RLLM vs LLM for 32B model size .",
                "position": 3994
            }
        ]
    },
    {
        "header": "Appendix JDetailed Empirical Results for RLLM Serving Optimization",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.18672/x13.png",
                "caption": "Figure 11:Results of 7B RLLM with SD enabled .",
                "position": 5368
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x14.png",
                "caption": "Figure 12:KV cache usage of 7B models under real-world workload across different datasets.",
                "position": 6098
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x15.png",
                "caption": "Figure 13:KV cache usage of 32B models under real-world workload across different datasets.",
                "position": 6101
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x16.png",
                "caption": "Figure 14:Num of running requests in the inference engine for 7B models under real-world workload.",
                "position": 6105
            },
            {
                "img": "https://arxiv.org/html/2510.18672/x17.png",
                "caption": "Figure 15:Num of running requests in the inference engine for 32B models under real-world workload.",
                "position": 6108
            }
        ]
    },
    {
        "header": "Appendix KExtended Results for Real World Benchmarking",
        "images": []
    }
]