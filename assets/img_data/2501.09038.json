[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09038/x1.png",
                "caption": "Figure 1:Sample scenarios from the Physics-IQ dataset for testing physical understanding in generative video models. Models are shown the beginning of a video (single frame for image2video models; 3 seconds for video2video models) and need to predict how the video continues over the next 5 seconds, which requires understanding different physical properties: Solid Mechanics, Fluid Dynamics, Optics, Thermodynamics, and Magnetism. Seeherefor an animated version of this figure.",
                "position": 95
            },
            {
                "img": "https://arxiv.org/html/2501.09038/x2.png",
                "caption": "Figure 2:Overview of the Physics-IQ evaluation protocol. A video generative model produces a 5 second continuation of the conditioning frame(s), optionally including a textual description of the conditioning frames for models that accept text input. They are compared against the ground truth test frames using four metrics that quantify different properties of physical understanding. The metrics are defined and explained in the methods section. Code to run the evaluation is available atPhysics-IQ-benchmark.",
                "position": 116
            }
        ]
    },
    {
        "header": "Physics-IQ benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09038/x3.png",
                "caption": "Figure 3:A qualitative overview of recent synthetic datasets related to physical understanding(19,20,17,38,18,36,37,39). These datasets are great for the purposes they were designed for, but not ideal for evaluating models trained on real-world videos due to the distribution shift.",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2501.09038/x4.png",
                "caption": "Figure 4:How well do current video generative models understand physical principles?Left.The Physics-IQ score is an aggregated measure across four individual metrics, normalized such that pairs of real videos that differ only by physical randomness score 100%. All evaluated models show a large gap, with the best model scoring 24.1%, indicating that physical understanding is severely limited.Right.In addition, the mean rank of models across all four metrics is shown here; the Spearman correlation between aggregated results on the left and mean rank on the right is high (-‚Å¢.87,p<.005-.87p.005\\text{-}.87,\\emph{p}<.005- .87 , p < .005), thus aggregating to a single Physics-IQ score largely preserves model rankings.",
                "position": 196
            },
            {
                "img": "https://arxiv.org/html/2501.09038/x5.png",
                "caption": "Figure 5:Relationship between visual realism and physical understanding.Left.A multimodal large language model (Gemini 1.5 Pro) is asked to identify the generated video among the real and the generated video for each scenario (MLLM score) in a two-alternative forced choice paradigm. Chance rate is 50%; lower scores indicate that the model finds it harder to tell apart generated from real videos (= better realism). Sora-generated videos are hardest to distinguish from real videos for the model, whereas Lumiere (multiframe) is easiest.Right.Do models that produce ‚Äòrealistic-looking‚Äô videos (as assessed by the MLLM score) also score better in terms of physical understanding (as assessed via the Physics-IQ score)? This scatterplot with linear fit and 95% confidence interval as a shaded blue area shows that this is not the case: Visual realism is uncorrelated with physical understanding (Pearson‚Äôsr= - 0.46,p=.247 not significant). Note that the y axis on this plot is inverted for easier interpretation (up & right are best).",
                "position": 279
            },
            {
                "img": "https://arxiv.org/html/2501.09038/x5.png",
                "caption": "",
                "position": 282
            },
            {
                "img": "https://arxiv.org/html/2501.09038/x6.png",
                "caption": "",
                "position": 286
            }
        ]
    },
    {
        "header": "Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09038/x7.png",
                "caption": "Figure 6:Performance comparison of video generative models across different physical categories (columns) and metrics (rows). For the top three metrics, higher is better; for the last metric lower values are best. Throughout, physical variance (i.e., the performance that is achievable by real videos differing only by physical randomness) is indicated by a dashed line. Across metrics and categories, models show a considerable lack in physical understanding. More lenient metrics likeùñ≤ùóâùñ∫ùóçùóÇùñ∫ùóÖ‚Å¢-‚Å¢ùñ®ùóàùñ¥ùñ≤ùóâùñ∫ùóçùóÇùñ∫ùóÖ-ùñ®ùóàùñ¥\\mathsf{Spatial}\\text{-}\\mathsf{IoU}sansserif_Spatial - sansserif_IoU(top row) that only assesswherean action occurred lead to higher scores than more strict metrics that also take into account e.g.whenorhow muchaction should be taking place.",
                "position": 303
            },
            {
                "img": "https://arxiv.org/html/2501.09038/x8.png",
                "caption": "Figure 7:We here visualize success and failure scenarios within the fluid dynamics and solid mechanics categories for the two best models, VideoPoet and Runway Gen 3, according to our metrics. Both models are able to generate physics plausible frames for scenarios such as smearing paint on glass (VideoPoet) and pouring red liquid on a rubber duck (Runway Gen 3). At the same time, the models fail to simulate a ball falling into a crate or cutting a tangerine with a knife. Seeherefor an animated version.",
                "position": 307
            }
        ]
    },
    {
        "header": "Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.09038/extracted/6132070/figures/combined_setup_views.png",
                "caption": "Figure 8:Illustration of recording setup (top) and perspectives (bottom).",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2501.09038/extracted/6132070/figures/all_scenarios_overview.png",
                "caption": "Figure 9:The switch frames (here: center view only) of all scenarios in the Physics-IQ benchmark. A switch frame is the last conditioning frame before a model is asked to predict 5 seconds of future frames.",
                "position": 958
            },
            {
                "img": "https://arxiv.org/html/2501.09038/extracted/6132070/figures/mse_viz.jpg",
                "caption": "Figure 10:Since mean squared error (MSE) values can be hard to interpret, this figure shows the effect of a distortion applied to the scene, serving as a rough intuition for the effect of a MSE at different noise levels.",
                "position": 964
            }
        ]
    },
    {
        "header": "Supplementary Material",
        "images": []
    }
]