[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04214/x1.png",
                "caption": "Figure 1:Interactive object-aware audio generation. We generate sound aligned with specific visual objects in complex scenes. Users can select one or more objects in the scene using segmentation masks, and our model generates audio corresponding to the selected objects. Here, we show a busy street with multiple sound sources (left). After training, our model generates object-specific audio (right), such as crowd noise for people, engine sounds for cars, and blended audio for multiple objects.",
                "position": 197
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Interactive Object-Aware Audio Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04214/x2.png",
                "caption": "Figure 2:Model architecture.We encode the reference spectrogram via a pre-trained latent encoder. An image and text prompt are processed by separate encoders, and their embeddings are fused using an attention mechanism to highlight relevant objects. We then feed these conditioned features and noisy latent into a latent diffusion model to generate the object-specific audio. Finally, the latent decoder reconstructs the spectrogram, and a pre-trained HiFi-GAN vocoder generates the final audio waveform. At test time, we replace the attention with a user-provided segmentation mask, and the latent encoder for the reference spectrogram isnotused.",
                "position": 268
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04214/x3.png",
                "caption": "Figure 3:Qualitative model comparison. We show audio generation results for our method and the baselines, each of which is conditioned on an image, text, or segmentation mask.",
                "position": 869
            },
            {
                "img": "https://arxiv.org/html/2506.04214/x4.png",
                "caption": "Figure 4:Visualization results. We visualize the difference between attention maps and segmentation masks using images from Places(Zhou et al.,2017)and text prompts from BLIP(Li et al.,2022a).",
                "position": 1068
            },
            {
                "img": "https://arxiv.org/html/2506.04214/x5.png",
                "caption": "Figure 5:Interactive audio generation. Our model generates object-specific sounds in the city (left) and beach (right) scenes, and composes a complete soundscape when one or more objects are selected.",
                "position": 1074
            },
            {
                "img": "https://arxiv.org/html/2506.04214/x6.png",
                "caption": "Figure 6:Generating soundscapes from visual texture changes.. We generate different soundscapes by manipulating the visual textures of the same scene, such as changing weather (left) or materials (right).",
                "position": 1077
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AResults Video",
        "images": []
    },
    {
        "header": "Appendix BDataset Preprocessing Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04214/x7.png",
                "caption": "Figure 7:Distribution of matching scores.We present the scores for audio-visual pairs in the AudioSet.",
                "position": 2664
            },
            {
                "img": "https://arxiv.org/html/2506.04214/x8.png",
                "caption": "Figure 8:Model architecture of audio-visual matching.We train a model to quantify the correspondence between a video and its corresponding soundtrack.",
                "position": 2667
            },
            {
                "img": "https://arxiv.org/html/2506.04214/x9.png",
                "caption": "Figure 9:Prompt for Llama. We extract common features between the audio and visual caption using Llama, ensuring the resulting caption focuses on events present in both modalities while avoiding overly specific details.",
                "position": 2679
            },
            {
                "img": "https://arxiv.org/html/2506.04214/x10.png",
                "caption": "Figure 10:Categorical distribution of the filtered AudioSet.We show top 8 categories derived from AudioSet annotations.",
                "position": 2691
            }
        ]
    },
    {
        "header": "Appendix CAdditional Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.04214/extracted/6509998/figs/mturk.png",
                "caption": "Figure 11:Human evaluation interface.We show the interface used for the subjective evaluation of generated audio samples. Participants are presented with input text, an image, and a corresponding audio sample, and are instructed to rate the audio on four criteria. All ratings must be completed before advancing to the next sample.",
                "position": 2724
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    },
    {
        "header": "Appendix EAdditional Dataset Evaluations",
        "images": []
    },
    {
        "header": "Appendix FProof of Theorem 3.1",
        "images": []
    }
]