[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05618/x1.png",
                "caption": "Figure 1:Reward design for Long-Form Factuality (bottom).\nUnlike other tasks (top), the factuality of long-form responses cannot be reliably assessed by rule-based heuristics or an LLM judge.\nRelying solely on automatic evaluation methods such as VeriScore may lead to lessdetailedorrelevantresponses.\nWe propose a new reward design that simultaneously considers factual precision, response detail level, and answer relevance (Section3.1).",
                "position": 177
            }
        ]
    },
    {
        "header": "2Offline Training for Factual Reasoning",
        "images": []
    },
    {
        "header": "3Online Training for Factual Reasoning",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05618/x2.png",
                "caption": "Table 1:Factual Reasoning results on six long-form factuality benchmarks: LongFact, FAVA, AlpacaFact, Biography, Factory-Hard and FactBench-Hard.Reasoning modelsare indicated with a background color.\nPrecision (Prec.) is calculated by dividing the number of correct facts (supported claims) by the total number of facts generated in a model response. Detail level (Dtl.) is measured by the number of supported claims in the response.\nThe win rate (WR) is calculated with respect to the non-reasoning base model for each model.",
                "position": 470
            }
        ]
    },
    {
        "header": "5Ablations and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.05618/x2.png",
                "caption": "Table 2:Ablation results on the reward function design. Factuality, Detail, and Relevance evaluations are reported as the averages on the six benchmarks.",
                "position": 558
            },
            {
                "img": "https://arxiv.org/html/2508.05618/x2.png",
                "caption": "Figure 2:Length trajectory of the CoT reasoning traces during Factual Reasoning training.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2508.05618/x2.png",
                "caption": "Figure 2:Length trajectory of the CoT reasoning traces during Factual Reasoning training.",
                "position": 606
            },
            {
                "img": "https://arxiv.org/html/2508.05618/x3.png",
                "caption": "Figure 3:CoT length distribution on 3920 training prompts.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2508.05618/x4.png",
                "caption": "Figure 4:Top 20 commonly used reasoning strategies based on 3920 training prompts.",
                "position": 617
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Prompt for Generating Synthetic Training Prompts",
        "images": []
    },
    {
        "header": "9Few-shot Prompt for Generating Long CoT Responses",
        "images": []
    },
    {
        "header": "10Prompt for the LLM Judge in the Answer Relevance Reward",
        "images": []
    },
    {
        "header": "11Prompt for LLM-based Meta-Reasoning Strategy Identification",
        "images": []
    }
]