[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17091/extracted/6423864/fig2.png",
                "caption": "",
                "position": 51
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Dataset",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17091/extracted/6423864/fig3.png",
                "caption": "Figure 1:Two popular AudioLLM architectures are shown here with figures taken from respective papers LTU-AS[32]and Qwen-Audio[33]for illustration. Red highlights the block that we replaced in this work. We propose learning an audio encoder, typically a classifier/pre-trained model that takes in an audio representation and gives an embedding vector using a pre-trained text-LLM.",
                "position": 91
            }
        ]
    },
    {
        "header": "4Results And Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.17091/extracted/6423864/fig1.png",
                "caption": "Figure 2:Results for FSD-50K showing performance improvement with scale. All of the four models share the same learnable front end that patches the waveform and learns embedding for the first layer of Transformer stack. Audio Transformer trains everything from scratch whereas others use GPT-2 weights pretrained on next text token prediction tuned LORA for various model sizes.",
                "position": 219
            }
        ]
    },
    {
        "header": "5Conclusion And Future Work",
        "images": []
    },
    {
        "header": "REFERENCES",
        "images": []
    }
]