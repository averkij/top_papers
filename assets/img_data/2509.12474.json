[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12474/x1.png",
                "caption": "Figure 1:(a) Discrepancy between reconstruction and generation task imposes a latent token distribution difference between them. Specifically, reconstruction always rely on true tokens whereas generation task always sample out-of-distribution (OOD) tokens. To resolve this problem, we propose RobusTok (b) to enhance the robustness of tokenizer during main-trainig by latent perturbation, and (c) align the generated latent space with its target image in post-training stage.",
                "position": 151
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3Preliminary - Tokenizer Robustness",
        "images": []
    },
    {
        "header": "4RobusTok",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12474/x2.png",
                "caption": "Figure 2:RobusTok overview. We adopt vision transformer as our encoder‚Ñ∞\\mathcal{E}and decoderùíü\\mathcal{D}.Œ≤\\betaof data in one batch will process our Latent Perturbation, which will be randomly replaced by top-Œ¥\\deltaneighbor from codebook with probabilityŒ±\\alpha. A frozen DINO encoder is utilized to supervise our latent space.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x3.png",
                "caption": "Figure 3:Visualization of (a) traditional tokenizer, (b) semantic tokenizer, and (c) our RobusTok in reconstruction task with Latent Perturbation. Non-semantic tokenizer leads to distorted reconstructions when perturbations are introduced while our method shows promising robustness to those perturbations.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x4.png",
                "caption": "Figure 4:Generated images under differentœÉ\\sigmafor (left) autoregressive and (right) diffusion model.",
                "position": 371
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12474/x5.png",
                "caption": "(a)rFIDvs. gFID with and without CFG.",
                "position": 407
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x5.png",
                "caption": "(a)rFIDvs. gFID with and without CFG.",
                "position": 410
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x6.png",
                "caption": "(b)pFIDvs. gFID with and without CFG.",
                "position": 416
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x7.png",
                "caption": "Figure 6:Maximum Mean Discrepency (MMD) between generated andŒ±\\alpha-perturbed latent.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x8.png",
                "caption": "Figure 7:T-SNE visualization of latent space of tokenizer trained with and without latent perturbation. Colors and thresholds represent the frequency of tokens being used during inference without perturbation.",
                "position": 839
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x9.png",
                "caption": "Figure 8:Visualization of256√ó256256\\times 256image generation before (top) and after (bottom) post-training. Three improvements are observed: (a) OOD mitigation, (b) color fidelity, and (c) detail refinement.",
                "position": 847
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12474/x10.png",
                "caption": "Figure 9:T-SNE visualization of latent space in baseline with varying codebook sizes setting: (a) 1024, (b) 4096, and (c) 8192. Each subfigure presents embeddings derived from (left) 1,000, (middle) 10,000, and (right) 50,000 samples from the ImageNet validation set. Compared to larger codebook sizes, XQGAN-1024 fails to maintain a well-structured latent space, leading to increased fragmentation and reduced robustness.",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2509.12474/figure/DINO-pixel.png",
                "caption": "T-SNE visualization of DINO Pixel features.",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2509.12474/figure/DINO-pixel.png",
                "caption": "T-SNE visualization of DINO Pixel features.",
                "position": 1128
            },
            {
                "img": "https://arxiv.org/html/2509.12474/figure/DINO-class.png",
                "caption": "T-SNE visualization of DINO Class features.",
                "position": 1134
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x11.png",
                "caption": "(b) Visualization of gFID trends forRAR,XQGAN, andOurswith (left) and without (right) CFG.",
                "position": 1151
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x11.png",
                "caption": "(b) Visualization of gFID trends forRAR,XQGAN, andOurswith (left) and without (right) CFG.",
                "position": 1154
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x12.png",
                "caption": "(a) RAR training loss (left) and accuracy (right) forNone,Half, andZeroannealing strategies.",
                "position": 1160
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x13.png",
                "caption": "(a)rFIDvs. gFID with and without CFG.",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x13.png",
                "caption": "(a)rFIDvs. gFID with and without CFG.",
                "position": 1978
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x14.png",
                "caption": "(b)pFIDvs. gFID with and without CFG.",
                "position": 1984
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x15.png",
                "caption": "Figure 13:Detailed t-SNE visualization of latent space of tokenizer training with and without our proposed latent perturbation.",
                "position": 1998
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x16.png",
                "caption": "Figure 14:qualitative analysis of tokenizers in our latent perturbation.",
                "position": 2001
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x17.png",
                "caption": "Figure 15:Visualization of 256 √ó 256 image within ImageNet class.",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2509.12474/x18.png",
                "caption": "Figure 16:More visualization for the improvement of tokenizer post-training in failed case.",
                "position": 2221
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]