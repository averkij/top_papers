[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13585/x1.png",
                "caption": "Figure 1:Left: Benchmark performance comparison of leading commercial and open-weight models across competition-level mathematics, coding, software engineering, agentic tool use, and long-context understanding tasks. We use the MiniMax-M1-80k model here for MiniMax-M1.Right: Theoretical inference FLOPs scaling with generation length (# tokens).",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x1.png",
                "caption": "",
                "position": 91
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x2.png",
                "caption": "",
                "position": 95
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Preparation for Scalable RL: Continual Pretraining and SFT",
        "images": []
    },
    {
        "header": "3Efficient RL Scaling: Algorithms and Lightning Attention",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13585/x3.png",
                "caption": "Figure 2:Comparison of GRPO, DAPO, and our proposed CISPO on AIME 2024, based on Qwen2.5-32B-base. CISPO outperforms both GRPO and DAPO in terms of performance at the same number of training steps, and achieves comparable performance to DAPO using 50% of the training steps.",
                "position": 260
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x4.png",
                "caption": "Figure 3:Probability of tokens in training-mode code vs. probability of tokens in inference-mode code. Each point in the figures represents an individual token. The Pearson correlation coefficient is indicated in the figures. Theoretically, the two probabilities should be identical, and all the tokens should be exactly on the diagonal line.Left:Correlation of the M1 model before our fix;Right:Correlation of the M1 model after applying our fix of using FP32 precision for the LM output head.",
                "position": 390
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x4.png",
                "caption": "",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x5.png",
                "caption": "",
                "position": 397
            }
        ]
    },
    {
        "header": "4Scaling Reinforcement Learning with Diverse Data",
        "images": []
    },
    {
        "header": "5Extending RL Scaling to Longer Thinking",
        "images": []
    },
    {
        "header": "6Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.13585/x6.png",
                "caption": "Figure 4:Accuracy and generation length versus RL training steps for MiniMax-M1.",
                "position": 1117
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x6.png",
                "caption": "",
                "position": 1120
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x7.png",
                "caption": "",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2506.13585/x8.png",
                "caption": "",
                "position": 1128
            }
        ]
    },
    {
        "header": "7Conclusion and Future work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContributors",
        "images": []
    }
]