[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21204/x1.png",
                "caption": "Figure 1:Inner-Loop Optimization vs. Performance. Increasing inner-loop iterations improves inner-loop loss but degrades task performance, contradicting the memorization-based interpretation of TTT. Experiments are based on LaCT(Zhanget al.,2025).",
                "position": 294
            }
        ]
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Empirical Contradictions to Memorization",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21204/x2.png",
                "caption": "Figure 2:Distributional Asymmetry BetweenQQandKK.t-SNE visualizations of(Q,K)(Q,K)and(V,O)(V,O)features in a pretrained LaCT(Zhanget al.,2025)model on the NVS task, showing that the TTT inner loop is evaluated out of distribution and thus does not perform reliable retrieval.",
                "position": 415
            }
        ]
    },
    {
        "header": "5TTT is secretly Linear Attention",
        "images": []
    },
    {
        "header": "6Practical Implications",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.21204/x3.png",
                "caption": "Figure 3:Perplexity Metric for Ablation om LaCT-LLM.Evaluated on 2.5B tokens from the Book-3 dataset.",
                "position": 920
            },
            {
                "img": "https://arxiv.org/html/2602.21204/x4.png",
                "caption": "Figure 4:Training loss vs. wall-clock time on LaCT-LLM.We compare the original LaCT-TTT with both parallel and recurrent form of Variant 2.\nThe parallel form achieves a1.19Ã—1.19\\timesend-to-end speedup while maintaining comparable convergence.",
                "position": 963
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Setup",
        "images": []
    },
    {
        "header": "Appendix BProof of Theorem5.1",
        "images": []
    },
    {
        "header": "Appendix CProof of Theorem5.2",
        "images": []
    },
    {
        "header": "Appendix DProof of Theorem5.3",
        "images": []
    },
    {
        "header": "Appendix EDerivation: LaCT as Linear Attention",
        "images": []
    },
    {
        "header": "Appendix FDerivation: ViTTT GLU as Linear Attention",
        "images": []
    },
    {
        "header": "Appendix GDerivation: ViTTT Depthwise Convolution as Linear Attention",
        "images": []
    },
    {
        "header": "Appendix HParallel Form of TTT",
        "images": []
    },
    {
        "header": "Appendix INon-Reducible Case Analysis",
        "images": []
    }
]