[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12751/x1.png",
                "caption": "Figure 1:Overall framework ofGenieDrive.OurGenieDriveadopts a two-stage generation pipeline that first predicts future occupancy and then generates multi-view driving videos. In the occupancy generation stage, the current occupancy is encoded using a tri-plane VAE and processed by our Mutual Control Attention (MCA). The predicted occupancy is rendered into multi-view semantic maps, which are then fed into the DiT blocks enhanced by our Normalized Multi-View Attention (MVA) module to produce the final driving videos.",
                "position": 152
            }
        ]
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12751/x2.png",
                "caption": "Figure 2:Comparison of Trajectory-Controlled Driving Video Generation.Our method can generate physics-aware future frames for the trajectoriesTurn Left,Go Straight, andTurn Right. In contrast, Vista[16]and Epona[67]struggle withTurn LeftandTurn Right.",
                "position": 302
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12751/x3.png",
                "caption": "Figure 3:Occupancy guided physics-aware multi-view video generation.Given the same initial occupancy and frames, our model can generatively predict the future occupancy based on driving controls. Then we use these generated occupancies to guide the video generation. We display the simulated physics-aware futures forgo straight,turn rightandturn left.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x4.png",
                "caption": "Figure 4:(a) Comparison with UniScene[30].Our method generates longer driving videos while maintaining high quality.(b) Ablation Study.Removing normalization during fine-tuning results in noticeable grid artifacts and blurry outputs, while removing the MVA leads to multi-view inconsistent generation.(c) Driving Scenario Editing.With our two-stage generation method, edits such as removal or insertion can be easily applied to the occupancy, allowing for the generation of edited driving videos.",
                "position": 851
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "6Implementation Details of Tri-Plane VAE",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12751/x5.png",
                "caption": "Figure 5:Concatenated Tri-Plane Feature.To make tri-plane representation more suitable for the following processing, we concatenate three planes to get a unified feature representation.",
                "position": 2032
            }
        ]
    },
    {
        "header": "7Details of 4D Occupancy Forecasting",
        "images": []
    },
    {
        "header": "8Qualitative Comparison on Forecasting",
        "images": []
    },
    {
        "header": "9End-to-End Training of Occupancy World Model",
        "images": []
    },
    {
        "header": "10Efficiency of Video Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.12751/sup_figures/cost.png",
                "caption": "Figure 6:Comparison of Video Generation Efficiency.We compare our method with previous approaches in terms of training time, number of GPUs used for training, inference time, and VRAM consumption during inference.",
                "position": 2142
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x6.png",
                "caption": "Figure 7:Qualitative Comparison of 4D Occupancy Forecasting.We highlight the differences using bounding boxes. Previous methods tend to produce unreasonable predictions or miss important details such as pedestrians. In contrast, our method generates physically reasonable results while preserving the detailed structures of the driving scene.",
                "position": 2212
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x7.png",
                "caption": "Figure 8:Qualitative Comparison Before and After End-to-End Training.We highlight the differences using bounding boxes. The visualization results show that end-to-end training helps our occupancy world model forecastpedestrians,cars,trucks,trailers, androadway featuresmore accurately. In contrast, the variant without end-to-end training tends to lose these details in its predictions.",
                "position": 2218
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x8.png",
                "caption": "Figure 9:Effect of End-to-End Training on the Comparison Methods.We visualize the impact of end-to-end (E2E) training on the comparison methods DOME[17]andI2I^{2}-World[32]by presenting the ground truth along with their predictions before and after E2E training. For DOME, the forecasting capability completely breaks down after E2E training. ForI2I^{2}-World, E2E training fails to produce more accurate forecasts and further leads to noticeable loss of scene details.",
                "position": 2224
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x9.png",
                "caption": "Figure 10:Long Video Generation Examples.We provide two examples of our generated 20-second multi-view driving videos: one captured under daytime conditions and the other at night. Our method maintains both generation quality and multi-view consistency even over such long 20-second sequences.",
                "position": 2230
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x10.png",
                "caption": "Figure 11:Driving Scenes Editing.We visualize the editing process applied to driving videos. BothRemovalandInsertionoperations take effect progressively over time. Compared with the original video, the edited results demonstrate that our method can effectively remove and insert objects within driving scenes.",
                "position": 2235
            },
            {
                "img": "https://arxiv.org/html/2512.12751/x11.png",
                "caption": "Figure 12:Sim-to-Real Generation.The left side shows the BEV map of simulated driving scenes in the CARLA simulator. Our method possesses the ability to transform these simulated scenarios into realistic multi-view driving videos. The visualization results demonstrate that our method not only generates accurate ego-vehicle behaviors, such asleft turnsandovertaking, but also preserves important scene details, including surrounding vehicles highlighted with red boxes.",
                "position": 2240
            }
        ]
    },
    {
        "header": "11More Video Generation Results",
        "images": []
    }
]