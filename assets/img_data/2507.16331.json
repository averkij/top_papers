[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16331/extracted/6642347/logo_hf.png",
                "caption": "",
                "position": 238
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16331/x1.png",
                "caption": "Figure 1.1:Model Performance on DafnyComp benchmark.\nWe roughly categorize the cases into four groups in an ascending quality order:Syntax Error,Syntax Correct,VerifiedandVerified with Superior Specification.\nThe growing proportion ofVerified with Superior Specificationsuggests a rudimentary of exploration capabitliy to generate stronger specifcation than the ground-truth of the models incentivized by reinforcement learning.\nFor further detailed model behavioural analysis, a more refined set of definition of benchmarking metrics is provided in ¬ß3.1as standard protocol.",
                "position": 317
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x2.png",
                "caption": "Figure 1.2:The Illustration of Re:Form Pipeline. Human prior is extensively removed across different components of the pipeline.\nHeuristic cleansing rules and model-based conversion are introduced in the data construction and benchmark annoation for scaling along with compute investment.\nThe task is formalized as a simple and flexible specification generation, providing the model a vast landscape of self-exploration under the reinforcement learning paradigm.",
                "position": 385
            }
        ]
    },
    {
        "header": "2Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16331/x3.png",
                "caption": "Table 2.1:Statistics of the Dataset.",
                "position": 434
            }
        ]
    },
    {
        "header": "3Results and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16331/x4.png",
                "caption": "Figure 3.2:The figure shows the comparison between GPT-4o, our Qwen base models, SFT models and RL-trained models scaling over model size on our in-domain evaluation set. The pass@1 improvement of SFT and subsequent RL over our base models is substantial.",
                "position": 1077
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x5.png",
                "caption": "Figure 3.3:The figure reports SFT and RL performance with128128128128rollouts. The plotted rate measures whether at least one rollout attains the corresponding reward. RL yields a clear improvement from SFT, indicating genuine quality gains rather than mere compression of rollouts.",
                "position": 1080
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x6.png",
                "caption": "Figure 3.5:Our14141414B RL model dominates the pass@‚Å¢1@1@1@ 1performance over SFT and GPT-4o, the best performing proprietary LLM other than our data-generator, Claude. Notably, GPT-4o attains the best score on DafnyBench, highlighting an asymmetry toward that benchmark.",
                "position": 1182
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x7.png",
                "caption": "Figure 3.6:These figures present the training curves for different reward schemes and regularization choices. The left figure shows that using the subset reward stops the quality drop, demonstrated by the spec superiority rate. The right figure shows that entropy regularization leads to instability in training, and all regularization choices show similar pass@1 performance before crashing.",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x8.png",
                "caption": "",
                "position": 1206
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x9.png",
                "caption": "Figure 3.7:This figure compares the spec superiority rate (SSR) among three RL configurations: the subset reward with entropy only, with KL divergence added only and without any regularization. Adding entropy regularization is the key to an increasing pass@‚Å¢128@128@128@ 128performance, which injects stochasticy and thus encourages exploration. The combination with KL divergence can further improve the performance, and thus, we stick to this configuration.",
                "position": 1226
            }
        ]
    },
    {
        "header": "4Conclusion and Discussion",
        "images": []
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AAuthorship",
        "images": []
    },
    {
        "header": "Appendix BTechinical Details and Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.16331/x10.png",
                "caption": "Figure C.1:Training curves with0.50.50.50.5B,1.51.51.51.5B and3333B models for verification reward model, subset reward model without regularization, subset reward model with KL and entropy, and our \"explore variant\". Here, our \"explore variant\" is trained under the syntax and subset reward without optimizing the verification reward or adding any regularization, but gives the highest exploration scores shown in the next Section.",
                "position": 3754
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x11.png",
                "caption": "Figure C.2:Training curves with7777B and14141414B models for verification reward model, subset reward model without regularization, and subset reward model with KL and entropy.",
                "position": 3757
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x12.png",
                "caption": "Figure C.4:Left:Novel specification generation rate versus rollout count across different models. The SFT model yields zero novel specifications and serves as a baseline.Right:Diversity score (measured as embedding variance) versus rollout count for the same models. These plots illustrate how novelty and diversity evolve with increasing rollouts. All models with subset rewards shown here are trainedwithoutthe verification reward.",
                "position": 3886
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x13.png",
                "caption": "",
                "position": 3889
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x14.png",
                "caption": "Figure C.5:Left:Novel specification generation rate versus rollout count across different models. The SFT model yields zero novel specifications and serves as a baseline.Right:Diversity score (measured as embedding variance) versus rollout count for the same models. These plots illustrate how novelty and diversity evolve with increasing rollouts. All models with subset rewards shown here are trainedwiththe verification reward.",
                "position": 3894
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x15.png",
                "caption": "",
                "position": 3897
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x16.png",
                "caption": "Figure C.6:Left:Scatter plot of spec superiority rate versus novel specification rate.Right:Scatter plot of diversity score versus spec superiority rate. Each data point corresponds to a rollout group. Different colors indicate different models. Pearson correlation coefficients (rùëüritalic_r) are computed separately for each model.",
                "position": 3910
            },
            {
                "img": "https://arxiv.org/html/2507.16331/x17.png",
                "caption": "",
                "position": 3913
            }
        ]
    },
    {
        "header": "Appendix CExperimental Results and Analysis",
        "images": []
    }
]