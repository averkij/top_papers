[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/cc_red.png",
                "caption": "",
                "position": 67
            },
            {
                "img": "https://arxiv.org/html/2504.16030/x1.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16030/x2.png",
                "caption": "Figure 2:LiveCC data production pipeline. We begin by integrating several large-scale YouTube video datasets[88,96,89,60,105], followed by metadata filtering, resulting in a curated pool of 5.7M videos. Then, the pre-training dataset is built using the original YouTube CC, while the SFT dataset leverages higher-quality ASR transcriptions generated by WhisperX[7,72]. We also introduce a set of efficient filtering techniques to improve the SFT data quality. Please refer to Section3.1for details.",
                "position": 124
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/pretrain_statistics.png",
                "caption": "Figure 3:Overview of our proposed Live-CC-5M dataset.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/sft_category.png",
                "caption": "(a)Statistics of our proposed Live-WhisperX-526K dataset.",
                "position": 167
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/sft_category.png",
                "caption": "(a)Statistics of our proposed Live-WhisperX-526K dataset.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2504.16030/x3.png",
                "caption": "(b)An example of ASD removal in SFT data pipeline.",
                "position": 176
            },
            {
                "img": "https://arxiv.org/html/2504.16030/x4.png",
                "caption": "(c)An example from the Live-WhisperX-526K dataset.",
                "position": 182
            },
            {
                "img": "https://arxiv.org/html/2504.16030/x5.png",
                "caption": "Figure 5:Modeling Overview of LiveCC.The model processes streaming video frames through a visual encoder to produce visual tokens while assigning ASR text from corresponding frame intervals as text tokens. The LLM autoregressively predicts text tokens within this densely interleaved token sequence. To mitigate learning ambiguity, additional context of preceding ASR text or video title is provided during pre-training. During SFT, the context part is only user query to match the real-world applications.",
                "position": 189
            },
            {
                "img": "https://arxiv.org/html/2504.16030/x6.png",
                "caption": "Figure 6:(a)Category Distribution of the LiveSports Benchmark: The benchmark includes 3k live CCs and MCQs, split into two tracks: CC and QA.(b)Event Duration and ASR Word Count in the CC track: For CC, event duration (left y-axis) and ASR word count (right y-axis) are analyzed, with durations categorized as short, medium, and long.(c)Question Count by Type in the QA track: Questions are grouped into three query types, with additional tracking of those requiring OCR for each type.(d)Sample CCs and Query Types.",
                "position": 255
            }
        ]
    },
    {
        "header": "4The LiveSports-3K Benchmark",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16030/x7.png",
                "caption": "Figure 7:Comparison of pre-trained and instruction tuning enhanced modelâ€™s predictions on the same video.This example is sourced from Video-MME[23], with the YouTube IDwhksDmTR9YEfeaturing animal fights.",
                "position": 1107
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16030/x8.png",
                "caption": "Figure 8:The prompts used during the pre-training instruction-tuning (aka. SFT) stages. CM represents commentary, QA denotes question-answering. For pre-training and instruction-tuning, the previous ASR texts are concatenated to form the context for the live commentary task if they are available. Otherwise, the context is formed by the video title. These contexts are masked during loss calculation. Note that QA data is incorporated exclusively during the instruction-tuning stage. As for inference, we remove the groundtruth in the prompts,i.e., the words followed by a frame or the answer to a multiple-choice question.",
                "position": 2589
            }
        ]
    },
    {
        "header": "7Demo",
        "images": []
    },
    {
        "header": "8Implemetation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.16030/x9.png",
                "caption": "Figure 9:The comparison between the commentary generated by LLaVA-Video-72B and our LiveCC-7B-Instruct.",
                "position": 2718
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/MCWJNOfJoSM_1.png",
                "caption": "(a)Video Time: 8.6s",
                "position": 2731
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/MCWJNOfJoSM_1.png",
                "caption": "(a)Video Time: 8.6s",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/MCWJNOfJoSM_2.png",
                "caption": "(b)Video Time: 30.3s",
                "position": 2740
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/MCWJNOfJoSM_3.png",
                "caption": "(c)Video Time: 51.2s",
                "position": 2746
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/MCWJNOfJoSM_4.png",
                "caption": "(d)Video Time: 77.2s",
                "position": 2752
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/lcZTcfdZ3Ow_1.png",
                "caption": "(a)Video Time: 2.6s",
                "position": 2759
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/lcZTcfdZ3Ow_1.png",
                "caption": "(a)Video Time: 2.6s",
                "position": 2762
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/lcZTcfdZ3Ow_2.png",
                "caption": "(b)Video Time: 13.3s",
                "position": 2768
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/lcZTcfdZ3Ow_3.png",
                "caption": "(c)Video Time: 31.6s",
                "position": 2774
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/lcZTcfdZ3Ow_4.png",
                "caption": "(d)Video Time: 45.0s",
                "position": 2780
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/8XajZdrCDsk_1.png",
                "caption": "(a)Video Time: 4.3s",
                "position": 2787
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/8XajZdrCDsk_1.png",
                "caption": "(a)Video Time: 4.3s",
                "position": 2790
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/8XajZdrCDsk_2.png",
                "caption": "(b)Video Time: 24.8s",
                "position": 2796
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/8XajZdrCDsk_3.png",
                "caption": "(c)Video Time: 43.8s",
                "position": 2802
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/8XajZdrCDsk_4.png",
                "caption": "(d)Video Time: 59.8s",
                "position": 2808
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/115amzVdV44_1.png",
                "caption": "(a)Video Time: 4.6s",
                "position": 2815
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/115amzVdV44_1.png",
                "caption": "(a)Video Time: 4.6s",
                "position": 2818
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/115amzVdV44_2.png",
                "caption": "(b)Video Time: 15.6s",
                "position": 2824
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/115amzVdV44_3.png",
                "caption": "(c)Video Time: 44.0s",
                "position": 2830
            },
            {
                "img": "https://arxiv.org/html/2504.16030/extracted/6378277/fig/demo/115amzVdV44_4.png",
                "caption": "(d)Video Time: 72.8s",
                "position": 2836
            }
        ]
    },
    {
        "header": "9Additional Experiments",
        "images": []
    }
]