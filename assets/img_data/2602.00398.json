[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00398/x1.png",
                "caption": "Figure 1:Architecture comparison of Conventional Transformer (base\\mathrm{base}) v/s MemoryLLM with Residual Stream Perspective:(a) FFN input in conventional transformers is a sequential and non-interpretable latent snapshot of a residual stream, including prior self-attention module output; (b) MemoryLLM decouples FFNs across all transformer blocks completely from self-attention modules and trains them in isolation of the residual stream, directly on token-indexed input embeddings.",
                "position": 148
            }
        ]
    },
    {
        "header": "2MemoryLLM: LLMs with Interpretable Token-Indexed Feed-Forward Memory",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00398/x2.png",
                "caption": "Figure 2:TKV Framework:Input text is tokenized into discrete token IDs as context-free query vectors for FFN memory cells.WU​pW_{Up}andWD​o​w​nW_{Down}projection matrices emulate the behavior ofKeysandValueswhile theWG​a​t​eW_{Gate}matrix can be interpreted as a reweighting function of token memory coefficients.",
                "position": 294
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x3.png",
                "caption": "Figure 3:FFNs as Pre-computed Token-wise Lookups:Outputs corresponding to each vocabulary tokens for all FFN modules acrossNNtransformer blocks can be pre-computed offline and stored as static token-indexed lookups (ToLs) in storage devices.",
                "position": 332
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x4.png",
                "caption": "Figure 4:Percentage increase in perplexity when FFN computation for layerLLis dropped inBase\\mathrm{Base}and MemoryLLM.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x5.png",
                "caption": "Figure 5:Semantically Similar Tokens Build Memory Outputs with Similar Keys:t-SNE plot with K-Means clustering ofckc_{k}vectors, which represent each key’s contribution to memory outputs, yields clusters of tokens with semantically similar properties.",
                "position": 380
            }
        ]
    },
    {
        "header": "3Empirical Study of FFN Neural Memory",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00398/x6.png",
                "caption": "Figure 6:(a) Clustering coefficient forckc_{k}vectors from FFN memory across 24 layers of MemoryLLM-1B. (b) Average outliers count inckc_{k}vectors corresponding to each vocabulary token.",
                "position": 401
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x7.png",
                "caption": "Figure 7:Model performance comparison with regulated contribution of FFNs in MemoryLLM andbase\\mathrm{base}checkpoints.",
                "position": 414
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x8.png",
                "caption": "Figure 8:Architecture comparison of MemoryLLM & Flex-MemoryLLM with exactly same total number of parameters.",
                "position": 505
            }
        ]
    },
    {
        "header": "4MemoryLLM: Performance and Efficiency",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00398/x9.png",
                "caption": "Figure 9:Performance comparison of conventional LLMbase\\mathrm{base}with MemoryLLM and Flex-MemoryLLM models with∼1\\sim 1Btotalparameter count and varyingactiveparameter counts. Training is performed with same recipes & equal token (150B) counts for fairness.",
                "position": 592
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x10.png",
                "caption": "Figure 10:Performance comparison of conventional dense LLMs with MemoryLLM and variants of Flex-MemoryLLM at different scale of parameter training tokens and model parameters with same training recipes for fair comparison.",
                "position": 598
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x11.png",
                "caption": "Figure 11:PPL comparison of MemoryLLM, Flex-MemoryLLM, and LLM pruning methods wrt. active parameters.",
                "position": 608
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AImplementation Details",
        "images": []
    },
    {
        "header": "Appendix BUnderstanding Decoding Cost with ToLs",
        "images": []
    },
    {
        "header": "Appendix CBackground Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.00398/x12.png",
                "caption": "Figure 12:Performance change when layerLLToL is dropped in MemoryLLM-1B. Each subplot is a task; within each subplot, the y-axis is the task performance and the x-axis the is the layer of the dropped.",
                "position": 1200
            },
            {
                "img": "https://arxiv.org/html/2602.00398/x13.png",
                "caption": "Figure 13:Normalized and sorted 2048 singular values of the ToLs corresponding to different 24 layers of MemoryLLM and Flex-MemoryLLM models with 1B scale.",
                "position": 1206
            }
        ]
    },
    {
        "header": "Appendix DUnderstanding Storage Challenges of ToLs",
        "images": []
    }
]