[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08396/x1.png",
                "caption": "Figure 1:Comparison of subject-consistent generation methods: Vanilla SDXLpodell2023sdxl, ConsiStorytewel2024training, StoryDiffusionzhou2024storydiffusionandCoDi(ours).(a&b)Existing methods sacrifice pose diversity for subject consistency,e.g., ConsiStory produces similar poses in Figure1(a); and the lower right with hands placed in front in Figure1(b). In contrast,CoDigenerates consistent subjects, while matching the pose diversity of Vanilla SDXL.(c)We report two metrics to assess pose quality: 1) fidelity, measuring the distance to the pose of SDXL, and 2) diversity (see Sec.4.1for details). OurCoDiattains the best performance on both metrics.",
                "position": 72
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08396/x2.png",
                "caption": "Figure 2:Illustration of ourCoDi.(a)Extract subject masks (Mğ—‚ğ–½subscriptğ‘€ğ—‚ğ–½M_{\\mathsf{id}}italic_M start_POSTSUBSCRIPT sansserif_id end_POSTSUBSCRIPTandMnsubscriptğ‘€ğ‘›M_{n}italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT) by averaging the image-text cross-attentionat the final denoising timestepfor subject-related tokens (e.g.,â€˜â€˜fairyâ€™â€™).(b)Compute the OT planTnsubscriptğ‘‡ğ‘›T_{n}italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPTusing the cost matrixCğ¶Citalic_Cand the probability massesğšğš\\mathbf{a}bold_aandğ›ğ›\\mathbf{b}bold_b(detailed in Sec.3.2).(c)Identity transport (IT) operates in the early denoising steps to\ntransfer reference subject features to targe images in a pose-aware manner.(d)Identity refinement (IR) operates in the late denoising steps to refine subject details using selective cross-image attention mechanism.",
                "position": 170
            },
            {
                "img": "https://arxiv.org/html/2507.08396/x3.png",
                "caption": "Figure 3:Qualitative comparisonamong Vanilla SDXLpodell2023sdxl, ConsiStorytewel2024training, StoryDiffusionzhou2024storydiffusion, and 1-Prompt-1-Storyliu2025one. ConsiStory and StoryDiffusion generate similar poses across examples, while 1-Prompt-1-Story preserves pose diversity but struggles with subject consistency. In contrast, ourCoDiachieves both.",
                "position": 316
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08396/x4.png",
                "caption": "Figure 4:Main component analysis (qualitative)on identity transport (IT) and identity refinement (IR).ITenhances subject consistency in thecoarse-grained leveland preserves pose diversity.IRenhances subject consistency in thefine-grained levelreduces pose diversity. Their combination yields the best consistency and preserves diversity.",
                "position": 440
            },
            {
                "img": "https://arxiv.org/html/2507.08396/x5.png",
                "caption": "Figure 5:Ablation studies on(a)stage transition point, and(b)the effect ofÎ±ğ›¼\\alphaitalic_Î±.",
                "position": 529
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Evaluation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08396/x6.png",
                "caption": "Figure 6:Pose diversity scores across different confidence thresholdsÏ„ğœ\\tauitalic_Ï„. OurCoDiconsistently outperforms other SCG methods and performs comparably to Vanilla SDXL[26].",
                "position": 1352
            }
        ]
    },
    {
        "header": "Appendix CLimitations",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08396/x7.png",
                "caption": "Figure 7:Limitations. Our method relies on the quality of cross-attention from the pre-trained diffusion model to accurately localize the subject.",
                "position": 1359
            },
            {
                "img": "https://arxiv.org/html/2507.08396/x8.png",
                "caption": "Figure 8:Additional qualitative comparisons. OurCoDiachieves the best trade-off among subject consistency, pose diversity, and prompt fidelity.",
                "position": 1380
            },
            {
                "img": "https://arxiv.org/html/2507.08396/x9.png",
                "caption": "Figure 9:Additional qualitative results generated by ourCoDidemonstrate strong subject consistency and pose diversity.",
                "position": 1384
            },
            {
                "img": "https://arxiv.org/html/2507.08396/x10.png",
                "caption": "Figure 10:Long Story Generation.CoDisupports extended visual storytelling by generating diverse scene compositions while consistently preserving subject identity throughout the sequence.",
                "position": 1387
            }
        ]
    },
    {
        "header": "Appendix DAdditional Results",
        "images": []
    }
]