[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07964/x1.png",
                "caption": "Figure 1:Comparison of OLMoE-1B-7B (1B activated parameters) with C3PO against multiple 7B dense models across six benchmarks. C3PO improves OLMoE-1B-7B‚Äôs accuracy by 7-15%, outperforming 7B models over all benchmarks, validating the efficiency of MoE architecture and C3PO‚Äôs optimization effectiveness.",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2504.07964/x1.png",
                "caption": "Figure 1:Comparison of OLMoE-1B-7B (1B activated parameters) with C3PO against multiple 7B dense models across six benchmarks. C3PO improves OLMoE-1B-7B‚Äôs accuracy by 7-15%, outperforming 7B models over all benchmarks, validating the efficiency of MoE architecture and C3PO‚Äôs optimization effectiveness.",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2504.07964/x2.png",
                "caption": "Figure 2:Pathway optimization in C3PO. For a test sample, C3PO retrieves successful pathways (green arrows) from similar samples in the reference set and adjusts the initial pathway (red arrow) based on them to achieve better prediction.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/layer.png",
                "caption": "Figure 3:Analysis of critical layersin OLMoE (F: early layers, M: middle layers, L: late layers). Optimizing only the last five layers (L5) achieves the highest accuracy, outperforming full-layer optimization (All16) and partial combinations (e.g., F2L3).",
                "position": 346
            },
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/FLOPs.png",
                "caption": "Figure 4:Accuracy-FLOPs Trade-off by changing the number of core experts (nùëõnitalic_n) of OLMoE to optimize by C3PO. The accuracy achieves the greatest boosting atn=8ùëõ8n=8italic_n = 8and plateaus atn=20ùëõ20n=20italic_n = 20, indicating8-20 core experts suffices to retain most gain by pathway optimization.",
                "position": 356
            },
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/FLOPs.png",
                "caption": "Figure 4:Accuracy-FLOPs Trade-off by changing the number of core experts (nùëõnitalic_n) of OLMoE to optimize by C3PO. The accuracy achieves the greatest boosting atn=8ùëõ8n=8italic_n = 8and plateaus atn=20ùëõ20n=20italic_n = 20, indicating8-20 core experts suffices to retain most gain by pathway optimization.",
                "position": 359
            },
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/topn.png",
                "caption": "Figure 5:Average percentage of the top-8 experts (after optimizing all experts) being retained in the top-nùëõnitalic_nexperts identified by pretrained router in OLMoE. The results indicate thatselectingn‚â•20ùëõ20n\\geq 20italic_n ‚â• 20in advance can effectively cover almost all the 8 core experts contributing to performance.",
                "position": 364
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/step_transition.png",
                "caption": "Figure 6:Impact of NGD optimization steps (x-axis) on OLMoE for ARC-C task accuracy for OLMoE. The first 6 steps yield an 11.6% gain (initial 51.3% ‚Üí 62.9%), reaching 66.3% at Step 10. Only 5.1% of initially correct predictions flip, confirming stable and efficient convergence.",
                "position": 1016
            },
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/step_transition.png",
                "caption": "Figure 6:Impact of NGD optimization steps (x-axis) on OLMoE for ARC-C task accuracy for OLMoE. The first 6 steps yield an 11.6% gain (initial 51.3% ‚Üí 62.9%), reaching 66.3% at Step 10. Only 5.1% of initially correct predictions flip, confirming stable and efficient convergence.",
                "position": 1019
            },
            {
                "img": "https://arxiv.org/html/2504.07964/extracted/6347186/figure/heatmap_combined.png",
                "caption": "Figure 7:Heatmap comparison of expert activation frequency in OLMoE‚Äôs last five layers for ARC-C (top: base model, right: C3PO-optimized). Post-optimization, activations concentrate, focusing on high-frequency experts per layer (darker = higher usage), showing C3PO enhances expert specialization and reduces redundancy.",
                "position": 1024
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07964/x3.png",
                "caption": "Figure 8:An example of how C3PO optimizes the expert routing weights. Here we only show the routing weights of the last1 layer. The polyline with red dots represents the original routing weights of the test sample, while the polyline with blue dots represents the optimized routing weights. C3PO optimizes the original routing weights by leveraging similar questions in the reference set, then changing the test sample‚Äôs top-8 experts and their corresponding weights, eventually turning an incorrect answer into a correct one.",
                "position": 1520
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]