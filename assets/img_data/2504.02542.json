[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02542/extracted/6333493/assets/homepage.png",
                "caption": "",
                "position": 74
            },
            {
                "img": "https://arxiv.org/html/2504.02542/extracted/6333493/assets/blog.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2504.02542/extracted/6333493/assets/github.png",
                "caption": "",
                "position": 76
            },
            {
                "img": "https://arxiv.org/html/2504.02542/extracted/6333493/assets/youtube.jpg",
                "caption": "",
                "position": 77
            },
            {
                "img": "https://arxiv.org/html/2504.02542/extracted/6333493/assets/huggingface.png",
                "caption": "",
                "position": 78
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x1.png",
                "caption": "",
                "position": 87
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02542/x2.png",
                "caption": "Figure 2:Illustration of our ACTalker framework. ACTalker takes multiple signals inputs (i.e., audio and visual facial motion) to drive the generation of talking head videos. In addition to the standard layers (e.g., spatial convolution, temporal convolution, spatial attention, and temporal attention) in the stable video diffusion model, we introduce a parallel-control mamba layer to harness the power of multiple signals control. Audio and facial motion signals are fed into this parallel-control mamba layer, along with their corresponding masks, which indicates the regions to focus on for manipulation.",
                "position": 116
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02542/x3.png",
                "caption": "Figure 3:Illustration of parallel-control mamba layer. There are two parallel branches in this layer, one for audio control and the other is for expression control. We utilize a gate in each branch to control the accessing of control signal during training. During inference, we can manually modify the statue of gates to enable single signal control or multiple signals control.",
                "position": 156
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02542/x4.png",
                "caption": "Figure 4:The illustrating of the Mask-SSM in audio branch of parallel-control mamba layer. The visual branch is the same but replace with the motion embedding and motion mask",
                "position": 249
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x5.png",
                "caption": "Figure 5:Comparison of different methods for audio-driven talking head generation. Our method can produce more natural and accurate lip-synced videos. Due to the page limitation, the results of SadTalker[63]and Hallo[56]are reported inSupplementary Material",
                "position": 259
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02542/x6.png",
                "caption": "Figure 6:Comparison of different methods on VFHQ. Self reenactment (first row) and cross reenactment (last row).",
                "position": 480
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x7.png",
                "caption": "Figure 7:Visualization of multiple signals control. Our generated video accurately replicates the lip movements driven by the audio source and captures the head motion—particularly the eye movements and pose—as guided by the motion source. Once we remove the masks in both Mask-SSMs and generate the video using multiple driving signals, the motion source can also affect the mouth movement (“Ours w/o MD”), causing a control conflict.",
                "position": 597
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x8.png",
                "caption": "Figure 8:The visualization of ablation studies driven by audio. Our full method can produce more natural videos.",
                "position": 601
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExperiment Detail",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.02542/x9.png",
                "caption": "Figure 9:The type of masks we used in our framework.",
                "position": 1750
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x10.png",
                "caption": "Figure 10:The results generated by our method under facial motion control.",
                "position": 1754
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x11.png",
                "caption": "Figure 11:The results generated by our method under audio control.",
                "position": 1762
            },
            {
                "img": "https://arxiv.org/html/2504.02542/x12.png",
                "caption": "Figure 12:The results generated by our method under audio-visual joint control.",
                "position": 1767
            }
        ]
    },
    {
        "header": "Appendix BVisualization",
        "images": []
    },
    {
        "header": "Appendix CEthics Considerations and AI Responsibility",
        "images": []
    }
]