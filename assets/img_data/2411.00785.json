[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00785/extracted/5934558/cube.png",
                "caption": "",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2411.00785/x1.png",
                "caption": "Figure 1:Image-GOal Representations¬†(IGOR) based training framework for embodied AI.IGOR learns a unified latent action space for humans and robots by compressing visual changes between an image and its goal state on data from both robot and human activities. By labeling latent actions, IGOR facilitates the learning of foundation policy and world models from internet-scale human video data, covering a diverse range of embodied AI tasks. With a semantically consistent latent action space, IGOR enables human-to-robot generalization. The foundation policy model acts as a high-level controller at the latent action level, which is then integrated with a low-level policy to achieve effective robot control.",
                "position": 182
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00785/extracted/5934558/figures/exp3-5-3_sgsprimegprime-v4.png",
                "caption": "Figure 2:We extract latent actions from Image-Goal pairs in the solid line boxes, and apply the latent actions to different initial frames, generating subsequent videos via world model as shown in the corresponding dashed boxes.\nThe first half illustrates examples from real-world videos with diverse object categories, while the second half demonstrates generalization from human to robot arms. Full videos are available on ourwebsite.",
                "position": 231
            }
        ]
    },
    {
        "header": "2Methodology",
        "images": []
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.00785/extracted/5934558/figures/fig2_vis_3-5-1_v3-3.png",
                "caption": "Figure 3:Image-goal pairs with similar latent actions\nin OOD RT-1 dataset.\nIn each row, we choose the leftmost image-goal pair, and retrieve 3 nearest pairs on latent action embedding. The original task instructions of the pairs are shown under the images. We find that each row shares the similar visual changes semantically, and the latent actions generalize across different raw language tasks.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2411.00785/extracted/5934558/figures/exp3-5-5-control-v3.png",
                "caption": "Figure 4:Controllability of latent action among multiple objects. The last two rows show the generated image by applying 6 different latent actions to the initial frame. Effects of applying different latent actions are highlighted in dashed squares: (a,b) move the apple, (c,d) move the tennis, (e,f) move the orange. Full generated videos from the world model are available on ourwebpage.",
                "position": 413
            },
            {
                "img": "https://arxiv.org/html/2411.00785/extracted/5934558/figures/exp3-5-4-text2video.png",
                "caption": "Figure 5:Generated image sequence jointly by the foundation policy and world model via only latent actions, following 3 different instructions from the same initial image. Full generated videos from the world model are available on ourwebpage.",
                "position": 485
            },
            {
                "img": "https://arxiv.org/html/2411.00785/extracted/5934558/figures/exp4-simpler-std.png",
                "caption": "Figure 6:(a).Success rate of IGOR and the low-level policy trained from scratch methods on Google Robot tasks under SIMPLER simulator, finetuned on 1% data of RT-1.(b).Predictiveness of latent action on robot action. X-axis:log‚Å°(N)ùëÅ\\log(N)roman_log ( italic_N ), whereNùëÅNitalic_Nis the number of nearest neighbours in latent action embedding. Y-axis: normalized standard deviation in action embedding with respect to movement actions (orange), rotation actions (blue), and gripper actions (green).",
                "position": 511
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusions, Limitations, and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": []
    },
    {
        "header": "Appendix BTraining Details",
        "images": []
    },
    {
        "header": "Appendix CAdditional Ablation Results",
        "images": []
    },
    {
        "header": "Appendix DAdditional Highlight of Contributions",
        "images": []
    }
]