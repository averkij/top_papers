[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04270/x1.png",
                "caption": "Figure 1:Overview of our approach. (a) We consider a full RLHF training setup, where a language model (LM) policy is iteratively refined through active data collection. Our goal is to develop an optimal response sampling method for preference labeling. (b) We introduce PILAF, which generates responses by interpolating between the current policy and a reference policy, balancing exploration and exploitation. (c) Our theoretical analysis shows that T-PILAF aligns the parameter gradient with the steepest direction for maximizing human values and achieves more favorable convergence in regions of high sensitivity.",
                "position": 239
            }
        ]
    },
    {
        "header": "2Problem Setup and Motivation",
        "images": []
    },
    {
        "header": "3T-PILAF: Theoretical Sampling Scheme",
        "images": []
    },
    {
        "header": "4Theoretical Analysis",
        "images": []
    },
    {
        "header": "5PILAF Algorithm",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04270/x2.png",
                "caption": "Figure 2:Reward-KL curve for Iterative DPO. All training runs start from the same model obtained at the end of the first iteration viaVanilla Sampling. Each dot represents an evaluation performed every 50 training steps.",
                "position": 1029
            },
            {
                "img": "https://arxiv.org/html/2502.04270/x3.png",
                "caption": "Figure 3:Reward-KL curve for Online DPO. Each dot represents an evaluation performed every 50 training steps.",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2502.04270/x4.png",
                "caption": "Figure 4:Online DPO with an overfitted initial policy. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.",
                "position": 1165
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Contents",
        "images": []
    },
    {
        "header": "Appendix AAdditional Literature Review",
        "images": []
    },
    {
        "header": "Appendix BProof of Main Results",
        "images": []
    },
    {
        "header": "Appendix CProof of Auxiliary Results",
        "images": []
    },
    {
        "header": "Appendix DSupporting Theorem:Master Theorem forZùëçZitalic_Z-Estimators",
        "images": []
    },
    {
        "header": "Appendix EExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.04270/x5.png",
                "caption": "Figure 5:Online DPO with an overfitted initial policy. Full results of theFigure4. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.",
                "position": 5021
            },
            {
                "img": "https://arxiv.org/html/2502.04270/x5.png",
                "caption": "Figure 5:Online DPO with an overfitted initial policy. Full results of theFigure4. Each dot represents an evaluation performed every 50 training steps. Color saturation indicates the training step, with darker colors representing later steps.",
                "position": 5024
            }
        ]
    },
    {
        "header": "Appendix FExtension to Proximal Policy Optimization (PPO)",
        "images": []
    }
]