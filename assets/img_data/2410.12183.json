[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12183/x1.png",
                "caption": "Figure 1:An overview of our TransAgent.(a) TransAgent transfers multi-source knowledge from heterogeneous agents to enhance the generalization ability of vision-language foundation models. It demonstrates knowledge versatility, transfer flexibility and deployment efficiency through elaborate agent collaboration and knowledge ensemble strategy. (b) SOTA comparison for base-to-novel generalization on 11 visual recognition benchmarks. Our method outperforms previous SOTA, especially on the more diversified target domains.",
                "position": 168
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12183/x2.png",
                "caption": "Figure 2:Vision Agent Collaboration and Language Agent Collaboration.(a) VAC integrates visual knowledge via MoA gating and transfers the knowledge through layer-wise feature distillation. (b) LAC enhances the textual representations through class-specific feature distillation between the prompted textual feature and the gated textual feature.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x3.png",
                "caption": "Figure 3:Multi-modal Agent Collaboration.Top left:We first extract the cross attention maps from the T2I agents and then obtain the score vectors through LSE pooling.Top right:We compute the score vectors from the I2T agents as the cosine similarity between the projected visual feature and the LLMâ€™s textual feature. Finally, we perform score distillation between the learned score vectors and the gated score vectors to further align the learnable prompts.",
                "position": 393
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12183/x4.png",
                "caption": "Figure 4:Accuracy comparison in few-shot classification.TransAgent demonstratesstate-of-the-artperformance for all few-shot settings on different datasets, which proves promising learning capability even under extremely limited supervision.",
                "position": 1003
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x5.png",
                "caption": "",
                "position": 1012
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x6.png",
                "caption": "",
                "position": 1017
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x7.png",
                "caption": "",
                "position": 1023
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x8.png",
                "caption": "",
                "position": 1028
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x9.png",
                "caption": "",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x10.png",
                "caption": "",
                "position": 1039
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x11.png",
                "caption": "",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x12.png",
                "caption": "",
                "position": 1049
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x13.png",
                "caption": "",
                "position": 1055
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x14.png",
                "caption": "",
                "position": 1060
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x15.png",
                "caption": "",
                "position": 1065
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x16.png",
                "caption": "Figure 5:Averaged gating weights of each agent on different datasets.Deeper color indicates more contributions to the gated feature(s) or score vectors.",
                "position": 1082
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix CAdditional Ablative Analysis",
        "images": []
    },
    {
        "header": "Appendix DAdditional Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.12183/x17.png",
                "caption": "Figure 6:Variance and performance of TransAgent compared with CoOp.TransAgent demonstrates better robustness and outperforms CoOp on most low-shot cases.",
                "position": 3191
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x18.png",
                "caption": "",
                "position": 3200
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x19.png",
                "caption": "",
                "position": 3205
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x20.png",
                "caption": "",
                "position": 3211
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x21.png",
                "caption": "",
                "position": 3216
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x22.png",
                "caption": "",
                "position": 3221
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x23.png",
                "caption": "",
                "position": 3227
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x24.png",
                "caption": "",
                "position": 3232
            },
            {
                "img": "https://arxiv.org/html/2410.12183/x25.png",
                "caption": "",
                "position": 3237
            }
        ]
    },
    {
        "header": "Appendix EExamples of Generated Descriptions from Chatbots",
        "images": []
    },
    {
        "header": "NeurIPS Paper Checklist",
        "images": []
    }
]