[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x1.png",
                "caption": "Figure 1:LinGen generates photorealistic high-resolution long videos with linear computational complexity.(a) High-quality videos generated using our LinGen model. (b) The computational cost scaling curves across different video resolutions and lengths. LinGen achieves 15×\\times×speed-up compared to the standard DiT when generating 68s-length videos at 512p resolution.",
                "position": 97
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x2.png",
                "caption": "Figure 2:Overview of the LinGen denoising module.LinGen replaces self-attention layers with a MATE block, which inherits linear complexity from its two branches:MA-branchandTE-branch. TheMA-branchconsists of a bidirectional Mamba2 block, RMS, and review tokens to cover short-to-long-range correlations. TheTE-branchis a TEmporal Swin Attention block that addresses the adjacency preservation issue and improves the consistency of generated videos significantly.",
                "position": 139
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x3.png",
                "caption": "Figure 3:The bidirectional Mamba2 module.Native Mamba2 only generates the lower triangular part of the attention map due to its causal characteristic. Thus, we deploy bidirectional Mamba2 to obtain the complete attention map for vision tasks.",
                "position": 174
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x4.png",
                "caption": "Figure 4:Rotary-Major Scan (RMS).We apply different scan schedules across layers to preserve adjacency along various dimensions. Note that scan is bidirectional in practice, but for clarity, only one direction is illustrated for each scan schedule.",
                "position": 194
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x5.png",
                "caption": "Figure 5:TEmporal Swin Attention (TESA).We divide the token tensor into small windows and calculate self-attention within each window. The windows are alternately shifted across layers to cross the boundaries of local windows. The window size remains fixed across different resolutions, hence maintaining linear complexity.",
                "position": 242
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x6.png",
                "caption": "Figure 6:Computational cost comparison between DiT-4B and LinGen-4B.(a) Latency. (b) FLOPs. The cost of LinGen scales significantly slower with both video length and video resolution than DiT. Latency is measured on a single H100 GPU.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x7.png",
                "caption": "Figure 7:Visual examples of videos generated from different models. LinGen-4B generates videos that have similar quality to state-of-the-art commercial video generative models, including Gen-3, LumaLabs, and Kling, while achieving linear complexity and significant speed-up relative to the standard DiT architecture.",
                "position": 306
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x8.png",
                "caption": "Figure 8:Human evaluation on the quality and text-video alignment of videos generated by DiT-4B and LinGen-4B. LinGen outperforms DiT due to it faster adapation to longer token sequences.",
                "position": 842
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x9.png",
                "caption": "Figure 9:Win rates of human evaluation on the quality and text-video alignment of videos generated by LinGen and state-of-the-art video generative models. LinGen has comparable performance to them, given that the variance of human evaluation is 3%.",
                "position": 845
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x10.png",
                "caption": "Figure 10:LinGen adapts much faster to the new task than DiT. (a) Loss curves when transferring the model trained on 256p video generation to 512p. (b) Win rates of human evaluation on quality and text-video faithfulness comparison between LinGen-4B and DiT-4B. Checkpoints are selected after 1K pre-training steps.",
                "position": 861
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x11.png",
                "caption": "Figure 11:Loss curves of 256p text-to-video pre-training under different settings. (a) Ablation on the TESA block and RMS. (b) Ablation on different scan methods.",
                "position": 871
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x12.png",
                "caption": "Figure 12:Win rates of human evaluation on quality comparison between the LinGen default setting and corresponding variants.",
                "position": 874
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVisual Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x13.png",
                "caption": "Figure 13:Examples of 17-second and 68-second videos generated by LinGen.",
                "position": 1963
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x14.png",
                "caption": "Figure 14:Comparisons with typical open-source video generative models.",
                "position": 1966
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x15.png",
                "caption": "Figure 15:Comparisons with state-of-the-art accessible commercial models.",
                "position": 1969
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x16.png",
                "caption": "Figure 16:Comparisons with existing trials on generating minute-length videos.",
                "position": 1972
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x17.png",
                "caption": "Figure 17:Visual examples of ablation experiments on the TESA block, RMS, and review tokens.",
                "position": 1975
            },
            {
                "img": "https://arxiv.org/html/2412.09856/x18.png",
                "caption": "Figure 18:Visual examples of ablation experiments on hybrid training and quality-tuning.",
                "position": 1978
            }
        ]
    },
    {
        "header": "Appendix BComparisons with Prior Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x19.png",
                "caption": "Figure 19:Win rates of human evaluation of quality and text-video alignment of videos generated by LinGen and typical open-source video generative models.",
                "position": 3773
            }
        ]
    },
    {
        "header": "Appendix CMore Ablation Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.09856/x20.png",
                "caption": "Figure 20:Latency of generating 512p 17s videos with different model designs. The latency of LinGen models scales more slowly with model size than self-attention-based standard DiT models. Note that we perform 100 inference steps to measure average latency. This is different from the default setting of 50 steps employed in our main paper.",
                "position": 3852
            }
        ]
    },
    {
        "header": "Appendix DModel Implementation Details",
        "images": []
    }
]