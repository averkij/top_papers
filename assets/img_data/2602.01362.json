[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01362/x1.png",
                "caption": "Figure 1:Left: XDLM combines the noise kernel of UDLM (ùêÆ\\mathbf{u}) and MDLM (ùê¶\\mathbf{m}) to achieve a favorable trade-off between the two methods.[NORMAL]denotes normal tokens, while[MASK]represents the mask token.Right: The trade-off between understanding capability (zero-shot perplexity; lower is better) and generation capability (generation perplexity in 32 sampling steps; lower is better). The proposed XDLM with a mixing ratio ofk=0.1k=0.1achieves the optimal balance, labeled as the ‚ÄòSweet Spot‚Äô.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3miXed Diffusion Language Modeling via Stationary Noise Kernels",
        "images": []
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01362/x2.png",
                "caption": "Figure 2:Language Generation Quality. Results on OWT (top) and LM1B (bottom) demonstrate that XDLM achieves a better balance across both few-step and multi-step regimes. For clarity, PPL and Generation Steps are reported in logarithmic scale.",
                "position": 674
            },
            {
                "img": "https://arxiv.org/html/2602.01362/x3.png",
                "caption": "Figure 3:Evaluation of adapting LLaDA-8B to our XDLM formulation (LLaDA-XDLM):\n(a) LLaDA-XDLM consistently outperforms baselines across diverse benchmarks;\n(b) Improvements are particularly pronounced in code generation (MBPP), where the model substantially reduces generation failures.",
                "position": 833
            },
            {
                "img": "https://arxiv.org/html/2602.01362/x4.png",
                "caption": "Figure 4:Training dynamics for (a) text and (b) image generation tasks.\nColored regions indicate the dominant model during each phase, while transitions between colors mark points of performance crossover.",
                "position": 863
            }
        ]
    },
    {
        "header": "5Related work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Impact Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AThe Forward Process with Stationary Kernels",
        "images": []
    },
    {
        "header": "Appendix BEfficient Sampling and Training via Scalar\nFormulation",
        "images": []
    },
    {
        "header": "Appendix CRelationship to MDLM and UDLM",
        "images": []
    },
    {
        "header": "Appendix DLM1B Sampling Case",
        "images": []
    },
    {
        "header": "Appendix EImageNet Sampling Case",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01362/x5.png",
                "caption": "Figure 5:Qualitative comparison of class-conditional generation on ImageNet-1K without CFG.In the absence of guidance, baseline models struggle significantly: MDLM and GIDD produce chaotic artifacts with poor structural coherence, while UDLM yields recognizable but over-smoothed images. XDLM, by effectively balancing the characteristics of MDLM and UDLM, generates the most coherent and semantically correct samples (e.g., the distinct rocket structure and pizza toppings) even without guidance.",
                "position": 2428
            },
            {
                "img": "https://arxiv.org/html/2602.01362/x6.png",
                "caption": "Figure 6:Qualitative comparison of class-conditional generation on ImageNet-1K with a CFG scale of2.02.0.With the addition of guidance, all models improve, but quality disparities remain. MDLM and GIDD still exhibit texture distortion and \"burn\" artifacts. UDLM produces clean but blurry outputs, lacking fine-grained texture. XDLM demonstrates superior fidelity, combining sharp high-frequency details (seen in the strawberry and volcanoes) with accurate global structure.",
                "position": 2434
            },
            {
                "img": "https://arxiv.org/html/2602.01362/x7.png",
                "caption": "Figure 7:Visual comparison of sampling dynamics over 8 steps on ImageNet-1K.The figure contrasts the generation trajectories of MDLM, GIDD, XDLM (k=0.1k=0.1), and UDLM with CFG scale=2.0=2.0. XDLM demonstrates superior structural coherence and detail refinement compared to baseline models, successfully transitioning from noise to a high-fidelity image.",
                "position": 2461
            }
        ]
    },
    {
        "header": "Appendix FDetailed Configurations of Experiments",
        "images": []
    },
    {
        "header": "Appendix GDetails of Zeroshot Capability of XDLM Trained on OWT",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.01362/x8.png",
                "caption": "Figure 8:Average Zero-Shot PPL trajectory throughout training.Consistent with the final results, XDLM (k=0.1k=0.1) closely aligns with the MDLM baseline from early stages (64.85) to convergence (54.11), while the UDLM curve remains notably higher across all training steps.",
                "position": 2677
            }
        ]
    },
    {
        "header": "Appendix HDetailed Language Generation Results",
        "images": []
    },
    {
        "header": "Appendix IDetailed Image Generation Results",
        "images": []
    },
    {
        "header": "Appendix JDetailed LLaDA Continual Pretraining Results",
        "images": []
    },
    {
        "header": "Appendix KDetailed Computational Efficiency Analysis",
        "images": []
    },
    {
        "header": "Appendix LDetailed Training Dynamics",
        "images": []
    }
]