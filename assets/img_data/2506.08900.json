[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08900/x1.png",
                "caption": "Figure 1:Overview of the proposed model (MIRAGE) and other general (DINOv2) and domain-specific (MedSAM, RETFound) foundation models.In contrast to existing unimodal foundation models, our approach utilizes multimodal self-supervised learning to train a Vision Transformer on a large dataset of paired multimodal retinal images, including optical coherence tomography (OCT), scanning laser ophthalmoscopy (SLO), and automatically generated labels for retinal layers.\nWe evaluated the model on a comprehensive benchmark consisting of 19 tasks from 14 publicly available datasets and two private datasets, covering both OCT and SLO classification and segmentation tasks.\nStatistical significance was calculated using the Wilcoxon signed-rank test across all datasets.\nOur foundation model, MIRAGE, significantly outperforms state-of-the-art foundation models across all task types.",
                "position": 228
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08900/x2.png",
                "caption": "Figure 2:Comparison of MIRAGE and other SOTA models for ocular disease diagnosis.aOCT diagnosis.bSLO diagnosis.\nFor each classification task, we trained the models (all based on ViT-Large) with linear probing using five different random seeds (determining the shuffling of the training data and the augmentation).\nWe then evaluated these models on the hold-out test set and obtained five replicas from which the statistics were derived.\nThe error bars show the standard deviation.\nIn each case, the performance of the two best models was compared to check for statistically significant differences. Thep-value is calculated using the one-tailed Student‚Äôst-test and is indicated in the figure.\nMIRAGE outperforms all other models in all but one dataset, with statistically significant differences in 7 out of 11 cases.",
                "position": 283
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x3.png",
                "caption": "Figure 3:Cross-dataset evaluation results for OCT classification tasks.For each task, we trained the models (all based on ViT-Large) with linear probing using five different random seeds.\nWe then evaluated these models on the full external dataset and obtained five replicas from which the statistics were derived.\nThe error bars show the standard deviation, while the colored bars show the mean AUROC.\nThe performance of the two best models was compared using the one-tailed Student‚Äôst-test, with the resultingp-values indicated in the figure.\nMIRAGE outperforms all other models in both cases, with significant differences on Noor Eye Hospital dataset.",
                "position": 439
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x4.png",
                "caption": "Figure 4:Performance comparison of MIRAGE and SOTA FMs at the retinal lesion and layer segmentation tasks.aOCT segmentation.bSLO segmentation.\nThe error bars show the patient-wise standard deviation, while the colored bars show the mean Dice score.\nThe dagger symbol (‚Ä†‚Ä†\\dagger‚Ä†) indicates that the patient information was not available, so the standard deviation is not shown.\nIn each case, the performance of the two best models was compared to see if there were statistically significant differences.\nThep-value is calculated using the one-tailed Student‚Äôst-test and is indicated in the figure.\nMIRAGE outperforms all other FMs on all datasets, with significant differences in all cases for which the standard deviation was available.",
                "position": 452
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x5.png",
                "caption": "Figure 5:Cross-dataset evaluation results for OCT segmentation.The error bars show the patient-wise standard deviation, while the colored bars show the mean Dice score.\nThe performance of the two best models was compared to see if there were statistically significant differences.\nThep-value is calculated using the one-tailed Student‚Äôst-test and is indicated in the figure.\nMIRAGE significantly outperforms all other FMs.",
                "position": 611
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x6.png",
                "caption": "Figure 6:Performance comparison of MIRAGE and specialist models for retinal lesion and layer segmentation.aOCT segmentation.bCross-dataset OCT segmentation.cSLO segmentation.\nMIRAGE was evaluated using both linear probing and full fine tuning (FFT).\nThe error bars show the patient-wise standard deviation, when available, while the colored bars show the mean Dice score.\nThe dagger symbol (‚Ä†‚Ä†\\dagger‚Ä†) indicates that the patient information was not available, so the standard deviation is not shown.\nIn each case, the performance of the two best models was compared to see if there were statistically significant differences.\nThep-value is calculated using the one-tailed Student‚Äôst-test and is indicated in the figure.\nMIRAGE performs similarly to the specialist models on most in-domain tasks, while significantly outperforming them on the cross-dataset task.",
                "position": 641
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x7.png",
                "caption": "Figure 7:Examples of segmentations from different models.The examples belong, from top to bottom, to the following datasets: Duke iAMD (cross-dataset evaluation), AROI, Duke DME, GOALS, RETOUCH, and SGA.\nTrue positives are depicted in the grayscale value of the class; false background pixels, inred; false lesion or layer pixels, incyan; and true but wrongly classified lesion or layer pixels, inviolet.\nThe results show that our model produces precise and accurate segmentations, rarely missing or misclassifying lesions and layers.\nThe quality of our segmentations is appreciably higher than that of nnUNet in the cross-dataset evaluation (first row), and similar in the other datasets.\nCompared to MedSAM and RETFound, the differences are more pronounced, as these models often misclassify pathological regions.",
                "position": 917
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08900/x8.png",
                "caption": "Figure 8:Overview of the approach used for training and tuning our multimodal foundation model.aMultimodal pretraining.bClassification tuning.cSegmentation tuning.\nThe approach consists of training a Vision Transformer (ViT) model on a large dataset of multimodal retinal images (including OCT, SLO, and pseudo-labels of retinal layers) by reconstructing the input images from a masked or partial view of them.\nBlack arrows represent cross-attention operations between all encoded tokens and the modality tokens.\nThe reconstruction loss‚Ñí=‚ÑíO+‚ÑíN+‚ÑíL‚Ñísubscript‚ÑíùëÇsubscript‚ÑíùëÅsubscript‚Ñíùêø\\mathcal{L}=\\mathcal{L}_{\\scriptscriptstyle O}+\\mathcal{L}_{\\scriptscriptstyle\nN%\n}+\\mathcal{L}_{\\scriptscriptstyle L}caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTon the masked tokens is used as the objective function.\nIn fine-tuning, the model can be trained on downstream tasks by replacing the decoders used for pretraining with task-specific heads.\nMoreover, it accepts both OCT and SLO images as input during inference.",
                "position": 2088
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x9.png",
                "caption": "Figure 9:Overview of the ViT encoder.The linear projection layer projects the input patches into embeddings of dimensiondùëëditalic_d, which are then passed through a stack ofLùêøLitalic_LTransformer blocks containingAùê¥Aitalic_Aattention heads.",
                "position": 2164
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x10.png",
                "caption": "Figure 10:Overview of the modality-specific decoders.Modality-specific features are fed into the transformer-based decoders along with the rest of the encoded tokens through cross-attention operations.",
                "position": 2186
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x11.png",
                "caption": "Figure 11:Overview of the ConvNeXt-based segmentation decoder.The output tokens of the encoder are projected to a higher dimension and reshaped into a feature map, which is then processed by a series of ConvNeXt blocks and upsampled to full resolution.",
                "position": 2191
            }
        ]
    },
    {
        "header": "5Data availability",
        "images": []
    },
    {
        "header": "6Code availability",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "8Author contributions",
        "images": []
    },
    {
        "header": "9Competing interests",
        "images": []
    },
    {
        "header": "Appendix 1Supplementary Note 1: Extended results",
        "images": []
    },
    {
        "header": "Appendix 2Supplementary Note 2: Benchmark datasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08900/x12.png",
                "caption": "Supplementary Figure 1:Example images from classification datasets.OCT and/or SLO images are shown for each dataset along with the corresponding ground truth labels.",
                "position": 5961
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x13.png",
                "caption": "Supplementary Figure 2:Example images from segmentation datasets.OCT and/or SLO images are shown for each dataset along with their corresponding segmentation masks.",
                "position": 6107
            }
        ]
    },
    {
        "header": "Appendix 3Supplementary Note 3: Masking strategy",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08900/x14.png",
                "caption": "Supplementary Figure 3:3D scatter plots of token allocations across OCT, SLO, and Layers for different Dirichlet concentration parameters (Œ±ùõº\\alphaitalic_Œ±= 0.1, 1, and 100).Each point represents a sample, with the coordinates corresponding to the number of non-masked tokens for each modality.\nA total of 2000 samples are shown for eachŒ±ùõº\\alphaitalic_Œ±value, with the number of total tokens fixed at 98.\nLowerŒ±ùõº\\alphaitalic_Œ±values (left) result in sparse, modality-dominant allocations, while higherŒ±ùõº\\alphaitalic_Œ±values (right) enforce more uniform token distributions.",
                "position": 6220
            }
        ]
    },
    {
        "header": "Appendix 4Supplementary Note 4: Pre-experimental results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.08900/x15.png",
                "caption": "Supplementary Figure 4:Loss curves of MIARGE during pretraining.All training losses for the three modalities (OCT, SLO, and Layers) are significantly minimized during the pretraining stage.",
                "position": 6242
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x16.png",
                "caption": "Supplementary Figure 5:MIRAGE predicitons on our internal dataset.For each sample (composed of an OCT B-scan, an SLO image, and a layer segmentation) we show the predictions with different masking proportions for each modality.\nThe visualizations show that the model is able to reconstruct masked patches from the different modalities, suggesting that the learned representations encode meaningful multimodal information.",
                "position": 6258
            },
            {
                "img": "https://arxiv.org/html/2506.08900/x17.png",
                "caption": "Supplementary Figure 6:Visualization of feature embeddings from different foundation models.These plots show the embeddings produced by DINOv2, RETFound, and MIRAGE for the OCTID and Kermany datasets after projection to 2D using UMAP.\nMIRAGE embeddings show better separation and clustering of samples compared to DINOv2 and RETFound, suggesting better representation learning.",
                "position": 6276
            }
        ]
    },
    {
        "header": "Appendix 5Supplementary Note 5: Computational efficiency",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]