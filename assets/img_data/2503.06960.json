[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06960/x1.png",
                "caption": "Figure 1:An overview of this paper.(a) We conduct a comprehensive study evaluating pre-trained vision models (PVMs) on visuomotor control and perception tasks, analyzing how different pretraining (model, data) combinations affect performance. Our analysis reveals that DINO/iBOT excels while MAE underperforms.\n(b) We investigate the performance drop of DINO/iBOT when trained on non-(single-)object-centric (NOC) data, discovering they struggle to learn objectness from NOC dataâ€”a capability that strongly correlates with robot manipulation performance.\n(c) We introduce SlotMIM, which incorporates explicit objectness guidance during training to effectively learn object-centric representations from NOC data.\n(d) Through scaled-up pre-training and evaluation across six tasks, we demonstrate that SlotMIM adaptively learns different types of objectness based on the pre-training dataset characteristics, outperforming existing methods.",
                "position": 193
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06960/x2.png",
                "caption": "Table 1:Overview of pre-training datasets.We uniformly sample subsets of 241K images from ImageNet, CC12M, and Ego4D. COCO+ is formed by combiningtrainandunlabeledsubsets of COCO. Ego4D frames are extracted at 0.2 fps and then sampled to subsets. 1.28M subsets are also considered in later experiments. For scene-centric data, we use the Open Images[39]dataset to scale up.",
                "position": 221
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3When are PVMs More Effective for Visuomotor Control and Perception Tasks?",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06960/x3.png",
                "caption": "Figure 2:Performance of PVMs trained with different (model, data) combinations on visuomotor control and perception tasks.(241K scale, best viewed together withFig.1a)\nOur analysis of existing works reveals several key findings: 1) MAE with ego-centric data shows only moderate performance on visuomotor control tasks and performs poorly on ADE20K; 2) DINO and iBOT lead performance across all tasks, with their best models typically trained on object-centric data (except for ADE20K); 3) The top-3 models (DINO, iBOT, and MAE) struggle to learn effective representations for manipulation when trained on scene-centric data.\nMost notably, 4) SlotMIM (Sec.4) consistently outperforms prior methods regardless of whether it is pre-trained on object-centric data or not.",
                "position": 312
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x4.png",
                "caption": "Table 2:Overview of robot manipulation and navigation tasks.Bottom: example tasks of each benchmark suite.",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x5.png",
                "caption": "Figure 3:Behavior cloning with attentive probing.An additional token is trained with cross attention to gather information from all patch tokens from the backbone.",
                "position": 419
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x6.png",
                "caption": "(a)Clustering assignment of patch tokens.Each patch is assigned to its nearest-neighbor prototype, with different colors indicating different prototypes.",
                "position": 422
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x6.png",
                "caption": "(a)Clustering assignment of patch tokens.Each patch is assigned to its nearest-neighbor prototype, with different colors indicating different prototypes.",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x7.png",
                "caption": "(b)Top-5 segments retrieved by the prototypes (by column).A segment consists of patches assigned to the same prototype within an image. Each column shows the top-5 segments with the highest cosine similarity to the prototype corresponding to the column.",
                "position": 430
            }
        ]
    },
    {
        "header": "4Object-centric Learning on NOC Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06960/x8.png",
                "caption": "Figure 5:Overview of SlotMIM.Our framework extends iBOT by: 1) repurposing its within-view patch-level self-distillation for object discovery, 2) introducing a cross-view objective for semantic guidance, and 3) performing object-centric contrastive learning on slots (object features grouped from patches with matching cluster assignments). This approach provides explicit objectness supervision without requiring object-centric data, making it applicable to various types of NOC data (seeFig.1cfor comparison andFig.1dfor results).",
                "position": 492
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x9.png",
                "caption": "Figure 6:Results of scaling PVM training data.It considers three factors that influence manipulation success rates: data types, pre-training methods, and data scale. Highlighted lines represent the best-performing data scaling for each method, whilefadedlines indicate sub-optimal performance. It shows that 1) SlotMIM achieves the best performance, scalability, and data efficiency across evaluation tasks by training on NOC data; 2) On manipulation tasks, most methods (except MAE) face performance drop when scaling up pre-training data.",
                "position": 561
            }
        ]
    },
    {
        "header": "5Scaling Up Pre-Training Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06960/x10.png",
                "caption": "Figure 7:Results on COCO object detection and instance segmentation.SlotMIM shows better data efficiency with both object-centric and NOC data, and its results continue to improve with more data, surpassing prior models by a notable margin.",
                "position": 729
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Implementation Details",
        "images": []
    },
    {
        "header": "Appendix BExtended Analysis and Discussion",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.06960/x11.png",
                "caption": "Figure 8:Results on ImageNet tasks.SlotMIM consistently outperforms prior arts whether pre-trained on object-centric data or not. Notably, when trained on COCO+, it transfers better than most ImageNet models despite the domain gap.",
                "position": 2437
            },
            {
                "img": "https://arxiv.org/html/2503.06960/x12.png",
                "caption": "Figure 9:Scaling on different data sources.We scale up object-centric, scene-centric, and web-crawled data, and highlight the best (model, data) combinations. Our method learns strong and transferable representations with significant data efficiency and continues to improve with more data.",
                "position": 2462
            }
        ]
    },
    {
        "header": "Appendix CExtended Experiments",
        "images": []
    }
]