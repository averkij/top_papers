[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/x1.png",
                "caption": "Figure 1:Overview of the MMOral. It consists of four sub-datasets: MMOral-Attribute, MMOral-Report, MMOral-VQA, and MMOral-Chat. MMOral-Attribute contains a total of 49 categories of anatomical structures within panoramic X-rays. MMOral-Report consists of two types of textual descriptions: the grounding caption and the medical report. MMOral-VQA includes closed-ended and open-ended QA pairs spanning five diagnostic dimensions. MMOral-Chat simulates the dialogue process between patients and radiology experts regarding the interpretation of panoramic X-rays.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x2.png",
                "caption": "Figure 2:The MMOral dataset curation pipeline, which consists of four sequential steps.",
                "position": 173
            }
        ]
    },
    {
        "header": "2MMOral Dataset Curation",
        "images": []
    },
    {
        "header": "3MMOral Dataset Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/x3.png",
                "caption": "Figure 3:The data statistic distribution and human evaluation results.",
                "position": 304
            }
        ]
    },
    {
        "header": "4MMOral-Bench",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/x4.png",
                "caption": "Table 2:Results on MMOral-Bench for existing various LVLMs across both closed-ended and open-ended VQA tasks. The best-performing model in each category is highlightedin-bold, while the second-best isunderlined. 36 out of 64 models for demonstration in the main text; additional results are provided in thesupplementary material.",
                "position": 366
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x4.png",
                "caption": "Figure 4:Performance comparison on both closed-ended and open-ended QA across multiple LVLMs.",
                "position": 1083
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x5.png",
                "caption": "Figure 5:Three examples of case studies on closed-ended QA and open-ended QA, respectively. More examples can be found in thesupplementary materials.",
                "position": 1235
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ARelated works",
        "images": []
    },
    {
        "header": "Appendix BMMOral Curation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/figures/prompt_report_1st.png",
                "caption": "Figure 6:The prompt for DeepSeek-R1-Distill-Llama-70B to generate the medical report of panoramic X-ray images.",
                "position": 2972
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x6.png",
                "caption": "Figure 7:The manually designed in-context examples for medical report generation. Given a grounding caption of panoramic X-rays (left side), the ideal medical report exemplar is shown on the right side.",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x7.png",
                "caption": "Figure 8:The prompt for GPT-4-Turbo to revise the generated report.\nWe manually check the generated medical reports from the DeepSeek-R1-Distill-Llama-70B and summarize several rules for validation and correction. We ask the GPT-4-Turbo simultaneously output both revised reports and corresponding revision logs for convenient human verification.",
                "position": 2978
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x8.png",
                "caption": "Figure 9:The prompt for GPT-4-Turbo to generate both closed-ended and open-ended question-answering pairs based on the medical report.",
                "position": 2982
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x9.png",
                "caption": "Figure 10:The prompt for GPT-4-Turbo to generate a multi-turn conversation between the assistant and a person asking questions about the panoramic X-ray.",
                "position": 2985
            }
        ]
    },
    {
        "header": "Appendix CMMOral Visualization",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/figures/cloud_maps.jpg",
                "caption": "Figure 11:The word cloud maps for MMOral-Report, MMOral-VQA, and MMOral-Chat sub-datasets.",
                "position": 3006
            }
        ]
    },
    {
        "header": "Appendix DEvaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/x10.png",
                "caption": "Figure 12:Few-shot prompt for evaluating model predictions using GPT-4-Turbo, whereùí¨\\mathcal{Q}is the question,ùí¨\\mathcal{Q}is the ground truth andùí´\\mathcal{P}is the model‚Äôs prediction for the question. The prompt demonstrates exemplar scoring criteria for diverse open-ended responses.\nTaking the prompt filled withùí¨\\mathcal{Q},ùí¢\\mathcal{G},andùí´\\mathcal{P}, GPT-4-Turbo will generate a soft grading score from 0 to 1.",
                "position": 3045
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x11.png",
                "caption": "Figure 13:The means and standard deviations of each category on 5 repeated evaluations across four LVLMs‚Äô predictions.",
                "position": 3506
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x12.png",
                "caption": "Table 8:Results on MMOral-Bench for existing various LVLMs across both closed-ended and open-ended VQA tasks. The best-performing model in each category is highlightedin-bold, while the second-best isunderlined.",
                "position": 3517
            }
        ]
    },
    {
        "header": "Appendix EEfficacy Validation of MMOral Instruction Data",
        "images": []
    },
    {
        "header": "Appendix FLimitations",
        "images": []
    },
    {
        "header": "Appendix GExperiments Compute Resources",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.09254/x12.png",
                "caption": "Figure 14:An example of MMOral-Attribute and MMOral-Report.",
                "position": 4979
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x13.png",
                "caption": "Figure 15:An example of MMOral-Attribute and MMOral-Report.",
                "position": 4982
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x14.png",
                "caption": "Figure 16:An example of MMOral-Attribute and MMOral-Report.",
                "position": 4985
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x15.png",
                "caption": "Figure 17:An example of MMOral-Attribute and MMOral-Report.",
                "position": 4988
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x16.png",
                "caption": "Figure 18:A closed-ended QA example.Redhighlights the right answer.",
                "position": 4997
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x17.png",
                "caption": "Figure 19:A closed-ended QA example.Redhighlights the right answer.Bluehighlights the wrong answer.",
                "position": 5000
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x18.png",
                "caption": "Figure 20:A closed-ended QA example.Redhighlights the right answer.",
                "position": 5003
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x19.png",
                "caption": "Figure 21:A closed-ended QA example.Redhighlights the right answer.Bluehighlights the wrong answer.",
                "position": 5006
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x20.png",
                "caption": "Figure 22:A closed-ended QA example.Redhighlights the right answer.",
                "position": 5009
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x21.png",
                "caption": "Figure 23:A closed-ended QA example.Redhighlights the right answer.Bluehighlights the wrong answer.",
                "position": 5012
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x22.png",
                "caption": "Figure 24:An open-ended QA example.Redhighlights the right description.",
                "position": 5015
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x23.png",
                "caption": "Figure 25:An open-ended QA example.Redhighlights the right description.",
                "position": 5018
            },
            {
                "img": "https://arxiv.org/html/2509.09254/x24.png",
                "caption": "Figure 26:An open-ended QA example.Redhighlights the right description.",
                "position": 5021
            }
        ]
    },
    {
        "header": "Appendix HCase Study",
        "images": []
    }
]