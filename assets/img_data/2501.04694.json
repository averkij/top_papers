[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/teaser_fig.png",
                "caption": "Figure 1:Benchmark performance of \nEpiCoder-Qwen-7B\n(fine-tuned on Qwen2.5-Coder-7B-Base) and its counterparts. XFileDep is file-level code generation benchmark, all others are function-level.",
                "position": 145
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/x1.png",
                "caption": "Figure 2:Overview of our feature tree-based code generation framework, which consists of three steps: (a)Feature Tree Extraction, where we first extract the feature set to construct the tree structure demonstration and then extract the feature trees; (b)Feature Tree Evolution, where the feature tree is iteratively expanded in depth and breadth; and (c)Feature Tree-Based Code Generation, where the evolved feature tree is used to generate diverse code instruction data. A detailed example of feature evolution and code generation is shown in AppendixA.",
                "position": 171
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/x2.png",
                "caption": "Figure 3:An example of file-level code generation (including test code file). Different files contain different functional modules, with dependencies existing across files.",
                "position": 263
            }
        ]
    },
    {
        "header": "3Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/deepseek.png",
                "caption": "Table 1:Pass@1 (%) results of different LLMs on HumanEval (+) and MBPP (+) computed with greedy decoding. We report the results uniformly from the EvalPlus Leaderboard444https://evalplus.github.io/leaderboard.html.",
                "position": 456
            },
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/qwen2.png",
                "caption": "",
                "position": 651
            },
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/deepseek.png",
                "caption": "Table 2:Pass@1 (%) results of different LLMs on BigCodeBench computed with greedy decoding. We conducted the evaluation on theFullandHardsubsets of this benchmark, including theCompleteandInstructtasks. Except for the results underlined, which are sourced from their respective papers, all other results are obtained from theBigCodeBench-Leaderboard666https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard.",
                "position": 685
            },
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/xfiledep_barchart.png",
                "caption": "Figure 4:Pass@1 (%) results of different LLMs on XFileDep computed with greedy decoding.",
                "position": 1543
            },
            {
                "img": "https://arxiv.org/html/2501.04694/x3.png",
                "caption": "Figure 5:An example of our repo-level code generation. The left part shows the original LLaMA-Factory repository structure, the middle part presents the structure of LLMTune, which we generated based on the extracted feature tree, and the right part illustrates an example file from the generated repository.",
                "position": 1562
            }
        ]
    },
    {
        "header": "4Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/data_leakage_dis.png",
                "caption": "Figure 6:The distribution of cosine similarity scores between different various datasets and the benchmark datasets HumanEval, MBPP, and BigCodeBench.",
                "position": 2136
            },
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/data_leakage_dis.png",
                "caption": "Figure 6:The distribution of cosine similarity scores between different various datasets and the benchmark datasets HumanEval, MBPP, and BigCodeBench.",
                "position": 2139
            },
            {
                "img": "https://arxiv.org/html/2501.04694/x4.png",
                "caption": "Figure 7:The scaling law of code instruction data. The results obtained from randomly sampled subsets of 380k data points across the HumanEval, MBPP, and BigCodeBench benchmarks.",
                "position": 2144
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix of Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/evol_process.png",
                "caption": "Figure 8:An example of feature evolution.",
                "position": 2758
            }
        ]
    },
    {
        "header": "Appendix BAppendix of Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/XFileDep_sankey.png",
                "caption": "Figure 9:The Sankey diagram for the creation of the XFileDep benchmark, with numbers indicating the quantity of data samples.",
                "position": 3557
            },
            {
                "img": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/XFileDep_folder_stats.png",
                "caption": "Figure 10:the distribution of file quantities and the average file length for each data sample.",
                "position": 3563
            },
            {
                "img": "https://arxiv.org/html/2501.04694/x5.png",
                "caption": "Figure 11:Cases from the HumanEval benchmark dataset (left) and theevol-codealpaca-v1dataset (right) with varying similarity. The embeddings are computed based on the \"output\" portions of the training dataset and the \"prompt + canonical_solution\" of the HumanEval benchmark data.",
                "position": 4382
            }
        ]
    },
    {
        "header": "Appendix CAppendix of Analysis",
        "images": []
    }
]