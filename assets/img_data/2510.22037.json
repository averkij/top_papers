[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.22037/",
                "caption": "",
                "position": 88
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x2.png",
                "caption": "Figure 1:Optimal Scaling Trajectoriesfor English, French, Russian, Chinese, Hindi, and Swahili.\nThe law for [monolingual vocabulary, monolingual training]=(—), the law for [multilingual vocabulary, monolingual training]=(- - -), and the law for [multilingual vocabulary, unimax training]=(···).We find (1) per-language optimal scaling trajectories are similar, (2) there is a compute efficiency tax for training withmultilingualvocabularies or training sets (especially for English), and (3) as Hindi and Swahili observe data repetition their curves slope upward from diminishing returns.",
                "position": 157
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x3.png",
                "caption": "Figure 2:TheCross-Lingual Transfer Matrix, depicting the measured Language Transfer Score across30×3030\\times 30language pairs.\nPositive scores indicate more positive transfer, negative scores more interference, during bilingual co-training.\nThe dashed boxes indicate the top-5 source languages for each target language.\nRefer to AppendixB.6for full details, andFigure˜C.2for the larger38×3838\\times 38matrix.While English is the best source language for many of the languages, we find language similarity is highly predictive of these scores.",
                "position": 399
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x4.png",
                "caption": "Figure 3:Left:A language transfer scatter plot, comparing score symmetry: is language A as helpful to B as vice versa? Points cluster near the diagonal, indicating strong symmetry.The most synergistic and symmetric pairs almost exclusively share both language family and script.Greater linguistic distance correlates with increased asymmetry and reduced positive transfer.Right:The impact of linguistic similarity (via language family or script) on transfer scores. The box spans the inter-quartile range, with the median line at the center.We find the differences between each group are statistically significant (p<.001p<.001), suggesting that sharing either a language family or a script independently contribute greater positive cross-lingual transfer.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x4.png",
                "caption": "",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x5.png",
                "caption": "",
                "position": 415
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x6.png",
                "caption": "Figure 4:We empirically measure the relative degradation in loss (y-axis), as compared to a monolingual model of the same(N,D)(N,D), from adding pretraining languages (z-axis).Left:We fixD=25​BD=25Btokens, and vary the model sizeNN.Right:We fixN=2​BN=2Bparameters, and vary the tokensDD.\nThe points are real empirical observations, averaged across languages. The mesh-grid is a surface estimate, using a cubic spline.Target language loss is most affected by the number of training languages, but this loss penalty declines for larger models with more capacity.",
                "position": 475
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x7.png",
                "caption": "Figure 5:Iso-loss from expanding language coverage:When scaling the number of languages a model serves fromK→r⋅KK\\rightarrow r\\cdot K, we can estimate how much they need to increase model sizeN′/NN^{\\prime}/Nand/or training tokensDt​o​t′/Dt​o​tD^{\\prime}_{tot}/D_{tot}, without degrading any of their languages’ loss.\nWe derive the compute optimal scaling equations.Theiso-lossesindicate that a practitioner should expand their compute budget byC⋅r0.97C\\cdot r^{0.97}to expand their language coverage byrr.",
                "position": 513
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x8.png",
                "caption": "Figure 6:For eight languages, we plot the loss curves for 2B parameter models pretraining monolingually from scratch (···), finetuning monolingually from the Unimax base model (- - -), and the difference between these losses (—).\nWe annotate the number of tokens at which the pretrained loss becomes better than the finetuning loss.\nThe difference between the interpolated pretraining curve and finetuning curve.\nWhen the loss difference reaches zero, pretraining from scratch has surpassed finetuning using equal compute.\nDepending on the token (or compute) budget, it is usually more effective to finetune a multilingual checkpoint if there are<144​B<144Btokens, and pretrain from scratch if the budget accommodates≥283​B\\geq 283Btokens.",
                "position": 555
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x9.png",
                "caption": "Figure 7:In terms ofNN, we model the compute budgetCCrequired for a model pretrained from scratch to outperform the model finetuned from the generic Unimax multilingual checkpoint.\nWe estimate this relationship aslog​(C)=10283128×N1.65\\text{log}(C)=10283128\\,\\times\\,N^{1.65}.",
                "position": 570
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x10.png",
                "caption": "Figure C.1:Left: X-axis is training tokens (D), Y-axis is language-transfer score.\nRight: X-axis is model size (N), Y-axis is language-transfer score.",
                "position": 4129
            },
            {
                "img": "https://arxiv.org/html/2510.22037/x11.png",
                "caption": "Figure C.2:The language transfer scores measure the benefit to the target language of (co-)training with the source language. We bold the top-5 source languages for each target language. The top left 9x9 are the Bilingual Transfer Score computed directly, whereas the rest are estimated from the Finetuning Adaptation Score. While English is the best source language for many of of languages, we find language similarity is highly predictive of these scores.",
                "position": 4134
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]