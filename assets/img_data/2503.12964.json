[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/main.png",
                "caption": "Figure 1:VFM Training Stack.NeMo provides an end-to-end stack for training video foundation models, leveraging NeMo Curator for video curation, Megatron Core for scaling transformer models, and the NeMo Framework for pre-training, fine-tuning, and accelerated inference.",
                "position": 108
            }
        ]
    },
    {
        "header": "2High-Throughput Video Curation with NeMo Curator",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/high-res-video-arch.png",
                "caption": "Figure 2:Video Curation Pipeline.The video curation pipeline clips and processes large amounts of raw video. Then, the clips are sharded and stored on the cloud in the Webdataset format.",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/auto_balance.png",
                "caption": "Figure 3:Auto-Balanced Curation Pipeline.Certain curation stages can be rate-limiting the throughput of the entire curation pipeline. We created an auto-balancing system to match the throughput of the overall pipeline by allocating the optimal number of workers depending on the curation stage.",
                "position": 145
            }
        ]
    },
    {
        "header": "3Efficient Multimodal Dataloading with Megatron Energon",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/sequence_packing.png",
                "caption": "Figure 4:Mixed Resolution Image-Video Training.We utilize sequence packing with padding to enable joint training of images and videos with different resolutions and video length.",
                "position": 170
            }
        ]
    },
    {
        "header": "4Scaling Video Foundation Model Training",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/diffusion_training_pipeline.png",
                "caption": "Figure 5:Video Diffusion Transformer.Our pipeline consists of various input signals such as text, videos, and noise timestep which are compressed and used to train a video diffusion transformer.",
                "position": 215
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/pipeline_conditioning.png",
                "caption": "Figure 6:Parallel Conditioning Scheme.The DiT model uses additional conditioning signals that must be available at each pipeline stage. We determined that recomputing these embeddings at each pipeline stage is more efficient than communicating them across pipeline stages during training.",
                "position": 276
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/dit_adaln_improvement.png",
                "caption": "Figure 7:Compute Performance Improvement with AdaLN-LoRA.We benchmark the DiT training speedup with AdaLN-LoRA for 7B and 28B models on sequence lengths of 8k and 74k tokens. We observe a noticeable speedup across all configurations and up to 1.2x for DiT-7B at 8k sequence length.",
                "position": 304
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/fast_dit_comp.png",
                "caption": "Figure 8:Best Compute Performance per Workload.We benchmark our training framework and compare it with the Fast-DiT framework. Our framework consistently delivers higher training throughput than Fast-DiT, and it also supports training larger models that Fast-DiT cannot handle.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/scaling_eff.png",
                "caption": "Figure 9:DiT Training Scaling Efficiency.We measure the scaling efficiency across four workloads, scaling from 8 to 32 nodes of 8xH100 GPUs. All configurations exceed 95% efficiency, with most surpassing 98%. These results highlight our frameworkâ€™s near-linear scaling across training workloads.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/4d_parallel.png",
                "caption": "Figure 10:Context Scaling Performance Across Parallelism Strategies.We observe that the optimal parallelism configuration changes based on the context length. FSDP is performant for shorter context lengths, but 4D parallelism becomes necessary for efficient long-context training.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/st_dit.png",
                "caption": "Figure 11:ST-DiT Hybrid Parallelism.We propose a new hybrid parallelism approach for ST-DiT models that reduces the total necessary all-to-all (A2A) collectives while fully hiding the communication overhead of full attention and minimizing the communication overhead for both spatial and temporal attention.",
                "position": 456
            }
        ]
    },
    {
        "header": "5Efficient Video Generation Inference",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/parallelized-video-generation-context-parallelism.png",
                "caption": "Figure 12:Parallelized Inference with Context Parallel.To generate high-resolution videos efficiently, we leverage CP to parallelize the diffusion denoising process on multiple GPUs. This strategy provides near-linear scaling in inference speedup with respect to the number of GPUs.",
                "position": 594
            },
            {
                "img": "https://arxiv.org/html/2503.12964/extracted/6286218/images/inference-performance-gpu-count.png",
                "caption": "Figure 13:Inference Performance Analysis.We benchmark the Cosmos-1.0-Diffusion-7B-Text2World model across different compute, precision, and model parallelism settings. Our experiments show that we can achieve near-linear scaling with our proposed CP-accelerated inference algorithm.",
                "position": 607
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix AContributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]