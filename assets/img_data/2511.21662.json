[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21662/x1.png",
                "caption": "Figure 1:Data Construction Pipeline.Multi-Crit is built from diverse prompts across open-ended and reasoning tasks, responses from various LMMs reflecting subtle quality distinctions, and multi-criterion human annotations highlighting preference conflicts across criteria.",
                "position": 220
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Multi-Crit Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21662/x2.png",
                "caption": "Table 1:Multi-Critâ€™s key statistics.",
                "position": 277
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x2.png",
                "caption": "Figure 2:Distribution of prompt sources (left) and evaluation criteria (right).",
                "position": 345
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x3.png",
                "caption": "",
                "position": 349
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21662/x4.png",
                "caption": "Figure 3:Average performance across each criterion. While the top model differs across criteria, all models show stronger pluralistic alignment inverifiable reasoningthan inopen-endedtasks.",
                "position": 1196
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x5.png",
                "caption": "Figure 4:Results of RL-tuned reasoning models on the Multi-Crit reasoning split, all based on Qwen2.5-VL-7B.",
                "position": 1258
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x6.png",
                "caption": "Figure 5:Test-time scaling behavior by pluralistic accuracy.",
                "position": 1268
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x7.png",
                "caption": "Figure 6:Correlation between criterion-level top judge accuracy and inter-annotator agreement.\nEach point denotes one criterion.",
                "position": 1277
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AOverview of the Appendix",
        "images": []
    },
    {
        "header": "Appendix BComparison Against Prior Benchmarks",
        "images": []
    },
    {
        "header": "Appendix CBenchmark Construction and Statistics",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21662/x8.png",
                "caption": "Figure 7:Correlation of criterion-level human preferences for prompts that exhibit preference conflicts in the open-ended (left) and reasoning (right) splits of Multi-Crit.",
                "position": 1825
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x9.png",
                "caption": "Figure 8:Counts of criterion pairs exhibiting human preference conflicts in the open-ended (left) and reasoning splits (right) of Multi-Crit.",
                "position": 1828
            }
        ]
    },
    {
        "header": "Appendix DQualitative Examples",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.21662/x10.png",
                "caption": "Figure 9:Example prompts in the open-ended (top) and verifiable reasoning (bottom) splits of Multi-Crit.Our benchmark spans diverse multimodal prompts, reflecting the broad evaluation domains in which LMM judges are used.",
                "position": 1902
            },
            {
                "img": "https://arxiv.org/html/2511.21662/figures/appendix/conflict/non-61.jpg",
                "caption": "Table 10:Example of an open-ended evaluation instance for blind storytelling. Response A conveys themoral of the storyand fullyfollows the instruction to avoid mentioning rectangles. Response B includes morevisual detailsbuthallucinates the shirt colors.",
                "position": 1908
            },
            {
                "img": "https://arxiv.org/html/2511.21662/figures/appendix/conflict/non-17.png",
                "caption": "Table 11:Example of an open-ended evaluation instance for creative-image captioning. Response A creatively highlights thehumorous and unexpected effectand introducesless hallucinated content. Response B includes a briefTL;DRand aninferenceabout image usage, with a morecoherent flow across elements.",
                "position": 2028
            },
            {
                "img": "https://arxiv.org/html/2511.21662/figures/appendix/conflict/non-122.jpg",
                "caption": "Table 12:Example of an evaluation instance in instruction-rich image analysis. Both responses are randomly sampled from GPT-4.1. Response A provides morebackground detailsand isless hallucinated, while Response B better followsall required elements in the user promptand offers a moreclear and readable description of the foreground and composition.",
                "position": 2133
            },
            {
                "img": "https://arxiv.org/html/2511.21662/x11.png",
                "caption": "Table 13:Example of an evaluation instance for math reasoning.\nBoth responses appeal to rotational symmetry but arrive at incorrect final answers. Response A is moreconciseandavoid hallucinated colors. Response B demonstrates slightly bettergrounding for the inner orange shapesand actively exploresalternative solutions, yet contains somelogical shiftsthat reduce coherence.",
                "position": 2251
            },
            {
                "img": "https://arxiv.org/html/2511.21662/figures/appendix/conflict/reasoning-104.png",
                "caption": "Table 14:Example of an evaluation instance for object counting. Both Gemini-2.5-Flash responses correctly identify more green buses than blue ones. Response A is morecompact. Response Bprovides finer object details and spatial relationshipsandexplicitly checks each visible object, but introduces a minorhallucination by stating that no school bus is present, even though one appears in the image.",
                "position": 2404
            },
            {
                "img": "https://arxiv.org/html/2511.21662/figures/appendix/conflict/reasoning-57.png",
                "caption": "Table 15:Example of an evaluation instance in judging biological reasoning. Both responses reach the correct conclusion (grass increases). Response A provides a more complete and detaileddescription of the food web. Response B is moreefficientand together showsdeeper reflectionby explicitly recognizing that the decrease in lizardsdoes not counteract the effectof fewer grasshoppers eating grass.",
                "position": 2528
            },
            {
                "img": "https://arxiv.org/html/2511.21662/figures/appendix/batch/nonreasoning-83.jpg",
                "caption": "Table 19:In this case,o4-minicorrectly identifies thecompleteness vs. no-hallucinationconflict under single-criterion judgments but collapses to a uniform preference under joint multi-criterion judgment.With single-criterion evaluation prompts, the model successfully adheres to each criterion independently and captures the preference disagreement, whereas under joint prompting, completeness dominates and the model fails to follow the no-hallucination criterion.",
                "position": 3181
            }
        ]
    },
    {
        "header": "Appendix EAdditional Experimental Results",
        "images": []
    }
]