[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08730/x1.png",
                "caption": "(a)Countable objects (e.g., cars).",
                "position": 119
            },
            {
                "img": "https://arxiv.org/html/2512.08730/x1.png",
                "caption": "(a)Countable objects (e.g., cars).",
                "position": 122
            },
            {
                "img": "https://arxiv.org/html/2512.08730/x2.png",
                "caption": "(b)Amorphous regions (e.g., road).",
                "position": 128
            },
            {
                "img": "https://arxiv.org/html/2512.08730/x3.png",
                "caption": "Figure 2:The overall inference pipeline of SegEarth-OV3. Given an input image and a list of text prompts, we leverage SAM 3’s decoupled outputs. The pipeline involves: (1) instance aggregation to consolidate sparse object queries; (2) dual-head mask fusion to combine the fine-grained instance details with the global coverage of the semantic head; and (3) presence-guided filtering (using the presence score) to suppress false positives from absent categories.denotes the element-wise maximum operation, anddenotes multiplication.",
                "position": 135
            },
            {
                "img": "https://arxiv.org/html/2512.08730/figures/max.png",
                "caption": "",
                "position": 136
            },
            {
                "img": "https://arxiv.org/html/2512.08730/figures/mul.png",
                "caption": "",
                "position": 136
            }
        ]
    },
    {
        "header": "3Methods",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08730/x4.png",
                "caption": "Figure 3:Impact of vocabulary size and our filtering strategy. Querying a vast vocabulary introduces severe noise due to distracting candidates (b to c). Our presence-guided filtering leverages presence scores to suppress absent categories, effectively eliminating interference and restoring segmentation quality.",
                "position": 265
            },
            {
                "img": "https://arxiv.org/html/2512.08730/x5.png",
                "caption": "Table 1:Open-vocabulary semantic segmentation quantitative comparison on remote sensing datasets. Evaluation metric: mIoU.Bestandsecond bestperformances are highlighted. SCAN, SAN, SED, Cat-Seg, OVRS, GSNet, RSKT-Seg are tuned on dataset (7,002 images with 17 categories). SkySense-O is tuned on Sky-SA dataset (35,000 images with 1,763 categories). “Oracle” is achieved by a fully supervised SegFormer-b0[63]model using full training data.",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2512.08730/x5.png",
                "caption": "Table 4:Comparison with state-of-the-art OVSS methods on Pascal VOC20, COCO Stuff, and Cityscapes benchmarks.Boldindicates the best performance.",
                "position": 736
            },
            {
                "img": "https://arxiv.org/html/2512.08730/x5.png",
                "caption": "Figure 4:Inference results of SegEarth-OV3on a remote sensing image exceeding 10k×\\times10k resolution. The image originates from[13].",
                "position": 996
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]