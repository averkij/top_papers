[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08826/x1.png",
                "caption": "Figure 1:RewardDance consistently boosts T2I and T2V generation (validated by alignment/GSB). The reward variance during the later stages of RL training, represented by bubble size, serves as an indicator of policy \"hacking.\" Low variance implies mode collapse, where the model tends to generate uniform, high-reward outputs. High variance signifies the policy maintains output diversity across various prompts, indicating that it has not collapsed.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08826/x2.png",
                "caption": "Figure 2:Comparison of training dynamics for Regressive vs. Generative reward models during diffusion RL fine-tuning. At the same 2B model scale (Left vs. Middle panel), the generative reward model exhibits significantly superior training dynamics compared to the regression-based one: it facilitates higher exploration magnitude, manifested as greater reward variance, and a more favorable reward growth trend. This higher diversity in reward signals indicates that the generative RM exhibits stronger robustness against reward hacking. Under a regression-based RM, the diffusion model risks learning to exploit reward loopholes to achieve high scores without making substantive progress. This inherent robustness is key to the generative RMâ€™s successful scaling to 26B parameters (Right panel).",
                "position": 277
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08826/x3.png",
                "caption": "Figure 3:Overview of RewardDance framework compared with existing reward model Architecture. (Top): Previous works use CLIP-based or VLM-based reward models to provide scalar reward scores for diffusion model training. (Bottom): Our RewardDance approach (from 1B to 26B parameters) uses task-aware CoT instructions for reward modeling with reasoning. Red flames indicate trainable components; blue ice cubes indicate frozen parameters.",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2509.08826/resources/cot_example.png",
                "caption": "Figure 4:Examples of task-aware instruction and CoT response used for training our RewardDance model.",
                "position": 415
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.08826/x4.png",
                "caption": "Figure 5:This figure shows the reward curves for Seedream during the RL stage, experiments with a separate reward model of varying sizes (1B to 26B). While reward scores consistently improve with more RL iterations across all models, a key trade-off emerges with RM size: larger RMs tend to exhibit a higher standard deviation, suggesting stronger robustness and less susceptibility to reward hacking.",
                "position": 992
            },
            {
                "img": "https://arxiv.org/html/2509.08826/x5.png",
                "caption": "Figure 6:This figure shows the GSB alignment scores for Seedance during the RL stage, experiments with a separate reward model of varying sizes (1B to 26B). Note that the apparent sparsity of fluctuations in this plot (compared to the image task results in Figure5) stems from the lower sampling frequency of data points recorded for the video task. While reward scores consistently improve with more RL iterations, which phenomenon is consistent with Seedream.",
                "position": 995
            },
            {
                "img": "https://arxiv.org/html/2509.08826/x6.png",
                "caption": "Table 7:Both generative reward modeling paradigm and reward context scaling consistently improve performance.",
                "position": 1002
            },
            {
                "img": "https://arxiv.org/html/2509.08826/x6.png",
                "caption": "Figure 7:The performance of Seedream and Seedream-Lite models with different RM model sizes. While both models benefit from larger RMs, the performance improvement is substantially more pronounced for the larger model. This suggests that larger generative models require commensurately larger reward models to realize their full potential.",
                "position": 1148
            },
            {
                "img": "https://arxiv.org/html/2509.08826/x7.png",
                "caption": "Figure 8:Text-to-Image generation comparison across reward models of increasing size (Baseline, 1B, 4B, 8B, 26B). Larger reward models demonstrate progressively better prompt adherence, visual quality, and semantic understanding.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2509.08826/x8.png",
                "caption": "Figure 9:Video generation quality comparison between 2B and 8B reward models. Top two examples: Image-to-Video generation; Bottom two examples: Text-to-Video generation. The 8B reward model shows improved visual quality and temporal consistency compared to the 2B reward model.",
                "position": 1184
            }
        ]
    },
    {
        "header": "5Discussion and Further work",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]