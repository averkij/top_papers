[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x1.png",
                "caption": "",
                "position": 83
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x2.png",
                "caption": "Figure 2:Overview of DyaDiT.DyaDiT conditions on multiple input modalities, including audio, partner motion, relationship type, and personality scores. It employs an Audio Orthogonalization Cross Attention (ORCA) module to obtain cleaner audio representations and a motion dictionary to guide style aware gesture generation.",
                "position": 127
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x3.png",
                "caption": "Figure 3:ORCAreduces ambiguity between the two audio streams, allowingDyaDiTto generate realistic motion even when one person interrupts the other during the conversation. The example demonstrates the generated motions adjusts naturally as the conversation shifts.",
                "position": 165
            }
        ]
    },
    {
        "header": "3DyaDiT",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x4.png",
                "caption": "Figure 4:Qualitative Results.Comparison of visualization results betweenDyaDiT,ConvoFusion[29], andAudio2PhotoReal[3]. The gestures generated by DyaDiT exhibit higher diversity and greater realism compared to the other methods.",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x5.png",
                "caption": "Figure 5:Visualization results under different personality score conditionings. All samples are generated using classifier-free guidance with CFG = 2.5.",
                "position": 400
            }
        ]
    },
    {
        "header": "5User Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x6.png",
                "caption": "Figure 6:Example video pairs used in the user study for evaluating participant preference in conversational gesture generation.",
                "position": 543
            },
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x7.png",
                "caption": "Figure 7:A/B subjective evaluation percentages comparing our method with ConvoFusion[29]and with ground truth. Participants preferred our generated motion due to its more natural and socially aware conversational behavior.",
                "position": 546
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitation & Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "8Clustering of Generated Gestures",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x8.png",
                "caption": "Figure 8:t-SNE clustering results of Relationships (left), Personality Scores (right).",
                "position": 1410
            }
        ]
    },
    {
        "header": "9Implementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23165/2602.23165v1/x9.jpg",
                "caption": "Figure 9:Example of Questionnaires in GoogleForm",
                "position": 1494
            }
        ]
    },
    {
        "header": "10Questionnaire",
        "images": []
    }
]