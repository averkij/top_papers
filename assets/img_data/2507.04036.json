[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04036/extracted/6598338/figs/speaker_logo.png",
                "caption": "",
                "position": 75
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04036/x1.png",
                "caption": "Figure 1:Overview of PresentAgent.It takes documents (e.g., web pages) as input and follows a generation pipeline: (1) document processing, (2) structured slide generation, (3) synchronized caption creation, and (4) audio synthesis. The final output is a presentation video combining visual slides with aligned narration. The purple-highlighted middle results emphasize the system’s key transitional outputs during generation.",
                "position": 140
            },
            {
                "img": "https://arxiv.org/html/2507.04036/x2.png",
                "caption": "Figure 2:Document Diversity in Our Evaluation Benchmark.",
                "position": 151
            },
            {
                "img": "https://arxiv.org/html/2507.04036/x3.png",
                "caption": "Figure 3:Overview of our framework.Our approach addresses the full pipeline of document-to-presentation video generation and evaluation. Left: Given diverse input documents—including papers, websites, blogs, slides, and PDFs—PresentAgent generates narrated presentation videos by producing synchronized slide decks with audio. Right: To evaluate these videos, we introduce PresentEval, a two-part evaluation framework: (1) Objective Quiz Evaluation (top), which measures factual comprehension using Qwen-VL; and (2) Subjective Scoring (bottom), which uses vision-language models to rate content quality, visual design, and audio comprehension across predefined dimensions.",
                "position": 158
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Presentation Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04036/x4.png",
                "caption": "Figure 4:Overview of the PresentAgent framework.Our system takes diverse documents (e.g., papers, websites, PDFs) as input and follows a modular generation pipeline. It first performs outline generation (Step 1) and retrieves the most suitable template (Step 2), then generates slides and narration notes via a vision-language model (Step 3). The notes are converted into audio via TTS and composed into a presentation video (Step 4). To evaluate video quality, we design multiple prompts (Step 5) and feed them into a VLM-based scoring pipeline (Step 6) that outputs dimension-specific metrics.",
                "position": 360
            }
        ]
    },
    {
        "header": "4PresentAgent",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.04036/x5.png",
                "caption": "Figure 5:PresentAgent Demo.Automatically generates academic-style slides and narrated videos from research papers, streamlining the transformation from written content to engaging visual presentations.",
                "position": 583
            }
        ]
    },
    {
        "header": "6Discussion",
        "images": []
    },
    {
        "header": "7Limitation and Future Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]