[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23918/x1.png",
                "caption": "(a)",
                "position": 127
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x1.png",
                "caption": "(a)",
                "position": 130
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x2.png",
                "caption": "(b)",
                "position": 135
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23918/x3.png",
                "caption": "(a)",
                "position": 166
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x3.png",
                "caption": "(a)",
                "position": 169
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x4.png",
                "caption": "(b)",
                "position": 174
            }
        ]
    },
    {
        "header": "2BitStack",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23918/x5.png",
                "caption": "Figure 3:Illustration of a residual block in BitStack. A residual block consists of a sign matrix and singular vectors obtained through absolute value decomposition. The sign matrix can be packed into GPU-supported data types to minimize memory usage.denotes the sign matrix whiledenotes the packed sign matrix.",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2410.23918/extracted/5968472/figures/pdfs/sign_matrix.png",
                "caption": "",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2410.23918/extracted/5968472/figures/pdfs/packed_sign_matrix.png",
                "caption": "",
                "position": 225
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23918/x6.png",
                "caption": "(a)Performance with various sizes.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x6.png",
                "caption": "(a)Performance with various sizes.",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x7.png",
                "caption": "(b)Pair-wise comparison with AWQ.",
                "position": 1152
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x8.png",
                "caption": "Figure 5:Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with or without activation-aware scaling and absolute value decomposition(AVD). In the ‚Äùw/o scaling‚Äù experiments, no scaling is applied as in Eq.4; in the ‚Äùw/o AVD‚Äù experiments, vanilla SVD is used instead of AVD as in Eq.5. For vanilla SVD, we setk‚Ä≤=k+m√ón16√ó(m+n)superscriptùëò‚Ä≤ùëòùëöùëõ16ùëöùëõk^{\\prime}=k+\\frac{m\\times n}{16\\times(m+n)}italic_k start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT = italic_k + divide start_ARG italic_m √ó italic_n end_ARG start_ARG 16 √ó ( italic_m + italic_n ) end_ARG(forùëæ‚àà‚Ñùm√ónùëæsuperscript‚Ñùùëöùëõ{\\bm{W}}\\in\\mathbb{R}^{m\\times n}bold_italic_W ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT) to ensure the size of each residual block matches that of the main experiments. Solid lines represent average zero-shot performance, while dotted lines represent perplexity scores.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x9.png",
                "caption": "Figure 6:Perplexity and average zero-shot performance of BitStack Llama 3.1 8B with 3 different sorting approaches for residual blocks. Solid lines represent average zero-shot performance, while dotted lines represent perplexity scores.",
                "position": 1189
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x10.png",
                "caption": "(a)",
                "position": 1199
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x10.png",
                "caption": "(a)",
                "position": 1202
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x11.png",
                "caption": "(b)",
                "position": 1207
            }
        ]
    },
    {
        "header": "4Related Work",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.23918/x12.png",
                "caption": "Table 4:Qualitative results of BitStack Llama-3.1-8B-Instruct at different compression ratios comparing to AWQ.",
                "position": 3766
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x12.png",
                "caption": "Table 5:Qualitative results of BitStack Llama-3.1-70B-Instruct at different compression ratios comparing to AWQ.",
                "position": 3828
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x12.png",
                "caption": "Figure 8:Perplexity scores on the WikiText2 test set for the BitStack Llama 3.1 8B model. We plot the perplexity scores for memory usage ranging from 4000MB to 5000MB, with a stride of 10MB, to assess BitStack‚Äôs capability for fine-grained trade-offs.",
                "position": 3985
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x13.png",
                "caption": "Figure 9:Generation time for 50 tokens with BitStack Llama 3.1 8B using different lengths of weight stacks(setting the same number of loaded residual blocks for all stacks). Results are evaluated on an NVIDIA H800 GPU.",
                "position": 3991
            },
            {
                "img": "https://arxiv.org/html/2410.23918/x14.png",
                "caption": "Figure 10:Visualization of the weight stacks in BitStack Llama 3.1 8B with three different sorting approaches. We plot the number of residual blocks in each weight stack in the BitStack model, ranging from 4000MB to 6000MB, with a stride of 500MB, due to space constraints.",
                "position": 4012
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]