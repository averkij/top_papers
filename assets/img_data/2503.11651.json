[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11651/x1.png",
                "caption": "",
                "position": 158
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11651/x2.png",
                "caption": "Figure 2:Architecture Overview.Our model first patchifies the input images into tokens by DINO, and appends camera tokens for camera prediction.\nIt then alternates between frame-wise and global self attention layers.\nA camera head makes the final prediction for camera extrinsics and intrinsics, and a DPT[87]head for any dense output.",
                "position": 227
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11651/x3.png",
                "caption": "Figure 3:Qualitative comparison of our predicted 3D points to DUSt3R on in-the-wild images.As shown in the top row, our method successfully predicts the geometric structure of an oil painting, while DUSt3R predicts a slightly distorted plane. In the second row, our method correctly recovers a 3D scene from two images with no overlap, while DUSt3R fails.\nThe third row provides a challenging example with repeated textures, while our prediction is still high-quality.\nWe do not include examples with more than 32 frames, as DUSt3R runs out of memory beyond this limit.",
                "position": 274
            },
            {
                "img": "https://arxiv.org/html/2503.11651/x4.png",
                "caption": "Figure 4:Additional Visualizations of Point Map Estimation.Camera frustums illustrate the estimated camera poses.\nExplore our interactive demo for better visualization quality.",
                "position": 281
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11651/x5.png",
                "caption": "Figure 5:Visualization of Rigid and Dynamic Point Tracking.Top: VGGT‚Äôs tracking moduleùíØùíØ\\mathcal{T}caligraphic_Toutputs keypoint tracks for an unordered set of input images depicting a static scene.\nBottom: We finetune the backbone of VGGT to enhance a dynamic point tracker CoTracker[56], which processes sequential inputs.",
                "position": 947
            },
            {
                "img": "https://arxiv.org/html/2503.11651/x6.png",
                "caption": "Figure 6:Qualitative Examples of Novel View Synthesis.The top row shows the input images, the middle row displays the ground truth images from target viewpoints, and the bottom row presents our synthesized images.",
                "position": 1142
            }
        ]
    },
    {
        "header": "5Discussions",
        "images": []
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AFormal Definitions",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.11651/x7.png",
                "caption": "Figure 7:Single-view Reconstruction by Point Map Estimation.Unlike DUSt3R, which requires duplicating an image into a pair, our model can predict the point map from a single input image.\nIt demonstrates strong generalization to unseen real-world images.",
                "position": 1575
            }
        ]
    },
    {
        "header": "Appendix CAdditional Experiments",
        "images": []
    },
    {
        "header": "Appendix DQualitative Examples",
        "images": []
    },
    {
        "header": "Appendix ERelated Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]