[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related work",
        "images": []
    },
    {
        "header": "Method",
        "images": []
    },
    {
        "header": "3Reconstructor pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/features_reconstruction.drawio.png",
                "caption": "Figure 1:A frozen vision model generates image embeddings, which are then processed by a reconstructor model that learns to approximate image reconstruction.",
                "position": 267
            }
        ]
    },
    {
        "header": "4Feature transformation",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/channel-example.png",
                "caption": "Figure 2:Original image (left) and spatial activation maps for four selected channels of its encoded feature tensorfùëìfitalic_f, illustrating how individual feature channels capture coherent image structures.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/features_reconstruction_manipulation_train_Q.drawio.png",
                "caption": "(a)QùëÑQitalic_QMatrix Calculation",
                "position": 463
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/features_reconstruction_manipulation_eval_Q.drawio.png",
                "caption": "(b)QùëÑQitalic_QMatrix Application",
                "position": 465
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/siglip12_clip_reconstruction_similarities.png",
                "caption": "(a)CLIP-Score",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/siglip12_clip_reconstruction_similarities.png",
                "caption": "(a)CLIP-Score",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/siglip12_siglip_reconstruction_similarities.png",
                "caption": "(b)SigLIP2-Score",
                "position": 582
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/siglip12_reconstruction_sample_stop.png",
                "caption": "(c)Reconstruction Samples",
                "position": 587
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/color-swap-mini.png",
                "caption": "Figure 5:Color-swap via orthogonal rotations in SigLIP2 feature space. Each row presents: (1) the original image, (2) its reconstruction from encoder features, (3) the image after swapping red and blue channels in pixel space, (4) the reconstruction of the pixel-swapped image, and (5) the reconstruction obtained by applying the corresponding orthogonal self-conjugated channel-swap directly in feature space. The near-identical results in columns 4 and 5 confirm that simple rotations in latent space induce coherent, interpretable color edits in the reconstructed images.",
                "position": 655
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/color_swap_all_eigen_values.png",
                "caption": "Figure 6:Color swap operator properties ablation.Rows:(1) sample for transformed image reconstruction, (2)Afsubscriptùê¥ùëìA_{f}italic_A start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPToperator eigenvalues visualization, (3) values of real part ofAfsubscriptùê¥ùëìA_{f}italic_A start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPTeigenvalues.Columns:(1) self-conjugated and orthogonal operator constraints, (2) orthogonal only constraint (3) no constraints",
                "position": 667
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/b_suppression_all_eigen_values.png",
                "caption": "Figure 7:Ablation of operator properties for blue-channel suppression with coefficientŒ±=0.9ùõº0.9\\alpha=0.9italic_Œ± = 0.9.Rows:(1) Sample for transformed image reconstruction, (2)Afsubscriptùê¥ùëìA_{f}italic_A start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPToperator eigenvalues visualization, (3) Values of real part ofAfsubscriptùê¥ùëìA_{f}italic_A start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPTeigenvalues.Columns:(1) Single application of the operator , (2) Quadruple operator application, (3) Eightfold operator application, (4) Twelvefold operator application. As predicted, the eigenvalues are either equal to 1 or inside the circle.",
                "position": 700
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/colorized_examples.png",
                "caption": "Figure 8:Examples of solving the colorization problem by applying a linear transformation in the feature space.",
                "position": 716
            }
        ]
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/all_encoders_metrics.png",
                "caption": "Figure 9:Encoder performance comparison on the COCO val set, showing average CLIP similarity and SigLIP2 similarity between original images and their reconstructions for each vision encoder. Higher bars indicate better alignment of reconstructed images with the originals under each metric.",
                "position": 2415
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/all_encoders_samples.png",
                "caption": "Figure 10:Qualitative comparison of original images and their reconstructions obtained from different vision encoders.",
                "position": 2418
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/siglip12_reconstruction_samples.png",
                "caption": "Figure 11:Qualitative analysis of the SigLIP and SigLIP2 reconstruction samples.",
                "position": 2463
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/color_swap_all_transformations.png",
                "caption": "Figure 12:Color-swap via simple transformations in SigLIP2 feature space. Each row presents: (1) the original image, (2) its reconstruction from encoder features, (3) the image after swapping red and blue channels in pixel space, (4) the reconstruction of the pixel-swapped image, (5) the reconstruction obtained by applying the corresponding orthogonal self-conjugated channel-swap directly in feature space, (6) the reconstruction obtained by applying the corresponding orthogonal channel-swap directly in feature space, (7) the reconstruction obtained by applying the corresponding linear channel-swap directly in feature space. The near-identical results in columns 4 and 5, 6, 7 confirm that simple transformations in latent space induce coherent, interpretable color edits in the reconstructed images.",
                "position": 2466
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/b_suppression_all_transformations.png",
                "caption": "Figure 13:B-channel suppression via linear transformations in SigLIP2 feature space. Each row presents: (1) the original image, (2) its reconstruction from encoder features (3) reconstruction obtained by quadrupling the corresponding linear blue channel suppression operator directly in the feature space, (4) reconstruction obtained by applying the corresponding linear blue channel suppression operator eight times directly in the fisheye space, (5) reconstruction obtained by twelvefold the corresponding linear blue channel suppression operator directly in the feature space, (6) the image after blue channel nulling in pixel space.\nThe near-identical results in columns 5 and 6 confirm that simple transformations in latent space induce coherent,\ninterpretable color edits in the reconstructed images.",
                "position": 2469
            },
            {
                "img": "https://arxiv.org/html/2506.07803/extracted/6525793/images/colorized_all_examples.png",
                "caption": "Figure 14:Examples of solving the colorization problem by applying a linear transformation in the feature space.",
                "position": 2474
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]