[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "23D Shape Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14247/x1.png",
                "caption": "Figure 1:3D Geometry variational autoencoder. (A): Our base VAE for 3D geometry compression. (B) Extended VAE for efficient 3D geometry compression.",
                "position": 200
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/diffusion/diffusion.png",
                "caption": "Figure 2:Diffusion pipline. In the process of training a diffusion model, the DinoV2, CLIP, and VAE Decoder components are kept frozen",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/AM/am.png",
                "caption": "Figure 3:Pipeline for Artist-Created Mesh Generation. Initially, meshes are encoded into discrete token sequences. These sequences are then processed through a decoder-only autoregressive model that utilizes a Transformer network architecture. To enforce multi-modality condition control, a pretrained condition encoder network is employed. This network effectively integrates diverse modalities, ensuring that the generated meshes adhere to specified conditions.",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/AM/am4.png",
                "caption": "Figure 4:Example Meshes Generated by Our Artist-Created Mesh Generation Model. The meshes produced by our model demonstrate superior performance in maintaining topological consistency, showcasing the effectiveness of our approach in generating high-quality artistic meshes.",
                "position": 299
            }
        ]
    },
    {
        "header": "3Texture Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/texture_simp_2.jpg",
                "caption": "Figure 5:Texture Generation Pipeline (input image and mesh from Trellis3D).",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/frontal.png",
                "caption": "Figure 6:Both textual and visual prompts are transformed into a frontal image that is aligned with the frontal-view geometry.",
                "position": 320
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/final_images_d2rgb.png",
                "caption": "Figure 7:Multi-view RGB, albedo, metallic and roughness images.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/final_images_albedo.png",
                "caption": "",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/final_images_matallic.png",
                "caption": "",
                "position": 341
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/final_images_roughness.png",
                "caption": "",
                "position": 342
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/upscaled_albedo.png",
                "caption": "Figure 8:High-resolution albedo images.",
                "position": 354
            }
        ]
    },
    {
        "header": "43D Model Data Processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/dataset/process_pipeline.png",
                "caption": "Figure 9:Data processing pipline. The procedures marked in red are one-off implementations, while the green-boxed elements demand tailored development according to the algorithmic modules deployed on the dataset, we thereby provide exampled steps for jobs described in Section2and Section3.2.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/dataset/scanned_object_image.png",
                "caption": "Figure 10:Typical images of scanned objects in Objaverse dataset[7].\n(A) and (C) are rendered images of the two objects;\n(B) and (D) are corresponding object demonstration in blender.",
                "position": 483
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/dataset/mesh_classification_process.png",
                "caption": "Figure 11:Classify mesh using vLLM by making the vLLM model to describe the model using9999rendered images. Some parts of the structured data (marked with red box) can be used to classify mesh; the aggregated full sentence can be used as the caption of the mesh.",
                "position": 526
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/image-2-mesh-res-01.png",
                "caption": "Figure 12:Visual results with color image as input, the green areas show the rendered multi-view images without textures and the red areas show the rendered multi-view images with textures.",
                "position": 628
            },
            {
                "img": "https://arxiv.org/html/2502.14247/extracted/6223781/figures/texture/text-2-mesh-res-01.png",
                "caption": "Figure 13:Visual results with prompt as input, the green areas show the rendered multi-view images without textures and the red areas show the rendered multi-view images with textures.",
                "position": 631
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]