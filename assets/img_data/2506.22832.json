[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22832/extracted/6578689/nonpdf_list.png",
                "caption": "Figure 1:While naive GRPO provides good generalization and already outperforms supervised fine-tuning, we observe that the reasoning model often contradicts itself, giving correct answers that do not actually align with a reasoning trace. Our Listener mechanism helps with this issue: the model aligns reasoning with the answer better which gives a boost in OOD accuracy.",
                "position": 106
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": []
    },
    {
        "header": "4Soft‚ÄìListener Rewarding",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22832/extracted/6578689/listener_motivation.png",
                "caption": "Figure 2:Listener‚Äìreasoner disagreement is a strong error signal.\nEach point aggregates ImageReward test pairs whose‚Ñì2subscript‚Ñì2\\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTdistance‚à•(s1instr,s2instr)‚àí(s1reason,s2reason)‚à•2subscriptdelimited-‚à•‚à•superscriptsubscriptùë†1instrsuperscriptsubscriptùë†2instrsuperscriptsubscriptùë†1reasonsuperscriptsubscriptùë†2reason2\\lVert(s_{1}^{\\text{instr}},s_{2}^{\\text{instr}})-(s_{1}^{\\text{reason}},s_{2}%\n^{\\text{reason}})\\rVert_{2}‚à• ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT instr end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT instr end_POSTSUPERSCRIPT ) - ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT reason end_POSTSUPERSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT reason end_POSTSUPERSCRIPT ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPTfalls in a bin.\nAccuracy drops as the two score vectors diverge.",
                "position": 325
            },
            {
                "img": "https://arxiv.org/html/2506.22832/x1.png",
                "caption": "Figure 3:Accuracy on the high-qualityrapidata2025humanstylemodern dataset at different human agreement thresholds. Listener mechanism consistently improves generalization beyond the strong GRPO baseline. Supervised Fine-Tuning and Reasoners are initialized from the same Qwen2.5-VL-7B-Instruct checkpoint.",
                "position": 439
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22832/extracted/6578689/majority_vote.jpg",
                "caption": "Figure 4:Majority voting across multiple reasoning rollouts improves models insignificantly in OOD.",
                "position": 634
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "7Limitations & Future Work",
        "images": []
    },
    {
        "header": "Societal Impacts",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]