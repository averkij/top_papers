[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11218/x1.png",
                "caption": "Figure 1:Illustration of ourShort-Long Form Alignment for Factual Question Answering(SLAQ) framework. An instance in ourSLAQbenchmark is acomplexknowledge-seeking query, i.e., alongquery, which consists of fivesimplefactual sub-queries, i.e.,shortqueries, each with an unambiguous correct answer. LLMs independently generate the answers to (1) thelongquery (i.e., all five short queries combined) and (2) each of the five short queries in isolation.\nWe use a state-of-the-art commercial LLM to judge the correctness of the generated answers to both thelongquery andshortqueries against the set of reference answers; we use these judgments\nto compute models’ short- and long-form accuracy (FSF_{S},FLF_{L}) as well as theshort-long alignmentscores.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Background and Related Work",
        "images": []
    },
    {
        "header": "3Factual Consistency over Query Complexity",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11218/x2.png",
                "caption": "Figure 2:Short–long factual alignment results across model families.\n(a)Factual Correctness: per-model short-form accuracyFSF_{S}(green) and long-form accuracyFLF_{L}(purple).\n(b)Alignment:Align\\mathrm{Align}= percentage of facts with the same correctness label in short vs. long responses.\n(c)Signed Alignment: average over topics; for a single topic, the score is the average ofAlign±\\mathrm{Align}_{\\pm}of its five facts.\nModels key: G = Gemma, L = Llama, Q = Qwen (e.g., Q3–8B-R = Qwen-3, 8B parameters, R - reasoning).",
                "position": 224
            },
            {
                "img": "https://arxiv.org/html/2510.11218/x3.png",
                "caption": "Figure 3:Long-form QA dynamics by sub-fact position.\n(a)Slot accuracy: percent correct for each fact position (slots 1–5) in theLQanswer.\n(b)Trailing 1-streak:P​(correct)P(\\mathrm{correct})for the current slot (2–5), conditioned on the length of the immediately preceding run of correct slots.\n(c)Trailing 0-streak:P​(correct)P(\\mathrm{correct})for the current slot (2–5), conditioned on the length of the immediately preceding run of incorrect slots.",
                "position": 249
            }
        ]
    },
    {
        "header": "4Benchmarking and Evaluations",
        "images": []
    },
    {
        "header": "5Mechanistic Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.11218/x4.png",
                "caption": "Figure 4:Circuit similarity comparison between aligned and misaligned facts across six metrics. Aligned facts (green) show significantly (p < 0.001) higher mechanistic similarity than misaligned facts (red) for all measures.",
                "position": 311
            },
            {
                "img": "https://arxiv.org/html/2510.11218/x5.png",
                "caption": "Figure 5:Predictive modeling performance using circuit similarity metrics. (a) Individual feature performance shows Spearman Attention as the strongest single predictor of factual alignment (ROC-AUC = 0.83). (b) Combined features achieve robust performance across all evaluation metrics (ROC-AUC = 0.81, Accuracy = 0.76). (c) Feature importance reveals Spearman Attention as the dominant predictor (coefficient = 1.36).",
                "position": 373
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Limitations",
        "images": []
    },
    {
        "header": "7Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]