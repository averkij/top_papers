[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17487/x1.png",
                "caption": "Figure 1:Overview.(1) We first analyze how downscaling language model size affects multimodal performance, finding that tasks which rely more heavily on the base LLM (e.g., general or knowledge tasks) are largely unaffected, whereas visually-demanding tasks show a disproportionate drop. (2) To uncover the mechanisms underlying the deteriorating visual capabilities under LLM downscaling, we perform a decoupled analysis of perception and reasoning, revealing that perception (alongside reasoning) is a critical bottleneck in small multimodal models. (3) To address these limitations, we present a two-stage perception–reasoning framework, featuringvisual extraction tuning–which trains the model to extract instruction-relevant visual details consistently across tasks–coupled with step-by-step reasoning about the extracted visual details.",
                "position": 64
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3LLM Downscaling Exploration",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17487/x2.png",
                "caption": "Figure 2:LLM downscaling exploration.(Left)Performance dropoff from LLM downscaling most notable for visually demanding tasks.Tasks likeGroundingandPerceptual Similarity(e.g., NIGHTS and PieAPP) which primarily focus on visual processing are most affected by LLM downscaling, rather than tasks which rely heavily on the base LLM (such as ScienceQA evaluating knowledge or GQA assessing general abilities). (Right)The more a task’s performance declines under LLM downscaling, the greater it depends on visual information. As the impact of LLM downscaling increases (8B→\\rightarrow0.6B), so does the task’s reliance on visual information (measured by performance difference with and without visual input). IEI=Image Edit Instruction, VST=Visual Story Telling, Spot-Diff=Spot the Difference, TR-VQA=Text-Rich VQA, MI-VQA=Multi-Image-VQA. Full plots for all datasets are provided in the supplemental material.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x3.png",
                "caption": "Figure 3:Decoupled perception and reasoning downscaling analysis.(a)Decoupled Setup.We disentangle perceptual and reasoning abilities using a two-stage framework: the perception module (VLM) first extracts visually relevant information, then the reasoning module (LLM) generates answers based on the extracted visual information.(b)Perception and reasoning emerge as key bottlenecks under LLM downscaling.We see that LLM downscaling of either the perception module or reasoning module largely degrades in-domain and out-of-domain task performance.(c)Perceptual degradation limits performance across tasks.Even for tasks targeting visual reasoning (e.g., IR and LR), downscaling perception has an impact comparable to–or even exceeding–that of downscaling reasoning.\nIn this per-task analysis, the non-downscaled module is set at 8B. CP=Coarse Perception, FP=Fine-grained Perception, IR=Instance Reasoning, LR=Logical Reasoning, ST=Science & Technology.",
                "position": 338
            }
        ]
    },
    {
        "header": "4Extract+Think",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17487/x4.png",
                "caption": "Figure 4:Captioning alleviates perception bottleneck.Decoupled frameworks use an 8B reasoning module.",
                "position": 424
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x5.png",
                "caption": "Figure 5:Visual extraction tuning.(Top)Simple pipeline for generating visual extraction tuning data.Given a visual instruction tuning example, it is converted to avisual extractiontask by prompting a VLM to describe fine-grained visual details relevant to the original question.(Bottom)Visual extraction tuning enhances perception.Post-training on visual extraction data improves both in-domain and out-of-domain (MMStar) performance. Size indicates the number of parameters of the perception module’s LLM. All setups use an 8B reasoning module.",
                "position": 427
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x5.png",
                "caption": "",
                "position": 430
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x6.png",
                "caption": "Figure 6:CoT reasoning enhances in-domain and out-of-domain performance.Performance gains exhibited at intermediate model scales (4B and 1.7B) for in-domain tasks, while out-of-domain performance improves across all LLM sizes. Both setups use 8B baseline perception module. Per-task plots provided in supplemental material.",
                "position": 516
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "A1Additional LLM Downscaling Results and Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17487/x7.png",
                "caption": "Figure A1:Performance dropoff from downscaling LLM across all datasets.",
                "position": 1026
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x8.png",
                "caption": "Figure A2:Full decoupled results. CP=Coarse Perception, FP=Fine-grained Perception, IR=Instance Reasoning, LR=Logical Reasoning, ST=Science & Technology.",
                "position": 1033
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x9.png",
                "caption": "Figure A3:Decoupled analysis using LLaVA-OneVision as the perception module and Qwen3 as the reasoning module. CP=Coarse Perception, FP=Fine-grained Perception, IR=Instance Reasoning, LR=Logical Reasoning, ST=Science & Technology.",
                "position": 1044
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x10.png",
                "caption": "Figure A4:Decoupled analysis using LLaVA-OneVision as the perception module and Qwen2 as the reasoning module. CP=Coarse Perception, FP=Fine-grained Perception, IR=Instance Reasoning, LR=Logical Reasoning, ST=Science & Technology.",
                "position": 1048
            }
        ]
    },
    {
        "header": "A2Additional Visual Extraction Tuning Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.17487/x11.png",
                "caption": "Figure A7:Visual extraction tuning data examples.",
                "position": 1163
            },
            {
                "img": "https://arxiv.org/html/2511.17487/x12.png",
                "caption": "Figure A8:Full results showing impact of step-by-step reasoning on in-domain and out-of-domain (MMStar) performance.",
                "position": 1174
            }
        ]
    },
    {
        "header": "A3Additional Step-by-step Reasoning Details and Results",
        "images": []
    }
]