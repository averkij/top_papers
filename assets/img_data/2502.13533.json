[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/extracted/6213935/fig/loram_logo.png",
                "caption": "",
                "position": 146
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x1.png",
                "caption": "Figure 1:Idea ofLoRAM",
                "position": 212
            }
        ]
    },
    {
        "header": "2Memory-Efficient LoRA Training —LoRAM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x2.png",
                "caption": "Figure 2:Comparison ofLoRAMand LoRA: Training (subfigures a and b) and Inference (c and d). Key stages include the offline process of the frozen full-rank matrix𝐖0∗superscriptsubscript𝐖0\\mathbf{W}_{0}^{*}bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT(subfigure e) and the online generation of the learnable low-rank matrix𝐖Δ∗superscriptsubscript𝐖Δ\\mathbf{W}_{\\Delta}^{*}bold_W start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT(f) duringLoRAMtraining (b) and inference (d).",
                "position": 328
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x3.png",
                "caption": "Figure 3:The test perplexity of training LLaMA-2-13B & LLaMA-2-70B on OpenHermes.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x4.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x5.png",
                "caption": "Figure 5:The test perplexity & downstream performance of training LLaMA-3.1-70B on OpenHermes.",
                "position": 1216
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x6.png",
                "caption": "Figure 6:Necessity of Recovery & Alignment across different pruning strategies on LLaMA-2-13B.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x7.png",
                "caption": "Figure 7:Effect of scaling parameter reduction ratio.",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x8.png",
                "caption": "Figure 8:Performance of downstream tasks across different parameter reduction ratios.",
                "position": 1276
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CVisualization of Dimension Evolution",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x9.png",
                "caption": "Figure 9:Dimensional evolution of the weight matrices:𝐖0⇒𝐖0𝙿⇒subscript𝐖0superscriptsubscript𝐖0𝙿\\mathbf{W}_{0}\\Rightarrow\\mathbf{W}_{0}^{\\mathtt{P}}bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ⇒ bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_P end_POSTSUPERSCRIPT(a),𝐖Δ⇒𝐖Δ𝙿⇒subscript𝐖Δsuperscriptsubscript𝐖Δ𝙿\\mathbf{W}_{\\Delta}\\Rightarrow\\mathbf{W}_{\\Delta}^{\\mathtt{P}}bold_W start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT ⇒ bold_W start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_P end_POSTSUPERSCRIPT(b), and𝐖Δ𝙿⋆⇒𝐖Δ𝚁⋆⇒superscriptsubscript𝐖Δsuperscript𝙿⋆superscriptsubscript𝐖Δsuperscript𝚁⋆\\mathbf{W}_{\\Delta}^{\\mathtt{P}^{\\star}}\\Rightarrow\\mathbf{W}_{\\Delta}^{%\n\\mathtt{R}^{\\star}}bold_W start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_P start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ⇒ bold_W start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_R start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT(c) duringLoRAM-Strutraining. This includes updates for𝐖qsubscript𝐖q\\mathbf{W}_{\\text{q}}bold_W start_POSTSUBSCRIPT q end_POSTSUBSCRIPT,𝐖ksubscript𝐖k\\mathbf{W}_{\\text{k}}bold_W start_POSTSUBSCRIPT k end_POSTSUBSCRIPT,𝐖vsubscript𝐖v\\mathbf{W}_{\\text{v}}bold_W start_POSTSUBSCRIPT v end_POSTSUBSCRIPT, and𝐖osubscript𝐖o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPTin the attention layer, as well as𝐖upsubscript𝐖up\\mathbf{W}_{\\text{up}}bold_W start_POSTSUBSCRIPT up end_POSTSUBSCRIPT,𝐖gatesubscript𝐖gate\\mathbf{W}_{\\text{gate}}bold_W start_POSTSUBSCRIPT gate end_POSTSUBSCRIPT, and𝐖downsubscript𝐖down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPTin the MLP layer.",
                "position": 2865
            }
        ]
    },
    {
        "header": "Appendix DVisualization of Low-rank Matrices",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x10.png",
                "caption": "Figure 10:Visualization of low-rank matrices in the attention layers of LLaMA-2-13B.",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x11.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x12.png",
                "caption": "Figure 12:Visualization of low-rank matrices in the attention layers of LLaMA-2-70B.",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x13.png",
                "caption": "",
                "position": 2966
            }
        ]
    },
    {
        "header": "Appendix EPerformance of Sub-Tasks in CSR",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x14.png",
                "caption": "Figure 14:Performance of six CSR sub-tasks on the trained LLaMA-2-13B usingLoRAM.",
                "position": 3028
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x15.png",
                "caption": "",
                "position": 3037
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x16.png",
                "caption": "",
                "position": 3044
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x17.png",
                "caption": "",
                "position": 3049
            }
        ]
    },
    {
        "header": "Appendix FAlgorithm ofLoRAM",
        "images": []
    },
    {
        "header": "Appendix GTuning of Learning Rate",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x18.png",
                "caption": "Figure 16:Learning rate tuning for LLaMA-2-7B and LLaMA-2-13B on OpenHermes using LoRA.",
                "position": 3266
            }
        ]
    },
    {
        "header": "Appendix HPerformance of Domain-Specific Task",
        "images": []
    },
    {
        "header": "Appendix IAnalysis of LoRAM Cost",
        "images": []
    },
    {
        "header": "Appendix JAnalysis of Changes in Performance Trends",
        "images": []
    }
]