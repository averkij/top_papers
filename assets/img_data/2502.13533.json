[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/extracted/6213935/fig/loram_logo.png",
                "caption": "",
                "position": 146
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x1.png",
                "caption": "Figure 1:Idea ofLoRAM",
                "position": 212
            }
        ]
    },
    {
        "header": "2Memory-Efficient LoRA Training â€”LoRAM",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x2.png",
                "caption": "Figure 2:Comparison ofLoRAMand LoRA: Training (subfigures a and b) and Inference (c and d). Key stages include the offline process of the frozen full-rank matrixğ–0âˆ—superscriptsubscriptğ–0\\mathbf{W}_{0}^{*}bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT(subfigure e) and the online generation of the learnable low-rank matrixğ–Î”âˆ—superscriptsubscriptğ–Î”\\mathbf{W}_{\\Delta}^{*}bold_W start_POSTSUBSCRIPT roman_Î” end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT(f) duringLoRAMtraining (b) and inference (d).",
                "position": 328
            }
        ]
    },
    {
        "header": "3Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x3.png",
                "caption": "Figure 3:The test perplexity of training LLaMA-2-13B & LLaMA-2-70B on OpenHermes.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x4.png",
                "caption": "",
                "position": 615
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x5.png",
                "caption": "Figure 5:The test perplexity & downstream performance of training LLaMA-3.1-70B on OpenHermes.",
                "position": 1216
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x6.png",
                "caption": "Figure 6:Necessity of Recovery & Alignment across different pruning strategies on LLaMA-2-13B.",
                "position": 1246
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x7.png",
                "caption": "Figure 7:Effect of scaling parameter reduction ratio.",
                "position": 1266
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x8.png",
                "caption": "Figure 8:Performance of downstream tasks across different parameter reduction ratios.",
                "position": 1276
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ARelated Work",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": []
    },
    {
        "header": "Appendix CVisualization of Dimension Evolution",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x9.png",
                "caption": "Figure 9:Dimensional evolution of the weight matrices:ğ–0â‡’ğ–0ğ™¿â‡’subscriptğ–0superscriptsubscriptğ–0ğ™¿\\mathbf{W}_{0}\\Rightarrow\\mathbf{W}_{0}^{\\mathtt{P}}bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT â‡’ bold_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_P end_POSTSUPERSCRIPT(a),ğ–Î”â‡’ğ–Î”ğ™¿â‡’subscriptğ–Î”superscriptsubscriptğ–Î”ğ™¿\\mathbf{W}_{\\Delta}\\Rightarrow\\mathbf{W}_{\\Delta}^{\\mathtt{P}}bold_W start_POSTSUBSCRIPT roman_Î” end_POSTSUBSCRIPT â‡’ bold_W start_POSTSUBSCRIPT roman_Î” end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_P end_POSTSUPERSCRIPT(b), andğ–Î”ğ™¿â‹†â‡’ğ–Î”ğšâ‹†â‡’superscriptsubscriptğ–Î”superscriptğ™¿â‹†superscriptsubscriptğ–Î”superscriptğšâ‹†\\mathbf{W}_{\\Delta}^{\\mathtt{P}^{\\star}}\\Rightarrow\\mathbf{W}_{\\Delta}^{%\n\\mathtt{R}^{\\star}}bold_W start_POSTSUBSCRIPT roman_Î” end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_P start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT â‡’ bold_W start_POSTSUBSCRIPT roman_Î” end_POSTSUBSCRIPT start_POSTSUPERSCRIPT typewriter_R start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT(c) duringLoRAM-Strutraining. This includes updates forğ–qsubscriptğ–q\\mathbf{W}_{\\text{q}}bold_W start_POSTSUBSCRIPT q end_POSTSUBSCRIPT,ğ–ksubscriptğ–k\\mathbf{W}_{\\text{k}}bold_W start_POSTSUBSCRIPT k end_POSTSUBSCRIPT,ğ–vsubscriptğ–v\\mathbf{W}_{\\text{v}}bold_W start_POSTSUBSCRIPT v end_POSTSUBSCRIPT, andğ–osubscriptğ–o\\mathbf{W}_{\\text{o}}bold_W start_POSTSUBSCRIPT o end_POSTSUBSCRIPTin the attention layer, as well asğ–upsubscriptğ–up\\mathbf{W}_{\\text{up}}bold_W start_POSTSUBSCRIPT up end_POSTSUBSCRIPT,ğ–gatesubscriptğ–gate\\mathbf{W}_{\\text{gate}}bold_W start_POSTSUBSCRIPT gate end_POSTSUBSCRIPT, andğ–downsubscriptğ–down\\mathbf{W}_{\\text{down}}bold_W start_POSTSUBSCRIPT down end_POSTSUBSCRIPTin the MLP layer.",
                "position": 2865
            }
        ]
    },
    {
        "header": "Appendix DVisualization of Low-rank Matrices",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x10.png",
                "caption": "Figure 10:Visualization of low-rank matrices in the attention layers of LLaMA-2-13B.",
                "position": 2952
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x11.png",
                "caption": "",
                "position": 2957
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x12.png",
                "caption": "Figure 12:Visualization of low-rank matrices in the attention layers of LLaMA-2-70B.",
                "position": 2961
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x13.png",
                "caption": "",
                "position": 2966
            }
        ]
    },
    {
        "header": "Appendix EPerformance of Sub-Tasks in CSR",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x14.png",
                "caption": "Figure 14:Performance of six CSR sub-tasks on the trained LLaMA-2-13B usingLoRAM.",
                "position": 3028
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x15.png",
                "caption": "",
                "position": 3037
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x16.png",
                "caption": "",
                "position": 3044
            },
            {
                "img": "https://arxiv.org/html/2502.13533/x17.png",
                "caption": "",
                "position": 3049
            }
        ]
    },
    {
        "header": "Appendix FAlgorithm ofLoRAM",
        "images": []
    },
    {
        "header": "Appendix GTuning of Learning Rate",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.13533/x18.png",
                "caption": "Figure 16:Learning rate tuning for LLaMA-2-7B and LLaMA-2-13B on OpenHermes using LoRA.",
                "position": 3266
            }
        ]
    },
    {
        "header": "Appendix HPerformance of Domain-Specific Task",
        "images": []
    },
    {
        "header": "Appendix IAnalysis of LoRAM Cost",
        "images": []
    },
    {
        "header": "Appendix JAnalysis of Changes in Performance Trends",
        "images": []
    }
]