[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_a.png",
                "caption": "(a)",
                "position": 126
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_a.png",
                "caption": "(a)",
                "position": 129
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_b.png",
                "caption": "(b)",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_c.png",
                "caption": "(c)",
                "position": 139
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_d.png",
                "caption": "(d)",
                "position": 145
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_e.png",
                "caption": "(e)",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/frame_f.png",
                "caption": "(f)",
                "position": 155
            }
        ]
    },
    {
        "header": "2Generative Modeling of Operating System Interfaces",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/mouse_color_clipped.png",
                "caption": "(a)High-level temporal architecture of NeuralOS.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/mouse_color_clipped.png",
                "caption": "(a)High-level temporal architecture of NeuralOS.",
                "position": 205
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/keyboard_color_clipped.png",
                "caption": "",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/mouse_color_clipped.png",
                "caption": "(b)RNN structure at steptùë°titalic_t.",
                "position": 236
            }
        ]
    },
    {
        "header": "3NeuralOS Architecture",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/nocircle_rnnpretrain_wait7_frame_08.png",
                "caption": "Figure 3:Multi-stage training pipeline for NeuralOS.(1)RNN Pretraining:The RNN is pretrained to predict latent frames using a mean squared error (MSE) loss.\n(2)Joint Training:The pretrained RNN and the diffusion-based renderer are jointly optimized using a standard diffusion loss.\n(3)Scheduled Sampling:To mitigate error accumulation caused by exposure bias, the most recent input frame is occasionally replaced by a previously generated frame during training.\n(4)Context Length Extension:Input context is extended to enable the model to capture long-term dependencies.",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/nocircle_wait7_384k_frame_11.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/nocircle_movess_prev_frame_11.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/nocircle_prev_frame_34.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/nocircle_movess_prev_frame_25.png",
                "caption": "",
                "position": 382
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/nocircle_wait7_372k_frame_07.png",
                "caption": "",
                "position": 382
            }
        ]
    },
    {
        "header": "4Multi-Stage Training Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/search_tree.png",
                "caption": "Figure 4:Illustration of Search-Tree-Based Data Collection.\nWe construct a search tree representing OS states, starting from the initial desktop screen (root node). Each node corresponds to a distinct OS state, created by clicking or double-clicking interactable GUI elements identified by a computer-use agent. For clarity, only first-level transitions (opening applications) and one deeper exploration within Firefox are shown. This approach enables collecting diverse interaction data efficiently.",
                "position": 398
            }
        ]
    },
    {
        "header": "5Data Collection",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/x1.png",
                "caption": "(a)",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2507.08800/x1.png",
                "caption": "(a)",
                "position": 490
            },
            {
                "img": "https://arxiv.org/html/2507.08800/x2.png",
                "caption": "(b)",
                "position": 495
            }
        ]
    },
    {
        "header": "6Experimental Setup",
        "images": []
    },
    {
        "header": "7Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/rnn_blurry_frame1_rnnpretrain_newckpt76k_frame_06.png",
                "caption": "(a)",
                "position": 532
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/rnn_blurry_frame1_rnnpretrain_newckpt76k_frame_06.png",
                "caption": "(a)",
                "position": 535
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/rnn_blurry_frame2_rnnpretrain_newckpt76k_frame_08.png",
                "caption": "",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/rnn_blurry_frame3_rnnpretrain_newckpt76k_frame_15.png",
                "caption": "",
                "position": 539
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/rnn_blurry_frame4_rnnpretrain_newckpt76k_frame_20.png",
                "caption": "",
                "position": 540
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/no_sched_sampling_frame1_movess_followrnnpretrain_newckpt76k_frame_06.png",
                "caption": "(b)",
                "position": 547
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/no_sched_sampling_frame2_movess_followrnnpretrain_newckpt76k_frame_43.png",
                "caption": "",
                "position": 550
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/no_sched_sampling_frame3_movess_followrnnpretrain_newckpt76k_frame_46.png",
                "caption": "",
                "position": 551
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/no_sched_sampling_frame4_movess_followrnnpretrain_newckpt76k_frame_86.png",
                "caption": "",
                "position": 552
            }
        ]
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Related Work",
        "images": []
    },
    {
        "header": "10Conclusion and Future Work",
        "images": []
    },
    {
        "header": "Acknowledgments and Disclosure of Funding",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_2_past1_frame_63_history.png",
                "caption": "Figure 7:Correct prediction examples by NeuralOS. Each row shows two past frames (columns 1‚Äì2), ground-truth next frame (column 3), and NeuralOS‚Äôs prediction (column 4). Cursor positions are marked one frame in advance with circles (red: move-only, blue: left-click, yellow: right-click). NeuralOS correctly captures various GUI transitions, including opening menus and launching applications.",
                "position": 1079
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_2_past2_frame_64_history.png",
                "caption": "",
                "position": 1089
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_2_gt_frame_65_target.png",
                "caption": "",
                "position": 1090
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_2_pred_frame_66_pred.png",
                "caption": "",
                "position": 1091
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_5_past1_frame_63_history.png",
                "caption": "",
                "position": 1094
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_5_past2_frame_64_history.png",
                "caption": "",
                "position": 1095
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_5_gt_frame_65_target.png",
                "caption": "",
                "position": 1096
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_5_pred_frame_66_pred.png",
                "caption": "",
                "position": 1097
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_3_past1_frame_63_history.png",
                "caption": "",
                "position": 1100
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_3_past2_frame_64_history.png",
                "caption": "",
                "position": 1101
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_3_gt_frame_65_target.png",
                "caption": "",
                "position": 1102
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_3_pred_frame_66_pred.png",
                "caption": "",
                "position": 1103
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_4_past1_frame_63_history.png",
                "caption": "",
                "position": 1106
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_4_past2_frame_64_history.png",
                "caption": "",
                "position": 1107
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_4_gt_frame_65_target.png",
                "caption": "",
                "position": 1108
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/correct_4_pred_frame_66_pred.png",
                "caption": "",
                "position": 1109
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_1_past1_frame_63_history.png",
                "caption": "Figure 8:Prediction examples where the generated frame does not match the ground truth frame. Layout followsFigure7. Note that not all mismatches represent errors. For example, the third row illustrates a case where the screenshot tool window is closed in the ground truth frame but remains open in the prediction. This discrepancy arises because the window-closing action (not shown due to the limited context window) can have variable timing. Thus, both the predicted and ground truth frames are valid outcomes in such scenarios.",
                "position": 1114
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_1_past2_frame_64_history.png",
                "caption": "",
                "position": 1124
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_1_gt_frame_65_target.png",
                "caption": "",
                "position": 1125
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_1_pred_frame_66_pred.png",
                "caption": "",
                "position": 1126
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_2_past1_frame_63_history.png",
                "caption": "",
                "position": 1129
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_2_past2_frame_64_history.png",
                "caption": "",
                "position": 1130
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_2_gt_frame_65_target.png",
                "caption": "",
                "position": 1131
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_2_pred_frame_66_pred.png",
                "caption": "",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_3_past1_frame_63_history.png",
                "caption": "",
                "position": 1135
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_3_past2_frame_64_history.png",
                "caption": "",
                "position": 1136
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_3_gt_frame_65_target.png",
                "caption": "",
                "position": 1137
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_3_pred_frame_66_pred.png",
                "caption": "",
                "position": 1138
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_4_past1_frame_63_history.png",
                "caption": "",
                "position": 1141
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_4_past2_frame_64_history.png",
                "caption": "",
                "position": 1142
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_4_gt_frame_65_target.png",
                "caption": "",
                "position": 1143
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/incorrect_4_pred_frame_66_pred.png",
                "caption": "",
                "position": 1144
            }
        ]
    },
    {
        "header": "Appendix AQualitative Analysis",
        "images": []
    },
    {
        "header": "Appendix BFull State Transition Heatmap",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/x3.png",
                "caption": "Figure 9:Complete heatmap of NeuralOS state transitions. Each cell represents the percentage of predictions corresponding to a predicted cluster (x-axis) given a ground-truth cluster (y-axis). Diagonal entries indicate exact cluster matches.",
                "position": 1168
            }
        ]
    },
    {
        "header": "Appendix CInteractive Web Demo",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/original_image_53.png",
                "caption": "Figure 10:Examples of original images (top row) and their corresponding reconstructions (bottom row) from the trained autoencoder. Despite significant spatial compression (8√ó8\\times8 √ódownsampling), the autoencoder preserves details.",
                "position": 1181
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/original_image_53.png",
                "caption": "",
                "position": 1184
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/original_image_98.png",
                "caption": "",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/original_image_146.png",
                "caption": "",
                "position": 1192
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/original_image_180.png",
                "caption": "",
                "position": 1196
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/reconstruction_image_53.png",
                "caption": "",
                "position": 1201
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/reconstruction_image_98.png",
                "caption": "",
                "position": 1205
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/reconstruction_image_146.png",
                "caption": "",
                "position": 1209
            },
            {
                "img": "https://arxiv.org/html/2507.08800/extracted/6613097/figures/reconstruction_image_180.png",
                "caption": "",
                "position": 1213
            }
        ]
    },
    {
        "header": "Appendix DAutoencoder Details",
        "images": []
    },
    {
        "header": "Appendix EMulti-Stage Training Hyperparameters",
        "images": []
    },
    {
        "header": "Appendix FComputer-Use Agent Prompts",
        "images": []
    },
    {
        "header": "Appendix GCursor Position Prediction Model Training",
        "images": []
    },
    {
        "header": "Appendix HScheduled Sampling Implementation Details",
        "images": []
    }
]