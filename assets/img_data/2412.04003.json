[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04003/extracted/6047129/Figures/marco_fig_init.png",
                "caption": "Figure 1:Comparison of English-centric performance vs Multilingual performance on MMMLU and Flores. Our Marco-LLM demonstrates strong performance on both dimensions.",
                "position": 308
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04003/extracted/6047129/Figures/Marco_Figure.png",
                "caption": "Figure 2:An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM.",
                "position": 395
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Massive Multilingual Continual Pretraining for Large Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04003/x1.png",
                "caption": "Figure 3:The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM.",
                "position": 751
            },
            {
                "img": "https://arxiv.org/html/2412.04003/x2.png",
                "caption": "Figure 7:The performance of different model size on Flores benchmark.Marco-w/o-parallel-data-filteringdenotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data.",
                "position": 1981
            }
        ]
    },
    {
        "header": "4Extensive Multilingual Post-training for Large Language Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2412.04003/x3.png",
                "caption": "Figure 9:Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with a persistent performance gap of  29%.",
                "position": 4032
            },
            {
                "img": "https://arxiv.org/html/2412.04003/x4.png",
                "caption": "Figure 10:Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for a specific language. Win rates indicate Marco-LLM’s superior responses, loss rates represent baseline models’ better performance, and tie rates show equivalent quality responses.",
                "position": 4073
            }
        ]
    },
    {
        "header": "5Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]