[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20540/figures/teaser.png",
                "caption": "Figure 1:Interactive world simulation across diverse environments.The figure showcases selected samples generated byLingBot-World, demonstrating its capability to synthesize high-fidelity videos in various domains, including photorealistic landscapes, scientific visualizations, and stylized artistic worlds. The overlaid keyboard icons (W,A,S,D) highlight the model’s controllability, allowing users to navigate and interact with these dynamic environments seamlessly.",
                "position": 130
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Data Engine",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20540/x1.png",
                "caption": "Figure 2:Game and synthetic data acquisition.The system leverages computational resources and software platforms to capture visual observations that are temporally aligned with action signals and camera states.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x2.png",
                "caption": "Figure 3:Overview of data profiling engine.The process bridges the gap between raw video collections and training-ready assets. It integrates physical attribute filtering, semantic profiling, and geometric annotation to establish a robust foundation for the subsequent hierarchical captioning generation.",
                "position": 421
            }
        ]
    },
    {
        "header": "3LingBot-World",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20540/x3.png",
                "caption": "Figure 4:Overview of the LingBot-World training pipeline.We propose a multi-stage evolution strategy to transform a foundation video generator into an interactive world simulator.Pre-trainingstage establishes a robust general video prior to ensure high-fidelity open-domain generation.Middle-trainingstage injects world knowledge and action controllability, enabling the model to simulate long-term dynamics with consistent interactive logic.Post-trainingstage adapts the architecture for real-time interaction, employing causal attention and few-step distillation to achieve low latency and strict causality.",
                "position": 531
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x4.png",
                "caption": "Figure 5:Pipeline of LingBot-World.The left part shows the pipeline ofLingBot-Worldvideo generation.LingBot-Worlduses an image or a video, noisy latents, and user-defined action signals as inputs to generate video sequences with spatial memory, long-term consistency, and precise action following.\nThe right part shows the architecture of the DiT blocks inLingBot-World.\nThe video latent first passes through a self-attention layer, enablingLingBot-Worldto learn spatiotemporal coherence, and further emerge spatial memory ability.\nThen, action signals are injected through a Plücker Encoder, where the input actions are projected into Plücker embeddings and modulate the video latent via adaptive normalization that transforms the Plücker embeddings into scaling and shifting factors.\nFinally, a cross-attention layer is applied to condition the video latent on text embeddings.",
                "position": 538
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x5.png",
                "caption": "Figure 6:(a) Causal generator adaptation.To enable autoregressive streaming generation, we adapt the high-noise expert usingblock causal attention. This mechanism enforces global causality across chunks while maintaining local bidirectional consistency for efficient action-conditioned rollouts.(b) Discriminator architecture.For long-horizon training, we attach a GAN classification headD​(⋅)D(\\cdot)to the fake score network features. This discriminator uses cross-attention to distinguish real from synthesized sequences to mitigate accumulative drift during distribution matching distillation.",
                "position": 723
            }
        ]
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20540/x6.png",
                "caption": "Figure 7:Qualitative results ofLingBot-World-Base.",
                "position": 822
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x7.png",
                "caption": "Figure 8:Qualitative results ofLingBot-World-Base.",
                "position": 825
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x8.png",
                "caption": "Figure 9:Qualitative results ofLingBot-World-Base.",
                "position": 828
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x9.png",
                "caption": "Figure 10:Qualitative results ofLingBot-World-Fast.",
                "position": 831
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x10.png",
                "caption": "Figure 11:Qualitative results ofLingBot-World-Fast.",
                "position": 834
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x11.png",
                "caption": "Figure 12:Emergent memory capability.Our model exhibits the emergent ability to maintain long-term consistency and reason about unobserved states. Row 1-3: Static landmarks, such as Stonehenge, preserve their structural integrity even after being out of view for 60 seconds. Row 4-5: The model simulates coherent world dynamics even for unobserved regions: the distant bridge appears closer when the camera returns to the frontal view after moving forward (row 4), and the car continues traveling down the road while out of view (row 5).",
                "position": 851
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x12.png",
                "caption": "Figure 13:Ultra-long video generation.We demonstrate the capability of our model to generate coherent video sequences extending up to 10 minutes in duration.",
                "position": 855
            }
        ]
    },
    {
        "header": "5Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2601.20540/x13.png",
                "caption": "Figure 14:Promptable world event. Given a single initial context (left), our model generates diverse future trajectories steered by text prompts. We demonstrate this capability across distinct domains: a fantasy scenario (top) and a realistic scene (bottom). The results highlight our model’s ability to handle bothglobalenvironmental shifts (e.g., “winter”, “pixel art”) and preciselocalinterventions (e.g., “fireworks”, “fish”), all while maintaining physical and temporal coherence.",
                "position": 944
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x14.png",
                "caption": "Figure 15:Application of action agent.\nGiven an initial image, the action agent predicts a sequence of actions that simulate exploration in the environment. The predicted actions are converted into camera trajectories, which drive the subsequent world generation.",
                "position": 950
            },
            {
                "img": "https://arxiv.org/html/2601.20540/x15.png",
                "caption": "Figure 16:3D reconstruction results from LingBot-World generated videos.Reconstructed point clouds from indoor, sci-fi, and outdoor scenarios demonstrate high spatial consistency and geometric fidelity across diverse environments.",
                "position": 954
            }
        ]
    },
    {
        "header": "6Conclusion and Discussion",
        "images": []
    },
    {
        "header": "7Contributors",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]