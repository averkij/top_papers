[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12125/x1.png",
                "caption": "(a)The empirical effectiveness of our methodExOPDcompared with off-policy distillation (SFT), standard OPD, and the weight-extrapolation method ExPO(Zhenget al.,2025)in multi-teacher and strong-to-weak distillation settings (results averaged over 4 math reasoning and 3 code generation benchmarks). (a) When merging multiple domain experts—obtained by applying domain-specific RL to the same base model—back into the original base model, ExOPD is the only method thatyields a unified student that consistently outperforms all domain teachers. (b) ExOPD also yields significant improvements over standard OPD when distilling a smaller student from a larger teacher. Moreover, applying reward correction in ExOPD can further boost distillation performance (Figure14(a)).",
                "position": 112
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Methodology",
        "images": []
    },
    {
        "header": "4Experiments and Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.12125/x1.png",
                "caption": "Figure 2:On-policy distillation results on fourmath reasoningbenchmarks under different choices of reward scaling factorλ\\lambda.",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2602.12125/x1.png",
                "caption": "Figure 2:On-policy distillation results on fourmath reasoningbenchmarks under different choices of reward scaling factorλ\\lambda.",
                "position": 444
            },
            {
                "img": "https://arxiv.org/html/2602.12125/x2.png",
                "caption": "Figure 3:On-policy distillation results on threecode generationbenchmarks under different choices of reward scaling factorλ\\lambda.",
                "position": 450
            },
            {
                "img": "https://arxiv.org/html/2602.12125/x3.png",
                "caption": "(a)Trends in the average number of tokens and the average accuracy of the on-policy distilled models across different benchmarks under varying reward scaling factors. The teacher for math reasoning tasks is Qwen3-4B-Non-thinking-GRPO-Math, while the teacher for code generation tasks is Qwen3-4B-Non-thinking-GRPO-Code.",
                "position": 458
            },
            {
                "img": "https://arxiv.org/html/2602.12125/x3.png",
                "caption": "(a)Training dynamics of OPD and ExOPD in multi-teacher distillation experiments. We visualize using Exponential Moving Average (EMA) smoothing with a coefficient of 0.5.",
                "position": 701
            }
        ]
    },
    {
        "header": "5Conclusion and Discussion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetailed Math Derivations",
        "images": []
    },
    {
        "header": "Appendix BDetailed Training Settings",
        "images": []
    },
    {
        "header": "Appendix CPrompt Templates",
        "images": []
    }
]