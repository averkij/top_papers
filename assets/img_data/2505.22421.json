[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22421/x1.png",
                "caption": "Figure 1:GeoDriveenables precise trajectory following, correct novel view synthesis, and dynamic scene editing in autonomous driving scenarios. Our method integrates robust 3D conditions into driving world models, enhancing spatial understanding and action controllability.",
                "position": 97
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22421/x2.png",
                "caption": "Figure 2:Overview of our training pipeline.We use a pretrained dense stereo model to obtain 3D point clouds and camera trajectories. A dynamic video is rendered from the first-frame point cloud using ourdynamic editingtechnique. The noisy latent representation and rendered video are encoded via a VAE and concatenated as input for ourcondition encoder, modulating the DiT modelâ€™s features. The DiT then generates photorealistic video that accurately follows the specified action conditions.",
                "position": 150
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22421/x3.png",
                "caption": "Figure 3:Illustration ofdynamic editdesign.Compared with default rendering, it effectively reduces disparity between static rendering and dynamic real-world scenarios.",
                "position": 202
            },
            {
                "img": "https://arxiv.org/html/2505.22421/x4.png",
                "caption": "Figure 4:Qualitative comparison of action fidelity under the same conditional frame and action control.Our model precisely follows desired trajectory, while Vista[13]produce misaligned results.",
                "position": 287
            },
            {
                "img": "https://arxiv.org/html/2505.22421/x5.png",
                "caption": "Figure 5:Qualitative Comparisons:Left - Enhanced visual fidelity in our predictions; Right - Superior scene dynamics understanding.",
                "position": 300
            },
            {
                "img": "https://arxiv.org/html/2505.22421/x6.png",
                "caption": "Figure 6:Qualitative comparison on novel-view synthesis on Waymo validation subset.Our model generates sharp results for deviated trajectories in a zero-shot manner, whereas the reconstruction-based method StreetGaussian[73]produces significant artifacts.",
                "position": 323
            }
        ]
    },
    {
        "header": "4Experiments and Applications",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22421/x7.png",
                "caption": "Figure 7:Qualitative Results on Vehicle Manipulation.Our approach allows for the manipulation of vehicle movement directions within a scene by specifying different bounding boxes.",
                "position": 436
            },
            {
                "img": "https://arxiv.org/html/2505.22421/x8.png",
                "caption": "Figure 8:Qualitative Results on Scene Editing.Our approach enables the removal or replacement of vehicles within a scene, allowing for the prediction of seamless future scenarios.",
                "position": 642
            },
            {
                "img": "https://arxiv.org/html/2505.22421/x9.png",
                "caption": "Figure 9:Illustration of application to VLA planning.By simulating each possible planned trajectory, we can assist the VLA model in refining its decisions until it reaches the optimal decision.",
                "position": 645
            }
        ]
    },
    {
        "header": "5Conclusions and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix APreliminary Details",
        "images": []
    },
    {
        "header": "Appendix BExperimental Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2505.22421/x10.png",
                "caption": "Figure 10:Our model can faithfully follow given trajectory and predict consistent future, even when such trajectory is out of training distribution. Yet previous work fail to follow the given action.",
                "position": 1972
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": []
    }
]