[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.20937/figures/huggingface.png",
                "caption": "",
                "position": 217
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x1.png",
                "caption": "",
                "position": 237
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x2.png",
                "caption": "Figure 2:Overview ofENACTdata curation pipeline.We first obtain aligned scene graphs (states) and RGB observations from a mobile manipulation dataset in a robotics simulation (BEHAVIOR). The trajectory is then segmented by identifying key-frames where an abstract state change occurs (i.e., the scene graph difference is non-empty). From this set of key-frames, we sample multiple key-frame trajectories, which are used to construct the forward and inverse world modeling VQA questions. HereNNrefers to the number of all sampled trajectories across all step lengths.",
                "position": 281
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x3.png",
                "caption": "Figure 3:Data sources and QA examples.ENACTis built from diverse, long-horizon activities performed by real robots (left). We provide examples for (mid) forward world modeling and (right) inverse world modeling. More QA examples and prompts are available in the AppendixA.3.3.",
                "position": 333
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x4.png",
                "caption": "Figure 4:Real-World Evaluations.Key frames from the three real-world scenes used in our evaluation: kitchen, dinner table, and workspace. Together, these scenes contain diverse rigid, deformable, and articulated objects in diverse environments with varying lighting conditions.",
                "position": 858
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x5.png",
                "caption": "Figure 5:Evaluations on image realism and anthropocentric bias on human vision throughENACT.Heatmaps show two-tailed unpaired t-test results against the baseline, usingPairwise Accuracy.p<0.05p<0.05is consideredsignificant. Darker red means more significant.Δ\\Deltais the performance change from the baseline. IfsignificantandΔ<0\\Delta<0, the setting is worse than the baseline. C.2 reports the robot’s performance on the left- and right-hand predicates, whereMixingis the proportion of ground truth left or right cases that are predicted as the other hand (i.e., mixing one hand into the other).±\\pmmeans standard error.",
                "position": 1014
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x6.png",
                "caption": "Figure 6:Error Distribution, broken down by forward and inverse tasks, evaluated on GPT-5.",
                "position": 1103
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x7.png",
                "caption": "Figure 9:The distribution of problems by the number of steps in our ENACT benchmark dataset is shown for both forward (left) and inverse (right) world modeling tasks. The dataset is balanced, with a nearly uniform distribution of problems ranging from 3 to 10 steps.",
                "position": 1779
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x8.png",
                "caption": "Figure 10:3-StepForward World Modeling(left) andInverse World Modeling(right) samples.",
                "position": 1885
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x9.png",
                "caption": "Figure 11:6-StepForward World Modeling(left) andInverse World Modeling(right) samples.",
                "position": 1888
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x10.png",
                "caption": "Figure 12:9-StepForward World Modeling(left) andInverse World Modeling(right) samples.",
                "position": 1891
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x11.png",
                "caption": "Figure 13:The annotation interface used for evaluating human performance onForward World Modelingproblems. Annotators are presented with a“Current State”image (top left) and an ordered list of textual actions. The main task is to fill the“Next State”slots by selecting the correct image from the shuffledCandidate Image Libraryon the right. The annotator must follow the sequence of actions, using the result of the previous action as the starting point for the next, to determine the correct chronological order of all future states.",
                "position": 1906
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x12.png",
                "caption": "Figure 14:The annotation interface used for evaluating human performance onInverse World Modelingproblems. Annotators are shown an ordered sequence of state transitions, displayed as pairs of“Current State”and“Next State”images. For each transition, their task is to select the correct action description from a shuffledCandidate Action Librarythat caused the visual change between the two states.",
                "position": 1909
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x13.png",
                "caption": "Figure 17:Illustrative trajectories ofForward World ModelingandInverse World Modelingfor a representative baseline question.",
                "position": 3953
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x14.png",
                "caption": "Figure 18:Ablation experiment results with InternVL3.5-241B-A28B onENACT.Heatmaps show two-tailed unpaired p-values against the baseline, usingPairwise Accuracy.p<0.05p<0.05is consideredsignificant. Darker red means more significant.Δ\\Deltais the performance change from the baseline. IfsignificantandΔ<0\\Delta<0, the setting is worse than the baseline. C.2 reports the robot’s performance on the left- and right-hand predicates, whereMixingis the proportion of ground truth left or right cases that are predicted as the other hand (i.e., mixing one hand into the other hand). Note that, although InternVL3.5-241B-A28B performance is less significant than GPT-5 mini, the|Δ||\\Delta|across unnatural camera configurations still remains high (>0.05>0.05) when the same settings are significant for GPT-5 mini.",
                "position": 3962
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x15.png",
                "caption": "Figure 19:Comparison between Cosmos-Reason1 and other similar-sized models.",
                "position": 3972
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x16.png",
                "caption": "(a)",
                "position": 3984
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x16.png",
                "caption": "(a)",
                "position": 3987
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x17.png",
                "caption": "(b)",
                "position": 3992
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x18.png",
                "caption": "Figure 21:Examples of simulator frames converted into realistic styles for bothForward World Modeling(left) andInverse World Modeling(right) trajectories.",
                "position": 3999
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x19.png",
                "caption": "Figure 23:The figure illustratesForward World Modeling(left) andInverse World Modeling(right) trajectories rendered using the path tracing engine in NVIDIA Isaac Sim.",
                "position": 4067
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x20.png",
                "caption": "Figure 24:Examples of an intermediate rendering style created with a simplified ray tracing pipeline forForward World Modeling(left) andInverse World Modeling(right) trajectories.",
                "position": 4077
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x21.png",
                "caption": "Figure 25:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), captured with a camera aperture of 30.",
                "position": 4179
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x22.png",
                "caption": "Figure 26:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), captured with a camera aperture of 60.",
                "position": 4182
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x23.png",
                "caption": "Figure 27:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), captured with a camera aperture of 80.",
                "position": 4185
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x24.png",
                "caption": "(a)",
                "position": 4188
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x24.png",
                "caption": "(a)",
                "position": 4191
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x25.png",
                "caption": "(b)",
                "position": 4196
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x26.png",
                "caption": "Figure 29:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), captured with a fisheye-style camera.",
                "position": 4210
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x27.png",
                "caption": "Figure 30:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), captured from a camera height of 2.25 m.",
                "position": 4220
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x28.png",
                "caption": "Figure 31:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), captured from a camera height of 1.5 m.",
                "position": 4223
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x29.png",
                "caption": "(a)",
                "position": 4226
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x29.png",
                "caption": "(a)",
                "position": 4229
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x30.png",
                "caption": "(b)",
                "position": 4234
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x31.png",
                "caption": "Figure 33:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), with the robot gripper rendered in white.",
                "position": 4252
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x32.png",
                "caption": "Figure 34:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), with the robot gripper rendered in a random color at each frame.",
                "position": 4255
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x33.png",
                "caption": "Figure 35:Example trajectories ofForward World Modeling(left) andInverse World Modeling(right), with the robot gripper rendered in a human skin–like color.",
                "position": 4258
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x34.png",
                "caption": "(a)",
                "position": 4261
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x34.png",
                "caption": "(a)",
                "position": 4264
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x35.png",
                "caption": "(b)",
                "position": 4269
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x36.png",
                "caption": "(a)",
                "position": 4280
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x36.png",
                "caption": "(a)",
                "position": 4283
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x37.png",
                "caption": "(b)",
                "position": 4288
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x38.png",
                "caption": "(a)",
                "position": 4301
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x38.png",
                "caption": "(a)",
                "position": 4304
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x39.png",
                "caption": "(b)",
                "position": 4309
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x40.png",
                "caption": "(a)",
                "position": 4316
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x40.png",
                "caption": "(a)",
                "position": 4319
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x41.png",
                "caption": "(b)",
                "position": 4324
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x42.png",
                "caption": "(a)",
                "position": 4331
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x42.png",
                "caption": "(a)",
                "position": 4334
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x43.png",
                "caption": "(b)",
                "position": 4339
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x44.png",
                "caption": "(a)",
                "position": 4404
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x44.png",
                "caption": "(a)",
                "position": 4407
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x45.png",
                "caption": "(b)",
                "position": 4412
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x46.png",
                "caption": "Figure 42:The structural error distributions of typical LLMs (GPT-5, GPT-5 mini, Gemini2.5Pro and InternVL3.5-241B-A28B (referred as InternVL3.5… in figure)) and Human-level prediction in both forward and inverse tasks.",
                "position": 4419
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x47.png",
                "caption": "Figure 43:Example of structural errorEntity Substitutionby GPT-5 under forward and inverse tasks.",
                "position": 4422
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x47.png",
                "caption": "",
                "position": 4425
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x48.png",
                "caption": "",
                "position": 4429
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x49.png",
                "caption": "Figure 44:Example of structural errorPredicate Substitutionby GPT-5 under forward and inverse tasks.",
                "position": 4435
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x49.png",
                "caption": "",
                "position": 4438
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x50.png",
                "caption": "",
                "position": 4442
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x51.png",
                "caption": "Figure 45:Example of structural errorPolarity Inversionby GPT-5 under forward and inverse tasks.",
                "position": 4448
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x51.png",
                "caption": "",
                "position": 4451
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x52.png",
                "caption": "",
                "position": 4455
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x53.png",
                "caption": "Figure 46:Example of structural errorOmissionby GPT-5 under forward and inverse tasks.",
                "position": 4461
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x53.png",
                "caption": "",
                "position": 4464
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x54.png",
                "caption": "",
                "position": 4468
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x55.png",
                "caption": "Figure 47:Example of structural errorHallucinationby GPT-5 under forward and inverse tasks.",
                "position": 4474
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x55.png",
                "caption": "",
                "position": 4477
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x56.png",
                "caption": "",
                "position": 4481
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x57.png",
                "caption": "Figure 48:The semantic error distributions of typical LLMs (GPT-5, GPT-5 mini, Gemini2.5Pro and InternVL3.5-241B-A28B (referred as InternVL3.5… in figure)) and Human-level prediction in both forward and inverse tasks.",
                "position": 4494
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x58.png",
                "caption": "Figure 49:Example of semantic errorSpatial Relationsby GPT-5 under forward and inverse tasks.",
                "position": 4497
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x58.png",
                "caption": "",
                "position": 4500
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x59.png",
                "caption": "",
                "position": 4504
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x60.png",
                "caption": "Figure 50:Example of semantic errorFunctional Statesby GPT-5 under forward and inverse tasks.",
                "position": 4510
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x60.png",
                "caption": "",
                "position": 4513
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x61.png",
                "caption": "",
                "position": 4517
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x62.png",
                "caption": "Figure 51:Example of semantic errorMaterial Statesby GPT-5 under forward and inverse tasks.",
                "position": 4523
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x62.png",
                "caption": "",
                "position": 4526
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x63.png",
                "caption": "",
                "position": 4530
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x64.png",
                "caption": "Figure 52:Example of semantic errorAgent Interactionsby GPT-5 under forward and inverse tasks.",
                "position": 4536
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x64.png",
                "caption": "",
                "position": 4539
            },
            {
                "img": "https://arxiv.org/html/2511.20937/x65.png",
                "caption": "",
                "position": 4543
            }
        ]
    },
    {
        "header": "Appendix",
        "images": []
    }
]