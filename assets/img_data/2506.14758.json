[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x1.png",
                "caption": "Figure 1:Top: We augment the advantage in PPO(Schulman et¬†al.,2017b)or GRPO(Shao et¬†al.,2024)with a minimal per-token entropy-based term.Bottom: Our entropy-based advantage effectively encourages exploratory reasoning in LMs, achieving superiorP‚Å¢a‚Å¢s‚Å¢s‚Å¢@‚Å¢KùëÉùëéùë†ùë†@ùêæPass@Kitalic_P italic_a italic_s italic_s @ italic_Kperformance even with extremely largeKùêæKitalic_Kvalues.",
                "position": 118
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x2.png",
                "caption": "Figure 2:Entropy Visualization and Comparison between Exploratory Reasoning and Others. We categorize tokens/actions/behaviors based on their role in the reasoning process. In the visualization, tokens with higher entropy appear in bold and larger sizes, with colors denoting different reasoning roles. In the comparison, we show average entropy values across different categories.",
                "position": 161
            }
        ]
    },
    {
        "header": "2Preliminary Analysis: Entropy and Exploratory Reasoning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x3.png",
                "caption": "Figure 3:Behavior Clustering. t-SNE projection of response embeddings. Base denotes the pre-RL model outputs; RL Other and RL Rare represent common and rare behaviors after RL, respectively.",
                "position": 182
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x4.png",
                "caption": "Figure 4:Dynamics of Entropy-Based Advantage. High entropy initially largely amplifies the advantage, accelerating confidence gain and leading to reduced entropy-based shaping in subsequent steps.",
                "position": 371
            }
        ]
    },
    {
        "header": "4Experiment Settings",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x5.png",
                "caption": "Figure 5:Pass@K Performanceof the LMs with different RL algorithms.",
                "position": 671
            }
        ]
    },
    {
        "header": "6Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x6.png",
                "caption": "Figure 6:Reward, Response Length and Entropyduring the RL training process. The base model is Qwen2.5-Base, and the RL baseline is GRPO. ‚ÄúRL w/ Entropy Reg.‚Äù adds an entropy regularizer to the training objective. ‚ÄúRL w/ Entropy Adv.‚Äù applies entropy-based advantage.",
                "position": 701
            },
            {
                "img": "https://arxiv.org/html/2506.14758/x7.png",
                "caption": "Figure 7:Ratio of Entropy-Based Advantageto the original advantage (i.e.,œà‚Å¢(‚Ñãt)|At|ùúìsubscript‚Ñãùë°subscriptùê¥ùë°\\frac{\\psi(\\mathcal{H}_{t})}{|A_{t}|}divide start_ARG italic_œà ( caligraphic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG | italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | end_ARG).",
                "position": 758
            },
            {
                "img": "https://arxiv.org/html/2506.14758/x8.png",
                "caption": "Figure 8:Comparison of Pivotal Tokens (left), Reflection Actions (middle), and Response Length and Repetition (right)on the testing task between the baseline RL algorithm‚ÄîGRPO (grey)‚Äîand RL with entropy advantage (purple/blue/orange), evaluated across training process.",
                "position": 771
            },
            {
                "img": "https://arxiv.org/html/2506.14758/x9.png",
                "caption": "Figure 9:Response Comparisonbetween RL-trained LMs with and without entropy-based advantage shaping. Certain portions are omitted and the completion version is in AppendixC.",
                "position": 774
            }
        ]
    },
    {
        "header": "7Related Work",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AExtended Preliminary Analysis",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.14758/x10.png",
                "caption": "Figure 10:Entropy Comparison between Exploratory Reasoning and Othersusing DeepSeek-R1-Distill-Qwen-1.5B on code domain.",
                "position": 1614
            }
        ]
    },
    {
        "header": "Appendix BExperiment Settings",
        "images": []
    },
    {
        "header": "Appendix CCase Study",
        "images": []
    }
]