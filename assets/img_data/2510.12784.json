[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12784/logo/HKU_MMLAB.png",
                "caption": "",
                "position": 69
            },
            {
                "img": "https://arxiv.org/html/2510.12784/logo/hku-logo-color1.png",
                "caption": "",
                "position": 75
            },
            {
                "img": "https://arxiv.org/html/2510.12784/logo/nork.png",
                "caption": "",
                "position": 81
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12784/x1.png",
                "caption": "Figure 1:The example on the left suggests that the current UMMs’ understanding module has exceeded the capability of its generation module: the generation module is prone to producing incorrect candidate images based on a given prompt in relevant scenarios, a situation which the understanding module can reasonably identify. This not only highlights a gap between understanding and generation but also reveals the potential for understanding to guide generation. Inspired by this insight, we proposeSRUMto bridge this gap, particularly in complex generation domains.",
                "position": 105
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": []
    },
    {
        "header": "3SRUM: Self-Rewarding for Unified Multimodal Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12784/x2.png",
                "caption": "Figure 2:Showcase of the SRUM pipeline. It consists of two main stages: Self-Rewarding Data Generation and Reward-Weighted Training. The first stage generates high-quality data and scores it to produce a reward signal for the next training stage for self-improvement.",
                "position": 163
            }
        ]
    },
    {
        "header": "4Analysis of Self-Rewarding: Generalization and Principles",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.12784/figures/ab_study.jpg",
                "caption": "Figure 3:Left:Module Evaluation. We report the accuracy drop (Δ\\DeltaAcc. %) from our SRUM. Specifically, 0-1 Reward represents the sparse reward.Right:Hyperparameters Evaluation on T2I-CompBench. We report the accuracy in differentλ\\lambdaunder two modes: CoT and without CoT.",
                "position": 473
            },
            {
                "img": "https://arxiv.org/html/2510.12784/figures/kl_ab.png",
                "caption": "",
                "position": 482
            },
            {
                "img": "https://arxiv.org/html/2510.12784/figures/plot_detail_score_comparisons_absolute.png",
                "caption": "(a)Detail score per step during inference",
                "position": 497
            },
            {
                "img": "https://arxiv.org/html/2510.12784/figures/plot_detail_score_comparisons_absolute.png",
                "caption": "(a)Detail score per step during inference",
                "position": 500
            },
            {
                "img": "https://arxiv.org/html/2510.12784/figures/plot_layout_score_comparisons_absolute.png",
                "caption": "(b)Layout score per step during inference",
                "position": 506
            },
            {
                "img": "https://arxiv.org/html/2510.12784/figures/activation.png",
                "caption": "",
                "position": 518
            },
            {
                "img": "https://arxiv.org/html/2510.12784/figures/wise.jpg",
                "caption": "Figure 6:The results of Bagel on WISE. We use one of the three tasks in WISE and evaluate on the other two, which shows the knowledge in-domain generalization and translation for SRUM.",
                "position": 645
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetail Settings",
        "images": []
    },
    {
        "header": "Appendix BDefinition and Calculation of Average Activation Strength",
        "images": []
    },
    {
        "header": "Appendix CData Curation",
        "images": []
    }
]