[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3The Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.05668/images/ENGEBA_MODEL_acc_comparison.png",
                "caption": "Figure 1:Comparative Performance Across Evaluation Tasks",
                "position": 382
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.05668/images/languagesplitcomparision.png",
                "caption": "Figure 2:Performance For Different Language Splits",
                "position": 449
            }
        ]
    },
    {
        "header": "5Evaluation and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.05668/images/lmeval_accuracy_base_models.png",
                "caption": "Figure 3:Comparison of Llama-GENBA-10B-base against peer base models, showing competitive performance with strong results in English, moderate results in German, and solid results in Bavarian, where it ranks fourth among baseline models.",
                "position": 577
            },
            {
                "img": "https://arxiv.org/html/2509.05668/images/lmeval_accuracy_instruct_models.png",
                "caption": "Figure 4:Performance comparison of Llama-GENBA-10B-instruct against peer instruction-tuned models, showing GENBA-10B-instruct as the best-performing model in Bavarian.",
                "position": 732
            }
        ]
    },
    {
        "header": "6Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]