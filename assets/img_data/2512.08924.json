[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/darts/black_down.png",
                "caption": "",
                "position": 66
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/x1.png",
                "caption": "Figure 1:D4RTis a unified, efficient, feedforward method forDynamic4DReconstruction andTracking, unlocking a variety of outputs including point cloud (), point tracks (), camera parameters () through a single interface.",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/darts/orange_down.png",
                "caption": "",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/darts/pink_down.png",
                "caption": "",
                "position": 110
            },
            {
                "img": "https://arxiv.org/html/2512.08924/x2.png",
                "caption": "Figure 2:D4RTmodel overview‚Äì\nA global self-attention encoder first transforms the input video into the latentGlobal Scene RepresentationFF, which is passed to a lightweight decoder.\nThe decoder can be independently queried for the3D positionùêè\\mathbf{P}of any given 2D point (uu,vv) from the source timesteptsrct_{\\text{src}}at target timestepttgtt_{\\text{tgt}}in camera coordinatetcamt_{\\text{cam}}, unlocking full decoding at any point in space and time.\nThe query also contains an embedding of the local video patch centered around (uu,vv), providing additional spatial context.",
                "position": 113
            }
        ]
    },
    {
        "header": "2Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/darts/blue_down.png",
                "caption": "Table 1:Unified decoding‚Äì\nA diverse set of geometry-related tasks can be inferred by querying the Cartesian product of the respective entries.\nNote that for intrinsics and extrinsics, we only query a coarse(h,w)(h,w)grid for faster inference.",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/darts/red_down.png",
                "caption": "",
                "position": 295
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/darts/white_down.png",
                "caption": "",
                "position": 326
            },
            {
                "img": "https://arxiv.org/html/2512.08924/x3.png",
                "caption": "Figure 3:Pose accuracyvs. speed‚Äì\nWe compare pose accuracyvs. throughput against recent state-of-the-art methods.\nPose accuracy is1 ‚Äì error, averaged over ATE/RTE/RPE on Sintel and ScanNet.\nThroughput is measured in FPS on an A100 GPU.D4RTachieves 200+ FPS pose estimation, ¬†9√ó\\timesfaster than VGGT, and 100√ó\\timesfaster than MegaSaM, while delivering superior accuracy.",
                "position": 569
            }
        ]
    },
    {
        "header": "3Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/x4.png",
                "caption": "Figure 4:Reconstruction results across methods‚Äì\nPure reconstruction methods (MegaSaM andœÄ3\\pi^{3}) are only able to accumulate point clouds of all pixels; exhibiting clear failure cases in dynamic scenes.\nFor example, the swan is repeated in MegaSaM‚Äôs reconstruction,\nandœÄ3\\pi^{3}is failing entirely to reconstruct the flower.\nSpatialTrackerV2, a state-of-the-art tracking method, successfully captures dynamics, however its design only allows tracking points fromoneframe, leaving gaps in the reconstruction (behind the swan and train).D4RTis the only method that successfully reconstructs a full 4D representation of the scene includingallpixels of the video.",
                "position": 603
            },
            {
                "img": "https://arxiv.org/html/2512.08924/x5.png",
                "caption": "Figure 5:Visualizations on in-the-wild videos‚ÄìD4RTdemonstrates accurate reconstructions on static (top row) and dynamic scenes (bottom row).\nIn the presence of motion,D4RTadditionally produces robust 3D point trajectories.",
                "position": 659
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/",
                "caption": "Figure 6:Preservation of low-level details‚Äì We ablate the effect of including local patch information into the model‚Äôs queries, visualizing the resulting depth maps on an example from the Sintel dataset. We find that the local RGB patches help preserve fine-grained details and produce sharper object boundaries.",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/patch/rgb_5.png",
                "caption": "",
                "position": 1530
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/patch/lp_5.png",
                "caption": "",
                "position": 1532
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/patch/np_5.png",
                "caption": "",
                "position": 1532
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AModel Overview & Training Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/x9.png",
                "caption": "Figure 7:FullD4RTmodel overview‚Äì\nWe provide a holistic overview of the model together with its inputs and outputs.\nFC corresponds to a fully connected layer, and PE for positional encoding.\nSeeSec.Àú2of the main paper for reference.",
                "position": 2715
            }
        ]
    },
    {
        "header": "Appendix BGeneralization to Long Videos",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/kitti_1k_frames.png",
                "caption": "(a)Visualization of reconstruction of 1000 frames from KITTI sequence 00.",
                "position": 2767
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/kitti_1k_frames.png",
                "caption": "(a)Visualization of reconstruction of 1000 frames from KITTI sequence 00.",
                "position": 2770
            },
            {
                "img": "https://arxiv.org/html/2512.08924/x10.png",
                "caption": "(b)Comparison of raw chunked prediction alignment results against VGG-T andœÄ3\\pi^{3}baselines (no loop closure).",
                "position": 2776
            }
        ]
    },
    {
        "header": "Appendix CHigh-Resolution Decoding with Subpixel Precision",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/3_rgb.png",
                "caption": "Figure 9:Visualizing sub-pixel detail recovery‚Äì\nWe propose a visual comparison of the different high-res configurations.\nConfig\\scriptsize4‚Éùachieves the highest fidelity, it preserves sharp edges and recovers fine details‚Äîsuch as the hair in the bottom row‚Äîwithout increasing the computational cost or memory requirements of the overall model.",
                "position": 2907
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/3_cfg1.png",
                "caption": "",
                "position": 2913
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/3_cfg2.png",
                "caption": "",
                "position": 2915
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/3_cfg3.png",
                "caption": "",
                "position": 2917
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/3_cfg4.png",
                "caption": "",
                "position": 2919
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/3_gt.png",
                "caption": "",
                "position": 2921
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/2_rgb.png",
                "caption": "",
                "position": 2939
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/2_cfg1.png",
                "caption": "",
                "position": 2941
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/2_cfg2.png",
                "caption": "",
                "position": 2943
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/2_cfg3.png",
                "caption": "",
                "position": 2945
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/2_cfg4.png",
                "caption": "",
                "position": 2947
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/2_gt.png",
                "caption": "",
                "position": 2949
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/8_rgb.png",
                "caption": "",
                "position": 2967
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/8_cfg1.png",
                "caption": "",
                "position": 2969
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/8_cfg2.png",
                "caption": "",
                "position": 2971
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/8_cfg3.png",
                "caption": "",
                "position": 2973
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/8_cfg4.png",
                "caption": "",
                "position": 2975
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/sintel_highres_ablation_v2/8_gt.png",
                "caption": "",
                "position": 2977
            }
        ]
    },
    {
        "header": "Appendix DFurther Ablations",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.08924/x11.png",
                "caption": "Figure 11:Additional visualizations ofD4RT‚ÄìD4RTproduces accurate reconstructions for both static environments (top three rows) and dynamic sequences (bottom three rows).",
                "position": 3090
            },
            {
                "img": "https://arxiv.org/html/2512.08924/x12.png",
                "caption": "Figure 12:Additional reconstruction results across methods‚Äì Pure reconstruction methods (MegaSaM andœÄ3\\pi^{3}) are visualized as accumulated point clouds. For SpatialTrackerV2, we visualize sparse tracks on a representative frame. In contrast,D4RTreconstructs a complete 4D scene representation, trackingallpixels across the entire video.",
                "position": 3094
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/bus_rgb_40.png",
                "caption": "Figure 13:Qualitative depth comparison across methods‚ÄìD4RTis able to perform dense depth estimation with finer details than current state-of-the-art methods, preserving geometric accuracy even in scenarios with large motion blur.",
                "position": 3136
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/bus_megasam_40.png",
                "caption": "",
                "position": 3142
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/bus_pi3_40.png",
                "caption": "",
                "position": 3144
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/bus_stv2_40.png",
                "caption": "",
                "position": 3146
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/bus_ours_40.png",
                "caption": "",
                "position": 3148
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/orchid_rgb_15.png",
                "caption": "",
                "position": 3154
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/orchid_megasam_15.png",
                "caption": "",
                "position": 3155
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/orchid_pi3_15.png",
                "caption": "",
                "position": 3156
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/orchid_stv2_15.png",
                "caption": "",
                "position": 3157
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/orchid_ours_15.png",
                "caption": "",
                "position": 3158
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/libby_rgb_16.png",
                "caption": "",
                "position": 3161
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/libby_megasam_16.png",
                "caption": "",
                "position": 3162
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/libby_pi3_16.png",
                "caption": "",
                "position": 3163
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/libby_stv2_16.png",
                "caption": "",
                "position": 3164
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/libby_ours_16.png",
                "caption": "",
                "position": 3165
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/mtb-race_rgb_21.png",
                "caption": "",
                "position": 3168
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/mtb-race_megasam_21.png",
                "caption": "",
                "position": 3169
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/mtb-race_pi3_21.png",
                "caption": "",
                "position": 3170
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/mtb-race_stv2_21.png",
                "caption": "",
                "position": 3171
            },
            {
                "img": "https://arxiv.org/html/2512.08924/imgs/davis17_depth_comparison/mtb-race_ours_21.png",
                "caption": "",
                "position": 3172
            }
        ]
    },
    {
        "header": "Appendix EFurther Qualitative Results",
        "images": []
    }
]