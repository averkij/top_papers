[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13820/x1.png",
                "caption": "Figure 1:Overview of the CheXWorld framework222The x-rays are taken fromkenhub.com, Radiopedia[47]and ChestX-ray14[65]..The upper part of the figure depicts three dimensions of medical knowledge that are formulated in our framework, including (a) local anatomical structures describing the fine-grained characteristics of local tissues, (b) global anatomical layouts describing the global organization of the human body and (c) domain variations that encourage CheXWorld to model the transitions across different appearance domains of radiographs. The middle part of the figure illustrates the world modeling tasks corresponding to these aspects of medical knowledge. (d) shows our unified pipeline that combines the\nmerits of all three tasks.",
                "position": 100
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Basic Framework of World Modeling",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13820/x2.png",
                "caption": "Figure 2:A basic framework of world modeling.",
                "position": 146
            }
        ]
    },
    {
        "header": "4CheXWorld: World Modeling for Radiograph Representation Learning",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13820/x3.png",
                "caption": "Figure 3:Formulation of global anatomical layout modeling.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x4.png",
                "caption": "Figure 4:Visualization of the CheXWorld predictor outputs(zooming in for details). The images presented in this figure were not included in the pre-training of CheXWorld or the training of the diffusion model.\nRegions in red bounding boxes denote the predictor outputs that are mapped to pixel space using the RCDM[8]framework.\nIn (a), gray areas indicate masked regions excluded from the context. In (b), the two overlapping regions alternately serve as context and target.",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x5.png",
                "caption": "Figure 5:CheXWorld prioritizes relevant medical features over spurious signals(e.g., lateral markers) in the image. Regions in red boxes denote the predictor outputs.",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x5.png",
                "caption": "Figure 5:CheXWorld prioritizes relevant medical features over spurious signals(e.g., lateral markers) in the image. Regions in red boxes denote the predictor outputs.",
                "position": 377
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x6.png",
                "caption": "Figure 6:CheXWorld learns anatomical correspondence.We compute pixel-level embeddings using RoI-pooling[23]and calculate the embedding similarity between four anatomical landmarks in a reference image and each pixel in the test image to create feature similarity heatmaps.",
                "position": 382
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.13820/x7.png",
                "caption": "Table 2:Results on segmentation (left) and few-shot learning (right) tasks.The dice score and the AUROC score are reported for the segmentation and few-shot learning benchmarks respectively.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x7.png",
                "caption": "Figure 7:Fine-tuning with 1%, 10%, and 100% training data on VinDr-CXR.",
                "position": 712
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x8.png",
                "caption": "Figure 8:Visualization of segmentation maskson SIIM-ACR pneumothorax dataset. GT stands for the ground truth masks.",
                "position": 717
            },
            {
                "img": "https://arxiv.org/html/2504.13820/x8.png",
                "caption": "Figure 8:Visualization of segmentation maskson SIIM-ACR pneumothorax dataset. GT stands for the ground truth masks.",
                "position": 719
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgements.",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADatasets and Baselines",
        "images": []
    },
    {
        "header": "Appendix BImplementation Details",
        "images": []
    },
    {
        "header": "Appendix CNumerical Results",
        "images": []
    }
]