[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19427/x1.png",
                "caption": "Figure 1:The Pareto frontier of recent models regarding activated parameters and decoding costs. The darker area is GQA models’ Pareto frontier. Note: Step-3 also has the highest attention effective rank[7], the same as DSv3 and doubling some other models like Qwen3 MoE 235B and Kimi K2.",
                "position": 89
            }
        ]
    },
    {
        "header": "2Step-3 Model Card",
        "images": []
    },
    {
        "header": "3Attention-FFN Disaggregation",
        "images": []
    },
    {
        "header": "4Cost Analysis for LLM Decoding",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19427/x2.png",
                "caption": "Figure 2:Decoding costs (per 1M tokens) of different models and inference configurations. For AFD, we combine the lowest costs on different hardware for attention and FFN, respectively. Reminder: Step-3 has the most activated parameters among them.",
                "position": 1419
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x3.png",
                "caption": "",
                "position": 1422
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x4.png",
                "caption": "Figure 3:Total KV cache size and decoding cost comparison on H800 with hybrid linear attention models like MiniMax M1 and Llama 4 Maverick.",
                "position": 1453
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x5.png",
                "caption": "",
                "position": 1457
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x6.png",
                "caption": "Figure 4:Step-3 and Pangu Pro MoE have very different trends of decoding cost and training cost.",
                "position": 1474
            }
        ]
    },
    {
        "header": "5Model-System Co-design",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19427/x7.png",
                "caption": "Figure 5:The compute and memory access of different attention designs during decoding, including DSv3’s MLA, Qwen3 MoE’s GQA and Step-3’s MFA. The compute-memory-bandwidth ratios of different hardware are also plotted.",
                "position": 1507
            }
        ]
    },
    {
        "header": "6Non-Flagship Hardware Support",
        "images": []
    },
    {
        "header": "7Implementation and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.19427/x8.png",
                "caption": "Figure 6:Module disaggregation in AFD architecture. FFN can be deployed in TP-only, EP-only, or a hybrid TP+EP way, depending on hardware and model architecture.",
                "position": 1766
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x9.png",
                "caption": "Figure 7:Communication topology and the multi-stages pipeline of the AFD architecture.",
                "position": 1784
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x10.png",
                "caption": "Figure 8:StepMesh communication workflow tailored for AFD.",
                "position": 1787
            },
            {
                "img": "https://arxiv.org/html/2507.19427/x11.png",
                "caption": "Figure 9:StepMesh framework for multiple accelerators. AFTensorWorker and AFTensorServer APIs are for attention and FFN instances respectively.",
                "position": 1790
            }
        ]
    },
    {
        "header": "8Conclusion and Future Work",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Core System Contributors",
        "images": []
    },
    {
        "header": "Core Model Architecture Contributors",
        "images": []
    },
    {
        "header": "Contributors (Pretrain, Post-train, Multi-modal, System, Data)",
        "images": []
    },
    {
        "header": "Sponsors",
        "images": []
    }
]