[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07527/x1.png",
                "caption": "(a)Average Accuracy",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x1.png",
                "caption": "(a)Average Accuracy",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x2.png",
                "caption": "(b)Average Response Length",
                "position": 158
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x3.png",
                "caption": "(c)Initial and Final Accuracy",
                "position": 163
            }
        ]
    },
    {
        "header": "2Reinforcement Learning Interleaved with Online Fine-Tuning",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07527/x4.png",
                "caption": "Figure 2:Overview of the ReLIFT Training Framework.The model is mainly trained with RL. When it encounters particularly hard questions, high-quality solutions are collected or generated, then stored in a buffer. Once enough hard examples are gathered, a fine-tuning (FT) step is performed using these examples. This process adaptively alternates between RL and FT to help the model learn from its mistakes and improve reasoning ability. In addition,NùëÅNitalic_Ndenotes the number ofhardest(q,s)ùëûùë†(q,s)( italic_q , italic_s )pairs in the buffer, whileMùëÄMitalic_Mrepresents a predefined threshold, typically set to the batch size for FT.",
                "position": 223
            }
        ]
    },
    {
        "header": "3Experimens",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07527/x5.png",
                "caption": "(a)Training Rewards",
                "position": 1185
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x5.png",
                "caption": "(a)Training Rewards",
                "position": 1188
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x6.png",
                "caption": "(b)Response Lengths",
                "position": 1193
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x7.png",
                "caption": "(c)Number ofHardest",
                "position": 1198
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x8.png",
                "caption": "(d)Training Entropy",
                "position": 1203
            },
            {
                "img": "https://arxiv.org/html/2506.07527/x9.png",
                "caption": "Figure 4:Number of Demonstrations",
                "position": 1605
            }
        ]
    },
    {
        "header": "4Related Works",
        "images": []
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.07527/x10.png",
                "caption": "Figure 5:Chat template for all experiments.",
                "position": 2926
            }
        ]
    },
    {
        "header": "Appendix AExperiment Details",
        "images": []
    }
]