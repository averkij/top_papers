[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01126/extracted/6173720/sections/figures/intro_fig.png",
                "caption": "Figure 1:Relative Confidence Estimation.We first prompt models to elicit their answers to different questions. For each questionqisubscriptùëûùëñq_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we matchqisubscriptùëûùëñq_{i}italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPTwithnùëõnitalic_nother questionsqjsubscriptùëûùëóq_{j}italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPTand generate confidence preference data. We ask the model to compare its level of confidence in the pair of questions and decide which question it ismoreconfident in answering correctly. We treat the questions and answers as ‚Äúplayers‚Äù in these matchups and the confidence preferences as match outcomes. Leveraging rank aggregation techniques used in competitive games, such as Elo rating, we translate the model‚Äôs confidence preferences into confidence scores.",
                "position": 79
            }
        ]
    },
    {
        "header": "2Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01126/extracted/6173720/sections/figures/direct_confidence_prompt_instruction.png",
                "caption": "Figure 2:Direct Confidence Prompt Instruction.Asks the model to directly score its confidence in its answer to a question.",
                "position": 108
            },
            {
                "img": "https://arxiv.org/html/2502.01126/extracted/6173720/sections/figures/relative_confidence_prompt.png",
                "caption": "Figure 3:Relative Confidence Prompt.Asks model to compare its confidence in two questions.",
                "position": 117
            }
        ]
    },
    {
        "header": "3Absolute Confidence Estimation",
        "images": []
    },
    {
        "header": "4Relative Confidence Estimation",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.01126/extracted/6173720/sections/figures/auc_barplot.png",
                "caption": "Figure 4:Selective Classification AUC Across Models.For each model, we plot the selective classification AUC averaged across the 14 tasks for each confidence estimation method. The absolute confidence estimation baselines‚Äîdirect prompting (Direct) and self-consistency (Hybrid SC)‚Äîare indicated in blue, while relative confidence estimation with different rank aggregation methods is in green (Elo Rating, TrueSkill, Bradley-Terry). For Llama 3.1 405B, GPT-4, Gemini 1.5 Pro, and GPT-4o, relative confidence estimates outperform both the direct and hybrid SC absolute confidence baselines. For Claude 3.5 Sonnet, relative confidences outperform direct prompting but slightly underperform self-consistency prompting.",
                "position": 379
            }
        ]
    },
    {
        "header": "6Related Work",
        "images": []
    },
    {
        "header": "7Discussion",
        "images": []
    },
    {
        "header": "8Future Work",
        "images": []
    },
    {
        "header": "9Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]