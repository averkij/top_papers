[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20939/x1.png",
                "caption": "Figure 1:Model capabilities ofARC-Hunyuan-Video-7B, which supports multi-granular timestamped captioning (output time span and corresponding description), summarization, temporal grounding, and open-ended question answering through integrating and reasoning over both visual and audio cues in the user-generated short videos.",
                "position": 219
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20939/x2.png",
                "caption": "Figure 2:(a) Model architecture. Built upon the Hunyuan-7B VLM, we incorporate an audio encoder with fine-grained visual-audio synchronization to obtain temporally aligned multimodal inputs. Timestamps are overlaid on visual frames to provide the model with temporal awareness.(b) Training stages including pre-training, instruction fine-tuning, cold start initialization, RL post-training and final instruction fine-tuning using high-quality human-annotated data and trajectories selected via rejection sampling.",
                "position": 268
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x3.png",
                "caption": "Figure 3:Our automated bootstrapped annotation pipeline for pre-training. It extracts timestamped speech via ASR model and frame-level descriptions via MLLM; these, along with meta information (e.g., title), are input to an LLM for initial video annotation. The annotated data is used to train an initial version of the model, whose inference results are further integrated to produce the final annotations.",
                "position": 300
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20939/x4.png",
                "caption": "Figure 4:An example ofARC-Hunyuan-Video-7B. Given an instructional short video, our model can accurately identify and summarize the content of each step along with the corresponding time spans. For specific questions, the model is also able to locate the relevant time segments within the video, thereby providing precise answers.",
                "position": 438
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x5.png",
                "caption": "Figure 5:An example ofARC-Hunyuan-Video-7B. Given a real-world video with excellent audiovisual quality, our model can analyze the video from visual, auditory, and thematic perspectives, and through reasoning, provide fine-grained segment recommendations.",
                "position": 442
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x6.png",
                "caption": "Figure 6:Examples ofARC-Hunyuan-Video-7B. Given a review-style short video, the model can extract the characteristics of different products based on both visuals and speech. Given a short video consisting of multiple distinct scenes, our model is able to analyze the transitions between these scenes and accurately discern the main theme.",
                "position": 446
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x7.png",
                "caption": "Figure 7:A qualitative comparison between baseline models and our model in understanding short videos where one person plays multiple roles. Our model can accurately identify the events in each scene and provide precise understanding of the main video theme.",
                "position": 487
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x8.png",
                "caption": "Figure 8:A qualitative comparison between baseline models and our model in understanding short videos with rich visual information. Our model can accurately describe the visual content, analyze the background music, and identify the main theme of the video.",
                "position": 491
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x9.png",
                "caption": "Figure 9:A qualitative comparison between baseline models and our model in the ability of temporal video grounding on real-world videos. Our model can effectively analyze visual and audio cues to accurately determine the start and end times of events.",
                "position": 495
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x10.png",
                "caption": "Figure 10:Examples from ShortVid-Bench. The questions, spanning six distinct dimensions, require integrating both visual and audio information for a genuine comprehension of the real-world short videos.",
                "position": 617
            },
            {
                "img": "https://arxiv.org/html/2507.20939/x11.png",
                "caption": "Figure 11:Demonstration ofARC-Hunyuan-Video-7Bâ€™s versatility through minimal fine-tuning for various downstream applications. Specific supervised fine-tuning tasks are mapped to their corresponding real-world scenarios: (a) Brief Summary for Video Retrieval, (b) Detailed Summary for comprehensive Video Tagging, and (c) Extended Browsing Words for Video Recommendation.",
                "position": 621
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.20939/x12.png",
                "caption": "Figure 12:Examples of brief summary. In the first case, we observe that the fine-tunedARC-Hunyuan-Video-7Bcan correctly infer the relationship between the characters in the video, and infer that the person is going home from school through the behavior (carrying a schoolbag into the door). In the second example, the fine-tunedARC-Hunyuan-Video-7Balso correctly identifies the relationship between the characters in the video and describes the complex interaction details between the characters in more detail.",
                "position": 775
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]