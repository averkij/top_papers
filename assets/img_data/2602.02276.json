[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/x1.png",
                "caption": "",
                "position": 210
            },
            {
                "img": "https://arxiv.org/html/2602.02276/figures/k25-main-result.png",
                "caption": "Figure 1:Kimi K2.5 main results.",
                "position": 223
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Joint Optimization of Text and Vision",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/x2.png",
                "caption": "Figure 2:Vision RL training curves on vision benchmarks starting from minimal zero-vision SFT. By scaling vision RL FLOPs, the performance continues to improve, demonstrating that zero-vision activation paired with long-running RL is sufficient for acquiring robust visual capabilities.",
                "position": 386
            }
        ]
    },
    {
        "header": "3Agent Swarm",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/figures/multi-agent-rl-system.png",
                "caption": "Figure 3:An agent swarm has a trainable orchestrator that dynamically creates specialized frozen subagents and decomposes complex tasks into parallelizable subtasks for efficient distributed execution.",
                "position": 468
            },
            {
                "img": "https://arxiv.org/html/2602.02276/x3.png",
                "caption": "Figure 4:In our parallel-agent reinforcement learning environment, the training accuracy increases smoothly as training progresses. At the same time, the level of parallelism during training also gradually increases.",
                "position": 525
            }
        ]
    },
    {
        "header": "4Method Overview",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/figures/te-k2-thinking-radar.png",
                "caption": "Figure 5:Comparison of model performance and token usage for Kimi K2 Thinking following token-efficient RL.",
                "position": 761
            }
        ]
    },
    {
        "header": "5Evaluations",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/x4.png",
                "caption": "Figure 6:The word cloud visualizes heterogeneous K2.5-based sub-agents dynamically instantiated by the Orchestrator across tests.",
                "position": 2923
            },
            {
                "img": "https://arxiv.org/html/2602.02276/x4.png",
                "caption": "Figure 6:The word cloud visualizes heterogeneous K2.5-based sub-agents dynamically instantiated by the Orchestrator across tests.",
                "position": 2926
            },
            {
                "img": "https://arxiv.org/html/2602.02276/x5.png",
                "caption": "Figure 7:Comparison of Kimi K2.5 performance under Agent Swarm and Discard-all context management in BrowseComp.",
                "position": 2931
            },
            {
                "img": "https://arxiv.org/html/2602.02276/x6.png",
                "caption": "Figure 8:Agent Swarm achieves 3×\\times–4.5×\\timesfaster execution time compared to single-agent baselines as target Item-F1 increases from 30% to 70% in WideSearch testing.",
                "position": 2937
            }
        ]
    },
    {
        "header": "6Conclusions",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AContributors",
        "images": []
    },
    {
        "header": "Appendix BPre-training",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/x7.png",
                "caption": "Figure 9:Learning curves comparing vision-to-text ratios (10:90, 20:80, 50:50) under fixed vision-text token budget across vision and language tasks. Early fusion with lower vision ratios tend to yield better results.",
                "position": 4318
            }
        ]
    },
    {
        "header": "Appendix CInfra",
        "images": []
    },
    {
        "header": "Appendix DUnified Agentic Reinforcement Learning Environment",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/x8.png",
                "caption": "Figure 10:Overview of our agentic RL framework.",
                "position": 4402
            }
        ]
    },
    {
        "header": "Appendix EEvaluation Settings",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.02276/x9.png",
                "caption": "Figure 11:Qualitative example of Kimi K2.5 analyzing a complete playthrough ofBlack Myth: Wukong(24 hours of continuous gameplay across 32 videos at 1080p) using parallel visual agents. Seegenerated webpageandsource videos(all rights reserved by source authors).",
                "position": 4871
            },
            {
                "img": "https://arxiv.org/html/2602.02276/x10.png",
                "caption": "Figure 12:Qualitative examples of Kimi K2.5 solving visual reasoning tasks via tool use.",
                "position": 4874
            }
        ]
    },
    {
        "header": "Appendix FVisualization",
        "images": []
    }
]