[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08485/x1.png",
                "caption": "",
                "position": 74
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08485/x2.png",
                "caption": "Figure 2:Overview of InstructX. The MLLM serves as the understanding module, generating editing guidance given the input instruction and visual inputs. The DiT serves as the generation module and connects to the MLLM via learnable queries and an MLP connector.",
                "position": 114
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08485/x3.png",
                "caption": "Figure 3:Different design choices for unified editing modeling.",
                "position": 141
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x4.png",
                "caption": "Figure 4:Illustration of alignment ability (left) and editing performance (right) for different design choices.",
                "position": 147
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08485/x5.png",
                "caption": "Figure 5:Illustration of three training stages of our methods.",
                "position": 178
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x6.png",
                "caption": "Figure 6:Examples for emergent video editing capabilities through image data.",
                "position": 184
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x7.png",
                "caption": "Figure 7:Visual comparsion between our InstructX and other methods on image editing tasks.",
                "position": 190
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x8.png",
                "caption": "Figure 8:Visual comparsion between our InstructX and other methods on video editing tasks.",
                "position": 196
            }
        ]
    },
    {
        "header": "4Experiment",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08485/x9.png",
                "caption": "Figure 9:Ablation study of image/video independent queries (a) and MLLM inputs (b).",
                "position": 1024
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2510.08485/x10.png",
                "caption": "Figure 10:Pipeline for synthesizing paired video data.",
                "position": 2081
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x11.png",
                "caption": "Figure 11:User study example.",
                "position": 2147
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x12.png",
                "caption": "Figure 12:User study result of image edit.",
                "position": 2153
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x13.png",
                "caption": "Figure 13:User study result of video edit.",
                "position": 2159
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x14.png",
                "caption": "Figure 14:Comparison of understanding abilities between MLLM+Diffusion and Diffusion-only setting in instructional editing tasks.",
                "position": 2172
            },
            {
                "img": "https://arxiv.org/html/2510.08485/images/comparison_t5.png",
                "caption": "Figure 15:Comparison of understanding abilities between MLLM+Diffusion and Diffusion-only setting in instructional editing tasks.",
                "position": 2178
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x15.png",
                "caption": "Figure 16:MLLM score system prompt for video edit.",
                "position": 2191
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x16.png",
                "caption": "Figure 17:MLLM score system prompt for reference base video edit.",
                "position": 2197
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x17.png",
                "caption": "Figure 18:Visual comparsion on VIE-Bench.",
                "position": 2210
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x18.png",
                "caption": "Figure 19:Visual comparsion on VIE-Bench.",
                "position": 2216
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x19.png",
                "caption": "Figure 20:Visual comparsion on VIE-Bench.",
                "position": 2222
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x20.png",
                "caption": "Figure 21:Visual comparsion on VIE-Bench.",
                "position": 2228
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x21.png",
                "caption": "Figure 22:More video editing results of our method.",
                "position": 2234
            },
            {
                "img": "https://arxiv.org/html/2510.08485/x22.png",
                "caption": "Figure 23:Visual comparsion on image editing.",
                "position": 2240
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]