[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17198/x1.png",
                "caption": "Figure 1:The BSC-Nav framework for cognitive spatial intelligence.a, Structured spatial memory in biological brains, comprising landmarks, route knowledge, and survey knowledge.b, The BSC-Nav framework instantiates structured spatial memory in embodied agents. Environment observations (RGB-D images and agent poses) are processed by (i) a landmark memory module encodes and retrieves durable associations of multi-modal environmental cues as thelandmarks; and (ii) a cognitive map module accumulates and organizes movement trajectories as theroute knowledgeinto allocentric, map-like representations as thesurvey knowledge. Upon task invocation, (iii) a working memory module dynamically composes relevant spatial knowledge for adaptive planning and reasoning.c, Structured spatial memory enables not only universal navigation but also higher-level spatial-aware skills.",
                "position": 203
            }
        ]
    },
    {
        "header": "2Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17198/x2.png",
                "caption": "Figure 2:Precise localization via hierarchical retrieval in working memory.a, For simple category-level goals, working memory prioritizes retrieval from the landmark memory module for rapid matching and candidate coordinate generation.b, For complex instance-level goals, working memory employs association-enhanced retrieval, converting text instructions into visual features to query the cognitive map module. For image-based goals, the cognitive map is queried directly using extracted visual features.",
                "position": 240
            },
            {
                "img": "https://arxiv.org/html/2508.17198/x3.png",
                "caption": "Figure 3:Goal-directed multi-modal navigation.a, Category-level navigation tasks, including object-goal navigation and open-vocabulary object navigation.b, Instance-level navigation tasks, including text-instance navigation and image-instance navigation.c, Visualization of BSC-Nav navigation trajectories, including the agent’s egocentric observations and target verification via MLLM interaction.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2508.17198/x4.png",
                "caption": "Figure 4:Higher-level spatially-aware skills enabled by BSC-Nav.a, Comparison of BSC-Nav and baseline methods on long-horizon instruction-following tasks using the VLN-CE R2R benchmark.b, Comparison of BSC-Nav and baseline methods on active embodied question answering tasks using the A-EQA benchmark.c, Category-wise performance breakdown across seven question types for BSC-Nav, baselines, and humans in embodied question answering tasks.d,e, Representative examples of BSC-Nav’s navigation trajectories in human instruction navigation (d) and embodied question answering (e) tasks.",
                "position": 284
            },
            {
                "img": "https://arxiv.org/html/2508.17198/x5.png",
                "caption": "Figure 5:Universal navigation capabilities in real-world scenarios.a, The custom-built robotic platform integrates perception, navigation, and manipulation capabilities.b, The indoor experimental environment spans 200 m2and includes diverse functional zones (e.g., office, lounge, reception, and kitchen).c-f, Real-world navigation performance across object-goal, text-instance, and image-instance tasks with 15 distinct targets (c).\nFor each target, we report SR from 5 trials with randomized starting positions (d), final distance to goal (e), and mean navigation velocity (f).g,h, Representative examples of real-world navigation. Each shows a top-down trajectory with timestamps (left) and corresponding egocentric/allocentric views (right).",
                "position": 307
            },
            {
                "img": "https://arxiv.org/html/2508.17198/x6.png",
                "caption": "Figure 6:Real-world mobile manipulation.The coordination between spatial memory-driven navigation and manipulation primitives enables the execution of long-horizon tasks specified through human instructions.a, Single-waypoint task, cleaning stains on a marble table next to a shredder.b, Object transfer task, relocating a cookie box from table to the kitchen island, requiring navigation between manipulation actions.c, Complex multi-step task, preparing breakfast by sequentially navigating to and manipulating three open-vocabulary targets (oatmeal jar, coco balls, and milk bottle) to assemble ingredients on a plate.\nFull demonstration are provided in Supplementary video 8-10.",
                "position": 315
            }
        ]
    },
    {
        "header": "3Discussion",
        "images": []
    },
    {
        "header": "4Methods",
        "images": []
    },
    {
        "header": "Data Availability",
        "images": []
    },
    {
        "header": "Code Availability",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "Author Contributions Statement",
        "images": []
    },
    {
        "header": "Competing Interests Statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APrompt Template",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17198/x7.png",
                "caption": "Figure 1:Construction and exploitation of structured spatial memory.BSC-Nav first initializes structured spatial memory through frontier-based exploration strategies. Observational information is processed in parallel, employing YOLO-World for salient object detection and DINO-V2 for patch-level visual embedding extraction, which are respectively used to construct the landmark memory and the cognitive map. During navigation initialization, the working memory module performs hierarchical retrieval based on instruction modality and granularity. For simple textual targets, BSC-Nav employs LLMs for contextual retrieval over the landmark memory.\nFor specific textual descriptions and image targets, BSC-Nav performs associative enhanced retrieval over the cognitive map to generate candidate exploration sequences.",
                "position": 3452
            },
            {
                "img": "https://arxiv.org/html/2508.17198/x8.png",
                "caption": "Figure 2:Additional results of navigation performance.a, Comparison of SR and SPL between BSC-Nav and the state-of-the-art Unigoal method in object-goal navigation using landmark memory only, cognitive map only, and both.b, Category-wise SR and SPL across 20 object categories in MP3D and 6 object categories in HM3D.c, Number of candidate coordinates explored during successful navigation episodes on MP3D and HM3D. B, landmark memory retrieval only. Q, cognitive map retrieval only. B+Q, both B and Q.\nIt can be seen that most successful navigation episodes achieve the goal at the first explored coordinate.d, The cumulative SR and cumulative SPL as the number of explored candidate locations increases. Additional explorations improve SR (navigation success) but reduce SPL (navigation efficiency).",
                "position": 3458
            }
        ]
    },
    {
        "header": "Appendix BSupplementary Figures and Videos",
        "images": []
    }
]