[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04534/x1.png",
                "caption": "",
                "position": 84
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Text-Music-Motion Aligned Data Generation",
        "images": []
    },
    {
        "header": "4UniMuMo Framework",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04534/x2.png",
                "caption": "Figure 2:Overview: The training of UniMuMo consists of three stages: In stage 1, we train a motion RVQ-VAE using the frozen codebook from a pre-trained music RVQ-VAE to encode motion into the same space as music. In stage 2, we fine-tune a pre-trained music transformer decoder model on the text-to-music-motion task using the music-motion parallel generation scheme. In stage 3, we fine-tune a T5 decoder for music-motion captioning using the previous music-motion decoder as a feature extractor.",
                "position": 212
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AVisual Beats Detection Details",
        "images": []
    },
    {
        "header": "Appendix BMusic Motion Alignment Details",
        "images": []
    },
    {
        "header": "Appendix CText Description Construction Details",
        "images": []
    },
    {
        "header": "Appendix DIllustration of Music Motion Parallel Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04534/extracted/5905231/figs/Parrallel_training.png",
                "caption": "(a)An illustration of one forward pass during UniMuMo’s training. Music tokens (including the music start token) are denoted in blue blocks and motion tokens (including the motion start token) are denoted in orange blocks. The numbers in the block denote the timestep of each token.",
                "position": 2113
            },
            {
                "img": "https://arxiv.org/html/2410.04534/extracted/5905231/figs/Parrallel_training.png",
                "caption": "(a)An illustration of one forward pass during UniMuMo’s training. Music tokens (including the music start token) are denoted in blue blocks and motion tokens (including the motion start token) are denoted in orange blocks. The numbers in the block denote the timestep of each token.",
                "position": 2116
            },
            {
                "img": "https://arxiv.org/html/2410.04534/x3.png",
                "caption": "(b)An illustration of the delay pattern in MusicGen. Each color represents a different layer of the residual codebook, and the numbers on the blocks indicate the timestep. After applying the delay pattern, the tokens denoted in grey are padded with a special empty token.",
                "position": 2122
            },
            {
                "img": "https://arxiv.org/html/2410.04534/extracted/5905231/figs/Parrallel_inference.png",
                "caption": "(a)An illustration of how UniMuMo conduct music motion parallel generation. In each timestep T, one forward pass is performed.",
                "position": 2129
            },
            {
                "img": "https://arxiv.org/html/2410.04534/extracted/5905231/figs/Parrallel_inference.png",
                "caption": "(a)An illustration of how UniMuMo conduct music motion parallel generation. In each timestep T, one forward pass is performed.",
                "position": 2132
            },
            {
                "img": "https://arxiv.org/html/2410.04534/extracted/5905231/figs/Parrallel_inference_single_modal.png",
                "caption": "(b)An illustration of UniMuMo’s motion-to-music generation. The music-to-motion generation is similar to this process.",
                "position": 2138
            }
        ]
    },
    {
        "header": "Appendix EUser Study on Music-Motion Alignment",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.04534/extracted/5905231/figs/user_study.png",
                "caption": "Figure 5:A screen shot of the user study form for evaluating our music-motion alignment algorithm.",
                "position": 2158
            }
        ]
    },
    {
        "header": "Appendix FImplementation Details",
        "images": []
    },
    {
        "header": "Appendix GMore Analysis on Ablation Study",
        "images": []
    }
]