[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08635/x1.png",
                "caption": "Figure 1:Overview of the proposed 3D LDAE framework for unsupervised representation learning in brain medical imaging. The framework consists of three key components: (i) acompression model(â„°â„°\\mathcal{E}caligraphic_Eandğ’Ÿğ’Ÿ\\mathcal{D}caligraphic_D), which encodes high-dimensional MRI brain scans (ğ±0subscriptğ±0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT) into a lower-dimensional latent representation (ğ³0subscriptğ³0\\mathbf{z}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT), facilitating efficient processing; (ii) aLatent Diffusion Model (LDM), trained to learn the distribution of the compressed representations through a diffusion process, progressively transformingğ³0subscriptğ³0\\mathbf{z}_{0}bold_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPTintoğ³Tsubscriptğ³ğ‘‡\\mathbf{z}_{T}bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTand vice versa via a U-Net-based denoising network (UNetÎ¸subscriptUNetğœƒ\\text{UNet}_{\\theta}UNet start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT); and (iii) thesemantic encoder-decoder model(GÏˆsubscriptğºğœ“G_{\\psi}italic_G start_POSTSUBSCRIPT italic_Ïˆ end_POSTSUBSCRIPTandEâ¢nâ¢cÏ•ğ¸ğ‘›subscriptğ‘italic-Ï•Enc_{\\phi}italic_E italic_n italic_c start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT), which learns a meaningful latent representation (ğ²sâ¢eâ¢msubscriptğ²ğ‘ ğ‘’ğ‘š\\mathbf{y}_{sem}bold_y start_POSTSUBSCRIPT italic_s italic_e italic_m end_POSTSUBSCRIPT) from the input scan and utilizes it to guide the reverse diffusion process via a gradient estimator (GÏˆsubscriptğºğœ“G_{\\psi}italic_G start_POSTSUBSCRIPT italic_Ïˆ end_POSTSUBSCRIPT). This approach enables structured semantic learning, facilitating interpretable image synthesis, counterfactual generation, and disease-specific attribute disentanglement.",
                "position": 202
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08635/x2.png",
                "caption": "Figure 2:Architecture of the Semantic Encoder used in the LDAE framework.The input 3D brain MRI scanx0âˆˆâ„HÃ—WÃ—Dsubscriptğ‘¥0superscriptâ„ğ»ğ‘Šğ·x_{0}\\in\\mathbb{R}^{H\\times W\\times D}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_H Ã— italic_W Ã— italic_D end_POSTSUPERSCRIPTis sliced along the axial plane into a sequence of 2D slices, each processed independently through a shared 2D CNN backbone (e.g., ConvNeXt-Small) to extract slice-level embeddingseiâˆˆâ„dsubscriptğ‘’ğ‘–superscriptâ„ğ‘‘e_{i}\\in\\mathbb{R}^{d}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. These embeddings are then aggregated via a two-stage attention mechanism:SoftAttentioncomputes a global summary vectorQğ‘„Qitalic_Qas a weighted mean over the sequence, andCrossAttentionsimulates self-attention by queryingQğ‘„Qitalic_Qwith the original embeddingsEğ¸Eitalic_E, yielding a final global non-spatial semantic vectorysemâˆˆâ„dsubscriptğ‘¦semsuperscriptâ„ğ‘‘y_{\\text{sem}}\\in\\mathbb{R}^{d}italic_y start_POSTSUBSCRIPT sem end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPTused to guide the reverse diffusion process.",
                "position": 363
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/549_AD_AE_reconstruction.png",
                "caption": "Figure 3:Qualitative reconstructions from AutoencoderKL. Top row: original scan slices. Middle row: reconstructed outputs from compressed latent codes. Bottom row: reconstruction error. The reconstructions preserve global and local anatomical features despite the170Ã—170\\times170 Ã—compression.",
                "position": 684
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/0_CN_stochastic.png",
                "caption": "Figure 4:Reconstruction from semantic code and stochastic latent.Reconstruction results for two representative subjects from the test set: a CN subject (left block) and an AD subject (right block).First column:original brain MR scan.Second column:reconstruction obtained using both the semantic embeddingysem=EncÏ•â¢(x0)subscriptğ‘¦semsubscriptEncitalic-Ï•subscriptğ‘¥0y_{\\text{sem}}=\\text{Enc}_{\\phi}(x_{0})italic_y start_POSTSUBSCRIPT sem end_POSTSUBSCRIPT = Enc start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )and the stochastic latentzTsubscriptğ‘§ğ‘‡z_{T}italic_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTobtained via DDIM inversion of the encoded compressed latentz0=â„°â¢(x0)subscriptğ‘§0â„°subscriptğ‘¥0z_{0}=\\mathcal{E}(x_{0})italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = caligraphic_E ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ).Columns 3â€“5:reconstructions obtained by keepingysemsubscriptğ‘¦semy_{\\text{sem}}italic_y start_POSTSUBSCRIPT sem end_POSTSUBSCRIPTfixed and sampling differentzT(i)âˆ¼ğ’©â¢(0,I)similar-tosuperscriptsubscriptğ‘§ğ‘‡ğ‘–ğ’©0ğ¼z_{T}^{(i)}\\sim\\mathcal{N}(0,I)italic_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT âˆ¼ caligraphic_N ( 0 , italic_I ). Despite the stochastic variation, the reconstructions retain global anatomical and disease-relevant structure, indicating thatysemsubscriptğ‘¦semy_{\\text{sem}}italic_y start_POSTSUBSCRIPT sem end_POSTSUBSCRIPTcaptures high-level semantics whilezTsubscriptğ‘§ğ‘‡z_{T}italic_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPTencodes low-level variability. In the AD subject, expected pathological traits (e.g., ventricular enlargement) are consistently preserved, whereas in the CN subject, normal cortical volume and structure remain stable across samples.",
                "position": 687
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/356_AD_stochastic.png",
                "caption": "",
                "position": 690
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": []
    },
    {
        "header": "5Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/lda_plot.png",
                "caption": "Figure 5:LDA projection of semantic representations (ğ²sâ¢eâ¢msubscriptğ²ğ‘ ğ‘’ğ‘š\\mathbf{y}_{sem}bold_y start_POSTSUBSCRIPT italic_s italic_e italic_m end_POSTSUBSCRIPT) extracted from the LDAE encoder. Each point corresponds to a 3D brain scan colored by diagnostic class (AD or CN). The clear separation suggests that the learned semantic space captures clinically meaningful features relevant to Alzheimerâ€™s disease.",
                "position": 1374
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/manipulation_mutiple_alphas.png",
                "caption": "Figure 6:Progressive manipulation of an AD subject toward the CN class. The manipulation strengthÎ±ğ›¼\\alphaitalic_Î±ranges from 0.0 (original reconstruction) to 5.0. Structural changesâ€”especially hippocampal recovery and ventricle shrinkageâ€”become more evident with largerÎ±ğ›¼\\alphaitalic_Î±.",
                "position": 1562
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/356_AD_to_CN_1.5.png",
                "caption": "Figure 7:Semantic manipulation examples along the direction defined by the vector orthogonal to the classifierâ€™s decision boundary.\nIn the ADâ†’â†’\\rightarrowâ†’CN case (top), we observe a reduction of hippocampal atrophy;\nin the CNâ†’â†’\\rightarrowâ†’AD case (bottom), atrophy becomes more prominent.",
                "position": 1565
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/628_CN_to_AD_1.5.png",
                "caption": "",
                "position": 1569
            },
            {
                "img": "https://arxiv.org/html/2504.08635/x3.png",
                "caption": "(a)Mean Squared Error (MSE) as a function of prediction gap. Larger gaps correspond to more difficult interpolation tasks.",
                "position": 1575
            },
            {
                "img": "https://arxiv.org/html/2504.08635/x3.png",
                "caption": "(a)Mean Squared Error (MSE) as a function of prediction gap. Larger gaps correspond to more difficult interpolation tasks.",
                "position": 1578
            },
            {
                "img": "https://arxiv.org/html/2504.08635/x4.png",
                "caption": "(b)Structural Similarity Index (SSIM) as a function of prediction gap. The model shows robust perceptual consistency across all temporal ranges.",
                "position": 1583
            },
            {
                "img": "https://arxiv.org/html/2504.08635/extracted/6354541/interpolation_comparison.png",
                "caption": "Figure 9:Qualitative example of latent interpolation for missing scan generation on a single subject with four longitudinal scans acquired at 0, 6, 12, and 24 months. The images at months 0 and 24 serve as endpoints (Î±=0ğ›¼0\\alpha=0italic_Î± = 0andÎ±=1ğ›¼1\\alpha=1italic_Î± = 1) for interpolation in the latent space. Intermediate scans at 6 months (Î±=0.25ğ›¼0.25\\alpha=0.25italic_Î± = 0.25) and 12 months (Î±=0.5ğ›¼0.5\\alpha=0.5italic_Î± = 0.5) are synthesized via linear interpolation in the semantic space and spherical interpolation in the stochastic space.",
                "position": 1643
            }
        ]
    },
    {
        "header": "6Discussion and conclusion",
        "images": []
    },
    {
        "header": "Declaration of generative AI and AI-assisted technologies in the writing process",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]