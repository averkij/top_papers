[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02357/x1.png",
                "caption": "Figure 1:Illustration of the unified evaluation dimensions ofQ-Eval-100K. We focus onvisual quality(including all factors that may impact the viewing experience) andalignment level, which measures the accuracy of the generated content to the prompt.",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02357/x2.png",
                "caption": "Figure 2:Illustration of theSample and Scrutinizequality control strategy for annotations inQ-Eval-100K. We randomly select a sample of 5K instances from the full dataset, which are then reviewed by experts to establish golden scores. A batch of annotations is approved only if the scores of the sampled instances show a high correlation with these expert-assigned golden scores.",
                "position": 324
            }
        ]
    },
    {
        "header": "3Q-Eval-100K Construction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02357/x3.png",
                "caption": "Figure 3:Overview of theQ-Eval-100Kconstruction process. We design a wide range of prompts and employ various text-to-vision models to generate diverse content. Subjective evaluations are then conducted to rate the visual quality and alignment of these generated instances. The resulting scores form the SFT dataset, which can help inject corresponding knowledge into LMMs.",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x4.png",
                "caption": "(a)Image Alignment",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x4.png",
                "caption": "(a)Image Alignment",
                "position": 368
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x5.png",
                "caption": "(b)Video Alignment",
                "position": 373
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x6.png",
                "caption": "(c)Image Visual Quality",
                "position": 379
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x7.png",
                "caption": "(d)Video Visual Quality",
                "position": 384
            }
        ]
    },
    {
        "header": "4Q-Eval-Score",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02357/x8.png",
                "caption": "Figure 5:The pipeline of the proposedQ-Eval-Scoremodel involves multiple stages. First, theQ-Eval-100KSFT dataset is used to train the LMM on visual quality and alignment knowledge. Then, context prompts are applied to guide the LMM towards generating more detailed and accurate outputs. Finally, the rating token probabilities are converted into predicted scores. Additionally, long prompt alignment is achieved through aVague-to-Specificstrategy to further refine the modelâ€™s responses.",
                "position": 412
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x9.png",
                "caption": "Figure 6:An example of theVague-to-Specificstrategy. The original long prompt is divided by the LLM (QwenLM[5]) into aVague Promptand severalSpecific Prompts. The alignment score is first calculated separately for each part, then combined using weighted averaging to form the final score.",
                "position": 564
            }
        ]
    },
    {
        "header": "5Experiment",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Acknowledgment",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "7Dataset Construction Details",
        "images": []
    },
    {
        "header": "8Long Prompt Split",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.02357/x10.png",
                "caption": "(a)",
                "position": 2535
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x10.png",
                "caption": "(a)",
                "position": 2538
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x11.png",
                "caption": "(b)",
                "position": 2543
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x12.png",
                "caption": "Figure 8:Visualization comparison results.",
                "position": 2551
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x13.png",
                "caption": "Figure 9:Variance probability distributions for images/videos of Q-Eval-100K respectively.",
                "position": 2565
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x13.png",
                "caption": "",
                "position": 2568
            },
            {
                "img": "https://arxiv.org/html/2503.02357/x14.png",
                "caption": "",
                "position": 2573
            }
        ]
    },
    {
        "header": "9Performance Details",
        "images": []
    },
    {
        "header": "10Data Statement",
        "images": []
    },
    {
        "header": "11Broader Impact and Limitations",
        "images": []
    }
]