[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04676/x1.png",
                "caption": "Figure 1:Traditional replay vs. GeRe: unlike traditional replay requiring laborious collection of an increasing set of downstream replay samples, GeRe simply employs a fixed set of general replay samples to not only retain general capabilities in continual learning, but also enhance the overall performance of learned downstream tasks. The blue oval is the threshold-based margin loss that imposes consistency constraint on neural activation state under GeRe frameworks.",
                "position": 115
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04676/x2.png",
                "caption": "Figure 2:(a) Flowchart of the GeRe framework using general replay samples, including distillation of hidden states and the derived activation state in offline mode, and continual learning across sequential tasks with mixing general samples for replay.\n(b) Illustration of threshold-based margin loss, which transforms the hidden values into discrete activation states on both the target and prediction followed by margin loss calculation.",
                "position": 201
            }
        ]
    },
    {
        "header": "3Proposed Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.04676/x3.png",
                "caption": "Figure 3:A comparable baseline series of distinct replay-based optimization targets (left to right): native non-replay Baseline, vanilla replay BaselineR, replay with different distillation strategies regarding logits imitation BaselineR+KL and feature imitation BaselineR+L1/L2.\nThe rightmost BaselineR+TM is our proposed method, which employs the TM loss.",
                "position": 560
            },
            {
                "img": "https://arxiv.org/html/2508.04676/x4.png",
                "caption": "Figure 4:Performance trend during continual learning 15 tasks of different methods. Y-axis of each figure indicates the specific task that has just been learned. Two rows depict full-parameter and LoRA settings, respectively. Three columns are metrics: current MMLU score, average performance over tasks learned so far, F1 average.",
                "position": 1141
            },
            {
                "img": "https://arxiv.org/html/2508.04676/x5.png",
                "caption": "Figure 5:MMLU, AP and F1 Avg performance of three major representative methods across different learning rate is compared under full-parameter and LoRA settings, with the LR axis displayed on a logarithmic scale.",
                "position": 1144
            },
            {
                "img": "https://arxiv.org/html/2508.04676/x6.png",
                "caption": "Figure 6:Landscapes of (a) replay samples loss, and (b) MMLU score underfull-parameter setting.\nOrigin point (0,0) is base untuned model.\nY-axis is weight update direction of Baseline (0,1), representing the learning dedicated to downstream tasks.\nX-axis is weight update direction of target method for comparison (1,0).\nThe upper-right area of interest simulates the target model guided by the learning direction of downstream tasks (yellow arrow), where the flatness (see zoomed-in view) can imply the optimizing robustness against latent forgetting even under potential overtraining in practice.",
                "position": 1147
            },
            {
                "img": "https://arxiv.org/html/2508.04676/x7.png",
                "caption": "Figure 7:Landscapes of (a) replay samples loss, and (b) MMLU score underLoRA setting.\nOrigin point (0,0) is base untuned model.\nY-axis is weight update direction of Baseline (0,1), representing the learning dedicated to downstream tasks.\nX-axis is weight update direction of target method for comparison (1,0).\nThe upper-right area of interest simulates the target model guided by the learning direction of downstream tasks (yellow arrow), where the flatness (see zoomed-in view) can imply the optimizing robustness against latent forgetting even under potential overtraining in practice.",
                "position": 1155
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "A",
        "images": []
    }
]