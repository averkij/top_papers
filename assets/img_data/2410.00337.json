[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00337/x1.png",
                "caption": "Figure 1:A showcase of application ofSytheOcc. We enable geometric-controlled generation that conveys the user editing in 3D voxel space to generate realistic street view images. In this case, we create a rare scene that traffic cones block the way. This advancement facilitates the evaluation of autonomous systems, such as the end-to-end planner VAD[9], in simulated corner case scenes.",
                "position": 122
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00337/x2.png",
                "caption": "Figure 2:The overall architecture ofSytheOcc. We achieve 3D geometric control in image generation by utilizing our proposed 3D semantic multiplane images to encode scene occupancy. In our framework, we can edit the occupied state and semantics of every voxel in 3D space to control the image generation, thereby opening up a wide spectrum of applications as shown in the top right.",
                "position": 161
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x3.png",
                "caption": "Figure 3:Visualizations of geometric controlled generation.Top row: Fusion of 3D semantic MPI.Bottom row: our generation concatenated from neighboring views.",
                "position": 223
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x4.png",
                "caption": "Figure 4:Visualizations of the reweighing function in Eq.6.",
                "position": 256
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x5.png",
                "caption": "Figure 5:Visualizations of generated multi-view images. The generation conditions (occupancy labels) are from nuScenes validation set. We highlight that(i)Geometry alignment of trees in red rectangle in (b).(ii)Use text prompt to control high-level appearance in (c,d).",
                "position": 289
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00337/x6.png",
                "caption": "Figure 6:Top row: Comparison with ControlNet. We achieve a precise alignment between conditional labels and synthesized images, while ControlNet generates objects with incorrect pose due to ambiguous 2D condition.Mid and Bottom row: Visualizations of geometry-controlled image generation. We can faithfully generate objects with the desired topology in a specific 3D position.",
                "position": 589
            }
        ]
    },
    {
        "header": "5Limitation and Broader Impacts",
        "images": []
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix AStatement of Geometric Control",
        "images": []
    },
    {
        "header": "Appendix BLong-Tailed Scene Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00337/x7.jpg",
                "caption": "Figure 7:From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth.",
                "position": 1404
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x8.png",
                "caption": "Figure 8:UseSytheOccto create long-tailed scenes for testing.Top: In the ordinary scene of a bus placed in front of the ego vehicle, the end-to-end planner VAD[9]predicts future waypoints without movement, thus not plotted in the image.Bottom: By harnessing the prompt-level control in our framework, we simulate a scene with the same layout but filled with fog. VAD predicts wrong waypoints that will collide with the bus.",
                "position": 1407
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x9.png",
                "caption": "Figure 9:Comparison with baselines.",
                "position": 1410
            }
        ]
    },
    {
        "header": "Appendix CAblation of plane number of MPIs",
        "images": []
    },
    {
        "header": "Appendix DQualitative Comparison with Baselines and SOTA",
        "images": []
    },
    {
        "header": "Appendix EExtend to Video Generation",
        "images": []
    },
    {
        "header": "Appendix FGeneralize to New Cameras",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00337/x10.png",
                "caption": "Figure 10:We demonstrate the generalizability of SytheOcc to new camera intrinsic. We multiply factors to the focal length while keeping the resolution the same. In (b,c), focal length×0.8absent0.8\\times 0.8× 0.8denotes a camera with a larger field of view similar to zoom out, focal length×1.2absent1.2\\times 1.2× 1.2denotes a camera with a smaller field of view similar to zoom in.",
                "position": 1485
            }
        ]
    },
    {
        "header": "Appendix GThe Influence of the Amount of Augmented Data",
        "images": [
            {
                "img": "https://arxiv.org/html/2410.00337/x11.png",
                "caption": "Figure 11:Video generation results. In the temporal progression, the distant buildings maintain a high degree of consistency, and objects retain their identical shapes and textures across different views and frames.",
                "position": 1515
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x12.jpg",
                "caption": "Figure 12:Video generation results of large dynamics scenes. The white car comes across different views and frames depicting consistent shapes with only a slight appearance change.",
                "position": 1518
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x13.jpg",
                "caption": "Figure 13:From top to bottom, we display images of fusion of 3D semantic MPI, synthesized images of sandstorm, snow, foggy, rainy, day night, day time, and ground truth.",
                "position": 1521
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x14.jpg",
                "caption": "Figure 14:Weather variation. Same structure with Fig.13.",
                "position": 1524
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x15.jpg",
                "caption": "Figure 15:Out of distribution generation. We use prompts to control the high-level appearance of images with specific styles. From top to bottom, we display (1) fusion of 3D semantic MPI. (2) Sunny day. (3) Science fiction style. (4) 8-bit pixel art style. (5) Snowfall. (6) Minecraft style. (7) Pokémon style. (8) Diablo style. (9) Ghibli style. (10) Metropolis style. (11) Gotham style. (12) Ground truth.",
                "position": 1527
            },
            {
                "img": "https://arxiv.org/html/2410.00337/x16.jpg",
                "caption": "Figure 16:Failure case of video generation results. Our cross-frame attention module is challenging to distinguish a crowd of people across different views and frames.",
                "position": 1538
            }
        ]
    },
    {
        "header": "Appendix HFailure Cases",
        "images": []
    }
]