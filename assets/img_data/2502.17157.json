[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17157/x1.png",
                "caption": "Figure 1:With one single model,DiCeptionsolves\nmultiple tasks\nwithout relying on any task-specific modules (rows 1 to 3). The red dots in the figure indicate the input points used for point-prompted segmentation.DiCeptionpreserves fine details in segmentation, such as hair (row 4).DiCeptionsupports both human pose estimation and semantic segmentation (row 5, 6).DiCeptioncan quickly adapt to new tasks by fine-tuning less than 1% of its parameters on as few as 50 images (row 7). For additional visualizations, please refer to FiguresS1,S2,S4,S6,S7,S8,S9,S10,S11,S12,S13,S14in the Appendix.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17157/x2.png",
                "caption": "Figure 2:Comparisons of mIoU with SAM-vit-h.We achieve results on par with SAM using only 0.06% of their data (600K vs.Â  1B).",
                "position": 1224
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x3.png",
                "caption": "Figure 3:Comparisons between 1-point and 5-point segmentation of mIoU on all 23 validation datasets.",
                "position": 1270
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "Appendix ADataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17157/x4.png",
                "caption": "Figure S1:Additional visualizations. Our one single model tackles multiple perception tasks.",
                "position": 3058
            }
        ]
    },
    {
        "header": "Appendix BPost-processing",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17157/x5.png",
                "caption": "Figure S2:Segmentation results on furry objects.",
                "position": 3191
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x6.png",
                "caption": "Figure S3:When post-processing RGB masks, small regions and excessive numbers of objects lead to significant metric degradation.",
                "position": 3371
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x7.png",
                "caption": "Figure S4:Additional few-shot fine-tuning results on image highlighting.",
                "position": 3431
            }
        ]
    },
    {
        "header": "Appendix CAdditional Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2502.17157/x8.png",
                "caption": "Figure S5:Our segmentation not only separates semantically identical objects but also distinguishes different instances of the same category, achieving higher segmentation quality. Moreover, One Diffusion tends to generate an image similar to the input when performing image understanding tasks, as red-highlighted in the figure.",
                "position": 3447
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x9.png",
                "caption": "Figure S6:Additional few-shot fine-tuning results on lung segmentation and tumor segmentation.",
                "position": 3452
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x10.png",
                "caption": "Figure S7:Additional depth estimation visualizations.",
                "position": 3475
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x11.png",
                "caption": "Figure S8:Additional normal visualizations.",
                "position": 3481
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x12.png",
                "caption": "Figure S9:Additional entity segmentation visualizations.",
                "position": 3487
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x13.png",
                "caption": "Figure S10:Additional point-prompted segmentation visualizations.",
                "position": 3493
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x14.png",
                "caption": "Figure S11:Comparison of the segmentation results betweenDiCeptionand SAM-vit-h with 1-point input.",
                "position": 3499
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x15.png",
                "caption": "Figure S12:Comparison of the segmentation results betweenDiCeptionand SAM-vit-h with 5-point input.",
                "position": 3505
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x16.png",
                "caption": "Figure S13:Additional pose estimation visualizations.",
                "position": 3511
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x17.png",
                "caption": "Figure S14:Additional semantic segmentation visualizations.",
                "position": 3517
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x18.png",
                "caption": "Figure S15:The model tends to produce more failure cases in 1-step scenario.",
                "position": 3522
            },
            {
                "img": "https://arxiv.org/html/2502.17157/x19.png",
                "caption": "Figure S16:A\nUNet-based model fails to perform multi-task.",
                "position": 3527
            }
        ]
    },
    {
        "header": "Appendix DLimitations",
        "images": []
    }
]