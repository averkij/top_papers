[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24160/images/six.png",
                "caption": "Figure 2:Illustrative overview of IMDD-1M showing diverse image-text pairs across multiple industrial domains, each with expert-verified annotations capturing fine-grained defect types, materials, and manufacturing contexts. The dataset serves as a large-scale foundation for vision-language modeling in industrial inspection.",
                "position": 199
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Large-Scale Industrial Defect Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24160/images/threeinone.jpg",
                "caption": "Figure 3:Dataset analysis. (1) Sample distribution among the top 100 defect categories (log-scaled). (2) Three-step workflow for dataset construction.",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/figg.png",
                "caption": "Figure 4:Dataset composition. (1) Distribution of normal versus anomaly samples. (2) Pie chart showing dataset composition across domains.",
                "position": 335
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/figgg.png",
                "caption": "Figure 5:Anomaly ratio distribution across datasets.\nEach bar represents the normalized proportion of anomaly and normal samples within a specific dataset, illustrating data imbalance and diversity across industrial domains.",
                "position": 568
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/095.png",
                "caption": "Figure 6:Top 10 datasets ranked by (1) object class count and (2) defect type count.\nMVTec AD and VisA stand out for their broad coverage and diversity across industrial components and surface conditions.",
                "position": 574
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/overview.jpg",
                "caption": "Figure 7:Overview of our method. An implicit captioner encodes the defect image into a text embedding, which, together with the image, is fed into a frozen diffusion U-Net to extract multi-scale features. A VAE decoder reconstructs features, while a mask generator predicts binary masks and embeddings. Classification is performed via dot products between mask and text embeddings (orange/green boxes) under cross-entropy and grounding supervision.",
                "position": 580
            }
        ]
    },
    {
        "header": "4Method",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.24160/images/FID.png",
                "caption": "Figure 8:Comparison of generative quality between our IMDD-1M–trained model and Stable Diffusion XL (SDXL) on the Magnetic Tile dataset.\n(1) IS: class consistency and diversity.\n(2) FID: realism gap to real images.\nOur model attains higher IS and lower FID.",
                "position": 838
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/GEN.png",
                "caption": "Figure 9:Qualitative comparison of real (left) vs. generated (right) defect samples across multiple industrial datasets including Magnetic Tile, VisA, wall stain, and aircraft surface panel. Generated images exhibit high fidelity in texture reproduction.",
                "position": 846
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/PPP.png",
                "caption": "Figure 10:Qualitative visualization of multimodal results on various MVTec AD samples.\n(a)–(d) show segmentation outputs with text-conditioned masks highlighting localized defects,\nwhile (e)–(h) illustrate object detection results with bounding boxes accurately identifying defect regions across different material domains.",
                "position": 849
            },
            {
                "img": "https://arxiv.org/html/2512.24160/images/data_efficiency_.png",
                "caption": "Figure 11:Our method achieves 96.1% accuracy using only 200 samples per class, requiring less than 5% of the training data compared to conventional approaches (approximately 4,000 samples per class for comparable performance).\nPerformance rapidly improves up to 200 samples and then plateaus, demonstrating effective learning under limited supervision.",
                "position": 1106
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "Appendix",
        "images": []
    },
    {
        "header": "7Reproducibility and Code Release",
        "images": []
    },
    {
        "header": "8Limitations",
        "images": []
    },
    {
        "header": "9Societal Impact",
        "images": []
    },
    {
        "header": "10Preliminaries",
        "images": []
    },
    {
        "header": "11Implementation Details",
        "images": []
    },
    {
        "header": "12Additional Experiments",
        "images": []
    }
]