[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x1.png",
                "caption": "((a))",
                "position": 150
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x1.png",
                "caption": "((a))",
                "position": 153
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x2.png",
                "caption": "((b))",
                "position": 159
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x3.png",
                "caption": "Figure 2:The overview of our Adaptive Data Refinement Framework (ADR).\n(a) In the Analyzing Stage, we first extract tokens, objects, co-occurrences, and interrogations from the training instances, then construct corresponding distribution using a reverse-indexed mapping.\n(b) In the Data Rebalancing stage, we analyze the optimizing direction and adaptively rebalance the redundant data based on the entity distribution identified in the Analyzing stage.\n(c) Finally, in the Data Synthesis stage, we utilize DDPM and the latent representations of scarce image instances to synthesize the underrepresented data.",
                "position": 168
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x4.png",
                "caption": "((a))",
                "position": 222
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x4.png",
                "caption": "((a))",
                "position": 225
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x5.png",
                "caption": "((b))",
                "position": 230
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x6.png",
                "caption": "((c))",
                "position": 236
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x7.png",
                "caption": "((d))",
                "position": 241
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x8.png",
                "caption": "((a))",
                "position": 340
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x8.png",
                "caption": "((a))",
                "position": 343
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x9.png",
                "caption": "((b))",
                "position": 348
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x10.png",
                "caption": "((c))",
                "position": 354
            }
        ]
    },
    {
        "header": "3Analysis",
        "images": []
    },
    {
        "header": "4Approach",
        "images": []
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x11.png",
                "caption": "Figure 5:Ablation study on data rebalancing combinations. T, O, C, and W refer to Token, Object, Co-occurrence, and Interrogation respectively. The values displayed in the graph represent average scores across a variety of comprehensive benchmarks. The blue dashed line indicates the baseline performance of LLaVA 1.5.",
                "position": 817
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x12.png",
                "caption": "Figure 6:Ablation study on data rebalancing synthesis methods. The meaning of abbrs occurred here is explained in Section6.2. The values displayed in the graph represent average scores across a variety of comprehensive benchmarks. The blue dashed line indicates the baseline performance of LLaVA 1.5.",
                "position": 889
            }
        ]
    },
    {
        "header": "6Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x13.png",
                "caption": "Figure 7:Qualitative comparison between the baseline model (LLaVA 1.5) and our proposed method (LLaVA w/ ADR) on a tail example. LLaVA w/ ADR can handle tail questions while LLaVA 1.5 fails to answer. While LLaVA 1.5 fails to answer tail questions, LLaVA w/ ADR successfully addresses them.",
                "position": 964
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x14.png",
                "caption": "Figure 8:Comparison between the original instruction-tuning (IT) data and our synthesized IT data. Tail concepts in the original data are highlighted usingredboxes and fonts, whereas synthesized tail concepts are marked withgreenboxes andyellowfonts.",
                "position": 969
            }
        ]
    },
    {
        "header": "7Qualitive Results",
        "images": []
    },
    {
        "header": "8Conclusion",
        "images": []
    },
    {
        "header": "9Acknowledgement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ADetails of Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x15.png",
                "caption": "((a))A bear resting peacefully beside a rock wall.",
                "position": 2059
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x15.png",
                "caption": "((a))A bear resting peacefully beside a rock wall.",
                "position": 2062
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x16.png",
                "caption": "((b))A cell phone displaying a cartoon princess on its screen.",
                "position": 2068
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x17.png",
                "caption": "((c))A dump truck.",
                "position": 2074
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x18.png",
                "caption": "((a))A train traveling along a railway near a church.",
                "position": 2083
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x18.png",
                "caption": "((a))A train traveling along a railway near a church.",
                "position": 2086
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x19.png",
                "caption": "((b))A bench by the lake, with a forest on the opposite shore.",
                "position": 2092
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x20.png",
                "caption": "((c))A furniture arrangement complemented by a variety of planters.",
                "position": 2098
            }
        ]
    },
    {
        "header": "Appendix BDetails of Analyzing Stage",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x21.png",
                "caption": "Figure 11:Top 20 most frequent entities in the instruction-tuning dataset of LLaVA 1.5.",
                "position": 2708
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x22.png",
                "caption": "((a))MME: Tok",
                "position": 2721
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x22.png",
                "caption": "((a))MME: Tok",
                "position": 2724
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x23.png",
                "caption": "((b))LCS558K: Tok",
                "position": 2729
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x24.png",
                "caption": "((c))InstructMix665K: Tok",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x25.png",
                "caption": "((d))MME: Obj",
                "position": 2740
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x26.png",
                "caption": "((e))LCS558K: Obj",
                "position": 2745
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x27.png",
                "caption": "((f))InstructMix665K: Obj",
                "position": 2750
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x28.png",
                "caption": "((g))InstructMix665K: Co-occurrence",
                "position": 2756
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x29.png",
                "caption": "((h))InstructMix665K: Interrogation",
                "position": 2761
            }
        ]
    },
    {
        "header": "Appendix CDetails of our ADR Approach",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.12821/x30.png",
                "caption": "Figure 13:Complete prompts used to guide the language model in extracting object information.",
                "position": 2993
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x31.png",
                "caption": "Figure 14:Complete prompts used to guide the language model in converting captions into conversation instructions.",
                "position": 3003
            },
            {
                "img": "https://arxiv.org/html/2503.12821/x32.png",
                "caption": "Figure 15:Complete prompts used to guide the language model in rewrite conversation instructions using given tokens.",
                "position": 3010
            }
        ]
    },
    {
        "header": "Appendix DPrompts",
        "images": []
    }
]