[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/logo/link.png",
                "caption": "",
                "position": 98
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/x1.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/25_graph.png",
                "caption": "(a)",
                "position": 112
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/25_graph.png",
                "caption": "(a)",
                "position": 115
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/overview_double.png",
                "caption": "(b)",
                "position": 120
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/concept.png",
                "caption": "Figure 2:Non-parametric updates can encourage exploration, bootstrapping parametric updates.",
                "position": 137
            }
        ]
    },
    {
        "header": "2Preliminaries",
        "images": []
    },
    {
        "header": "3The Exploration Problem of LLM Agents",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/exploration_problem.png",
                "caption": "Figure 3:When training LLM with GRPO in ScienceWorld, the agent struggles because of insufficient exploration.For instance, in the task “turn on the red light bulb,” the agent must first find the red light bulb before activating it. However, the agent fails to locate it and, as a result, cannot complete the task. Rather than analyzing the cause of failure and exploring alternative actions, the agent proceeds unchanged, so its score stagnates even as additional training steps are taken.",
                "position": 193
            }
        ]
    },
    {
        "header": "4Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/motivation3.png",
                "caption": "Figure 4:In EMPO2, the current policy parametersπθ\\pi_{\\theta}are used to review past rollouts, with the resulting insights added to memory. This updated memory conditions subsequent rollouts and promotes exploration.",
                "position": 214
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/EMPO.png",
                "caption": "Figure 5:EMPO2mode combinations.By combining the two rollout modes and update modes, three EMPO mode configurations are possible: on-policy learning without memory, on-policy learning with memory and off-policy learning.",
                "position": 242
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/empo_design/low_filtering.png",
                "caption": "Figure 6:Masking tokens stabilizes training.",
                "position": 289
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/empo_design/entropy.png",
                "caption": "Figure 7:Policy entropy comparison with vs. without intrinsic rewards.",
                "position": 313
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ood/13.png",
                "caption": "Table 1:Comparison results of ScienceWorld.Each task in ScienceWorld contains multiple variants. We use the first five variants for training and evaluate on the 20 unseen test variants. Bold shows the best performance per task, while red shading marks cases where parametric updates score lower than non-parametric updates. The EMPO2performance we evaluate is the performance of the trained model without memory at test time.",
                "position": 362
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ood/13.png",
                "caption": "Figure 8:Comparison of GRPO and EMPO2adapting to new tasks. Step 0 has no memory, while later steps use accumulated memory as in EMPO2training.",
                "position": 742
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ood/17.png",
                "caption": "",
                "position": 745
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ood/25.png",
                "caption": "",
                "position": 746
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ablation/3_ablation.png",
                "caption": "Figure 9:Comparison of training curves between EMPO2and variants that exclude either off-policy learning or on-policy learning with memory.",
                "position": 819
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ablation/25_ablation.png",
                "caption": "",
                "position": 822
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "Ethics statement",
        "images": []
    },
    {
        "header": "Reproducibility statement",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix APseudo Code",
        "images": []
    },
    {
        "header": "Appendix BPrompts",
        "images": []
    },
    {
        "header": "Appendix CDetailed Explanation of Importance Sampling Ratios in Policy Updates",
        "images": []
    },
    {
        "header": "Appendix DExperiments Details",
        "images": []
    },
    {
        "header": "Appendix Equalitative analysis on tips",
        "images": []
    },
    {
        "header": "Appendix FMore Ablation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ablation/p_ablation.png",
                "caption": "(a)",
                "position": 1999
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ablation/p_ablation.png",
                "caption": "(a)",
                "position": 2002
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ablation/q_ablation.png",
                "caption": "(b)",
                "position": 2007
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/ablation/intrinsic_ablation.png",
                "caption": "Figure 11:EMPO2learning curves with different intrinsic reward configurations on ScienceWorldchemistry-mix-paint-secondary-colortask. We compare our full method against four variants: scaling the intrinsic reward coefficient by 0.5× and 2×, substituting it with a Random Network Distillation (RND) bonus, and its complete removal (w/o Intrinsic Reward).",
                "position": 2034
            }
        ]
    },
    {
        "header": "Appendix GAnalysis of Computational Cost",
        "images": [
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/time.png",
                "caption": "Figure 12:A breakdown of the time each component spends during the rollout of each training step.",
                "position": 2054
            },
            {
                "img": "https://arxiv.org/html/2602.23008/2602.23008v1/figure/score_time_graph.png",
                "caption": "Figure 13:Time–performance curves for EMPO2and GRPO on ScienceWorldpower-componenttask.",
                "position": 2068
            }
        ]
    },
    {
        "header": "Appendix HThe Use of Large Language Models",
        "images": []
    }
]