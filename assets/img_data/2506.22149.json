[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Methods",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22149/x1.png",
                "caption": "Figure 1:RetFiner method. Squares represent patch features and circles represent global features (CLS tokens). Cross-attention layers are activated only during the forward passes for ITM, MLM, and GM. An example of an OCT image and report is shown.",
                "position": 147
            }
        ]
    },
    {
        "header": "3Experiments and Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.22149/x2.png",
                "caption": "Figure 2:Examples of RetFiner-R and RetFiner-U attention maps for disease cases.",
                "position": 887
            }
        ]
    },
    {
        "header": "4Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]