[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21218/images/livr_teaser.jpg",
                "caption": "Figure 1:The model is asked to determine which image option is most similar to the reference image. Standard LMMs can only output text, which cannot capture all visual information and may introduce ambiguity. While methods using explicit supervision can train models to output intermediate reasoning steps, these approaches may fail when the reasoning steps themselves are unclear. Our approach allows the model to learn useful representations implicitly. Visualizing the attention maps of the latent tokens shows that the model has learned to recognize underlying visual structures relevant to answering the question that would have been hard for humans to design supervision for.",
                "position": 149
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21218/images/livr_method_combined2.jpg",
                "caption": "Figure 2:An illustration of our method and bottleneck attention masking. Latent tokens are appended to the prompt and losses are computed on the answer tokens. In our bottleneck attention masking, answers and prompt tokens cannot attend to image tokens.",
                "position": 173
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21218/images/latent_viz.jpg",
                "caption": "Figure 3:An illustration of latent-to-image attention maps for different tasks. The left columns show the input images, and the right columns show the attention overlays. In the Semantic Correspondence task, the model identifies the option in the second image that aligns with the REF point in the first image. In the Localization task, it selects bounding boxes that best localize the motorcycle and the dog, and in the Counting task, it counts the cows and balloons. We observe that latent-to-image attention concentrates on regions corresponding to the correct answers or the visual evidence needed to resolve each task. Although some attention sinks persist, the dominant patterns align with task-relevant regions, indicating that the latents capture meaningful visual structure without explicit supervision.",
                "position": 593
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/T-SNE_new.jpg",
                "caption": "Figure 4:t-SNE Visualization of Different Tokens",
                "position": 855
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Limitations and Future Work",
        "images": []
    },
    {
        "header": "Appendix ADatasets",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21218/images/counting.png",
                "caption": "",
                "position": 952
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/jigsaw.png",
                "caption": "",
                "position": 993
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/localization.png",
                "caption": "",
                "position": 1015
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/vis_corr_1.png",
                "caption": "",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/vis_corr_2.png",
                "caption": "",
                "position": 1063
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/art_style.png",
                "caption": "",
                "position": 1085
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/sem_corr_1.png",
                "caption": "",
                "position": 1107
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/sem_corr_2.png",
                "caption": "",
                "position": 1107
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/func_corr_1.png",
                "caption": "",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/func_corr_2.png",
                "caption": "",
                "position": 1132
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/reflectance.png",
                "caption": "",
                "position": 1162
            },
            {
                "img": "https://arxiv.org/html/2512.21218/images/vis_sim.png",
                "caption": "",
                "position": 1184
            }
        ]
    },
    {
        "header": "Appendix BTraining Setup",
        "images": [
            {
                "img": "https://arxiv.org/html/2512.21218/images/supp-viz5.jpg",
                "caption": "Figure 5:Additional visualizations of latent token to image attention.",
                "position": 1226
            }
        ]
    },
    {
        "header": "Appendix CAdditional Visualizations",
        "images": []
    }
]