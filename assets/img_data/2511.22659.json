[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22659/x1.png",
                "caption": "Figure 1:Overview.(a) Semantic-Geometric Gap.The geometric details required for spatial reasoning are lost when translating visual information into textual space, leading to VLM‚Äôs flawed reasoning or unconstrained planning.(b) Geometrically-Constrained Spatial Reasoning.We propose a formal task constraint that serves as a deterministic bridge between semantics and geometry in spatial reasoning.",
                "position": 141
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22659/x2.png",
                "caption": "Figure 2:Overall Paradigm of GCA.Given a spatial reasoning query, our GCA leverages a geometrically-constrained reasoning strategy centered on the formal task constraint (ùíûtask{\\mathcal{C}}_{\\text{task}}).\nThe VLM first translates the ambiguous query into this explicitùíûtask{\\mathcal{C}}_{\\text{task}}, establishing a non-negotiable reference frame (ùíû‚Ñõ{\\mathcal{C}}_{\\mathcal{R}}) and objective (ùíûùí™{\\mathcal{C}}_{\\mathcal{O}}).\nStrictly constrained byùíûtask{\\mathcal{C}}_{\\text{task}}, the VLM then orchestrates a toolbox to perform deterministic geometric computation and derive the final answer.",
                "position": 194
            }
        ]
    },
    {
        "header": "3Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22659/x3.png",
                "caption": "Figure 3:Reference Frame.Here,vsink‚Üíowenv_{\\text{sink}\\rightarrow\\text{owen}}denotes a vector calculated by ‚Äúnormalize‚Äã(Centroid(owen)‚àíCentroid(sink))\\text{normalize}\\left(\\text{Centroid(owen)}-\\text{Centroid(sink)}\\right)‚Äù.",
                "position": 271
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22659/x4.png",
                "caption": "Figure 4:Ablation Study on Formalization.We compare our method in against several baselines:\n(1) no tool integration (‚ÄúBaseline (CoT-Only)‚Äù),\n(2) unconstrained tool integration with (‚ÄúTool (Prompt)‚Äù) or without (‚ÄúTool (Uncon.)‚Äù) hints,\n(3) using a human-annotatedùíûtask{\\mathcal{C}}_{\\text{task}}(‚ÄúOracle (Anno.)‚Äù).",
                "position": 876
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x5.png",
                "caption": "Figure 5:Ablation Study on Generalizability across Different VLMs.Our GCA achieves an average of 37% relative performance improvement across all tested foundation VLMs.",
                "position": 900
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x6.png",
                "caption": "Figure 6:Error Attribution and Failure Cases.We provide a detailed error attribution analysis to identify the main failure modes within the VLM‚Äôs reasoning trajectory.",
                "position": 920
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ASpatial Task Constraint",
        "images": []
    },
    {
        "header": "Appendix BMore Ablation Studies",
        "images": []
    },
    {
        "header": "Appendix CMore Implementation Details",
        "images": []
    },
    {
        "header": "Appendix DEvaluation Benchmark Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22659/x7.png",
                "caption": "Figure 7:In Context Examples Used in Formalizing Reference Frame.The output format follows the prompt inTable4.",
                "position": 2408
            }
        ]
    },
    {
        "header": "Appendix EPrompts Used in GCA",
        "images": []
    },
    {
        "header": "Appendix FQualitative Case Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2511.22659/x8.png",
                "caption": "Figure 8:Case Study #1.Unique object counting across multiple views.",
                "position": 2713
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x9.png",
                "caption": "Figure 9:Case Study #2.Direction-based reference frame.",
                "position": 2720
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x10.png",
                "caption": "Figure 10:Case Study #3.Object-based reference frame",
                "position": 2727
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x11.png",
                "caption": "Figure 11:Case Study #4.Camera rotation analysis.",
                "position": 2734
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x12.png",
                "caption": "Figure 12:Case Study #5.Object movement analysis.",
                "position": 2741
            },
            {
                "img": "https://arxiv.org/html/2511.22659/x13.png",
                "caption": "Figure 13:Case Study #6.Metric-scale estimation.",
                "position": 2748
            }
        ]
    },
    {
        "header": "Appendix GBroader Impacts",
        "images": []
    }
]