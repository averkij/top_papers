[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/figures/studio_teaser2.png",
                "caption": "Figure 1:High quality 3D assets generated by Hunyuan3D Studio.",
                "position": 128
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Hunyuan3D Studio Pipeline",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/x1.png",
                "caption": "Figure 2:The pipeline of Hunyuan3D Studio.",
                "position": 153
            }
        ]
    },
    {
        "header": "3Controllable Image Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/x2.png",
                "caption": "Figure 3:Visualization results of our image stylization module with pre-defined styles.",
                "position": 218
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/image_figures/pipeline_pose.png",
                "caption": "Figure 4:Overall workflow of our pose standardization module.",
                "position": 231
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/image_figures/vis_pose.png",
                "caption": "Figure 5:Visualization results of our pose standardization module.",
                "position": 234
            }
        ]
    },
    {
        "header": "4High-Fidelity Geometry Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/figures/geo_figures/cond-gen-pipeline.png",
                "caption": "(a)3D shape generation pipeline",
                "position": 285
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/geo_figures/cond-gen-pipeline.png",
                "caption": "(a)3D shape generation pipeline",
                "position": 288
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/geo_figures/multiview-pipeline.png",
                "caption": "(b)Image-to-multiview pipeline",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/geo_figures/bbox_vis.png",
                "caption": "Figure 7:3D geometry generated with bounding box control.",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/geo_figures/character_vis.png",
                "caption": "Figure 8:3D geometry generated with generated multi-view image control.",
                "position": 343
            }
        ]
    },
    {
        "header": "5Part-level 3D Generation",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/figures/part/xpart/x-part-v4.jpg",
                "caption": "Figure 9:Our part level shape generation results.",
                "position": 351
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/part/whole_pipe.jpg",
                "caption": "Figure 10:Pipeline of our image to 3D part generation. Given an input image, we first obtain the holistic shape using Huyuan3D 2.5Lai et al. (2025a).\nThe holistic mesh is then fed to part detection moduleP3-SAMMa et al. (2025)to obtain the semantic features and part bounding boxes.",
                "position": 365
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/part/p3sam/method.png",
                "caption": "Figure 11:Training pipeline ofP3-SAM.",
                "position": 369
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/part/p3sam/auto_seg.png",
                "caption": "Figure 12:Pipeline of automatic segmentation usingP3-SAM.",
                "position": 408
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/part/xpart/pipeline_v3.jpg",
                "caption": "Figure 13:Pipeline of our shape decomposition.",
                "position": 610
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/part/xpart/compare_parts.jpg",
                "caption": "Figure 14:Shape Decomposition Results.",
                "position": 669
            }
        ]
    },
    {
        "header": "6Polygon Generation with Auto-regressive Models",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/x3.png",
                "caption": "Figure 15:Mesh-RFT Framework Overview.The pipeline comprises two stages:1) Mesh Generation Pre-trainingusing an Hourglass AutoRegressive Transformer and a Shape Encoder; and2) Reinforcement Post-trainingwhich employs Mask DPO with reference and policy networks for subsequent refinement.",
                "position": 682
            },
            {
                "img": "https://arxiv.org/html/2509.12815/x4.png",
                "caption": "Figure 16:The effectiveness of the post-training stage.The post-training stage enhances the mesh completeness (Row #1) and connectivity (Row #2) and reduces the broken faces (Row #3).",
                "position": 726
            },
            {
                "img": "https://arxiv.org/html/2509.12815/x5.png",
                "caption": "Figure 17:Generalization results on dense, out-of-distribution meshes.Our model demonstrates superior geometric fidelity and surface continuity, maintaining high-quality reconstruction even under complex and unseen input conditions.",
                "position": 808
            },
            {
                "img": "https://arxiv.org/html/2509.12815/x6.png",
                "caption": "Figure 18:Part-aware polygon generation.With shapes segmented into several parts as input, our model can generate the corresponding meshes conditioned on partial point clouds separately without further fine-tuning.",
                "position": 811
            }
        ]
    },
    {
        "header": "7Semantic UV",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/figures/uv_figs/pipeline.jpg",
                "caption": "Figure 19:SeamGPT architecture: Point cloud encoder extracts shape context; Causal transformer decoder generates axis-ordered seam coordinates.\nColor indicates the prediction order is of the seam segments (red to blue).",
                "position": 843
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/uv_figs/fam-compare.jpg",
                "caption": "Figure 20:Qualitative UV flatten results on FAM benchmark ( Nefertiti, Cow, and Fandisk).",
                "position": 1122
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/uv_figs/ablation-sample.jpg",
                "caption": "Figure 21:Ablation of point sampling strategy.",
                "position": 1242
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/uv_figs/ablation-ptrnet-meshcond.jpg",
                "caption": "Figure 22:Ablation study of encoder and decoder.",
                "position": 1250
            },
            {
                "img": "https://arxiv.org/html/2509.12815/figures/uv_figs/bunny-ablation.jpg",
                "caption": "Figure 23:Seam length control and diversity. We can control the cutting granularity by adjusting seam length. Diverse valid cutting seams can be generated.",
                "position": 1270
            }
        ]
    },
    {
        "header": "8Texture Generation and Editing",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/x7.png",
                "caption": "Figure 24:visualization results of multimodal texture editing: Perform text- and image-guided editing on textured meshes.",
                "position": 1301
            },
            {
                "img": "https://arxiv.org/html/2509.12815/x8.png",
                "caption": "Figure 25:Visualization of material generation framework.",
                "position": 1323
            }
        ]
    },
    {
        "header": "9Animation Module",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.12815/figures/animation_figures/animation_result.png",
                "caption": "Figure 26:Comparison with UniRigZhang et al. (2025). Left: results of rigging for a general character using our generated mesh. Right: results of skinning applied to the general character input.",
                "position": 1343
            }
        ]
    },
    {
        "header": "10Conclusion",
        "images": []
    },
    {
        "header": "11Contributors",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]