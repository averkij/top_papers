[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/Ambedkar_logo_3.png",
                "caption": "",
                "position": 118
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x1.png",
                "caption": "Figure 1:Unmasking Hidden Bias through Identity Inference:The figure shows howLLMs, when asked to rewrite news passages withmasked identity terms, often substitute them with specificreligious or caste groups. For instance, the English example replaces“MASKED”with“Muslim”, linking a community tonational security risks, while the Hindi passage exhibits similar stereotyping. Such substitutions reveal the model’s reliance ondemographic priors, making hidden biases explicit and dangerously amplifying harmful narratives about identity groups.",
                "position": 185
            }
        ]
    },
    {
        "header": "1WhyAMBEDKAR? Rethinking Bias Mitigation in LLMs",
        "images": []
    },
    {
        "header": "2AI Constitution of India Dataset",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x2.png",
                "caption": "Figure 2:Annotator agreement heatmap based on Krippendorff’s alpha: The heatmap presentspairwise agreement scoresamong eighthuman annotators (H1–H8)who evaluated translated outputs on a3-point Likert scale, ranging from 1 (poor translation) to 3 (accurate translation).Krippendorff’s alpha, suitable forordinal data, is used to quantifyinter-annotator reliability. Higher values indicate stronger agreement whilelight shades represent low agreement.",
                "position": 262
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x3.png",
                "caption": "Figure 3:Bias Meters across Religions and Castes:Our benchmark dataset has been meticulously curated to represent 6 major religions and 136 caste groups, providing a comprehensive resource for stress-testing language models in the Indian sociocultural context. The dataset includes diverse textual prompts collected from real world news sources to evaluate representational and inferential bias across protected identity groups. Thebias metersdisplayed below each image indicate theIdentity Inference Rate (IIR)of GPT-4o, a state-of-the-art frontier model, reflecting the model’s propensity to infer caste or religious identity from the masked prompt.",
                "position": 290
            }
        ]
    },
    {
        "header": "3Ambedkar: Fairness Aware Speculative Decoding",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x4.png",
                "caption": "Figure 4:Overview of theAMBEDKARframework: Thedraft language modelgeneratesspeculative hypotheses, which are subsequently evaluated by averifier modelunder bothoriginalandcounterfactual contexts.Candidate completionsare scored based ondistributional divergences, and thetokenexhibiting maximalconsistencyandcontextual stabilityis selected for generation.",
                "position": 380
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x5.png",
                "caption": "Figure 5:TheAmbedkarframework operates through a 5-step pipeline:Promptorsamples prompts to elicitidentity-linked biases;Speculativageneratesdiverse candidate completions;Contrariumintroducescounterfactual perturbationsto challenge biased outputs;Aequitasevaluatesrepresentational fairnessviadivergence-based metrics; andModeratusselectsfair and semantically consistent tokens. This iterative process enforcescontrolled, bias-mitigated generation, systematically shifting outputs away fromdominant identity associations.",
                "position": 383
            },
            {
                "img": "https://arxiv.org/html/2509.02133/CasteHeatmap.png",
                "caption": "Figure 6:Mitigation Performance of AMBEDKAR Across Different LLMs.This heatmap reportsbias scoresacrossdiverse LLMsfor somerepresentative caste groups. For each caste, we display bothbaselineandpost-AMBEDKARbias levels, with post-AMBEDKAR rows\nannotated withrelative reductions(↓%). Each cell encodes thepost intervention bias scoreon the first line, and theabsolute reduction from baselineon the second line.AMBEDKARconsistently reduces caste–context entanglement across diverse architectures\nMitigation is pronounced in some groups (e.g.,Patel,Ezhava), underscoring AMBEDKAR’s ability to\ncounteractstructuralandrepresentational inequities. Our evaluation against our benchmark dataset establishes\nAMBEDKAR as arobust,generalizable, andsocially-grounded fairness alignment method.",
                "position": 639
            },
            {
                "img": "https://arxiv.org/html/2509.02133/ReligionHeatmap.png",
                "caption": "Figure 7:AMBEDKAR’s Impact on Religious Bias in LLMs.Heatmap showsbias scoresfor12 open-source LLMsacross6 religions,\nwith post-AMBEDKAR rows annotated byrelative reductions(↓%).AMBEDKARconsistently lowers bias—especially forMuslims,Sikhs, andChristians—\nacross all setups",
                "position": 651
            }
        ]
    },
    {
        "header": "4Performance",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x6.png",
                "caption": "(a)Throughput",
                "position": 826
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x6.png",
                "caption": "(a)Throughput",
                "position": 829
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x7.png",
                "caption": "(b)Latency Overhead",
                "position": 834
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "6Discussion and Limitations",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Frequently Asked Questions (FAQs)",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    },
    {
        "header": "Appendix BBackground",
        "images": []
    },
    {
        "header": "Appendix CThe Ubiquity of Bias in LLMs",
        "images": []
    },
    {
        "header": "Appendix DStress Testing",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/biasmetersappendix.jpg",
                "caption": "Figure 9:Bias Meters across Religions and Castes:Our benchmark dataset has been meticulously curated to represent 6 major religions and 136 caste groups, providing a comprehensive resource for stress-testing language models in the Indian sociocultural context. The dataset includes diverse textual prompts collected from real world news sources to evaluate representational and inferential bias across protected identity groups. Thebias metersdisplayed below each image indicate theIdentity Inference Rate (IIR)of GPT-4o, a state-of-the-art frontier model, reflecting the model’s propensity to infer caste or religious identity from the masked prompt.",
                "position": 2639
            },
            {
                "img": "https://arxiv.org/html/2509.02133/Hinduradar.png",
                "caption": "Figure 10:Bias Scores Across Models for Hindu Castes and communities:This radar plot comparesbaseline models(e.g., GPT2, GPT2-Large, OpenHaathi7B, GPT-OSS 20B) with theirPost-Ambedkar counterparts. By contrast, thePost-Ambedkar interventions(e.g., GPT2-Large+Llama3.2B, GPT2+GPT2-Large, OpenHaathi+GPT-OSS)systematically reduce both the magnitude and variance of caste bias, resulting in a flatter, more equitable distribution across groups.DeepSeek V3displays pronounced caste skew, with inflated bias toward dominant castes such as Brahmin and Bania, while significantly underrepresenting marginalized groups like Valmiki and Chamar. Similarly,GPT-4o, despite its scale and sophistication, continues to showuneven distributions, favoring forward castes (e.g., Brahmin, Kayastha) relative to Dalit and lower-caste categories We additionally evaluateSUTRA-Light, an Indic model on Hindi, alow-resource language. While it registers comparatively high bias overall, this case underlines the persistent difficulty of bias mitigation in Indic and low-resource contexts wherestructural hierarchies are deeply encodedin the training data. Taken together, these findings demonstrate that theAmbedkar framework is a robust and scalable method for caste bias mitigation, effective across architectures, languages, and training paradigms.",
                "position": 2900
            },
            {
                "img": "https://arxiv.org/html/2509.02133/SikhCastesbias.png",
                "caption": "(a)Sikh Castes",
                "position": 2904
            },
            {
                "img": "https://arxiv.org/html/2509.02133/SikhCastesbias.png",
                "caption": "(a)Sikh Castes",
                "position": 2907
            },
            {
                "img": "https://arxiv.org/html/2509.02133/MuslimCastebias.png",
                "caption": "(b)Muslim Castes",
                "position": 2912
            },
            {
                "img": "https://arxiv.org/html/2509.02133/Jaincastesbias.png",
                "caption": "(c)Jain Castes",
                "position": 2918
            },
            {
                "img": "https://arxiv.org/html/2509.02133/Buddhistcastes.png",
                "caption": "(d)Buddhist Castes",
                "position": 2923
            }
        ]
    },
    {
        "header": "Appendix EDesign of theAMBEDKARFramework",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x8.png",
                "caption": "Figure 12:Radar Chart illustrating AMBEDKAR’s contribution profile towards fairness across each modular components.The figure visualizes the relative contributions of each modular component—Promptor, Speculativa, Contrarium, Aequitas, Moderatus, and Aligning Verifier—towards mitigating identity-linked biases in generative models. Scores are normalized on a 0–5 scale, with higher values denoting stronger performance in balancing fairness with utility. The profile highlights that Aequitas and Aligning Verifier achieve the highest robustness, followed by Contrarium and Speculativa , while Promptor remains modest as its role is primarily sampling rather than corrective. This visualization underscores the complementary nature of the components, showing how they jointly contribute to a fairness-aware speculative decoding pipeline that balances alignment with generative performance.",
                "position": 3120
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x9.png",
                "caption": "Figure 13:Workflow comparison of fairness-aware (left) vs classical speculative decoding (right)In classical decoding, a draft model proposes tokens, which a larger verifier either accepts or rejects. Rejected ones trigger a fallback re-generation by the verifier model.\nFairness-aware decoding adds a fairness filter: the verifier compares token likelihoods under original and counterfactual contexts, committing the least diverging token.",
                "position": 3123
            }
        ]
    },
    {
        "header": "Appendix FComparison with Classical Speculative Decoding",
        "images": []
    },
    {
        "header": "Appendix GMathematical formulations",
        "images": []
    },
    {
        "header": "Appendix HAdditional Experimental Details",
        "images": []
    },
    {
        "header": "Appendix IQualitative Analysis ofAMBEDKAR",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x10.png",
                "caption": "Figure 15:Divergence Landscape Maps (2D Heatmap and 3D Surface).The plots depict Jensen–Shannon Divergence (JSD) between next-token probability distributions under original and counterfactual prompts, across contexts (y-axis) and candidate tokens (x-axis). Cooler valleys correspond to distributionally invariant completions, while elevated ridges denote high sensitivity to identity perturbations. The AMBEDKAR decoding trajectory (red) consistently selects tokens within low-divergence basins, operationalizing fairness-by-speculation, whereas baseline greedy decoding (blue dashed) traverses divergence ridges, exposing context-dependent demographic biases.",
                "position": 3644
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x11.png",
                "caption": "",
                "position": 3648
            }
        ]
    },
    {
        "header": "Appendix JComparative Study of Alignment Techniques",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x12.png",
                "caption": "Figure 17:Bias trajectories across 100 tokens for multiple prompts and decoding strategies. Each colored line represents the average bias score (frequency of identity mentions) across five example prompts for a given decoding algorithm. Standard decoding strategies—Greedy (blue), Beam Search (purple), Top-k Sampling (green), Top-p/Nucleus Sampling (brown), and Speculative (orange)—tend to show higher and more rapidly increasing bias scores as token generation progresses. In contrast, the AMBEDKAR method (red) maintains substantially lower bias scores across tokens while exhibiting realistic variability, demonstrating its effectiveness in mitigating identity-related bias. Individual lines depict the bias trajectory for each prompt, while the bold lines indicate the mean trajectory for each algorithm.",
                "position": 3921
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x13.png",
                "caption": "Figure 18:Bias–Fluency Tradeoff Across Decoding Strategies.This plot compares standard decoding methods (Greedy, Beam, Top-k, Speculative) with fairness-tuned AMBEDKAR decoding on two axes: perplexity (x-axis, lower = higher fluency) and bias score. Error bars show variance across runs. The red dashed Pareto frontier marks the optimal tradeoff boundary. Standard methods achieve lower perplexity but remain more biased, whereas AMBEDKAR modestly sacrifices fluency for substantial bias reduction, placing it nearer to the fairness–fluency frontier. The shaded zone indicates the desirable balance region.",
                "position": 3924
            }
        ]
    },
    {
        "header": "Appendix KComparision against existing Debiasing Approaches",
        "images": []
    },
    {
        "header": "Appendix LAblation Study",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.02133/x14.png",
                "caption": "Figure 19:Divergence metric sensitivity analysis across six religious groups.The bar chart illustrates the aggregate bias frequency for three divergence metrics: Fast Approximation, KL-divergence-based scoring, and Jensen–Shannon (JS) divergence-based scoring. The Fast Approximation method exhibits consistently higher bias counts, while JS divergence achieves the lowest across all groups, highlighting its superior discriminative capability and robustness in enforcing identity invariance in generations. The capped total frequency per group (200) ensures fair comparison across methods.",
                "position": 4007
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x15.png",
                "caption": "Figure 20:Heatmaps of Token Rank Shifts Across Caste Identity Tokens.This figure compares token rankings across caste identity tokens under models trainedwithandwithoutSupervised Fine-Tuning (SFT). Each row is a prompt with a masked identity (e.g., “MASK are good”), and each column is a caste token (e.g., Dalits, Brahmins, Yadavs). Color intensity denotes rank (lighter = higher). Without SFT, upper castes (Brahmins, Kayasthas, Kshatriyas) rank higher for positive prompts, while lower castes (Dalits, Yadavs, Guptas) rank higher for negative ones. SFT shifts these rankings: lower castes gain better ranks for positive prompts, and bias is partially mitigated for negative ones. Rankings are computed via JS-divergence from a verifier distribution.",
                "position": 4016
            },
            {
                "img": "https://arxiv.org/html/2509.02133/x16.png",
                "caption": "Figure 21:Per-prompt trajectories of Identity Inference Rate (IIR) across three decoding configurations.\nEach gray line represents a single evaluation prompt, connecting its IIR under baseline decoding without fairness control, fairness-aware decoding without contextual perturbations, and fairness-aware decoding with contextual perturbations.\nRed circles indicate the mean IIR for each configuration, with error bars denoting 95% confidence intervals.\nThe consistent downward slope from left to right shows that fairness-aware decoding reduces the model’s ability to infer masked identities from context, and that adding contextual perturbations yields further, systematic bias reduction across most prompts.",
                "position": 4029
            }
        ]
    },
    {
        "header": "Appendix MLimitations",
        "images": []
    },
    {
        "header": "Appendix NFuture Work",
        "images": []
    }
]