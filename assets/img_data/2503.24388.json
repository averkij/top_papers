[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24388/x1.png",
                "caption": "Figure 1:Comparison between conventional agents and RIG.RIG produces reasoning, actions, and imagination within a single Transformer.",
                "position": 98
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24388/x2.png",
                "caption": "Figure 2:Illustration of the data collection pipeline (S0–S4).Note that at S3 (Vision-Reviewing), we run the trained RIG-basicand policy model (STEVE-1[20]) in parallel, keeping instances where RIG-basicperforms poorly compared to STEVE-1.",
                "position": 158
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24388/x3.png",
                "caption": "Figure 3:Inference process in RIG.RIG follows a structuredconversation flowthrough multi-turn interactions. It consistently uses the fixed wordImagine:to clearly separate internally imagined scenarios from real observations, thereby guiding coherent reasoning, action prediction, and visual imagination.",
                "position": 227
            },
            {
                "img": "https://arxiv.org/html/2503.24388/x4.png",
                "caption": "Figure 4:Performance and data-efficiency comparison.RIG-basicsignificantly outperforms other baselines with higher sample efficiency and achieves superior performance using only111 hoursof training data (42h S0 MineRL-V0 and 69h S1-S4). MineDreamer[48], a hybrid-system model, separately trains a visual generation model (139 hours) but also relies on VPT for the policy model, increasing total data requirements. Duration of VPT[2]reflects only the IDM data used, measured as video frames, while STEVE-1[20]and Jarvis-1[35]also leverage the VPT dataset.",
                "position": 321
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24388/extracted/6315860/figs/exp_main.png",
                "caption": "Figure 5:Comparison with various baselines across embodied tasks, generation, understanding, and reasoning.RIG-basicincorporates reasoning without reviewing, while RIG-lookaheadintegrates both reasoning and reviewing capabilities.",
                "position": 339
            },
            {
                "img": "https://arxiv.org/html/2503.24388/x5.png",
                "caption": "Figure 6:Scalability Evaluation Across Training, Iteration, and Inference.We evaluate the scalability of RIG by testing its performance across three different aspects:training scalability,iteration scalability, andinference scalability.\nEach column corresponds to a different scalability setting. The top row presents the number of collected samples in material-gathering tasks, while the bottom row reports the success rate in exploration-based tasks.\nShaded regions represent variance. We exclude 42h MineRL-V0 pretraining from the total 111h in Figure. The training ratio is only counted before the lookahead reasoning.",
                "position": 343
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "Appendix AAppendix",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.24388/x6.png",
                "caption": "Figure A1:Detailed inference pipeline.RIG generates imagined visual states and corresponding reasoning to simulate multiple action trajectories, enabling self-review and corrective prediction.",
                "position": 922
            },
            {
                "img": "https://arxiv.org/html/2503.24388/x7.png",
                "caption": "Figure A2:Training pipeline of RIG.S0/S1 pretrain the model by aligning real and imagined flows. S2/S3 enhance reasoning and reviewing via GPT-4o relabeling. S4 aligns temporally predicted trajectories (dream flow) with environment-grounded traces.",
                "position": 983
            },
            {
                "img": "https://arxiv.org/html/2503.24388/x8.png",
                "caption": "Figure A3:Qualitative example of lookahead and review.The agent understands the environment (1–2), simulates future states (3), and refines its decision through internal review before acting (4), successfully avoiding a hidden hazard.",
                "position": 986
            },
            {
                "img": "https://arxiv.org/html/2503.24388/x9.png",
                "caption": "Figure A4:Task distribution.Our datasets include various embodied tasks with varying complexity, ensuring strong generalization across downstream goals.",
                "position": 1607
            },
            {
                "img": "https://arxiv.org/html/2503.24388/x10.png",
                "caption": "Figure A5:Case study comparison with GPT-4o.Given the same input and prompt (chop a tree), RIG reasons and imagines future states to choose a reachable tree and adjust position before acting. GPT-4o, despite high visual quality, misjudges the distance, executes an invalid action, and fails to revise its plan.",
                "position": 1642
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]