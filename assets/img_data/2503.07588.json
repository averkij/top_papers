[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07588/x1.png",
                "caption": "Figure 1:High-resolution strategy comparison for modular LVLMs. (a) and (b) show existing grid-based cropping methods face challenges when processing large RSIs. (c) The proposed dynamic pyramid-based token pruning strategy can dynamically select image tiles of key regions related to the input text, balancing image detail and computational cost.",
                "position": 93
            },
            {
                "img": "https://arxiv.org/html/2503.07588/x2.png",
                "caption": "Figure 2:The pipeline of the proposed method. The entire process iterates in a coarse-to-fine manner, dynamically retrieving high-resolution features from the next DIP level (leftward orange arrow) or performing token pruning at the current level (rightward orange arrow) based on the output of the RFM module at each iteration. During training, the RFM distillation text-related attention from the LLM; during inference, RFM generates the attention scores for the input vision tokens. GSD means ground sample distance.",
                "position": 136
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminaries",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07588/x3.png",
                "caption": "Figure 3:The proposed RFM and attention distillation strategy. The left part indicates our core idea: distill accurate text-related key region localization ability from the LLM part of the LVLM. The right part shows the distillation details. We only select specific layer pairs for distillation to avoid hidden state discontinuities. “sys token” represents the tokens from the system prompt.",
                "position": 194
            }
        ]
    },
    {
        "header": "4Method and Benchmark",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07588/extracted/6268299/Figures/VQA_create.png",
                "caption": "Figure 4:The construction pipeline of the proposed LRS-VQA dataset. The visual prompt (red box) is inspired by SoM[68].",
                "position": 434
            },
            {
                "img": "https://arxiv.org/html/2503.07588/extracted/6268299/Figures/acc_pixel_line.jpg",
                "caption": "Figure 5:The accuracy trends of Qwen2-VL across varying input maximum pixels. This demonstrates that accuracy on both the manually annotated MME-RealWorld-RS and our proposed LRS-VQA exhibit a positive correlation with resolution improvement, proving the effectiveness of LRS-VQA in evaluating LVLM’s high-resolution RSI perception capabilities.",
                "position": 450
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2503.07588/x4.png",
                "caption": "Figure 6:Text-related visual attention localization in the last layer if trained RFM under LLaVA-Next-Qwen2.",
                "position": 1211
            }
        ]
    },
    {
        "header": "6Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]