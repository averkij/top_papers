[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26278/GraphicalAbstract.png",
                "caption": "",
                "position": 100
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26278/ProfVLM.jpeg",
                "caption": "Figure 1:ProfVLM architecture. Multi-view video is encoded by a frozen TimeSformer, fused via a trainable AttentiveGatedProjector, and decoded by a LoRA-tuned SmolLM2 to generate both proficiency label and commentary.",
                "position": 157
            }
        ]
    },
    {
        "header": "2Background and Related Works",
        "images": []
    },
    {
        "header": "3Proposed Methodology",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.26278/AttentiveGatedProjector.png",
                "caption": "Figure 2:AttentiveGatedProjector architecture. Multi-view input features are initially normalized and combined using multi-head attention. The aggregated representation is then refined through a feed-forward block with residual paths and a trainable gating unit. Finally, the output undergoes linear projection, normalization, and alignment to the language model space via learned scaling and shifting parameters.",
                "position": 215
            }
        ]
    },
    {
        "header": "4Experimental Setup",
        "images": []
    },
    {
        "header": "5Results",
        "images": []
    },
    {
        "header": "6Limitations",
        "images": []
    },
    {
        "header": "7Discussion and Conclusions",
        "images": []
    },
    {
        "header": "Acknowledgements",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]