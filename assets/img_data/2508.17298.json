[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17298/x1.png",
                "caption": "Figure 1:Illustration of visual reasoning tasks, highlighting the differences between monolithic and compositional approaches. Monolithic models map input to output directly, which often leads to hallucinations or incorrect outputs due to the lack of intermediate reasoning. In contrast, compositional methods explicitly break down the task into a sequence of interpretable reasoning steps. Each step is grounded in visual evidence, allowing the model to progressively infer the answer with greater transparency, accuracy, and robustness.",
                "position": 268
            }
        ]
    },
    {
        "header": "2Background",
        "images": []
    },
    {
        "header": "3Why Compositional Visual Reasoning?",
        "images": []
    },
    {
        "header": "4Key Stages of Compositional VR Paradigms",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17298/x2.png",
                "caption": "Figure 2:Key shift from monolithic reasoning to compositional reasoning.",
                "position": 719
            },
            {
                "img": "https://arxiv.org/html/2508.17298/x3.png",
                "caption": "Figure 3:Roadmap of compositional visual reasoning models.",
                "position": 756
            },
            {
                "img": "https://arxiv.org/html/2508.17298/x4.png",
                "caption": "Figure 4:Overview of prompt-enhanced language-centric (Stage I) pipeline.",
                "position": 798
            },
            {
                "img": "https://arxiv.org/html/2508.17298/x5.png",
                "caption": "Figure 5:Tool-enhanced LLMs (Stage II) and VLMs (Stage III) pipeline.",
                "position": 859
            },
            {
                "img": "https://arxiv.org/html/2508.17298/x6.png",
                "caption": "Figure 6:Monolithic versus Chain-of-Thought (CoT) reasoning VLMs (Stage IV) pipeline.",
                "position": 924
            },
            {
                "img": "https://arxiv.org/html/2508.17298/x7.png",
                "caption": "Figure 7:Illustration of unified agentic VLM (Stage V) inference. The model iteratively refines its understanding by appending intermediate outputs to the input and dynamically invoking internal capabilities to resolve complex visual reasoning tasks.",
                "position": 972
            }
        ]
    },
    {
        "header": "5Benchmark and Evaluation",
        "images": [
            {
                "img": "https://arxiv.org/html/2508.17298/x8.png",
                "caption": "Figure 8:Examples of benchmarks and datasets",
                "position": 1029
            }
        ]
    },
    {
        "header": "6Insights, Challenges and Directions",
        "images": []
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]