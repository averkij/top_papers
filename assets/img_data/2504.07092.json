[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07092/x1.png",
                "caption": "Figure 1:Where Should We Go?Object-centric learning (OCL) has focused on developing unsupervised mechanisms to separate the representation space into discreteslots. However, the inherent challenges of this task have led to comparatively less emphasis on exploring downstream applications and exploring fundamental benefits. Here, we introduce simple, effective OCL mechanisms by separating objects in pixel space and encoding them independently. We present a case study that demonstrates the downstream advantages of our approach for mitigating spurious correlations. We outline the need to develop benchmarks aligned with fundamental goals of OCL, and explore the downstream efficacy of OCL representations.",
                "position": 77
            }
        ]
    },
    {
        "header": "2Related work",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07092/x2.png",
                "caption": "Figure 2:Overview of Object-Centric Classification with Applied Masks (OCCAM). There are two main parts. The first part (¬ß3.2.1) uses entity segmentation masks forobject-centric representation generation. The second part (¬ß3.2.2) performsrobust classificationby selecting representations corresponding to the foreground object and using them for classification. Indices[i0,‚Ä¶,ik,‚Ä¶]subscriptùëñ0‚Ä¶subscriptùëñùëò‚Ä¶[i_{0},\\ldots,i_{k},\\ldots][ italic_i start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ‚Ä¶ , italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , ‚Ä¶ ]correspond to each object in the scene.",
                "position": 111
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/images/movi_e/011.jpg",
                "caption": "Figure 3:Qualitative Results on Object Discovery.Dinosaur,SlotDiffusion, andFT-Dinosaurare existing object-centric learning (OCL) approaches.SamandHQESrefer to zero-shot segmentation methods. Images are fromMovi-E.Samand HQES masks fit objects much better than the masks predicted by OCL methods. All columns except for HQES are taken from[11].",
                "position": 293
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/dinosaur/movi_e/011.jpg",
                "caption": "",
                "position": 316
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/slotdiff/movi_e/011.jpg",
                "caption": "",
                "position": 323
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/hires_base/movi_e/011.jpg",
                "caption": "",
                "position": 330
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/sam/movi_e/011.jpg",
                "caption": "",
                "position": 337
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/crop_former/movi_e/011.jpg",
                "caption": "",
                "position": 344
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/images/movi_e/046.jpg",
                "caption": "",
                "position": 353
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/dinosaur/movi_e/046.jpg",
                "caption": "",
                "position": 360
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/slotdiff/movi_e/046.jpg",
                "caption": "",
                "position": 367
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/hires_base/movi_e/046.jpg",
                "caption": "",
                "position": 374
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/sam/movi_e/046.jpg",
                "caption": "",
                "position": 381
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/crop_former/movi_e/046.jpg",
                "caption": "",
                "position": 388
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/images/movi_e/055.jpg",
                "caption": "",
                "position": 397
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/dinosaur/movi_e/055.jpg",
                "caption": "",
                "position": 404
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/slotdiff/movi_e/055.jpg",
                "caption": "",
                "position": 411
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/hires_base/movi_e/055.jpg",
                "caption": "",
                "position": 418
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/sam/movi_e/055.jpg",
                "caption": "",
                "position": 425
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/crop_former/movi_e/055.jpg",
                "caption": "",
                "position": 432
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/images/movi_e/086.jpg",
                "caption": "",
                "position": 441
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/dinosaur/movi_e/086.jpg",
                "caption": "",
                "position": 448
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/slotdiff/movi_e/086.jpg",
                "caption": "",
                "position": 455
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/hires_base/movi_e/086.jpg",
                "caption": "",
                "position": 462
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/sam/movi_e/086.jpg",
                "caption": "",
                "position": 469
            },
            {
                "img": "https://arxiv.org/html/2504.07092/extracted/6349064/figures/images_movie/crop_former/movi_e/086.jpg",
                "caption": "",
                "position": 476
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07092/x3.png",
                "caption": "Figure 4:Foreground Object Detection.ROC-curves for foreground detection methods. For each scoring scheme, we measure how well the true foreground objects in the ImageNet-validation dataset are detected. More details in ¬ßE.",
                "position": 1159
            }
        ]
    },
    {
        "header": "5Discussion",
        "images": []
    },
    {
        "header": "6Conclusion and open problems",
        "images": []
    },
    {
        "header": "Author Contributions",
        "images": []
    },
    {
        "header": "Acknowledgments",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix ACounterAnimals: gaps between ‚ÄúCommon‚Äù and ‚ÄúCounter‚Äù subsets",
        "images": [
            {
                "img": "https://arxiv.org/html/2504.07092/x4.png",
                "caption": "Figure 5:Gaps in accuracies [Common-Counter] forCommonandCountersubsets of CounterAnimals[72]dataset correspondingly for different CLIP models and pre-training datasets. ‚ÄúGap‚Äù results are computed for CLIP[52]zero-shot performance without using any masks; ‚ÄúGap-FG‚Äù results are computed when using OCCAM with HQES[42]masks,Class-Aidedforeground selection method, and ‚ÄúGray BG + Crop‚Äù mask applying operation.",
                "position": 2289
            }
        ]
    },
    {
        "header": "Appendix BDetails on spurious backgrounds datasets",
        "images": []
    },
    {
        "header": "Appendix CClass-Aided foreground detector yields the closest approximation to ground truth foreground masks",
        "images": []
    },
    {
        "header": "Appendix DExtended implementation details",
        "images": []
    },
    {
        "header": "Appendix EAdditional details on FG detectors comparison",
        "images": []
    }
]