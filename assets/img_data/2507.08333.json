[
    {
        "header": "Abstract",
        "images": []
    },
    {
        "header": "1Introduction",
        "images": []
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Preliminary",
        "images": []
    },
    {
        "header": "4Audio Inpainting using Discrete Diffusion Model",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08333/x1.png",
                "caption": "Figure 1:Our method operates on audio signals with missing (silent) segments. First, the input waveform—containing a silence gap—is processed by theWavTokenizerencoder, which converts the audio into a discrete sequence of tokens.\nNext, aDiTperforms inpainting by iteratively predicting the masked tokens, resulting in reconstructed token sequence.\nFinally, the reconstructed tokens are passed through the WavTokenizer’s decoder to synthesize the output audio waveform in the masked part.",
                "position": 328
            }
        ]
    },
    {
        "header": "5Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08333/x2.png",
                "caption": "Figure 2:Training pipeline illustrating the application of token corruption and backpropagation guided by the DWDSE loss",
                "position": 411
            }
        ]
    },
    {
        "header": "6Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2507.08333/x3.png",
                "caption": "Figure 3:Average objective metrics—including Log-Spectral Distance (LSD), Objective Difference Grade (ODG), and Fréchet Audio Distance (FAD)—computed across gap lengths ranging from 25 to 300 ms, following the evaluations of[24].",
                "position": 473
            }
        ]
    },
    {
        "header": "7Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    }
]