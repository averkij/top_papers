[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05348/x1.png",
                "caption": "",
                "position": 114
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05348/x2.png",
                "caption": "Figure 2:Pipeline Overview.We represent a dynamic scene using Gaussian primitives that can appear anytime anywhere.\nEach Gaussian is assigned with a motion function to to model its movement.\nAnd its opacity is modulated by the temporal opacity function which control the impact of the Gaussian primitive over time.\nWith this 4D representation, we further regularize the Gaussians with a 4D regularization loss and optimize the rasterization result with rendering loss for reconstructing a dynamic 3D scene from multi-view videos.",
                "position": 163
            }
        ]
    },
    {
        "header": "2Related Work",
        "images": []
    },
    {
        "header": "3Method",
        "images": []
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05348/x3.png",
                "caption": "Figure 3:Qualitative comparison on the ENeRF-Outdoor Dataset.Our method achieves higher quality for fast-moving objects and regions, such as the swinging arms and dolls in hands, and clearer text details on the clothes.",
                "position": 393
            },
            {
                "img": "https://arxiv.org/html/2506.05348/x4.png",
                "caption": "Figure 4:Qualitative comparison on the Neural 3D Video Dataset.Our method achieves the best rendering quality compared with baseline methods, especially for distant static regions and fast-moving dynamic regions.",
                "position": 398
            },
            {
                "img": "https://arxiv.org/html/2506.05348/x5.png",
                "caption": "Figure 5:Qualitative comparison on ourSelfCapDataset.Our method achieves significantly higher rendering quality than other methods.\nFor example, in the dance sequence, other methods struggle to handle fast-moving regions, such as fingers, faces, and texture details on clothes, while our method retains their details.\nIn the bike sequence, other methods failed to model the complex motions of hands and rapidly rotating pedals, while our method maintain high-quality rendering results.",
                "position": 403
            },
            {
                "img": "https://arxiv.org/html/2506.05348/x6.png",
                "caption": "Figure 6:Ablation study of proposed components on dance1 sequence of ourSelfCapDataset.Removing our proposed components leads to visible artifacts in the rendered results, especially in the dynamic regions with fast motion.\nOur method produce high-quality results even in the challenging dynamic scenes.",
                "position": 410
            }
        ]
    },
    {
        "header": "5Conclusions",
        "images": []
    },
    {
        "header": "Appendix AMore Experiments Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05348/x7.png",
                "caption": "Figure 7:Dynamic Region Mask.An illustration of the use of dynamic region masking during evaluation.\n(a) Ground Truth image from theSelfCapdataset. (b) Masked Ground Truth, generated by cropping the Ground Truth image using a dynamic region mask extracted by Background Matting V2[24], with the outer area filled in black.",
                "position": 1301
            }
        ]
    },
    {
        "header": "Appendix BImplementation Details",
        "images": [
            {
                "img": "https://arxiv.org/html/2506.05348/x8.png",
                "caption": "Figure 8:Qualitative comparison on ourSelfCapDataset.",
                "position": 2511
            },
            {
                "img": "https://arxiv.org/html/2506.05348/x9.png",
                "caption": "Figure 9:Qualitative comparison on ourSelfCapDataset.",
                "position": 2515
            },
            {
                "img": "https://arxiv.org/html/2506.05348/x10.png",
                "caption": "Figure 10:Qualitative comparison on the Neural 3D Video Dataset.",
                "position": 2519
            }
        ]
    },
    {
        "header": "References",
        "images": []
    }
]