[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21842/Figure/model_performance_comparison.png",
                "caption": "Figure 1:Performance of DeepTravel on synthesized offline benchmark and online user data.",
                "position": 109
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21842/Figure/fig_2.png",
                "caption": "Figure 2:Comparison between existing studies and our autonomous travel planning agent paradigm.",
                "position": 128
            }
        ]
    },
    {
        "header": "2Preliminary",
        "images": []
    },
    {
        "header": "3DeepTravel",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21842/Figure/fig_3.png",
                "caption": "Figure 3:An overview of DeepTravel.",
                "position": 397
            }
        ]
    },
    {
        "header": "4Experiments",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21842/Figure/training_statistics_plots.png",
                "caption": "Figure 4:Validation reward (final pass rate), model entropy, average interaction turn and tool call accuracy (success rate) throughout agentic RL training process.",
                "position": 913
            },
            {
                "img": "https://arxiv.org/html/2509.21842/Figure/performance_evaluation_benchmarking.png",
                "caption": "Figure 5:Capacity comparison of the autonomous TP agent across 7 human-annotated dimensions, evaluated on 50 randomly sampled real-world online user case.",
                "position": 926
            }
        ]
    },
    {
        "header": "5Related Work",
        "images": []
    },
    {
        "header": "6Conclusion, Limitation and Future Work",
        "images": []
    },
    {
        "header": "Ethics and Reproducibility Statement",
        "images": []
    },
    {
        "header": "References",
        "images": [
            {
                "img": "https://arxiv.org/html/2509.21842/Figure/training_statistics_all.png",
                "caption": "Figure 6:Metrics monitoring in RL training process.\nDuring agentic RL training, we periodically monitor a set of key indicators: policy entropy, training gradient norm, sample keep rate, loss-mask ratio, average response length, training reward, verifier success rate, tool-call accuracy, and average number of turns.\nAnomalies or regressions in any of these metrics can precipitate training failure, which also indicates the challenges of agentic RL.",
                "position": 1748
            }
        ]
    },
    {
        "header": "Appendix AAppendix",
        "images": []
    }
]