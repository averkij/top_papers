[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x1.png",
                "caption": "Figure 1:We partition the next-scale prediction process into the efficient collaboration between large and small VAR models.",
                "position": 66
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x2.png",
                "caption": "Figure 2:Comparison of generation results between original VAR-d30 (up) and our VAR-CoDE (bottom) for ImageNet 256Ã—\\timesÃ—256. Our method achieves 1.7x speedup (3.62s to 2.11s), and needs only 0.5x memory space (40GB to 20GB), with negligible quality degradation.",
                "position": 72
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x3.png",
                "caption": "Figure 3:(a) Effectiveness of increasing parameters at thekğ‘˜kitalic_k-th scale is evaluated by predicting token maprksubscriptğ‘Ÿğ‘˜r_{k}italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTusing four VAR models with different parameter sizes (2B, 1B, 0.6B, and 0.3B), while other scales(r1,r2,â€¦,rkâˆ’1,rk+1,â€¦,r10)subscriptğ‘Ÿ1subscriptğ‘Ÿ2â€¦subscriptğ‘Ÿğ‘˜1subscriptğ‘Ÿğ‘˜1â€¦subscriptğ‘Ÿ10(r_{1},r_{2},\\dots,r_{k-1},r_{k+1},\\dots,r_{10})( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_r start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT , â€¦ , italic_r start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT )are generated using the largest VAR-d30 model. (b) Fourier spectrum analysis is conducted on generated content at the first 3 scales and the last 3 scales. (c) Training-free performance comparison of model collaboration decoding across various settings of draft tokensMğ‘€Mitalic_Mand refiner tokens680âˆ’M680ğ‘€680-M680 - italic_M.",
                "position": 98
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x4.png",
                "caption": "Figure 4:Overview of the collaborative decoding process, we use a drafting stepN=6ğ‘6N=6italic_N = 6for instance. CoDe uses a large VAR model as the drafterÏµÎ¸dsubscriptitalic-Ïµsubscriptğœƒğ‘‘\\epsilon_{\\theta_{d}}italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPTto generate the token mapsRL=(r1,r2,â€¦,rN)subscriptğ‘…ğ¿subscriptğ‘Ÿ1subscriptğ‘Ÿ2â€¦subscriptğ‘Ÿğ‘R_{L}=(r_{1},r_{2},\\ldots,r_{N})italic_R start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = ( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_r start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )at smaller scales. The small refiner modelÏµÎ¸rsubscriptitalic-Ïµsubscriptğœƒğ‘Ÿ\\epsilon_{\\theta_{r}}italic_Ïµ start_POSTSUBSCRIPT italic_Î¸ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUBSCRIPTthen usesRLsubscriptğ‘…ğ¿R_{L}italic_R start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTas an initial prefix to efficiently predict the remaining token mapsRH=(rN+1,rN+2,â€¦,rK)subscriptğ‘…ğ»subscriptğ‘Ÿğ‘1subscriptğ‘Ÿğ‘2â€¦subscriptğ‘Ÿğ¾R_{H}=(r_{N+1},r_{N+2},\\ldots,r_{K})italic_R start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = ( italic_r start_POSTSUBSCRIPT italic_N + 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_N + 2 end_POSTSUBSCRIPT , â€¦ , italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT )at larger scales. Both models are fine-tuned on their designated predictive scales using ground truth labelsrkâˆ—superscriptsubscriptğ‘Ÿğ‘˜r_{k}^{*}italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPTand teacher logitspteacherâ¢(rk)subscriptğ‘teachersubscriptğ‘Ÿğ‘˜p_{\\text{teacher}}(r_{k})italic_p start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT ( italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), respectively.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2411.17787/x5.png",
                "caption": "Figure 5:(a) Our CoDe demonstrates the optimal efficiency-quality trade-off among all evaluated methods. (b) Inference latency is measured across varying batch sizes for the original VAR-d30, our CoDe (N=6), and the VQVAE decoder. (c) We analyze the time cost associated with parallel decoding at each scale, showing that the refiner model is significantly more efficient than the drafter at larger scales.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x6.png",
                "caption": "Figure 6:Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps.",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2411.17787/x7.png",
                "caption": "Figure 7:Qualitative results of CoDeâ€™s zero-shot generalization on image inpainting and image editing.",
                "position": 1068
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x8.png",
                "caption": "Figure 8:Up: images generated by the original VAR-d16 models. Down: images generated by the perturbation fine-tuned VAR-d16.",
                "position": 3630
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Results.",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x9.png",
                "caption": "Figure 9:Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps.",
                "position": 4265
            },
            {
                "img": "https://arxiv.org/html/2411.17787/x10.png",
                "caption": "Figure 10:Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps.",
                "position": 4268
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Future Work",
        "images": []
    }
]