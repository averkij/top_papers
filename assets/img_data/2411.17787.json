[
    {
        "header": "Abstract",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x1.png",
                "caption": "Figure 1:We partition the next-scale prediction process into the efficient collaboration between large and small VAR models.",
                "position": 66
            }
        ]
    },
    {
        "header": "1Introduction",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x2.png",
                "caption": "Figure 2:Comparison of generation results between original VAR-d30 (up) and our VAR-CoDE (bottom) for ImageNet 256×\\times×256. Our method achieves 1.7x speedup (3.62s to 2.11s), and needs only 0.5x memory space (40GB to 20GB), with negligible quality degradation.",
                "position": 72
            }
        ]
    },
    {
        "header": "2Related Works",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x3.png",
                "caption": "Figure 3:(a) Effectiveness of increasing parameters at thek𝑘kitalic_k-th scale is evaluated by predicting token maprksubscript𝑟𝑘r_{k}italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPTusing four VAR models with different parameter sizes (2B, 1B, 0.6B, and 0.3B), while other scales(r1,r2,…,rk−1,rk+1,…,r10)subscript𝑟1subscript𝑟2…subscript𝑟𝑘1subscript𝑟𝑘1…subscript𝑟10(r_{1},r_{2},\\dots,r_{k-1},r_{k+1},\\dots,r_{10})( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT 10 end_POSTSUBSCRIPT )are generated using the largest VAR-d30 model. (b) Fourier spectrum analysis is conducted on generated content at the first 3 scales and the last 3 scales. (c) Training-free performance comparison of model collaboration decoding across various settings of draft tokensM𝑀Mitalic_Mand refiner tokens680−M680𝑀680-M680 - italic_M.",
                "position": 98
            }
        ]
    },
    {
        "header": "3Method",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x4.png",
                "caption": "Figure 4:Overview of the collaborative decoding process, we use a drafting stepN=6𝑁6N=6italic_N = 6for instance. CoDe uses a large VAR model as the drafterϵθdsubscriptitalic-ϵsubscript𝜃𝑑\\epsilon_{\\theta_{d}}italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPTto generate the token mapsRL=(r1,r2,…,rN)subscript𝑅𝐿subscript𝑟1subscript𝑟2…subscript𝑟𝑁R_{L}=(r_{1},r_{2},\\ldots,r_{N})italic_R start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = ( italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )at smaller scales. The small refiner modelϵθrsubscriptitalic-ϵsubscript𝜃𝑟\\epsilon_{\\theta_{r}}italic_ϵ start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUBSCRIPTthen usesRLsubscript𝑅𝐿R_{L}italic_R start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPTas an initial prefix to efficiently predict the remaining token mapsRH=(rN+1,rN+2,…,rK)subscript𝑅𝐻subscript𝑟𝑁1subscript𝑟𝑁2…subscript𝑟𝐾R_{H}=(r_{N+1},r_{N+2},\\ldots,r_{K})italic_R start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = ( italic_r start_POSTSUBSCRIPT italic_N + 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_N + 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT )at larger scales. Both models are fine-tuned on their designated predictive scales using ground truth labelsrk∗superscriptsubscript𝑟𝑘r_{k}^{*}italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPTand teacher logitspteacher⁢(rk)subscript𝑝teachersubscript𝑟𝑘p_{\\text{teacher}}(r_{k})italic_p start_POSTSUBSCRIPT teacher end_POSTSUBSCRIPT ( italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), respectively.",
                "position": 134
            },
            {
                "img": "https://arxiv.org/html/2411.17787/x5.png",
                "caption": "Figure 5:(a) Our CoDe demonstrates the optimal efficiency-quality trade-off among all evaluated methods. (b) Inference latency is measured across varying batch sizes for the original VAR-d30, our CoDe (N=6), and the VQVAE decoder. (c) We analyze the time cost associated with parallel decoding at each scale, showing that the refiner model is significantly more efficient than the drafter at larger scales.",
                "position": 238
            }
        ]
    },
    {
        "header": "4Experimental Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x6.png",
                "caption": "Figure 6:Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps.",
                "position": 1050
            },
            {
                "img": "https://arxiv.org/html/2411.17787/x7.png",
                "caption": "Figure 7:Qualitative results of CoDe’s zero-shot generalization on image inpainting and image editing.",
                "position": 1068
            }
        ]
    },
    {
        "header": "5Conclusion",
        "images": []
    },
    {
        "header": "References",
        "images": []
    },
    {
        "header": "Appendix AAdditional Quantitative Results",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x8.png",
                "caption": "Figure 8:Up: images generated by the original VAR-d16 models. Down: images generated by the perturbation fine-tuned VAR-d16.",
                "position": 3630
            }
        ]
    },
    {
        "header": "Appendix BMore Qualitative Results.",
        "images": [
            {
                "img": "https://arxiv.org/html/2411.17787/x9.png",
                "caption": "Figure 9:Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps.",
                "position": 4265
            },
            {
                "img": "https://arxiv.org/html/2411.17787/x10.png",
                "caption": "Figure 10:Qualitative comparison between the original VAR-d30 model and our proposed CoDe model, with different drafting steps.",
                "position": 4268
            }
        ]
    },
    {
        "header": "Appendix CLimitations and Future Work",
        "images": []
    }
]